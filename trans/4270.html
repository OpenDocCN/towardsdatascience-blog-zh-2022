<html>
<head>
<title>Prompt Context Learning in Vision-Language Fine-tuning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">视觉语言微调中的即时上下文学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/prompt-context-learning-in-vision-language-fine-tuning-3608e39ebcaf#2022-09-21">https://towardsdatascience.com/prompt-context-learning-in-vision-language-fine-tuning-3608e39ebcaf#2022-09-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cc24" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用于有效模型适应的参数有效方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a5c6348def79dd96125eceb0fc757071.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*x8HeIH4MF_jsmLx0"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@maxberg?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马克西姆·伯格</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="ff52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更新:</p><p id="087f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">【2022年9月27日】追加<a class="ae ky" href="https://arxiv.org/pdf/2204.03574.pdf" rel="noopener ugc nofollow" target="_blank">T5】CSPT7】</a></p><p id="ca20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">【2022年9月29日】追加<a class="ae ky" href="https://arxiv.org/pdf/2112.04478.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> V-VL </em> </a></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="ee64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">视觉语言模型已经在计算机视觉领域产生了影响。通过图像-文本对之间的对齐学习，训练的图像编码器能够具有少量/零镜头学习能力，这可以以数据有效的方式实现快速模型适应。感兴趣的读者可以参考我以前的文章，了解视觉语言模型和剪辑的一般介绍:</p><div class="md me gp gr mf mg"><a rel="noopener follow" target="_blank" href="/contrastive-pre-training-of-visual-language-models-848dd94c881b"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd iu gy z fp ml fr fs mm fu fw is bi translated">视觉语言模型的对比预训练</h2><div class="mn l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">对比视角下充分利用监督信号</h3></div><div class="mo l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">towardsdatascience.com</p></div></div><div class="mp l"><div class="mq l mr ms mt mp mu ks mg"/></div></div></a></div><p id="364f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下游视觉任务中仅利用图像编码器的缺点是来自文本编码器的提示信息被放弃。文本编码器提供即时嵌入，这有助于下游视觉任务的性能。因此，通过在下游任务的微调中协作两个编码器，我们可以研究图像-文本嵌入之间的交互，并理解它们在有效的模型适应中的有效性。</p><p id="4461" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有效的模型适应可以通过两种方式实现:数据有效和参数有效。这两种方法都通过减少相关资源(标记数据和模型参数)来减轻模型训练负担。前者可以帮助实现少/零镜头学习，而后者可以通过只训练总参数的一小部分来实现高性能。<strong class="lb iu">即时语境学习兼具两者优势</strong>。</p><p id="d3e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">提示上下文学习是一种微调<strong class="lb iu">提示向量</strong>的方法，以实现视觉语言模型的高效模型适应。<strong class="lb iu">如果没有学习，提示上下文是由人类创建的，并且最优性是未知的</strong>。在这篇文章中，我将总结一些最近在即时语境学习方面的成就。</p><h2 id="77bc" class="mv mw it bd mx my mz dn na nb nc dp nd li ne nf ng lm nh ni nj lq nk nl nm nn bi translated">CoOp和CoCoOp</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/ab262583d1ec044ab58533829d0d4d36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mH81xay90yU81WkA"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://arxiv.org/pdf/2109.01134.pdf" rel="noopener ugc nofollow" target="_blank">人类创建的与学习的提示上下文</a>)</p></figure><p id="0832" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看上面的图片。在即时上下文学习的帮助下，各种评估数据集的准确性都有很大提高。特别是对于一些专门的数据集，例如EuroSAT，精确度惊人地从大约30增加到超过80。结果来自于<a class="ae ky" href="https://arxiv.org/pdf/2109.01134.pdf" rel="noopener ugc nofollow" target="_blank"> CoOp </a>论文，模型架构如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/f4908a14bf91f41aad3d31721c85d96f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OgRb2lnrerfFvkXM"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://arxiv.org/pdf/2109.01134.pdf" rel="noopener ugc nofollow" target="_blank"> CoOp建筑</a></p></figure><p id="53fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CoOp是最早的即时上下文学习架构。CoOp和CLIP之间唯一区别是左上角的部分:<strong class="lb iu">可学习上下文</strong>。在微调过程中，只有这部分通过反向传播进行更新。很简单但是很有效！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/db2e00be702494c9e32400b568aa6ddf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d12jtlrbnJAahgs_nx23fQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.pdf" rel="noopener ugc nofollow" target="_blank"> CLIP vs. CoOp vs. CoCoOp </a>)</p></figure><p id="8a55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，CoOp的一个缺点是泛化能力差，这是由于学习类的<strong class="lb iu">过拟合</strong>。如上所示，在对看不见的类(如火车铁路)进行评估时，准确度会下降。<em class="lv">这很有趣，因为</em>在CoOp中冻结的文本编码器，当输入句子由人类创建的提示上下文和看不见的类名组合时，具有很强的零触发泛化能力，如<a class="ae ky" href="https://arxiv.org/pdf/2103.00020.pdf" rel="noopener ugc nofollow" target="_blank"> CLIP </a>论文所示。<strong class="lb iu">但是CLIP的零镜头概化对于已学习的提示上下文并不保留</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/6ba091ad3a9f702ae8342e4770337f82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NcJ8eawBlm0JZFGk"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.pdf" rel="noopener ugc nofollow" target="_blank"> CoCoOp架构</a>)</p></figure><p id="e5d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上所示，同一组作者提出了<a class="ae ky" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.pdf" rel="noopener ugc nofollow" target="_blank"> CoCoOp </a>作为CoOp的扩展，以恢复原始剪辑的零镜头可推广性。他们通过使用每个输入图像作为即时上下文学习的条件来实现它。编码图像嵌入通过<em class="lv">元网</em>、<em class="lv">、</em>两层瓶颈子网，生成条件元令牌<em class="lv"> π、</em>，添加到每个上下文令牌中。在训练期间，只有<em class="lv">元网</em>和上下文令牌向量用反向传播进行微调。</p><p id="4808" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在推理过程中，不可见类的图像嵌入与学习到的上下文标记融合，以动态生成文本嵌入<strong class="lb iu"/>。由于这种动态文本嵌入包括来自相应的看不见的类图像的信息，所以它更有可能与相应的图像嵌入对齐，并且提高零拍摄性能。</p><p id="c46d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与CoCoOp相反，CoOp在没有图像调节的情况下学习提示上下文，这导致了<strong class="lb iu">静态</strong>提示上下文。因此，在零镜头推理过程中，由于<strong class="lb iu">过度拟合</strong>提示上下文，而提示上下文对输入的看不见的类别图像是不可知的，所以性能会大大下降。回想一下，在CLIP中，人为设计的提示上下文是<em class="lv">中性</em>，不能过拟合。这可能是CLIP具有很强的零镜头泛化能力的原因之一。</p><h2 id="a6fa" class="mv mw it bd mx my mz dn na nb nc dp nd li ne nf ng lm nh ni nj lq nk nl nm nn bi translated">DenseCLIP</h2><p id="5a8e" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">今年CVPR与CoCoOp的一个平行作品是<a class="ae ky" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rao_DenseCLIP_Language-Guided_Dense_Prediction_With_Context-Aware_Prompting_CVPR_2022_paper.pdf" rel="noopener ugc nofollow" target="_blank"> DenseCLIP </a>。与CLIP和(Co)CoOp使用全局图像嵌入与图像分类中的相应文本嵌入对齐不同，DenseCLIP旨在通过利用像素级图像嵌入来提高下游<em class="lv">密集</em>任务的性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/d1f8501a93a247267b4370593276bfea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hk_VquZX4jgN3h0_"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rao_DenseCLIP_Language-Guided_Dense_Prediction_With_Context-Aware_Prompting_CVPR_2022_paper.pdf" rel="noopener ugc nofollow" target="_blank"> DenseCLIP架构</a>)</p></figure><p id="25f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上所示，图像编码后，我们有2D像素级图像嵌入。在CLIP和(Co)CoOp中，这些2D像素级图像嵌入被汇集成1D嵌入向量，表示输入图像的全局特征。在DenseCLIP中，这些2D嵌入被保留用于下游密集任务的性能改进，例如语义分割。2D嵌入的每个像素，即1D特征向量，也与相应的1D文本嵌入相乘，以获得像素-文本得分图，其中较高(较低)的得分表示较高(较低)的相似性。像素级得分图可以<strong class="lb iu">连接</strong>到2D图像嵌入，以提高下游微调的性能。</p><p id="599c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2D嵌入通过一个转换器作为条件来生成动态<em class="lv">提示上下文嵌入，如在CoCoOp中一样。作者使用交叉注意来交互图像和文本嵌入，其中文本嵌入充当<em class="lv">查询</em>，图像嵌入充当<em class="lv">键和值</em>。在交互之后，使用原始和交叉注意编码的文本嵌入的总和。该过程如下所示:</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/9349e961012817ab46d48528402eb9df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iFaUHD83vWbi-e837x7wvw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rao_DenseCLIP_Language-Guided_Dense_Prediction_With_Context-Aware_Prompting_CVPR_2022_paper.pdf" rel="noopener ugc nofollow" target="_blank">dense clip中的上下文感知提示</a></p></figure><p id="9634" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要学习的参数是transformer解码器中的提示上下文向量和交叉注意模块。与(Co)CoOp和CLIP中文本嵌入与图像级嵌入对齐不同，DenseCLIP中的文本嵌入与像素级嵌入交叉参与。通过这种细粒度的调节，DenseCLIP学习到的<strong class="lb iu">提示上下文涉及像素级的关联性信息和相应的2D图像嵌入</strong>，这有助于提高下游密集任务的性能。这种密集对齐学习也在另一个分支工作中进行研究，称为<em class="lv">视觉语言模型的对象级预训练</em>，这种训练更加复杂和棘手。我还有一篇文章详细阐述了这个话题:</p><div class="md me gp gr mf mg"><a href="https://ai.plainenglish.io/object-level-vision-language-contrastive-pre-training-8f0552b4b109" rel="noopener  ugc nofollow" target="_blank"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd iu gy z fp ml fr fs mm fu fw is bi translated">目标级视觉语言对比预训练</h2><div class="mn l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">用于在没有人工注释的情况下对齐对象级视觉和语言特征的显式和隐式方法</h3></div><div class="mo l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">ai .平原英语. io</p></div></div><div class="mp l"><div class="nx l mr ms mt mp mu ks mg"/></div></div></a></div><p id="25f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于在文本编码之后，文本嵌入与嵌入空间中的图像嵌入是交叉参与的，所以在推断期间，文本嵌入可以被保留，而无需从文本编码器进行推断。这不同于CoCoOp，在CoCoOp中，提示标记在文本编码器之前与图像嵌入相加，因此在推断期间不能省略计算。</p><p id="ef73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DenseCLIP的另一个优点是，该架构中的图像编码器可以由任意的<strong class="lb iu">图像编码器取代，而不一定是预先训练好的剪辑中的编码器。虽然图像嵌入最初没有与相应的文本嵌入对齐，但是它们可以在微调下游任务中以少量拍摄的方式有效地对齐。<strong class="lb iu">在文本编码器的帮助下，少镜头学习通过对齐相应的图像和文本嵌入来工作，实现类似于多镜头单峰图像编码器的性能</strong>。</strong></p><h2 id="2ac3" class="mv mw it bd mx my mz dn na nb nc dp nd li ne nf ng lm nh ni nj lq nk nl nm nn bi translated">芯片尺寸封装</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/5da8d299afa03df3fb20d549f5117c27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g36iYvp28tKQtnqfJY1quA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://arxiv.org/pdf/2204.03574.pdf" rel="noopener ugc nofollow" target="_blank"> CSP架构</a>)</p></figure><p id="98f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">提出合成软提示(CSP)作为上述方法的替代:当上下文(例如的照片)是如上所示的人类设计时，类名被调整。CSP起源于<strong class="lb iu">组合学习</strong>的思想，其中一个类概念被<strong class="lb iu">分解</strong>为属性和对象词汇，例如蓝猫、幼虎。在训练期间，可学习的(属性，对象)向量对，具有从预训练的剪辑文本编码器初始化的值，用于替换提示模板中的[属性]和[对象]以获得文本嵌入。<strong class="lb iu">在推理过程中，学习到的属性和对象词汇向量被重组以形成看不见的类别概念，从而提高零射击开放世界概括的性能</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/3230f16b076788519eeb5a3b64307d4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CZul3ivjkV0o1pV7nhDr6g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(【CLIP和CSP的提示模板替换比较</p></figure><p id="f9a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CSP可以被视为人类设计的提示上下文和可学习的类词汇的组合。根据CoOp的调查，由于可学习的提示上下文容易过度适应，<strong class="lb iu">人为设计的提示上下文对于防止过度适应很重要</strong>。虽然CSP不像CoCoOp和DenseCLIP那样使用图像条件提示学习，但学习的属性和对象词汇向量的重组可以生成<strong class="lb iu">大量</strong>看不见的类概念，这也大大提高了零镜头推广能力。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/d12ba3b67b5740d2214436e62cdd71ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tLtR40_nFU3amg6ij-qAPQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://arxiv.org/pdf/2204.03574.pdf" rel="noopener ugc nofollow" target="_blank">使用CSP的组合零镜头学习概述</a>)</p></figure><p id="a1c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CSP是CLIP比(Co)CoOp和DenseCLIP自然得多的扩展:保留了人类设计的提示上下文；提示中的类名扩展为属性和对象，并进行微调；学习的属性和对象向量在零镜头推理期间被重组。CSP也是参数高效的，尽管文本编码器也是微调过的。CSP的唯一缺点是巨大的重组提示表示，在推断过程中必须计算与图像嵌入的余弦相似性。然而，<strong class="lb iu">重组的提示表示可以预先计算并保存在内存中，以空间换取时间效率</strong>。</p><p id="c88e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据作者的说法，CSP比CoOp平均高出5.8%。我对它与CoCoOp的比较感兴趣，<strong class="lb iu">表明了图像条件和纯文本提示学习之间的普遍性</strong>。</p><h2 id="10d0" class="mv mw it bd mx my mz dn na nb nc dp nd li ne nf ng lm nh ni nj lq nk nl nm nn bi translated">五. VL</h2><p id="d565" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/pdf/2112.04478.pdf" rel="noopener ugc nofollow" target="_blank"> V-VL </a>，即基于视频的视觉语言预训练是I-VL(即基于图像的视觉语言预训练)的扩展，以将文本嵌入与相应的视频嵌入对齐，从而在零/少镜头模型适应中提高性能。由于(视频，文本)对是稀缺和沉重的，而(图像，文本)对是巨大的，可以从网络上大量爬取，V-VL的作者提出了一种基于预训练的I-VL模型(如CLIP)的参数有效的模型自适应方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/32037037058c038a64bee641439ef614.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mDS0M1o_D7JHn_fNc2gznA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://arxiv.org/pdf/2112.04478.pdf" rel="noopener ugc nofollow" target="_blank">在V-VL </a>中提示设计)</p></figure><p id="dc1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在处理视频帧时，如何对时间轴进行编码至关重要。在V-VL中，作者使用<strong class="lb iu">多头自关注来整合帧级图像特征序列</strong>。在处理提示学习时，如何设计提示架构，即可学习的提示向量和类名标记之间的关系，是至关重要的。如上所示，作者<strong class="lb iu">在类名标记</strong>中预先计划和添加可学习向量。要预先计划和附加的可学习向量的数量(作者选择16个)是超参数。然而，作者声称优化可以完全学习忽略这些向量中的任何一个，因此消融研究是不必要的。</p><p id="bbf0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦计算出视频和文本嵌入，它们就通过<em class="lv"> NCE </em>损失来对齐。V-VL是参数有效的，因为只有可学习的提示向量和多头自我注意层被调整，其架构如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/617bbe19b2bf767df8bd44afffe46168.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L886MaNJUdG9Trwd0kZJxg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://arxiv.org/pdf/2112.04478.pdf" rel="noopener ugc nofollow" target="_blank"> V-VL架构</a>)</p></figure><p id="fc3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">V-VL是I-VL的一个高效和轻量级的适应，特别是对于视频中的<em class="lv">动作识别和定位</em>。总体来看，在CoOp、CSP和V-VL中，提示上下文中类名标记的数量和位置是不同的，这<strong class="lb iu">表明它们是超参数，也是可学习的</strong>。我认为类名标记的数量和位置可以以更加动态和结构化的方式与提示上下文交错，而不是像当前方法那样包含在输入序列中。</p><h2 id="6e8f" class="mv mw it bd mx my mz dn na nb nc dp nd li ne nf ng lm nh ni nj lq nk nl nm nn bi translated">参考</h2><p id="70b5" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/pdf/2109.01134.pdf" rel="noopener ugc nofollow" target="_blank">学习提示视觉语言模型，2022 </a></p><p id="54f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.pdf" rel="noopener ugc nofollow" target="_blank">视觉语言模型的条件提示学习，2022 </a></p><p id="ab4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rao_DenseCLIP_Language-Guided_Dense_Prediction_With_Context-Aware_Prompting_CVPR_2022_paper.pdf" rel="noopener ugc nofollow" target="_blank"> DenseCLIP:具有上下文感知提示的语言引导的密集预测，2022 </a></p><p id="628c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/pdf/2204.03574.pdf" rel="noopener ugc nofollow" target="_blank">学习作文零投学习软提示，2022 </a></p><p id="80f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/pdf/2112.04478.pdf" rel="noopener ugc nofollow" target="_blank">促进高效视频理解的视觉语言模型，2022年</a></p><div class="md me gp gr mf mg"><a href="https://dushuchen.medium.com/membership" rel="noopener follow" target="_blank"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd iu gy z fp ml fr fs mm fu fw is bi translated">加入我的介绍链接-陈数杜媒体</h2><div class="mn l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">阅读陈数·杜(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接支持…</h3></div><div class="mo l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">dushuchen.medium.com</p></div></div><div class="mp l"><div class="oc l mr ms mt mp mu ks mg"/></div></div></a></div></div></div>    
</body>
</html>