<html>
<head>
<title>Load Data From Postgres to BigQuery With Airflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Airflow将数据从Postgres加载到BigQuery</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/postgres-bigquery-airflow-e857e3c6aa7a#2022-07-08">https://towardsdatascience.com/postgres-bigquery-airflow-e857e3c6aa7a#2022-07-08</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="3e0c" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">使用Apache Airflow从PostgreSQL到BigQuery数据摄取—分步指南</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/d92c07bf30e309672adcc74eda6bff10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*34wVEr2NcPMmTSt9WwAx4Q.jpeg"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">在<a class="ae kz" href="https://unsplash.com/s/photos/transfer?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae kz" href="https://unsplash.com/es/@ventiviews?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Venti Views </a>拍摄的照片</p></figure><p id="a7ac" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">将数据从Postgres数据库(本地托管)接收到Google Cloud BigQuery的一种方法是使用Airflow，它提供了大量可用于数据接收和集成过程的操作符。</p><p id="ed0b" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">支持使用这些操作符也很重要，因为它们可以帮助我们编写更少更简单的代码，以便在更广泛的数据工程环境中执行基本操作。</p><p id="c2a4" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">现在，为了从Postgres表中提取数据并将其加载到BigQuery中，我们可以编写一个管道，首先使用正确的操作符将Postgres中的数据以csv格式提取到Google云存储中，然后将GCS中的csv文件加载到BigQuery中。</p><p id="5b76" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在今天的教程中，我们将一步一步地介绍如何使用Apache Airflow将数据从Postgres数据库导入Google Cloud BigQuery。我们将遵循三个单独的步骤，最终让我们从Postgres执行<em class="lw">完全加载</em>到Google云平台上的数据仓库，该数据仓库每天都会执行。</p></div><div class="ab cl lx ly hy lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="in io ip iq ir"><h2 id="d360" class="me mf iu bd mg mh mi dn mj mk ml dp mm lj mn mo mp ln mq mr ms lr mt mu mv mw bi translated">步骤1:将Postgres表提取到Google云存储中</h2><p id="6468" class="pw-post-body-paragraph la lb iu lc b ld mx jv lf lg my jy li lj mz ll lm ln na lp lq lr nb lt lu lv in bi translated">因此，第一步涉及一个过程，我们将数据从源Postgres表转移到Google云存储中。换句话说，我们将以csv格式将数据从源表导出到对象存储中。</p><p id="2dc1" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在深入研究细节之前，首先需要确保已经创建了用于连接Postgres数据库的连接。这可以通过Airflow UI(管理-&gt;连接)来实现。或者，为了更安全和可扩展地管理连接，您可以参考我在Medium上的一篇最新文章，在这篇文章中，我讨论了如何使用Airflow设置HashiCorp Vault。</p><div class="nc nd gq gs ne nf"><a rel="noopener follow" target="_blank" href="/hashicorp-vault-airflow-cfdddab31ea"><div class="ng ab fp"><div class="nh ab ni cl cj nj"><h2 class="bd iv gz z fq nk fs ft nl fv fx it bi translated">如何用气流设置HashiCorp保险库</h2><div class="nm l"><h3 class="bd b gz z fq nk fs ft nl fv fx dk translated">将HashiCorp Vault与Apache Airflow集成</h3></div><div class="nn l"><p class="bd b dl z fq nk fs ft nl fv fx dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="np l nq nr ns no nt kt nf"/></div></div></a></div><p id="0f46" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">现在，假设我们有一个有效的Postgres连接，我们现在可以利用包含在<code class="fe nu nv nw nx b">apache-airflow-providers-google</code>包中的<code class="fe nu nv nw nx b"><a class="ae kz" href="https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/transfers/postgres_to_gcs/index.html" rel="noopener ugc nofollow" target="_blank">PostgresToGCSOperator</a></code>,将数据从源数据库传输到Google云存储中。</p><pre class="kk kl km kn gu ny nx nz oa aw ob bi"><span id="a817" class="me mf iu nx b gz oc od l oe of">from airflow.providers.google.cloud.transfers.postgres_to_gcs import PostgresToGCSOperator</span><span id="d1ac" class="me mf iu nx b gz og od l oe of">GCS_BUCKET = 'my-bucket'<br/>GCS_OBJECT_PATH = 'postgres-test'<br/>SOURCE_TABLE_NAME = 'mytable'<br/>POSTGRESS_CONNECTION_ID = 'postgres'<br/></span><span id="1091" class="me mf iu nx b gz og od l oe of">postgres_to_gcs_task = PostgresToGCSOperator(<br/>    task_id=f'postgres_to_gcs',<br/>    postgres_conn_id=POSTGRES_CONNECTION_ID,<br/>    sql=f'SELECT * FROM {SOURCE_TABLE_NAME};',<br/>    bucket=GCS_BUCKET,<br/>    filename=f'{GCS_OBJECT_PATH}/{SOURCE_TABLE_NAME}.{FILE_FORMAT}',<br/>    export_format='csv',<br/>    gzip=False,<br/>    use_server_side_cursor=False,<br/>)</span></pre><p id="48df" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">本质上，这个任务将把数据从名为<code class="fe nu nv nw nx b">'mytable'</code>的源Postgres表传输到Google云存储中，更具体地说是在<code class="fe nu nv nw nx b">gs://my-bucket/postgres-test/mytable.csv</code>下。</p></div><div class="ab cl lx ly hy lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="in io ip iq ir"><h2 id="635d" class="me mf iu bd mg mh mi dn mj mk ml dp mm lj mn mo mp ln mq mr ms lr mt mu mv mw bi translated">步骤2:将数据从GCS加载到BigQuery</h2><p id="797d" class="pw-post-body-paragraph la lb iu lc b ld mx jv lf lg my jy li lj mz ll lm ln na lp lq lr nb lt lu lv in bi translated">既然我们已经将数据从Postgres复制到Google云存储中，我们就可以利用另一个名为<code class="fe nu nv nw nx b"><a class="ae kz" href="https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/transfers/gcs_to_bigquery/index.html" rel="noopener ugc nofollow" target="_blank">GCSToBigQueryOperator</a></code>的操作符将数据从云存储转移到BigQuery。</p><pre class="kk kl km kn gu ny nx nz oa aw ob bi"><span id="65c9" class="me mf iu nx b gz oc od l oe of">from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator</span><span id="3640" class="me mf iu nx b gz og od l oe of"><br/>BQ_DS = 'my_dataset'<br/>BQ_PROJECT = 'my-project'</span><span id="f8f9" class="me mf iu nx b gz og od l oe of">schema = [<br/>    {<br/>        'name': 'id',<br/>        'type': 'STRING',<br/>        'mode': 'NULLABLE',<br/>    },<br/>    {<br/>        'name': 'name',<br/>        'type': 'STRING',<br/>        'mode': 'NULLABLE',<br/>    },<br/>    {<br/>        'name': 'age',<br/>        'type': 'INTEGER',<br/>        'mode': 'NULLABLE',<br/>    },<br/>    {<br/>        'name': 'is_active',<br/>        'type': 'BOOLEAN',<br/>        'mode': 'NULLABLE',<br/>    },<br/>]</span><span id="0e9f" class="me mf iu nx b gz og od l oe of">gcs_to_bq_task = GCSToBigQueryOperator(<br/>    task_id=f'gcs_to_bq',<br/>    bucket=GCS_BUCKET,<br/>    source_objects=[f'{GCS_OBJECT_PATH}/{SOURCE_TABLE_NAME}.csv'],<br/>    destination_project_dataset_table='.'.join(<br/>        [BQ_PROJECT, BQ_DS, SOURCE_TABLE_NAME]<br/>    ),<br/>    schema_fields=schema,<br/>    create_disposition='CREATE_IF_NEEDED',<br/>    write_disposition='WRITE_TRUNCATE',<br/>    skip_leading_rows=1,<br/>    allow_quoted_newlines=True,<br/>)</span></pre><p id="8eb0" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">上面的代码将把bucket <code class="fe nu nv nw nx b">gs://my-bucket/postgres-test/mytable.csv</code>上的Google云存储中的数据加载到BigQuery表<code class="fe nu nv nw nx b">my-project.my_dataset.mytable</code>中。</p><p id="289a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">请注意，我们还必须指定BigQuery表的所需模式——您可能希望对此进行调整，以适应您的特定用例。还要注意，我们指示操作符跳过GCS上源文件中的第一行(<code class="fe nu nv nw nx b">skip_leading_rows=1</code>)，因为该行对应于我们并不真正需要的表列，假设我们已经在BigQuery上为目标表指定了所需的模式。</p></div><div class="ab cl lx ly hy lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="in io ip iq ir"><h2 id="276b" class="me mf iu bd mg mh mi dn mj mk ml dp mm lj mn mo mp ln mq mr ms lr mt mu mv mw bi translated">第三步:清理混乱</h2><p id="c295" class="pw-post-body-paragraph la lb iu lc b ld mx jv lf lg my jy li lj mz ll lm ln na lp lq lr nb lt lu lv in bi translated">现在，下一步涉及删除我们存储在Google云存储中的中间文件，这些文件是为了将数据从Postgres转移到BigQuery而创建的。</p><p id="6081" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">为此，我们可以再一次使用一个名为<code class="fe nu nv nw nx b">GCSDeleteObjectsOperator</code>的操作符，该操作符用于从Google云存储桶中删除对象，或者从对象名称的显式列表中删除对象，或者从匹配前缀的所有对象中删除对象。</p><pre class="kk kl km kn gu ny nx nz oa aw ob bi"><span id="1474" class="me mf iu nx b gz oc od l oe of">from airflow.providers.google.cloud.operators.gcs import GCSDeleteObjectsOperator<br/></span><span id="6292" class="me mf iu nx b gz og od l oe of">cleanup_task = GCSDeleteObjectsOperator(<br/>    task_id='cleanup',<br/>    bucket_name=GCS_BUCKET,<br/>    objects=[f'{GCS_OBJECT_PATH}/{SOURCE_TABLE_NAME}.csv'],<br/>)</span></pre></div><div class="ab cl lx ly hy lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="in io ip iq ir"><h2 id="de75" class="me mf iu bd mg mh mi dn mj mk ml dp mm lj mn mo mp ln mq mr ms lr mt mu mv mw bi translated">步骤4:编写最终气流DAG</h2><p id="72ff" class="pw-post-body-paragraph la lb iu lc b ld mx jv lf lg my jy li lj mz ll lm ln na lp lq lr nb lt lu lv in bi translated">现在，我们需要做的就是创建一个DAG，它将包含我们在本教程的上一部分中创建的三个任务。请注意，在最后，我们还指定了任务之间的依赖关系，以便它们可以按顺序执行。</p><pre class="kk kl km kn gu ny nx nz oa aw ob bi"><span id="d35b" class="me mf iu nx b gz oc od l oe of">from datetime import timedelta</span><span id="39ea" class="me mf iu nx b gz og od l oe of">with DAG(<br/>    dag_id='load_postgres_into_bq',<br/>    start_date=days_ago(1),<br/>    default_args={<br/>        'owner': 'airflow',<br/>        'retries': 2,<br/>        'retry_delay': timedelta(minutes=5),<br/>    },<br/>    schedule_interval='0 9 * * *',<br/>    max_active_runs=1,<br/>) as dag:<br/>    postgres_to_gcs_task = PostgresToGCSOperator(<br/>        task_id=f'postgres_to_gcs',<br/>        postgres_conn_id=POSTGRES_CONNECTION_ID,<br/>        sql=f'SELECT * FROM {SOURCE_TABLE_NAME};',<br/>        bucket=GCS_BUCKET,<br/>        filename=f'{GCS_OBJECT_PATH}/{SOURCE_TABLE_NAME}.{FILE_FORMAT}',<br/>        export_format='csv',<br/>        gzip=False,<br/>        use_server_side_cursor=False,<br/>    )</span><span id="d261" class="me mf iu nx b gz og od l oe of">    gcs_to_bq_task = return GCSToBigQueryOperator(<br/>        task_id=f'gcs_to_bq',<br/>        bucket=GCS_BUCKET,<br/>        source_objects=[f'{GCS_OBJECT_PATH}/{SOURCE_TABLE_NAME}.csv'],<br/>        destination_project_dataset_table='.'.join(<br/>            [BQ_PROJECT, BQ_DS, SOURCE_TABLE_NAME]<br/>        ),<br/>        schema_fields=schema,<br/>        create_disposition='CREATE_IF_NEEDED',<br/>        write_disposition='WRITE_TRUNCATE',<br/>        skip_leading_rows=1,<br/>        allow_quoted_newlines=True,<br/>    )</span><span id="55da" class="me mf iu nx b gz og od l oe of">    cleanup_task = GCSDeleteObjectsOperator(<br/>        task_id='cleanup',<br/>        bucket_name=GCS_BUCKET,<br/>        objects=[f'{GCS_OBJECT_PATH}/{SOURCE_TABLE_NAME}.csv'],<br/>    )</span><span id="9de8" class="me mf iu nx b gz og od l oe of">    postgres_to_gcs_task &gt;&gt; gcs_to_bq_task &gt;&gt; cleanup_task</span></pre></div><div class="ab cl lx ly hy lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="in io ip iq ir"><h2 id="13dc" class="me mf iu bd mg mh mi dn mj mk ml dp mm lj mn mo mp ln mq mr ms lr mt mu mv mw bi translated">完整代码</h2><p id="4d67" class="pw-post-body-paragraph la lb iu lc b ld mx jv lf lg my jy li lj mz ll lm ln na lp lq lr nb lt lu lv in bi translated">你可以在下面分享的GitHub Gist上找到这个教程的完整代码。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="oh oi l"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">本教程中使用的完整代码——如何将数据从Postgres数据移动到Google Cloud BigQuery</p></figure></div><div class="ab cl lx ly hy lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="in io ip iq ir"><h2 id="78f3" class="me mf iu bd mg mh mi dn mj mk ml dp mm lj mn mo mp ln mq mr ms lr mt mu mv mw bi translated">最后的想法</h2><p id="21e9" class="pw-post-body-paragraph la lb iu lc b ld mx jv lf lg my jy li lj mz ll lm ln na lp lq lr nb lt lu lv in bi translated">在今天的教程中，我们演示了一个将数据从Postgres数据库摄取到Google Cloud BigQuery的逐步方法。因为我们不能将Postgres上的源表中的数据加载到BigQuery中，所以我们使用Google云存储作为中间对象存储，Postgres表将被提取到csv文件中，然后该文件将被加载到Google Cloud BigQuery上的目标表中。</p></div><div class="ab cl lx ly hy lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="in io ip iq ir"><p id="1b77" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><a class="ae kz" href="https://gmyrianthous.medium.com/membership" rel="noopener"> <strong class="lc iv">成为会员</strong> </a> <strong class="lc iv">阅读介质上的每一个故事。你的会员费直接支持我和你看的其他作家。你也可以在媒体上看到所有的故事。</strong></p><div class="nc nd gq gs ne nf"><a href="https://gmyrianthous.medium.com/membership" rel="noopener follow" target="_blank"><div class="ng ab fp"><div class="nh ab ni cl cj nj"><h2 class="bd iv gz z fq nk fs ft nl fv fx it bi translated">通过我的推荐链接加入Medium-Giorgos Myrianthous</h2><div class="nm l"><h3 class="bd b gz z fq nk fs ft nl fv fx dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="nn l"><p class="bd b dl z fq nk fs ft nl fv fx dk translated">gmyrianthous.medium.com</p></div></div><div class="no l"><div class="oj l nq nr ns no nt kt nf"/></div></div></a></div></div><div class="ab cl lx ly hy lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="in io ip iq ir"><p id="676e" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">相关文章你可能也喜欢</strong></p><div class="nc nd gq gs ne nf"><a rel="noopener follow" target="_blank" href="/airflow-dags-decorators-b5dc03c76f07"><div class="ng ab fp"><div class="nh ab ni cl cj nj"><h2 class="bd iv gz z fq nk fs ft nl fv fx it bi translated">使用气流装饰器创作Dag</h2><div class="nm l"><h3 class="bd b gz z fq nk fs ft nl fv fx dk translated">用Python decorators创作Apache Airflow DAGs和任务</h3></div><div class="nn l"><p class="bd b dl z fq nk fs ft nl fv fx dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="ok l nq nr ns no nt kt nf"/></div></div></a></div></div><div class="ab cl lx ly hy lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="in io ip iq ir"><div class="kk kl km kn gu nf"><a rel="noopener follow" target="_blank" href="/run-airflow-docker-1b83a57616fb"><div class="ng ab fp"><div class="nh ab ni cl cj nj"><h2 class="bd iv gz z fq nk fs ft nl fv fx it bi translated">如何使用Docker在本地运行气流</h2><div class="nm l"><h3 class="bd b gz z fq nk fs ft nl fv fx dk translated">在本地机器上使用Docker运行Airflow的分步指南</h3></div><div class="nn l"><p class="bd b dl z fq nk fs ft nl fv fx dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="ol l nq nr ns no nt kt nf"/></div></div></a></div></div><div class="ab cl lx ly hy lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="in io ip iq ir"><div class="kk kl km kn gu nf"><a rel="noopener follow" target="_blank" href="/hashicorp-vault-airflow-cfdddab31ea"><div class="ng ab fp"><div class="nh ab ni cl cj nj"><h2 class="bd iv gz z fq nk fs ft nl fv fx it bi translated">如何用气流设置HashiCorp保险库</h2><div class="nm l"><h3 class="bd b gz z fq nk fs ft nl fv fx dk translated">将HashiCorp Vault与Apache Airflow集成</h3></div><div class="nn l"><p class="bd b dl z fq nk fs ft nl fv fx dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="np l nq nr ns no nt kt nf"/></div></div></a></div></div></div>    
</body>
</html>