<html>
<head>
<title>Creating a Multilayer Perceptron (MLP) Classifier Model to Identify Handwritten Digits</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">创建多层感知器(MLP)分类器模型以识别手写数字</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-a-multilayer-perceptron-mlp-classifier-model-to-identify-handwritten-digits-9bac1b16fe10#2022-06-09">https://towardsdatascience.com/creating-a-multilayer-perceptron-mlp-classifier-model-to-identify-handwritten-digits-9bac1b16fe10#2022-06-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ac55" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">神经网络和深度学习课程:第16部分</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/eb8d1c6b5eb71d5fe47f046f1b492c86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kfs4_toFg2wh9rHDVTKQnQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@alinnnaaaa?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">阿丽娜·格鲁布尼亚</a>在<a class="ae ky" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的原始照片，由作者编辑</p></figure><p id="41e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们已经熟悉了神经网络的大部分基础知识，因为我们已经在前面的部分中讨论过了。是时候利用我们的知识为现实世界的应用建立一个神经网络模型了。</p><p id="c741" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与卷积神经网络(CNN)、递归神经网络(RNN)、自动编码器(AE)和生成对抗网络(GAN)等其他主要类型相比，多层感知器(MLP)是最基本的神经网络架构类型。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="be1f" class="ma mb it lw b gy mc md l me mf"><strong class="lw iu"><em class="mg">Table of contents<br/></em></strong>-----------------<br/><strong class="lw iu">1. Problem understanding</strong></span><span id="74cf" class="ma mb it lw b gy mh md l me mf"><strong class="lw iu">2. Introduction to MLPs</strong></span><span id="448e" class="ma mb it lw b gy mh md l me mf"><strong class="lw iu">3. Building the model<br/> </strong>- Set workplace<br/> - Acquire and prepare the MNIST dataset<br/> - Define neural network architecture<br/> - Count the number of parameters<br/> - Explain activation functions<br/> - Optimization (Compilation)<br/> - Train (fit) the model<br/> - Epochs, batch size and steps<br/> - Evaluate model performance<br/> - Make a prediction</span><span id="c13c" class="ma mb it lw b gy mh md l me mf"><strong class="lw iu">4. The idea of "Base Model"</strong></span><span id="3d21" class="ma mb it lw b gy mh md l me mf"><strong class="lw iu">5. Parameters vs hyperparameters in our MLP model<br/> - </strong>Hyperparameter examples</span><span id="e2ff" class="ma mb it lw b gy mh md l me mf"><strong class="lw iu">6. Summary<br/></strong> - The idea of "parameter efficient"</span><span id="bfef" class="ma mb it lw b gy mh md l me mf"><strong class="lw iu"><em class="mg">Prerequisites - My own articles<br/></em></strong>-------------------------------<br/>1. <a class="ae ky" href="https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75" rel="noopener">Previous parts of my neural networks and deep learning course</a></span></pre><h1 id="ecff" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">问题理解</h1><p id="248f" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">今天，我们将构建一个<strong class="lb iu">多层感知器(MLP) </strong>分类器模型来识别手写数字。我们有70，000张手写数字的灰度图像，分为10个类别(0到9)。我们将用它们来训练和评估我们的模型。</p><p id="98df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">识别手写数字是一个多类分类问题，因为手写数字的图像属于10个类别(0到9)。如果我们将手写数字2的图像输入到我们的MLP分类器模型，它将正确地预测数字是2。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/a61085aae7475d4a607895f3bd19f13c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MmrWSRkddKWmY7uAnp6DgQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf"> MLP多阶层预测演示</strong>(图片由作者提供，用draw.io制作)</p></figure><h1 id="4602" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">多层感知器简介(MLP)</h1><p id="9936" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">在我们继续之前，有必要介绍一下多层感知器(MLP)。我已经在第二部分中定义了什么是MLP。</p><p id="8f6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于MLP，强调以下几点:</p><ul class=""><li id="b716" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated">在MLP中，感知器(神经元)堆叠在多层中。</li><li id="7daf" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">每层上的每个节点都与下一层上的所有其他节点相连。</li><li id="e73f" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">单个层内的节点之间没有连接。</li><li id="fa89" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">MLP是一个<em class="mg">完全(密集)连接的神经网络(FCNN) </em>。所以，我们使用Keras中的<code class="fe nu nv nw lw b">Dense()</code>类来添加层。</li><li id="2d34" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">在MLP中，数据沿一个方向(正向)通过图层从输入移动到输出。</li><li id="5cdb" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">MLP在一些文献中也被称为<em class="mg">前馈神经网络</em>或<em class="mg">深度前馈网络</em>。</li><li id="54c6" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">MLP是一种<a class="ae ky" href="https://rukshanpramoditha.medium.com/implicit-and-explicit-input-layers-in-keras-sequential-models-733049f83a32#23c8" rel="noopener">顺序模型</a>。所以，我们使用Keras中的<code class="fe nu nv nw lw b">Sequential()</code>类来构建MLP。</li></ul><h1 id="1091" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">构建模型</h1><p id="20bd" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">我们将按照以下步骤构建模型。</p><h2 id="06e3" class="ma mb it bd mj nx ny dn mn nz oa dp mr li ob oc mt lm od oe mv lq of og mx oh bi translated">设置深度学习工作场所来运行代码</h2><p id="523c" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">我已经在<a class="ae ky" href="https://rukshanpramoditha.medium.com/setting-up-a-deep-learning-workplace-with-an-nvidia-graphics-card-gpu-for-windows-os-b6bff06eeec7" rel="noopener"> Part 12 </a>中详细解释了整个过程。</p><p id="6632" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想在谷歌实验室运行代码，请阅读<a class="ae ky" href="https://rukshanpramoditha.medium.com/how-to-use-google-colab-free-gpu-to-run-deep-learning-code-incredibly-faster-760604d26c7e" rel="noopener">第13部分</a>。</p><h2 id="1df6" class="ma mb it bd mj nx ny dn mn nz oa dp mr li ob oc mt lm od oe mv lq of og mx oh bi translated">获取、理解和准备MNIST数据集</h2><p id="15ca" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">我们使用<strong class="lb iu"> MNIST </strong> ( <strong class="lb iu"> M </strong>修正的<strong class="lb iu"> N </strong>国家<strong class="lb iu">I</strong>N代替<strong class="lb iu"> S </strong>标准和<strong class="lb iu"> T </strong>技术)数据集来训练和评估我们的模型。它包含10个类别(0到9)下的70，000幅手写数字的灰度图像。</p><p id="0871" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的代码块显示了如何在构建模型之前获取和准备数据。我不打算解释这段代码，因为我已经在<a class="ae ky" href="https://rukshanpramoditha.medium.com/acquire-understand-and-prepare-the-mnist-dataset-3d71a84e07e7" rel="noopener"> Part 15 </a>中详细解释过了。因此，我强烈建议您在进入下一步之前阅读它。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="ak">为我们的MLP车型获取并准备MNIST数据</strong>(代码由作者编写)</p></figure><h2 id="78f5" class="ma mb it bd mj nx ny dn mn nz oa dp mr li ob oc mt lm od oe mv lq of og mx oh bi translated">定义神经网络架构</h2><ul class=""><li id="5da1" class="ng nh it lb b lc mz lf na li ok lm ol lq om lu nl nm nn no bi translated"><strong class="lb iu">网络类型:</strong>多层感知器(MLP)</li><li id="0af4" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><strong class="lb iu">隐藏层数:</strong> 2</li><li id="6400" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><strong class="lb iu">总层数:</strong> 4(两个隐藏层+输入层+输出层)</li><li id="c386" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><strong class="lb iu">输入形状:</strong> (784，)—输入层的784个节点</li><li id="1d4d" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><strong class="lb iu">隐藏层1: </strong> 256个节点，<a class="ae ky" rel="noopener" target="_blank" href="/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c#b34f"> ReLU激活</a></li><li id="0ace" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><strong class="lb iu">隐藏层2: </strong> 256个节点，<a class="ae ky" rel="noopener" target="_blank" href="/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c#b34f"> ReLU激活</a></li><li id="7795" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><strong class="lb iu">输出层:</strong> 10节点，<a class="ae ky" rel="noopener" target="_blank" href="/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c#bd87"> Softmax激活</a></li></ul><p id="987e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是网络架构的代码。输入层是明确定义的。也可以隐式定义。阅读<a class="ae ky" href="https://rukshanpramoditha.medium.com/implicit-and-explicit-input-layers-in-keras-sequential-models-733049f83a32" rel="noopener">第10部分</a>中的完整指南。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="ak">定义MLP网络架构，获取参数个数</strong>(作者代码)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/e2f7c498a573482f20ca422132e130f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*3H7FyISJquateSxWQgsKLw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf"> MLP车型总结</strong>(图片由作者提供)</p></figure><h2 id="58a5" class="ma mb it bd mj nx ny dn mn nz oa dp mr li ob oc mt lm od oe mv lq of og mx oh bi translated">计算参数的数量</h2><p id="990b" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">可训练参数的数量是269，322！这是什么？通过训练我们的神经网络，我们将找到这些参数的最佳值。</p><p id="1bb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些参数包括网络中的权重和偏差项。要了解这方面的更多信息，请阅读本节。</p><p id="8235" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在深度学习中，这些参数用权重矩阵(<strong class="lb iu"> W1 </strong>、<strong class="lb iu"> W2 </strong>、<strong class="lb iu"> W3 </strong>)和偏置向量(<strong class="lb iu"> b1 </strong>、<strong class="lb iu"> b2 </strong>、<strong class="lb iu"> b3 </strong>)来表示。要了解更多相关信息，请阅读本节。</p><p id="ba65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可训练参数的总数等于权重矩阵和偏差向量中的元素总数。</p><ul class=""><li id="ee31" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated">从输入层到第一个隐藏层:784 x 256 + 256 = 200，960</li><li id="a5eb" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">从第一个隐藏层到第二个隐藏层:256 x 256 + 256 = 65，792</li><li id="fd27" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">从第二个隐藏层到输出层:10 x 256 + 10 = 2570</li><li id="3fcb" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">可转换参数总数:200，960 + 65，792 + 2570 = 269，322</li></ul><h2 id="39f9" class="ma mb it bd mj nx ny dn mn nz oa dp mr li ob oc mt lm od oe mv lq of og mx oh bi translated">解释激活功能</h2><p id="b15f" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">输入层不需要激活功能。</p><p id="c3d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要在隐藏层中使用非线性激活函数。这是因为手写数字分类是一项非线性任务。在隐藏层中没有非线性激活函数的情况下，我们的MLP模型将不会学习数据中的任何非线性关系。因此，我们在两个隐藏层中都使用了<a class="ae ky" rel="noopener" target="_blank" href="/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c#b34f"> ReLU激活函数</a>。ReLU是非线性激活函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/40541138125e15cba8eef451f91e2caf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*g7d-miQFiBkJHoa5BQqdHw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf"> ReLU激活函数</strong>(图片由作者提供，用latex编辑器和matplotlib制作)</p></figure><p id="98b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在输出层，我们使用<a class="ae ky" rel="noopener" target="_blank" href="/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c#bd87"> Softmax激活函数</a>。这是多类分类问题的唯一选择。输出层有10个节点，对应于10个标签(类)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/515c83597e2f4f9925ac969c5673be23.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*tDRk4k2A-ctY46B7Po3zaw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf"> Softmax激活功能</strong>(图片由作者提供，用latex编辑器制作)</p></figure><p id="181b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Softmax函数计算一个事件(类)在<strong class="lb iu"> K个</strong>不同事件(类)上的概率值。这些类是互斥的；如果我们将每一类的概率值相加，我们得到1.0。</p><h2 id="84d5" class="ma mb it bd mj nx ny dn mn nz oa dp mr li ob oc mt lm od oe mv lq of og mx oh bi translated">最佳化</h2><p id="ff96" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">这也叫编译。这里我们配置学习参数。阅读本节了解更多相关信息。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="ak">配置学习参数</strong>(作者代码)</p></figure><h2 id="e065" class="ma mb it bd mj nx ny dn mn nz oa dp mr li ob oc mt lm od oe mv lq of og mx oh bi translated">训练(拟合)模型</h2><p id="19af" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">这里，我们向<code class="fe nu nv nw lw b">fit()</code>方法提供训练数据(X和标签)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="ak">训练模型</strong>(作者代码)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/e48ab7aacc5c69e579907831205423da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WEeWVacS2LuqBYCPSTXlOQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">训练时期和步骤</strong>(作者代码)</p></figure><h2 id="7540" class="ma mb it bd mj nx ny dn mn nz oa dp mr li ob oc mt lm od oe mv lq of og mx oh bi translated">时期、批量和步骤</h2><p id="56d6" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">一个<strong class="lb iu">历元</strong>是整个训练数据集的完整传递。这里，<em class="mg"> Adam </em>优化器遍历整个训练数据集20次，因为我们在<code class="fe nu nv nw lw b">fit()</code>方法中配置了<code class="fe nu nv nw lw b">epochs=20</code>。</p><p id="1d6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们把训练集分成批次(样本数)。<strong class="lb iu"> batch_size </strong>为样本量(每批包含的训练实例数)。批次数量通过以下方式获得:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="11af" class="ma mb it lw b gy mc md l me mf"><strong class="lw iu">No. of batches = (Size of the train dataset / batch size) + 1</strong></span></pre><p id="1f89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据上面的等式，这里我们得到469 (60，000 / 128 + 1)个批次。我们加1来补偿任何小数部分。</p><p id="a0e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在一个时期内，<code class="fe nu nv nw lw b">fit()</code>方法处理469个<strong class="lb iu"> <em class="mg">步骤</em> </strong>。在每个优化时期，模型参数将被更新469次。</p><p id="66dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在每个时期，该算法采用前128个训练实例，并更新模型参数。然后，它采用接下来的128个训练实例并更新模型参数。该算法将进行这一过程，直到每个时期完成469个步骤。</p><h2 id="1403" class="ma mb it bd mj nx ny dn mn nz oa dp mr li ob oc mt lm od oe mv lq of og mx oh bi translated">评估模型性能</h2><p id="82a4" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">在这里，我们使用测试数据(X和标签)对<code class="fe nu nv nw lw b">evaluate()</code>方法评估我们的模型。我们从不使用训练数据来评估模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="ak">评估模型</strong>(作者代码)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/5be4e5db49bff50a9c826692d0c0a70c.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*J1Y8WjkFp-Gfl3LGeiKUpg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">损失和准确度得分</strong>(图片由作者提供)</p></figure><p id="4716" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据算法的随机性，你会得到稍微不同的结果。您可以通过如下设置随机种子来获得静态结果。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="1b8b" class="ma mb it lw b gy mc md l me mf">import numpy as np<br/>import tensorflow as tf</span><span id="b075" class="ma mb it lw b gy mh md l me mf">np.random.seed(42)<br/>tf.random.set_seed(42)</span></pre><h2 id="6f16" class="ma mb it bd mj nx ny dn mn nz oa dp mr li ob oc mt lm od oe mv lq of og mx oh bi translated">做一个预测</h2><p id="fd74" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">现在，我们使用<code class="fe nu nv nw lw b">predict()</code>方法对看不见的数据进行预测。我们使用<strong class="lb iu">测试图像</strong>组的第五幅图像。这个图像代表数字4。如果我们的模型是准确的，它应该为数字4预测更高的概率值。让我们看看。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="ak">做个预测</strong>(作者代码)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/318c1ca7ef762f161f5515c2bc137d00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*rxSZLqpqlzL5Io_0A3jANQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">我们的MLP模型做出的预测</strong>(图片由作者提供)</p></figure><p id="4d0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我们在输出图层中使用了Softmax激活函数，所以它会返回一个包含10个元素的1D张量，这些元素对应于每个类的概率值。预测数字位于具有最高概率值的索引处。请注意，索引从零开始。</p><p id="ecaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了获得具有最高概率值的索引，我们可以使用<code class="fe nu nv nw lw b">np.argmax()</code>函数。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="691f" class="ma mb it lw b gy mc md l me mf">np.argmax(MLP.predict(digit, verbose=0))</span></pre><p id="658e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这会返回4！所以，我们的MLP模型对新数据做出了正确的预测！</p><p id="8d72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于所有类别都是互斥的，所以上述1D张量中所有概率值的总和等于1.0。</p><h1 id="6169" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">基本模型</h1><p id="9fb5" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">我们刚刚在MNIST数据上建立的MLP分类器模型被认为是我们神经网络和深度学习课程中的<strong class="lb iu"><em class="mg"/></strong>基础模型。我们将在MNIST数据上构建几个不同的MLP分类器模型，并将这些模型与这个基础模型进行比较。</p><h1 id="f45b" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">MLP模型中的参数与超参数</h1><blockquote class="ot ou ov"><p id="033c" class="kz la mg lb b lc ld ju le lf lg jx lh ow lj lk ll ox ln lo lp oy lr ls lt lu im bi translated"><strong class="lb iu">注:</strong>要了解参数和超参数的区别，看<a class="ae ky" href="https://rukshanpramoditha.medium.com/parameters-vs-hyperparameters-what-is-the-difference-5f40e16e2e82" rel="noopener">我写的这篇文章</a>。</p></blockquote><p id="b83d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">之前我们计算了MLP模型中的参数数量(权重和偏差项)。</p><p id="326a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">即使对于简单的MLP，我们也需要为以下超参数指定最佳值，这些超参数控制参数值，然后控制模型的输出。</p><h2 id="ddbe" class="ma mb it bd mj nx ny dn mn nz oa dp mr li ob oc mt lm od oe mv lq of og mx oh bi translated">超参数</h2><ul class=""><li id="14ea" class="ng nh it lb b lc mz lf na li ok lm ol lq om lu nl nm nn no bi translated">网络中的隐藏层数</li><li id="ff31" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">每个隐藏层中的节点数</li><li id="7a1c" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">损失函数的类型</li><li id="d545" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">优化器的类型</li><li id="809f" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">评估指标的类型</li><li id="0c1d" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">每个隐藏层中激活函数的类型</li><li id="ec7c" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">批量</li><li id="5e91" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">时代数</li><li id="9e0e" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">学习率</li></ul><p id="d4f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过改变这些超参数的值，我们可以建立许多不同的模型。例如，我们可以在网络中添加3个隐藏层，建立一个新的模型。我们可以在每个隐藏层中使用512个节点，并建立一个新的模型。我们可以改变Adam优化器的学习率，建立新的模型。我们可以在隐藏层中使用泄漏的ReLU激活函数来代替ReLU激活函数，并建立新的模型。每次，我们都会得到不同的结果。</p><p id="7e4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，一些超参数的值只有一个选项。例如，损失函数的类型总是<strong class="lb iu">分类交叉熵</strong>，并且输出层中的激活函数的类型总是<strong class="lb iu"> Softmax </strong>，因为我们的MLP模型是多类分类模型。</p><h1 id="6667" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">摘要</h1><p id="eeda" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">我们的基本MLP模型获得了更高的准确度分数。然而，我们的MLP模型不是参数有效的。即使对于这个小的分类任务，它也需要269，322个可训练参数，仅用于两个隐藏层，每个隐藏层256个单元。</p><p id="9063" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下一篇文章的<a class="ae ky" href="https://rukshanpramoditha.medium.com/using-pca-to-reduce-number-of-parameters-in-a-neural-network-by-30x-times-fcc737159282" rel="noopener">中，我将向您介绍一个特殊的技巧，在不改变MLP模型的架构和不降低模型精度的情况下，显著减少可训练参数的数量！</a></p></div><div class="ab cl oz pa hx pb" role="separator"><span class="pc bw bk pd pe pf"/><span class="pc bw bk pd pe pf"/><span class="pc bw bk pd pe"/></div><div class="im in io ip iq"><p id="4076" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天的帖子到此结束。</p><p id="7b60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您有任何问题或反馈，请告诉我。</p><p id="2c4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢阅读这篇文章。如果你愿意支持我成为一名作家，请考虑 <a class="ae ky" href="https://rukshanpramoditha.medium.com/membership" rel="noopener"> <strong class="lb iu"> <em class="mg">注册会员</em> </strong> </a> <em class="mg">以获得无限制的媒体访问权限。它只需要每月5美元，我会收到你的会员费的一部分。</em></p><div class="pg ph gp gr pi pj"><a href="https://rukshanpramoditha.medium.com/membership" rel="noopener follow" target="_blank"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd iu gy z fp po fr fs pp fu fw is bi translated">通过我的推荐链接加入Medium</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="ps l"><div class="pt l pu pv pw ps px ks pj"/></div></div></a></div><p id="bd63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非常感谢你一直以来的支持！下一篇文章再见。祝大家学习愉快！</p><h2 id="8b8d" class="ma mb it bd mj nx ny dn mn nz oa dp mr li ob oc mt lm od oe mv lq of og mx oh bi translated">MNIST数据集信息</h2><ul class=""><li id="a16f" class="ng nh it lb b lc mz lf na li ok lm ol lq om lu nl nm nn no bi translated"><strong class="lb iu">引用:</strong>邓，l，2012。用于机器学习研究的手写数字图像mnist数据库。<strong class="lb iu"> IEEE信号处理杂志</strong>，29(6)，第141–142页。</li><li id="a617" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><strong class="lb iu">来源:</strong><a class="ae ky" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">http://yann.lecun.com/exdb/mnist/</a></li><li id="27ba" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><strong class="lb iu">许可:</strong><em class="mg">Yann le Cun</em>(NYU库朗研究所)和<em class="mg"> Corinna Cortes </em>(纽约谷歌实验室)持有MNIST数据集的版权，该数据集可根据<em class="mg">知识共享署名-共享4.0国际许可</em>(<a class="ae ky" href="https://creativecommons.org/licenses/by-sa/4.0/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">CC BY-SA</strong></a>)获得。您可以在此了解有关不同数据集许可类型<a class="ae ky" href="https://rukshanpramoditha.medium.com/dataset-and-software-license-types-you-need-to-consider-d20965ca43dc#6ade" rel="noopener">的更多信息。</a></li></ul><p id="a39a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="py pz ep" href="https://medium.com/u/f90a3bb1d400?source=post_page-----9bac1b16fe10--------------------------------" rel="noopener" target="_blank">鲁克山普拉莫迪塔</a><br/><strong class="lb iu">2022–06–09</strong></p></div></div>    
</body>
</html>