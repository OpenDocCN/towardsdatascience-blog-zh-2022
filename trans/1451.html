<html>
<head>
<title>Linear Regression vs. Logistic Regression: What is the Difference?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归与逻辑回归:区别在哪里？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-vs-logistic-regression-ols-maximum-likelihood-estimation-gradient-descent-bcfac2c7b8e4#2022-04-10">https://towardsdatascience.com/linear-regression-vs-logistic-regression-ols-maximum-likelihood-estimation-gradient-descent-bcfac2c7b8e4#2022-04-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2db2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">成本函数、普通最小二乘法(OLS)、梯度下降法(GD)和最大似然估计法(MLE)方面的差异。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a14fd304bf09405485772b7386204db8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1piDJety0j6_De-xA4gVUA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">路线图(图片由作者提供)</p></figure><p id="d7e2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我将介绍统计模型开发的路线图。具体来说，我们将讨论<strong class="la iu">线性回归</strong>与<strong class="la iu">逻辑回归</strong>的发展有何不同。我们还将讨论为什么一些方法在一个模型中有效，而在另一个模型中无效。以下是该路线图的概述。</p><p id="e0e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">第一步</strong>:设计一个<strong class="la iu">模型</strong>，解释响应变量和解释变量之间的关系</p><ul class=""><li id="ab36" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><strong class="la iu">回归</strong>模型:响应变量为<strong class="la iu">连续</strong>。</li><li id="753e" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><strong class="la iu">分类</strong>模型:响应变量为<strong class="la iu">分类</strong>。</li></ul><p id="f715" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">第二步</strong>:开发一个<strong class="la iu">成本函数</strong>(又名<strong class="la iu">损失函数</strong>)来确定这个模型对一组给定数据的解释有多好</p><ul class=""><li id="052e" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">根据普通最小二乘法(<strong class="la iu"> OLS </strong>)开发一个成本函数</li><li id="4e61" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">根据最大似然估计开发一个成本函数</li></ul><p id="0c5b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">第三步</strong>:寻找<strong class="la iu">最佳参数</strong>，解决<strong class="la iu">关于代价函数的优化问题</strong></p><ul class=""><li id="5950" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">解析方法:如果优化问题可以解析求解，则找到一个<strong class="la iu">闭合解</strong></li><li id="0e58" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">数值逼近法:使用<strong class="la iu">梯度下降</strong>迭代求解</li></ul></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h2 id="9b7d" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated">如何在线性回归中应用普通最小二乘法(OLS)？</h2><p id="19f5" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">首先，我们用下面的数据来谈谈线性回归。目标是开发一个模型来解释作为连续变量的响应变量(即Y)和解释变量(即X)之间的关系。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/1bfc875604980d04c4702b45a832e1f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X5UoH2_t1VxS_--SlphJww.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="bebf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">步骤1 </strong>:基于Y和X之间的线性关系，一个线性方程，如下所示，将是一个合适的模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/872c2d64d213b0cb3fae305ca7bd08f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0qt7rIuFzyVGLPmMpOpHMg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式1</p></figure><p id="134b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">参数(β0，β1)是通常未知的固定值，因为很难从总体中收集所有数据点。因此，我们使用从样本计算的OLS估计量(β̂0，β̂1)来估计总体参数(β0，β1)。</p><p id="1b65" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">误差项<strong class="la iu"> ε </strong>是一个随机变量，它用每个点到拟合直线的距离来解释误差。误差项的值可以是正的也可以是负的。总体来说ε <strong class="la iu"> </strong>需要有一个0 的<strong class="la iu">均值，可以用截距(即β0)来实现。ε<strong class="la iu">不是</strong>需要有一个<strong class="la iu">正态分布</strong>与OLS产生一个无偏模型。</strong></p><blockquote class="np nq nr"><p id="baf3" class="ky kz ns la b lb lc ju ld le lf jx lg nt li lj lk nu lm ln lo nv lq lr ls lt im bi translated">然而，<strong class="la iu">正态假设</strong>允许我们进行统计<strong class="la iu">假设检验。</strong>估计值(例如，β̂0、β̂1)可以写成ε的线性组合(<a class="ae nw" rel="noopener" target="_blank" href="/are-ols-estimators-normally-distributed-in-a-linear-regression-model-89b688fa8dc3">参见此处</a>的更多细节)，假设ε呈正态分布，则意味着估计值也呈正态分布。因此，正态假设将为预测和估计生成可靠的置信区间。</p></blockquote><p id="1c0f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">第二步</strong>:我们想出一个成本函数，它将量化<em class="ns">这个建议模型有多好。对于线性回归模型，我们找到和的最佳估计，使得易和之间的距离之和，其中= β̂1Xi + β̂0，是最小化的</em><strong class="la iu"><em class="ns"/></strong>。通常，在成本函数中使用<strong class="la iu">二次距离</strong>。我们可以用下面的等式来总结。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/5c59af6b341f6cb229bee77ef1da9c06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7NO8dv6PrOE38J1Q06WOIg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式2</p></figure><h2 id="8a45" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated">如何在线性回归中应用最大似然估计？</h2><p id="962e" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">上面的代价函数是我们能为线性回归设计的唯一函数吗？答案是否定的。或者，我们可以根据<strong class="la iu">最大似然估计</strong> (MLE)得出一个不同版本的成本函数。</p><p id="78e3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">MLE是一种在给定观测数据的情况下估计模型参数的常用方法。在假设的统计模型下，我们可以通过最大化观察一组给定数据的可能性来找到最佳参数。</p><p id="75ef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第一步:我们将从与OLS相同的线性模型开始(等式1)。这里唯一的修改是我们需要<strong class="la iu">对最大似然估计的误差项施加正态假设</strong>，因为我们需要假设数据的分布来计算可能性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/e0b1442f32bc3ac7905d85f539454b60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p3yiSKBg8nzsU6vsHRJ2Lg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式3</p></figure><p id="70c3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">第2步</strong>:相反，我们可以提出一个不同版本的成本函数，它将量化<em class="ns">在给定解释变量(例如X)的情况下，在建议的模型</em>下我们观察到结果变量(例如Y)的可能性有多大。我们希望找到最佳参数，β̂ 1和β̂ 0，这样观察我们数据的可能性<strong class="la iu">最大化</strong>。</p><p id="a58e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们首先将等式3改写为X和y的每个观测值的条件分布。换句话说，每个Yi都来自正态分布，对于给定的Xi，其均值为β1Xi + β0，标准差为σ。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/d7e6a24b1e577393b3ec96ece6453bad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nTFWZazu4U89GmGW7O5xFw.png"/></div></div></figure><p id="0e4d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，我们根据正态分布定义了观察易给定的似然函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/7789045dbc7e4b982b5ba650c35ca472.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BIedgUQvKg4BqKDBn5ifjw.png"/></div></div></figure><p id="deb8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个点I都是<strong class="la iu">独立且</strong>同分布(iid)的，所以我们可以把观测所有点的似然函数写成每个个体概率密度的<strong class="la iu">乘积</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/99779c0ae0a6297d2c69c57095a2f133.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-6292l9v0bzjRQb8E5C0Xw.png"/></div></div></figure><p id="fbf0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是<strong class="la iu">似然函数</strong>。通常，我们会将其转换为一个<strong class="la iu">对数似然函数</strong>，因为最大化似然函数相当于最大化对数似然函数，而对数似然函数在数学上更容易处理。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/ee39e85bbd087eb53c9a3ab10dc25466.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zVifX9RORluFnL1mlw_ZoQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式4</p></figure><p id="fa81" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在等式4中，你会注意到圈起来的项都是常量<strong class="la iu">值</strong>并且在求和前面有一个<strong class="la iu">负号</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/9d9108bf03f7985bcb9c43f21369e2e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dviSRqV2iUDzAelGnnf4Ww.png"/></div></div></figure><blockquote class="np nq nr"><p id="1a3e" class="ky kz ns la b lb lc ju ld le lf jx lg nt li lj lk nu lm ln lo nv lq lr ls lt im bi translated"><strong class="la iu">因此，最大化对数似然函数在数学上等价于最小化OLS的成本函数</strong>(见等式2) <strong class="la iu">。</strong></p></blockquote><p id="7d92" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">多酷啊！我们从关于OLS和最大似然估计的完全不同的想法开始，以线性回归模型的<strong class="la iu">相同的成本函数</strong>结束。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h2 id="fae1" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated">第三步:如何使用封闭形式的公式最小化线性回归的成本函数？</h2><p id="8bc3" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">寻找最小化或最大化一个函数的值的过程称为<strong class="la iu">优化</strong>。对于机器学习的任何优化问题，你可以用<strong class="la iu">分析方法</strong>或<strong class="la iu">数值近似方法</strong>来处理。</p><p id="0cc1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">分析方法是<strong class="la iu">确定性的，</strong>这意味着有一个<strong class="la iu">封闭形式的解决方案</strong>来解决优化问题。可以写下<strong class="la iu">精确</strong>解的数学表达式。</p><blockquote class="np nq nr"><p id="3355" class="ky kz ns la b lb lc ju ld le lf jx lg nt li lj lk nu lm ln lo nv lq lr ls lt im bi translated">然而，在统计建模中很难获得<strong class="la iu">封闭形式的解决方案</strong>。线性回归是极少数情况之一。</p></blockquote><p id="7df3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果最优化问题是一个线性方程，应该有一个解析解。我们应该可以用一点点<strong class="la iu">微积分</strong>推导出公式。</p><p id="3557" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了找到一个函数的最大值或最小值，我们可以对每个参数求导，并将它们设为0。解方程会给我们最好的参数。</p><p id="f290" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们推导公式。</p><p id="d145" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们的成本函数是误差平方和。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/48411dab9cb336a4d2ffc08c3a10763a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Ds_k6RBYstFRhVTd0THdg.png"/></div></div></figure><p id="d106" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们对β̂0求导，设为0。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/7a62edbec52913487515eb669c22df75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A22yOIdZDHxYthgqoqyCoA.png"/></div></div></figure><p id="a8a0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">解上面的方程，我们得到</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/e27dc5c50594790f820a8e7ffb6543f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LPWe4V0UGaNe5kkVU4WokA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式5</p></figure><p id="1de2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在对β1求导，用等式5代替β0，并将其设为0。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/173b499a2d8df2a9f8237e52b631066c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BOgkUm4LHNc85u3OuRsAgA.png"/></div></div></figure><p id="02e9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">解上面的方程，我们得到</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/d954514995fc83d5a4e9334fbe1ba858.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8f_0O-v8V7kpbAkBqVkpnA.png"/></div></div></figure><p id="7189" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们可以应用一些代数技巧来改写分子和分母，这很容易证明。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/5e66db0ef0c5c81e8cc1f4882f3530ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F-roDIYfSMlAF5ZMiFBd1A.png"/></div></div></figure><p id="dfcb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从β1和β0的公式中可以看出。对于给定的数据集，我们很可能有一个线性模型的封闭形式的解决方案。我们也可以做<strong class="la iu">矩阵形式的所有代数。</strong>你可以在这里找到它的衍生<a class="ae nw" href="https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/2b19918312a88c40ebec50b18a34b699.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ls-4JWVY5WWdDM9yBw3VVw.png"/></div></div></figure><blockquote class="np nq nr"><p id="f606" class="ky kz ns la b lb lc ju ld le lf jx lg nt li lj lk nu lm ln lo nv lq lr ls lt im bi translated">请记住，<strong class="la iu">多元线性回归</strong>的封闭解有时会有问题，其中有多个解释变量(例如，X1、X2、X3等)，因为逆矩阵仅在X具有<strong class="la iu">满秩</strong>时存在，这意味着如果存在<strong class="la iu">完美多重共线性</strong>，它就不会有封闭解。为了防止这种情况，我们需要在线性模型中排除这些变量。</p></blockquote></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h2 id="24bc" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated">第三步:如何利用梯度下降最小化代价函数？</h2><p id="393a" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">我们已经讨论了线性回归的解析解。接下来说一个<strong class="la iu">数值逼近法</strong>，叫做<strong class="la iu">梯度下降</strong>，解决优化问题。</p><p id="17a6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在数学上，<strong class="la iu"> </strong>梯度下降法是一种<strong class="la iu">一阶迭代</strong>优化算法，用于寻找可微函数的局部极小值。典型的梯度下降遵循下列步骤。</p><ol class=""><li id="6c34" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt ol ma mb mc bi translated">最初，我们可以让β1和β0为任意值(例如，β1 = 0和β0 = 0)。设<strong class="la iu"> L </strong>为<strong class="la iu">学习率</strong>。l确定我们将应用于更新参数的变化的<strong class="la iu">幅度</strong>。学习率越大，代价函数越快接近最小值。如果L太大，你的优化器将会越过“曲线”，错过最小点。相反，如果L太小，将需要很长时间才能达到最小值。因此，我们需要有策略地设定学习速度。</li><li id="2b2d" class="lu lv it la b lb md le me lh mf ll mg lp mh lt ol ma mb mc bi translated">计算关于每个参数的成本函数的<strong class="la iu">偏导数</strong>(我已经在上面讨论过了)。将β1，β0，，Yi的值插入每个偏导数中，并计算它们的值。导数值将决定这些参数变化的方向。</li><li id="31fa" class="lu lv it la b lb md le me lh mf ll mg lp mh lt ol ma mb mc bi translated">使用以下公式更新参数</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/36523a26757f6fc136cf94aa7ed83ee6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GooZroizH2me-QEEkoCDlw.png"/></div></div></figure><p id="c5ce" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">4.重复步骤2和3，直到参数停止变化或导数值足够接近零。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/d7d6ef83dfe3e988a26ea345c507c2fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZJo41UHTuWXUaVRErKIQuw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><blockquote class="np nq nr"><p id="bfac" class="ky kz ns la b lb lc ju ld le lf jx lg nt li lj lk nu lm ln lo nv lq lr ls lt im bi translated">请记住，如果在线性回归模型中有效，分析方法会给我们<strong class="la iu">精确解</strong>。然而，梯度下降只能给出一个<strong class="la iu">非常接近</strong>精确解的解。幸运的是，在大多数情况下，我们无法注意到这些差异。</p></blockquote><p id="c1a0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">梯度下降</strong>是机器学习中解决优化问题的流行工具。其算法简单，易于应用于大多数成本函数中。然而，它也有<strong class="la iu">限制</strong>，例如，它不能在非凸函数中工作，这我们将在下一节讨论。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h2 id="254b" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated">线性回归在分类问题中有用吗？</h2><p id="ddd3" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">简单的回答是没有。</p><p id="17f0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们知道线性回归的结果变量是一个连续变量。它能否处理<strong class="la iu">分类结果</strong>变量，例如，动物的种类——狗对猫、物体检测、火车对飞机、垃圾邮件检测、垃圾邮件对非垃圾邮件？显然不能，因为我们开发的线性方程的预测结果变量可以取无限个值。分类问题的预测结果只能取<strong class="la iu">个有限的</strong>个值，例如2个或少量的值。</p><p id="1082" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这并不意味着线性回归对分类问题没有用。事实上，我们可以开发一个基于线性回归的新模型，使用<strong class="la iu">逻辑回归</strong>处理分类问题。</p><h2 id="91ba" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated">什么是逻辑回归？</h2><p id="a2e5" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">简单来说，我们可以把Logistic回归分解成两部分，<strong class="la iu">回归和Logistic。</strong></p><p id="305a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">回归</strong>:线性回归模型用于估计<strong class="la iu">对数</strong>(又名<strong class="la iu">对数比数</strong>)的值</p><ul class=""><li id="46fa" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">设P为特定事件发生的概率(例如，电子邮件是垃圾邮件)。赔率被定义为“特定事件发生的概率”与“该事件不发生的概率”之比。对数赔率就是赔率的对数值。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/b8f22e565669d3a7b03a124709161a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MoW3M1IdC4-fiLqLwvwK0g.png"/></div></div></figure><ul class=""><li id="21e0" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">我们可以用线性模型来表示对数几率</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/a0880c8311621a0c7d78434c7d3e2a1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yla3pdwoQyFZUpsDOkJXoA.png"/></div></div></figure><ul class=""><li id="00f6" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">然后我们可以解出P</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/15c31791cd374df2afa1bb885a6ff363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5i7pQgRO75ZL6T6Hvf7YGA.png"/></div></div></figure><p id="e765" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">逻辑</strong>:我们也可以把逻辑回归模型想象成把一个线性回归模型输入到一个<strong class="la iu">逻辑函数</strong>(又名<strong class="la iu"> sigmoid函数</strong>)中。逻辑回归函数将范围从∞到+∞的logit值(即βXi)转换为范围从0到1的Yi值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/be79fa5e4bec7b7346639fad97e5b8a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*av5rJoDGXQrnJILO71EuIw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="0fc6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我认为我们有了一个合适的分类模型。我们可以为Y设置一个<strong class="la iu">阈值</strong>(例如0.5)来确定结果变量。比如Y ≥ 0.5就是垃圾邮件，Y &lt; 0.5就不是垃圾邮件。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/206a4dce5cd838a97debf3dc6726a62a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9OBXuvUCajYDaDqOy_GtqA.png"/></div></div></figure><h2 id="a8e9" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated">为什么我们不用OLS的误差平方和(SSE)作为逻辑回归的成本函数呢？</h2><p id="a7d4" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">一旦我们有了一个合理的模型来解决分类问题。接下来，我们需要提出一个成本函数。让我们试试线性回归模型中使用的平方和误差函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/9c2a53e42d5618c2d031a6bd001612f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CRCtbnt-1nqn9aiIWMxnCA.png"/></div></div></figure><blockquote class="np nq nr"><p id="b03a" class="ky kz ns la b lb lc ju ld le lf jx lg nt li lj lk nu lm ln lo nv lq lr ls lt im bi translated">在逻辑回归中，是否存在最小化平方和误差的解析解？简单的回答是否定的。</p></blockquote><p id="400e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以很快得出结论，这是一个非常困难的数学问题。函数的复杂性和<strong class="la iu">非线性</strong>使得不可能找到封闭形式的解。</p><blockquote class="np nq nr"><p id="0337" class="ky kz ns la b lb lc ju ld le lf jx lg nt li lj lk nu lm ln lo nv lq lr ls lt im bi translated"><strong class="la iu">我们可以使用梯度下降来最小化逻辑回归中的平方和误差吗？</strong>简单的回答也是否定的。</p></blockquote><p id="2994" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用梯度下降来最小化成本函数。我们需要确保成本函数是一个凸的函数。如果我们试图在逻辑回归中使用平方和误差的成本函数，它最终将是一个具有许多局部最小值的非凸函数。寻找一个非凸函数的全局最小值本身就是一个非常困难的数学问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/086631acc5d376365df4a0f6820c1ef2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lGoXvWH9up7W_sLhdsTrOQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="1fb0" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated">基于极大似然估计和交叉熵的Logistic回归</h2><blockquote class="np nq nr"><p id="698b" class="ky kz ns la b lb lc ju ld le lf jx lg nt li lj lk nu lm ln lo nv lq lr ls lt im bi translated">由于<strong class="la iu">误差平方和</strong>不能作为逻辑回归的代价函数，我们是否可以用<strong class="la iu"> MLE </strong>方法来寻找代价函数？简单的回答是肯定的。</p></blockquote><p id="11cd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用最大似然估计，我们希望在逻辑回归模型下，在给定X的情况下，最大化观察Y的似然性。</p><p id="e352" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设P是给定x的特定事件(例如，电子邮件是垃圾邮件，用Y =1表示)发生的概率。那么1-P将意味着给定x的Y=0的概率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/15c31791cd374df2afa1bb885a6ff363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5i7pQgRO75ZL6T6Hvf7YGA.png"/></div></div></figure><p id="5e42" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，对于一个给定的观察对象(即易和)，我们想</p><ul class=""><li id="0e9b" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><strong class="la iu">如果Y =1，P最大化</strong></li><li id="d0cc" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">如果Y =0，最大化1-P</li></ul><p id="b2ce" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们稍微扭曲一下，我们想</p><ul class=""><li id="5f72" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><strong class="la iu">如果Y= 1，最大化Y * P</strong></li><li id="2766" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><strong class="la iu">如果Y = 0 </strong>，最大化(1-Y)*(1-P)。</li></ul><p id="15bf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后让我们结合上述两个功能，我们想</p><ul class=""><li id="5b27" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><strong class="la iu">最大化Y*P + (1-Y)*(1-P) </strong></li></ul><p id="d539" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们应用对数函数，将最大化成本函数改为最小化成本函数的反函数，我们希望</p><ul class=""><li id="76b9" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><strong class="la iu">最小化-[Y*log(P) + (1-Y)*log(1-P)] </strong></li></ul><p id="2ae2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来我们可以把观察所有点的似然函数写成每个点的似然的乘积。然后我们可以把每个可能性的乘积的<strong class="la iu"> log重写为每个可能性的log</strong>的和。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/3bf8bda1ef01e41bae62ad85913a0c4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pqb3snSDOsJJf8UAvCHRTQ.png"/></div></div></figure><p id="d6fa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们给上面的代价函数起了个好听的名字，<strong class="la iu">二元交叉熵。</strong></p><h2 id="f275" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated">最小化逻辑回归中<strong class="ak">二元交叉熵的代价函数</strong></h2><blockquote class="np nq nr"><p id="35b0" class="ky kz ns la b lb lc ju ld le lf jx lg nt li lj lk nu lm ln lo nv lq lr ls lt im bi translated">logistic回归中是否存在最小化二元交叉熵的解析解？简单的回答是没有。</p></blockquote><p id="a221" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">类似于平方和误差，二进制交叉熵的复杂性和<strong class="la iu">非线性</strong>使得不可能找到封闭形式的解。</p><blockquote class="np nq nr"><p id="f9ad" class="ky kz ns la b lb lc ju ld le lf jx lg nt li lj lk nu lm ln lo nv lq lr ls lt im bi translated">可以用梯度下降来最小化logistic回归中的二元交叉熵吗？简单的回答是肯定的。</p></blockquote><p id="1403" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">二元交叉熵是一个凸函数</strong>，通过梯度下降保证找到全局最小值。这些步骤与线性回归模型中的步骤相同。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/2cd52f4713c73df723ade1b623dbd649.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_k48XvS2sfjhuwcFPUfhJw.png"/></div></div></figure><blockquote class="np nq nr"><p id="b303" class="ky kz ns la b lb lc ju ld le lf jx lg nt li lj lk nu lm ln lo nv lq lr ls lt im bi translated">酷的是逻辑回归的梯度下降公式中的<strong class="la iu">偏导函数</strong>与线性回归的偏导函数相同。</p></blockquote></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h2 id="1d0b" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated">超越传统的线性回归</h2><p id="a89e" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">在线性回归模型中，我们可以改变<strong class="la iu">成本函数</strong>来构建不同的模型，从而在线性模型中产生一组不同的最佳参数。</p><p id="ea2a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，在岭回归模型中，我们有一个成本函数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/a0f665af844d1ae585e5c9b657117078.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uRr4OgMMiol9J-FD1GfiUg.png"/></div></div></figure><p id="15d0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在套索回归模型中，我们有一个成本函数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/84e859f58a8195657a82b6c1f9a9d9d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iakm10ZGWayyQXaWWHpgQQ.png"/></div></div></figure><p id="951a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在弹性网回归模型中，我们有一个成本函数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/7c7dec1316ea537793b7c74211fcd409.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c3kneW52QChmJIsu2wqR3Q.png"/></div></div></figure><p id="c91a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">线性回归的这三个变量与<strong class="la iu">正则化</strong>相关联，正则化是一种惩罚模型的灵活性和<strong class="la iu">复杂性</strong>的技术，以防止<strong class="la iu">过度拟合</strong>的风险。这里λ是一个超参数，用于确定模型的灵活性将受到多大的惩罚。我们可以使用<strong class="la iu">交叉验证</strong>和<strong class="la iu">网格搜索</strong>来调整它的值。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h2 id="5e6e" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated">超越传统的逻辑回归</h2><p id="f01a" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">当两个类别的样本大致相同(T21)时，传统逻辑回归效果最佳。然而，现实世界中有许多情况并非如此。</p><p id="2b99" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，在信用卡交易数据中可能有非常少量的欺诈交易。使用逻辑回归，这个阶级不平衡的问题会变得棘手。如果只有0.1%的交易是欺诈性的。我们可以简单地预测所有正常的交易，最终达到99.9%的准确率。多棒啊！但是这种模型是没有用的，因为我们无法预测任何欺诈交易。</p><p id="3368" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了解决此类不平衡问题，我们可以将二元交叉熵的成本函数修改为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/354478f8a9729ac3e2f1efd8fa9c2100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cko50QSe4MIkSTJGgkjk5Q.png"/></div></div></figure><p id="0d08" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里<strong class="la iu"> α1和α2将决定我们惩罚类错误分类的严厉程度</strong>。它们的值通常计算为数据中它们频率的<strong class="la iu">倒数</strong>。</p><p id="33e7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">回到我们的例子，α1 = 1 / 0.1% = 100，α2 = 1/99.9% = 1。因此，我们对欺诈交易的处罚将是正常交易的100倍。因此，这一小小的修改将大大提高检测欺诈交易的准确性。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h2 id="336b" class="mp mq it bd mr ms mt dn mu mv mw dp mx lh my mz na ll nb nc nd lp ne nf ng nh bi translated">最终注释</h2><p id="24d0" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">线性回归和逻辑回归分别是处理回归和分类问题的两种广泛使用的模型。了解它们与普通最小二乘法和最大似然估计相关的基本形式将有助于我们理解基本原理，并探索它们的变体来解决现实世界的问题，如模型选择和不平衡类。</p><p id="a9b3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，了解成本函数优化问题背后的数学对于更有效地实现模型也很重要。请记住，只有当数据可以线性解释时，解析解才可用。否则，诸如梯度下降的数值近似方法更受欢迎，以找到机器学习模型中的最佳参数。</p><p id="8f14" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你对<strong class="la iu">线性回归</strong>和<strong class="la iu">因果推断</strong>感兴趣，这里有一些相关的帖子可以浏览。</p><ul class=""><li id="4a74" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><a class="ae nw" rel="noopener" target="_blank" href="/causal-inference-econometric-models-vs-a-b-testing-190781fe82c5"> <strong class="la iu">因果推断:计量经济模型vs. A/B检验</strong> </a></li><li id="5bbf" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae nw" rel="noopener" target="_blank" href="/linear-regression-vs-logistic-regression-ols-maximum-likelihood-estimation-gradient-descent-bcfac2c7b8e4"> <strong class="la iu">线性回归与逻辑回归:OLS、最大似然估计、梯度下降</strong> </a></li><li id="de60" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae nw" rel="noopener" target="_blank" href="/linear-regression-with-ols-unbiased-consistent-blue-best-efficient-estimator-359a859f757e"><strong class="la iu">OLS线性回归:无偏、一致、蓝色、最佳(有效)估计量</strong> </a></li><li id="af81" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae nw" rel="noopener" target="_blank" href="/understand-bias-and-variance-in-causal-inference-with-linear-regression-a02e0a9622bc"> <strong class="la iu">线性回归因果推断:省略变量和无关变量</strong> </a></li><li id="8805" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae nw" rel="noopener" target="_blank" href="/causal-inference-with-linear-regression-endogeneity-9d9492663bac"> <strong class="la iu">用线性回归进行因果推断:内生性</strong> </a></li><li id="773a" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae nw" rel="noopener" target="_blank" href="/linear-regression-with-ols-heteroskedasticity-and-autocorrelation-c12f1f65c13"> <strong class="la iu">与OLS的线性回归:异方差和自相关</strong> </a></li></ul><h1 id="929d" class="pb mq it bd mr pc pd pe mu pf pg ph mx jz pi ka na kc pj kd nd kf pk kg ng pl bi translated">感谢您的阅读！！！</h1><p id="73a8" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">如果你喜欢这篇文章，并且想<strong class="la iu">请我喝杯咖啡，请<a class="ae nw" href="https://ko-fi.com/aaronzhu" rel="noopener ugc nofollow" target="_blank">点击这里</a>。</strong></p><p id="89d7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以注册一个<a class="ae nw" href="https://aaron-zhu.medium.com/membership" rel="noopener"> <strong class="la iu">会员</strong> </a>来解锁我的文章的全部访问权限，并且可以无限制地访问介质上的所有内容。如果你想在我发表新文章时收到电子邮件通知，请<a class="ae nw" href="https://aaron-zhu.medium.com/subscribe" rel="noopener"> <strong class="la iu">订阅</strong> </a>。</p></div></div>    
</body>
</html>