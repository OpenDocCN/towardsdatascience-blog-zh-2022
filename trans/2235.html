<html>
<head>
<title>The Perceptron Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">感知器算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-perceptron-algorithm-b74d95d6e1cb#2022-05-17">https://towardsdatascience.com/the-perceptron-algorithm-b74d95d6e1cb#2022-05-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="53f2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解并用R或Python编写你自己的感知器算法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/60ff3388094236ff9d2661ed1f3c3cff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6U_nrKa-JB8P7PkMr-4Heg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="ky">照片由</em> <a class="ae kz" href="https://unsplash.com/@hvaandres?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> <em class="ky">安德烈斯【哈罗】</em></a> <em class="ky">上</em> <a class="ae kz" href="https://unsplash.com/images/things/arrow?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> <em class="ky">下</em> </a></p></figure><p id="8d2f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">感知器算法可能是最简单的二进制分类算法。最初由Frank Rosenblatt于1958年发明，人们很快发现单层感知器的限制使分类方法不切实际。然而，多层感知器今天仍然被用作“前馈”神经网络的基本构件。因此，对于任何有抱负的数据科学家来说，敏锐地理解这种算法是至关重要的。</p><p id="e63d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这篇文章旨在作为算法背后的数学的高层次概述，以及一个关于如何编写自己的感知器算法的例子。虽然代码示例是用R语言编写的，但我用Python重新编写了类似的函数，你可以在这里找到<a class="ae kz" href="https://github.com/bentontripp/perceptron-decepticon/blob/3a1e134e6747f5197c5edbc314c780ffb2d01539/notebooks/perceptron.ipynb" rel="noopener ugc nofollow" target="_blank"/>。</p><h2 id="b4b9" class="lw lx it bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn mo bi translated">r包</h2><p id="4c0f" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">在我们开始之前，我将快速分享本教程中使用的所有R包。如果当前没有安装，可以先安装。本教程使用的软件包有:<br/>—<a class="ae kz" href="https://cran.r-project.org/web/packages/data.table/index.html" rel="noopener ugc nofollow" target="_blank">data . table</a><br/>—<a class="ae kz" href="https://ggplot2.tidyverse.org/" rel="noopener ugc nofollow" target="_blank">gg plot</a><br/>—<a class="ae kz" href="https://plotly.com/r/" rel="noopener ugc nofollow" target="_blank">plot ly</a><br/>—<a class="ae kz" href="https://cran.r-project.org/web/packages/matlib/index.html" rel="noopener ugc nofollow" target="_blank">matlib</a></p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="e947" class="lw lx it mv b gy mz na l nb nc"># Install packages if needed<br/><br/>if (!require(data.table)) install.packages('data.table')</span><span id="abad" class="lw lx it mv b gy nd na l nb nc">if (!require(ggplot2)) install.packages('ggplot2')</span><span id="94f4" class="lw lx it mv b gy nd na l nb nc">if (!require(plotly)) install.packages('plotly')</span><span id="7300" class="lw lx it mv b gy nd na l nb nc">if (!require(matlib)) install.packages('matlib')</span><span id="cdb7" class="lw lx it mv b gy nd na l nb nc">library(data.table)<br/>library(ggplot2)<br/>library(plotly)<br/>library(matlib)</span></pre><h2 id="a605" class="lw lx it bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn mo bi translated">感知器算法的数学基础</h2><p id="c712" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">为了理解感知器算法，必须首先理解以下数学原理的基本知识:</p><ul class=""><li id="3b29" class="ne nf it lc b ld le lg lh lj ng ln nh lr ni lv nj nk nl nm bi translated">向量(具体来说，如何计算方向和范数)</li><li id="4645" class="ne nf it lc b ld nn lg no lj np ln nq lr nr lv nj nk nl nm bi translated">点积</li></ul><p id="3567" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我假设大多数阅读这篇文章的人都事先了解这些基础知识。如果你没有，在这里可以找到一个很好的资源。我不打算尝试教授基础知识，因为已经有这么多(更好的)可用资源。</p><h2 id="8ea6" class="lw lx it bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn mo bi translated">线性可分数据</h2><p id="ef3d" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">如前所述，单层感知器的挑战之一是约束限制了算法准确分类数据的能力。具体来说，主要约束是数据<em class="ns">必须</em>是线性可分的。</p><p id="1a83" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">线性可分数据的定义是不言自明的。在高层次上，对于可线性分离的数据来说，这仅仅意味着存在某个平面可以真正地“分离”数据。例如，考虑以下两个类的数据集:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="c08a" class="lw lx it mv b gy mz na l nb nc">DT &lt;- data.table(<br/>  class_ = c(rep(1, 31), rep(-1, 31)),<br/>  x = c(seq(1, 4, .1), seq(7, 10, .1)),<br/>  y = c(runif(31, 1, 4), runif(31, 7, 10))<br/>)<br/><br/>ggplot(DT, aes(x=x, y=y)) + <br/>  geom_point(colour=c(rep('blue',31), rep('darkred', 31))) + <br/>  theme(legend.position='none',<br/>        axis.text.x=element_blank(),<br/>        axis.text.y=element_blank())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/2d4266036739eb57fc46eeb6e54442e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pXVgwDvzWm0K42oT2WW9VQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="5e46" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">很明显，您可以画一条任意的线来分隔两个不同颜色的数据点:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="77b8" class="lw lx it mv b gy mz na l nb nc">ggplot(DT, aes(x=x, y=y)) + <br/>  geom_point(colour=c(rep('blue',31), rep('darkred', 31))) + <br/>  geom_abline(slope=-1.25, intercept=12) + <br/>  theme(legend.position='none',<br/>        axis.text.x=element_blank(),<br/>        axis.text.y=element_blank())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/112be361c5897378f5d8a6460125071a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WmhQgNEYQhzWFkv6d0yS3Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="6f44" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">类似地，考虑以下三维数据:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="dd55" class="lw lx it mv b gy mz na l nb nc">DT &lt;- DT[, z := c(runif(31, 1, 4), runif(31, 7, 10))]<br/><br/>scene = list(<br/>  camera = list(<br/>    center = list(x = 0, y = 0, z = 0), <br/>    eye = list(x = 1.75, y = .75, z = .25)<br/>    ),<br/>  xaxis=list(title='x', showticklabels=F),<br/>  yaxis=list(title='y', showticklabels=F),<br/>  zaxis=list(title='z', showticklabels=F)<br/>)<br/><br/>plot_ly(data=DT,<br/>        x=~x, y=~y, z=~z, <br/>        color=~as.factor(class_),<br/>        type='scatter3d',<br/>        mode='markers', <br/>        colors=c('darkred', 'blue'),<br/>        showlegend=F) %&gt;%<br/>  layout(scene=scene)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/ac4af326f5488182c9f7d9c9715584c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6_rDbFcM5pDn7EiUSYNKGA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="da70" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在这种情况下，当试图分离两个数据类时，一条线是不够的。相反，可以使用二维平面:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="d1c7" class="lw lx it mv b gy mz na l nb nc">plot_ly(data=DT,<br/>        colors=c('darkred', 'gray', 'blue')) %&gt;%<br/>  add_trace(x=~seq(min(x), max(x), .1)*1.5, <br/>            y=~seq(min(y), max(y), .1)*1.5, <br/>            z=~matrix(6, nrow=62, ncol=62),<br/>            type='surface') %&gt;%<br/>  add_trace(x=~x, y=~y, z=~z, <br/>            color=~as.factor(class_),<br/>            type='scatter3d',<br/>            mode='markers',<br/>            showlegend=F) %&gt;%<br/>  layout(scene=scene) %&gt;% <br/>  hide_colorbar()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/c5850f2aba0016ecf6ab78f69cf9faf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NrMSETLszf5nE8YvlM-kvg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="3016" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">当然，随着维度的不断增加，这变得更加难以想象，甚至难以解释。因此，使用了一个更广义的术语来描述这种线性分离:一个<em class="ns">超平面</em>。</p><h2 id="cd15" class="lw lx it bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn mo bi translated">超平面</h2><p id="e82e" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">根据<a class="ae kz" href="https://en.wikipedia.org/wiki/Hyperplane" rel="noopener ugc nofollow" target="_blank">维基百科的定义</a>:“在几何学中，超平面是一个维度比其周围空间的维度小一的子空间。”这是有道理的。看我们的第一个例子，数据的维度是两个，而用来分隔数据的线是一维的。在第二个例子中，数据是三维的，间隔是二。</p><h2 id="384b" class="lw lx it bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn mo bi translated">感知器算法</h2><p id="1033" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">现在假设你有一些向量的基础知识，我们已经回顾了线性可分性和超平面的含义，我们可以在这些知识的基础上理解感知器算法是如何工作的。</p><p id="7c9f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">首先，考虑以下几点:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/36d43577beb93fbaad74aedfe303b23b.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*dvjzfyV8AFdO7XWwNKiXWQ.png"/></div></figure><p id="04cb" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">更简单地说，我们说数据中有两个类(<strong class="lc iu"> x </strong>)，因此有一个二进制分类问题。我们称之为类1和-1，但它们可以是任何东西。假设这两个类是线性可分的，那么一定存在某个<strong class="lc iu"> w </strong>使得当<strong class="lc iu"> x </strong>和<strong class="lc iu"> w </strong>之间的点积(正偏差)为正时，类为1。而当点积为负时，类为负1。</p><p id="943f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了找到最优的<strong class="lc iu"> w </strong>，我们可以首先将向量初始化为随机值。从技术上讲，这些值可能都为零。我选择初始化0到1之间的随机值。</p><p id="4190" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在本例中，我们将处理本文前面演示线性可分性的例子中给出的二维数据。因为数据是二维的(<em class="ns"> x </em>，<em class="ns"> y </em>)，我们知道分隔类的超平面会是一条线。我们也知道向量<strong class="lc iu"> w </strong>也会有二维，加上一些偏差<em class="ns"> b </em>。</p><p id="8dcf" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了让事情变得简单一点，我们实际上可以完全消除等式中的偏差。为此，首先回忆一下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/415abb6444e5ce4698d095c6bc45fff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:262/format:webp/1*V6OhnTu5dQ0J4xZ09kFVjg.png"/></div></figure><p id="ba67" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这意味着:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/c4d2759ae53986edf27c21999e9a7d8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*fhaR7NsCEGSYWZ57JtTzWg.png"/></div></figure><p id="943b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了消除偏差<em class="ns"> b </em>，我们可以简单地向<strong class="lc iu"> x </strong>和<strong class="lc iu"> w </strong>添加一个零分量，这样:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/26a921a1f0b77e2c0a514f6d03cef18d.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*Xp-fjrDuW_PoK57vtzrzTA.png"/></div></figure><p id="0a74" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在，原始函数可以写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/f7b405e37caa3b23c268eab330c6f56c.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*zJUeUlWX2sjzmhN3AhmJGA.png"/></div></figure><p id="983e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在我们的二维例子中，这意味着<strong class="lc iu"> w </strong>应该用3个值初始化，而不是原来的两个值。我们的矩阵<strong class="lc iu"> x </strong>也应该增加一个附加的数据“列”,其中附加列中的每个值都等于1。在R中，这些步骤看起来像下面的代码:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="aeb2" class="lw lx it mv b gy mz na l nb nc"># Generate random weights (initialize `w` with 3 values)<br/>w &lt;- runif(3, min=0, max=1)<br/><br/># Remove `z` since we are using 2D data in this example<br/># Augment the data such that `x0` = 1<br/>DT[, z := NULL][, x0 := 1]</span></pre><p id="2398" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">既然<strong class="lc iu"> w </strong>和<strong class="lc iu"> x </strong>都被初始化并增加以考虑任何偏差<em class="ns"> b </em>，我们可以使用公式<em class="ns"> f(x) </em>进行一些初始预测。这可以通过取<strong class="lc iu"> x </strong>和<strong class="lc iu"> w </strong>的点积的符号来完成。我们还将确定初始预测哪里不正确，因为这些将在算法的下一步中发挥关键作用。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="29a2" class="lw lx it mv b gy mz na l nb nc"># Predict class (either -1 or 1) at each point by taking the dot<br/># product<br/>pred &lt;- apply(DT[, 2:4], 1, FUN = function(x) sign(sum(x * w)))<br/><br/># Get incorrect predictions<br/>incorrect_preds &lt;- DT[class_ != pred]</span></pre><p id="b3c4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在下一步中，我们将从错误分类的预测中抽取一个随机样本，将样本的三个<strong class="lc iu"> x </strong>值分别乘以实际类别(1或-1)，然后将样本的三个相应值分别添加到向量<strong class="lc iu"> w </strong>。这给了我们一个更新的矢量<strong class="lc iu"> w </strong>。然后，重复原来的过程。使用新的<strong class="lc iu"> w </strong>进行预测，不正确的预测被识别并用于更新<strong class="lc iu"> w </strong>。该过程重复进行，直到没有不正确的预测(或者达到预定义的阈值)。</p><p id="5999" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了理解这一过程背后的推理，它有助于描述向量实际上发生了什么。作为例子，考虑以下向量，其中<code class="fe ob oc od mv b">x_sample1</code>是从某个任意向量<strong class="lc iu"> x </strong>的不正确预测中取得的随机样本，并且<code class="fe ob oc od mv b">w1</code>是计算中使用的权重向量:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="cccf" class="lw lx it mv b gy mz na l nb nc">x_sample1 &lt;- c(.3, .8)<br/>w1 &lt;- c(.6, .1)</span></pre><p id="d70e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">形象化，我们得到如下(其中<em class="ns">θ</em>是<code class="fe ob oc od mv b">x_sample1</code>和<code class="fe ob oc od mv b">w1</code>之间的角度):</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="3f6b" class="lw lx it mv b gy mz na l nb nc">plot(1, type='n', xlim=c(-.1, 1), ylim=c(-.1, 1), <br/>     xlab='', ylab='', xaxt='n', yaxt='n')<br/>grid()<br/><br/>vectors(x_sample1, labels=expression(x))<br/>vectors(w1, labels=expression(w))<br/><br/>arc(w1, c(0, 0), x_sample1, d=.1, absolute=T)<br/>text(.15, .15, expression(theta), cex=1.5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f084abdd4ef5da6517fc3bdf7c872111.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*FOH1fOgnmgIOUJs3Fum9Xw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="ab68" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们可以看到两个向量<code class="fe ob oc od mv b">x_sample1</code>和<code class="fe ob oc od mv b">w1</code>之间的角度小于90°。我们也知道<code class="fe ob oc od mv b">x_sample1</code>的预测类别是不正确的，因为样本取自错误分类的数据。这意味着<code class="fe ob oc od mv b">x_sample1</code>的实际类与<code class="fe ob oc od mv b">x_sample1</code>和<code class="fe ob oc od mv b">w1</code>的点积相反。现在，我们需要以某种方式调整<code class="fe ob oc od mv b">w1</code>，使<code class="fe ob oc od mv b">x_sample1</code>和<code class="fe ob oc od mv b">w1</code>之间的点积的符号等于<code class="fe ob oc od mv b">x_sample1</code>的类。</p><p id="0fa4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为此，我们先来看看<code class="fe ob oc od mv b">x_sample1</code>的预测值是多少:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="bbd6" class="lw lx it mv b gy mz na l nb nc">print(paste0('The predicted class is: ', sign(x_sample1 %*% w1)[1]))</span><span id="8bd8" class="lw lx it mv b gy nd na l nb nc">## [1] "The predicted class is: 1"</span></pre><p id="e2b7" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">因为预测的类是1，并且我们知道这个例子中的预测是不正确的，所以我们可以推断实际的类是-1。为了得到预测值1，我们需要<code class="fe ob oc od mv b">x_sample1</code>和我们的<strong class="lc iu"> w </strong>值之间的点积为负。</p><p id="5eb9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为此，我们可以简单地从<code class="fe ob oc od mv b">w1</code>中减去<code class="fe ob oc od mv b">x_sample1</code>:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="7029" class="lw lx it mv b gy mz na l nb nc">plot(1, type='n', xlim=c(-.05, .7), ylim=c(-.8, .8), <br/>     xlab='', ylab='', xaxt='n', yaxt='n')<br/>grid()<br/><br/>new_w1 &lt;-  w1 - x_sample1<br/><br/>vectors(x_sample1, labels=expression(x))<br/>vectors(w1, labels=expression(w))<br/>vectors(new_w1, labels=expression(w - x), col='darkred')<br/><br/>arc(w1, c(0, 0), x_sample1, d=.3, absolute=F)<br/>text(.2, .3, expression(theta), cex=1.5)<br/><br/>arc(new_w1, c(0, 0), x_sample1, d=.2, col='darkred')<br/>text(.12, .16, expression(beta), cex=1.5, col='darkred')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/fe776b499672ec06adc38541b1659856.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*wMeKsq-o6k37svTc_LMJMg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="018e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这产生了以下预测:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="de1a" class="lw lx it mv b gy mz na l nb nc">print(paste0('The predicted class is: ', sign(x_sample1 %*% new_w1)[1]))</span><span id="b6f7" class="lw lx it mv b gy nd na l nb nc">## [1] "The predicted class is: -1"</span></pre><p id="a5ce" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在，让我们再取两个任意向量<code class="fe ob oc od mv b">x_sample2</code>和<code class="fe ob oc od mv b">w2</code>。设<code class="fe ob oc od mv b">x_sample2</code>与<code class="fe ob oc od mv b">w2</code>的夹角大于90度，实际<code class="fe ob oc od mv b">x_sample2</code>的等级为1，预测值为-1:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="2f9c" class="lw lx it mv b gy mz na l nb nc">plot(1, type='n', xlim=c(-.8, 1), ylim=c(-.1, .6), <br/>     xlab='', ylab='', xaxt='n', yaxt='n')<br/>grid()<br/><br/>x_sample2 &lt;- c(-.7, .45)<br/>w2 &lt;- c(.9, .05)<br/><br/>vectors(x_sample2, labels=expression(x))<br/>vectors(w2, labels=expression(w))<br/><br/>arc(w1, c(0, 0), x_sample2, d=.1)<br/>text(.05, .07, expression(theta), cex=1.5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/a1443b26b5ccabfbb2e4f79c77ff44bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*2ow6V0so6eJ_ybnZaMK7qg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="f9bb" class="lw lx it mv b gy mz na l nb nc">print(paste0('The predicted class is: ', sign(x_sample2 %*% w2)[1]))</span><span id="28fe" class="lw lx it mv b gy nd na l nb nc">## [1] "The predicted class is: -1"</span></pre><p id="873f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">因为预测的类是-1，我们需要它是1，我们实际上需要减小<code class="fe ob oc od mv b">x_sample</code>和<code class="fe ob oc od mv b">w2</code>之间的角度。为此，我们将它们相加，而不是相减:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="d7ac" class="lw lx it mv b gy mz na l nb nc">plot(1, type='n', xlim=c(-.9, 1.1), ylim=c(-.05, .6), <br/>     xlab='', ylab='', xaxt='n', yaxt='n')<br/>grid()<br/><br/>new_w2 &lt;-  w2 + x_sample2<br/><br/>vectors(x_sample2, labels=expression(x))<br/>vectors(w2, labels=expression(w))<br/>vectors(new_w2, labels=expression(w + x), col='darkblue')<br/><br/>arc(w2, c(0, 0), x_sample2, d=.1)<br/>text(-.02, .08, expression(theta), cex=1.2)<br/><br/>arc(new_w2, c(0, 0), x_sample2, d=.15, col='darkblue')<br/>text(-.05, .18, expression(beta), cex=1.2, col='darkblue')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/2d45bfaabc21d9a04aff95b971cca7a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*2c_WkXnDvOr1e0_v_lAwlQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="ed1d" class="lw lx it mv b gy mz na l nb nc">print(paste0('The predicted class is: ', sign(x_sample2 %*% new_w2)[1]))</span><span id="00cb" class="lw lx it mv b gy nd na l nb nc">## [1] "The predicted class is: 1"</span></pre><p id="f649" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如前所述，该过程对每个新的<strong class="lc iu"> w </strong>进行迭代，直到没有不正确的预测，或者满足某个预定义的阈值。如果数据不是线性可分的，那么<strong class="lc iu"> w </strong>永远不会收敛。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="1a7d" class="lw lx it mv b gy mz na l nb nc"># Iteratively update weights<br/>repeat{<br/>  w &lt;- w + unname(unlist(<br/>    incorrect_preds[sample(1:nrow(incorrect_preds), 1)][, .SD * class_, .SDcols=2:4][1,]<br/>    ))<br/>  pred &lt;- apply(DT[, 2:4], 1, FUN = function(x) sign(sum(x * w)))<br/>  incorrect_preds &lt;- DT[class_ != pred]<br/>    if (nrow(incorrect_preds) == 0) break<br/>}<br/><br/># Plot<br/>ggplot(data=DT, aes(x=x, y=y)) +<br/>  geom_point(color=c(rep('blue',31), rep('darkred', 31))) +<br/>  geom_abline(slope=-w[[2]] / w[[1]], intercept=-w[[3]] / w[[1]]) + <br/>  theme(legend.position='none',<br/>        axis.text.x=element_blank(),<br/>        axis.text.y=element_blank())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/054412223c70e973c5f73f792aa5a2ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5k4zpwjr_xJG2RKZWaKAPg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="44f1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们做到了！你可以自己尝试所有的代码。下面是一个完整的R脚本，算法中的所有步骤都被定义为函数。如果你还没有感受到用Python写的同样的代码<a class="ae kz" href="https://github.com/bentontripp/perceptron-decepticon/blob/3a1e134e6747f5197c5edbc314c780ffb2d01539/notebooks/perceptron.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure></div><div class="ab cl ok ol hx om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="im in io ip iq"><p id="94d8" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在以后的文章中，我打算分享关于多层和内核感知机的内容，敬请关注！</p><p id="7b36" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">也可以随意查看我最近的作品，这是一个关于使用Python解决微积分问题的3部分系列(目前仍在进行中)。</p><p id="7971" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">最后，如果你还没有订阅Medium，并想阅读无限的文章，请查看这个<a class="ae kz" href="https://medium.com/@bentontripp/membership" rel="noopener">链接</a>。</p><h2 id="1171" class="lw lx it bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn mo bi translated">参考</h2><p id="44d5" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated"><em class="ns">所有图片，除非特别注明，均为作者所有。</em></p><p id="0867" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">[1]维基百科<em class="ns">，</em>感知器(2022)，<a class="ae kz" href="https://en.wikipedia.org/wiki/Perceptron" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Perceptron</a><br/>【2】亚历山大·科瓦尔茨克，SVM简洁地(2017)，<a class="ae kz" href="https://bitbucket.org/syncfusiontech/svm-succinctly/src/master/" rel="noopener ugc nofollow" target="_blank">https://bitbucket . org/syncfusion tech/SVM-简洁地/src/master/</a><br/>【3】维基百科，Hyperplanes (2021)，<a class="ae kz" href="https://en.wikipedia.org/wiki/Hyperplane" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Hyperplane</a></p></div></div>    
</body>
</html>