<html>
<head>
<title>State-of-the-Art Machine Learning Hyperparameter Optimization with Optuna</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Optuna 优化最先进的机器学习超参数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/state-of-the-art-machine-learning-hyperparameter-optimization-with-optuna-a315d8564de1#2022-03-31">https://towardsdatascience.com/state-of-the-art-machine-learning-hyperparameter-optimization-with-optuna-a315d8564de1#2022-03-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="dc0b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Optuna 是一个高级的超参数优化框架，具有可视化的可解释性</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f16877eec88f84c76c094d475b9652e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*U0EtQwjGHe3KyrzN"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的<a class="ae kv" href="https://unsplash.com/@thisisengineering?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> ThisisEngineering RAEng </a></p></figure><h1 id="0828" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="486b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在本文中，我们将讨论 Optuna，这是一个超参数优化软件框架，专门为机器学习管道而设计。Optuna 使用户能够采用最先进的算法对超参数进行采样，并删除没有希望的试验。与 GridSearch 等传统方法相比，这有助于大大加快优化时间和性能。它还允许用户绘制优化历史，以便更好地理解模型。</p><p id="6dde" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在我们开始之前，让我们简单回顾一下超参数，并讨论一下传统优化方法和可以在 Optuna 框架中应用的最新、更先进的优化方法之间的区别。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="f583" class="kw kx iq bd ky kz mw lb lc ld mx lf lg jw my jx li jz mz ka lk kc na kd lm ln bi translated">超参数概述</h1><p id="3d08" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">超参数是控制机器学习模型学习过程的变量。它们最终决定了模型如何学习输入和预测之间的特定关系。</p><p id="b89a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">一个非常简单的例子是，是否“将截距固定到简单的线性回归上”是线性回归模型的超参数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/be2ce0fa7a4fe2e1fe1f687f78e49a29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BEpIBB8T_NYE2GMp"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@spencerarquimedes?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">斯潘塞·阿奎迈德斯</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="30e4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">优化模型的超参数以解决特定问题通常是必要的。以下是一些原因:</p><ul class=""><li id="1ebd" class="nc nd iq lq b lr mk lu ml lx ne mb nf mf ng mj nh ni nj nk bi translated">机器学习超参数<strong class="lq ir">不是万能的</strong>，因为这些模型可能需要不同的约束来概括不同的样本外数据模式和不同的问题。</li><li id="bbfd" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj nh ni nj nk bi translated">超参数优化允许我们<strong class="lq ir">使用最佳超参数集生成最优模型</strong>。该模型应该能够给出使损失函数最小化的<strong class="lq ir">最优结果。</strong></li></ul></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="cf40" class="kw kx iq bd ky kz mw lb lc ld mx lf lg jw my jx li jz mz ka lk kc na kd lm ln bi translated">优化方法概述</h1><p id="47b5" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">优化问题的问题在于，搜索空间通常是不确定的，并且执行这种搜索的预算是受限的(最多 x 次时间或迭代)。因此，为了有效地搜索全局最小值，算法必须实现有效利用预算的方法。</p><h2 id="fc6c" class="nq kx iq bd ky nr ns dn lc nt nu dp lg lx nv nw li mb nx ny lk mf nz oa lm ob bi translated">传统方法</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/618073eb89fe42594bf6ca732b49a575.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*U7Osu7ktRbZnOt6Y"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae kv" href="https://unsplash.com/@neonbrand?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> NeONBRAND </a>拍摄的照片</p></figure><p id="1334" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">执行超参数优化的传统方式是穷尽的，并且不了解先前的信息。一些例子是:</p><ul class=""><li id="9e22" class="nc nd iq lq b lr mk lu ml lx ne mb nf mf ng mj nh ni nj nk bi translated"><strong class="lq ir"> <em class="od">网格搜索</em> </strong>，穷尽搜索预定义的子集。</li><li id="272e" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj nh ni nj nk bi translated"><strong class="lq ir">随机搜索</strong>从预定义的子集中随机选择。</li></ul><p id="f128" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">随着数据量和超参数空间的增长，一些传统的搜索方法迅速退化。他们要么花太多时间，要么甚至找不到最小值。</p><h2 id="98cb" class="nq kx iq bd ky nr ns dn lc nt nu dp lg lx nv nw li mb nx ny lk mf nz oa lm ob bi translated">贝叶斯方法</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/c17996ee34a2bb757c9fedda301ee754.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*d-uAUJqsmECC88Ur"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">马修·安斯利在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="9250" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">贝叶斯优化方法通过迭代地建立从超参数值到目标函数的函数映射的<strong class="lq ir">概率模型</strong>来搜索<strong class="lq ir">全局优化</strong>。概率模型<strong class="lq ir">捕获关于函数</strong>行为的信念，以形成目标函数的后验分布。</p><p id="003e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">之后，进一步使用后验分布来形成<strong class="lq ir">获取函数</strong>，该函数确定具有<strong class="lq ir">最佳改进概率</strong>的下一点。</p><p id="c611" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然而，当它使用可用的信息来做决定时，一个探索与开发的问题就出现了。</p><ul class=""><li id="e7b9" class="nc nd iq lq b lr mk lu ml lx ne mb nf mf ng mj nh ni nj nk bi translated"><strong class="lq ir">探索</strong>类似于全局搜索；我们有兴趣探索超参数搜索空间，寻找更好的解决方案。</li><li id="d47b" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj nh ni nj nk bi translated"><strong class="lq ir">利用</strong>类似于本地搜索；我们希望改进我们当前的解决方案，并尽量避免在不必要的搜索空间上浪费宝贵的资源。</li></ul><p id="72f2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然后提出了多种算法来最佳地平衡勘探-开发的困境。一些例子是:树形结构的 Parzen 估计器(TPE)和高斯过程回归器，例如克里金法。</p><h2 id="9803" class="nq kx iq bd ky nr ns dn lc nt nu dp lg lx nv nw li mb nx ny lk mf nz oa lm ob bi translated">早期停止方法</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/b4d2a3f8f2288df9bb7fa0106aef4d0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WB0DAMU03vi3M5tI"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@cinematicphoto?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">安瓦尔·阿里</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="4a8f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">基于早期停止的超参数优化算法使用<strong class="lq ir">统计测试来丢弃表现不佳</strong>且不包含全局最小值的搜索空间。</p><p id="09fc" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">它的工作原理是<strong class="lq ir">用一组特定的超参数检查模型</strong>的中间分数。例如，在训练神经网络时，假设为网络选择了 16 组不同的超参数。在五个 epoches 之后，我们检查所有的中间分数并丢弃那些表现差的超参数。</p><p id="2440" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">两种流行早期停止优化算法是连续减半(SHA)和超带。</p><p id="6ac9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然而，对于不产生中间结果的模型，不使用基于提前停止的方法是可行的。</p><h2 id="e99c" class="nq kx iq bd ky nr ns dn lc nt nu dp lg lx nv nw li mb nx ny lk mf nz oa lm ob bi translated">进化方法</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/96cee1ee8e863350bb6cbdf198ad9f00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zzX674d_2PJu1iDq"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@jplenio?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Johannes Plenio </a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="fdd6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">进化优化方法使用进化算法搜索超参数空间。这种方法受到了查尔斯·达尔文进化论的启发。它通常遵循一个遵循<strong class="lq ir">生物进化概念</strong>的过程:</p><ul class=""><li id="20c2" class="nc nd iq lq b lr mk lu ml lx ne mb nf mf ng mj nh ni nj nk bi translated">从群体超参数搜索空间中抽取初始样本。</li><li id="bfa9" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj nh ni nj nk bi translated">使用<strong class="lq ir">适应度函数</strong>和按照相对适应度排序的来评估超参数。</li><li id="1b17" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj nh ni nj nk bi translated"><strong class="lq ir">表现最差的</strong>超参数被<strong class="lq ir">丢弃</strong>，通过<strong class="lq ir">交叉</strong>和<strong class="lq ir">变异</strong>生成新的超参数集。</li><li id="5640" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj nh ni nj nk bi translated">进化过程重复进行，直到出现预算限制或性能没有改善。</li></ul></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="ed85" class="kw kx iq bd ky kz mw lb lc ld mx lf lg jw my jx li jz mz ka lk kc na kd lm ln bi translated">介绍一下，奥普图纳。</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/682243e585436baadabcb27f45f4555c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vxT2EgSoi7ABJ5Id"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@dtopkin1?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">戴恩·托普金</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="140e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">Optuna 是一个超参数优化软件框架，能够轻松实现不同的最先进的优化方法，以高性能快速执行超参数优化。</p><p id="8bfd" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">默认情况下，Optuna 实现了一个贝叶斯优化算法(TPE ),但它可以很容易地切换到包中的其他现有算法。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="2c28" class="kw kx iq bd ky kz mw lb lc ld mx lf lg jw my jx li jz mz ka lk kc na kd lm ln bi translated">Optuna 中的优化算法</h1><p id="ad9d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">Optuna 将其优化算法分为两个不同的类别，即采样策略和修剪策略。</p><ul class=""><li id="0c19" class="nc nd iq lq b lr mk lu ml lx ne mb nf mf ng mj nh ni nj nk bi translated"><strong class="lq ir">采样策略:</strong>通过集中在超参数给出更好结果的区域来选择最佳参数组合的算法。</li><li id="39e8" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj nh ni nj nk bi translated"><strong class="lq ir">修剪策略:</strong>基于早期停止的优化方法如我们上面讨论的。</li></ul><p id="b95d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们将简要讨论上一节中讨论的三种算法类型的一个示例背后的直觉，以便我们可以对这些算法如何工作有一个大致的了解。</p><p id="dae6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然而，我们不会太深入，因为这需要一些时间来解释。这些算法都很容易在包中定义，并可用于生产。</p><h2 id="4494" class="nq kx iq bd ky nr ns dn lc nt nu dp lg lx nv nw li mb nx ny lk mf nz oa lm ob bi translated">抽样策略</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/3bbfe9b628fbcd4fc048edab396d34f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OC6wZOZO2Y5eRotV"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">米兰·塞特勒在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h2 id="760e" class="nq kx iq bd ky nr ns dn lc nt nu dp lg lx nv nw li mb nx ny lk mf nz oa lm ob bi translated">TPESampler(树形结构 Parzen 估计器):</h2><p id="56e6" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">一种<strong class="lq ir">贝叶斯</strong>优化算法，即:</p><ol class=""><li id="ab0f" class="nc nd iq lq b lr mk lu ml lx ne mb nf mf ng mj oi ni nj nk bi translated">首先，随机选择超参数的子集，并根据它们的分数对它们进行排序<strong class="lq ir"> </strong>。</li><li id="ed13" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj oi ni nj nk bi translated">基于一些预定义的分位数，超参数被进一步分成两组。</li><li id="d476" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj oi ni nj nk bi translated">然后使用 Parzen 估计器(核密度估计器)将这两个组建模为估计的密度<strong class="lq ir"> </strong> l(x1)和 g(x2)。</li><li id="5971" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj oi ni nj nk bi translated">找到具有最高预期改进的超参数[最低 l(x1)/g(x2)]。</li><li id="c0ea" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj oi ni nj nk bi translated">具有最高预期改进的超参数被再次评估、排序和划分。这个过程重复进行，直到预算完成，并返回最佳超参数。</li></ol><h2 id="8ce3" class="nq kx iq bd ky nr ns dn lc nt nu dp lg lx nv nw li mb nx ny lk mf nz oa lm ob bi translated">非支配排序遗传算法 II</h2><p id="b4a9" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">一种多目标函数的<strong class="lq ir">进化</strong>优化算法。</p><p id="23cb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">一个个体(A)被称为支配另一个个体(B)，如果</p><ul class=""><li id="35c4" class="nc nd iq lq b lr mk lu ml lx ne mb nf mf ng mj nh ni nj nk bi translated">A 目标没有比 B 的目标更坏的了</li><li id="884f" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj nh ni nj nk bi translated">至少有一个 A 目标比 b 的目标更好</li></ul><p id="4aa2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">算法的主要过程:</p><ol class=""><li id="e72f" class="nc nd iq lq b lr mk lu ml lx ne mb nf mf ng mj oi ni nj nk bi translated">最初，随机的亲代群体被采样，并且每个被评估的解被分配一个与其非支配水平相等的适合度等级。它首先从种群 P 中选择所有的非支配解并将它们分配到等级 1，然后从剩余的解中选择所有的解并将它们分配到等级 2，依此类推，直到所有的个体都被分配到一个等级。</li><li id="e071" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj oi ni nj nk bi translated">挑选两个随机试验，较好的一个成为亲本 1。重复该过程一次以选择另一个亲代 2 [ <strong class="lq ir">二元锦标赛选择</strong> ]。</li><li id="bbd4" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj oi ni nj nk bi translated">这两个父母重组产生后代，这些后代进入子群体。子进程经历了突变，并改变了它的一些值[ <strong class="lq ir">突变</strong> ]。重复此步骤，直到您拥有两倍的初始群体规模。</li><li id="8481" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj oi ni nj nk bi translated">再次根据非支配性对种群进行排序。新一代将按排名顺序选出。如果下一代只包括部分特定的等级，将执行拥挤排序来计算解的密度。不太密集的试验被选择到下一代中，直到群体计数再次达到初始群体大小。</li><li id="07a9" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj oi ni nj nk bi translated">新的一代被重复产生并再次被丢弃，直到满足最大数量的代，并且最佳超参数将被返回。</li></ol><p id="e2b8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">其他一些流行的采样策略:<strong class="lq ir"> CMA-ES 采样器，MOTPE 采样器</strong></p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h2 id="497c" class="nq kx iq bd ky nr ns dn lc nt nu dp lg lx nv nw li mb nx ny lk mf nz oa lm ob bi translated">剪枝策略</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/08e4108cfd02ff97b989a6614d66c5a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hX9TOZkeQ-Jh1Io6"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@crystalsjo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">水晶乔</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><h2 id="05f7" class="nq kx iq bd ky nr ns dn lc nt nu dp lg lx nv nw li mb nx ny lk mf nz oa lm ob bi translated">SuccessiveHalvingPruner(异步连续减半)</h2><ol class=""><li id="274f" class="nc nd iq lq b lr ls lu lv lx oj mb ok mf ol mj oi ni nj nk bi translated">随机选择一组初始超参数值。</li><li id="0324" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj oi ni nj nk bi translated">训练 1 个时期的试验，直到达到定义的最大试验次数。</li><li id="65ba" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj oi ni nj nk bi translated">同时，每当试验的分数在梯级内的前 d 个百分比中时，试验被同时提升到另一个梯级(类似于等级)以训练更多的时期，其中 d 是预定义的除数。</li></ol><p id="9513" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">注意，这不同于同步连续减半，在同步连续减半中，算法等待一个梯级中所有定义的试验完成它们的时期，然后才决定哪些试验具有要提升的最高分数，以在另一个梯级中训练更多的时期。</p><p id="deeb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">其他一些流行的修剪策略:<strong class="lq ir"> MedianPruner，HyperbandPruner </strong></p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="8372" class="kw kx iq bd ky kz mw lb lc ld mx lf lg jw my jx li jz mz ka lk kc na kd lm ln bi translated">代码实现</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/8af065cad10529384a97ea55e8d18cf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FQjf5vfHLhOzXJ_7"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@clemhlrdt?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Clément Hélardot </a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h2 id="e2aa" class="nq kx iq bd ky nr ns dn lc nt nu dp lg lx nv nw li mb nx ny lk mf nz oa lm ob bi translated">目标函数</h2><p id="c05c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在我们开始用 Optuna 实现优化之前，它要求我们定义一个目标函数。<br/>目标函数将包含常规模型定义、训练和测试过程的全部逻辑。在模型评估之后，它应该返回评估指标，这也是由用户选择的。</p><h2 id="2245" class="nq kx iq bd ky nr ns dn lc nt nu dp lg lx nv nw li mb nx ny lk mf nz oa lm ob bi translated">审判</h2><p id="1a16" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">试用类将用于存储机器学习模型稍后使用的超参数的一个特定组合的信息。</p><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="91fc" class="nq kx iq on b gy or os l ot ou">import optuna<br/>import sklearn<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.model_selection import cross_val_score</span><span id="fc03" class="nq kx iq on b gy ov os l ot ou">def objective(trial):<br/>    digits = sklearn.datasets.load_digits()<br/>    x, y = digits.data, digits.target</span><span id="faa1" class="nq kx iq on b gy ov os l ot ou">    max_depth = trial.suggest_int("rf_max_depth", 2, 64, log=True)<br/>    max_samples = trial.suggest_float("rf_max_samples", 0.2, 1)<br/>   <br/>    rf_model = RandomForestClassifier(<br/>        max_depth = max_depth,<br/>        max_samples = max_samples,<br/>        n_estimators = 50,<br/>        random_state = 42)</span><span id="6cc5" class="nq kx iq on b gy ov os l ot ou">score = cross_val_score(rf_model, x, y,  cv=3).mean()</span><span id="ada7" class="nq kx iq on b gy ov os l ot ou">return score</span></pre><h2 id="526d" class="nq kx iq bd ky nr ns dn lc nt nu dp lg lx nv nw li mb nx ny lk mf nz oa lm ob bi translated">这项研究</h2><p id="cbec" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">然后可以调用研究对象来优化目标函数，以找到最佳的超参数组合。然后，它将反复运行试验，直到用户定义的最大试验或时间。具有最佳超参数的试验将存储在 study.best_trial 中。</p><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="91d5" class="nq kx iq on b gy or os l ot ou">study = optuna.create_study(direction = "maximize")<br/>study.optimize(objective, n_trials = 100)</span><span id="395b" class="nq kx iq on b gy ov os l ot ou">trial = study.best_trial<br/>print("Best Score: ", trial.value)<br/>print("Best Params: ")<br/>for key, value in trial.params.items():<br/>    print("  {}: {}".format(key, value))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/eb4128ab9a83c9d8a7cb11964e5b85ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*Dt-id_5TGqPIZnffoDNlgw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者输出的代码</p></figure><p id="1d86" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">之后，我们可以继续在 Optuna 包中绘制一些可视化图形，以理解超参数之间的关系。</p><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="fb29" class="nq kx iq on b gy or os l ot ou">optuna.visualization.plot_contour(study)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/78193cb497bc1148832b0ed576e5e79a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4KqEP6FE_lNHX7aC5YWHWg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者的等高线图图像</p></figure><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="bd86" class="nq kx iq on b gy or os l ot ou">optuna.visualization.plot_param_importances(study)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oy"><img src="../Images/f13812b5d4f00251691db1c6c264b3a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eAzZoG6fLMQZHjoxus5u8A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者绘制的超参数重要性图</p></figure><p id="35f2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们可以看到，影响模型性能的主要超参数是 max_depth。在等高线图中，随着最大深度的增加，无论 max_samples 如何，性能都会提高。</p><h2 id="0ae2" class="nq kx iq bd ky nr ns dn lc nt nu dp lg lx nv nw li mb nx ny lk mf nz oa lm ob bi translated">实施修剪</h2><p id="da64" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在机器学习模型不产生任何中间结果的许多情况下，修剪实现不是很实际。但是，对于存在中间值的情况，如神经网络，修剪是减少计算时间的一种很好的技术。</p><p id="1b64" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">具有中间结果的模型的一个例子是随机梯度下降。</p><p id="d0dd" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">首先，我们初始化一个目标函数。</p><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="0160" class="nq kx iq on b gy or os l ot ou">import optuna<br/>from optuna.pruners import SuccessiveHalvingPruner<br/>from optuna.samplers import TPESampler<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.datasets import load_iris<br/>from sklearn.linear_model import SGDClassifier</span><span id="b2bc" class="nq kx iq on b gy ov os l ot ou">def objective(trial):</span><span id="d291" class="nq kx iq on b gy ov os l ot ou"># Loading the data set and splitting into train, test sets<br/>    iris = load_iris()<br/>    classes = list(set(iris.target))<br/>    train_x, valid_x, train_y, valid_y = train_test_split(<br/>        iris.data, iris.target, test_size=0.25<br/>    )</span><span id="61e6" class="nq kx iq on b gy ov os l ot ou">    # Prompting Optuna to suggest a hyperparameter value<br/>    alpha = trial.suggest_float("alpha", 1e-5, 1e-1, log=True)</span><span id="c26f" class="nq kx iq on b gy ov os l ot ou">    sgd = SGDClassifier(alpha = alpha, random_state = 42)</span><span id="c209" class="nq kx iq on b gy ov os l ot ou">    # Report the intermediate score for every step<br/>    for step in range(100):<br/>        sgd.partial_fit(train_x, train_y, classes=classes)</span><span id="38a4" class="nq kx iq on b gy ov os l ot ou">        # Report the intermediate objective value.<br/>        intermediate_value = sgd.score(valid_x, valid_y)<br/>        trial.report(intermediate_value, step)</span><span id="b4fc" class="nq kx iq on b gy ov os l ot ou">        # Prune the intermediate value if neccessary.<br/>        if trial.should_prune():<br/>            raise optuna.TrialPruned()</span><span id="878b" class="nq kx iq on b gy ov os l ot ou">    return sgd.score(valid_x, valid_y)</span></pre><p id="c974" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">之后，我们可以使用 Optuna 创建一个研究，并优化目标函数。</p><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="1a5c" class="nq kx iq on b gy or os l ot ou">study = optuna.create_study(sampler = TPESampler(), <br/>                            pruner = SuccessiveHalvingPruner(),<br/>                            direction= "maximize")</span><span id="6946" class="nq kx iq on b gy ov os l ot ou">study.optimize(objective, n_trials = 20)<br/></span><span id="3660" class="nq kx iq on b gy ov os l ot ou">pruned_trials = study.get_trials(states=[optuna.trial.TrialState.PRUNED])<br/>complete_trials = study.get_trials(states=[optuna.trial.TrialState.COMPLETE])<br/>print("# Pruned trials: ", len(pruned_trials))<br/>print("# Complete trials: ", len(complete_trials))</span><span id="273c" class="nq kx iq on b gy ov os l ot ou">trial = study.best_trial<br/>print("Best Score: ", trial.value)<br/>print("Best Params: ")<br/>for key, value in trial.params.items():<br/>    print("  {}: {}".format(key, value))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/2528d85620b9f42305381148563213ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*XPtk3mcKJSud9NwmJHKl4Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者输出的代码</p></figure><p id="c367" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们将看到 Optuna 提供的另外两个图。这些图可以帮助我们检查优化历史，并查看哪些试验被删除。</p><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="f768" class="nq kx iq on b gy or os l ot ou">optuna.visualization.plot_optimization_history(study)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/94e01ceeada03fc744a32dbcf8ec17e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K9z0ra3jN1TIIiAbKHtBOA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者的优化历史图</p></figure><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="28f0" class="nq kx iq on b gy or os l ot ou">optuna.visualization.plot_intermediate_values(study)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/9e20a436830fa3162371e28933798f33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DCx1bXEAEh1qnR3dGOrUcg.png"/></div></div></figure><p id="20c6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们可以看到剪枝器是如何通过停止具有不良中间值的不需要的超参数并专注于训练更好的超参数来工作的。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="617a" class="kw kx iq bd ky kz mw lb lc ld mx lf lg jw my jx li jz mz ka lk kc na kd lm ln bi translated">外卖食品</h1><p id="02c5" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">Optuna 允许我们实现最先进的优化算法，以加快机器学习管道中必不可少的超参数调整过程。</p><p id="1749" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然而，我们必须明白，当迭代模型训练过程的成本过于昂贵时，这些高级算法是为了有效地搜索最佳目标而构建的。</p><ul class=""><li id="3181" class="nc nd iq lq b lr mk lu ml lx ne mb nf mf ng mj nh ni nj nk bi translated">如果我们只使用少量数据进行训练，同时使用不太复杂的模型，GridSearch 和 RandomSearch 就有可能在速度和性能方面击败这些算法。</li><li id="74ca" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj nh ni nj nk bi translated">也就是说，随着数据量越来越大，模型越来越复杂，在没有任何信息的情况下随机训练一组超参数的成本会大大增加，这些先进的算法大大优于传统方法。</li></ul><p id="318b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">非常感谢您花时间阅读这篇文章。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pc"><img src="../Images/cb82683adae4e73a4cee3b7285c98e33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*R-ebFpW47sL72RZH"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">刘汉宁·奈巴霍在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="b19a" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">参考</h1><p id="8c94" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated"><strong class="lq ir">【1】</strong>秋叶 t、佐野 s、柳濑 t、太田 t、小山 m。<a class="ae kv" href="https://arxiv.org/abs/1907.10902" rel="noopener ugc nofollow" target="_blank"> Optuna:下一代超参数优化框架</a> (2019) <em class="od">，arXiv </em>。</p><p id="63f1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">【2】</strong>l .李，k .贾米森，a .罗斯塔米扎德，e .戈尼娜，m .哈特，b .雷希特，a .塔尔瓦尔卡尔。<a class="ae kv" href="https://arxiv.org/abs/1810.05934" rel="noopener ugc nofollow" target="_blank">大规模并行超参数调优系统</a> (2018) <em class="od">，arXiv </em>。</p><p id="d3aa" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">j .伯格斯特拉，r .巴登内，y .本吉奥和 b .凯格尔。<a class="ae kv" href="https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf" rel="noopener ugc nofollow" target="_blank">超参数优化算法</a> (2011)，《第 24 届国际神经信息处理系统会议录》(NIPS'11)。美国纽约州红钩市柯伦联合有限公司，邮编:2546–2554。</p><p id="16d9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">【4】</strong>k . Deb，A. Pratap，S. Agarwal 和 T. Meyarivan，<a class="ae kv" href="https://ieeexplore.ieee.org/document/996017" rel="noopener ugc nofollow" target="_blank">一种快速的精英多目标遗传算法:NSGA-II </a> (2002)，<em class="od"> IEEE 进化计算汇刊</em>，第 6 卷，第 2 期，第 182–197 页。doi: 10.1109/4235.996017。</p></div></div>    
</body>
</html>