<html>
<head>
<title>Automatic word relatedness evaluation | WordNet, SpaCy &amp; Wikidata API</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自动单词相关度评估| WordNet、SpaCy和Wikidata API</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/automatic-word-relatedness-evaluation-wordnet-spacy-wikidata-api-9fd1c72ac73c#2022-08-17">https://towardsdatascience.com/automatic-word-relatedness-evaluation-wordnet-spacy-wikidata-api-9fd1c72ac73c#2022-08-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="e533" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl"> ⚠️在本文中，我将依靠赫斯特模式和用法的先验知识进行上位词提取。然而，我将使用专利的数据集，所以我建议检查我在这个周期的前一篇文章。</em></p><div class="km kn gp gr ko kp"><a rel="noopener follow" target="_blank" href="/improving-the-ner-model-with-patent-texts-spacy-prodigy-and-a-bit-of-magic-44c86282ea99"><div class="kq ab fo"><div class="kr ab ks cl cj kt"><h2 class="bd ir gy z fp ku fr fs kv fu fw ip bi translated">用专利文本改进NER模型| SpaCy、Prodigy和一点魔法🪄</h2><div class="kw l"><h3 class="bd b gy z fp ku fr fs kv fu fw dk translated">本文提出了一种利用专利文本训练特定领域NER模型的方法。</h3></div><div class="kx l"><p class="bd b dl z fp ku fr fs kv fu fw dk translated">towardsdatascience.com</p></div></div><div class="ky l"><div class="kz l la lb lc ky ld le kp"/></div></div></a></div><div class="km kn gp gr ko kp"><a rel="noopener follow" target="_blank" href="/implementing-hearst-patterns-with-spacy-216e585f61f8"><div class="kq ab fo"><div class="kr ab ks cl cj kt"><h2 class="bd ir gy z fp ku fr fs kv fu fw ip bi translated">用SpaCy实现Hearst模式</h2><div class="kw l"><h3 class="bd b gy z fp ku fr fs kv fu fw dk translated">上下位关系的自动提取</h3></div><div class="kx l"><p class="bd b dl z fp ku fr fs kv fu fw dk translated">towardsdatascience.com</p></div></div><div class="ky l"><div class="lf l la lb lc ky ld le kp"/></div></div></a></div></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="001e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi ln translated"><span class="l lo lp lq bm lr ls lt lu lv di"> W </span>我们为什么要关心单词的相关性？那么，让我们从什么是关联性及其在NLP中的应用开始。</p><p id="f50e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">拓扑结构上下文中的相关度可以通过具有“父”单词和“子”单词来解释。到目前为止，这些术语被称为:</p><p id="69b9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">上位词</em></strong><em class="kl">——…一个具有广泛意义的词，构成了一个具有更具体意义的词所属的范畴；</em></p><p id="e8a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">下义</em></strong><em class="kl">——……是反向的意思；比适用于它的一般术语有更具体含义的词。</em></p><p id="b0aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了便于理解，这里有一个例子:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi lw"><img src="../Images/50cc5eab3f03d73294d1ada64268d419.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k0ssn0dKUBO55SQqTf92zw.png"/></div></div></figure><p id="263e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这里，“光盘”和“硬盘”是“存储单元”的<strong class="jp ir">的下位词</strong>。反过来，“存储单元”是“光盘”和“硬盘”的<strong class="jp ir">上位词</strong>。</p><p id="8689" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简而言之，这样的词汇关系是NLP任务的基本构件。与NER(命名实体识别)一起，这种关系可以应用于各种任务，例如<strong class="jp ir">分类预测、信息提取(IE) </strong>和简单的<strong class="jp ir">数据集创建</strong>(高级模型的示例)。</p><p id="b1d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你会在我之前的文章中找到更多关于上位词以及如何提取它们的信息。在这里，我将集中使用几种工具来评估单词之间的关系类型。它不仅可以用于检查上位词对，还可以用于评估词对关系的任何其他应用。</p><h1 id="1e00" class="mi mj iq bd mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf bi translated">数据</h1><p id="41a9" class="pw-post-body-paragraph jn jo iq jp b jq ng js jt ju nh jw jx jy ni ka kb kc nj ke kf kg nk ki kj kk ij bi translated">作为一个例子，我将使用专利的<strong class="jp ir">G06K</strong>T30【数据识别/数据呈现】小节中的专利文本。最重要的是，我训练了一个定制的NER模型来识别技术术语。<strong class="jp ir">我在我的</strong> <a class="ae mh" href="https://medium.com/towards-data-science/improving-the-ner-model-with-patent-texts-spacy-prodigy-and-a-bit-of-magic-44c86282ea99" rel="noopener"> <strong class="jp ir">上一篇</strong> </a> <strong class="jp ir">中详细描述了这个数据集。</strong></p><blockquote class="nl nm nn"><p id="0131" class="jn jo kl jp b jq jr js jt ju jv jw jx no jz ka kb np kd ke kf nq kh ki kj kk ij bi translated">⚠️数据不受版权保护，可安全用于商业目的。 <strong class="jp ir"> <em class="iq">根据</em></strong><a class="ae mh" href="http://www.uspto.gov/news/media/ccpubguide.jsp" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir"><em class="iq">USPTO</em></strong></a><strong class="jp ir"><em class="iq">:</em></strong><em class="iq">受37 CFR 1.71(d) &amp; (e)和1.84(s)</em><strong class="jp ir"><em class="iq">中反映的有限例外限制，专利的文本和图纸通常不受版权限制</em> </strong> <em class="iq">。</em></p></blockquote><h1 id="e937" class="mi mj iq bd mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf bi translated">用于评估的上位词对</h1><p id="0cf2" class="pw-post-body-paragraph jn jo iq jp b jq ng js jt ju nh jw jx jy ni ka kb kc nj ke kf kg nk ki kj kk ij bi translated">为了实现，我们将重用已经提取的上位词对。你可以从<a class="ae mh" href="https://github.com/kinivi/patent_ner_linking" rel="noopener ugc nofollow" target="_blank">项目的repo </a>中下载这个文件。这里有几个例子。</p><figure class="lx ly lz ma gt mb"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="4b25" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">“关系”和“标签”行表示连接的方向。“1”或“hyper”意味着word1是word2的更广泛的含义(上位词)。</p><h1 id="6fde" class="mi mj iq bd mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf bi translated">方法1 | WordNet相似度</h1><p id="bb3d" class="pw-post-body-paragraph jn jo iq jp b jq ng js jt ju nh jw jx jy ni ka kb kc nj ke kf kg nk ki kj kk ij bi translated">第一种方法基于WordNet数据库，以距离的形式比较单词之间的相似性。尽管这个分数不能给出一个精确的答案，如果对是上下位关系，它仍然是有价值的简单和快速评估提取的数据。</p><p id="f13c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">实现非常简单；你需要安装nltk包并下载WordNet到里面。</p><figure class="lx ly lz ma gt mb"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="05a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，为每个单词创建一个名为<em class="kl"> synset </em>的对象包装器，并运行它们之间的相似度计算。Synset是“一组认知同义词，每个同义词表达一个不同的概念”。有时单词可以根据上下文表示不同的意思，因此<em class="kl"> synset </em>提供了一组这些意思以及到其他单词的链接。</p><p id="6056" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面是一个关于“光盘”和“计算机”与<strong class="jp ir">吴-帕尔默相似性</strong>的演示。这个分数表示两个词义的相似程度，基于这两个词义在分类法中的深度。正是我们需要的！</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nt"><img src="../Images/27e420acc48ef1998cb376c401cb99bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*95CF3Os-B3ky3FOV4wcxtw.png"/></div></div><p class="nu nv gj gh gi nw nx bd b be z dk translated">然后，为每个单词创建一个名为synset的对象包装器，并</p></figure><p id="9ea1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于使用了WordNet，这种方法可以修改，并在基于单词分类法的任务中有很大的潜力。然而，这种方法的一个很大的缺点是很多单词不能在字典中找到。这尤其与术语和缩写有关。它们不会被nltk识别，因此相似度无法计算。</p><h1 id="a87a" class="mi mj iq bd mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf bi translated">方法2 |空间</h1><p id="ae78" class="pw-post-body-paragraph jn jo iq jp b jq ng js jt ju nh jw jx jy ni ka kb kc nj ke kf kg nk ki kj kk ij bi translated">另一种直接的方法是利用深度学习NLP模型的单词间嵌入距离。目前最简单的方法是使用<a class="ae mh" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank">空间</a>。有了这个库，你可以利用一个已经预先训练好的模型或者训练定制模型。在前面的步骤中，我们已经为模式数据集中的命名实体识别(NER)调优了一个定制模型。<strong class="jp ir">推荐使用微调模型，因为它在相似性评估时会给出更好的准确性</strong>。</p><p id="b721" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要计算相似度，只需对目标词加载模型运行<em class="kl">相似度()</em>函数:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi ny"><img src="../Images/83b91bb60e6afc9f6632825e515948c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0KXjXdnrxnkheZOZer9VdA.png"/></div></div><p class="nu nv gj gh gi nw nx bd b be z dk translated">利用空间和定制训练模型计算单词间的相似度</p></figure><p id="927d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">或者，下面是使用默认模型的方法:</p><figure class="lx ly lz ma gt mb"><div class="bz fp l di"><div class="nr ns l"/></div><p class="nu nv gj gh gi nw nx bd b be z dk translated">与默认空间模型的计算相似性</p></figure><h1 id="418e" class="mi mj iq bd mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf bi translated">方法3 |维基数据API</h1><p id="f7c3" class="pw-post-body-paragraph jn jo iq jp b jq ng js jt ju nh jw jx jy ni ka kb kc nj ke kf kg nk ki kj kk ij bi translated">最新的、最伟大的、老实说也是最慢的方法——<a class="ae mh" href="https://www.wikidata.org/wiki/Wikidata:Main_Page\" rel="noopener ugc nofollow" target="_blank">Wikidata</a>。</p><p id="2c50" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Wikidata <em class="kl">“充当其Wikimedia姊妹项目(包括Wikipedia、Wikivoyage、Wiktionary、Wikisource等)的</em> <strong class="jp ir"> <em class="kl">结构化数据</em> </strong> <em class="kl">的中央存储。”</em>它具有基于图形的结构和开放的API来访问数据。与WordNet相比，Wikidata包含大量经常更新的信息。那些<strong class="jp ir">，</strong>我们确实会发现大部分新旧技术术语和缩略语。</p><p id="a886" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可以在这里玩<a class="ae mh" href="https://query.wikidata.org" rel="noopener ugc nofollow" target="_blank"/>来搜索图表中的数据，并可视化实例的属性。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nz"><img src="../Images/20cdc99e7c07433f88c0cf3fcf92bbac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oToCqzT1lwdheLWtz3JMlA.png"/></div></div><p class="nu nv gj gh gi nw nx bd b be z dk translated">作者提供的word " <strong class="bd oa">计算机</strong> " |图像属性的图形可视化</p></figure><p id="c598" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">具体来说，我们对属性<strong class="jp ir">p268</strong>(的<em class="kl">子类)感兴趣，它在图结构中具有“父单词”的ID。</em></p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi ob"><img src="../Images/74def20d7d3f806460488ca96cdadcef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gVdh9W3DxEbYVQNfh9eJcQ.png"/></div></div><p class="nu nv gj gh gi nw nx bd b be z dk translated">Web界面上的必要字段|作者提供的图片</p></figure><p id="82a9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要在Python中连接和使用这个API，安装<a class="ae mh" href="https://qwikidata.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> qwikidata </a>和<a class="ae mh" href="https://github.com/siznax/wptools" rel="noopener ugc nofollow" target="_blank"> wptools </a>。</p><p id="106b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在导入libs之后，我们需要获得单词对中两个单词的id。下面是一个“iPad:和母词“电脑”的例子。通过利用<em class="kl"> get_parse() </em>，我们搜索并解析该术语的页面，然后从<em class="kl">【wikibase】</em>字段中提取ID。</p><figure class="lx ly lz ma gt mb"><div class="bz fp l di"><div class="nr ns l"/></div><p class="nu nv gj gh gi nw nx bd b be z dk translated">用于检查维基数据中单词子类的Python脚本</p></figure><p id="1488" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，我们通过使用<em class="kl">get _ subclass _ of _ item()</em>和<strong class="jp ir">查找父类的所有可能子类的ID，简单地检查子类的ID是否在这个列表中</strong>。</p><h1 id="6dd0" class="mi mj iq bd mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf bi translated">最终结果|所有方法一起</h1><p id="cf89" class="pw-post-body-paragraph jn jo iq jp b jq ng js jt ju nh jw jx jy ni ka kb kc nj ke kf kg nk ki kj kk ij bi translated">综上所述，最好的方法是什么？没有明确的答案，下面是原因。</p><p id="6c52" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们在144个提取对的整个列表上运行了所有三种方法，并创建了一个表来比较结果。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nz"><img src="../Images/d59c9325a0f4b385893dd40b59a1796f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7U6bB6WoErmkFIZwVkvV_w.png"/></div></div><p class="nu nv gj gh gi nw nx bd b be z dk translated">最终结果表</p></figure><p id="afb5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">总的来说，每个方法的输出被转换成3类:<strong class="jp ir">相关、不相关和错误</strong>。对于距离测量的方法，我们根据经验选择阈值。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi oc"><img src="../Images/35c3f0e539b8f66ca99081dcef3523c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-2pSPcEWYSGV2dKt6CRTYQ.png"/></div></div><p class="nu nv gj gh gi nw nx bd b be z dk translated">评分系统|图片由作者提供</p></figure><p id="be5a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">完整的表和Python函数来创建这样的结果，您可以在<a class="ae mh" href="https://github.com/kinivi/patent_ner_linking/blob/main/project.ipynb" rel="noopener ugc nofollow" target="_blank">项目报告</a>中找到。</p><p id="c50a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以，关于得到的结果。已经可以看出，WordNet大部分时间都是给出错误的。为什么？这就是我们所说的WordNet字典的局限性。WordNet不太可能包含专利中的缩写或高度特定的技术术语。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nz"><img src="../Images/bee6c9e1bc296e5b1aa13904bf910a70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JjdAa273otYa_-xpHn51wQ.png"/></div></div><p class="nu nv gj gh gi nw nx bd b be z dk translated">按最高空间分数排序的最佳结果</p></figure><p id="f4b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们来看看空间分数最高的一对。很明显“HSV颜色空间”是“颜色空间”的子类。这里空间相似性工作得最好。基本上，它只是由类似的话触发，没有什么特别的，但它的工作。</p><p id="4789" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一方面，这并不是一种持续的行为。看看下面的例子。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nz"><img src="../Images/a16d25d429c912b62da0405ab584ed86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gfud5d3DQZW03-FoPX_Oww.png"/></div></div><p class="nu nv gj gh gi nw nx bd b be z dk translated">按最低空间分数排序的最佳结果</p></figure><p id="439c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">按照预期，“显示设备”和“液晶<strong class="jp ir">显示器</strong>应该有相当大的相似度得分，但是没有。然而，Wikidata API有望在这种情况下工作，并提供正确的结果。</p><p id="0375" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你已经注意到了WordNet的一个常量⚠️(错误)。但不代表没用。这就是证据:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi od"><img src="../Images/be354939637c4434268ea7a9f8a96c2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZeAxKxbprFzyhhnesyEfEA.png"/></div></div><p class="nu nv gj gh gi nw nx bd b be z dk translated">WordNet有用性的例子</p></figure><p id="6b65" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">“手机”明明是“电子设备”；它对维基数据没有影响，仍然给空间相似性一个很小的分数。</p><p id="89a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它显示了什么？任何问题都没有灵丹妙药。然而，利用几种不同方法的复杂方法可能是一种防弹解决方案。</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="2448" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个解决方案会更好吗？当然啦！这里有一些想法和可能的改进</p><ul class=""><li id="66d6" class="oe of iq jp b jq jr ju jv jy og kc oh kg oi kk oj ok ol om bi translated">🔪<strong class="jp ir">利用WordNet的n-grams</strong>。像<strong class="jp ir">图像处理单元</strong> - &gt;图像处理单元。拆分单词以查找字典中的相似类别</li><li id="b771" class="oe of iq jp b jq on ju oo jy op kc oq kg or kk oj ok ol om bi translated">🪞尝试同义词</li><li id="1444" class="oe of iq jp b jq on ju oo jy op kc oq kg or kk oj ok ol om bi translated">🛠 <strong class="jp ir">改进了维基数据的Graph-Q查询。</strong>老实说，获取所有子类既慢又不可靠。最好创建一个自定义查询来检查特定的ID并将其传递给API。</li></ul></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="46e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">完整的代码和解析的数据可以在这里找到:</p><div class="km kn gp gr ko kp"><a href="https://github.com/kinivi/patent_ner_linking" rel="noopener  ugc nofollow" target="_blank"><div class="kq ab fo"><div class="kr ab ks cl cj kt"><h2 class="bd ir gy z fp ku fr fs kv fu fw ip bi translated">GitHub-kini VI/patent _ ner _ linking:📰命名实体识别(NER)和实体链接(EL)</h2><div class="kw l"><h3 class="bd b gy z fp ku fr fs kv fu fw dk translated">这个项目的主要目标是:用特定领域的专利数据集训练NER模型，用</h3></div><div class="kx l"><p class="bd b dl z fp ku fr fs kv fu fw dk translated">github.com</p></div></div><div class="ky l"><div class="os l la lb lc ky ld le kp"/></div></div></a></div></div></div>    
</body>
</html>