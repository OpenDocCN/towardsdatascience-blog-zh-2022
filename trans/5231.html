<html>
<head>
<title>Ace your Machine Learning Interview — Part 7</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">赢得机器学习面试——第七部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ace-your-machine-learning-interview-part-7-2688de34805f#2022-11-22">https://towardsdatascience.com/ace-your-machine-learning-interview-part-7-2688de34805f#2022-11-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/c24176e742cdc0990e8ce0c4891a348c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WAN-ENq8OoVe9Vb1"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae jd" href="https://unsplash.com/@airfocus?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">空中聚焦</a>拍摄的照片</p></figure><div class=""/><div class=""><h2 id="7423" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">使用 Python 研究硬投票分类器的集成学习</h2></div><p id="23f2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们来到了我的<em class="lr">“Ace your Machine Learning Interview”</em>系列的第七篇文章，在这篇文章中，我回顾了机器学习的一些基础知识，希望对你面对下一次面试有所帮助！😁</p><p id="e47a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果您对本系列之前的文章感兴趣，我在这里留下了链接:</p><ol class=""><li id="880d" class="ls lt jg kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/ace-your-machine-learning-interview-part-1-e6a5897e6844"> <em class="lr"> Ace your Machine Learning 面试—第一部分</em> </a> <em class="lr">:深入线性、套索和岭回归及其假设</em></li><li id="6112" class="ls lt jg kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/ace-your-machine-learning-interview-part-2-c58526b5faba"><em class="lr">Ace your Machine Learning 访谈—第二部分</em> </a> <em class="lr">:使用 Python 深入研究分类问题的逻辑回归</em></li><li id="1145" class="ls lt jg kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/ace-your-machine-learning-interview-part-3-af432f922aa7"><em class="lr">Ace your Machine Learning 面试—第三部分</em> </a> <em class="lr">:使用 Python 深入研究朴素贝叶斯分类器</em></li><li id="6b59" class="ls lt jg kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated"><a class="ae jd" href="https://medium.com/towards-data-science/ace-your-machine-learning-interview-part-4-e30b695ce63" rel="noopener"><em class="lr">Ace your Machine Learning 访谈—第四部分</em> </a> <em class="lr">:深入研究使用 Python 的支持向量机</em></li><li id="ad71" class="ls lt jg kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated"><a class="ae jd" href="https://medium.com/towards-data-science/ace-your-machine-learning-interview-part-5-3de48703cd65" rel="noopener"><em class="lr">Ace your Machine Learning 访谈—第五部分</em> </a> <em class="lr">:使用 Python 深入研究内核支持向量机</em></li><li id="8a52" class="ls lt jg kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated"><a class="ae jd" href="https://medium.com/towards-data-science/ace-your-machine-learning-interview-part-6-5f0d84e435a1" rel="noopener"><em class="lr">Ace your Machine Learning 面试—第六部分</em> </a> <em class="lr"> : </em>使用 Python 深入决策树</li></ol><h2 id="44a6" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated"><strong class="ak">简介</strong></h2><p id="5f78" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">集成学习是一种允许你一起使用多个预测器、分类器或回归器的方法。通过这种方式，您可以基于各种机器学习模型的知识创建一个更健壮的系统。</p><p id="6014" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你仔细想想，当我们对某事犹豫不决时，这是我们人类通常会做的事情。我记得《生活大爆炸》系列中有一集，谢尔顿还没有决定是买 Xbox 还是 PlayStation。然后，他询问朋友的意见，希望得到大多数人的支持，并据此做出决定。我把视频留在这里给你！</p><figure class="ne nf ng nh gt is"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="6b1f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Ensembling 方法做的是同样的事情。如果必须对一个点进行分类，它会征求几个分类者的意见，并采用大多数结果。(多数的使用并不总是如此，但却是最常见的)</p><h2 id="7dfe" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">通用集成学习</h2><p id="5569" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">所以我们已经知道了如何构造一个集合方法。例如，我们训练几个分类器，然后采用多数投票来分类一个新实例。</p><p id="8539" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下图很好地总结了这一点。</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/f7bb9cff2bf4bbf1f371dd8f17610ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rlQcZzp6wFcvpNIkuJfXxw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">集成学习架构(图片由作者提供)</p></figure><p id="0723" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从技术上来说，当我们处理二元分类时，我们说我们在使用<strong class="kx jh">多数表决</strong>。另一方面，当我们做多类分类时，我们说<strong class="kx jh">多数投票</strong>。基本上你实际做的是使用<strong class="kx jh">模式函数，它在统计中从列表中取出最频繁的元素。</strong></p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/4ba73db4c7f911cc93bc9651360a78f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*zcRLvdEPfNMMxBgIPZkOuw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">寻找多数的模式(作者图片)</p></figure><p id="10c2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是这些 C1，C2，…，Cn 量词是什么样子的呢？他们可以是你喜欢的分类器，重要的是，你然后分组，并在一个函数中使用他们的预测，比如多数投票。例如，我们可以使用逻辑回归、SVM 和决策树。</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nm"><img src="../Images/80232522e325627759d6ed56f778dbc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5O0lpsfBGxS1s5Ow-SBxSg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">整体方法示例(图片由作者提供)</p></figure><p id="34d7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是，除了在集成方法中使用所有不同的算法，<strong class="kx jh">我们可以总是使用相同的算法吗？答案是肯定的！</strong></p><h2 id="6628" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">装袋、粘贴和随机森林</h2><p id="9f5a" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">到目前为止，我们总是使用我们的整个数据集，并用它来训练不同的算法，然后我们将这些算法组合成一个集成方法。</p><p id="6b8d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以做的另一件事是始终使用相同的算法，例如我们可以使用三个支持向量机，但是每个 SVM 将在初始数据集的不同子集上绘制。 <strong class="kx jh">子集可以是数据集中所有记录的子集，也可以是特征</strong>(保留所有记录)的子集。最后，我们像往常一样对预测进行多数投票。</p><p id="b59a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我们有 900 条记录的训练集。我们可以从 300 条记录中随机抽取一个子集用于每个 SVM。请注意，如果我们对这个子集进行重复，我们将使用一种叫做<strong class="kx jh">打包</strong>的方法。否则，不重复，称为<strong class="kx jh">粘贴</strong>。</p><p id="75de" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">您曾经使用过或见过使用基于装袋或粘贴的<strong class="kx jh">组装方法吗？我打赌你有。随机森林</strong>！</p><p id="0a9b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们在上一篇文章中提到，通常在集成方法中使用决策树(DT)算法来提高其鲁棒性。而只由一棵决策树组成的<strong class="kx jh">ense bleng 被称为随机森林</strong>，因为它是一种广泛使用的算法，性能非常好。</p><p id="6ce7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们已经提到，DT 总是试图在它找到的最佳特性上进行分割。另一方面，一个<strong class="kx jh">随机森林</strong>，<strong class="kx jh">在众多 DT 中实例化一个时，只给它分配特征</strong>的一个子集。因此，DT 必须只在该特征子集内搜索最佳分割。</p><h2 id="5aa7" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated"><strong class="ak">多余的树</strong></h2><p id="4e61" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">有一个随机森林的实现，允许更快的训练，但增加了噪声。在这种情况下，DT 的每个特征除了从特征的随机子集中提取之外，还将在<strong class="kx jh">随机施加的阈值</strong>上进行分割。因此，它们非常快，我们可以同时实例化多个 DTs。额外的树以更大的偏差换取更低的方差，训练起来更快。</p><h2 id="5ac1" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">我们来编码吧！</h2><p id="bcfc" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">让我们首先来看看 3 种不同算法在 Iris 数据集上的结果。数据集由 sklearn 在开放许可下提供，可以在这里找到<a class="ae jd" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#" rel="noopener ugc nofollow" target="_blank"/>。数据集如下。</p><figure class="ne nf ng nh gt is"><div class="bz fp l di"><div class="nn nj l"/></div></figure><p id="6215" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将使用这三种算法:<em class="lr">逻辑回归</em>、<em class="lr">决策树分类器</em>和<em class="lr">近邻分类器</em>。</p><p id="c299" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">此外，对于第一次和最后一次，我们将使用标准定标器进行一些数据处理。为了方便起见，我们将算法和定标器放在一个 sklearn 管道中。</p><figure class="ne nf ng nh gt is"><div class="bz fp l di"><div class="nn nj l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">输入数据</p></figure><p id="b649" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">每个算法和管道将使用<strong class="kx jh">交叉折叠验证</strong>进行 10 次评估。</p><figure class="ne nf ng nh gt is"><div class="bz fp l di"><div class="nn nj l"/></div></figure><p id="d1c9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我在下面列出了我的算法:</p><ul class=""><li id="fccc" class="ls lt jg kx b ky kz lb lc le lu li lv lm lw lq no ly lz ma bi translated"><strong class="kx jh"> <em class="lr"> Logistic 回归</em></strong><em class="lr">:ROC AUC:0.92-STD:0.15</em></li><li id="2dfb" class="ls lt jg kx b ky mb lb mc le md li me lm mf lq no ly lz ma bi translated"><strong class="kx jh"> <em class="lr">决策树</em></strong><em class="lr">:ROC AUC:0.87 STD:0.18</em></li><li id="0391" class="ls lt jg kx b ky mb lb mc le md li me lm mf lq no ly lz ma bi translated"><strong class="kx jh"><em class="lr">KNN</em></strong><em class="lr">:ROC AUC:0.85 STD:0.13</em></li></ul><p id="e688" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们看看我们能否在多数投票制下做得更好。我们首先用下面的方式<strong class="kx jh">定义一个名为 MajorityVotingClassifier </strong>的类。我们只需要<strong class="kx jh">继承 BaseEstimator 和 ClassifierMixin </strong>这两个类，就可以继承常见的估计器功能，比如 get/set_params 函数。</p><figure class="ne nf ng nh gt is"><div class="bz fp l di"><div class="nn nj l"/></div></figure><p id="3df5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们只需要实例化一个新的 MajorityVotingClassifier 类型的分类器，并将其添加到我们已经比较过的分类器列表中。</p><figure class="ne nf ng nh gt is"><div class="bz fp l di"><div class="nn nj l"/></div></figure><p id="f305" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们看看取得的成果。你的可能会和我的略有不同。</p><ul class=""><li id="6997" class="ls lt jg kx b ky kz lb lc le lu li lv lm lw lq no ly lz ma bi translated"><strong class="kx jh"> <em class="lr"> Logistic 回归</em></strong><em class="lr">:ROC AUC:0.92 STD:0.15</em></li><li id="bc81" class="ls lt jg kx b ky mb lb mc le md li me lm mf lq no ly lz ma bi translated"><strong class="kx jh"> <em class="lr">决策树</em></strong><em class="lr">:ROC AUC:0.87 STD:0.18</em></li><li id="45e3" class="ls lt jg kx b ky mb lb mc le md li me lm mf lq no ly lz ma bi translated"><strong class="kx jh"><em class="lr">KNN</em></strong><em class="lr">:ROC AUC:0.85 STD:0.13</em></li><li id="c54e" class="ls lt jg kx b ky mb lb mc le md li me lm mf lq no ly lz ma bi translated"><strong class="kx jh"> <em class="lr">多数投票</em></strong><em class="lr">:ROC AUC 0.98 STD:0.05</em></li></ul><p id="6641" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你可以看到，这一次多数投票成功地超过了单个分类器几个百分点。</p><p id="3598" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，通过利用较弱的分类器，我们能够创建一个达到 98%分数的分类器！</p><h1 id="4080" class="np mh jg bd mi nq nr ns ml nt nu nv mo km nw kn mr kp nx kq mu ks ny kt mx nz bi translated">最后的想法</h1><p id="0d77" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">合奏法通常用于像 Kaggle 或类似的比赛，有时可以让你赢得额外的几个百分点。另一方面，<strong class="kx jh">它在工业中并不总是有用的，因为通常不值得为了性能上的小改进而训练多分类器，所以人们必须有意识地使用这种方法。</strong></p><p id="eb0e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我自己用它在名为<a class="ae jd" href="https://www.evalita.it/campaigns/evalita-2020/best-system-award/" rel="noopener ugc nofollow" target="_blank"><strong class="kx jh">evalita 2020</strong></a>的 NLP 竞赛中赢得了<strong class="kx jh">最佳系统奖，在该竞赛中，我基于集成技术创建了一个分类器。你可以在这里阅读更多关于我开发的做<a class="ae jd" href="https://pdfs.semanticscholar.org/c248/cc7cfe59e27f4e14e53840a9dcd29febafaf.pdf" rel="noopener ugc nofollow" target="_blank">姿态检测的系统。😁</a></strong></p><h1 id="89df" class="np mh jg bd mi nq nr ns ml nt nu nv mo km nw kn mr kp nx kq mu ks ny kt mx nz bi translated">结束了</h1><p id="39cd" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated"><em class="lr">马赛洛·波利蒂</em></p><p id="5dca" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" href="https://www.linkedin.com/in/marcello-politi/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>，<a class="ae jd" href="https://twitter.com/_March08_" rel="noopener ugc nofollow" target="_blank"> Twitter </a>，<a class="ae jd" href="https://march-08.github.io/digital-cv/" rel="noopener ugc nofollow" target="_blank"> CV </a></p></div></div>    
</body>
</html>