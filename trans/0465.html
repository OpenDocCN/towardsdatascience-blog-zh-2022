<html>
<head>
<title>Uncertainty in Deep Learning — Epistemic Uncertainty and Bayes by Backprop</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的不确定性——认知不确定性和反向投影贝叶斯</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/uncertainty-in-deep-learning-epistemic-uncertainty-and-bayes-by-backprop-e6353eeadebb#2022-02-18">https://towardsdatascience.com/uncertainty-in-deep-learning-epistemic-uncertainty-and-bayes-by-backprop-e6353eeadebb#2022-02-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><blockquote class="jn"><p id="e623" class="jo jp iq bd jq jr js jt ju jv jw jx dk translated">知识是在不确定性边缘的无止境的冒险。</p><p id="15c5" class="jo jp iq bd jq jr jy jz ka kb kc jx dk translated">雅各布·布朗诺夫斯基</p></blockquote><figure class="ke kf kg kh ki kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi kd"><img src="../Images/363c0b35d63f936da1cd98bb6cef844e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xva1QS0KVS3-NIig"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">由<a class="ae ku" href="https://unsplash.com/@kylejglenn?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">凯尔·格伦</a>在<a class="ae ku" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="e19c" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">这是深度学习<strong class="kx ir">不确定性系列<strong class="kx ir">第三部</strong>。</strong></p><ul class=""><li id="e98c" class="ls lt iq kx b ky kz lc ld lg lu lk lv lo lw jx lx ly lz ma bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-brief-introduction-1f9a5de3ae04">第1部分—简介</a></li><li id="22d8" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-aleatoric-uncertainty-and-maximum-likelihood-estimation-c7449ee13712">第2部分—随机不确定性和最大似然估计</a></li><li id="78ed" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated"><strong class="kx ir">第3部分——认知不确定性和反向投影贝叶斯</strong></li><li id="759d" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-bayesian-cnn-tensorflow-probability-758d7482bef6">第4部分——实现完全概率贝叶斯CNN </a></li><li id="a93d" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-experiments-with-bayesian-cnn-1ca37ddb6954">第五部分——贝叶斯CNN实验</a></li><li id="58c7" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated">第六部分——贝叶斯推理和变形金刚</li></ul><h1 id="b1bb" class="mg mh iq bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">介绍</h1><p id="f581" class="pw-post-body-paragraph kv kw iq kx b ky ne la lb lc nf le lf lg ng li lj lk nh lm ln lo ni lq lr jx ij bi translated">在本文中，我们将探讨如何使用张量流概率来表示认知不确定性，以及理解底层算法和理论背景。</p><p id="ffbd" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">文章组织如下:</p><ul class=""><li id="a431" class="ls lt iq kx b ky kz lc ld lg lu lk lv lo lw jx lx ly lz ma bi translated"><strong class="kx ir">什么是认知不确定性</strong></li><li id="1dfb" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated"><strong class="kx ir">普通神经网络的问题</strong></li><li id="a9e4" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated"><strong class="kx ir">贝叶斯神经网络</strong></li><li id="2375" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated">这个方案背后的数学原理</li><li id="daa5" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated"><strong class="kx ir">变分贝叶斯方法</strong></li><li id="047d" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated"><strong class="kx ir">贝叶斯神经网络中的反向传播</strong></li><li id="e91f" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated"><strong class="kx ir">迷你批次&amp;对KL背离重新加权</strong></li><li id="49a0" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated"><strong class="kx ir">使用张量流概率的重量不确定性</strong></li><li id="adf7" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated"><strong class="kx ir">结论&amp;下一步</strong></li></ul><h1 id="7c90" class="mg mh iq bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">什么是认知不确定性？</h1><p id="69af" class="pw-post-body-paragraph kv kw iq kx b ky ne la lb lc nf le lf lg ng li lj lk nh lm ln lo ni lq lr jx ij bi translated"><strong class="kx ir">认知不确定性是关于世界的知识缺失、不精确或者可能是错误的。</strong>它存在于现实世界中，不仅仅是一种主观感受。</p><p id="b274" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">如果你问我，这是需要处理的最重要的不确定性，因为它阻止你确定任何事情。我们的现实往往是一个很大的未知数，我们做出决定所依据的信息可能是有缺陷的、不完整的，或者根本不可用。</p><p id="5023" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">没有人确切地知道任何事情，因为我们出生在一个不确定的世界。</p><p id="c3f8" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated"><strong class="kx ir">例如</strong>，你可能对一个数学问题的答案非常有信心，但这并不意味着你是正确的。这也是深度神经网络的一个问题，由于输出激活，它们被认为过于自信。</p><p id="d44c" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">一个更具体的例子，如果你随机测量<strong class="kx ir"> 10个人</strong>来找出拥挤人群的平均身高，你对人群平均身高的估计将会不准确。这是因为你可能选择了比平均水平高或矮的人。你衡量的人越多，你的估计就越准确。这被称为认知的不确定性。</p><p id="2a1d" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated"><strong class="kx ir">显然，认知的不确定性可以通过更多的数据来减少。因为如果测量的人多，估计会更准确。</strong></p><p id="e9bd" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">在我们开始之前，我将添加导入:</p><figure class="nj nk nl nm gt kj"><div class="bz fp l di"><div class="nn no l"/></div></figure><h1 id="62c0" class="mg mh iq bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">普通神经网络的问题</h1><p id="3b71" class="pw-post-body-paragraph kv kw iq kx b ky ne la lb lc nf le lf lg ng li lj lk nh lm ln lo ni lq lr jx ij bi translated">由于我们使用最大似然估计来获得解释数据的最佳权重值，并且还考虑到我们的数据是有限的，因此应该有不止一个模型来解释或拟合数据。<strong class="kx ir">关键在于，权重是单个确定性点估计值。</strong></p><p id="45fa" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">我们来看看是不是这样，线性回归:</p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi np"><img src="../Images/baea44901d9c14a1143a1758b7ada45e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uOfxRSTR5eVWBO5xnCUiKw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片作者。</p></figure><p id="1432" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">当拟合3个独立的模型时，它们的起点将会不同，因为权重是随机初始化的。此外，优化过程是一个随机过程，因此每次的最佳权重值都会不同。</p><p id="23cc" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">这是关于模型权重的认知不确定性，因为没有唯一的答案。有一组合理的权重。如果数据集变大，我们可以得到更好的估计。</p><p id="faa0" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">让我们检查一下非线性回归问题:</p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi nq"><img src="../Images/d85fd3d08b3ebe75cff0ef946c9f510f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uGZmF7d1mMkJqK10Mch3Ow.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">作者图片</p></figure><p id="d235" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">正如我们所料，尽管学习到的权重不同，但这三行似乎都是合理的。同样的事情也适用于这种情况。</p><p id="4979" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">而如果考虑现实生活问题，比较复杂，应该有<strong class="kx ir"> </strong>多组权重才能很好的解释数据。因为在现实生活中，模型也更复杂，同时考虑到在损失情况下存在许多局部极小值，权重集对于获得合理的结果更有意义。</p><p id="f315" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">简而言之，问题是我们不知道权重的可能值，但有点估计值。</p><p id="0d10" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated"><strong class="kx ir">但是，我们如何在神经网络中融入认知不确定性呢？</strong></p><h1 id="031f" class="mg mh iq bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">贝叶斯神经网络</h1><h2 id="4a8a" class="nr mh iq bd mi ns nt dn mm nu nv dp mq lg nw nx mu lk ny nz my lo oa ob nc oc bi translated">想法</h2><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi od"><img src="../Images/bf2333adf69f46fd118aa605003a8bd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ETXO54i48DOdBTGKqaECMQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">神经网络中的权重不确定性[1]。</p></figure><p id="025a" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">当我们训练一个神经网络时，我们将最终得到权重的点估计值。然而，正如我们所讨论的，有多组权重应该能够合理且很好地解释数据。</p><p id="8452" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">为了捕捉模型权重中的认知不确定性，我们简单地将它们转换成概率分布。因此，我们不是学习这些点估计值，而是通过反向传播来学习这些分布的均值和标准差。</p><h2 id="1f13" class="nr mh iq bd mi ns nt dn mm nu nv dp mq lg nw nx mu lk ny nz my lo oa ob nc oc bi translated">深潜</h2><p id="91ac" class="pw-post-body-paragraph kv kw iq kx b ky ne la lb lc nf le lf lg ng li lj lk nh lm ln lo ni lq lr jx ij bi translated">由于每个权重都被一个概率分布所代替，所以现在没有这样的单一值。为了进行预测或获得前馈值，我们需要从这些分布中抽取样本。</p><ul class=""><li id="1b8d" class="ls lt iq kx b ky kz lc ld lg lu lk lv lo lw jx lx ly lz ma bi translated"><strong class="kx ir"> 1) </strong>样本来自网络的权重</li><li id="371e" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated"><strong class="kx ir"> 2) </strong>确定输出值</li><li id="0f72" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated"><strong class="kx ir"> 3) </strong>更新平均值和标准差。</li></ul><h1 id="e4dc" class="mg mh iq bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">这个方案背后的数学</h1><p id="ee88" class="pw-post-body-paragraph kv kw iq kx b ky ne la lb lc nf le lf lg ng li lj lk nh lm ln lo ni lq lr jx ij bi translated">论文<strong class="kx ir">神经网络中的权重不确定性</strong><strong class="kx ir">【1】</strong>介绍了一种完成这项任务的方法<strong class="kx ir">，称为反向投影贝叶斯</strong>。关键思想依赖于著名的贝叶斯定理:</p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi oe"><img src="../Images/e752a9f018d07f0e64ed5183a5ebdc65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fhUMPVO3gc8hcK536ppgWQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片作者。</p></figure><p id="20c4" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">如果你替换分母，贝叶斯定理的最终形式可以写成:</p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div class="gh gi of"><img src="../Images/17eda286a92743d68dd459719215bb0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*OvmgyiUjg6ApEuOyjSTa_A.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">作者图片</p></figure><p id="4b73" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">贝叶斯定理使得<strong class="kx ir">结合先验置信和似然</strong>获得模型参数的分布成为可能，称为<strong class="kx ir">后验分布。</strong></p><h2 id="09ae" class="nr mh iq bd mi ns nt dn mm nu nv dp mq lg nw nx mu lk ny nz my lo oa ob nc oc bi translated">原则上的培训流程</h2><ol class=""><li id="89c2" class="ls lt iq kx b ky ne lc nf lg og lk oh lo oi jx oj ly lz ma bi translated">选择先前的分布</li><li id="3c1b" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx oj ly lz ma bi translated">确定可能性</li><li id="9fe3" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx oj ly lz ma bi translated">使用贝叶斯定理确定后验分布</li></ol><p id="2c8a" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">然而，计算真实的后验概率是困难的，并且可能根本不可能，因为它包含复杂的积分。</p><p id="a20c" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated"><strong class="kx ir">最后，我们的目标是得到模型权重的分布。</strong></p><h1 id="06cc" class="mg mh iq bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">变分贝叶斯方法</h1><p id="984c" class="pw-post-body-paragraph kv kw iq kx b ky ne la lb lc nf le lf lg ng li lj lk nh lm ln lo ni lq lr jx ij bi translated">您可能已经注意到，分母包含一个复杂的积分来计算真实的后验分布。为此，我们需要近似后验分布。</p><p id="de33" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">变分贝叶斯方法试图用已知的分布来逼近真实的后验分布，具体称为变分后验分布。你可能认为用另一个函数逼近一个函数会有风险，因为逼近的函数可能非常糟糕。是的，这是真的，但是为了减轻这个问题，变分后验概率有一些参数。这些参数被调整，使得近似的一个应该尽可能接近真实的后验分布。</p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/ce40aea934aca527e40e35da29b6259f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*eLXKbUDFmpOrefMDMdnzRQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片作者。</p></figure><p id="b94e" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">变分后验在这里用<code class="fe ol om on oo b">phi</code>参数化。<code class="fe ol om on oo b">Theta</code>代表权重，而<code class="fe ol om on oo b">x</code>代表数据。</p><h2 id="aa79" class="nr mh iq bd mi ns nt dn mm nu nv dp mq lg nw nx mu lk ny nz my lo oa ob nc oc bi translated">库尔贝克-莱布勒散度</h2><p id="ccca" class="pw-post-body-paragraph kv kw iq kx b ky ne la lb lc nf le lf lg ng li lj lk nh lm ln lo ni lq lr jx ij bi translated">凭直觉，这两个分布应该彼此接近。那么，我们如何衡量这一点？嗯，有一个指标叫做<strong class="kx ir"> KL-Divergence，定义为</strong>:</p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi op"><img src="../Images/8f2e51a00839143950eab521e825ee5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qn9zvSCakqVQBGT4EZlZ6Q.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片作者。</p></figure><p id="4113" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">最后一个等式实际上告诉我们:</p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi oq"><img src="../Images/e3a060678e7b1b04f910ce2843c2bb22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ye_VUcVRGlKX_vHVdn7kiQ.png"/></div></div></figure><p id="d057" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">我们将第一个积分转换为<code class="fe ol om on oo b">log(P(x))</code>，因为<code class="fe ol om on oo b">Q(Theta | Phi)</code>是一个概率分布，积分为1。</p><p id="bb73" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">请记住，我们希望KL偏差尽可能低。所以现在，这是一个优化问题。因为<code class="fe ol om on oo b">P(x)</code>是常数，我们可以忽略它。最终，我们只剩下这个等式:</p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi or"><img src="../Images/190eadd0665d0883e4ade7d5c4738fbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KtnBI_vbo_vbjWCYOqr6hw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片作者。</p></figure><p id="780d" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">我们将会看到NLL的确切含义。但首先让我们总结一下，我们的损失函数是:</p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi os"><img src="../Images/62bdc81d905db21cb1587b0b7fc3bc9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*esrpb6xQTUXBV1DKZm6Ssw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片作者。</p></figure><p id="7d56" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">还有一件事我想提一下，我不会讨论它是如何得出的，但想展示一下<strong class="kx ir">证据下限</strong>或<strong class="kx ir"> ELBO。</strong></p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi ot"><img src="../Images/b7e996d69bb3143044591342e659a40a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0OOL_p795AsKrT5cikxZfw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片作者。</p></figure><p id="97e8" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">你可能已经注意到了，<strong class="kx ir"> ELBO </strong>是我们损失函数的负值。因此，最小化损失实际上相当于最大化<strong class="kx ir"> ELBO。</strong></p><p id="247a" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">在一天结束时，我们希望先验和变分后验之间的KL-Divergence尽可能低，同时保持T21期望的可能性尽可能高。</p><h1 id="74a9" class="mg mh iq bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">贝叶斯神经网络中的反向传播<strong class="ak"/></h1><p id="2d70" class="pw-post-body-paragraph kv kw iq kx b ky ne la lb lc nf le lf lg ng li lj lk nh lm ln lo ni lq lr jx ij bi translated">回忆一下我们的损失函数:</p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi os"><img src="../Images/62bdc81d905db21cb1587b0b7fc3bc9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*esrpb6xQTUXBV1DKZm6Ssw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片作者。</p></figure><p id="3f21" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">因此，期望项意味着对负对数似然乘以每个参数进行积分。另一方面，我们有一个KL散度，这是另一个积分。跳过很多数学，如果我们把这些方程写成积分，我们将得到:</p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi ou"><img src="../Images/400440a77d68ab2fe4e338021cdd6b62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BYumQRkjaKOmibM5Iu3YYw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片作者。</p></figure><blockquote class="ov ow ox"><p id="78c2" class="kv kw oy kx b ky kz la lb lc ld le lf oz lh li lj pa ll lm ln pb lp lq lr jx ij bi translated">天真地精确最小化这个成本在计算上是禁止的。相反，使用梯度下降和各种近似法[1]。</p></blockquote><p id="7a03" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">对<code class="fe ol om on oo b">phi</code>求导涉及对<code class="fe ol om on oo b">Theta</code>的积分，也就是模型权重。从计算的角度来看，这可能是非常昂贵的，甚至是不可能的！</p><p id="950c" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">一种方法是，我们可以改变这种期望形式，并应用蒙特卡罗近似法来计算w.r.t <code class="fe ol om on oo b">Theta</code>的导数。</p><p id="8d01" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">现在我们有:</p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi pc"><img src="../Images/782b058974a8e64ebea8031ffd296378.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6QV7t0uwWzmCIB5RB0TcXA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片作者。</p></figure><p id="41a6" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">这个预期在计算上也有一些问题，因为底层分布依赖于<code class="fe ol om on oo b">phi</code>。</p><h2 id="b90d" class="nr mh iq bd mi ns nt dn mm nu nv dp mq lg nw nx mu lk ny nz my lo oa ob nc oc bi translated">无偏蒙特卡罗梯度</h2><p id="429c" class="pw-post-body-paragraph kv kw iq kx b ky ne la lb lc nf le lf lg ng li lj lk nh lm ln lo ni lq lr jx ij bi translated">解决计算复杂性问题的一种方法是使用<strong class="kx ir">重新参数化技巧。</strong></p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi pd"><img src="../Images/f1fc256be81f2d2ba1fafeb3e77f69ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PJ8IYl0aR-d2meMkS2_IfA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">期望值的导数[1]。</p></figure><blockquote class="ov ow ox"><p id="c8f8" class="kv kw oy kx b ky kz la lb lc ld le lf oz lh li lj pa ll lm ln pb lp lq lr jx ij bi translated">命题1是高斯重新参数化技巧的推广(奥珀和阿尚博，2009；金玛和韦林，2014；Rezende等人，2014年)— [1]</p></blockquote><p id="9e1f" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">我不会在这里更深入，但简单地说，通过<strong class="kx ir">重新参数化，我们试图将依赖性从</strong> <code class="fe ol om on oo b"><strong class="kx ir">phi</strong></code> <strong class="kx ir">移开，这样期望最终将被独立地获取。</strong></p><p id="c0e4" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">如果你假设<code class="fe ol om on oo b">Q(theta | phi)</code>，它是变分后验概率，<strong class="kx ir">是高斯分布，</strong>你将得到上图中的公式(<em class="oy">期望值的导数[1])。</em></p><p id="84b9" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">所以，我们现在有了所有的片段。让我们将学习过程视为一个高层次的概述:</p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi pe"><img src="../Images/a785e530cff0f86f747aa5d317a18d0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VMLyfGenxvDamU2C0IZ29A.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片作者。</p></figure><h1 id="0d04" class="mg mh iq bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">迷你批次&amp;重新加权KL-散度</h1><p id="77c5" class="pw-post-body-paragraph kv kw iq kx b ky ne la lb lc nf le lf lg ng li lj lk nh lm ln lo ni lq lr jx ij bi translated">回忆一下我们的损失函数:</p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi os"><img src="../Images/62bdc81d905db21cb1587b0b7fc3bc9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*esrpb6xQTUXBV1DKZm6Ssw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片作者。</p></figure><p id="5be1" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">一种常见的方法是使用小批量，并对所有元素的梯度取平均值:</p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi pf"><img src="../Images/046079e5ca05e5423f267757ae6e663b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SRWFGI-Y5uQKWeDssQVANg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><em class="pg"> (BatchSize写为</em>缩写<em class="pg"> ) </em></p></figure><p id="ebd4" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated"><strong class="kx ir">右边的项由TensorFlow自动计算。</strong>我们还需要重新权衡KL术语的权重，以便进行适当的培训。这可以通过以下方式实现</p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi ph"><img src="../Images/fcb0798a1b793dbcb610fbecc67e6cdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JH2M8-ShTX2wjAmOer0T1g.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片作者。</p></figure><p id="fb63" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">其中<strong class="kx ir"> M </strong>为样本总数。默认情况下，TensorFlow将始终对您训练的每个损失函数或模型的小批量中所有元素的损失进行平均。这也是KL背离需要重新加权的原因。从这个推导中可以看出，我们得到了真实ELBO物镜的无偏估计。</p><p id="4725" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">就是这样，这就是贝叶斯反推方法。现在是时候使用张量流概率来实现这一点了。</p><h1 id="bacb" class="mg mh iq bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">使用张量流概率的重量不确定性</h1><h2 id="ae45" class="nr mh iq bd mi ns nt dn mm nu nv dp mq lg nw nx mu lk ny nz my lo oa ob nc oc bi translated">致密变异层</h2><p id="32f5" class="pw-post-body-paragraph kv kw iq kx b ky ne la lb lc nf le lf lg ng li lj lk nh lm ln lo ni lq lr jx ij bi translated">我们所说的可以在<code class="fe ol om on oo b">DenseVariational</code>层的帮助下实现。</p><p id="91fd" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">该层有四个重要参数。该层具有:</p><p id="967c" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated"><code class="fe ol om on oo b">make_prior_fn</code> : Python可调用函数取<code class="fe ol om on oo b"><a class="ae ku" href="https://www.tensorflow.org/api_docs/python/tf/size" rel="noopener ugc nofollow" target="_blank">tf.size(kernel)</a></code>、<code class="fe ol om on oo b"><a class="ae ku" href="https://www.tensorflow.org/api_docs/python/tf/size" rel="noopener ugc nofollow" target="_blank">tf.size(bias)</a></code>、<code class="fe ol om on oo b">dtype</code>并返回另一个可调用函数，该函数取一个输入并产生一个<code class="fe ol om on oo b">tfd.Distribution</code>实例。</p><p id="fe95" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated"><code class="fe ol om on oo b">make_posterior_fn</code> : Python callable取<code class="fe ol om on oo b"><a class="ae ku" href="https://www.tensorflow.org/api_docs/python/tf/size" rel="noopener ugc nofollow" target="_blank">tf.size(kernel)</a></code>、<code class="fe ol om on oo b"><a class="ae ku" href="https://www.tensorflow.org/api_docs/python/tf/size" rel="noopener ugc nofollow" target="_blank">tf.size(bias)</a></code>、<code class="fe ol om on oo b">dtype</code>，返回另一个callable取一个输入，产生一个<code class="fe ol om on oo b">tfd.Distribution</code>实例。</p><p id="e645" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated"><code class="fe ol om on oo b">kl_weight</code>缩放之前和之后的KL发散损失的量。</p><p id="d8ad" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated"><code class="fe ol om on oo b">kl_use_exact</code> Python <code class="fe ol om on oo b">bool</code>表示应该使用解析KL散度，而不是蒙特卡罗(MC)近似。</p><p id="8b0f" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">那些解释摘自<a class="ae ku" href="https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DenseVariational" rel="noopener ugc nofollow" target="_blank">的原始文献。让我们用例子来探究它们。</a></p><h2 id="e08a" class="nr mh iq bd mi ns nt dn mm nu nv dp mq lg nw nx mu lk ny nz my lo oa ob nc oc bi translated">回归中的权重不确定性</h2><figure class="nj nk nl nm gt kj"><div class="bz fp l di"><div class="nn no l"/></div></figure><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi pi"><img src="../Images/6143fb700bb0b51a0d8a1fda394a127b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7XmSVif8PwkAqASE41qmpQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片作者。</p></figure><p id="0018" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">让我们通过反向传播算法实现我们在Bayes中描述过的<code class="fe ol om on oo b">prior</code>和<code class="fe ol om on oo b">posterior</code>函数。</p><figure class="nj nk nl nm gt kj"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="5975" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated"><code class="fe ol om on oo b">prior</code>和<code class="fe ol om on oo b">posterior</code>函数的参数为<code class="fe ol om on oo b">kernel_size</code>和<code class="fe ol om on oo b">bias_size</code>。它们被添加并引用到我们想要学习的参数总数中。由于<strong class="kx ir">这个先验是不可训练的，</strong>当使用<code class="fe ol om on oo b">Laplace</code>先验时，我们简单地返回一个<code class="fe ol om on oo b">Sequential</code>模型的可调用对象。<strong class="kx ir">先验是不可训练的，因为我们对分布的均值和标准差进行了硬编码。</strong></p><p id="5441" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated"><code class="fe ol om on oo b">posterior</code>函数遵循相同的逻辑，但是我们现在使用<code class="fe ol om on oo b">VariableLayer</code>来表示这将是可训练分布。而对于<code class="fe ol om on oo b">params_size</code>，我们让TFP来决定为其修正形状，并选择正态分布作为后验。</p><p id="4bca" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">到目前为止，我们的先验是拉普拉斯先验，后验是正态先验。这就是我们以前讨论过的<strong class="kx ir"/><code class="fe ol om on oo b"><strong class="kx ir">posterior</strong></code><strong class="kx ir">，即变分后验。</strong></p><figure class="nj nk nl nm gt kj"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="d7cb" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">由于输出不是分布，我们可以用<code class="fe ol om on oo b">mse</code>作为损失函数。换句话说，这个模型不能捕捉任意的不确定性。</p><p id="7fb2" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated"><code class="fe ol om on oo b">kl_weight</code>参数是我们在<strong class="kx ir">迷你批次&amp;部分讨论的加权KL-Divergence项。通过对KL项重新加权，我们将得到无偏估计。回忆公式(ELBO目标):</strong></p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi ph"><img src="../Images/fcb0798a1b793dbcb610fbecc67e6cdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JH2M8-ShTX2wjAmOer0T1g.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片作者。</p></figure><p id="132c" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated"><code class="fe ol om on oo b">M</code>对应于数据集中元素的总数。所以在图层中我们设置了<code class="fe ol om on oo b">kl_weight = x_100.shape[0]</code>。</p><p id="c0ff" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">我们也没有设置<code class="fe ol om on oo b">kl_use_exact = True</code>。根据用于后验和先验的分布的选择，有可能解析地计算KL散度，并且如果它是并且解析解被登记在TFP库中，那么<code class="fe ol om on oo b">kl_use_exact</code>自变量可以被设置为<code class="fe ol om on oo b">True</code>。否则这将引发一个错误。这里我们用MC近似来计算。</p><p id="ff7f" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">你可能会问，在<code class="fe ol om on oo b">model.compile()</code>我们把亏损设为<code class="fe ol om on oo b">mse</code>而没有关于<code class="fe ol om on oo b">KL Term</code>的东西？没错，KL项是在使用这一层时由<code class="fe ol om on oo b">model.add_loss</code>内部添加的。实际上，我们可以通过运行以下命令来查看KL项:</p><pre class="nj nk nl nm gt pj oo pk pl aw pm bi"><span id="80bc" class="nr mh iq oo b gy pn po l pp pq">model_100.losses # Returns list.</span></pre><p id="d835" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">这将提供:</p><pre class="nj nk nl nm gt pj oo pk pl aw pm bi"><span id="6470" class="nr mh iq oo b gy pn po l pp pq">[&lt;tf.Tensor 'dense_variational/kldivergence_loss/batch_total_kl_divergence:0' shape=() dtype=float32&gt;]</span></pre><p id="faf0" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">我们看到，我们不需要担心损失函数，唯一需要做的事情是设置<code class="fe ol om on oo b">kl_weight</code>，它完成了。</p><p id="47fd" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">我们已经完成了我们的模型和层规范，让我们看看摘要:</p><pre class="nj nk nl nm gt pj oo pk pl aw pm bi"><span id="a240" class="nr mh iq oo b gy pn po l pp pq">Layer (type)                Output Shape              Param #   <br/>=================================================================<br/> dense_variational (DenseVar  (None, 1)                4         <br/> iational)                                                       <br/>                                                                 <br/>=================================================================<br/>Total params: 4<br/>Trainable params: 4<br/>Non-trainable params: 0</span></pre><p id="b5ef" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">输出形状是预期的，但是在正常情况下，我们只有2个参数，它们对应于问题<strong class="kx ir"> y = ax + b. </strong>中的系数和y截距</p><p id="2eae" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated"><strong class="kx ir">问题还是一样。</strong>但是现在我们学习权重和偏差的分布参数。在这种情况下，它们中的每一个都有一个均值和方差，<strong class="kx ir">总共4个可学习参数。</strong></p><figure class="nj nk nl nm gt kj"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="78f9" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">现在，我们有一个权重和一个偏差项，因此在<code class="fe ol om on oo b">mean()</code>和<code class="fe ol om on oo b">variance</code>数组中有两个值。例如，后验方差的第一指数对应于权重分布的学习方差。</p><figure class="nj nk nl nm gt kj"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="c7f7" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">现在，每次向前传递都会给出不同的预测，因为我们是从权重的分布中取样的。也就是说，可以认为它是一个集成分类器。运行上面的代码，我们将得到:</p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi pr"><img src="../Images/22833b05d0564f03475fa14d56c4a2ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vPUkvsdtjpG14mFgAC5SXg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片作者。</p></figure><p id="0856" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">注意红线是如何<strong class="kx ir">摇晃的。这些线是由模型在每次向前传递时生成的。不稳定的线条意味着在这种情况下认知的不确定性很高。</strong>因为我们没有无限多的数据点，换句话说，我们的数据集是有限的，我们最终可能会有不同的权重值来很好地解释数据。</p><p id="e057" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">起初，<strong class="kx ir">我们说过认知的不确定性可以随着更多的数据而减少。</strong>我们来看看是不是这样。</p><figure class="nj nk nl nm gt kj"><div class="bz fp l di"><div class="nn no l"/></div></figure><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi pi"><img src="../Images/662c8df4068523ead3a425f7b685f045.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QAHWz5ZIbzuj1Poc22ba_A.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片作者。</p></figure><p id="0a97" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">关系是相同的，但是这次我们有1000个数据点，而不是100。型号配置也保持不变。我不会重复安装过程，因为它是相同的。当我们从新模型中取样时，我们会得到:</p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi pr"><img src="../Images/d694d9def2f78b904312bbf60f8ab66d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aFyMEsJC75vgrw73dsGM0g.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片作者。</p></figure><p id="72cd" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">有一点要注意，现在这些线彼此更近了。我们可以用支线剧情来比较它们:</p><figure class="nj nk nl nm gt kj"><div class="bz fp l di"><div class="nn no l"/></div></figure><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi ps"><img src="../Images/933f508efcd5dd57b236537b63a189cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UYDt-DjIhT4GLNsJ0mBvww.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片作者。</p></figure><h2 id="12ff" class="nr mh iq bd mi ns nt dn mm nu nv dp mq lg nw nx mu lk ny nz my lo oa ob nc oc bi translated">数据集大小的影响</h2><p id="20d6" class="pw-post-body-paragraph kv kw iq kx b ky ne la lb lc nf le lf lg ng li lj lk nh lm ln lo ni lq lr jx ij bi translated">我们得出结论，增加更多的数据会减少认知的不确定性，这可以从图表中看出。当认知的不确定性较小时，线彼此更接近。这是因为当有更多的数据时，我们就有更多的信息。</p><h2 id="1ad7" class="nr mh iq bd mi ns nt dn mm nu nv dp mq lg nw nx mu lk ny nz my lo oa ob nc oc bi translated">非线性回归中的权重不确定性</h2><p id="19ee" class="pw-post-body-paragraph kv kw iq kx b ky ne la lb lc nf le lf lg ng li lj lk nh lm ln lo ni lq lr jx ij bi translated">假设我们有这样的数据:</p><figure class="nj nk nl nm gt kj"><div class="bz fp l di"><div class="nn no l"/></div></figure><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi nq"><img src="../Images/9038de769e688dedee36a99f4c5e72ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9FDe-hJW7TLDtFDpNPWCJQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">作者图片</p></figure><p id="89b1" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">让我们稍微改变一下我们的先验和变分后验。</p><figure class="nj nk nl nm gt kj"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="19de" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">现在，我们已经扩大了我们的先验，因为尺度是2的倍数。在我们的<code class="fe ol om on oo b">VariableLayer</code>中，我们通过了<code class="fe ol om on oo b">2 * n</code>,这意味着我们想要学习均值和标准差。这是以前由<code class="fe ol om on oo b">params_size</code>完成的。</p><p id="6ba0" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">另一件事是在事后，我们已经缩小了我们的标准差。其他值可能比<code class="fe ol om on oo b">0.003</code>更好。如果我们不缩小后验标准差，可能会有一些收敛问题。</p><figure class="nj nk nl nm gt kj"><div class="bz fp l di"><div class="nn no l"/></div></figure><pre class="nj nk nl nm gt pj oo pk pl aw pm bi"><span id="9c51" class="nr mh iq oo b gy pn po l pp pq">Layer (type)                Output Shape              Param #   <br/>=================================================================<br/> dense_variational (DenseVar  (None, 128)              512       <br/> iational)                                                       <br/>                                                                 <br/> dense_variational_1 (DenseV  (None, 64)               16512     <br/> ariational)                                                     <br/>                                                                 <br/> dense_variational_2 (DenseV  (None, 1)                130       <br/> ariational)                                                     <br/>                                                                 <br/>=================================================================<br/>Total params: 17,154<br/>Trainable params: 17,154<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="6140" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">现在，这一次层有非线性激活，其余的过程是相同的。我们指定我们的先验和后验函数，并重新加权KL项。</p><p id="8bd6" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">但是，我们需要训练这个模型更长的时间，因为它的工作更难。</p><figure class="nj nk nl nm gt kj"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="007a" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">为了绘制输出，我们从模型中提取前馈输出5次。</p><figure class="nj nk nl nm gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="gh gi pt"><img src="../Images/ab2dd546216f869a0895effcd418ee7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ochv8cnFKS9UYP0ysGqPlw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片作者。</p></figure><p id="5f8b" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">如我们所见，我们有合理的路线。通过调整变分后验概率可以改善结果。</p><p id="e402" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">在下结论之前，我想提一下变分后验概率的选择会影响模型的总参数。在这种情况下，如果我们将后验概率转换为多元高斯分布<strong class="kx ir">,那么模型总结看起来就是这样:</strong></p><figure class="nj nk nl nm gt kj"><div class="bz fp l di"><div class="nn no l"/></div></figure><pre class="nj nk nl nm gt pj oo pk pl aw pm bi"><span id="ea97" class="nr mh iq oo b gy pn po l pp pq">Layer (type)                Output Shape              Param #   <br/>=================================================================<br/> dense_variational_3 (DenseV  (None, 128)              33152     <br/> ariational)                                                     <br/>                                                                 <br/> dense_variational_4 (DenseV  (None, 64)               34093152  <br/> ariational)                                                     <br/>                                                                 <br/> dense_variational_5 (DenseV  (None, 1)                2210      <br/> ariational)                                                     <br/>                                                                 <br/>=================================================================<br/>Total params: 34,128,514<br/>Trainable params: 34,128,514<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="fac0" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">这是因为现在我们知道了:</p><ul class=""><li id="44b4" class="ls lt iq kx b ky kz lc ld lg lu lk lv lo lw jx lx ly lz ma bi translated">平均</li><li id="a900" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated">标准偏差</li><li id="572a" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated">协方差</li></ul><p id="669f" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">这在这个模型中计算量很大。</p><h1 id="6a41" class="mg mh iq bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">结论</h1><p id="5f32" class="pw-post-body-paragraph kv kw iq kx b ky ne la lb lc nf le lf lg ng li lj lk nh lm ln lo ni lq lr jx ij bi translated">在这篇文章中我们已经看到:</p><ul class=""><li id="129c" class="ls lt iq kx b ky kz lc ld lg lu lk lv lo lw jx lx ly lz ma bi translated">普通神经网络的问题</li><li id="59db" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated">贝叶斯神经网络的核心思想</li><li id="fe01" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated">变分贝叶斯方法</li><li id="399d" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated">迷你批次和重新加权KL散度</li><li id="0bbc" class="ls lt iq kx b ky mb lc mc lg md lk me lo mf jx lx ly lz ma bi translated">致密变异层</li></ul><p id="a771" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr jx ij bi translated">你可以在这里找到完整的笔记本和代码。</p><h1 id="3009" class="mg mh iq bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">后续步骤</h1><p id="6ceb" class="pw-post-body-paragraph kv kw iq kx b ky ne la lb lc nf le lf lg ng li lj lk nh lm ln lo ni lq lr jx ij bi translated">在接下来的文章中，我们将只专注于编写包括卷积层的模型，以及捕捉认知和任意的不确定性。</p><h1 id="ac47" class="mg mh iq bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">参考</h1><p id="c3c9" class="pw-post-body-paragraph kv kw iq kx b ky ne la lb lc nf le lf lg ng li lj lk nh lm ln lo ni lq lr jx ij bi translated">[1]:查尔斯·布伦德尔，朱利安·科尔内比斯，科雷·卡武克库奥卢，金奎大·威斯特拉，<a class="ae ku" href="https://arxiv.org/pdf/1505.05424.pdf" rel="noopener ugc nofollow" target="_blank">神经网络中的权重不确定性</a>，2015</p></div></div>    
</body>
</html>