<html>
<head>
<title>Reinforcement Learning with SARSA — A Good Alternative to Q-Learning Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">SARSA强化学习——Q-学习算法的一个很好的替代方案</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-with-sarsa-a-good-alternative-to-q-learning-algorithm-bf35b209e1c#2022-10-20">https://towardsdatascience.com/reinforcement-learning-with-sarsa-a-good-alternative-to-q-learning-algorithm-bf35b209e1c#2022-10-20</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="04b3" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph">强化学习</h2><div class=""/><div class=""><h2 id="dfad" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">如何使用状态-动作-奖励-状态-动作(SARSA)算法教智能代理玩简单的游戏</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/25716a2f4db40c4e00d6726dfa60b27d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jVnqkmLgnIbuVlFYl5-T_Q.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由<a class="ae li" href="https://pixabay.com/users/bamenny-2092731/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1214536" rel="noopener ugc nofollow" target="_blank"> bamenny </a>来自<a class="ae li" href="https://pixabay.com//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1214536" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><h1 id="b389" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">介绍</h1><p id="332e" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">机器学习的美妙之处在于，它不缺乏处理复杂任务的方法。例如，<strong class="md je">强化学习(RL) </strong>从业者已经开发了多种算法，能够教会<strong class="md je">智能代理</strong>导航他们的环境并执行动作。</p><p id="cf3c" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">在本文中，我深入研究了一种叫做<strong class="md je"> SARSA </strong>(状态-动作-奖励-状态-动作)的RL算法，它是Q-Learning的近亲。</p><p id="1186" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">如果您不熟悉强化学习或Q-Learning，请随意探索我以前的文章:</p><ul class=""><li id="234e" class="nc nd iu md b me mx mh my mk ne mo nf ms ng mw nh ni nj nk bi translated"><a class="ae li" rel="noopener" target="_blank" href="/reinforcement-learning-rl-what-is-it-and-how-does-it-work-1962cf6db103">强化学习(RL)——什么是强化学习，它是如何工作的？</a></li><li id="0d94" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><a class="ae li" rel="noopener" target="_blank" href="/q-learning-algorithm-how-to-successfully-teach-an-intelligent-agent-to-play-a-game-933595fd1abf">Q-学习算法</a></li></ul><h1 id="90a1" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">内容</h1><ul class=""><li id="bf08" class="nc nd iu md b me mf mh mi mk nq mo nr ms ns mw nh ni nj nk bi translated">SARSA在机器学习(ML)算法领域的地位</li><li id="b4a2" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">SARSA是如何工作的？<br/> - Q表<br/> - SARSA算法<br/> -与Q学习的比较</li><li id="08e3" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">Python示例，我们教一个智能代理如何玩Taxi-v3游戏</li></ul><h1 id="6155" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated"><strong class="ak"> SARSA在机器学习(ML)算法领域的地位</strong></h1><p id="105c" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">机器学习算法之间存在相似之处，这使我们能够根据架构和用例对它们进行分类。因此，我创造了一个观想，帮助我们看到整个ML宇宙。</p><p id="962e" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated"><strong class="md je">图表是交互式的</strong>，请点击👇在不同的类别上探索和揭示更多。</p><p id="4469" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">你会在<strong class="md je">基于值的方法</strong>组中的<strong class="md je">强化学习</strong>ML算法分支下找到<strong class="md je"> SARSA </strong>(下一节会有更多相关内容)。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="nt nu l"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">机器学习算法分类。由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>创建的互动图表。</p></figure><h1 id="616b" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">SARSA是如何工作的？</h1><h2 id="c28d" class="nv lk iu bd ll nw nx dn lp ny nz dp lt mk oa ob lv mo oc od lx ms oe of lz ja bi translated">q表</h2><p id="1ca6" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">SARSA是一种<strong class="md je">基于值的方法</strong>，类似于Q-learning。因此，它使用一个<strong class="md je"> Q表</strong>来存储每个<strong class="md je">状态-动作对</strong>的值。使用基于价值的策略，我们通过教代理<strong class="md je">识别哪些状态(或状态-动作对)更有价值来间接训练代理</strong>。</p><p id="da47" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">通常，我们从Q表中初始化为0的所有值开始，并使用训练来优化Q表。然后，我们的代理可以使用存储在Q表中的信息来选择每个状态下的最佳动作(即，代理选择的每个状态下具有最高值的动作)。</p><p id="9ee7" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">下面是一个空Q表的示例:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj og"><img src="../Images/6135ed2ece7768046ec4bed925962081.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qLstyhI-QmGE2KbucGI0iw.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">空Q表。图片作者<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>。</p></figure><h2 id="8ffa" class="nv lk iu bd ll nw nx dn lp ny nz dp lt mk oa ob lv mo oc od lx ms oe of lz ja bi translated">SARSA算法</h2><p id="6f64" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">SARSA是一个<strong class="md je">基于策略的</strong>算法，这是它与Q-Learning(非策略算法)的区别之一。<strong class="md je"> On-policy </strong>是指在训练过程中，我们对agent使用相同的策略<strong class="md je"> act </strong>(代理策略)和<strong class="md je">update</strong>value function(更新策略)。同时，使用<strong class="md je">非策略</strong>方法，我们使用不同的策略来执行和更新。</p><p id="3922" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">现在让我们来看看SARSA算法本身:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj oh"><img src="../Images/94cf1b69bdc4b0a3df3687ee949c8171.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cXlwV7vlOhQUZiATkmln3A.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">SARSA算法。图片来自<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>。</p></figure><ul class=""><li id="2272" class="nc nd iu md b me mx mh my mk ne mo nf ms ng mw nh ni nj nk bi translated">q是<strong class="md je">值函数</strong>，左边Q(𝑆𝑡,𝐴𝑡项是特定状态-动作对的<strong class="md je">新值</strong>。注意，S指状态，A指动作。</li><li id="1888" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">在等式的右边，我们找到相同的Q(𝑆𝑡,𝐴𝑡项，在这种情况下，它是同一状态-动作对的当前值。</li><li id="b1f5" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">为了更新当前值，我们在代理采取行动后获取<strong class="md je">奖励(</strong> 𝑅𝑡+1)，为下一个状态-下一个行动对 𝛾𝑄(𝑆𝑡+1,𝐴𝑡+1) <strong class="md je">加上用γ</strong>贴现的<strong class="md je">值，并减去<strong class="md je">当前值</strong> Q(𝑆𝑡,𝐴𝑡).</strong></li><li id="e32f" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">因此，方括号中的项产生正值、零或负值，这导致Q(𝑆𝑡,𝐴𝑡).的新值增加、不变或减少注意，我们还应用了一个<strong class="md je">学习率(alpha) </strong>来控制每次更新的“大小”。</li></ul><p id="f81e" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">由于SARSA使用<strong class="md je">时间差分(TD)方法</strong>，该算法将在每一步后不断更新Q表，直到我们达到最大迭代次数或解决方案收敛到最优。</p><h2 id="cc15" class="nv lk iu bd ll nw nx dn lp ny nz dp lt mk oa ob lv mo oc od lx ms oe of lz ja bi translated"><strong class="ak">与Q-Learning的比较</strong></h2><p id="90f0" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">如果我们看一下Q-Learning算法使用的等式，我们可以看到，区别在于它如何选择下一个状态的值。即<strong class="md je"> Q-Learning基于Q-table中的现有值为下一个状态取最大值</strong>。同时，<strong class="md je"> SARSA取下一个状态-下一个动作对的值，</strong>，如上所示。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj oi"><img src="../Images/9df1bd46fd0affffa5c05b7cb0422a74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f5OgZkCVcoWz1sYCbV7WTw.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">q-学习算法。图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><h1 id="6e99" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated"><strong class="ak"> Python示例，我们教智能代理如何玩Taxi-v3游戏</strong></h1><p id="a7b5" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">为了更好地理解它是如何工作的，让我们使用SARSA算法来教代理如何玩在<a class="ae li" href="https://www.gymlibrary.dev/environments/toy_text/taxi/" rel="noopener ugc nofollow" target="_blank"> Open AI的健身房环境</a>中可用的Taxi-v3游戏。</p><p id="cde6" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">请看下面的gif图片，它展示了游戏的样子:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj oj"><img src="../Images/dbbb2dff7f18299e6837487140ca52ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/1*ULEPVOT54qOIOl644fvZBQ.gif"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">Gif图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>使用来自<a class="ae li" href="https://www.gymlibrary.dev/environments/toy_text/taxi/" rel="noopener ugc nofollow" target="_blank"> Taxi-v3游戏</a>的组件创建。</p></figure><p id="6e28" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">游戏的目标是让出租车从一个上车地点<strong class="md je">搭载一名乘客</strong>(看到一个人)，该地点可以是四个彩色方块中的任何一个，然后<strong class="md je">将该乘客带到下车地点</strong>(看到一座建筑)，该地点也可以是四个彩色方块中的任何一个。</p><p id="dacf" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">默认游戏奖励如下:</p><ul class=""><li id="61f2" class="nc nd iu md b me mx mh my mk ne mo nf ms ng mw nh ni nj nk bi translated">-每一步1分，除非触发了另一个奖励</li><li id="6c9b" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">运送一名乘客+20</li><li id="3e29" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">-10分因非法执行“上车”和“下车”行动</li></ul><h2 id="e2d2" class="nv lk iu bd ll nw nx dn lp ny nz dp lt mk oa ob lv mo oc od lx ms oe of lz ja bi translated">设置</h2><p id="87d3" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">我们需要获得以下库:</p><ul class=""><li id="c06a" class="nc nd iu md b me mx mh my mk ne mo nf ms ng mw nh ni nj nk bi translated"><a class="ae li" href="https://www.gymlibrary.dev/" rel="noopener ugc nofollow" target="_blank">为<a class="ae li" href="https://www.gymlibrary.dev/environments/toy_text/taxi/" rel="noopener ugc nofollow" target="_blank"> Taxi-v3 </a>游戏环境打开艾的健身房库</a></li><li id="fbea" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><a class="ae li" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"> Numpy </a>用于数据操作</li><li id="3169" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><a class="ae li" href="https://matplotlib.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> Matplotlib </a>和内置的IPython库，用于显示代理如何导航其环境</li></ul><p id="dbf3" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">让我们导入库:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ok nu l"/></div></figure><p id="c9cb" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">上面的代码打印了本例中使用的包版本:</p><pre class="kt ku kv kw gu ol om on oo aw op bi"><span id="8906" class="nv lk iu om b gz oq or l os ot">numpy: 1.23.3<br/>gym: 0.26.0<br/>matplotlib: 3.6.0</span></pre><p id="1354" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">接下来，我们设置一个Taxi-v3环境:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ok nu l"/></div></figure><p id="6401" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">设置好环境后，让我们检查一下<strong class="md je">描述</strong>、<strong class="md je">状态空间</strong>和<strong class="md je">动作空间</strong>:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ok nu l"/></div></figure><pre class="kt ku kv kw gu ol om on oo aw op bi"><span id="b90b" class="nv lk iu om b gz oq or l os ot">Environment Array: <br/>[[b'+' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'+']<br/> [b'|' b'R' b':' b' ' b'|' b' ' b':' b' ' b':' b'G' b'|']<br/> [b'|' b' ' b':' b' ' b'|' b' ' b':' b' ' b':' b' ' b'|']<br/> [b'|' b' ' b':' b' ' b':' b' ' b':' b' ' b':' b' ' b'|']<br/> [b'|' b' ' b'|' b' ' b':' b' ' b'|' b' ' b':' b' ' b'|']<br/> [b'|' b'Y' b'|' b' ' b':' b' ' b'|' b'B' b':' b' ' b'|']<br/> [b'+' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'+']]</span><span id="b4d2" class="nv lk iu om b gz ou or l os ot">State(Observation) space: Discrete(500)<br/>Action space: Discrete(6)</span></pre><p id="2b06" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">您可以看到环境布局与本节开头的gif图像中显示的布局相匹配。</p><p id="b61e" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">状态空间包含500个离散状态，代表25个出租车位置、5个可能的乘客位置(包括乘客在出租车中的情况)和4个目的地位置的所有可能组合，即25 * 5 * 4 = 500。</p><p id="1ef6" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">同时，<strong class="md je">动作</strong>是:</p><ul class=""><li id="3696" class="nc nd iu md b me mx mh my mk ne mo nf ms ng mw nh ni nj nk bi translated">0:向南移动</li><li id="fb82" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">1:向北移动</li><li id="1223" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">2:向东移动</li><li id="6bcb" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">3:向西移动</li><li id="0790" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">4:搭载一名乘客</li><li id="b1db" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">5:让乘客下车</li></ul><p id="eb30" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">在我们开始模型训练之前，让我们的代理(出租车)执行随机动作，并渲染它在环境中的运动。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ok nu l"/></div></figure><p id="2931" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">以下是上述代码的输出:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj oj"><img src="../Images/c1b599a2d774f40f3bb6ea3f6b56c55c.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/1*-8ObtvWU_KbEtgCpKtRvqA.gif"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>使用来自<a class="ae li" href="https://www.gymlibrary.dev/environments/toy_text/taxi/" rel="noopener ugc nofollow" target="_blank"> Taxi-v3游戏</a>的组件创建的Gif图像。</p></figure><h2 id="229a" class="nv lk iu bd ll nw nx dn lp ny nz dp lt mk oa ob lv mo oc od lx ms oe of lz ja bi translated">模特培训</h2><p id="7f1e" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">设置完成后，让我们使用<strong class="md je"> SARSA </strong>为我们的代理在这个游戏中找到最好的<strong class="md je"> policy(𝜋) </strong>。</p><p id="4cca" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">我们首先初始化几个参数:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ok nu l"/></div></figure><p id="9a5f" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">为了平衡<strong class="md je">探索与开发</strong>，我们将在整个培训中改变epsilon。我们将从epsilon=1(纯探索)开始，并随着每一集而衰减epsilon，以逐渐从纯探索转向开发。</p><p id="3618" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">接下来，让我们初始化Q表。正如我们在上一节中所看到的，这将是一个500 x 6的表格，行代表状态，列代表动作。我们用全0初始化Q表，因为在开始训练之前，我们不知道每个状态的价值。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ok nu l"/></div></figure><p id="9aa8" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">以下是初始化Q表的片段:</p><pre class="kt ku kv kw gu ol om on oo aw op bi"><span id="cf8e" class="nv lk iu om b gz oq or l os ot">array([[0., 0., 0., 0., 0., 0.],<br/>       [0., 0., 0., 0., 0., 0.],<br/>       [0., 0., 0., 0., 0., 0.],<br/>       ...,<br/>       [0., 0., 0., 0., 0., 0.],<br/>       [0., 0., 0., 0., 0., 0.],<br/>       [0., 0., 0., 0., 0., 0.]])</span></pre><p id="4628" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">SARSA是一个<strong class="md je"> on-policy </strong>算法，因此我们将使用相同的<strong class="md je">ε-贪婪策略</strong>来操作和更新Q表。现在让我们定义用于培训和评估的函数:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ok nu l"/></div></figure><p id="a715" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">如果你仔细观察<strong class="md je"> update_Q </strong>函数，你会发现它包含了上一节分析的SARSA算法方程。</p><p id="4f14" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">最后，让我们定义我们的培训功能:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ok nu l"/></div></figure><p id="49a8" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">现在我们可以调用训练函数并查看结果:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ok nu l"/></div></figure><p id="9344" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">在训练之后，我们得到Q表作为输出。但是，很难说是否最优。因此，我们需要做一些额外的评估。以下是培训后Q表的一个片段:</p><pre class="kt ku kv kw gu ol om on oo aw op bi"><span id="9d87" class="nv lk iu om b gz oq or l os ot">array([[  0.        ,   0.        ,   0.        ,   0.        ,<br/>          0.        ,   0.        ],<br/>       [-15.88478151, -14.78900627, -16.69581259, -14.66434947,<br/>        -12.6181495 , -23.80156577],<br/>       [ -6.19468628,  -2.88294982,  -6.49649702,  -2.24076811,<br/>         -0.99650814, -13.12010172],<br/>       ...,<br/>       [ -4.25834456,   2.30997394,  -5.05986368,  -3.97161199,<br/>        -12.53096203, -15.46710745],<br/>       [-17.96569618, -12.49725207, -16.13686029, -10.22935569,<br/>        -24.18094778, -27.10102464],<br/>       [ 12.22333031,   6.43016417,  12.76697657,  15.36125068,<br/>          4.18029065,   2.6316856 ]])</span></pre><h2 id="a609" class="nv lk iu bd ll nw nx dn lp ny nz dp lt mk oa ob lv mo oc od lx ms oe of lz ja bi translated"><strong class="ak">评估</strong></h2><p id="6e6c" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">既然很难通过考察Q表来判断结果，那我们就来运行一万集来评价代理的表现。我们将使用<strong class="md je">平均值</strong>、<strong class="md je">标准差</strong>和<strong class="md je">报酬的最小/最大值</strong>等统计数据以及<strong class="md je">分配图</strong>。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ok nu l"/></div></figure><p id="21a8" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">我们调用上述函数，并使用以下代码显示结果:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ok nu l"/></div></figure><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ov"><img src="../Images/906cd376c33ea9afc75eb868339f75e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mbBm6_kLx5KrYL1uo98NcQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">模型评估结果。图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="46dc" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">让我们来看看结果:</p><ul class=""><li id="3883" class="nc nd iu md b me mx mh my mk ne mo nf ms ng mw nh ni nj nk bi translated"><strong class="md je">一万集的平均回报是7.96 +/- 2.59。</strong>它表示奖励有所变化，但那是因为在每集开始时，出租车、乘客和下车地点会发生变化。因此，每次我们需要不同数量的步骤来完成任务，所以我们并不总是得到相同的奖励(每走一步减1分)。</li><li id="114b" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">最小奖励是3，这证实了我们总是得到正的奖励。</strong>查看地图，我们可以很容易地计算出，如果我们在相对的角落初始化出租车和乘客，下车位置与出租车的原始位置相同，则需要17步(16次移动+ 1次上车动作)。因此，为最远的场景选择最佳路线将导致奖励3 (20 - 17)。</li><li id="dce1" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">最高奖励是15英镑，所以让我们看看为什么我们不能期望更高。如果乘客在下车地点被初始化，该集立即结束(即，不是有效场景，因此没有希望获得20分)。因此，我们所能希望的最好情况是乘客和出租车在同一地点初始化。如果我们选择两个最接近的颜色方块(红色和黄色)，代理将使用5次移动(1次上车，4次移动)将乘客从上车地点运送到下车地点，给我们最大可能的奖励15 (20 - 5)。</li><li id="a0a7" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">奖励分布图显示奖励接近正态分布。因此，虽然它不能证明我们有一个最优策略，但它表明代理人正在做出理性的行动。</strong></li></ul><p id="323a" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">确保我们有最优策略的一种方法是计算每个初始状态所需的最少步骤数。然后验证代理从未采取超过最小数量的步骤。然而，我将把它作为一个练习留给读者。</p><p id="52df" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">相反，让我展示一个按照我们的策略执行行动的代理的可视化。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ok nu l"/></div></figure><p id="47ac" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">上面的代码给了我们下面的可视化，它确认了，至少在下面捕获的两个场景中，代理已经采取了一个最佳的方法:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj oj"><img src="../Images/ed88eef22849bbc1edb2db900a5737f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/1*laJx74jnwfMFqXiw29bLFA.gif"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>使用来自<a class="ae li" href="https://www.gymlibrary.dev/environments/toy_text/taxi/" rel="noopener ugc nofollow" target="_blank"> Taxi-v3游戏</a>的组件创建的Gif图像。</p></figure><h1 id="729c" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">结束语</h1><p id="0a3e" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">现在你知道了SARSA是如何工作的，以及它与标准的Q学习算法有什么不同。健身房图书馆提供了许多其他环境(游戏)，所以我鼓励你用你新学到的知识来训练一个代理玩不同的游戏。</p><p id="5d39" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">你可以在我的<a class="ae li" href="https://github.com/SolClover/Art057_RL_SARSA" rel="noopener ugc nofollow" target="_blank"> <strong class="md je"> GitHub库</strong> </a>找到完整的Python代码。请注意，有两个Jupyter笔记本，一个用于Taxi-v3游戏，另一个用于冰封湖游戏。</p><p id="9b83" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">请不要忘记<a class="ae li" href="https://solclover.com/subscribe" rel="noopener ugc nofollow" target="_blank">订阅</a>到<strong class="md je">了解其他强化学习算法</strong>，我将在我即将发表的文章中涉及这些算法。</p><p id="b463" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">干杯！🤓<br/> <strong class="md je">索尔·多比拉斯</strong></p><p id="61dd" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated"><strong class="md je"> <em class="ow">如果您不是中等会员，</em> </strong> <em class="ow">请考虑使用我下面的个性化链接加入:</em></p><div class="ox oy gq gs oz pa"><a href="https://bit.ly/3J6StZI" rel="noopener  ugc nofollow" target="_blank"><div class="pb ab fp"><div class="pc ab pd cl cj pe"><h2 class="bd je gz z fq pf fs ft pg fv fx jd bi translated">通过我的推荐链接加入Medium索尔·多比拉斯</h2><div class="ph l"><h3 class="bd b gz z fq pf fs ft pg fv fx dk translated">阅读索尔·多比拉斯和媒体上成千上万的其他作家的每一个故事。你的会员费直接支持索尔…</h3></div><div class="pi l"><p class="bd b dl z fq pf fs ft pg fv fx dk translated">solclover.com</p></div></div><div class="pj l"><div class="pk l pl pm pn pj po lc pa"/></div></div></a></div></div></div>    
</body>
</html>