<html>
<head>
<title>Metrics for uncertainty evaluation in regression problems</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回归问题中不确定性评估的度量</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/metrics-for-uncertainty-evaluation-in-regression-problems-210821761aa#2022-08-12">https://towardsdatascience.com/metrics-for-uncertainty-evaluation-in-regression-problems-210821761aa#2022-08-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d159" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何用有效性、锐度、负对数似然和连续排序概率得分(CRPS)度量来评估不确定性</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c8eb8855897e4956b8282078ff9ba663.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*opdJUfpuKnSNyb0T"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">圣地亚哥·拉卡尔塔在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="208e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">许多现实世界的问题需要对未来结果进行不确定性估计，以便更好地做出决策。然而，大多数最新的机器学习算法只能估计单值预测，该预测通常是假设与真实结果匹配良好的条件分布的平均值或中值。单值预测不能暗示的是预测有多有把握。因此，最近提出了更复杂的方法，如<a class="ae kv" href="https://arxiv.org/abs/1910.03225" rel="noopener ugc nofollow" target="_blank"> NGBoost </a>、<a class="ae kv" href="https://arxiv.org/abs/2106.01682" rel="noopener ugc nofollow" target="_blank"> PGBM </a>、贝叶斯方法和基于深度学习的方法，可以估计每个数据点的条件分布。由于这些方法估计条件分布，就产生了如何评价这些方法的问题。显然，常用的准确性指标，如单点预测的RMSE或MAE并不适合。因此，在本文中，我展示了几种评估估计条件分布的模型的方法，以及如何评估不直接估计条件分布但提供预测区间的分位数回归。具体来说，我评估了三种算法:</p><ul class=""><li id="22b9" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">带分位数回归的LightGBM</li><li id="7b56" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">NGBoost——一种具有概率预测的梯度推进算法</li><li id="bd7a" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">模拟平均值和标准差的概率回归神经网络</li></ul><p id="47ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">并使用四个度量标准进行不确定性评估:</p><ul class=""><li id="a006" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">有效性——评估概率环境中分位数和偏差的可靠性</li><li id="ad66" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">锐度—估计概率的集中程度(预测区间)</li><li id="24d3" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">负对数似然(NLL)-给定条件分布的推断参数时，观察数据出现的似然性</li><li id="4dbc" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">连续等级概率得分(close估计条件分布与观察点的接近程度</li></ul></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="c0d5" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">数据</h1><p id="fa65" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">我正在使用由<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>提供的<a class="ae kv" href="https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html" rel="noopener ugc nofollow" target="_blank">加州住房数据集</a>。这是一个小数据集，有20640条记录，只有8个数字特征。目标变量是加州各区的房价中值，以十万美元计。</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="92ed" class="np mo iq nl b gy nq nr l ns nt">from sklearn.datasets import fetch_california_housing</span><span id="3ea2" class="np mo iq nl b gy nu nr l ns nt">california_housing = fetch_california_housing(as_frame=True)</span><span id="812d" class="np mo iq nl b gy nu nr l ns nt">#get the data<br/>data = california_housing.frame<br/>data</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/eea71957d2960773d465567268cb6eb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FuC561k2ZkMRpjP_LKKVvw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据快照。作者图片</p></figure><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="14d5" class="np mo iq nl b gy nq nr l ns nt">#get the target<br/>target = california_housing.target<br/>target</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/c815b473038c37b785b4f6d1435b8b64.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*nNq_02QvEVYfMGu-0VGVjg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">目标变量快照。作者图片</p></figure><p id="5ab0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">像往常一样，数据被分成训练集和测试集。我将使模型适合训练集，并在测试集上对它们进行评估。因为本文的重点是评估指标，所以我没有执行任何超参数调优，也没有应用算法的默认参数。</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="c09c" class="np mo iq nl b gy nq nr l ns nt">X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42, shuffle=True)</span></pre><p id="c1da" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将深度学习解决方案的功能标准化</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="924b" class="np mo iq nl b gy nq nr l ns nt">x_scaler = StandardScaler()<br/>X_train_scaled = x_scaler.fit_transform(X_train)<br/>X_test_scaled = x_scaler.transform(X_test)</span></pre><h1 id="7b03" class="mn mo iq bd mp mq nx ms mt mu ny mw mx jw nz jx mz jz oa ka nb kc ob kd nd ne bi translated">算法</h1><p id="0fb2" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">我选择了三种算法进行评估。</p><h2 id="0d03" class="np mo iq bd mp oc od dn mt oe of dp mx lf og oh mz lj oi oj nb ln ok ol nd om bi translated">带分位数回归的LightGBM</h2><p id="b7e5" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated"><a class="ae kv" href="https://lightgbm.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>，一种梯度推进算法，广泛应用于机器学习社区。使用现有算法获得预测区间的最直接方法可能是构建至少两个分位数回归模型，以针对一些低和高条件分位数。例如，90%的预测区间需要拟合两个具有5%和95%分位数水平的分位数回归。幸运的是，LightGBM实现了分位数回归，但是任何其他支持分位数回归的<a class="ae kv" href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_quantile_regression.html" rel="noopener ugc nofollow" target="_blank">算法也可以。</a></p><p id="83bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我定义了13个分位数水平，建立了14个模型。一个模型预测中值价格的预期平均值，其余13个模型预测给定分位数水平的中值价格:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="9aa5" class="np mo iq nl b gy nq nr l ns nt">quantiles = [0.05, 0.1, 0.15, 0.2, 0.3, <br/>             0.4, 0.5, 0.6, 0.7, 0.8, <br/>             0.85, 0.9, 0.95]</span><span id="8fce" class="np mo iq nl b gy nu nr l ns nt">#save quantile predictions<br/>quantile_predictions = {}</span><span id="aa29" class="np mo iq nl b gy nu nr l ns nt">train_data = <strong class="nl ir">lgb.Dataset</strong>(X_train, <br/>                         label=y_train, <br/>                         free_raw_data=False)</span><span id="4160" class="np mo iq nl b gy nu nr l ns nt">#first model that predicts expected mean<br/>params = {'objective': 'regression'}<br/>lgb_model = <strong class="nl ir">lgb.train</strong>(params=params, <br/>                      train_set=train_data, <br/>                      num_boost_round=100)</span><span id="6e72" class="np mo iq nl b gy nu nr l ns nt">lgb_prediction = <strong class="nl ir">lgb_model.predict</strong>(X_test)</span><span id="ecdc" class="np mo iq nl b gy nu nr l ns nt">#train models on quantiles<br/>for quantile in <strong class="nl ir">quantiles</strong>:<br/>    print(f"modeling quantile {quantile}")<br/>    params = {'objective': 'quantile', 'alpha': quantile}<br/>    lgb_model = lgb.train(params=params, <br/>                          train_set=train_data, <br/>                          num_boost_round=100)<br/>    pred = <strong class="nl ir">lgb_model.predict</strong>(X_test)<br/>    <br/>    quantile_predictions[quantile] = pred</span></pre><h2 id="1817" class="np mo iq bd mp oc od dn mt oe of dp mx lf og oh mz lj oi oj nb ln ok ol nd om bi translated">NGBoost</h2><p id="cfa5" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated"><a class="ae kv" href="https://arxiv.org/abs/1910.03225" rel="noopener ugc nofollow" target="_blank"> NGBoost </a>是一种估计条件(正态)分布参数的梯度推进算法。</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="b032" class="np mo iq nl b gy nq nr l ns nt">ngb = <strong class="nl ir">ngboost.NGBoost</strong>(Base=learners.default_tree_learner, Dist=distns.Normal, Score=scores.LogScore, natural_gradient=True, verbose=True)<br/>ngb.fit(X_train, y_train)</span><span id="5ce0" class="np mo iq nl b gy nu nr l ns nt"><strong class="nl ir">#predicted mean</strong><br/><strong class="nl ir">ngb_mean_pred </strong>= ngb.predict(X_test)</span><span id="94f8" class="np mo iq nl b gy nu nr l ns nt"><strong class="nl ir">#predicted distribution parameters</strong><br/><strong class="nl ir">ngb_dist_pred </strong>= ngb.pred_dist(X_test)</span></pre><h2 id="d2dd" class="np mo iq bd mp oc od dn mt oe of dp mx lf og oh mz lj oi oj nb ln ok ol nd om bi translated">回归神经网络</h2><p id="df42" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">回归神经网络使用<a class="ae kv" href="https://www.tensorflow.org/probability/" rel="noopener ugc nofollow" target="_blank">张量流概率</a>框架估计条件(正态)分布的参数。</p><p id="42ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是神经网络的架构:</p><p id="3d6b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">损失函数被定义为基础正态分布的负对数似然性:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="cf52" class="np mo iq nl b gy nq nr l ns nt">def <strong class="nl ir">nll_loss</strong>(y, distr):<br/>    return -distr.log_prob(y)</span></pre><p id="1c5e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下函数接收由两个节点组成的输入，一个节点用于平均值，另一个节点用于标准差:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="a114" class="np mo iq nl b gy nq nr l ns nt">def <strong class="nl ir">model_distribution</strong>(params): <br/>    return tfd.Normal(<strong class="nl ir">loc</strong>=params[:,0:1],   <br/>                      <strong class="nl ir">scale</strong>=tf.math.softplus(params[:,1:2]))</span></pre><p id="e896" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">均值和标准差分别建模。</p><p id="2d64" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">平均:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="11ba" class="np mo iq nl b gy nq nr l ns nt">inputs = layers.Input(shape=((len(X_test.columns),)))</span><span id="b1e2" class="np mo iq nl b gy nu nr l ns nt">hidden1 = layers.Dense(100, activation = "relu", name = "dense_mean_1")(inputs)<br/>hidden2 = layers.Dense(50, activation = "relu", name = "dense_mean_2")(hidden1)<br/><strong class="nl ir">output_mean </strong>= layers.Dense(1, name = "mean_output")(hidden2)</span></pre><p id="36fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">标准偏差:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="b53b" class="np mo iq nl b gy nq nr l ns nt">hidden1 = layers.Dense(100,activation="relu", name = "dense_sd_1")(inputs)<br/>hidden1 = layers.Dropout(0.1)(hidden1)<br/>hidden2 = layers.Dense(50,activation="relu", name = "dense_sd_2")(hidden1)<br/>hidden2 = layers.Dropout(0.1)(hidden2)<br/>hidden3 = layers.Dense(20,activation="relu", name = "dense_sd_3")(hidden2)<br/><strong class="nl ir">output_sd </strong>= layers.Dense(1, name = "sd_output")(hidden3)</span></pre><p id="7c2b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">两者被连接并传播到分布层:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="056e" class="np mo iq nl b gy nq nr l ns nt"><strong class="nl ir">mean_sd_layer </strong>= layers.Concatenate(name = "mean_sd_concat")([output_mean, output_sd]) <br/><strong class="nl ir">dist </strong>= tfp.layers.DistributionLambda(model_distribution)(mean_sd_layer)</span></pre><p id="8351" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后一步是编译分布模型，并使用分布模型分别预测均值和标准差。</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="3aef" class="np mo iq nl b gy nq nr l ns nt">dist_mean = <strong class="nl ir">tfp.layers.DistributionLambda</strong>( make_distribution_fn=model_distribution, convert_to_tensor_fn=tfp.distributions.Distribution.mean)(mean_sd_layer)<br/>dist_std = <strong class="nl ir">tfp.layers.DistributionLambda</strong>( make_distribution_fn=model_distribution, convert_to_tensor_fn=tfp.distributions.Distribution.stddev)(mean_sd_layer)</span><span id="064d" class="np mo iq nl b gy nu nr l ns nt"><strong class="nl ir">model_distr </strong>= <strong class="nl ir">models.Model</strong>(inputs=inputs, outputs=dist)<br/><strong class="nl ir">model_distr</strong>.compile(optimizers.Adagrad(learning_rate=0.001), loss=nll_loss)</span><span id="8543" class="np mo iq nl b gy nu nr l ns nt">#the model for mean<br/><strong class="nl ir">model_mean </strong>= <strong class="nl ir">models.Model</strong>(inputs=inputs, outputs=dist_mean)</span><span id="b6f5" class="np mo iq nl b gy nu nr l ns nt">#the model for std<strong class="nl ir"><br/>model_sd </strong>= <strong class="nl ir">models.Model</strong>(inputs=inputs, outputs=dist_std)</span><span id="2e3b" class="np mo iq nl b gy nu nr l ns nt">history = <strong class="nl ir">model_distr.fit</strong>(X_train_scaled, y_train, <br/>                          <strong class="nl ir">epochs</strong>=150, <br/>                          verbose=1, <br/>                          <strong class="nl ir">batch_size </strong>= 2**7, <br/>                          <strong class="nl ir">validation_data</strong>=(X_test_scaled,y_test))</span><span id="79c1" class="np mo iq nl b gy nu nr l ns nt">#mean predictions<br/>dl_mean_prediction  = <strong class="nl ir">model_mean.predict</strong>(X_test_scaled).reshape(-1)<br/>dl_mean_prediction</span><span id="5e06" class="np mo iq nl b gy nu nr l ns nt">#stand deviation predictions<br/>dl_sd_prediction = <strong class="nl ir">model_sd.predict</strong>(X_test_scaled).reshape(-1)<br/>dl_sd_prediction</span></pre><h1 id="61ac" class="mn mo iq bd mp mq nx ms mt mu ny mw mx jw nz jx mz jz oa ka nb kc ob kd nd ne bi translated">韵律学</h1><p id="8464" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">下面描述的四个指标是研究中最常用的指标。基于预测区间直接估计的方法，如分位数回归或共形分位数回归，通常使用覆盖率和区间长度(锐度)指标(<a class="ae kv" href="https://arxiv.org/abs/1905.03222" rel="noopener ugc nofollow" target="_blank">论文</a>)，估计条件分布的方法使用负对数似然(NLL) ( <a class="ae kv" href="https://research.yandex.com/publications/323" rel="noopener ugc nofollow" target="_blank">论文1 </a>、<a class="ae kv" href="https://arxiv.org/abs/1910.03225v1" rel="noopener ugc nofollow" target="_blank">论文2 </a>)或CRPS ( <a class="ae kv" href="https://journals.ametsoc.org/view/journals/wefo/15/5/1520-0434_2000_015_0559_dotcrp_2_0_co_2.xml" rel="noopener ugc nofollow" target="_blank">论文</a>)</p><h2 id="25f5" class="np mo iq bd mp oc od dn mt oe of dp mx lf og oh mz lj oi oj nb ln ok ol nd om bi translated">有效性/覆盖范围</h2><p id="1fb5" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">对于某个水平为0 &lt; α &lt; 1的分位数预测，我们期望覆盖(100*α)%的观察值。例如，如果α是0.8，我们期望分位数预测覆盖80%的观察值。</p><p id="8706" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了更直观，让我们使用正态分布生成一些数据，并检查某个分位数级别覆盖的数据比例，比如10%或α为0.1</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="1ec4" class="np mo iq nl b gy nq nr l ns nt">r = stats.norm.rvs(size=1000)</span></pre><p id="66a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们需要找出10%分位数的Z值:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="7bb4" class="np mo iq nl b gy nq nr l ns nt">quantile = 0.1<br/>percent_point = stats.norm.ppf(q = quantile)<br/>#-1.281552</span></pre><p id="82ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们找出10%分位数覆盖生成的观察值的情况的经验比例:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="aced" class="np mo iq nl b gy nq nr l ns nt">len(r[r &lt; percent_point])/len(r)<br/>#10.5%</span></pre><p id="56ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">无效条件分位数的一个明显标志是经验分位数偏离名义分位数太多。</p><p id="4409" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们计算LightGBM分位数回归的有效性:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="1bdd" class="np mo iq nl b gy nq nr l ns nt">empirical_quantiles = []<br/>for quantile in <strong class="nl ir">quantiles</strong>:<br/>    empirical = (<strong class="nl ir">quantile_predictions[quantile] &gt;= y_test</strong>).mean()<br/>    empirical_quantiles.append(empirical)</span></pre><p id="8ddb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">并将结果可视化:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="5ff4" class="np mo iq nl b gy nq nr l ns nt">plt.figure(figsize=(16, 10))<br/>sns.set_context("notebook", font_scale=2)<br/>sns.lineplot(x = quantiles, y = quantiles, color = "magenta", linestyle='--', linewidth=3, label = "ideal")<br/>sns.lineplot(x = quantiles, y = empirical_quantiles, color = "black", linestyle = "dashdot", linewidth=3, label = "observed")<br/>sns.scatterplot(x = quantiles, y = empirical_quantiles, marker="o", s = 150)<br/>plt.legend()<br/>plt.xlabel("True Quantile")<br/>plt.ylabel("Empirical Quantile")<br/>_ = plt.title("Reliability diagram: assessment of quantile predictions")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/04ce4eba6e966b57ae12ccbd17206e7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2HBTdljQXEvDLoT8CQUyFA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="ec54" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">经验分位数(黑线)非常接近理想分位数(洋红色线)，这表明LightGBM生成的分位数预测是有效的。</p><h2 id="e651" class="np mo iq bd mp oc od dn mt oe of dp mx lf og oh mz lj oi oj nb ln ok ol nd om bi translated">清晰度/间隔长度</h2><p id="5002" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">完美的概率预测应该为单个值估计100%的概率。实际上，条件分布将为每个可能的结果分配不同的概率。锐度是预测密度紧密程度的量度。区间长度越窄，我们对预测就越有信心。</p><p id="da8a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">锐度计算的程序如下:</p><ul class=""><li id="d1cf" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">选择用于评估的预测间隔</li><li id="bb61" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">迭代预测每个观察值的平均值和标准差(NGBoost和神经网络)</li><li id="3a00" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">对于测试集中的每个观察值，计算区间的边界</li><li id="e248" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">计算间隔的长度</li><li id="9a93" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">计算所有间隔的平均值</li></ul><p id="ea7d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了更直观，让我们举两个预测的例子:</p><ul class=""><li id="79e3" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">预测1:均值= 0.5，标准差= 0.08</li><li id="93c6" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">预测2:均值= 0.5，标准差= 0.15</li></ul><p id="e69e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">并比较它们对应的覆盖60%概率的预测区间的宽度:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/ca59127e3e86da298f225e2dc42317c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E5rRwFQX1eiaP_WqAXWegg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="8490" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">绿色虚线是间隔的边界。60%的概率集中在两条线之间。第二预测比第一预测具有更高的预测不确定性，第一预测的区间更窄。</p><p id="fb6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">知道了分布的参数，我们可以使用scipy包计算区间宽度:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="b07f" class="np mo iq nl b gy nq nr l ns nt">probability = 0.6<br/>interval = <strong class="nl ir">stats.norm.interval</strong>(probability, loc = 0.5, scale=0.08)<br/>interval<br/>#(0.43267030131416684, 0.5673296986858332)</span><span id="b4df" class="np mo iq nl b gy nu nr l ns nt">width = interval[1] - interval[0]<br/>#0.135</span></pre><p id="fcf3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在分位数回归的情况下，过程略有不同。我们有每个分位数的预测，因此为了计算对应于60%的间隔宽度，我们取20%和80%的两个分位数，并使用预测中的差异计算这两个分位数之间的间隔。</p><p id="5dbc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们可以在不同的时间间隔使用清晰度指标来比较三种模型:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/9e18af04d46a036c50411ec070d508d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zg4dnO-QShN86vpeDInB-A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="95f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随着区间覆盖率从60%增加到90%，LightGBM分位数回归的平均预测区间以比其他算法高得多的速度增加。对于非常高的区间覆盖率(低分位数水平)，<a class="ae kv" href="https://arxiv.org/abs/2107.00363v2" rel="noopener ugc nofollow" target="_blank">分位数回归器可能不会给出最佳结果</a>。NGBoost在预测密度方面表现出最高的可信度。</p><p id="4f91" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">也有可能某个算法对预测很有信心，但仍然是错误的。我们如何衡量这一点？我们可以计算不在给定区间的预测边界内的真实观测值的比例。</p><p id="6b2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">过程与上面几乎相同，但是我们不是计算平均间隔宽度，而是计算间隔内缺失观测值的数量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/b09e89c26550f2e72baa4729649b7c96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nXJCFEeGKQ2l1j6vKyEZwg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="69df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随着时间间隔的增加，缺失观测值的比例下降，但是正如我们所看到的，LightGBM分位数回归具有最多的有偏观测值，其次是NGBoost。</p><h2 id="38fd" class="np mo iq bd mp oc od dn mt oe of dp mx lf og oh mz lj oi oj nb ln ok ol nd om bi translated">负对数似然</h2><blockquote class="or os ot"><p id="a271" class="kw kx ou ky b kz la jr lb lc ld ju le ov lg lh li ow lk ll lm ox lo lp lq lr ij bi translated">似然函数将观测数据的联合概率描述为所选统计模型参数的函数</p></blockquote><p id="44ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该指标适用于生成条件概率分布的算法。NGBoost和回归神经网络都使用NLL作为损失函数。</p><p id="bd6a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正态分布的NLL定义为正态分布概率密度函数的负对数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oy"><img src="../Images/392040b7574b0a222abe1141bc6b507f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uqszUelouR175CP94ZcBSA.png"/></div></div></figure><p id="a507" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用(正态)分布的参数在每次观察时评估NLL。为了比较算法之间的NLL，我们对所有观测值的NLL进行平均。(平均)NLL越低，拟合越好。</p><p id="ed98" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是回归神经网络和NGBoost的第一次观察及其相应NLL的预测分布的直观比较:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oz"><img src="../Images/977f04a18216759518e68a3c9ebd02e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FQyMhOSRxkPCl1K_Y72lOA.png"/></div></div></figure><p id="15e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看一个例子。观测值为4，正态分布的均值和标准差为3和1。这个观察的NLL是什么？</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="445f" class="np mo iq nl b gy nq nr l ns nt">#first let's calculate the likelihood</span><span id="371e" class="np mo iq nl b gy nu nr l ns nt">#using the formula<br/>L1 = (1/np.sqrt(2 * np.pi * 1**2)) * np.exp ( - (4 - 3)**2/(2 * 1**2))</span><span id="ada1" class="np mo iq nl b gy nu nr l ns nt">#using the scipy norm pdf function<br/>L2 = stats.norm.pdf(4, loc = 3, scale = 1)<br/>#0.2419707</span><span id="3c38" class="np mo iq nl b gy nu nr l ns nt">assert(L1 == L2)</span><span id="2aa7" class="np mo iq nl b gy nu nr l ns nt">#NLL<br/>-np.log(L2)<br/>#1.418939</span></pre><p id="4212" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">NGBoost的NLL可以用几种方法计算:</p><p id="af4c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">NGBoost提供了函数<em class="ou"> pred_dist </em>返回一个对象，该函数提供了一个<em class="ou"> logpdf </em>函数来计算每个观察的对数似然性。得到NLL只是否定结果。最终NLL是单个NLL的平均值</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="3ed7" class="np mo iq nl b gy nq nr l ns nt">ngb_dist_pred = <strong class="nl ir">ngb.pred_dist</strong>(X_test)<br/>nll_ngboost = <strong class="nl ir">-ngb_dist_pred.logpdf</strong>(y_test)<br/><strong class="nl ir">nll_ngboost.mean()</strong><br/><strong class="nl ir">#-4.888</strong></span></pre><p id="63e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二种方法是从<em class="ou"> ngb_dist_pred </em>对象获取每个条件分布的参数，并手动计算NLLs。例如，可以如下提取对应于第一观察的条件分布的参数:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="5660" class="np mo iq nl b gy nq nr l ns nt"><strong class="nl ir">ngb_dist_pred[0].params</strong><br/>#{'loc': 0.47710276090295406, 'scale': 0.0022844303231773374}</span></pre><p id="c9df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有观测值的参数都可以这样提取:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="c9a3" class="np mo iq nl b gy nq nr l ns nt"><strong class="nl ir">ngb_dist_pred.params</strong><br/>#{'loc': array([0.477,...,1.517]),<br/>#'scale': array([0.002,...,0.002])}</span></pre><p id="608e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在回归神经网络的情况下，我们迭代所有观察值及其相应的分布参数:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="e646" class="np mo iq nl b gy nq nr l ns nt">#extract mean predictions<br/>dl_mean_prediction = <strong class="nl ir">model_mean.predict</strong>(X_test_scaled).reshape(-1)</span><span id="d39e" class="np mo iq nl b gy nu nr l ns nt">#extract standard deviations<br/>dl_sd_prediction = <strong class="nl ir">model_sd.predict</strong>(X_test_scaled).reshape(-1)</span><span id="76cd" class="np mo iq nl b gy nu nr l ns nt">#iterate over all observations<br/>nll_dl = []<br/><strong class="nl ir">for</strong> (<strong class="nl ir">true_mean, mean_temp, sd_temp</strong>) in zip(<strong class="nl ir">y_test</strong>, <br/>                                           <strong class="nl ir">dl_mean_prediction</strong>,<br/>                                           <strong class="nl ir">dl_sd_prediction</strong>):<br/>    nll_temp = <strong class="nl ir">-stats.norm.logpdf</strong>(<strong class="nl ir">true_mean</strong>, <br/>                                  <strong class="nl ir">loc </strong>= mean_temp, <br/>                                  <strong class="nl ir">scale </strong>= sd_temp)<br/>    nll_dl.append(nll_temp)</span><span id="f695" class="np mo iq nl b gy nu nr l ns nt"><strong class="nl ir">#NLL</strong><br/><strong class="nl ir">np.mean(nll_dl)</strong><br/>#<strong class="nl ir">-1.566</strong></span></pre><p id="9dbb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就NLL而言，NGBoost得到的值(-4.888)低于回归神经网络(-1.566)，因此我们可以得出结论，NGBoost具有更好的拟合性。</p><h2 id="6a3f" class="np mo iq bd mp oc od dn mt oe of dp mx lf og oh mz lj oi oj nb ln ok ol nd om bi translated">连续分级概率得分</h2><blockquote class="or os ot"><p id="4299" class="kw kx ou ky b kz la jr lb lc ld ju le ov lg lh li ow lk ll lm ox lo lp lq lr ij bi translated">连续排序概率得分，也称为CRPS，是一种在不知道数据真实分布的情况下，衡量建议分布如何逼近数据的得分</p></blockquote><p id="eea0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是<a class="ae kv" href="https://datumorphism.leima.is/cards/time-series/crps/" rel="noopener ugc nofollow" target="_blank">链接</a>，它很好地解释了CRPS背后的公式。简单地说，CRPS测量对应于观察值的预测分布和理想分布之间的平方距离。</p><p id="d428" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我用<em class="ou"> properscoring </em>软件包计算了CRPS</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="dcff" class="np mo iq nl b gy nq nr l ns nt">#<strong class="nl ir">Computes the CRPS of observations x relative to normally distributed with mean, mu, and standard deviation, sig.</strong></span><span id="63b6" class="np mo iq nl b gy nu nr l ns nt">import properscoring as ps<br/><strong class="nl ir">crps_ngboost </strong>= <strong class="nl ir">ps.crps_gaussian</strong>(y_test, <br/>                               ngb_dist_pred.params["loc"],<br/>                               ngb_dist_pred.params["scale"]).mean()<br/><strong class="nl ir">crps_dl </strong>= <strong class="nl ir">ps.crps_gaussian</strong>(y_test, <br/>                           dl_mean_prediction,  <br/>                           dl_sd_prediction).mean()</span><span id="1bc5" class="np mo iq nl b gy nu nr l ns nt">(crps_ngboost, crps_dl)<br/>#(0.001, 0.0266)</span></pre><p id="6dac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">NGBoost具有比回归神经网络小得多的CRPS，这再次表明NGBoost具有更好的适应性。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="161b" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">结论</h1><p id="f4de" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">在本文中，我介绍了评估回归问题中预测不确定性的四个指标。有效性和锐度最适合直接估计预测区间的方法，如分位数和保形回归。负对数似然和CRPS用于比较模拟条件分布的算法。</p><p id="6dee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">完整的代码可以从我的<a class="ae kv" href="https://github.com/slavakx/medium_posts" rel="noopener ugc nofollow" target="_blank"> Github repo </a>下载</p><p id="4638" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢阅读！</p></div></div>    
</body>
</html>