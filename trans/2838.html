<html>
<head>
<title>SHAP for Categorical Features</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类特征的SHAP</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/shap-for-categorical-features-7c63e6a554ea#2022-06-21">https://towardsdatascience.com/shap-for-categorical-features-7c63e6a554ea#2022-06-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="82ae" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">将已经用一键编码转换的分类特征的SHAP值相加</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7240e88734eff370c80271bd7b58246b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*a9t146fSa0rgG-Bj"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@kalineri?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">卡利内里</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="33b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">分类特征在用于模型之前需要进行转换。一键编码是实现这一点的常用方法:我们最终为每个类别提供一个二进制变量。这很好，直到开始理解使用SHAP的模型。每个二元变量都有自己的SHAP值。这使得很难理解原始分类特征的总体贡献。</p><p id="2fea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一种简单的方法是将每个二元变量的SHAP值相加。这可以解释为原始分类特征的SHAP值。为此，我们将带您浏览一下<strong class="lb iu"> Python代码</strong>。我们将会看到我们能够使用SHAP聚集图。然而，当涉及到理解分类特征的关系的本质时，这些是有限的。最后，我们向您展示如何使用箱线图来可视化SHAP值。</p><p id="c873" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你不熟悉SHAP或者python包，我建议你阅读下面的文章。我们深入探讨如何诠释SHAP价值观。我们还探索了本文中使用的一些聚合。</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/introduction-to-shap-with-python-d27edc23c454"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">Python SHAP简介</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">如何创造和解释SHAP情节:瀑布，力量，决定，SHAP和蜂群</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><p id="c229" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在处理分类变量时，还有一个替代解决方案。那就是使用CatBoost进行建模。您可以在下面的文章中找到这个解决方案。</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/shap-for-categorical-features-with-catboost-8315e14dac1"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">使用CatBoost实现分类特征的SHAP</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">避免对分类要素的SHAP值进行后处理</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mn l mj mk ml mh mm ks ly"/></div></div></a></div><h1 id="69c5" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">资料组</h1><p id="054d" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">为了演示分类特征的问题，我们将使用蘑菇分类数据集。您可以在图1 中看到这个数据集的快照。目标变量是蘑菇的<strong class="lb iu">类。也就是说蘑菇是有毒的(p)还是可食用的(e)。你可以在UCI的MLR 中找到这个数据集。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/27e2cd9fc1abced586a2bbf3218998d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PTxb58_c6miCxVOeLcgdgA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:蘑菇数据集快照(来源:作者)(数据集来源:<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/Mushroom" rel="noopener ugc nofollow" target="_blank"> UCI </a>)(许可证:CC BY 4.0)</p></figure><p id="5b82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于模型特征，我们有22个分类特征。对于每个功能，类别由一个字母表示。例如<strong class="lb iu">气味</strong>有9个独特的类别——杏仁(a)、茴香(l)、杂酚油(c)、鱼腥味(y)、恶臭(f)、霉味(m)、无(n)、刺鼻(p)、辛辣(s)。这是蘑菇的味道。</p><h1 id="76d9" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">系统模型化</h1><p id="f843" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">我们将带您浏览用于分析该数据集的代码，您可以在<a class="ae ky" href="https://github.com/conorosully/medium-articles/blob/master/src/interpretable%20ml/SHAP/SHAP_categorical.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到完整的脚本。首先，我们将使用下面的Python包。我们有一些处理和可视化数据的通用包(第2-4行)。我们使用<strong class="lb iu"> OneHotEncoder </strong>来转换分类特征(第6行)。我们使用<strong class="lb iu"> xgboost </strong>进行建模(第8行)。最后，我们使用<strong class="lb iu"> shap </strong>来理解我们的模型是如何工作的(第10行)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="f2c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们导入数据集(第2行)。我们需要一个数字目标变量，所以我们通过设置有毒的= 1和可食用的= 0来转换它(第6行)。我们还得到分类特征(第7行)。我们不使用<strong class="lb iu"> X_cat </strong>数据集进行建模，但它以后会派上用场。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="86ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要使用分类特征，我们还需要转换它们。我们从安装编码器开始(第2–3行)。然后，我们用它来转换我们的分类特征(第6行)。对于每个分类特征，每个类别都有一个二元特征。我们为每个二进制特性创建特性名称(第9行到第10行)。最后，我们将这些放在一起创建我们的特征矩阵(第12行)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="404a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们有117个特征。您可以在<strong class="lb iu">图2 </strong>中看到特征矩阵的快照。例如，你可以看到<strong class="lb iu">帽形</strong>现在已经被转换成6个二元变量。功能名称末尾的字母来自原始功能的类别。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/94129e15849f4f17646ba5ab84649624.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l_CzUb_XxRo7rN_SHkLA6A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2: X特征矩阵(来源:作者)</p></figure><p id="1d55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用这个特征矩阵训练一个模型(第2-5行)。我们正在使用一个<strong class="lb iu"> XGBClassifier。</strong>XGBoost模型由10棵树组成，每棵树的最大深度为2。该模型在训练集上的准确率为97.7%。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><h1 id="ace6" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">标准SHAP值</h1><p id="8c4d" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">此时，我们想了解模型是如何做出这些预测的。我们从计算SHAP值开始(第2-3行)。然后，我们使用瀑布图(第6行)可视化第一次预测的SHAP值。你可以在<strong class="lb iu">图3 </strong>中看到这个情节。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="a560" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以看到每个二元要素都有自己的SHAP值。以气味为例。它在瀑布图中出现了4次。odor_n = 0的事实增加了蘑菇有毒的可能性。同时，odor_a = 1，odor_f = 0，odor_I = 0都降低了概率。还不清楚蘑菇气味的总体贡献是什么。在下一节中，我们将会看到，当我们把所有的个人贡献加在一起时，情况就变得很清楚了。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/fccf5ad84f55063f5338c6ddd6485057.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sa-evvKSjwnKtUV32k05gw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:第一次观察的瀑布图(来源:作者)</p></figure><h1 id="f226" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">分类特征的SHAP</h1><p id="eb27" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">让我们从探索<strong class="lb iu"> shap_values </strong>对象开始。我们在下面的代码中打印该对象。您可以在下面的输出中看到，它由3个组件组成。我们有每个预测的SHAP值(<strong class="lb iu">值</strong>)。<strong class="lb iu">数据</strong>给出二进制特征的值。每个预测也将具有相同的基值(<strong class="lb iu"> base_values </strong>)。这是平均预测对数概率。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/0a392b1457fa8cf6242a03594d16f93e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CVAOawouqP7YxMGRi2ToHg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(来源:作者)</p></figure><p id="e020" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以通过打印下面的内容来仔细查看第一个预测的SHAP值。有117个值。每个二进制变量一个。SHAP值与X特征矩阵的顺序相同。记住，第一个分类特征<strong class="lb iu">帽形</strong>有6个类别。这意味着前6个SHAP值对应于该特征的二进制特征。接下来的4个对应于<strong class="lb iu">盖面</strong>特征等等。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/083b804318277b5bb6440c76ac865f83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FYGFgN39JNW0EMZuE3KeVg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(来源:作者)</p></figure><p id="689a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们希望将每个分类特征的SHAP值相加。为此，我们首先创建<strong class="lb iu"> n_categories </strong>数组。这包含每个分类变量的唯一类别数。数组中的第一个数字对于<strong class="lb iu">帽形</strong>将是6，然后对于<strong class="lb iu">帽面</strong>将是4，依此类推…</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="caeb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用<strong class="lb iu"> n_categories </strong>来分割SHAP值数组(第5行)。我们得到了一个子列表。然后，我们对每个子列表中的值求和(第8行)。这样我们就从117个SHAP值变成了22个SHAP值。我们对<strong class="lb iu">形状值</strong>对象(第2行)中的每一个观察都这样做。对于每次迭代，我们将求和的shap值添加到<strong class="lb iu"> new_shap_values </strong>数组中(第10行)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="6eb9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们需要做的就是用新值替换原来的SHAP值(第2行)。我们还用原始分类特征的类别字母替换二元特征数据(第5–6行)。最后，我们用原始的特性名称替换二进制特性名称(第9行)。分别以数组和列表的形式传递这些新值非常重要。这些是<strong class="lb iu">形状值</strong>对象使用的数据类型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="abb0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更新后的<strong class="lb iu">形状值</strong>对象可以像原始对象一样使用。在下面的代码中，我们为第一次观察绘制了瀑布。您会注意到这段代码和以前完全一样。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="45a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以在<strong class="lb iu">图4 </strong>中看到输出。我们现在有22个SHAP值。您还可以看到左侧的特征值已被类别标签所取代。我们之前讨论过气味特征。您现在可以清楚地看到这个特性的总体贡献。它将对数概率降低了<strong class="lb iu"> 0.29 </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/57ac5423fa50834322b4042668406c93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VVFEcaWTsqw6mXkeqGdLIA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4:更新SHAP值后的第一次观测的瀑布图(来源:作者)</p></figure><p id="6846" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的图中，我们有气味= a。这告诉我们蘑菇有“杏仁”的气味。我们应该避免把这个情节理解为“杏仁味降低了对数几率”。我们将多个SHAP值加在一起。因此，我们应该把它解释为“杏仁气味和其他气味的缺乏降低了对数几率”。例如，查看第一个瀑布图，缺少“恶臭”气味(odor_f = 0)也降低了对数几率。</p><p id="0682" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们继续讨论这些新SHAP值的集合之前，有必要讨论一些理论。我们能够用SHAP值做到这一点的原因是因为它们的可加性。也就是平均预测值(<strong class="lb iu">E[f(x)】</strong>)加上所有的SHAP值等于实际预测值(<strong class="lb iu"> f(x) </strong>)。通过把一些SHAP值加在一起，我们不会干扰这个性质。这就是为什么<strong class="lb iu"> f(x) = -2.444 </strong>在<strong class="lb iu">图3 </strong>和<strong class="lb iu">图4 </strong>中都是一样的。</p><h2 id="b2f1" class="nt mp it bd mq nu nv dn mu nw nx dp my li ny nz na lm oa ob nc lq oc od ne oe bi translated">卑鄙的SHAP</h2><p id="56c9" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">与瀑布图一样，我们可以像使用原始SHAP值一样使用SHAP聚合。例如，我们在下面的代码中使用平均SHAP图。查看<strong class="lb iu">图5 </strong>，我们可以使用该图来突出重要的分类特征。例如，我们可以看到，气味往往具有较大的正/负SHAP值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/f9f944ab2db5285256e221e580f12861.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nILaxYvsimzugvwCiY7G6g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5:平均SHAP(来源:作者)</p></figure><h2 id="c402" class="nt mp it bd mq nu nv dn mu nw nx dp my li ny nz na lm oa ob nc lq oc od ne oe bi translated">蜂群</h2><p id="35e3" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">另一种常见的聚合是蜂群图。对于连续变量，此图很有用，因为它可以帮助解释关系的性质。我们可以看到SHAP值是如何与特征值相关联的。但是，对于分类特征，我们已经用标签替换了特征值。因此，在<strong class="lb iu">图6 </strong>中，您可以看到SHAP值都被赋予了相同的颜色。我们需要创造自己的情节来理解这些关系的本质。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/a4505a8d664f78235f0276b24d4b747c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JfqNtgjboG4rXMeT_mpaWQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6:分类变量的蜂群(来源:作者)</p></figure><h2 id="af1c" class="nt mp it bd mq nu nv dn mu nw nx dp my li ny nz na lm oa ob nc lq oc od ne oe bi translated">SHAP箱线图</h2><p id="ebd1" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">一种方法是使用SHAP值的箱线图。在<strong class="lb iu">图7 </strong>中，您可以看到一个气味特征。这里，我们根据气味类别对气味特征的SHAP值进行了分组。你可以看到，臭味导致更高的SHAP值。这些蘑菇很可能有毒。请不要吃任何难闻的蘑菇！同样，没有气味的蘑菇更容易食用。一条橙色的线意味着这些蘑菇的所有SHAP值都是相同的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/58d2259de709c825c17a422c1c4d9943.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*KmVY-EESHynu1RD65LTZ3Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7:气味SHAP值的箱线图(来源:作者)</p></figure><p id="3516" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用下面的代码创建这个箱线图。我们从获得气味SHAP值开始(第2行)。记住这些是更新值。对于每个预测，气味特征只有一个SHAP值。我们还得到气味类别标签(第3行)。我们根据这些标签分割SHAP值(第6-11行)。最近，我们用这些值为每一种气味类别绘制了一个箱线图(第27-32行)。为了使图表更容易理解，我们还用完整的类别名称替换了字母(第14-24行)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="6272" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实际上，很可能只有少数特征是绝对的。您将需要更新上述过程，以便只对分类进行求和。你也可以想出自己的方式来想象这些特征之间的关系。如果你有其他的方法，我很乐意在评论中听到。</p><p id="632d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我也有兴趣了解特性依赖将如何影响这个分析。根据定义，变换后的二元特征将是相关的。这可能会影响SHAP值的计算。我们使用<strong class="lb iu">树形图</strong>来估计SHAP值。我的理解是，这些不像<strong class="lb iu"> KernelSHAP </strong>那样受到依赖的影响。我很有兴趣在评论中听到你的想法。</p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><p id="e6f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望这篇文章对你有帮助！你可以成为我的<a class="ae ky" href="https://conorosullyds.medium.com/membership" rel="noopener"> <strong class="lb iu">推荐会员</strong> </a> <strong class="lb iu">来支持我。你可以访问medium上的所有文章，我可以得到你的部分费用。</strong></p><div class="lv lw gp gr lx ly"><a href="https://conorosullyds.medium.com/membership" rel="noopener follow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">通过我的推荐链接加入Medium康纳·奥沙利文</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">conorosullyds.medium.com</p></div></div><div class="mh l"><div class="op l mj mk ml mh mm ks ly"/></div></div></a></div><p id="ff02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在|<a class="ae ky" href="https://twitter.com/conorosullyDS" rel="noopener ugc nofollow" target="_blank">Twitter</a>|<a class="ae ky" href="https://www.youtube.com/channel/UChsoWqJbEjBwrn00Zvghi4w" rel="noopener ugc nofollow" target="_blank">YouTube</a>|<a class="ae ky" href="https://mailchi.mp/aa82a5ce1dc0/signup" rel="noopener ugc nofollow" target="_blank">时事通讯</a>上找到我——注册免费参加<a class="ae ky" href="https://adataodyssey.com/courses/shap-with-python/" rel="noopener ugc nofollow" target="_blank"> Python SHAP课程</a></p><h2 id="3d0a" class="nt mp it bd mq nu nv dn mu nw nx dp my li ny nz na lm oa ob nc lq oc od ne oe bi translated">参考</h2><p id="1177" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">南伦德伯格，<em class="oq"> SHAP蟒包</em> (2021) 【T2，<a class="ae ky" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"/></p><p id="d78d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">南伦德伯格和s .李，<em class="oq">解释模型预测的统一方法</em> (2017年)，<a class="ae ky" href="https://arxiv.org/pdf/1705.07874.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1705.07874.pdf</a></p></div></div>    
</body>
</html>