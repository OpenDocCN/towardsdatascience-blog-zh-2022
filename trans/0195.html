<html>
<head>
<title>Why Do We Use Gradient Descent for Optimization in Machine Learning?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中为什么要用梯度下降进行优化？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-do-we-use-gradient-descent-for-optimization-in-machine-learning-19428760a7ed#2022-02-08">https://towardsdatascience.com/why-do-we-use-gradient-descent-for-optimization-in-machine-learning-19428760a7ed#2022-02-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="293d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">选择计算优化技术</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3525a0c7a0203b3cf6efb210872116f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VuC9pI9B7UNQsuFl"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">马里乌斯·哈克斯塔德在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="7639" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我注意到，大多数机器学习指南都直接进入梯度下降，而没有探索为什么我们实际上使用这种方法来优化我们的模型。从数学的角度来看，这似乎很奇怪。梯度下降依赖于梯度的知识来“滑下斜坡”，但在学校，当我们想找到一个函数的最小值，其梯度可以计算，我们通常不应用梯度下降。相反，我们只是通过直接求解梯度为 0 时，解析地找到最小值。那么，为什么当我们转向计算机领域时，我们会突然改变策略并使用迭代技术呢？此外，为什么我们使用梯度下降，而不是，例如，牛顿下降，其他流行的迭代优化技术？</p><h1 id="24e7" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">牛顿下降</h1><p id="d864" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">首先给不熟悉的人介绍一下牛顿下降法。这扩展了对函数使用线性近似的思想。考虑寻找<strong class="ky ir"> x </strong>使得<strong class="ky ir"> f(x)=0 </strong>的问题。让我们构建一个线性近似:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/b806261ca93419bdfa24c6ef5905302d.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*kyX4oaz6hRKer7E8JhyqUQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片:线性函数逼近</p></figure><p id="ebff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们挑选<strong class="ky ir"> x_k </strong>和<strong class="ky ir"> x_{k+1} </strong>来求解<strong class="ky ir"> f(x)=0 </strong>的线性近似:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/331d58970b481b2d88844a153aaaff07.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*EgIRpPpv4TAIOU-wBhJSsg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="b769" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们需要切换到优化框架。我们想要一个算法，收敛到梯度为 0 的驻点。所以如果我们用函数项代替梯度项，我们可以得到最终的牛顿下降公式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/b26582e8b9fb916a1aa7aff414322f5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*xns4F51nIELLb_qi3Nv7Ag.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者:牛顿后裔</p></figure><h1 id="46e4" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">梯度下降与分析优化</h1><p id="212a" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">相比解析优化，我们使用梯度下降优化模型的主要原因是它通常更快！解析解通常需要复杂的线性代数运算，例如矩阵求逆，这在大规模计算时计算成本非常高，并且可能在数值上不稳定。</p><p id="777b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了演示这一点，我们将使用简单的 OLS 线性回归作为例子。在这种情况下，给定输入观测值矩阵、<strong class="ky ir"> X </strong>和目标向量<strong class="ky ir"> y </strong>，线性 OLS 解为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/61d1f5a1debaf7f7e9b0482b1845333d.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*JuAYUq9N96hb9x98HNFUOA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片:线性 OLS 解析解</p></figure><p id="7229" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不过这里要注意的关键是方阵求逆，它的时间复杂度为<strong class="ky ir"> O(n ) </strong>，其中 n 是特征的数量<strong class="ky ir">。</strong>因此，这对于机器学习中经常遇到的更大的问题来说并不是很适用👎。</p><p id="9009" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同时，计算梯度下降中使用的梯度要简单得多，计算机可以非常快速地运行这些计算数千次，即使是在更大的范围内👍。</p><h1 id="3def" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">梯度下降 vs 牛顿下降</h1><p id="f287" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在，让我们继续比较梯度下降和牛顿下降，这两种算法都是迭代优化算法。起初，牛顿血统看起来占了上风:</p><ol class=""><li id="79b4" class="mt mu iq ky b kz la lc ld lf mv lj mw ln mx lr my mz na nb bi translated">👍<strong class="ky ir">更快的收敛速度</strong>:与梯度下降中的<em class="nc">次线性</em>收敛速度相比，牛顿下降具有<em class="nc">二次</em>收敛速度。这意味着与梯度下降相比，它需要更少的步骤来收敛。</li><li id="7b8e" class="mt mu iq ky b kz nd lc ne lf nf lj ng ln nh lr my mz na nb bi translated">👍<strong class="ky ir">更快的二次收敛速度</strong>:此外，在二次函数中(例如一直使用的线性 MSE)，牛顿收敛改进为一步下降！！</li></ol><p id="6b27" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，有几个主要缺陷阻碍了它的使用:</p><ol class=""><li id="f9b7" class="mt mu iq ky b kz la lc ld lf mv lj mw ln mx lr my mz na nb bi translated"><strong class="ky ir">👎没有可调的学习率</strong>:再看一下更新等式，你会注意到没有学习率项。事实上，牛顿下降假设学习率为 1，这意味着由于不灵活的下降步骤，没有保证全局收敛，梯度下降中没有发现这个问题。这显然是一个大问题，因为模型初始化现在会影响收敛。这也意味着下降模式不太清晰。虽然梯度下降会逐渐收敛到一个值越来越小的解，但牛顿下降往往会跳跃很多，这使得评估进展更加困难。</li><li id="0683" class="mt mu iq ky b kz nd lc ne lf nf lj ng ln nh lr my mz na nb bi translated"><strong class="ky ir">👎逆 Hessian 计算</strong>:牛顿更新方程使用逆二阶偏导数矩阵，称为 Hessian。然而，这并不能保证存在！作为对称矩阵，对于 Hessian 的逆的存在，也必须是<em class="nc">正定</em>。但是即使它确实存在，大规模的计算也是极其昂贵的！这意味着即使牛顿下降需要更少的步骤来收敛，梯度下降对于机器学习中的用例来说通常仍然更快。</li></ol></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><p id="59da" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这激发了更多使用梯度下降作为机器学习中优化的基础。</p><p id="0ef4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您觉得这篇文章有用，请考虑:</p><ul class=""><li id="0e19" class="mt mu iq ky b kz la lc ld lf mv lj mw ln mx lr np mz na nb bi translated">跟踪我🙌</li><li id="78ec" class="mt mu iq ky b kz nd lc ne lf nf lj ng ln nh lr np mz na nb bi translated"><a class="ae kv" href="https://medium.com/subscribe/@rohan.tangri" rel="noopener"> <strong class="ky ir">订阅我的电子邮件通知</strong> </a>永不错过上传📧</li><li id="e48c" class="mt mu iq ky b kz nd lc ne lf nf lj ng ln nh lr np mz na nb bi translated">使用我的媒介<a class="ae kv" href="https://medium.com/@rohan.tangri/membership" rel="noopener"> <strong class="ky ir">推荐链接</strong> </a> <strong class="ky ir"> </strong>直接支持我并获得无限量的优质文章🤗</li></ul><p id="a534" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">推广的方式，我真的希望你觉得这篇文章有趣，让我知道你的想法！！</p></div></div>    
</body>
</html>