<html>
<head>
<title>Logistic Regression from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-from-scratch-870f0163bfc9#2022-02-18">https://towardsdatascience.com/logistic-regression-from-scratch-870f0163bfc9#2022-02-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="c3d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">逻辑模型是机器学习和许多社会科学领域的基础。在这篇文章中，我解释了如何从基本原理中推导出<strong class="jp ir">逻辑模型。</strong>因为我喜欢边做边学，所以我展示了如何使用<strong class="jp ir">梯度下降</strong>或<strong class="jp ir">牛顿-拉夫森</strong>算法来估计它的参数。在现实生活的应用方面，我们将使用关于 NBA 球员的数据来看看是什么因素影响了投篮的成功。</p><p id="d938" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">TL；博士:从基本原则推导+如何训练+ NBA 应用</strong></p><p id="ad41" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章的 GitHub 库可以在<a class="ae kl" href="https://github.com/JulienPascal/LogisticRegression" rel="noopener ugc nofollow" target="_blank">这里</a>找到。如果你喜欢这篇文章，请不要犹豫，关注我。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/abd255e07f11a0765102f890bde0d5a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zb9fviOCKckDlN5wtynW7Q.jpeg"/></div></div></figure><p id="2db6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">照片由<a class="ae kl" href="https://www.pexels.com/@serjosoza?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">塞尔吉奥·索萨</strong> </a>拍摄自<a class="ae kl" href="https://www.pexels.com/photo/top-view-photo-of-basketball-court-2291004/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">佩克斯</strong> </a></p><p id="a2a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">—</p><h1 id="dc1a" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">一.理论</h1><h2 id="9841" class="lw kz iq bd la lx ly dn le lz ma dp li jy mb mc lm kc md me lq kg mf mg lu mh bi translated">逻辑模型</h2><p id="552c" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">结果变量 yi 要么是 1(“赢”)，要么是 0(“输”)。逻辑模型假设获胜的概率由逻辑函数给出(因此得名):</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/ff87e9bfa9b57103e8990619fba06c71.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*4QdO1bl_iDuWlmnyGxzYnQ.png"/></div></figure><p id="8188" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随着</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/1a02fc56b56bec8807b28e9cab396850.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*t6Q3UbYRDkSbdp7LmuSubg.png"/></div></figure><p id="cdaf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输的概率是 1 减去赢的概率:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/ea4a55be5c3998826ff6c8b17adb4077.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*yRytxZFQxxHafb5WEVHBig.png"/></div></figure><h2 id="064d" class="lw kz iq bd la lx ly dn le lz ma dp li jy mb mc lm kc md me lq kg mf mg lu mh bi translated">潜在变量公式</h2><p id="d4c1" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">解释逻辑模型的一个强有力的方法是将其视为潜在变量模型的结果。不可观察的潜在变量 zi 线性依赖于 xi 加上噪声项εi:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/bdd9eef90c5bd70e6b9a270d69edf808.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/format:webp/1*mn7-HUq_PSj-wdotLhINEA.png"/></div></figure><p id="ec3d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们只观察 yi，当 zi 严格正时等于 1，否则等于 0。如果误差项按照<a class="ae kl" href="https://en.wikipedia.org/wiki/Logistic_distribution" rel="noopener ugc nofollow" target="_blank">逻辑分布</a>分布，我们最终得到上述逻辑模型。如果误差项呈正态分布，则该模型为<a class="ae kl" href="https://en.wikipedia.org/wiki/Probit_model" rel="noopener ugc nofollow" target="_blank">概率单位模型</a>。要了解这一点，只需将潜在变量的概率表示为大于 0:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/c5272bd8065ab2c1076ea4e1f4a16343.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*g5cRLuyLKOp7Pyvmd0fFNA.png"/></div></figure><p id="2c18" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中最后一行来自使用对数分布的<a class="ae kl" href="https://en.wikipedia.org/wiki/Logistic_distribution" rel="noopener ugc nofollow" target="_blank"> cdf 表达式，平均值为零，标度参数等于 1。</a></p><h2 id="948b" class="lw kz iq bd la lx ly dn le lz ma dp li jy mb mc lm kc md me lq kg mf mg lu mh bi translated">系数怎么读？</h2><p id="153a" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">我们如何从逻辑模型中读取系数？xij(Xi 的第 j 个分量)的变化对 yi=1 的概率的边际影响由下式给出:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ms"><img src="../Images/2bf7b8e7f3034acc1b3adb1a7b67df92.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*8-ZESlXY8bUv1JWn24yxMA.png"/></div></div></figure><p id="5fb2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一个观察是边际效应取决于 xi，不像线性回归模型。第二个观察结果是，前两项总是正的，所以我们有这样的解释，如果θj 是正的，xi 的第 j 个分量的增加导致获得成功的更大概率(保持其他一切不变)。</p><p id="2651" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从逻辑模型中读取结果的另一种方式是认识到它意味着奇数比的对数是线性的:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/06e56e35d8a6857e24ce9feab7620c55.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*maDCktawLmLvSuEsWMwTJA.png"/></div></figure><p id="9482" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">实际上，如果系数θj 等于 0.1，这意味着 xij 增加一个单位，则“获胜”(输出为 1)的<strong class="jp ir">相对</strong>概率增加约 10%。</p><p id="0801" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">—</p><h1 id="d80c" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">二。寻找系数</h1><h2 id="c377" class="lw kz iq bd la lx ly dn le lz ma dp li jy mb mc lm kc md me lq kg mf mg lu mh bi translated">对数似然函数</h2><p id="5649" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">到目前为止，一切顺利。但是我们如何找到θ呢？一个“自然”的标准是找到θ的值，使得<strong class="jp ir">最大化观察样本</strong>的概率。在商业中，这个过程被称为<a class="ae kl" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank">最大似然估计</a>。让我们假设样本是<a class="ae kl" href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables" rel="noopener ugc nofollow" target="_blank">I . I . d .</a>。如果 i.i.d .假设成立，观察到样本的概率(易，)是每次观察到的概率的乘积。<strong class="jp ir">最大化对数似然比最大化似然更方便，对数似然将概率的乘积转换为总和:</strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/e3162b737067fcf8a8c1b17e021d4665.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*xxHc4MylrHjNB6-tyDhFBQ.png"/></div></figure><p id="092a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">观察到 yi(yi = 1 或 0)的概率可以简洁地写成</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/2898572ca9829b424bc05244f6d090c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*BWzJMus5oM0VQnkSMBIjdg.png"/></div></figure><p id="54ae" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，对数似然函数写道:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/dfad2ffea71410eeb8608085dd66b088.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*BsFLX68qsBpR1UhwzShtyQ.png"/></div></figure><h2 id="a053" class="lw kz iq bd la lx ly dn le lz ma dp li jy mb mc lm kc md me lq kg mf mg lu mh bi translated">最大似然估计</h2><p id="03aa" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">对 f(yi|xi，θ)对参数θ求导得到</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mx"><img src="../Images/57bb5d4b36ba668f8c6045b22effe01e.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*9P7keJyAs9p7U9JBHhcfJA.png"/></div></div></figure><p id="d36f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对数似然函数相对于θ 的<strong class="jp ir">导数为</strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/c3fcca411779c017c528ad3bb8e43127.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*MCElRvbKoyYU0GT1c7NRiw.png"/></div></figure><h2 id="5851" class="lw kz iq bd la lx ly dn le lz ma dp li jy mb mc lm kc md me lq kg mf mg lu mh bi translated">梯度下降</h2><p id="4115" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">既然知道了对数似然函数关于θ的导数，我们可以使用<strong class="jp ir">古老的梯度下降</strong>算法:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/6d5d4afe0acee4ca7b5d6b96fe3ee5b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*duc8AKZVcJlzzTwoclIEeQ.png"/></div></figure><p id="f345" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>算法是一种寻找函数极小值的迭代程序。在每一步，<strong class="jp ir">该算法朝着最陡下降方向</strong>前进一步，步长为γ。</p><h2 id="9dc3" class="lw kz iq bd la lx ly dn le lz ma dp li jy mb mc lm kc md me lq kg mf mg lu mh bi translated">牛顿-拉夫森方法</h2><p id="17b3" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">我们能做得比梯度下降更好吗？用这个简单的模型，是的。使用牛顿-拉夫森方法可以大幅提高速度。粗略地说，<strong class="jp ir">牛顿-拉夫逊法是一种“智能”梯度下降法，它利用了对数似然 HL((易，)的 Hessian </strong>中包含的信息；θi)向右移向极小值。该迭代算法如下进行:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/b6bf52d07ebcd1a17394e8f7d3e310aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*7hP8eGwYId_eQKHau_KwCg.png"/></div></figure><p id="2a06" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下一张图显示了牛顿-拉夫森方法如何解决一维求根问题:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/7463b1756531fce0e07e591ff912a2ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/1*6Qj5chzIJBX6fnDHYzUfOg.gif"/></div></figure><p id="bb3a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">来源:https://en.wikipedia.org/wiki/Newton%27s_method<a class="ae kl" href="https://en.wikipedia.org/wiki/Newton%27s_method" rel="noopener ugc nofollow" target="_blank"/></p><p id="36f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">应该用梯度下降法还是牛顿-拉夫逊法？当计算 Hessian 简单快速时，你应该选择 Newton-Raphson，因为它收敛得更快。见下面引用自<a class="ae kl" href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization" rel="noopener ugc nofollow" target="_blank">维基百科关于牛顿-拉夫森的文章</a></p><blockquote class="nc nd ne"><p id="eb0f" class="jn jo nf jp b jq jr js jt ju jv jw jx ng jz ka kb nh kd ke kf ni kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="iq">在适用的地方</em> </strong> <em class="iq">，牛顿法</em> <strong class="jp ir"> <em class="iq">比梯度下降法</em> </strong> <em class="iq">向局部最大值或最小值收敛快得多。事实上，每个局部最小值都有一个邻域 N，这样，如果我们从 x0 ∈ N 开始，步长γ = 1 </em> <strong class="jp ir"> <em class="iq">的牛顿法二次收敛</em> </strong> <em class="iq">(如果 Hessian 是可逆的，并且 x 的一个 Lipschitz 连续函数在那个邻域中)。</em></p></blockquote><p id="f766" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于逻辑斯谛模型，牛顿-拉夫森算法很容易应用，因为存在一个闭合形式的 Hessian 公式:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/3fa6d2148fa53c8f9ea81b06d9d83bb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*SkqCWJzinFNHTw8k0RBbGQ.png"/></div></figure><p id="bd8a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">—</p><h1 id="f9c8" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">三。应用:NBA 球员</h1><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nk"><img src="../Images/95577bb96f566596345f5950a84f9981.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a46zMzt0Pxk4Skvqjlrs2w.jpeg"/></div></div></figure><p id="5e59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">照片由<a class="ae kl" href="https://www.pexels.com/@chbani-med-3235139?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> Chbani Med </strong> </a>发自<a class="ae kl" href="https://www.pexels.com/photo/silhouette-of-people-playing-basketball-during-sunset-4863981/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> Pexels </strong> </a></p><p id="fad5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">是什么让一个 NBA 球员成功？为了回答这个问题，我使用了 2014-2015 赛季期间拍摄的 NBA 照片的数据集。例如，它包含以下信息:</p><ul class=""><li id="b105" class="nl nm iq jp b jq jr ju jv jy nn kc no kg np kk nq nr ns nt bi translated">谁开的枪</li><li id="c99a" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated">这一枪是从地板上的什么地方拍的</li><li id="c931" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated">谁是最近的防守队员，</li><li id="558a" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated">最近的防守队员有多远</li><li id="0151" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated">拍摄时间</li></ul><p id="596c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此处的<strong class="jp ir">Kaggle</strong>T36】上有数据。在接下来的内容中，我使用了 Julia 并附上了代码，这样您就可以理解了。需要几个包:</p><pre class="kn ko kp kq gt nz oa ob oc aw od bi"><span id="4d05" class="lw kz iq oa b gy oe of l og oh">using Distributions<br/>using Plots<br/>pyplot()<br/>using DataFrames<br/>using GLM<br/>using Optim<br/>using CSV<br/>using GLM</span></pre><p id="7f24" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">数据集非常广泛。让我们选择射门是否成功，<strong class="jp ir">射门时间，射门距离，以及与最近的防守队员</strong>的距离:</p><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h2 id="ffdc" class="lw kz iq bd la lx ly dn le lz ma dp li jy mb mc lm kc md me lq kg mf mg lu mh bi translated">使用包装</h2><p id="71ea" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">获得结果的最快方法是使用包 GLM:</p><pre class="kn ko kp kq gt nz oa ob oc aw od bi"><span id="586f" class="lw kz iq oa b gy oe of l og oh">fittedmodel = glm(@formula(SHOT_RESULT ~ SHOT_CLOCK + SHOT_DIST + CLOSE_DEF_DIST), df_nba, Binomial(), LogitLink(), verbose=true)</span><span id="dc1b" class="lw kz iq oa b gy ok of l og oh">SHOT_RESULT ~ 1 + SHOT_CLOCK + SHOT_DIST + CLOSE_DEF_DIST<br/></span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ol"><img src="../Images/431f274dd118589c7884233f44978c49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iY2ewmYgXX5aB9gYXzPPBA.png"/></div></div><p class="om on gj gh gi oo op bd b be z dk translated">GLM 的估计结果</p></figure><p id="5643" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们如何解释这些结果？</p><ul class=""><li id="2393" class="nl nm iq jp b jq jr ju jv jy nn kc no kg np kk nq nr ns nt bi translated"><strong class="jp ir">时间压力让 NBA 球员更成功:投篮时间越高，得分的可能性越大</strong></li><li id="5860" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated"><strong class="jp ir">距离较远的镜头更容易被错过</strong></li><li id="d60a" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated"><strong class="jp ir">离最近的防守队员越远，射门成功的可能性越大</strong></li></ul><h2 id="83c5" class="lw kz iq bd la lx ly dn le lz ma dp li jy mb mc lm kc md me lq kg mf mg lu mh bi translated">编写自己的代码</h2><p id="4652" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">能不能“手动”找到类似的结果？答案是<strong class="jp ir">是的</strong>。为了看到这一点，让我们首先创建二元变量<code class="fe oq or os oa b">y</code>并将解释变量放入<code class="fe oq or os oa b">X.</code>中，然后我定义几个函数来计算损失函数的梯度和 hessian:</p><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="6993" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个逻辑模型的实现给我们的参数值几乎与我们使用<code class="fe oq or os oa b">GLM.</code>软件包得到的参数值相同。</p><pre class="kn ko kp kq gt nz oa ob oc aw od bi"><span id="8a1a" class="lw kz iq oa b gy oe of l og oh">Estimate for theta is [-0.0575129, 0.0185199, -0.0597451, 0.108392]</span></pre><h1 id="3ede" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">结论</h1><p id="2c56" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">这篇博文展示了如何从基本原则(潜在变量解释)中推导出逻辑模型，以及如何用几行代码实现它。</p></div></div>    
</body>
</html>