<html>
<head>
<title>A Very Basic Overview of Neural Radiance Fields (NeRF)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经辐射场(NeRF)的基本概述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-very-basic-overview-of-neural-radiance-fields-nerf-db4a0d4c391b#2022-07-31">https://towardsdatascience.com/a-very-basic-overview-of-neural-radiance-fields-nerf-db4a0d4c391b#2022-07-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ec7b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">它们有一天能取代照片吗？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d73dcb571f67fe5100eccd103b978bc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nk0V1AJo44OWQ5579ivjhg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。NeRF管道。给定一大组图像，NeRF学会隐式地表示3D形状，以便稍后可以合成新的视图。图片由<a class="ae ky" href="https://arxiv.org/abs/2003.08934" rel="noopener ugc nofollow" target="_blank">米尔登霍尔</a>等人从原始NeRF论文中检索而来。</p></figure><p id="af96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">深度学习时代始于它在传统2D图像识别任务(如分类、检测和实例分割)中带来的进步。随着技术的成熟，基于深度学习的计算机视觉的研究已经转向基本的3D计算机视觉问题——其中最值得注意的是合成物体的新视图，并从图像中重建其3D形状。许多方法将此作为传统的机器学习问题来解决，目标是在有限的一组训练迭代之后，学习系统从图像中“膨胀”出3D几何形状。然而，最近，一个全新的方向，即神经辐射场(NeRF)，已被引入。这篇文章深入探讨了最初提出的<a class="ae ky" href="https://arxiv.org/abs/2003.08934" rel="noopener ugc nofollow" target="_blank"> NeRF </a>的基本概念以及近年来它的几个扩展。</p><h1 id="be60" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">隐式表示几何图形</h1><p id="d384" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">用于3D重建的NeRF模型和传统神经网络之间的最大区别在于NeRF是对象的特定于实例的隐式表示。</p><p id="d7d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，给定从多个角度捕捉相同对象的一组图像以及它们相应的姿态，网络学习表示3D对象，使得新的视图可以以与训练视图集一致的方式合成。</p><h1 id="cc0e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">从基本的MLP开始</strong></h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/cc66a471d79ed84508263de3e2811efc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cKChqiNKm0QTySQs2go3fg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。NeRF培训概述。由<a class="ae ky" href="https://arxiv.org/abs/2003.08934" rel="noopener ugc nofollow" target="_blank">米尔登霍尔</a>等人从原始NeRF论文中检索的图像。</p></figure><p id="c1df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然这种隐式表示似乎很困难，但Mildenhall等人在他们的第一篇NeRF论文中表明，一个简单的多层感知器(MLP)拥有足够的能力来执行这样一个复杂的任务。</p><p id="c7db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具体来说，这个全连接网络的输入是单个5D坐标(3个用于位置，2个用于观察方向)，输出是给定位置的密度和颜色。实际上，密度只与位置有关，而与观察方向无关，因此仅使用位置来预测密度，而观察方向与位置特征相结合来预测所看到的颜色。</p><h1 id="0ac3" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">优化NeRF</h1><p id="afab" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">有两种实现技术可以更好地改进NeRF，以更好地表示复杂场景——位置编码和分层体采样。</p><h2 id="7750" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">位置编码</h2><p id="8022" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">以前的文献表明，将输入映射到更高维的空间有助于网络学习更复杂的功能。位置编码是一种特殊的编码功能，它通过使用高频函数来精确地执行该功能。在输入到MLP之前，位置坐标和视角方向都被输入到该编码函数中。</p><h2 id="ecbd" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">分层体积取样</h2><p id="7528" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">当训练一个NeRF时，两个网络，一个粗略的，一个精细的，被联合优化。具体来说，我们首先使用标准采样训练一个粗网络。然后，给定粗略网络的输出，精细网络样本旨在对体积的更相关部分进行采样，以提高训练效率。</p><h1 id="6232" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">NeRF的应用</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/1d328cf28800327f4db1aedadea44bd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U7D4AZaLJzH2cllLTXpxxg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3。NeRF可能有一天会取代照片成为捕捉视觉记忆的新媒介。</p></figure><p id="8ce6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们想要记住一个旅行过的地方，一个我们爱的人，或者一段我们珍惜的记忆时，照片一直是我们的首选媒介。NeRF的兴起可能是一个更好的解决方案。</p><p id="47ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们能够消除训练时间和图像数量的限制，NeRF在多视图中存储视觉记忆的能力会大得多。它可能是一张“3D”照片，每个角度(甚至是你没有捕捉到的)都以高分辨率恰当地呈现在你面前。</p><h1 id="91e1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">超越标准NeRFs</h1><p id="71ef" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">NeRF的引入对3D重建领域来说是一股新鲜空气。将一个模型“过度拟合”到一个特定的3D实例是非正统的，但却产生了令人印象深刻的、新颖的视图合成品质。然而，首先提出的体系结构有一些主要的缺点。这些问题包括:</p><ol class=""><li id="1301" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated">它需要同一物体的大量图像。</li><li id="274a" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">训练时间很长。</li><li id="214f" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">每个图像的相机姿态是必需的。</li></ol><p id="3d61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最近已经引入了许多作品来解决所有这些问题。下面我们列出了一些旨在解决这些问题的方法。</p><h2 id="f276" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">它需要同一物体的大量图像。</h2><ul class=""><li id="b540" class="ng nh it lb b lc mn lf mo li nv lm nw lq nx lu ny nm nn no bi translated">RegNeRF:从稀疏输入中正则化用于视图合成的神经辐射场—【https://arxiv.org/abs/2112.00724 T2】</li><li id="a6a3" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu ny nm nn no bi translated">pixelNeRF:来自一个或几个图像的神经辐射场—【https://arxiv.org/abs/2012.02190 T4】</li></ul><h2 id="2d39" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">训练时间很长。</h2><ul class=""><li id="8c9c" class="ng nh it lb b lc mn lf mo li nv lm nw lq nx lu ny nm nn no bi translated">具有多分辨率哈希编码的即时神经图形图元—<a class="ae ky" href="https://nvlabs.github.io/instant-ngp/" rel="noopener ugc nofollow" target="_blank">https://nvlabs.github.io/instant-ngp/</a></li></ul><h2 id="53e4" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">每个图像的相机姿态是必需的。</h2><ul class=""><li id="55fa" class="ng nh it lb b lc mn lf mo li nv lm nw lq nx lu ny nm nn no bi translated">GNeRF:无姿态相机的GAN基神经辐射场—<a class="ae ky" href="https://arxiv.org/abs/2103.15606" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2103.15606</a></li><li id="38e0" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu ny nm nn no bi translated">NeRF---没有已知摄像机参数的神经辐射场—<a class="ae ky" href="https://arxiv.org/abs/2102.07064" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2102.07064</a></li></ul><h2 id="5059" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">其他有趣的NeRF相关论文</h2><ul class=""><li id="c7f7" class="ng nh it lb b lc mn lf mo li nv lm nw lq nx lu ny nm nn no bi translated">零射击文本引导的梦域对象生成—【https://ajayj.com/dreamfields T2】</li><li id="64c5" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu ny nm nn no bi translated">Block-NeRF:可扩展的大场景神经视图合成—【https://arxiv.org/abs/2202.05263 T4】</li></ul><h1 id="41e6" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结束注释</h1><p id="5f04" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这就是你要的——对NeRF原始论文的一个非常简单的概述。这种表示视觉数据的新方式带来了无穷的潜力，并激发了大量不断改进的最新研究。也许有一天我们的记忆会以现实和想象的结合来储存。</p><blockquote class="nz"><p id="9cff" class="oa ob it bd oc od oe of og oh oi lu dk translated">“想象力比知识更重要。因为知识是有限的，而想象力包含整个世界，刺激进步，催生进化”——阿尔伯特·爱因斯坦</p></blockquote><p id="fdbd" class="pw-post-body-paragraph kz la it lb b lc oj ju le lf ok jx lh li ol lk ll lm om lo lp lq on ls lt lu im bi translated"><em class="oo">感谢你坚持到现在</em>🙏<em class="oo">！</em> <em class="oo">我定期写关于计算机视觉/深度学习的不同领域，所以</em> <a class="ae ky" href="https://taying-cheng.medium.com/membership" rel="noopener"> <em class="oo">加入并订阅</em> </a> <em class="oo">如果你有兴趣了解更多！此外，这篇文章没有涉及任何数学或实现的细节，辐射场的扩展远远超出了我在这里提到的几个。详细解释请阅读原文。</em></p></div></div>    
</body>
</html>