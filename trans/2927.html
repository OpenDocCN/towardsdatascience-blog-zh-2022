<html>
<head>
<title>Feature Extraction with BERT for Text Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于BERT的文本分类特征提取</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-extraction-with-bert-for-text-classification-533dde44dc2f#2022-06-27">https://towardsdatascience.com/feature-extraction-with-bert-for-text-classification-533dde44dc2f#2022-06-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/f3e3cbbbe9daab1c6de4b3dfdce9ba9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AbehLuK2M9buNLFT"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">照片由<a class="ae jd" href="https://unsplash.com/@sigmund?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">西格蒙德</a>在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><div class=""/><div class=""><h2 id="00c3" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">使用Pytorch和拥抱脸从预训练的模型中提取信息</h2></div><h2 id="83ca" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">目标</h2><p id="154d" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">让我们从定义这篇实践文章的目的开始。我们希望建立一个模型，将一个或多个文档作为输入，并设法根据它们的内容对它们进行分类。<br/>一些类别例如:政治、旅游、体育等。<br/>为了做到这一点，我们想使用一个预训练模型，如BERT。</p><figure class="ml mm mn mo gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mk"><img src="../Images/b85fa9d31689f4f520466edce1918fbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZczBPqOVR25F85zDCuMW1g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><h2 id="5456" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated"><strong class="ak">背景</strong></h2><p id="3c20" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">BERT是一种基于Transformer编码器的语言模型。如果你对变形金刚不熟悉，我推荐你阅读<a class="ae jd" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">这篇惊人的文章</a>。</p><p id="1571" class="pw-post-body-paragraph lr ls jg lt b lu mp kh lw lx mq kk lz le mr mb mc li ms me mf lm mt mh mi mj ij bi translated"><strong class="lt jh">伯特简单地说</strong>:</p><ul class=""><li id="aa6a" class="mu mv jg lt b lu mp lx mq le mw li mx lm my mj mz na nb nc bi translated">它将一个或多个句子的嵌入标记作为输入。</li><li id="aaa0" class="mu mv jg lt b lu nd lx ne le nf li ng lm nh mj mz na nb nc bi translated">第一个令牌始终是一个名为<strong class="lt jh">【CLS】</strong>的特殊令牌。</li><li id="4a46" class="mu mv jg lt b lu nd lx ne le nf li ng lm nh mj mz na nb nc bi translated">句子之间由另一个叫做<strong class="lt jh">【SEP】</strong>的特殊记号分隔。</li><li id="f159" class="mu mv jg lt b lu nd lx ne le nf li ng lm nh mj mz na nb nc bi translated">对于每个令牌，BERT输出一个称为<strong class="lt jh">隐藏状态</strong>的嵌入。</li><li id="5dbe" class="mu mv jg lt b lu nd lx ne le nf li ng lm nh mj mz na nb nc bi translated">伯特接受了<strong class="lt jh">掩蔽语言模型</strong>和<strong class="lt jh">下一句预测</strong>任务的训练。</li></ul><p id="44e2" class="pw-post-body-paragraph lr ls jg lt b lu mp kh lw lx mq kk lz le mr mb mc li ms me mf lm mt mh mi mj ij bi translated">在<strong class="lt jh">屏蔽语言模型(MLM) </strong>中，一个输入单词(或标记)被屏蔽，伯特必须试图找出被屏蔽的单词是什么。对于<strong class="lt jh">下一个句子预测(NSP) </strong>任务，输入给BERT两个句子，他必须弄清楚第二个句子是否在语义上跟随第一个句子。</p><p id="00a1" class="pw-post-body-paragraph lr ls jg lt b lu mp kh lw lx mq kk lz le mr mb mc li ms me mf lm mt mh mi mj ij bi translated">对于像我们这样的分类问题，<strong class="lt jh">我们只对与初始标记【CLS】</strong>相关联的隐藏状态感兴趣，不知何故<strong class="lt jh">比其他的更好地捕捉了整个句子</strong>的语义。因此，我们可以使用这种嵌入作为分类器的输入，我们在它的基础上构建分类器。</p><figure class="ml mm mn mo gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/66d6f1fd8d9daad8951784d3ea6fb476.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fOdb7SbBeMyWcHfTuVyeQA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><p id="a517" class="pw-post-body-paragraph lr ls jg lt b lu mp kh lw lx mq kk lz le mr mb mc li ms me mf lm mt mh mi mj ij bi translated">从上面的图片中，你可以看到我们将使用一个叫做<strong class="lt jh"> DistilBERT </strong>的较轻版本的BERT。这个经过提炼的模型比原来的<strong class="lt jh">小了40%</strong>,但是在各种NLP任务上仍然保持了大约97%的性能。<br/>你可以注意到的另一件事是，伯特的输入不是原始单词，而是标记。简单地说，BERT关联了一个预处理文本的记号化器，以便它对模型有吸引力。单词通常被分成子词，此外还会添加特殊的标记<strong class="lt jh">【CLS】</strong>来指示句子的开始，<strong class="lt jh">【SEP】</strong>来分隔多个句子，以及<strong class="lt jh">【PAD】</strong>来使每个句子具有相同数量的标记。</p><p id="dfa3" class="pw-post-body-paragraph lr ls jg lt b lu mp kh lw lx mq kk lz le mr mb mc li ms me mf lm mt mh mi mj ij bi translated">如果您想了解更多关于Bert或他的wordpartickizer的信息，请查看以下资源:</p><ul class=""><li id="2737" class="mu mv jg lt b lu mp lx mq le mw li mx lm my mj mz na nb nc bi translated"><a class="ae jd" href="https://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">https://jalammar.github.io/illustrated-bert/</a></li><li id="77df" class="mu mv jg lt b lu nd lx ne le nf li ng lm nh mj mz na nb nc bi translated"><a class="ae jd" href="https://huggingface.co/blog/bert-101" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/blog/bert-101</a></li><li id="d79a" class="mu mv jg lt b lu nd lx ne le nf li ng lm nh mj mz na nb nc bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/wordpiece-subword-based-tokenization-algorithm-1fbd14394ed7">https://towards data science . com/word piece-sub word-based-token ization-algorithm-1 FBD 14394 ed 7</a></li><li id="fca0" class="mu mv jg lt b lu nd lx ne le nf li ng lm nh mj mz na nb nc bi translated"><a class="ae jd" href="https://huggingface.co/docs/transformers/tokenizer_summary" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/docs/transformers/tokenizer_summary</a></li></ul><h1 id="fd5a" class="nj kw jg bd kx nk nl nm la nn no np ld km nq kn lh kp nr kq ll ks ns kt lp nt bi translated">我们来编码吧！</h1><h2 id="560f" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">资料组</h2><p id="0a75" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">我们要用的数据集叫做<a class="ae jd" href="https://www.kaggle.com/datasets/shivamkushwaha/bbc-full-text-document-classification" rel="noopener ugc nofollow" target="_blank"> <em class="nu"> BBC全文文档分类</em> </a> <em class="nu"> </em>，可以在Kaggle上公开获取。</p><blockquote class="nv nw nx"><p id="7f4a" class="lr ls nu lt b lu mp kh lw lx mq kk lz ny mr mb mc nz ms me mf oa mt mh mi mj ij bi translated">该数据集包含2225条记录，总共包含5个类别。我们想要确定的五个类别是体育、商业、政治、科技和娱乐。</p></blockquote><p id="53c8" class="pw-post-body-paragraph lr ls jg lt b lu mp kh lw lx mq kk lz le mr mb mc li ms me mf lm mt mh mi mj ij bi translated">直接从colab下载kaggle数据集。(记得上传您的个人密钥)</p><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="4166" class="pw-post-body-paragraph lr ls jg lt b lu mp kh lw lx mq kk lz le mr mb mc li ms me mf lm mt mh mi mj ij bi translated">我不会详细介绍如何读取这些数据并将其转换成dataframe，但我假设您可以创建一个包含两列的data frame:<em class="nu">short _ description</em>和<em class="nu"> category。</em></p><h2 id="31a9" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">编码标签</h2><p id="e148" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">数据集的标签有:<em class="nu">政治、体育</em>、<em class="nu">等……</em>。<br/>我们需要<strong class="lt jh">通过简单地使用标签编码器将这些标签转换成数字</strong>。</p><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="ob oc l"/></div></figure><h2 id="384b" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">生成文本嵌入</h2><p id="377a" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">为每个文本生成一个嵌入向量，它可以用作我们最终分类器的输入。与每个文本相关联的<strong class="lt jh">矢量嵌入只是Bert为[CLS]令牌输出的隐藏状态。</strong></p><p id="82f0" class="pw-post-body-paragraph lr ls jg lt b lu mp kh lw lx mq kk lz le mr mb mc li ms me mf lm mt mh mi mj ij bi translated">先从<a class="ae jd" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank"> <strong class="lt jh"> HuggingFace </strong> </a> <strong class="lt jh">导入模型和分词器开始。</strong></p><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="f79e" class="pw-post-body-paragraph lr ls jg lt b lu mp kh lw lx mq kk lz le mr mb mc li ms me mf lm mt mh mi mj ij bi translated">现在我们必须对文本进行标记。记住定义填充符<strong class="lt jh"><em class="nu"/></strong>，这样每个标记化的句子将具有相同的长度，并且<strong class="lt jh"> <em class="nu">截断</em> </strong>因此如果句子太长，它将被截断。最后一个参数是返回PyTorch张量。</p><p id="cf40" class="pw-post-body-paragraph lr ls jg lt b lu mp kh lw lx mq kk lz le mr mb mc li ms me mf lm mt mh mi mj ij bi translated">对文本进行标记化的结果将是一个字典，其中包含以数字表示的标记<strong class="lt jh">input _ id</strong>，以及告诉我们该标记是否为[PAD]的<strong class="lt jh"> attention_mask </strong>。</p><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="865b" class="pw-post-body-paragraph lr ls jg lt b lu mp kh lw lx mq kk lz le mr mb mc li ms me mf lm mt mh mi mj ij bi translated">通过运行模型获得文本([CLS])隐藏状态。</p><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="ob oc l"/></div></figure><h2 id="162f" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">构建分类器</h2><p id="02ae" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">至此，你可以使用你最喜欢的分类器，给它输入隐藏状态，并让它预测标签。在这种情况下，我将使用随机森林。</p><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="8f8b" class="pw-post-body-paragraph lr ls jg lt b lu mp kh lw lx mq kk lz le mr mb mc li ms me mf lm mt mh mi mj ij bi translated">这个分类器的性能不会很好，因为我们使用了很少的数据，并且没有在分类器上做很多工作。但是出于好奇，我建议你将它与一个随机预测标签的虚拟分类器进行比较。</p><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="ob oc l"/></div></figure><h1 id="b90e" class="nj kw jg bd kx nk nl nm la nn no np ld km nq kn lh kp nr kq ll ks ns kt lp nt bi translated">最后的想法</h1><p id="3cd1" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">在这篇动手实践文章中，我们看到了如何利用预训练模型的功能在很短的时间内创建一个简单的分类器。请记住，我们训练的只是最终的分类器，即随机森林。<br/>另一方面，Bert只在推理中用于生成嵌入，以某种方式捕捉文本的主要特征，这就是为什么我们说我们使用了特征提取方法。但是我们能训练Bert自己教他如何在我们的具体案例中创建更好的文本嵌入吗？当然可以！在下一篇文章中，我将向你展示如何微调(蒸馏)伯特！</p><h1 id="0033" class="nj kw jg bd kx nk nl nm la nn no np ld km nq kn lh kp nr kq ll ks ns kt lp nt bi translated">结束了</h1><p id="1a99" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated"><em class="nu">马赛洛·波利蒂</em></p><p id="e6a1" class="pw-post-body-paragraph lr ls jg lt b lu mp kh lw lx mq kk lz le mr mb mc li ms me mf lm mt mh mi mj ij bi translated"><a class="ae jd" href="https://www.linkedin.com/in/marcello-politi/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>，<a class="ae jd" href="https://twitter.com/_March08_" rel="noopener ugc nofollow" target="_blank"> Twitter </a>，<a class="ae jd" href="https://march-08.github.io/digital-cv/" rel="noopener ugc nofollow" target="_blank"> CV </a></p></div></div>    
</body>
</html>