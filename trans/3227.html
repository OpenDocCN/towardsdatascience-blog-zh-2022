<html>
<head>
<title>Is ML Explainability the wrong goal?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML可解释性是错误的目标吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/is-ml-explainability-the-wrong-goal-d2e94e1f4bf2#2022-07-18">https://towardsdatascience.com/is-ml-explainability-the-wrong-goal-d2e94e1f4bf2#2022-07-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cdf7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在高风险的应用中，可解释性甚至可能适得其反</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d9965a3b936a0f4151a53345a437bf61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OANpyjuq88VFZ7Yk"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@yosef_fxum?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Yosef Futsum </a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="0204" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在他2006年的经典作品<a class="ae kv" href="https://en.wikipedia.org/wiki/The_Shock_of_the_Old" rel="noopener ugc nofollow" target="_blank"><em class="ls"/></a><em class="ls"/>中，大卫·艾顿认为历史学家对技术史的理解过于受发明的支配。我们理所当然地怀念和钦佩伟大的发明家和科学家。事实上，David Edgerton详细描述道，技术的采用和使用通常和发明本身一样重要，如果不是更重要的话。技术投入使用的方式在历史上有很大影响。在这种背景下，ML的可解释性和可解释性成为非常重要和有争议的概念就不足为奇了，有时会误入滥用的口号中。ML是一项关键技术，目前正被集成到基础设施和决策过程的关键部分，因此采用的方式无疑是重要的。具体来说，一个已部署的ML系统可解释或可说明的程度决定性地影响了人在该系统操作中的角色。</p><p id="8c7c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">可解释性对可解释性</strong></p><p id="b3b6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">“可解释性”是指用户/接收者证明人工智能模型所做预测的能力。这通常是一种用于深入了解复杂模型的技术。例如，由于所用算法的复杂性，人类可能无法理解发生在数据上的转换(尽管他们会理解该过程在高层次上是如何工作的)。在这种情况下，可解释性技术提供了一些为什么做出复杂预测的建议。可解释性指的是他们从因果角度解释为什么做出预测的能力。</p><p id="5874" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从这个意义上来说，可解释性是可解释性的更强版本(对模型输出的更彻底的基于因果关系的解释)。)可解释性通常被用来证明黑盒模型所做的预测是合理的，而黑盒模型是不可解释的。例如，通过置换输入或用代理模型拟合黑盒模型的预测，我们也许可以更好地解释预测过程中发生的事情，但不能从因果上证明为什么会做出决策。</p><p id="0325" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">有‘可解释性’就够了吗？</strong></p><p id="2f74" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，有些模型可能永远无法解释，尤其是深度学习(DL)模型。这是因为对于这些模型，输入通过训练过程被不可识别地转换。DL的核心原则之一是<a class="ae kv" href="https://arxiv.org/abs/1206.5538" rel="noopener ugc nofollow" target="_blank">‘表示学习’</a>，这意味着模型将它接收到的输入迭代地转换成新的表示(当输入通过神经网络的连续层时)。这种转换旨在最大化数据中的信号，为算法提供更大的牵引力，以进行准确预测。换句话说，这个输入转换过程允许机器在输入上获得更多购买，同时限制了人类分析师理解相同输入的能力。这种权衡是神经网络所固有的，也是这套强大的模型存在问题的原因之一。这些是附加了特别解释工具的黑盒模型(例如，计算机视觉中的显著性图，表格数据的SHAP值等)。)以抵消人类固有的无法理解这些转换输入的能力。</p><p id="3191" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">事实上，已经表明，一些流行的用于证明深度学习模型预测的可解释性技术是不可靠的。显著图是理解卷积神经网络中预测的常用方法，旨在揭示哪些图像像素在进行预测时最重要。然而，已经证明这些方法<a class="ae kv" href="https://www.medrxiv.org/content/10.1101/2021.02.28.21252634v1" rel="noopener ugc nofollow" target="_blank">并不总是能够</a>识别用于分类的图像的关键区域，这让我们质疑它们的实用性。一个可能不正确的可解释性方法有什么用？事实上，这可能会给用户一种错误的自信感。</p><p id="00a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">我们如何定义“高风险”？</strong></p><p id="84a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，一个关键的问题是，在人工智能系统实现之前，可解释性何时是必要的先决条件。Yoshia Bengio对“系统1”和“系统2”的有影响力的区分可能有助于理解仍然使用黑盒和可解释性的辩护。他认为，系统1 DL(构成“快速”感知式思维)已经由ML系统实现，如计算机视觉。然而，系统2 DL(“慢”逻辑推理，可能涉及训练集数据分布之外的概括)还没有实现。本吉奥本人并没有提出这个论点，但是基于这种想法，有些人可能会认为系统1 DL不需要可解释性。我们大多数人都无法向朋友解释为什么我们会看到某个物体，或者为什么我们能够以某种方式闻到某样东西。</p><p id="3915" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，在系统1的实现中(用大卫·埃哲顿的话来说，这是技术变革的<em class="ls">创新</em>部分),应用程序中DL的感知能力有时会取代人类的推理和逻辑，即使模型本身并不执行逻辑或推理。例如，以胸部x光片作为输入并预测相应患者哪里患有急性疾病的计算机视觉模型正在取代放射科医师可能用于根据x光片进行诊断的推理。与此同时，像这样的应用程序可以<a class="ae kv" href="https://pubmed.ncbi.nlm.nih.gov/34623478/" rel="noopener ugc nofollow" target="_blank">通过排除模型以高置信度预测扫描正常的扫描，从而给放射科医生更多的时间来诊断棘手的问题，从而显著改善患者的结果</a>。</p><p id="a319" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这显然是一个使用黑盒模型做出高风险决策的例子。但是，其他一些实现更难分类。使用谷歌搜索是高风险决策的一个例子吗？网飞的建议是高风险决策吗？可能需要对“高风险”的含义进行严格的定义，以便就每个用例所需的可解释性水平达成共识。与此同时，我们应该非常小心地调用可解释的方法，特别是当它们给予模型预测背后的推理以信心的时候。</p><p id="5861" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">参考文献</strong></p><p id="2191" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1] C. Rudin，<a class="ae kv" href="https://arxiv.org/pdf/1811.10154.pdf?fbclid=IwAR01WIlfIiC1cgM99nhwIjAT0tHWYxHk7ZA_o9nEK9jJ75KdFMNZlv5Y0AU" rel="noopener ugc nofollow" target="_blank">停止解释高风险决策的黑盒机器学习模型，转而使用可解释模型(2019) </a>，《自然机器智能》第1卷，206–215页。</p><p id="fb19" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]萨波塔等人。al，<a class="ae kv" href="https://www.medrxiv.org/content/10.1101/2021.02.28.21252634v1" rel="noopener ugc nofollow" target="_blank">‘深度学习显著图没有准确地突出医学图像解释的诊断相关区域’</a>(2021)，medRxiv。</p></div></div>    
</body>
</html>