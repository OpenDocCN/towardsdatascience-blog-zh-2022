<html>
<head>
<title>Towards Geometric Deep Learning II: The Perceptron Affair</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">走向几何深度学习II:感知机事件</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/towards-geometric-deep-learning-ii-the-perceptron-affair-fafa61b5c40a#2022-07-11">https://towardsdatascience.com/towards-geometric-deep-learning-ii-the-perceptron-affair-fafa61b5c40a#2022-07-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="631d" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">几何深度学习的起源</h2><div class=""/><div class=""><h2 id="1a05" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">几何深度学习从对称性和不变性的角度处理了一大类ML问题，为多种多样的神经网络架构(如CNN、gnn和Transformers)提供了一个通用蓝图。在一系列新的帖子中，我们研究了可以追溯到古希腊的几何思想如何塑造了现代深度学习。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/c60c59fbcb26028d2760defdf6d037b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P6u_7NPi8gXqYuh2GW0-nw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片:基于Shutterstock。</p></figure><p id="ea7b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在“走向几何深度学习系列”的第二篇文章中，我们讨论了早期的神经网络模型，以及它们的批评如何产生了计算几何的新领域。这篇文章基于M. M .布朗斯坦、j .布鲁纳、t .科恩和p .韦利奇科维奇、 <a class="ae me" href="https://arxiv.org/abs/2104.13478" rel="noopener ugc nofollow" target="_blank"> <em class="md">几何深度学习</em> </a> <em class="md">(在麻省理工学院出版社完成后出现)一书的介绍章节，并伴随</em> <a class="ae me" href="https://youtube.com/playlist?list=PLn2-dEmQeTfSLXW8yXP4q_Ii58wFdxb3C" rel="noopener ugc nofollow" target="_blank"> <em class="md">我们的课程</em> </a> <em class="md">参加非洲机器智能大师赛(AMMI)。参见</em> <a class="ae me" rel="noopener" target="_blank" href="/towards-geometric-deep-learning-i-on-the-shoulders-of-giants-726c205860f5?sk=fd04bfaab732177ba7b4d7da90d88e9e"> <em class="md">第一部分</em> </a> <em class="md">讨论对称性，</em> <a class="ae me" rel="noopener" target="_blank" href="/towards-geometric-deep-learning-iii-first-geometric-architectures-d1578f4ade1f?sk=89a4bf9164d5ef43a25ad1fc23bd1372"> <em class="md">第三部分</em> </a> <em class="md">研究第一个“几何”架构，</em> <a class="ae me" rel="noopener" target="_blank" href="/towards-geometric-deep-learning-iv-chemical-precursors-of-gnns-11273d74125?sk=00a1aa8fb968b95245e1f2cf275198ea"> <em class="md">第四部分</em> </a> <em class="md">献给早期GNNs，以及我们之前的</em> <a class="ae me" rel="noopener" target="_blank" href="/geometric-foundations-of-deep-learning-94cdd45b451d?sk=184532175cb936d7b25d9adebd512629"> <em class="md">帖子</em> </a> <em class="md">总结几何深度学习的概念。</em></p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="7ad2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi mm translated">虽然很难就“人工智能”作为一个科学领域诞生的具体时间点达成一致(最终，人类一直痴迷于理解智能并从文明的黎明开始学习)，但我们将尝试一项风险较小的任务，看看深度学习的前身——我们讨论的主要话题。这段历史可以压缩到不到一个世纪。</p><h1 id="63b8" class="mv mw it bd mx my mz na nb nc nd ne nf ki ng kj nh kl ni km nj ko nk kp nl nm bi translated"><strong class="ak">感知器的兴衰</strong></h1><p id="1e54" class="pw-post-body-paragraph lh li it lj b lk nn kd lm ln no kg lp lq np ls lt lu nq lw lx ly nr ma mb mc im bi mm translated">到了20世纪30年代，人们已经清楚意识存在于大脑中，研究工作转向从大脑网络结构的角度来解释大脑的功能，如记忆、感知和推理。麦卡洛克和皮茨[1]被认为是第一个对神经元进行数学抽象的人，展示了神经元计算逻辑功能的能力。就在创造了“人工智能”这个术语的传奇<a class="ae me" href="https://en.wikipedia.org/wiki/Dartmouth_workshop" rel="noopener ugc nofollow" target="_blank">达特茅斯学院研讨会</a>一年后，来自康乃尔航空实验室的美国心理学家<a class="ae me" href="https://en.wikipedia.org/wiki/Frank_Rosenblatt" rel="noopener ugc nofollow" target="_blank"> Frank Rosenblatt </a>提出了一类他称之为“感知机”的神经网络。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/eef5ac7f7284dd27aace0f552fa1c8df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Po8r185momDtPO8pBxgggw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Frank Rosenblatt和他的Mark I感知器神经网络是在康奈尔航空实验室开发的，用于简单的视觉模式识别任务。肖像:伊霍尔·戈尔斯基。</p></figure><p id="9ba0" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">感知机首先在数字机器上实现，然后在专用硬件上实现，设法解决简单的模式识别问题，如几何形状的分类。然而，“连接主义”(从事人工神经网络研究的人工智能研究人员如何给自己贴标签)的迅速崛起受到了一桶冷水，这就是现在已经臭名昭著的马文·明斯基和西蒙·派珀特的书<em class="md"> Perceptrons </em>。</p><p id="9ebc" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在深度学习社区，人们通常会回顾性地指责明斯基和帕佩特造成了第一个“人工智能冬天”，这使得神经网络在十多年里不再流行。一个典型的叙述提到了“<a class="ae me" href="https://en.wikipedia.org/wiki/Perceptrons_(book)#The_XOR_affair" rel="noopener ugc nofollow" target="_blank"> XOR事件</a>”，这是一个证据，证明感知机甚至不能学习非常简单的逻辑功能，这是它们表达能力差的证据。一些消息来源甚至添加了一些戏剧性的内容，回忆起罗森布拉特和明斯基曾就读于同一所学校，甚至声称罗森布拉特在1971年的一次划船事故中过早死亡是在同事们批评他的工作后自杀的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/2e1f2d7852750d34956785a5023b57c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DsWkLnk_0APeaExD8bvsGg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">颇具影响力的著作《感知机》的作者马文·明斯基和西蒙·派珀特这本书封面上的两个形状(其中一个是相连的)暗示了“宇称问题”。这本书考虑了简单的单层感知(左上)，可能是最早的几何学习方法，包括群不变性的介绍(左下)。</p></figure><p id="7ca3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现实可能更加平凡，同时也更加微妙。首先，美国“人工智能冬天”的一个更合理的原因是1969年的曼斯菲尔德修正案，该修正案要求军方资助“以任务为导向的直接研究，而不是基本的非直接研究”由于当时人工智能领域的许多努力，包括罗森布拉特的研究，都是由军事机构资助的，并没有显示出即时的效用，因此资金的削减产生了巨大的影响。第二，神经网络和人工智能总体上被过分夸大了:回想一下1958年<a class="ae me" href="https://www.newyorker.com/magazine/1958/12/06/rival-2" rel="noopener ugc nofollow" target="_blank">《纽约客》的一篇文章</a>称感知机为</p><blockquote class="nt"><p id="efaf" class="nu nv it bd nw nx ny nz oa ob oc mc dk translated">《纽约客》(1958年)，“人类大脑有史以来第一个真正的对手”</p></blockquote><p id="65fb" class="pw-post-body-paragraph lh li it lj b lk od kd lm ln oe kg lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">和“非凡的机器”，它们“有能力思考”[5]，或者过于自信的<a class="ae me" href="https://people.csail.mit.edu/brooks/idocs/AIM-100.pdf" rel="noopener ugc nofollow" target="_blank">麻省理工学院夏季视觉项目</a>期望“构建一个视觉系统的重要部分”，并在1966年的一个夏季学期中实现执行“模式识别”的能力[6]。研究界认识到，最初对“解决智力问题”的希望过于乐观，这只是一个时间问题。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/83f7df1147fa7fb71daf21b64e07452b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FqVo69_x0BBy5Oyt_61YcA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">早期炒作:1958年《纽约客》的一篇文章称赞感知“能够思考”(左)，1966年，过于乐观的“<a class="ae me" href="https://people.csail.mit.edu/brooks/idocs/AIM-100.pdf" rel="noopener ugc nofollow" target="_blank">麻省理工学院夏季视觉项目</a>”旨在在几个月内构建一个“视觉系统的重要部分”。</p></figure><p id="0e8a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi mm translated">然而，如果我们深入探讨这场争论的实质，很明显罗森布拉特所谓的“感知机”与明斯基和帕佩特所理解的“感知机”有很大不同。Minsky和Papert将他们的分析和批评集中在他们称为“简单感知器”的一个狭窄的单层神经网络类别上(这在现代通常与该术语相关联)，其计算输入的加权线性组合，后跟非线性函数[7]。另一方面，Rosenblatt考虑了更广泛的一类架构，这些架构早于现在被认为是“现代”深度学习的许多想法，包括具有随机和本地连接的多层网络[8]。</p><p id="d0cf" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果Rosenblatt知道Vladimir Arnold和安德雷·柯尔莫哥洛夫[10-11]对第十三个希尔伯特问题[9]的<a class="ae me" href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem" rel="noopener ugc nofollow" target="_blank">证明，即一个连续的多元函数可以写成一个单变量连续函数的叠加，他可能会反驳一些关于感知器表达能力的批评。Arnold–Kolmogorov定理是多层(或“深度”)神经网络的“通用逼近定理”的前身，它解决了这些问题。</a></p><p id="9d22" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi mm translated"><span class="l mn mo mp bm mq mr ms mt mu di"> W </span>虽然大多数人记得明斯基和帕佩特的书，因为它在削弱早期联结主义者的翅膀和哀叹失去的机会方面发挥了作用，但一个被忽视的重要方面是，它第一次提出了学习问题的几何分析。这一事实反映在书名中，副标题为<em class="md">《计算几何导论</em>。在当时，这是一个全新的想法，一篇对这本书的评论[12](本质上是为罗森布拉特辩护)质疑道:</p><blockquote class="nt"><p id="cedb" class="nu nv it bd nw nx ny nz oa ob oc mc dk translated">“计算几何”这门新学科是否会成长为一个活跃的数学领域；还是会在一堆死胡同里慢慢消失？”区块(1970年)</p></blockquote><p id="15ea" class="pw-post-body-paragraph lh li it lj b lk od kd lm ln oe kg lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">前者发生了:计算几何现在是一个完善的领域[13]。</p><p id="65fc" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">此外，Minsky和Papert可能值得称赞的是，他们首次将群论引入了机器学习领域:他们的群不变性定理表明，如果神经网络对于某个群是不变的，那么它的输出可以表示为该群的轨道的函数。虽然他们使用这一结果来证明感知器可以学习的局限性，但类似的方法随后被jun ' ichi Amari[14]用于模式识别问题中不变特征的构造。Terrence Sejnowski [15]和John Shawe-Taylor[16–17]的著作中的这些思想的演变，不幸的是今天很少被引用，提供了几何深度学习蓝图的基础。</p><h1 id="6df9" class="mv mw it bd mx my mz na nb nc nd ne nf ki ng kj nh kl ni km nj ko nk kp nl nm bi translated">普适近似和维数灾难</h1><p id="d667" class="pw-post-body-paragraph lh li it lj b lk nn kd lm ln no kg lp lq np ls lt lu nq lw lx ly nr ma mb mc im bi mm translated">前面提到的普遍近似的概念值得进一步讨论。该术语指的是以任何期望的精度逼近任何连续多元函数的能力；在机器学习文献中，这种类型的结果通常归功于Cybenko [18]和Hornik [19]。与Minsky和Papert批评的“简单”(单层)感知器不同，多层神经网络是通用的近似器，因此是学习问题的一种有吸引力的架构选择。我们可以将监督机器学习视为一个函数逼近问题:给定训练集(例如，猫和狗的图像)上某个未知函数(例如，图像分类器)的输出，我们试图从某个假设类中找到一个函数，该函数很好地符合训练数据，并允许预测以前看不见的输入的输出(“泛化”)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/cf9e605df5b4eea2c98e98d5349c6c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3QfAq0CWr6WqTzDMlvP3yQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">走向普遍逼近:安德雷·柯尔莫哥洛夫和弗拉迪米尔·阿诺德证明的戴维·希尔伯特<a class="ae me" href="https://en.wikipedia.org/wiki/Hilbert%27s_thirteenth_problem" rel="noopener ugc nofollow" target="_blank">第十三个问题</a>是第一批结果之一，表明多元连续函数可以表示为简单一维函数的组合和总和。George Cybenko和Kurt Hornik证明了特定于神经网络的结果，表明具有一个隐藏层的感知器可以以任何期望的精度逼近任何连续函数。</p></figure><p id="2ca9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通用逼近保证了我们可以通过多层神经网络来表达来自非常广泛的正则类(连续函数)的函数。换句话说，存在具有一定数量的神经元和一定权重的神经网络，其逼近从输入到输出空间(例如，从图像空间到标签空间)的给定函数映射。然而，通用逼近定理并没有告诉我们如何找到这样的权重。事实上，在早期，神经网络中的学习(即，寻找权重)一直是一个很大的挑战。</p><p id="53c1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Rosenblatt展示了一个只针对单层感知器的学习算法；为了训练多层神经网络，<a class="ae me" href="https://en.wikipedia.org/wiki/Alexey_Ivakhnenko" rel="noopener ugc nofollow" target="_blank">阿列克谢·伊瓦赫年科</a>和瓦伦丁·帕拉【20】使用了一种叫做“数据处理分组法”的分层学习算法这使得Ivakhnenko [21]能够钻到八层深处——这在20世纪70年代早期是一个非凡的壮举！</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/f6616845fe5944a7c0ab63046076420a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*92o-3BWqIV038RtNWhE2fQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">如何训练自己的神经网络？现在无处不在的反向传播只是在20世纪80年代大卫·鲁梅尔哈特的论文发表后才成为标准(尽管保罗·沃博斯和塞波·林奈马在更早的时候介绍过)。早在20世纪70年代初，Aleksey Ivakhnenko的“数据处理分组方法”等早期方法就允许训练深度神经网络。</p></figure><p id="bbaa" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi mm translated"><span class="l mn mo mp bm mq mr ms mt mu di">一项</span>突破来自反向传播的发明，这是一种使用链规则计算权重相对于损失函数的梯度的算法，并允许使用基于梯度下降的优化技术来训练神经网络。截至今天，这是深度学习的标准方法。虽然反向传播的起源至少可以追溯到1960年[22]，但这种方法在神经网络中的首次令人信服的演示是在被广泛引用的Rumelhart、Hinton和Williams的《自然》论文中[23]。这种简单有效的学习方法的引入是神经网络在20世纪80年代和90年代重返人工智能领域的一个关键因素。</p><p id="7a0a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通过近似理论的镜头来看待神经网络，导致一些愤世嫉俗者将深度学习称为“美化的曲线拟合”。我们将让读者通过尝试回答一个重要问题来判断这条格言有多正确:需要多少样本(训练样本)来精确地逼近一个函数？近似理论家会立即反驳说，多层感知器可以表示的连续函数类显然太大了:人们可以通过有限的点集合传递无限多种不同的连续函数[24]。有必要施加额外的规律性假设，如<a class="ae me" href="https://en.wikipedia.org/wiki/Lipschitz_continuity" rel="noopener ugc nofollow" target="_blank">李普希茨连续性</a>【25】，在这种情况下，可以提供所需样本数量的界限。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ol"><img src="../Images/58bcab216d4f03f179335eb9d3ec8282.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xXTNkRzm0P9wNyGNT9sRmQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">维数灾难是发生在高维空间中的一种几何现象。一种形象化的方法是查看单位超立方体中单位度量球的体积比例(后者代表特征空间，而前者可以解释为“最近邻”分类器)。体积比随尺寸成指数衰减:对于d=2，该比值约为0.78，对于d=3，它下降到约0.52，对于d=10，它已经约为0.01。图:改编自视觉假人。</p></figure><p id="1bc0" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">不幸的是，这些界限随着维度呈指数级增长——这种现象俗称“维数灾难”[26]——这在机器学习问题中是不可接受的:即使是小规模的模式识别问题，如图像分类，也要处理数千维的输入空间。如果一个人必须只依赖近似理论的经典结果，机器学习将是不可能的。在我们的例子中，为了学会区分猫和狗，理论上需要的猫和狗图像的例子数量将比宇宙中的原子数量大得多[27]——周围没有足够的猫和狗来做这件事。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/fdb49d71f5cb9d6d7faefdebe137b55f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w4NXo9esMcCZtqJr9o9Rqw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">维数灾难:近似理论的标准结果与维数不成比例。因此，即使在简单的机器学习任务中，人们也会预测训练样本的数量远远大于实际可能的数量。</p></figure><h1 id="f25c" class="mv mw it bd mx my mz na nb nc nd ne nf ki ng kj nh kl ni km nj ko nk kp nl nm bi translated">艾冬天来了</h1><p id="3b22" class="pw-post-body-paragraph lh li it lj b lk nn kd lm ln no kg lp lq np ls lt lu nq lw lx ly nr ma mb mc im bi mm translated">英国数学家詹姆斯·莱特希尔爵士(Sir James Lighthill)在一篇被人工智能历史学家称为“<a class="ae me" href="https://en.wikipedia.org/wiki/Lighthill_report" rel="noopener ugc nofollow" target="_blank">莱特希尔报告</a>”的论文[28]中提出了机器学习方法向高维度扩展的问题，他在论文中使用了“组合爆炸”一词，并声称现有的人工智能方法只能解决玩具问题，在现实世界的应用中会变得难以处理。</p><blockquote class="nt"><p id="780a" class="nu nv it bd nw nx ny nz oa ob oc mc dk translated">“人工智能研究和相关领域的大多数工作者承认，他们对过去25年取得的成就明显感到失望。[……]在这个领域的任何一个领域，迄今为止的发现都没有产生当时所承诺的重大影响。”—詹姆斯·莱特希尔爵士(1972年)</p></blockquote><p id="f4d3" class="pw-post-body-paragraph lh li it lj b lk od kd lm ln oe kg lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">由于莱特希尔报告是由英国科学研究委员会委托评估人工智能领域的学术研究，其悲观的结论导致了整个池塘的资金削减。加上美国资助机构的类似决定，这相当于20世纪70年代人工智能研究的一个灾难。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="a741" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi mm translated"><span class="l mn mo mp bm mq mr ms mt mu di"> F </span>对我们来说，经典泛函分析无法提供处理学习问题的适当框架，这一认识将促使我们寻求更强的<em class="md">几何</em>形式的规律性，这种规律性可以在神经网络的特定布线中实现——例如卷积神经网络的局部连通性。公平地说，我们十年前目睹的深度学习的胜利重现至少部分归功于这些见解。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="78af" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[1] W. S .麦卡洛克和w .皮茨，<a class="ae me" href="https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf" rel="noopener ugc nofollow" target="_blank">神经活动中固有思想的逻辑演算</a> (1943)，数学生物物理学通报5(4):115–133。</p><p id="ecad" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[2]<a class="ae me" href="https://en.wikipedia.org/wiki/Dartmouth_workshop" rel="noopener ugc nofollow" target="_blank">达特茅斯人工智能夏季研究项目</a>是1956年在达特茅斯学院举办的一个夏季研讨会，被认为是人工智能领域的创始事件。</p><p id="47b2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[3] F .罗森布拉特，<a class="ae me" href="https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="md">感知机，一种感知和识别自动机</em> </a> (1957)，康奈尔航空实验室。这个名字是“感知”和表示乐器的希腊后缀- <em class="md">的组合。</em></p><p id="12c7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[4] M. Minsky和S. A. Papert，<em class="md">感知器:计算几何导论</em> (1969)，麻省理工学院出版社。</p><p id="b20e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[5]这就是为什么我们只能对最近类似的关于深层神经网络的“意识”的说法一笑置之:אין כל חדש תחת השמש.</p><p id="7f03" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[6]原文引用，因为“模式识别”尚未成为正式术语。</p><p id="9801" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[7]具体来说，Minsky和Papert考虑了2D网格(他们的术语是“视网膜”)和一组线性阈值函数上的二元分类问题。虽然无法计算XOR函数一直是这本书的主要批评点，但大部分注意力都集中在几何谓词上，如奇偶和连通性。这个问题在书的封面上有所暗示，封面上有两个图案点缀:一个是相连的，另一个不是。即使对人类来说，也很难确定哪个是哪个。</p><p id="2e6e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[8] E. Kussul，T. Baidyk，L. Kasatkina和V. Lukovich，Rosenblatt感知器用于手写数字识别(2001)，IJCNN表明，Rosenblatt在21世纪硬件上实现的3层感知器在MNIST数字识别任务上达到99.2%的准确率，与现代模型相当。</p><p id="aac1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[9] <a class="ae me" href="https://en.wikipedia.org/wiki/Hilbert%27s_thirteenth_problem" rel="noopener ugc nofollow" target="_blank">希尔伯特的第十三个问题</a>是戴维·希尔伯特在1900年编制的23个问题之一，需要使用两个自变量的连续函数证明所有七次方程是否存在解。Kolmogorov和他的学生Arnold展示了这个问题的一般化版本的解决方案，现在被称为<a class="ae me" href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem#:~:text=In%20real%20analysis%20and%20approximation,continuous%20functions%20of%20one%20variable." rel="noopener ugc nofollow" target="_blank">Arnold-Kolmogorov叠加定理</a>。</p><p id="41b9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[10]在。还有。阿诺德，《关于用较少变量的连续函数叠加来表示多个变量的连续函数》(1956年)，苏联科学院报告108:179–182。</p><p id="56c3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[11]在。还有。阿诺德，《关于三个变量T3的功能》(1957年)，苏联科学院报告114:679–681。</p><p id="0ce5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[12] H. D. Block，《国际法院案例汇编:国际法院案例汇编》</p><p id="2c55" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[13]《化学武器公约》第一. 3.5条。</p><p id="60c1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[14] S.-I. Amari，<a class="ae me" href="https://bsi-ni.brain.riken.jp/database/file/69/055.pdf" rel="noopener ugc nofollow" target="_blank">允许和检测不变信号变换的特征空间</a> (1978)，模式识别联合会议。</p><p id="c662" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[15] T. Sejnowski等人，学习具有隐藏单元的对称群:超越感知器(1986)，Physica D:非线性现象22(1–3):260–275。</p><p id="5835" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[16] J. Shawe-Taylor，将对称性纳入前馈网络(1989)，ICANN。</p><p id="5336" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[17] J. Shawe-Taylor，前馈网络结构中的对称性和可区分性(1993)，IEEE Trans .神经网络4(5):816–826。</p><p id="e07e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[18] G. Cybenko，<a class="ae me" href="https://link.springer.com/content/pdf/10.1007/BF02551274.pdf" rel="noopener ugc nofollow" target="_blank">通过叠加s形函数的近似法</a> (1989)，控制、信号和系统数学2(4):303–314。</p><p id="3585" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[19] K. Hornik，<a class="ae me" href="http://www.vision.jhu.edu/teaching/learning/deeplearning18/assets/Hornik-91.pdf" rel="noopener ugc nofollow" target="_blank">多层前馈网络的逼近能力</a> (1991)神经网络4(2):251–257。</p><p id="0bc8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[20] А.Г.Ивахненко, В.Г.кибернетическиепредсказывающие(1965年)。</p><p id="3e35" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[21] A. Ivakhnenko，<a class="ae me" href="http://gmdh.net/articles/history/polynomial.pdf" rel="noopener ugc nofollow" target="_blank">复杂系统的多项式理论</a> (1971)，IEEE Trans .系统、人和控制论4:364–378。</p><p id="2e15" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[22]反向传播基于微分的链式法则，该法则本身可追溯到1676年微分学的共同发明者戈特弗里德·威廉·冯·莱布尼茨。H. J. Kelley在《最佳飞行轨迹的梯度理论》( 1960年)Ars Journal 30(10):947-954中使用了反向传播的一个先驱，来完成复杂非线性多级系统的优化。在赫尔辛基大学S. Linnainmaa的芬兰硕士论文<a class="ae me" href="https://people.idsia.ch/~juergen/linnainmaa1970thesis.pdf" rel="noopener ugc nofollow" target="_blank"><em class="md">algorit min kumulativiinen pyoristysvirhe yksittaisten pyoristysvirheiden Taylor-kehitelmana</em></a>(1970)中描述了今天仍在使用的有效反向传播。在神经网络中的最早使用是由于P. J. Werbos，非线性灵敏度分析的进展的应用(1982)，<em class="md">系统建模和优化</em>762–770，Springer，这通常被引用为该方法的起源。参见J. Schmidhuber，<a class="ae me" href="https://arxiv.org/pdf/1404.7828.pdf" rel="noopener ugc nofollow" target="_blank">神经网络中的深度学习:概述</a> (2015)，神经网络61:85–117。</p><p id="5877" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[23] D. E. Rumelhart等人，<a class="ae me" href="https://www.nature.com/articles/323533a0.pdf" rel="noopener ugc nofollow" target="_blank">通过反向传播误差学习表征</a> (1986)，自然323(6088):533–536。</p><p id="8478" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[24]甚至还有连续<a class="ae me" href="https://en.wikipedia.org/wiki/Weierstrass_function" rel="noopener ugc nofollow" target="_blank">无处可微函数</a>的例子，如Weierstrass (1872)的构造。</p><p id="e544" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">【25】粗略地说，<a class="ae me" href="https://en.wikipedia.org/wiki/Lipschitz_continuity" rel="noopener ugc nofollow" target="_blank"> Lipschitz-continuous </a>函数不会任意缩小或扩大定义域上各点之间的距离。对于可微函数，Lipschitz连续性可以表示为梯度范数的上界，这意味着函数不会突然“跳跃”。</p><p id="a892" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[26]第一个使用这个术语的是理查德·贝尔曼，他在1957年出版的《T4》一书的序言中称维度是“多年来笼罩在物理学家和天文学家头上的诅咒”</p><p id="b4fe" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[27]在可观测的宇宙中，质子的数量，即所谓的<a class="ae me" href="https://en.wikipedia.org/wiki/Eddington_number" rel="noopener ugc nofollow" target="_blank">爱丁顿数</a>，是在10⁸⁰.估算出来的</p><p id="ba0e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[28] J. Lighthill，<a class="ae me" href="http://www.chilton-computing.org.uk/inf/literature/reports/lighthill_report/contents.htm" rel="noopener ugc nofollow" target="_blank">人工智能:一般调查</a> (1973)人工智能1–21，伦敦科学研究委员会。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="cd63" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">罗森布拉特、明斯基和帕佩特的肖像是由伊霍尔·戈尔斯基手绘的。几何深度学习的详细讲座资料可在 <a class="ae me" href="http://www.geometricdeeplearning.com/" rel="noopener ugc nofollow" target="_blank"> <em class="md">项目网页</em> </a> <em class="md">获取。参见迈克尔的</em> <a class="ae me" rel="noopener" target="_blank" href="https://towardsdatascience.com/graph-deep-learning/home"> <em class="md">其他帖子</em> </a> <em class="md">在走向数据科学，</em> <a class="ae me" href="https://michael-bronstein.medium.com/subscribe" rel="noopener"> <em class="md">订阅</em> </a> <em class="md">到他的帖子和</em> <a class="ae me" href="https://www.youtube.com/c/MichaelBronsteinGDL" rel="noopener ugc nofollow" target="_blank"> <em class="md"> YouTube频道</em> </a> <em class="md">，获得</em> <a class="ae me" href="https://michael-bronstein.medium.com/membership" rel="noopener"> <em class="md">中等会员资格</em> </a> <em class="md">，或者关注</em> <a class="ae me" href="https://twitter.com/mmbronstein" rel="noopener ugc nofollow" target="_blank"> <em class="md">迈克尔</em> </a> <em class="md">，”</em></p></div></div>    
</body>
</html>