<html>
<head>
<title>The Concept of Transformers and Training A Transformers Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变形金刚的概念和训练变形金刚模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-concept-of-transformers-and-training-a-transformers-model-45a09ae7fb50#2022-10-28">https://towardsdatascience.com/the-concept-of-transformers-and-training-a-transformers-model-45a09ae7fb50#2022-10-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9c26" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">变压器网络如何工作的逐步指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f41e76209b1eca0a253c5f948b977b11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U_AGItaRqf4Z_p33sA5caw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://pixabay.com/illustrations/artificial-intelligence-brain-think-3382507/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h2 id="8ead" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">什么是自然语言处理</strong></h2><p id="042e" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">自然语言处理是人工智能的一个分支，致力于赋予机器理解人类语言的能力。它使机器有可能阅读文本，理解语法结构，并解释句子中使用的单词的上下文含义。它在机器翻译中用于从一种语言翻译到另一种语言，常用的NLP翻译器是<em class="ml"> Google Translate </em>。谷歌翻译可用于将文档和网站从一种语言翻译成另一种语言，支持133种不同的语言。OpenAI GPT-3是创建的最先进的自然语言处理模型之一，它执行各种各样的语言任务，如文本生成，问答和文本摘要。情感分析是自然语言处理的一个重要分支，组织使用它来分析产品评论，以区分正面和负面评论。文本生成是自然语言处理的一个有趣的领域，它被用在手机的自动完成功能中，用于适当的单词建议，以及完成我们的句子。</p><p id="3851" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">NLP有不同的分支，我将解释其中的一些。</p><ul class=""><li id="3239" class="mr ms iq lu b lv mm ly mn lf mt lj mu ln mv mk mw mx my mz bi translated"><strong class="lu ir">情感分析</strong>:对文本进行分析，将文本的情感分为正面或负面。</li><li id="3829" class="mr ms iq lu b lv na ly nb lf nc lj nd ln ne mk mw mx my mz bi translated"><strong class="lu ir">文本生成</strong>:是文本的生成，在文本生成中我们提供文字提示，NLP模型自动补全句子。</li><li id="d51b" class="mr ms iq lu b lv na ly nb lf nc lj nd ln ne mk mw mx my mz bi translated"><strong class="lu ir">文本摘要</strong>:是利用NLP技术将长句归纳为短句。</li><li id="e69b" class="mr ms iq lu b lv na ly nb lf nc lj nd ln ne mk mw mx my mz bi translated"><strong class="lu ir">语言翻译</strong>:使用自然语言模型将文本从一种语言翻译成另一种语言，例如将英语句子翻译成法语句子。</li><li id="f709" class="mr ms iq lu b lv na ly nb lf nc lj nd ln ne mk mw mx my mz bi translated"><strong class="lu ir">屏蔽语言建模</strong>:利用NLP模型对句子中的屏蔽词进行预测。</li></ul><h2 id="13b7" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">什么是变压器网络</h2><p id="4b50" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">Transformer是一种神经网络架构，旨在解决自然语言处理任务。变压器网络使用一种称为注意力机制的机制来研究、理解句子中使用的单词的上下文，并从中提取有用的信息。《变形金刚》在流行论文<a class="ae kv" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> <em class="ml">中被介绍</em>由<em class="ml"> </em> Ashish Vaswani等人</a>。</p><h2 id="f542" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">变压器网络的类型</strong></h2><p id="670d" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">我们有三种主要类型的变压器网络，即编码器、解码器和序列2序列变压器网络。</p><p id="32d9" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">编码器变压器网络</strong>:这是一个双向变压器网络，它接收文本，为句子中的每个单词生成一个特征向量表示。编码器使用自我注意机制来理解句子中使用的单词的上下文，并从单词中提取有用的信息。</p><p id="1fbd" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">编码器如何能够理解这个简单的句子“<em class="ml">编码是惊人的</em>”的图示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/3a8f945785ba91ace802e411c32818c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2-oqy4hGrHXtO_n25rp_og.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><ul class=""><li id="06fb" class="mr ms iq lu b lv mm ly mn lf mt lj mu ln mv mk mw mx my mz bi translated"><strong class="lu ir">图分解:</strong>编码器利用自我注意机制为句子中的每个单词生成一个特征向量或数字表示。单词“<strong class="lu ir"><em class="ml"/></strong>”被分配一个特征向量1，单词“<strong class="lu ir"> <em class="ml">被分配一个特征向量2，单词“<strong class="lu ir"><em class="ml"/></strong>”被分配一个特征向量3。单词"<strong class="lu ir"> <em class="ml">是</em> </strong>"的特征向量既代表了单词"<strong class="lu ir"> <em class="ml">是</em> </strong>"的上下文，又代表了其两侧单词的特征向量信息，即"<strong class="lu ir"> <em class="ml">编码</em> </strong>"和"<strong class="lu ir"> <em class="ml">惊人的</em> </strong>"，因此得名双向网络，因为它研究的是左右两侧单词的上下文。特征向量用于研究单词之间存在的关系，以理解所用单词的上下文，并解释句子的意思。想象一下，这是一个情感分析任务，我们要对这个句子的情感是积极的还是消极的进行分类。通过研究每个单词的上下文以及它与每个单词的关系，网络已经能够理解这个句子，因此将这个句子分类为肯定的。它是正面的，因为我们描述的是“<strong class="lu ir"> <em class="ml">编码</em> </strong>”而我们用来描述它的形容词是“<strong class="lu ir"> <em class="ml">惊艳</em> </strong>”。</em></strong></li></ul><p id="1137" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">编码器网络用于解决分类问题，如掩蔽语言建模以预测句子中的掩蔽词，以及情感分析以预测句子中的正面和负面情感。常见的编码器变压器网络有BERT、ALBERT和DistilBERT。</p><p id="3651" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">解码器变压器网络或自回归模型</strong>。它使用掩蔽注意机制来理解句子中的上下文以生成单词。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/d480af9b63d21caed48b61e10d066959.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*CYN2-4bn59rBhr3wMVjBuQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><ul class=""><li id="1e2b" class="mr ms iq lu b lv mm ly mn lf mt lj mu ln mv mk mw mx my mz bi translated">想象一个简单的场景，我们在包含各种电影信息的文本语料库上训练解码器网络，以生成关于电影的句子或自动完成句子。我们将这个不完整的句子“<strong class="lu ir"> <em class="ml">【漫威复仇者联盟残局】</em> </strong>传入解码器模型，我们希望模型预测合适的单词来完成这个句子。解码器是一个单向网络，它为每个单词生成特征向量表示。它与编码器网络的区别在于它是单向的，而不是编码器的双向特性。解码器从单个上下文(右或左上下文)研究单词表示。在这种情况下，它会研究左边单词的上下文来生成下一个单词。它生成下一个单词“<strong class="lu ir"> <em class="ml">是</em> </strong>”，基于前面的单词，在“<strong class="lu ir"> <em class="ml">之后是</em> </strong>”它生成下一个单词“<strong class="lu ir"> <em class="ml">超级英雄</em> </strong>”，在“<strong class="lu ir"> <em class="ml">之后是</em> </strong>”它生成下一个单词<strong class="lu ir"><em class="ml">【a】，</em> </strong>最后它生成下一个单词<strong class="lu ir"> <em class="ml">【电影</em> </strong>因此完整的句子是“<strong class="lu ir"> <em class="ml">《漫威复仇者联盟》残局是一部超级英雄电影”</em> </strong>。我们可以观察它如何根据前面的单词生成单词，因此单词是自回归的，它必须向后看以研究单词的上下文，并从前面的单词中提取信息以生成后面的单词。自回归网络的例子有GPT-3和CTLR。</li></ul><p id="bd81" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">编码器-解码器或sequence 2序列变换器网络:</strong>它是编码器和解码器变换器网络的组合。它用于更复杂的自然语言任务，如翻译和文本摘要。</p><p id="1e40" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">编码器-解码器网络翻译图解:</strong>利用编码器网络对句子中的单词进行编码，生成单词的特征向量，理解上下文，从单词中提取有用的信息。来自编码器网络的输出被传递到解码器网络。解码器网络处理编码器生成的输出，并生成目标语言的适当单词，例如，我们将一个英语句子传递给编码器，它从英语上下文中提取有用的信息，并将其传递给解码器，解码器解码编码器输出，并生成法语句子。</p><h2 id="8a70" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">标记化概念</h2><p id="c624" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated"><strong class="lu ir">单词标记化:</strong>就是把一个句子转换成单个单词<strong class="lu ir">。</strong>它通常会生成很大的词汇量，这对于训练NLP模型来说并不理想。</p><p id="64bf" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">词汇量:是指一篇课文的字数。</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="cfa2" class="kw kx iq ni b gy nm nn l no np">text = "Python is my favourite programming language"</span><span id="1516" class="kw kx iq ni b gy nq nn l no np">print(text.split())</span><span id="5b25" class="kw kx iq ni b gy nq nn l no np">##Output<br/>['Python', 'is', 'my', 'favourite', 'programming', 'language']</span></pre><p id="04c7" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">这是一个示例代码，展示了单词标记化是如何完成的，我们将一个句子分割成单独的单词。</p><p id="dace" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">基于字符的标记化:</strong>是将句子中的单词转换成字符。例如，像“<em class="ml">你好，大家好</em>”这样的句子将被拆分成如下单个字符:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/278493465f3e435d4ca363fdce2b42fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*X_6RJ4AgWirPgfOMvKlEPg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="7e47" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">与单词标记化相比，它生成的词汇量较小，但还不够好，因为将单词拆分成单个字符与单个单词本身的含义不同。</p><p id="a926" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">子词标记化:</strong>这是大多数自然语言处理任务中使用的最好的标记化形式。单词标记化通过将句子拆分成单个单词来处理标记化，这种方法并不适合所有情况。两个单词"<strong class="lu ir"> <em class="ml">鸟</em> </strong> <em class="ml"> " </em>"和"<strong class="lu ir"> <em class="ml">鸟</em> </strong> <em class="ml"> " </em>在一个句子中，一个是单数，另一个是复数，单词标记化会将它们视为不同的单词，这就是子单词标记化的由来。子词标记化将合成词和生僻字划分为子词，它考虑了像<em class="ml"> bird </em>和<em class="ml"> birds、</em>这样的词的相似性，而不是将单词<strong class="lu ir"> <em class="ml"> birds </em> </strong>拆分成两个不同的词，它表示单词<strong class="lu ir"><em class="ml">【s】</em></strong>在单词<strong class="lu ir"><em class="ml"/></strong>的末尾是单词<strong class="lu ir"> <em class="ml">的子词像“<strong class="lu ir"> <em class="ml">有意义的</em> </strong>”这样的词会被拆分成“<strong class="lu ir"> <em class="ml">有意义的</em> </strong>”和子词“<strong class="lu ir"> <em class="ml"> ful </em> </strong>”，通常在这种情况下可以在子词中加上一个特殊的字符像“<strong class="lu ir"> <em class="ml"> ##ful </em> </strong>”，来表示它不是一个句子的开头，它是另一个词的子词。子字记号化算法被用在像伯特、GPT这样的变压器网络中。Bert使用单词片段标记器作为其子单词标记器。</em></strong></p><h2 id="ecb5" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">用变形金刚训练一个蒙面语言模型</h2><p id="3c45" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">我们的主要目标是使用编码器变压器网络来训练屏蔽语言模型，该模型可以为句子中的屏蔽词预测合适的词。在本教程中，我们将使用<a class="ae kv" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">拥抱脸变形金刚</a>，这是一个非常好的库，可以很容易地用变形金刚训练一个模型。</p><blockquote class="ns"><p id="3d4a" class="nt nu iq bd nv nw nx ny nz oa ob mk dk translated">这部分教程需要具备Python编程语言和Pytorch深度学习库的基础知识。</p></blockquote><p id="f8b8" class="pw-post-body-paragraph ls lt iq lu b lv oc jr lx ly od ju ma lf oe mc md lj of mf mg ln og mi mj mk ij bi translated"><strong class="lu ir">安装Pytorch </strong></p><div class="oh oi gp gr oj ok"><a href="https://pytorch.org/" rel="noopener  ugc nofollow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd ir gy z fp op fr fs oq fu fw ip bi translated">PyTorch</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">使用PyTorch、TorchServe和AWS Inferentia降低71%的推理成本并推动横向扩展。推动…的状态</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">pytorch.org</p></div></div><div class="ot l"><div class="ou l ov ow ox ot oy kp ok"/></div></div></a></div><p id="ba26" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">安装其他软件包</strong></p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="0b12" class="kw kx iq ni b gy nm nn l no np">pip3 install transformers</span><span id="1da4" class="kw kx iq ni b gy nq nn l no np">pip3 install datasets</span><span id="16da" class="kw kx iq ni b gy nq nn l no np">pip3 install accelerate</span></pre><p id="d251" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">微调电影评论的预训练屏蔽语言模型</strong></p><p id="7fb4" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">我们将利用一个预训练的DistilBERT transformer模型，一个在IMDb数据集(一个包含成千上万对不同电影的评论的数据集)上训练的BERT的简化版本，来预测句子中的屏蔽词。</p><h2 id="6705" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">加载和标记数据集</h2><p id="23dc" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">加载IMDB数据</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="a598" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">第1–10行:</strong>我们导入了用于加载IMDb数据集的模块，并打印出数据集信息以确认其已加载。我们还从数据集中随机打印出两篇评论。如果数据集加载正确，输出应该是:</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="d150" class="kw kx iq ni b gy nm nn l no np">DatasetDict({<br/>    train: Dataset({<br/>        features: ['text', 'label'],<br/>        num_rows: 25000<br/>    })<br/>    test: Dataset({<br/>        features: ['text', 'label'],<br/>        num_rows: 25000<br/>    })<br/>    unsupervised: Dataset({<br/>        features: ['text', 'label'],<br/>        num_rows: 50000<br/>    })<br/>})<br/>'&gt;&gt; Review: "Against All Flags" is every bit the classic swashbuckler. It has all the elements the adventure fan could hope for and more for in this one, the damsel in distress is, well, not really in distress. As Spitfire Stevens, Maureen O'Hara is at her athletic best, running her foes through in defiance of the social norms of the period. Anthony Quinn rounds out the top three billed actors as the ruthless Captain Roc Brasiliano and proves to be a wily and capable nemesis for Brian Hawke (Flynn). For the classic adventure fan, "Against All Flags" is a must-see. While it may not be in quite the same league as some of Errol Flynn's earlier work (Captain Blood and The Sea Hawk, for instance), it is still a greatly entertaining romp.'</span><span id="2a17" class="kw kx iq ni b gy nq nn l no np">'&gt;&gt; Review: Deathtrap gives you a twist at every turn, every single turn, in fact its biggest problem is that there are so many twists that you never really get oriented in the film, and it often doesn't make any sense, although they do usually catch you by surprise. The story is very good, except for the fact that it has so many twists. The screenplay is very good with great dialogue and characters, but you can't catch all the development <br/>because of the twists. The performances particularly by Caine are amazing. The direction is very good, Sidney Lumet can direct. The visual effects are fair, but than again most are actually in a play and are fake. Twists way to much, but still works and is worth watching.'</span></pre><p id="fc0c" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">它打印出IMDb数据集的轮廓、训练、测试和非监督部分，以及它们的行数。每行代表数据集中的一篇句子评论。训练和测试部分各有25000条评论，而非监督部分有50000条评论。最后两篇评论是从IMDB数据集中随机打印出来的。</p><h2 id="7cdd" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">标记化数据集</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="4204" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">第1–11行:</strong>导入了<strong class="lu ir"> <em class="ml"> Autokenizer包</em>，</strong>我们从DistilBERT模型中加载了tokenizer，这是一个<strong class="lu ir"> <em class="ml">词块子词块tokenizer </em> </strong>。我们创建了一个函数来表征IMDb数据集。</p><p id="3ae6" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">第<strong class="lu ir">13–15</strong>行:最后，我们调用了tokenizer函数，并将其应用于加载的数据集。当我们调用tokenizer函数时，我们从标记化的数据集中移除了文本和标签，因为不再需要它们了。我们打印了标记化的数据集，它显示了以下输出:</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="b6b8" class="kw kx iq ni b gy nm nn l no np">DatasetDict({<br/>    train: Dataset({<br/>        features: ['input_ids', 'attention_mask'],<br/>        num_rows: 25000<br/>    })<br/>    test: Dataset({<br/>        features: ['input_ids', 'attention_mask'],<br/>        num_rows: 25000<br/>    })<br/>    unsupervised: Dataset({<br/>        features: ['input_ids', 'attention_mask'],<br/>        num_rows: 50000<br/>    })<br/>})</span></pre><p id="da8d" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">在数据集的每一部分我们都有两个特征，<strong class="lu ir"><em class="ml"/></strong>和<strong class="lu ir"> <em class="ml">注意_屏蔽</em> </strong>。<strong class="lu ir"><em class="ml">input _ id</em></strong>是为分词生成的id。<strong class="lu ir"><em class="ml">attention _ mask</em></strong>是tokenizer模型生成的值，用来标识有用的单词的<strong class="lu ir"><em class="ml"/></strong>，以及要忽略的单词的输入id，attention值以1和0生成，1代表有用的单词，0代表要忽略的单词。</p><h2 id="3afb" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">串联和区块数据集</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="1de1" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">在自然语言处理中，我们需要为要训练的文本序列长度设置一个基准，要使用的DistilBERT预训练模型的最大长度是512。</p><p id="7425" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">第2–6行:</strong>我们将块大小设置为128。<strong class="lu ir"> </strong>因为GPU的利用，我们用了一个块大小为<em class="ml"> 128 </em>而不是<em class="ml"> 512 </em>。我们将数据集中的所有文本序列连接成一个单独的连接数据集。</p><p id="8d4a" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">第8–13行:</strong>我们获得了串联数据集的总长度，创建了一个字典理解来循环遍历串联长度，并根据128的块大小将该串联文本划分为块。如果有非常强大的GPU可用，则应该使用块大小512。连接的数据集被分成许多大小相等的块，但最后一个块通常较小，我们将删除最后一个块。</p><p id="adf1" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">第18–22行:</strong>带有组块的字典被赋予一个新的列标签，以包含组块样本的输入id。最后，我们在标记化的数据集上应用了concat chunk函数。</p><h2 id="9e52" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">用于评估的屏蔽测试数据集</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="c29d" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">第1–9行:</strong>我们从transformers导入了<strong class="lu ir"><em class="ml">DataCollatorForLanguageModeling</em></strong>，这是一个用于在数据集中创建屏蔽列的默认包。数据集被向下采样到10000，分割10%的样本，即1000个用于评估的测试数据集。</p><p id="460e" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">第13–24行:</strong>我们定义了一个数据收集器，并定义了一个在数据集中随机插入掩码的函数。对测试数据集应用插入随机掩码函数，用掩码列替换未掩码列。屏蔽的测试数据集将作为训练期间测试模型的基础事实标签。</p><h2 id="701a" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">培训程序</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="67c2" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">第9–23行:</strong>我们将批量大小设置为32，使用pytorch内置的数据加载器加载训练和测试数据集。我们加载了预训练的DistilBERT模型，并使用了Adam Optimizer。</p><p id="4cf5" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">第26–28行:</strong>我们调用transformers accelerator库进行训练，它接收预训练的模型、优化器、训练和评估数据集，为训练做准备。</p><p id="c2e2" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">第31–37行:</strong>我们设置了训练时期数，获得了训练数据加载器的长度，并计算了训练步数。最后，我们设置学习率调度功能，接受优化，热身步骤和训练步骤的训练。</p><h2 id="8e78" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">列车代码</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="92dc" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">第4–7行:</strong>我们使用python内置的<strong class="lu ir"> <em class="ml"> tqdm </em> </strong>定义了一个进度条用于训练进度监控，然后为输出训练好的模型设置一个目录。</p><p id="1bac" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">第9–19行:</strong>定义了一个for循环来循环通过多个历元，对于每个历元，我们开始数据集的训练，循环通过训练数据加载器，计算模型的输出，计算输出上的损失，使用变压器<strong class="lu ir"> <em class="ml">加速器</em> </strong>包导入来对模型执行反向传播，使用优化器来优化模型以最小化损失。我们应用了学习率调度器，使用优化器将累积梯度设置为零，并更新了进度条。我们这样做，直到我们完成一个时期的整个数据集的训练。</p><p id="577c" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">第22–38行:</strong>我们在测试数据集上评估了一个时期的训练模型，计算了测试数据集上的损失，类似于在训练期间所做的。我们计算了模型的交叉熵损失，然后计算了损失的指数，得到了模型的困惑度。</p><blockquote class="ns"><p id="f445" class="nt nu iq bd nv nw nx ny nz oa ob mk dk translated">困惑是一种用于评估语言模型的度量。它是交叉熵损失的指数。</p></blockquote><p id="a168" class="pw-post-body-paragraph ls lt iq lu b lv oc jr lx ly od ju ma lf oe mc md lj of mf mg ln og mi mj mk ij bi translated"><strong class="lu ir">第41–45行:</strong>我们使用<strong class="lu ir"> <em class="ml">加速器</em> </strong>保存预训练的模型，并使用标记器保存关于模型的重要文件，如标记器和词汇信息。<strong class="lu ir"> </strong>训练好的模型和配置文件保存在输出目录文件夹<strong class="lu ir"><em class="ml">MLP _训练好的模型</em> </strong>。输出目录将包含以下文件。我训练了30个纪元，得到了一个令人费解的值<strong class="lu ir"> <em class="ml"> 9.19 </em> </strong>。输出模型文件夹目录将如下所示:</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="e60e" class="kw kx iq ni b gy nm nn l no np">--<!-- -->MLP_TrainedModels<br/>    --config.json<br/>    --pytorch_model.bin<br/>    --<!-- -->special_tokens_map.json<br/>    --tokenizer_config.json<br/>    --tokenizer.json<br/>    --vocab.txt</span></pre><p id="15c6" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">全训码</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="2648" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">测试训练好的模型</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="5fd5" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">训练好的模型存储在<strong class="lu ir"><em class="ml">MLP _训练模型、</em> </strong>中，我们粘贴目录来设置模型值。我们打印出从模型中生成的句子列表，并为句子中的屏蔽词提供适当的值。</p><ul class=""><li id="e96e" class="mr ms iq lu b lv mm ly mn lf mt lj mu ln mv mk mw mx my mz bi translated">输出</li></ul><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="06f8" class="kw kx iq ni b gy nm nn l no np">&gt;&gt;&gt; this is an excellent movie.<br/>&gt;&gt;&gt; this is an amazing movie.<br/>&gt;&gt;&gt; this is an awesome movie.<br/>&gt;&gt;&gt; this is an entertaining movie.</span></pre><p id="0c94" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">我们可以从模型中看到对假面词的预测，分别是<strong class="lu ir"><em class="ml"/></strong><strong class="lu ir"><em class="ml">惊艳</em></strong><strong class="lu ir"><em class="ml">牛逼</em> </strong>和<strong class="lu ir"> <em class="ml">娱乐</em> </strong>。这些预测与完成句子完全吻合。</p><p id="bae2" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">我们已经成功地训练了一个带有编码器转换器网络的屏蔽语言模型，该网络可以找到正确的单词来替换句子中的屏蔽单词。</p><blockquote class="ns"><p id="6d60" class="nt nu iq bd nv nw nx ny nz oa ob mk dk translated">我已经将我训练的屏蔽语言模型推送到huggingface hub，它可供测试。<strong class="ak">检查拥抱人脸库上的蒙版语言模型</strong></p></blockquote><div class="pb pc pd pe pf ok"><a href="https://huggingface.co/ayoolaolafenwa/Masked-Language-Model" rel="noopener  ugc nofollow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd ir gy z fp op fr fs oq fu fw ip bi translated">ayoolaolafenwa/蒙面语言模型拥抱脸</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">我们正在通过开源和开放科学来推进和民主化人工智能的旅程。</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">huggingface.co</p></div></div><div class="ot l"><div class="pg l ov ow ox ot oy kp ok"/></div></div></a></div><h2 id="e27a" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">测试屏蔽语言模型的Rest API代码</strong></h2><p id="0b6d" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">这是直接从拥抱脸测试屏蔽语言模型的推理API python代码。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="2929" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">输出</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="6c99" class="kw kx iq ni b gy nm nn l no np">washington dc is the capital of usa.</span></pre><p id="0e6e" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">它产生正确的输出，<strong class="lu ir"> <em class="ml">华盛顿特区是美国的首都。</em> </strong></p><h2 id="2f59" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">用转换器加载屏蔽语言模型</h2><p id="fb72" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">使用这段代码，您可以轻松地用转换器加载语言模型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="349d" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">输出</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="09e2" class="kw kx iq ni b gy nm nn l no np">is</span></pre><p id="9ce4" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">它打印出预测的掩码字“<strong class="lu ir"> <em class="ml">是</em> </strong>”。</p><h2 id="2f7e" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">Colab培训</h2><p id="9d03" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">我创建了一个google colab笔记本，上面有创建拥抱脸账户、训练蒙面语言模型以及将模型上传到拥抱脸库的步骤。检查笔记本。</p><div class="oh oi gp gr oj ok"><a href="https://colab.research.google.com/drive/1BymoZgVU0q02zYv1SdivK-wChG-ooMXL?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd ir gy z fp op fr fs oq fu fw ip bi translated">谷歌联合实验室</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">编辑描述</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">colab.research.google.com</p></div></div><div class="ot l"><div class="ph l ov ow ox ot oy kp ok"/></div></div></a></div><blockquote class="ns"><p id="2024" class="nt nu iq bd nv nw pi pj pk pl pm mk dk translated">查看github资源库以获取本教程</p></blockquote><div class="pb pc pd pe pf ok"><a href="https://github.com/ayoolaolafenwa/TrainNLP" rel="noopener  ugc nofollow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd ir gy z fp op fr fs oq fu fw ip bi translated">GitHub - ayoolaolafenwa/TrainNLP:训练自然语言处理模型的示例教程…</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">这是一个一步一步的指南使用拥抱脸变形器创建一个蒙面语言模型来预测一个蒙面词…</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">github.com</p></div></div><div class="ot l"><div class="pn l ov ow ox ot oy kp ok"/></div></div></a></div><h2 id="8c3b" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">结论</h2><p id="0ae5" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">我们在本文中详细讨论了自然语言处理的基础知识、转换器的工作原理、不同类型的转换器网络、使用转换器训练屏蔽语言模型的过程，并且我们成功训练了一个可以预测句子中屏蔽单词的转换器模型。</p><h2 id="64e9" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">参考文献</strong></h2><p id="497e" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated"><a class="ae kv" href="https://huggingface.co/course/chapter1/2?fw=pt" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/course/chapter1/2?fw=pt</a></p><p id="5f7b" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><a class="ae kv" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">https://github.com/huggingface/transformers</a></p><p id="10e7" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><strong class="lu ir">通过以下方式联系我:</strong></p><p id="007f" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">电子邮件:<a class="ae kv" href="https://mail.google.com/mail/u/0/#inbox" rel="noopener ugc nofollow" target="_blank">olafenwaayoola@gmail.com</a></p><p id="783f" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">领英:<a class="ae kv" href="https://www.linkedin.com/in/ayoola-olafenwa-003b901a9/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/ayoola-olafenwa-003b901a9/</a></p><p id="ed16" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">推特:<a class="ae kv" href="https://twitter.com/AyoolaOlafenwa" rel="noopener ugc nofollow" target="_blank"> @AyoolaOlafenwa </a></p></div></div>    
</body>
</html>