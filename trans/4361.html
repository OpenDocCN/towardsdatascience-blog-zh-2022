<html>
<head>
<title>A Practical Guide to Feature Selection Using Sklearn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Sklearn进行功能选择的实用指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-practical-guide-to-feature-selection-using-sklearn-b3efa176bd96#2022-09-27">https://towardsdatascience.com/a-practical-guide-to-feature-selection-using-sklearn-b3efa176bd96#2022-09-27</a></blockquote><div><div class="fc ij ik il im in"/><div class="io ip iq ir is"><div class=""/><div class=""><h2 id="1337" class="pw-subtitle-paragraph js iu iv bd b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj dk translated">关于如何使用Python和scikit-learn为您的模型选择最佳要素的实践教程</h2></div><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi kk"><img src="../Images/a55cd0e0172bd6816cb81955c021235d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xQoNyqFDIg1LpriG"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">JOSHUA COLEMAN 在<a class="ae la" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="5821" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">构建预测模型时，我们的数据集中通常有许多可用于训练模型的特征或变量。然而，仅仅因为该特征存在于我们的数据集中，并不意味着它与我们的模型相关或者我们应该使用它。</p><p id="6feb" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">那么我们如何知道在我们的模型中使用哪些特性呢？</p><p id="18e6" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">这就是特性选择的用武之地。特征选择只是一个减少输入变量数量的过程，目的是只保留最重要的<em class="lx">变量。</em></p><p id="9956" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">减少输入特征的数量具有优势，因为它简化了模型，降低了计算成本，并且还可以提高模型的性能。</p><p id="1f9e" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">现在，我们如何决定哪个特性是重要的？一个特性变得<em class="lx">重要</em>意味着什么？</p><p id="dcb7" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">对此没有明确的答案，所以我们需要用不同的方法进行实验，看看哪种方法能得到最好的结果。</p><p id="d302" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在本文中，我们将探索并实现三种不同的特性选择方法:</p><ul class=""><li id="1332" class="ly lz iv ld b le lf lh li lk ma lo mb ls mc lw md me mf mg bi translated">方差阈值</li><li id="df5e" class="ly lz iv ld b le mh lh mi lk mj lo mk ls ml lw md me mf mg bi translated">k最佳功能</li><li id="62df" class="ly lz iv ld b le mh lh mi lk mj lo mk ls ml lw md me mf mg bi translated">递归特征消除(RFE)</li></ul><p id="264b" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">每种方法都有自己的重要性定义，我们将看到它如何影响我们模型的性能。我们将使用Python和scikit-learn实现每个方法。在任何时候，你都可以在GitHub上查看完整的源代码。</p><blockquote class="mm"><p id="20e5" class="mn mo iv bd mp mq mr ms mt mu mv lw dk translated">以后的文章不要错过！<a class="ae la" href="https://www.datasciencewithmarco.com/blog" rel="noopener ugc nofollow" target="_blank">订阅</a>我的列表，将新内容直接发送到您的收件箱！</p></blockquote><p id="2f23" class="pw-post-body-paragraph lb lc iv ld b le mw jw lg lh mx jz lj lk my lm ln lo mz lq lr ls na lu lv lw io bi translated">我们开始吧！</p><h1 id="045e" class="nb nc iv bd nd ne nf ng nh ni nj nk nl kb nm kc nn ke no kf np kh nq ki nr ns bi translated">获取数据</h1><p id="4b14" class="pw-post-body-paragraph lb lc iv ld b le nt jw lg lh nu jz lj lk nv lm ln lo nw lq lr ls nx lu lv lw io bi translated">第一步自然是获取我们将在本教程中使用的数据。</p><p id="05a4" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">这里，我们使用sklearn上的葡萄酒数据集。数据集包含178行，13个特征和一个包含三个唯一类别的目标。因此，这是一项分类任务。</p><pre class="kl km kn ko gt ny nz oa ob aw oc bi"><span id="bd51" class="od nc iv nz b gy oe of l og oh">import pandas as pd<br/>import numpy as np<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt</span><span id="8ce1" class="od nc iv nz b gy oi of l og oh">from sklearn.datasets import load_wine</span><span id="d705" class="od nc iv nz b gy oi of l og oh">wine_data = load_wine()</span><span id="1426" class="od nc iv nz b gy oi of l og oh">wine_df = pd.DataFrame(<br/>    data=wine_data.data, <br/>    columns=wine_data.feature_names)<br/>wine_df['target'] = wine_data.target</span></pre><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi oj"><img src="../Images/3be2e845d0342fee8ded395e2c5bf1f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vDWTcFCNw2k0EkqtlQXIvw.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">截断的葡萄酒数据集。作者图片</p></figure><p id="6ae8" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">从上面的图片中，我们可以看到我们有不同的葡萄酒特征，这将有助于我们对它进行分类。</p><p id="c6b7" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">我们可以选择绘制一些要素的箱线图，以查看是否有重叠。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ok"><img src="../Images/97fe91ba1fd30c72156965768dad9b0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pCONA-RExE_CIcOwZjivcQ.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">前三个特征的箱线图。我们可以看到酒精没有太多重叠，但苹果酸和灰分有一些重叠。图片由作者提供。</p></figure><p id="2469" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">从上面的图片中，我们可以看到<em class="lx">酒精</em>在每个类别之间没有太多重叠，而其他两个特征显然有一些重叠。</p><p id="a8b9" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">理想情况下，我们不会有任何重叠，这将使我们能够对每种葡萄酒进行完美的分类，但这不是这里的情况。</p><p id="8fbd" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">为了将重点放在特征选择技术上，我们在这里不做更多的探索。因此，在开始特性选择之前，让我们将数据分成训练集和测试集。</p><h2 id="0e2c" class="od nc iv bd nd ol om dn nh on oo dp nl lk op oq nn lo or os np ls ot ou nr ov bi translated">拆分数据</h2><p id="9e19" class="pw-post-body-paragraph lb lc iv ld b le nt jw lg lh nu jz lj lk nv lm ln lo nw lq lr ls nx lu lv lw io bi translated">在实现特征选择技术之前，我们首先将数据分成训练集和测试集。</p><p id="32b7" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">这样，我们就有了固定的起点和固定的测试集，这样我们就可以比较每种特征选择方法对模型性能的影响。</p><pre class="kl km kn ko gt ny nz oa ob aw oc bi"><span id="9dfa" class="od nc iv nz b gy oe of l og oh">from sklearn.model_selection import train_test_split</span><span id="d239" class="od nc iv nz b gy oi of l og oh">X = wine_df.drop(['target'], axis=1)<br/>y = wine_df['target']</span><span id="346b" class="od nc iv nz b gy oi of l og oh">X_train, X_test, y_train, y_test = train_test_split(X, <br/>                                                    y, <br/>                                                    test_size=0.3, <br/>                                                    shuffle=True, <br/>                                                    stratify=y, <br/>                                                    random_state=42)</span></pre><p id="2a62" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">通过这种拆分，我们有124个样本用于训练集，54个样本用于测试集。注意<em class="lx">分层</em>的使用，它确保训练集和测试集包含相同比例的目标类。</p><p id="b5e5" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">我们现在准备测试和实现不同的特性选择方法！</p><h1 id="550e" class="nb nc iv bd nd ne nf ng nh ni nj nk nl kb nm kc nn ke no kf np kh nq ki nr ns bi translated">选择最佳功能</h1><p id="9f52" class="pw-post-body-paragraph lb lc iv ld b le nt jw lg lh nu jz lj lk nv lm ln lo nw lq lr ls nx lu lv lw io bi translated">我们现在深入研究特征选择方法。如上所述，我们将尝试三种不同的方法，看看它如何影响模型的性能。</p><p id="1508" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">为了使这个实验更加健壮，我们将使用一个简单的决策树分类器。</p><h2 id="379e" class="od nc iv bd nd ol om dn nh on oo dp nl lk op oq nn lo or os np ls ot ou nr ov bi translated">方差阈值</h2><p id="44ab" class="pw-post-body-paragraph lb lc iv ld b le nt jw lg lh nu jz lj lk nv lm ln lo nw lq lr ls nx lu lv lw io bi translated">我们将探讨的第一个方法是方差阈值。当然，这是基于方差的，方差是离差的一种度量。换句话说，它衡量的是一组数字与它们的平均值相差多远。</p><p id="0318" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">例如，[1，1，1，1，1]的方差为0，因为每个数都等于它们的平均值。因此，它们不会超出平均值。</p><p id="4c87" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated"><strong class="ld iw">方差阈值</strong>然后简单地移除方差低于给定阈值的任何特征。</p><p id="822b" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">我们可以看到这对于移除方差接近于0的要素是多么有用，因为这意味着数据集的所有样本中的值都是恒定的或仅略有变化。因此，它们没有任何预测能力。</p><p id="0ce5" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">因此，让我们比较一下训练集中每个特征的方差。</p><pre class="kl km kn ko gt ny nz oa ob aw oc bi"><span id="8db4" class="od nc iv nz b gy oe of l og oh">X_train_v1 = X_train.copy()</span><span id="8134" class="od nc iv nz b gy oi of l og oh">X_train_v1.var(axis=0)</span></pre><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/89797b10c2a4d45746eac4eae535e955.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*XjvLKyCD4KivroalZHkM4A.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">训练集中每个特征的方差。图片由作者提供。</p></figure><p id="6957" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在上图中，我们可以看到训练集每个特征的方差。然而，我们现在还不能定义方差阈值，因为我们的数据没有相同的尺度，所以方差也不在相同的尺度上。</p><p id="eb68" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">较大比例的数据可能比较小比例的要素具有更高的方差，即使它们的分布相似。</p><p id="54e1" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">因此，在定义阈值之前，首先调整我们的数据是很重要的。在这里，我们将数据归一化，然后计算方差。</p><pre class="kl km kn ko gt ny nz oa ob aw oc bi"><span id="bebe" class="od nc iv nz b gy oe of l og oh">from sklearn.preprocessing import Normalizer</span><span id="4d1d" class="od nc iv nz b gy oi of l og oh">norm = Normalizer().fit(X_train_v1)</span><span id="bb63" class="od nc iv nz b gy oi of l og oh">norm_X_train = norm.transform(X_train_v1)</span><span id="34bd" class="od nc iv nz b gy oi of l og oh">norm_X_train.var(axis=0)</span></pre><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ox"><img src="../Images/21a70f82ad2c0c2dfded1f8ec9fc00b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cY6Jj2SDw1iKaM0qsTkrWg.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">归一化特征的方差。图片由作者提供。</p></figure><p id="1a81" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">从上图中，我们现在可以看到每个缩放特征的方差。虽然所有特性的方差都很小，但有些特性的方差小得令人难以置信，其幂为-8到-7。</p><p id="e450" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">因此，让我们将阈值设置为1e-6。方差低于该阈值的任何要素都将被移除。</p><pre class="kl km kn ko gt ny nz oa ob aw oc bi"><span id="300e" class="od nc iv nz b gy oe of l og oh">from sklearn.feature_selection import VarianceThreshold</span><span id="fe72" class="od nc iv nz b gy oi of l og oh">selector = VarianceThreshold(threshold = 1e-6)<br/>selected_features = selector.fit_transform(norm_X_train)</span><span id="abb0" class="od nc iv nz b gy oi of l og oh">selected_features.shape</span></pre><p id="0f66" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">这里去掉了两个特征，即<em class="lx">色相</em>和<em class="lx">非黄酮类_酚类</em>。</p><p id="5fd5" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">我们现在可以看到决策树分类器在使用所有可用特征时的表现，我们删除了上面提到的两个特征。</p><pre class="kl km kn ko gt ny nz oa ob aw oc bi"><span id="4d8e" class="od nc iv nz b gy oe of l og oh">from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.metrics import f1_score</span><span id="98fe" class="od nc iv nz b gy oi of l og oh">dt = DecisionTreeClassifier(random_state=42)</span><span id="3dba" class="od nc iv nz b gy oi of l og oh">#Classifier with all features<br/>dt.fit(X_train, y_train)<br/>preds = dt.predict(X_test)</span><span id="f229" class="od nc iv nz b gy oi of l og oh">f1_score_all = round(f1_score(y_test, preds, average='weighted'),3)</span><span id="aa76" class="od nc iv nz b gy oi of l og oh"># Classifier with selected features with variance threshold</span><span id="9d52" class="od nc iv nz b gy oi of l og oh">X_train_sel = X_train.drop(['hue', 'nonflavanoid_phenols'], axis=1)<br/>X_test_sel = X_test.drop(['hue', 'nonflavanoid_phenols'], axis=1)</span><span id="3368" class="od nc iv nz b gy oi of l og oh">dt.fit(X_train_sel, y_train)<br/>preds_sel = dt.predict(X_test_sel)</span><span id="b47f" class="od nc iv nz b gy oi of l og oh">f1_score_sel = round(f1_score(y_test, preds_sel, average='weighted'), 3)</span></pre><p id="389e" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">打印两种型号的F1分数，得到0.963。因此，删除两个特征并没有改进模型，但是我们确实实现了相同的性能。</p><p id="e837" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">因此，具有较少特征的更简单的模型获得了与使用所有特征相同的结果，这是一个好迹象。</p><p id="3e77" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">现在，使用方差阈值有些简单，设置阈值有些随意。</p><p id="734c" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">使用下一种方法，我们可以很容易地找到在决定选择标准时要保留的变量的最佳数量。</p><h2 id="1c7e" class="od nc iv bd nd ol om dn nh on oo dp nl lk op oq nn lo or os np ls ot ou nr ov bi translated">k最佳功能</h2><p id="1d2f" class="pw-post-body-paragraph lb lc iv ld b le nt jw lg lh nu jz lj lk nv lm ln lo nw lq lr ls nx lu lv lw io bi translated">在这里，我们使用一种方法，这种方法在评估一个特性的重要性时提供了更多的灵活性。</p><p id="c03e" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">算法很简单:我们简单地提供一种方法来计算一个特性的重要性和我们想要使用的特性的数量，记为<em class="lx"> k </em>。然后，算法简单地返回顶部的<em class="lx"> k </em>特征。</p><p id="ee33" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">这种方法的主要优点是，我们可以自由选择各种方法来计算特征的重要性。例如，我们可以使用<strong class="ld iw">卡方检验</strong>来量化一个特性对目标的独立性。分数越高，特征和目标之间的相关性就越高，因此该特征的重要性就越高。</p><p id="2e33" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">可以使用其他方法，例如计算互信息、使用假阳性率测试或计算回归任务的F统计量。</p><p id="d7ff" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">现在，我们仍然面临着确定应该为模型选择多少变量的挑战。在这里，由于我们总共只使用了13个特性，让我们试着对所有特性使用一个特性，看看哪种配置能得到最好的结果。</p><p id="beac" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">这里，我们使用卡方检验，因为我们正在处理一个分类任务。</p><pre class="kl km kn ko gt ny nz oa ob aw oc bi"><span id="76b5" class="od nc iv nz b gy oe of l og oh">from sklearn.feature_selection import SelectKBest<br/>from sklearn.feature_selection import chi2</span><span id="f6a8" class="od nc iv nz b gy oi of l og oh">X_train_v2, X_test_v2, y_train_v2, y_test_v2 = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()</span><span id="87c6" class="od nc iv nz b gy oi of l og oh">f1_score_list = []</span><span id="a4e3" class="od nc iv nz b gy oi of l og oh">for k in range(1, 14):<br/>    selector = SelectKBest(chi2, k=k)<br/>    selector.fit(X_train_v2, y_train_v2)<br/>    <br/>    sel_X_train_v2 = selector.transform(X_train_v2)<br/>    sel_X_test_v2 = selector.transform(X_test_v2)<br/>    <br/>    dt.fit(sel_X_train_v2, y_train_v2)<br/>    kbest_preds = dt.predict(sel_X_test_v2)</span><span id="3f4a" class="od nc iv nz b gy oi of l og oh">f1_score_kbest = round(f1_score(y_test, kbest_preds, average='weighted'), 3)<br/>    f1_score_list.append(f1_score_kbest)<br/>  <br/>print(f1_score_list)</span></pre><p id="c608" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">我们现在可以绘制模型中使用的每个变量的F1分数:</p><pre class="kl km kn ko gt ny nz oa ob aw oc bi"><span id="7d1d" class="od nc iv nz b gy oe of l og oh">fig, ax = plt.subplots(figsize=(12, 6))</span><span id="3cb8" class="od nc iv nz b gy oi of l og oh">x = ['1','2','3','4','5','6','7','8','9','10','11','12','13']<br/>y = f1_score_list</span><span id="659e" class="od nc iv nz b gy oi of l og oh">ax.bar(x, y, width=0.4)<br/>ax.set_xlabel('Number of features (selected using chi2 test)')<br/>ax.set_ylabel('F1-Score (weighted)')<br/>ax.set_ylim(0, 1.2)</span><span id="4dd2" class="od nc iv nz b gy oi of l og oh">for index, value in enumerate(y):<br/>    plt.text(x=index, y=value + 0.05, s=str(value), ha='center')<br/>    <br/>plt.tight_layout()</span></pre><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ok"><img src="../Images/6512d7676f1346fd67371d8358cf32dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_NyMoOrs3uNaHWp4WhxShQ.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">f1-根据模型中使用的特征数量进行评分。图片由作者提供。</p></figure><p id="417a" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">从上图中我们可以看出，根据卡方检验，仅使用最重要的特征会产生最差的性能，而使用前11个特征会产生完美的F1分数。但是，我们还要注意，使用前6个特性会得到0.981的F1分数，考虑到我们使用了一半的特性，这是一个小缺点。此外，我们看到仅使用前4项功能也有类似的性能。</p><p id="5701" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">通过这个例子，我们可以清楚地看到特征选择不仅可以简化模型，还可以提高模型的性能，即使我们正在处理一个简单的玩具数据集。</p><p id="32ab" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">当然，使用不同的要素重要性赋值器可能会导致不同的结果。我邀请你使用另一个测试，而不是卡方测试，自己看看结果图是否有很大的变化。</p><p id="503b" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">让我们继续讨论我们将实现的最后一个方法:<strong class="ld iw">R</strong>E反转<strong class="ld iw"> F </strong>特性<strong class="ld iw"> E </strong>消除或<strong class="ld iw"> RFE </strong>。</p><h2 id="c656" class="od nc iv bd nd ol om dn nh on oo dp nl lk op oq nn lo or os np ls ot ou nr ov bi translated">递归特征消除(RFE)</h2><p id="f25a" class="pw-post-body-paragraph lb lc iv ld b le nt jw lg lh nu jz lj lk nv lm ln lo nw lq lr ls nx lu lv lw io bi translated">我们在本文中实现的最后一种方法是递归特征消除或RFE。</p><p id="82e0" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">下图显示了RFE程序。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/94a3f1cfed52eb6b6c7a851f8b8a26bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*1VKA3nz__aI_a_Cf1yrHlA.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">RFE算法。图片由作者提供。</p></figure><p id="abcf" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">从上图中，我们看到RFE也要求我们设置一些特性。从那里开始，它将逐渐删除最不重要的功能，直到它达到所需的功能数量。</p><p id="f815" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">这种特征选择方法的有趣之处在于，它依赖于模型评估特征重要性的能力。因此，我们必须使用返回系数或特征重要性度量的模型。</p><p id="2cd8" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在这里，由于我们使用了决策树，模型实际上可以计算一个特性的重要性。</p><p id="090e" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在决策树中，特征的重要性是通过节点杂质的减少乘以到达该节点的概率来计算的。这是基于在决策树后台运行的CART算法。你可以在这里阅读更多相关信息<a class="ae la" href="https://www.analyticssteps.com/blogs/classification-and-regression-tree-cart-algorithm" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="76f0" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">现在，让我们测试RFE选择方法。我们将输入变量的数量设置为4，并查看与使用卡方检验选择前4个特征相比，性能如何。这样，我们将看到哪种评估特征重要性的方法(卡方检验或决策树计算的特征重要性)可以为我们的模型选择最佳的四个特征。</p><p id="3762" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">使用sklearn实现RFE非常简单。首先，我们定义选择器并指定我们想要的特性数量。在这种情况下，我们需要四个。然后，我们在训练集上安装选择器，这将为我们返回前4个特征。</p><pre class="kl km kn ko gt ny nz oa ob aw oc bi"><span id="1fd8" class="od nc iv nz b gy oe of l og oh">from sklearn.feature_selection import RFE</span><span id="5b0d" class="od nc iv nz b gy oi of l og oh">X_train_v3, X_test_v3, y_train_v3, y_test_v3 = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()</span><span id="5e76" class="od nc iv nz b gy oi of l og oh">RFE_selector = RFE(estimator=dt, n_features_to_select=4, step=1)</span><span id="894c" class="od nc iv nz b gy oi of l og oh">RFE_selector.fit(X_train_v3, y_train_v3)</span></pre><p id="4900" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">我们还可以通过访问<em class="lx"> support_ </em>属性来查看所选特性的名称。</p><pre class="kl km kn ko gt ny nz oa ob aw oc bi"><span id="c489" class="od nc iv nz b gy oe of l og oh">X_train_v3.columns[RFE_selector.support_]</span></pre><p id="e3d1" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在这里，我们看到前4大功能是:</p><ul class=""><li id="63fd" class="ly lz iv ld b le lf lh li lk ma lo mb ls mc lw md me mf mg bi translated">灰分的碱性</li><li id="72f0" class="ly lz iv ld b le mh lh mi lk mj lo mk ls ml lw md me mf mg bi translated">黄酮类化合物</li><li id="f907" class="ly lz iv ld b le mh lh mi lk mj lo mk ls ml lw md me mf mg bi translated">颜色强度</li><li id="79c5" class="ly lz iv ld b le mh lh mi lk mj lo mk ls ml lw md me mf mg bi translated">脯氨酸</li></ul><p id="131e" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">请记住，根据决策树本身的评估，这是前4个最重要的特征。</p><p id="d3e2" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">现在，我们可以仅使用前4个特征来拟合模型，并计算测试集的F1分数。</p><pre class="kl km kn ko gt ny nz oa ob aw oc bi"><span id="7c3b" class="od nc iv nz b gy oe of l og oh">sel_X_train_v3 = RFE_selector.transform(X_train_v3)<br/>sel_X_test_v3 = RFE_selector.transform(X_test_v3)</span><span id="42b3" class="od nc iv nz b gy oi of l og oh">dt.fit(sel_X_train_v3, y_train_v3)</span><span id="9528" class="od nc iv nz b gy oi of l og oh">RFE_preds = dt.predict(sel_X_test_v3)</span><span id="f00b" class="od nc iv nz b gy oi of l og oh">rfe_f1_score = round(f1_score(y_test_v3, RFE_preds, average='weighted'),3)</span><span id="990f" class="od nc iv nz b gy oi of l og oh">print(rfe_f1_score)</span></pre><p id="5627" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">或者，我们可以直观显示仅使用前4项功能的两种模型的性能。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/8f095073280cdd7f69aa5d584077eb5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*_5A6JHb834b-rqhzrJSHuw.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">f1-使用根据不同方法选择的前4个特征的两个模型的得分。图片由作者提供。</p></figure><p id="c4c5" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">从上图可以看出，卡方检验选出的前4个特性带来了更好的性能。</p><p id="122b" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">现在，带回家的信息不是K最佳特征是比RFE更好的特征选择方法。主要区别在于如何计算特征的重要性。一个使用卡方检验，而另一个使用决策树计算的特征重要性。</p><p id="536f" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">因此，我们可以得出结论，在这种情况下，使用卡方检验是评估前4个最佳特性的更好方法。</p><h1 id="df54" class="nb nc iv bd nd ne nf ng nh ni nj nk nl kb nm kc nn ke no kf np kh nq ki nr ns bi translated">结论</h1><p id="ceeb" class="pw-post-body-paragraph lb lc iv ld b le nt jw lg lh nu jz lj lk nv lm ln lo nw lq lr ls nx lu lv lw io bi translated">在本文中，我们研究了特征选择，这是一种减少模型中的特征数量以简化模型并提高其性能的方法。</p><p id="93c6" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">我们探索并实现了三种不同的特征选择方法:</p><ul class=""><li id="dd5b" class="ly lz iv ld b le lf lh li lk ma lo mb ls mc lw md me mf mg bi translated">方差阈值</li><li id="bd10" class="ly lz iv ld b le mh lh mi lk mj lo mk ls ml lw md me mf mg bi translated">k最佳功能</li><li id="7ca6" class="ly lz iv ld b le mh lh mi lk mj lo mk ls ml lw md me mf mg bi translated">递归特征消除(RFE)</li></ul><p id="d657" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">方差阈值有利于移除方差为0的要素，因为常量变量肯定不是好变量。但是，设置方差阈值很难，而且相当随意，我建议将它与K最佳要素或RFE一起使用。</p><p id="0f53" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">有了K个最佳特征，我们就能够选择如何评估一个特征的重要性，这也允许我们确定在我们的模型中包括的最佳方法和最佳数量的特征。</p><p id="c695" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">最后，RFE是另一种特征选择方法，它依靠模型本身来计算特征的重要性。</p><p id="0b59" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">请注意，没有选择特性的最佳方式。重要的是测试不同的特征选择方法和不同的特征重要性评估方法，以查看哪种方法对于给定的问题最有效。</p><p id="384a" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">谢谢你的阅读，我希望学到一些有用的东西！</p><p id="80b3" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">干杯🍺</p></div></div>    
</body>
</html>