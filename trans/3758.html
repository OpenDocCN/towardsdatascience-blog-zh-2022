<html>
<head>
<title>5 Popular CNN Architectures Clearly Explained and Visualized</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">5种流行的CNN架构得到清晰的解释和可视化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/5-most-well-known-cnn-architectures-visualized-af76f1f0065e#2022-08-22">https://towardsdatascience.com/5-most-well-known-cnn-architectures-visualized-af76f1f0065e#2022-08-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="37ea" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">深度学习</h2><div class=""/><div class=""><h2 id="f39e" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">为什么盗梦空间看起来像三叉戟？！</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/4e13ce1dd17102a27932182ac7ab7c96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G2QNIL0iEgzi-6hMvB54sg.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="316d" class="ln lo it lj b gy lp lq l lr ls"><strong class="lj jd">Table of Contents</strong></span><span id="4034" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj jd">· </strong><a class="ae lh" href="#c7c5" rel="noopener ugc nofollow"><strong class="lj jd">Fully Connected Layer and Activation Function</strong></a></span><span id="bf3d" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj jd">· </strong><a class="ae lh" href="#60dd" rel="noopener ugc nofollow"><strong class="lj jd">Convolution and Pooling Layer</strong></a></span><span id="0336" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj jd">· </strong><a class="ae lh" href="#b55c" rel="noopener ugc nofollow"><strong class="lj jd">Normalization Layer</strong></a><br/>  ∘ <a class="ae lh" href="#0f76" rel="noopener ugc nofollow">Local Response Normalization</a><br/>  ∘ <a class="ae lh" href="#182a" rel="noopener ugc nofollow">Batch Normalization</a></span><span id="e02f" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj jd">· </strong><a class="ae lh" href="#8f20" rel="noopener ugc nofollow"><strong class="lj jd">5 Most Well-Known CNN Architectures Visualized</strong></a><br/>  ∘ <a class="ae lh" href="#7ebd" rel="noopener ugc nofollow">LeNet-5</a><br/>  ∘ <a class="ae lh" href="#c691" rel="noopener ugc nofollow">AlexNet</a><br/>  ∘ <a class="ae lh" href="#8ab0" rel="noopener ugc nofollow">VGG-16</a><br/>  ∘ <a class="ae lh" href="#21f4" rel="noopener ugc nofollow">Inception-v1</a><br/>  ∘ <a class="ae lh" href="#6702" rel="noopener ugc nofollow">ResNet-50</a></span><span id="080d" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj jd">· </strong><a class="ae lh" href="#87a2" rel="noopener ugc nofollow"><strong class="lj jd">Wrapping Up</strong></a></span></pre><p id="0b6a" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi mq translated">Yann LeCun在1990年提出的<strong class="lw jd"> LeNet </strong>激发了深度神经网络在实践中的可能性。然而，有限的计算能力和存储容量使得该算法直到2010年才得以实现。</p><p id="7e70" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">虽然LeNet开启了卷积神经网络(CNN)的时代，但Alex Krizhevsky等人在2012年发明的<strong class="lw jd"> AlexNet </strong>开启了CNN用于图像网络分类的时代。</p><p id="f00e" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">AlexNet是CNN能够在这个历史复杂的ImageNet数据集上表现良好的第一个证据，它表现得如此之好，以至于社会陷入了一场开发CNN的竞争:从<strong class="lw jd"> VGG </strong>、<strong class="lw jd">盗梦空间</strong>、<strong class="lw jd"> ResNet </strong>，到<strong class="lw jd">高效网络</strong>。</p><p id="027f" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">这些架构中的大多数遵循相同的配方:组合<strong class="lw jd">卷积层</strong>和<strong class="lw jd">汇集层</strong>后接<strong class="lw jd">全连接层</strong>，一些层具有<strong class="lw jd">激活功能</strong>和/或<strong class="lw jd">归一化</strong>步骤。你将首先学习所有这些晦涩难懂的东西，这样以后所有的架构都很容易理解。</p><blockquote class="mz na nb"><p id="0d05" class="lu lv nc lw b lx ly kd lz ma mb kg mc nd me mf mg ne mi mj mk nf mm mn mo mp im bi translated">注意:如果你想要这个故事中解释的架构的可编辑版本，请访问下面我的<a class="ae lh" href="https://dwiuzila.gumroad.com/l/cnn-architectures-visualized" rel="noopener ugc nofollow" target="_blank"> <strong class="lw jd"> gumroad </strong> </a>页面。</p><p id="e660" class="lu lv nc lw b lx ly kd lz ma mb kg mc nd me mf mg ne mi mj mk nf mm mn mo mp im bi translated">有了这个模板，你可能想要编辑现有的架构，用于自己的演示、教育或研究目的，或者你可能想要使用我们美丽的传说来构建其他令人惊叹的深度学习架构，但不想从头开始。<strong class="lw jd">免费(或者想付多少就付多少)！</strong></p></blockquote><div class="ng nh gp gr ni nj"><a href="https://dwiuzila.gumroad.com/l/cnn-architectures-visualized" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd jd gy z fp no fr fs np fu fw jc bi translated">5个最著名的CNN架构可视化(可编辑)</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">这是我在媒体上的一个故事的附加材料:5个最著名的CNN架构的可视化。</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">dwiuzila.gumroad.com</p></div></div><div class="ns l"><div class="nt l nu nv nw ns nx lb nj"/></div></div></a></div><h1 id="c7c5" class="ny lo it bd nz oa ob oc od oe of og oh ki oi kj oj kl ok km ol ko om kp on oo bi translated">全连接层和激活功能</h1><p id="2497" class="pw-post-body-paragraph lu lv it lw b lx op kd lz ma oq kg mc md or mf mg mh os mj mk ml ot mn mo mp im bi translated">让我们回到过去。1958年，<a class="ae lh" href="https://en.wikipedia.org/wiki/Frank_Rosenblatt" rel="noopener ugc nofollow" target="_blank">弗兰克·罗森布拉特</a>推出了第一台<strong class="lw jd"> <em class="nc">感知器</em> </strong>，这是一种根据生物原理构造的电子设备，表现出一种学习的能力。感知器是一台机器，而不是一个程序，它的第一个实现是为图像识别而设计的。</p><p id="5552" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">这台机器非常像现代的感知机。在现代意义上，感知器是一种学习二元分类器的算法，如下所示。感知器利用依赖于数据的加权和的阶跃函数给出两个类之间的线性判定边界。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/9a9c55a840af44f943c64c63931cecb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ta-4sX6wmcLn27EmvsrV6w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">一个感知器|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="ad27" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">感知器中的阶跃函数是<strong class="lw jd"> <em class="nc">激活函数</em> </strong>的一个例子；它激活感知器的状态是开(1)还是关(0)。现在有很多种激活函数，其中大部分都可以用来给感知器引入非线性。除步进功能外，一些常见的激活功能有:</p><div class="ks kt ku kv gt ab cb"><figure class="ov kw ow ox oy oz pa paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/43bf2b60b7408044c71077b4382b6514.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*MQor_OEkF8q-847DWYk0pQ.png"/></div></figure><figure class="ov kw ow ox oy oz pa paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/7fa6a21a0f517923f663daf40d5074d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*8rFqDpTFnmhTKID8cAoR5w.png"/></div></figure><figure class="ov kw ow ox oy oz pa paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/a6790a13e35514ce0316c82243d10d41.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*eX7Y-NRu4bgy21jxKLlM5Q.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk pb di pc pd translated">激活函数示例|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure></div><p id="8842" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">如果激活函数是sigmoid，感知器被称为<strong class="lw jd"> <em class="nc">逻辑回归模型</em> </strong>。</p><p id="c54c" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">还有另一个值得一提的激活函数，在处理多类分类任务时使用:<em class="nc"> softmax </em>。考虑具有K个类的任务，则softmax表示为</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pe"><img src="../Images/81e90f1fc518f56cae548d1d8af40af5.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*Bcv91tnjADjv45oTihuFhg.png"/></div></div></figure><p id="1e16" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">Softmax给出了模型选择类别<em class="nc"> i </em>的预测概率。具有softmax激活函数的模型将挑选具有最高概率的类作为最终预测。</p><p id="3914" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">考虑具有任意激活函数<em class="nc"> g </em>的感知器。您可以将任意数量的感知机堆叠成一层，并且可以将一层的输出连接到另一层。其结果就是我们所说的<strong class="lw jd"> <em class="nc">多层感知器</em> </strong> (MLP)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/8d1a908b66aed10c76ea3d4008882aa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*50Vpjz6r1hE__frVFRmzNg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><em class="pg">具有一个隐藏层的多层感知器</em> |图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="731d" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">在矢量化公式中，具有<em class="nc"> L </em>层的MLP表示如下。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/3a473cd6c2231a3eae275c60df8b9484.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*tKfBuuKt0gBjAyQkHOM-mg.png"/></div></figure><p id="d4c3" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">MLP也被称为<strong class="lw jd"> <em class="nc">密集神经网络</em> </strong>，因为它类似于大脑的工作方式。上图最后一幅图中的每个单位圆称为一个<strong class="lw jd"> <em class="nc">神经元</em> </strong>。在神经网络的世界中，MLP层也被称为<strong class="lw jd"> <em class="nc">全连接层</em> </strong>，因为一层中的每个神经元都连接到下一层中的所有神经元。</p><h1 id="60dd" class="ny lo it bd nz oa ob oc od oe of og oh ki oi kj oj kl ok km ol ko om kp on oo bi translated">卷积和汇集层</h1><p id="a637" class="pw-post-body-paragraph lu lv it lw b lx op kd lz ma oq kg mc md or mf mg mh os mj mk ml ot mn mo mp im bi translated">当很难找到一种有意义的方法从数据(例如表格数据)中提取要素时，完全连接的图层非常有用。说到图像数据，CNN可以说是最著名的神经网络家族。对于图像数据，神经网络使用图像的RGB通道进行处理。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pi"><img src="../Images/ed919f2b6785e2b62a5fbc4a685dd60f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*oiCpg9Rnc6HJUXcVkJXo9g.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">卷积如何与两个滤波器一起工作，每个滤波器的空间范围F = 3，步长S = 2，填充P = 1 | <a class="ae lh" href="http://cs231n.github.io/convolutional-networks/" rel="noopener ugc nofollow" target="_blank">源</a></p></figure><p id="fcae" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">CNN允许神经网络在图像的不同空间位置重复使用参数。各种选择的过滤器(也称为内核)可以实现不同的图像操作:身份，边缘检测，模糊，锐化等。CNN的思想是通过引入随机矩阵作为卷积算子来发现图像的一些有趣特征。</p><p id="d47c" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">在图像与内核卷积之后，你就剩下了一个被称为<strong class="lw jd"> <em class="nc">的特征图</em> </strong>。您可以一次又一次地卷积您的特征地图来提取特征，然而，这被证明是非常昂贵的计算。相反，您可以使用<strong class="lw jd"> <em class="nc">池图层</em> </strong>来缩小要素地图的大小。</p><p id="1579" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">池图层通过单个像素表示要素地图的小区域。可以考虑一些不同的策略:</p><ul class=""><li id="b9b0" class="pj pk it lw b lx ly ma mb md pl mh pm ml pn mp po pp pq pr bi translated">max-pooling(从特征图的N×N个小块中选择最高值)，</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ps"><img src="../Images/c659afb8237c55b02390992ee5d68142.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Vf2EW2UuxQNgAs9uf3KaA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在3个大小为4×4的滤镜上应用2×2补丁的最大池|图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><ul class=""><li id="55d6" class="pj pk it lw b lx ly ma mb md pl mh pm ml pn mp po pp pq pr bi translated">平均池化(对前一层的N×N个特征图进行求和，并选择平均值)，以及</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ps"><img src="../Images/c8072ac0fef94175813f48d510669953.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*etwmrPsWBLkEfJYiubwcAw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在3个大小为4×4 <em class="pg"> </em>的滤镜上应用2×2补丁的平均池|图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><ul class=""><li id="940d" class="pj pk it lw b lx ly ma mb md pl mh pm ml pn mp po pp pq pr bi translated">全局平均池(类似于平均池，但不是使用N×N个特征图的补丁，而是一次使用所有特征图区域)。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pt"><img src="../Images/8e8d55f9712ee1e620dcd4be36cc7544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w0ff7csSYKm54gFLIZ4vDA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">全局平均池应用于3个大小为4×4的过滤器<em class="pg"> </em> |图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="7db8" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">池化使神经网络对输入图像中的小变换、扭曲和平移不变。输入中的微小失真不会改变汇集的结果，因为您在局部邻域中取最大值/平均值。</p><h1 id="b55c" class="ny lo it bd nz oa ob oc od oe of og oh ki oi kj oj kl ok km ol ko om kp on oo bi translated">标准化层</h1><h2 id="0f76" class="ln lo it bd nz pu pv dn od pw px dp oh md py pz oj mh qa qb ol ml qc qd on iz bi translated">局部响应标准化</h2><p id="4467" class="pw-post-body-paragraph lu lv it lw b lx op kd lz ma oq kg mc md or mf mg mh os mj mk ml ot mn mo mp im bi translated">Alex Krizhevsky和其他人在他的AlexNet论文中介绍了<strong class="lw jd"> <em class="nc">局部响应归一化</em> </strong> (LRN)，这是一种帮助AlexNet泛化能力的方案。</p><p id="ebae" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">LRN实现了<strong class="lw jd"> <em class="nc">侧抑制</em> </strong>的想法，这是神经生物学中的一个概念，指的是一个兴奋的神经元抑制其邻居的现象:这导致了一个局部最大值形式的峰值，在该区域产生了对比，增加了感官知觉。</p><p id="1be8" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">给定通过应用核<em class="nc"> i </em>然后应用激活函数计算的神经元的活动<em class="nc"> aᵢ </em>，响应标准化活动<em class="nc"> bᵢ </em>由以下表达式给出</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/29fe953e671a2e84f98f8d039275bac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*xC3Np94YYggrFVDZrwqPOw.png"/></div></figure><p id="46b6" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">其中总和超过在相同空间位置的<em class="nc"> n个</em>“相邻”内核映射，并且<em class="nc"> N </em>是该层中内核的总数。常数<em class="nc"> k </em>、<em class="nc"> n </em>、<em class="nc"> α </em>和<em class="nc"> β </em>是超参数。</p><p id="6ff2" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">具体来说，让(<em class="nc"> k </em>，<em class="nc"> n </em>，<em class="nc"> α </em>，<em class="nc"> β </em> ) = (0，2，1，1)。应用到LRN后，我们将计算左上角像素的值(下面是最浅的绿色)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qf"><img src="../Images/bab5b3aba70fabf3b4208ba53536851d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zn66VfBtxeqmAYGKSdmWgw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><em class="pg">用(k，n，α，β) = (0，2，1，1)进行局部响应归一化</em> |图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者提供</a></p></figure><p id="f1b5" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">计算如下。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qg"><img src="../Images/e814c0640e494ba69077bc86a6f7bc0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*Om8w50fp26OSs2I8OXD8fQ.png"/></div></figure><h2 id="182a" class="ln lo it bd nz pu pv dn od pw px dp oh md py pz oj mh qa qb ol ml qc qd on iz bi translated">批量标准化</h2><p id="da1e" class="pw-post-body-paragraph lu lv it lw b lx op kd lz ma oq kg mc md or mf mg mh os mj mk ml ot mn mo mp im bi translated">深度神经网络优化的另一个突破是<strong class="lw jd"> <em class="nc">批量归一化</em> </strong> (BN)。它解决的问题叫做<strong class="lw jd"> <em class="nc">内部协变移位</em> </strong>:</p><blockquote class="mz na nb"><p id="24b9" class="lu lv nc lw b lx ly kd lz ma mb kg mc nd me mf mg ne mi mj mk nf mm mn mo mp im bi translated">前几层神经网络参数的变化改变了当前层的输入/协变量。这种变化可能是巨大的，因为它可能会改变投入的分布。如果一个层输入发生变化，那么这个层就几乎没有用了。</p></blockquote><p id="1023" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">设<em class="nc"> m </em>为训练时的批量。对于每一批产品，BN都正式遵循以下步骤:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/e8fc8ba49c23b5e04f01afab856573c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*c7niuxsXWgi9veXWRIxcaA.png"/></div></figure><p id="a6aa" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">在哪里</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/e61121efb3787c88624f835aaa163cd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*-wMQUWHnxdSeI07Ep1dwBQ.png"/></div></figure><p id="cd8a" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">这样，BN有助于加速深度神经网络训练。网络收敛更快，在训练时表现出更好的正则化，对整体精度有影响。</p></div><div class="ab cl qj qk hx ql" role="separator"><span class="qm bw bk qn qo qp"/><span class="qm bw bk qn qo qp"/><span class="qm bw bk qn qo"/></div><div class="im in io ip iq"><h1 id="8f20" class="ny lo it bd nz oa qq oc od oe qr og oh ki qs kj oj kl qt km ol ko qu kp on oo bi translated">5个最著名的CNN架构被可视化</h1><p id="ab7d" class="pw-post-body-paragraph lu lv it lw b lx op kd lz ma oq kg mc md or mf mg mh os mj mk ml ot mn mo mp im bi translated">您已经了解了以下内容:</p><ul class=""><li id="1d98" class="pj pk it lw b lx ly ma mb md pl mh pm ml pn mp po pp pq pr bi translated">卷积层</li><li id="4105" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp po pp pq pr bi translated">汇集层</li><li id="b61c" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp po pp pq pr bi translated">标准化层</li><li id="5556" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp po pp pq pr bi translated">全连接层</li><li id="324d" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp po pp pq pr bi translated">激活功能</li></ul><p id="8d47" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">现在，我们准备介绍和设想5种CNN架构:</p><ol class=""><li id="adde" class="pj pk it lw b lx ly ma mb md pl mh pm ml pn mp ra pp pq pr bi translated"><a class="ae lh" href="#7ebd" rel="noopener ugc nofollow"> LeNet-5 </a></li><li id="8289" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp ra pp pq pr bi translated"><a class="ae lh" href="#c691" rel="noopener ugc nofollow"> AlexNet </a></li><li id="c387" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp ra pp pq pr bi translated"><a class="ae lh" href="#8ab0" rel="noopener ugc nofollow"> VGG-16 </a></li><li id="bb11" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp ra pp pq pr bi translated"><a class="ae lh" href="#21f4" rel="noopener ugc nofollow">盗梦空间-v1 </a></li><li id="340b" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp ra pp pq pr bi translated"><a class="ae lh" href="#6702" rel="noopener ugc nofollow"> ResNet-50 </a></li></ol><p id="83e3" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">它们将建立在您所学的层和功能之上。因此，为了简化事情，我们将删除一些信息，如过滤器的数量，步幅，填充和正则化的下降。你将使用下面的图例来帮助你。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi rb"><img src="../Images/bbcc6091ba6d9557bf1a12841726ad6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p7FgMJGnqQjfFco8j748fA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">本故事中使用的图例|图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a>提供</p></figure><blockquote class="mz na nb"><p id="e2ae" class="lu lv nc lw b lx ly kd lz ma mb kg mc nd me mf mg ne mi mj mk nf mm mn mo mp im bi translated">注意:故事的这一部分很大程度上是受莱米·卡里姆的以下故事的启发。一定要去看看！</p></blockquote><div class="ng nh gp gr ni nj"><a rel="noopener follow" target="_blank" href="/illustrated-10-cnn-architectures-95d78ace614d"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd jd gy z fp no fr fs np fu fw jc bi translated">插图:10个CNN架构</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">普通卷积神经网络的编译可视化</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">towardsdatascience.com</p></div></div><div class="ns l"><div class="rc l nu nv nw ns nx lb nj"/></div></div></a></div><p id="2771" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">我们开始吧，好吗？</p><h2 id="7ebd" class="ln lo it bd nz pu pv dn od pw px dp oh md py pz oj mh qa qb ol ml qc qd on iz bi translated"><a class="ae lh" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf" rel="noopener ugc nofollow" target="_blank"> LeNet-5 </a></h2><p id="cd7a" class="pw-post-body-paragraph lu lv it lw b lx op kd lz ma oq kg mc md or mf mg mh os mj mk ml ot mn mo mp im bi translated">这是一切的开始。不包括池化，LeNet-5由5层组成:</p><ul class=""><li id="e044" class="pj pk it lw b lx ly ma mb md pl mh pm ml pn mp po pp pq pr bi translated">内核大小为5×5的2个卷积层，然后是</li><li id="6a51" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp po pp pq pr bi translated">3个完全连接的层。</li></ul><p id="765f" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">每个卷积层之后是一个2×2的平均池，除了最后一层(它有softmax)之外，每一层都有tanh激活函数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi rd"><img src="../Images/545131c5310a914ce2b344dc0c2198e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F37bNpLKOwQUXc1Wb4MNwg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">LeNet-5建筑|图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="ca66" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">LeNet-5拥有<strong class="lw jd">60000个</strong>参数。该网络在灰度32×32数字图像上被训练，并试图将它们识别为十个数字(0到9)中的一个。</p><h2 id="c691" class="ln lo it bd nz pu pv dn od pw px dp oh md py pz oj mh qa qb ol ml qc qd on iz bi translated"><a class="ae lh" href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> AlexNet </a></h2><p id="a461" class="pw-post-body-paragraph lu lv it lw b lx op kd lz ma oq kg mc md or mf mg mh os mj mk ml ot mn mo mp im bi translated">AlexNet引入了ReLU激活功能和LRN。ReLU变得如此流行，以至于几乎所有在AlexNet之后开发的CNN架构都在它们的隐藏层中使用ReLU，放弃了LeNet-5中tanh激活函数的使用。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi re"><img src="../Images/349d2150a80957ed6f150a545250c0aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jgWMsiTS86Jp1yHN7uxfQw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">AlexNet架构|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="02ed" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">网络由8层组成:</p><ul class=""><li id="8e51" class="pj pk it lw b lx ly ma mb md pl mh pm ml pn mp po pp pq pr bi translated">内核大小不增加的5个卷积层，然后是</li><li id="7a8b" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp po pp pq pr bi translated">3个完全连接的层。</li></ul><p id="8b6c" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">最后一层用softmax激活函数，其他都用ReLU。应用ReLU后，LRN将应用于第一个和第二个卷积层。第一、第二和第五卷积层之后是一个最大3×3的池。</p><p id="d576" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">随着现代硬件的进步，AlexNet可以用庞大的<strong class="lw jd">6000万</strong>参数进行训练，并成为2012年ImageNet比赛的获胜者。ImageNet已经成为开发CNN架构的基准数据集，并且它的子集(ILSVRC)由具有1000个类的各种图像组成。默认AlexNet接受尺寸为224×224的彩色图像。</p><h2 id="8ab0" class="ln lo it bd nz pu pv dn od pw px dp oh md py pz oj mh qa qb ol ml qc qd on iz bi translated"><a class="ae lh" href="https://arxiv.org/pdf/1409.1556.pdf" rel="noopener ugc nofollow" target="_blank"> VGG-16 </a></h2><p id="e2fd" class="pw-post-body-paragraph lu lv it lw b lx op kd lz ma oq kg mc md or mf mg mh os mj mk ml ot mn mo mp im bi translated">研究人员调查了CNN深度对其在大规模图像识别设置中的准确性的影响。通过将深度推进到11-19层，VGG家族诞生了:VGG 11层，VGG 13层，VGG 16层，VGG 19层。一个版本的VGG-11与LRN也进行了调查，但LRN没有提高性能。因此，所有其他vgg都是在没有LRN的情况下实现的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi rf"><img src="../Images/343ec8c0dc3ca828b50bd2dc068beaec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s_tkui85Mxgim-nO6SN18w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">VGG-16建筑|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="b248" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">这个故事聚焦于VGG-16，一个有16层的深度CNN架构:</p><ul class=""><li id="419c" class="pj pk it lw b lx ly ma mb md pl mh pm ml pn mp po pp pq pr bi translated">内核大小为3×3的13个卷积层，接着是</li><li id="f1b2" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp po pp pq pr bi translated">3个完全连接的层。</li></ul><p id="0ecf" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">VGG-16是拥有<strong class="lw jd">1.38亿</strong>个参数的最大网络之一。就像AlexNet一样，最后一层配备了softmax激活功能，其他都配备了ReLU。第2、第4、第7、第10和第13卷积层之后是2×2最大池。默认的VGG 16接受尺寸为224×224的彩色图像，并输出1000个类别中的一个。</p><h2 id="21f4" class="ln lo it bd nz pu pv dn od pw px dp oh md py pz oj mh qa qb ol ml qc qd on iz bi translated"><a class="ae lh" href="https://arxiv.org/pdf/1409.4842.pdf" rel="noopener ugc nofollow" target="_blank">盗梦空间-v1 </a></h2><p id="886f" class="pw-post-body-paragraph lu lv it lw b lx op kd lz ma oq kg mc md or mf mg mh os mj mk ml ot mn mo mp im bi translated">往深了说有个告诫:<strong class="lw jd"> <em class="nc">爆炸/消失渐变</em> </strong>:</p><ol class=""><li id="e87a" class="pj pk it lw b lx ly ma mb md pl mh pm ml pn mp ra pp pq pr bi translated">当大的误差梯度累积并导致训练期间不稳定的权重更新时，爆炸梯度是一个问题。</li><li id="0d99" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp ra pp pq pr bi translated">当损失函数的偏导数接近于零并且网络不能训练时，消失梯度是一个问题。</li></ol><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi rg"><img src="../Images/a5c90233ae5c15178ef7788b8a55a8dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5r7ld2etYDNeI2t7MLCU5w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">盗梦空间-v1架构|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="6235" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">Inception-v1通过添加两个连接到中间层的辅助分类器来解决这个问题，希望增加传播回来的梯度信号。在训练期间，他们的损失以0.3的折扣权重被添加到网络的总损失中。在推理时，这些辅助网络被丢弃。</p><p id="bf6f" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">Inception-v1引入了<strong class="lw jd"> <em class="nc"> inception模块</em> </strong>，四个系列的一个或两个卷积和max-pool层并行堆叠，并在最后级联。初始模块旨在通过允许使用多种类型的核大小来近似CNN中的最佳局部稀疏结构，而不是局限于单个核大小。</p><p id="7e71" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">《盗梦空间-v1》的参数比AlexNet和VGG-16少，只有700万美元，尽管它有22层:</p><ul class=""><li id="b81e" class="pj pk it lw b lx ly ma mb md pl mh pm ml pn mp po pp pq pr bi translated">内核大小为7×7、1×1和3×3的3个卷积层，然后是</li><li id="a5c0" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp po pp pq pr bi translated">18层，由9个初始模块组成，每个模块有2层卷积/最大池，然后是</li><li id="9d58" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp po pp pq pr bi translated">1全连接层。</li></ul><p id="c8b7" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">主分类器的最后一层和两个辅助分类器配备有softmax激活功能，并且所有其他分类器配备有ReLU。第一和第三卷积层以及第二和第七初始模块之后是3×3最大池。最后一个初始模块之后是7×7平均池。LRN是适用于第一次最大池和第三卷积层。</p><p id="dcbf" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">辅助分类器在第3和第6初始模块之后分支，每个从5×5平均池开始，然后是3层:</p><ul class=""><li id="0bcb" class="pj pk it lw b lx ly ma mb md pl mh pm ml pn mp po pp pq pr bi translated">1个具有1×1内核大小的卷积层，以及</li><li id="a49a" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp po pp pq pr bi translated">2个完全连接的层。</li></ul><p id="9b38" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">默认的Inception-v1接受尺寸为224×224的彩色图像，并输出1000个类中的一个。</p><h2 id="6702" class="ln lo it bd nz pu pv dn od pw px dp oh md py pz oj mh qa qb ol ml qc qd on iz bi translated"><a class="ae lh" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank"> ResNet-50 </a></h2><p id="6a3a" class="pw-post-body-paragraph lu lv it lw b lx op kd lz ma oq kg mc md or mf mg mh os mj mk ml ot mn mo mp im bi translated">当更深的网络可以开始收敛时，一个<strong class="lw jd"> <em class="nc">退化问题</em> </strong>已经暴露出来:随着网络深度的增加，精度达到饱和，然后迅速退化。</p><p id="386c" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">出乎意料的是，这种退化不是由过拟合引起的(通常由较低的训练误差和较高的测试误差表示)，因为向适当深度的网络添加更多层会导致较高的<em class="nc">训练误差</em>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi rh"><img src="../Images/82a663f9104953201f8ef7fe22fa6f3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z_R5sE1ZowrqGwonyRPayg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">ResNet-50架构|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="f985" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">通过引入<strong class="lw jd"> <em class="nc">瓶颈剩余块</em> </strong>来解决退化问题。有两种残余块:</p><ol class=""><li id="5744" class="pj pk it lw b lx ly ma mb md pl mh pm ml pn mp ra pp pq pr bi translated">Identity block:由3个卷积层组成，核大小分别为1×1、3×3和1×1，所有卷积层都配有BN。ReLU激活函数应用于前两层，而身份块的输入在应用ReLU之前添加到最后一层。</li><li id="c144" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp ra pp pq pr bi translated">卷积块:与恒等块相同，但卷积块的输入首先通过一个具有1×1核大小和BN的卷积层，然后添加到主序列的最后一个卷积层。</li></ol><p id="4384" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">请注意，两个残差块都有3层。ResNet-50总共有<strong class="lw jd">2600万</strong>个参数和50层:</p><ul class=""><li id="a6d2" class="pj pk it lw b lx ly ma mb md pl mh pm ml pn mp po pp pq pr bi translated">1卷积层，然后应用ReLU，接着</li><li id="3f54" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp po pp pq pr bi translated">9层，由1个卷积块和2个单位块组成，然后是</li><li id="c56c" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp po pp pq pr bi translated">12层，由1个卷积块和3个单位块组成，然后是</li><li id="2f18" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp po pp pq pr bi translated">18层，由1个卷积块和5个单位块组成，然后是</li><li id="bf61" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp po pp pq pr bi translated">9层，由1个卷积块和2个单位块组成，然后是</li><li id="e1f8" class="pj pk it lw b lx qv ma qw md qx mh qy ml qz mp po pp pq pr bi translated">1个带softmax的全连接层。</li></ul><p id="c950" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">第一卷积层之后是3×3最大池，最后一个标识块之后是全局平均池。默认的ResNet-50接受尺寸为224×224的彩色图像，并输出1000个类别中的一个。</p><p id="010a" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">这里是所有架构的总结。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ri"><img src="../Images/7d96cdaf0fdfc8f9b21b5ef18fe4a9bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*roVpIDd9f_SGP_2OqCCdIQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">所有架构摘要|图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a>提供</p></figure></div><div class="ab cl qj qk hx ql" role="separator"><span class="qm bw bk qn qo qp"/><span class="qm bw bk qn qo qp"/><span class="qm bw bk qn qo"/></div><div class="im in io ip iq"><h1 id="87a2" class="ny lo it bd nz oa qq oc od oe qr og oh ki qs kj oj kl qt km ol ko qu kp on oo bi translated">包扎</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi rj"><img src="../Images/bec3125c2e22ce0b1542200e7edb59bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xAC7UuXNf7S3zoGZ"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@markusspiske?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Markus Spiske </a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="e120" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">在这个故事中，你已经了解到许多CNN架构是由<strong class="lw jd">卷积层</strong>和<strong class="lw jd">汇集层</strong>后跟<strong class="lw jd">全连接层</strong>的组合构建的，其中一些层具有<strong class="lw jd">激活功能</strong>和<strong class="lw jd">规范化</strong>步骤。重要的是，你知道这句话是什么意思。</p><p id="89e8" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">你还从零开始构建了5个最著名的CNN架构。你会看到，更深的网络导致更好的性能，直到某个点它遭受<strong class="lw jd"> <em class="nc">爆炸/消失梯度</em> </strong>或<strong class="lw jd"> <em class="nc">退化问题</em> </strong>。为了解决这些问题，您使用了一些技巧，例如辅助分类器和瓶颈剩余块，分别在Inception-v1和ResNet-50中找到。</p><p id="07eb" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">恭喜你！</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi rk"><img src="../Images/e1a6e3674ab93bcb99796285f9d0175c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*6HsoGpmIb1oibJc_JWbqJA.gif"/></div></div></figure></div><div class="ab cl qj qk hx ql" role="separator"><span class="qm bw bk qn qo qp"/><span class="qm bw bk qn qo qp"/><span class="qm bw bk qn qo"/></div><div class="im in io ip iq"><p id="4859" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">🔥你好！如果你喜欢这个故事，想支持我这个作家，可以考虑 <a class="ae lh" href="https://dwiuzila.medium.com/membership" rel="noopener"> <strong class="lw jd"> <em class="nc">成为会员</em> </strong> </a> <em class="nc">。每月只需5美元，你就可以无限制地阅读媒体上的所有报道。如果你注册使用我的链接，我会赚一小笔佣金。</em></p><p id="bd58" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">🔖<em class="nc">想了解更多关于经典机器学习模型如何工作以及如何优化其参数的信息？或者MLOps大型项目的例子？有史以来最优秀的文章呢？继续阅读:</em></p><div class="ng nh gp gr ni"><div role="button" tabindex="0" class="ab bv gv cb fp rl rm bn rn lb ex"><div class="ro l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw rp rq fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l rp rq fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated">艾伯斯·乌兹拉</p></div></div><div class="rt ru gw l"><h2 class="bd jd wt pj fp wu fr fs np fu fw jc bi translated">从零开始的机器学习</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wv au ww wx wy tm wz an eh ei xa xb xc el em eo de bk ep" href="https://dwiuzila.medium.com/list/machine-learning-from-scratch-b35db8650093?source=post_page-----af76f1f0065e--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xd l fo"><span class="bd b dl z dk">8 stories</span></div></div></div><div class="sg dh sh fp ab si fo di"><div class="di ry bv rz sa"><div class="dh l"><img alt="" class="dh" src="../Images/4b97f3062e4883b24589972b2dc45d7e.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*CWNoicci28F2TUQc-vKijw.png"/></div></div><div class="di ry bv sb sc sd"><div class="dh l"><img alt="" class="dh" src="../Images/b1f7021514ba57a443fe0db4b7001b26.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*wSRsSHYnIiGJFAqC"/></div></div><div class="di bv se sf sd"><div class="dh l"><img alt="" class="dh" src="../Images/deb73e42c79667024a46c2c8902b81fa.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HVEz7KwzO0tv1Q4d"/></div></div></div></div></div><div class="ng nh gp gr ni"><div role="button" tabindex="0" class="ab bv gv cb fp rl rm bn rn lb ex"><div class="ro l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw rp rq fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l rp rq fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----af76f1f0065e--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="rt ru gw l"><h2 class="bd jd wt pj fp wu fr fs np fu fw jc bi translated">高级优化方法</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wv au ww wx wy tm wz an eh ei xa xb xc el em eo de bk ep" href="https://dwiuzila.medium.com/list/advanced-optimization-methods-26e264a361e4?source=post_page-----af76f1f0065e--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xd l fo"><span class="bd b dl z dk">7 stories</span></div></div></div><div class="sg dh sh fp ab si fo di"><div class="di ry bv rz sa"><div class="dh l"><img alt="" class="dh" src="../Images/15b3188b0f29894c2bcf3d0965515f44.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*BVMamoNudzn9UlAE"/></div></div><div class="di ry bv sb sc sd"><div class="dh l"><img alt="" class="dh" src="../Images/3249ba2cf680952e2ccdff36d8ebf4a7.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*C1fv3HJdh1RBspwN"/></div></div><div class="di bv se sf sd"><div class="dh l"><img alt="" class="dh" src="../Images/a73f0494533d8a08b01c2b899373d2b9.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*QZvzgiM2VnhYyx8M"/></div></div></div></div></div><div class="ng nh gp gr ni"><div role="button" tabindex="0" class="ab bv gv cb fp rl rm bn rn lb ex"><div class="ro l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw rp rq fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l rp rq fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----af76f1f0065e--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="rt ru gw l"><h2 class="bd jd wt pj fp wu fr fs np fu fw jc bi translated">MLOps大型项目</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wv au ww wx wy tm wz an eh ei xa xb xc el em eo de bk ep" href="https://dwiuzila.medium.com/list/mlops-megaproject-6a3bf86e45e4?source=post_page-----af76f1f0065e--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xd l fo"><span class="bd b dl z dk">6 stories</span></div></div></div><div class="sg dh sh fp ab si fo di"><div class="di ry bv rz sa"><div class="dh l"><img alt="" class="dh" src="../Images/41b5d7dd3997969f3680648ada22fd7f.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*EBS8CP_UnStLesXoAvjeAQ.png"/></div></div><div class="di ry bv sb sc sd"><div class="dh l"><img alt="" class="dh" src="../Images/41befac52d90334c64eef7fc5c4b4bde.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*XLpRKnIMcJzBzCwvXrLvsw.png"/></div></div><div class="di bv se sf sd"><div class="dh l"><img alt="" class="dh" src="../Images/80908ef475e97fbc42efe3fae0dfcff5.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*K_gzBmjv-ZHlU0Q6HeXclQ.jpeg"/></div></div></div></div></div><div class="ng nh gp gr ni"><div role="button" tabindex="0" class="ab bv gv cb fp rl rm bn rn lb ex"><div class="ro l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw rp rq fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l rp rq fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----af76f1f0065e--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="rt ru gw l"><h2 class="bd jd wt pj fp wu fr fs np fu fw jc bi translated">我最好的故事</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wv au ww wx wy tm wz an eh ei xa xb xc el em eo de bk ep" href="https://dwiuzila.medium.com/list/my-best-stories-d8243ae80aa0?source=post_page-----af76f1f0065e--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xd l fo"><span class="bd b dl z dk">24 stories</span></div></div></div><div class="sg dh sh fp ab si fo di"><div class="di ry bv rz sa"><div class="dh l"><img alt="" class="dh" src="../Images/0c862c3dee2d867d6996a970dd38360d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*K1SQZ1rzr4cb-lSi"/></div></div><div class="di ry bv sb sc sd"><div class="dh l"><img alt="" class="dh" src="../Images/392d63d181090365a63dc9060573bcff.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*hSKy6kKorAfHjHOK"/></div></div><div class="di bv se sf sd"><div class="dh l"><img alt="" class="dh" src="../Images/f51725806220b60eccf5d4c385c700e9.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HiyGwoGOMI5Ao_fd"/></div></div></div></div></div><div class="ng nh gp gr ni"><div role="button" tabindex="0" class="ab bv gv cb fp rl rm bn rn lb ex"><div class="ro l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw rp rq fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l rp rq fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----af76f1f0065e--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="rt ru gw l"><h2 class="bd jd wt pj fp wu fr fs np fu fw jc bi translated">R中的数据科学</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wv au ww wx wy tm wz an eh ei xa xb xc el em eo de bk ep" href="https://dwiuzila.medium.com/list/data-science-in-r-0a8179814b50?source=post_page-----af76f1f0065e--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xd l fo"><span class="bd b dl z dk">7 stories</span></div></div></div><div class="sg dh sh fp ab si fo di"><div class="di ry bv rz sa"><div class="dh l"><img alt="" class="dh" src="../Images/e52e43bf7f22bfc0889cc794dcf734dd.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*10B3radiyQGAp-QA"/></div></div><div class="di ry bv sb sc sd"><div class="dh l"><img alt="" class="dh" src="../Images/945fa9100c2a00b46f8aca3d3975f288.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*o6A863Vdwq7ThlmW"/></div></div><div class="di bv se sf sd"><div class="dh l"><img alt="" class="dh" src="../Images/3ca9e4b148297dbc4e7da0a180cf9c99.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*ekmX89TW6N8Bi8bL"/></div></div></div></div></div></div><div class="ab cl qj qk hx ql" role="separator"><span class="qm bw bk qn qo qp"/><span class="qm bw bk qn qo qp"/><span class="qm bw bk qn qo"/></div><div class="im in io ip iq"><p id="52cc" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">[1]王、、拉杰、比丘(2017): <em class="nc">关于深度学习的起源</em>。<a class="ae lh" href="https://arxiv.org/abs/1702.07800v4" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1702.07800v4</a></p></div></div>    
</body>
</html>