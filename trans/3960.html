<html>
<head>
<title>Natural Policy Gradients In Reinforcement Learning Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释强化学习中的自然策略梯度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/natural-policy-gradients-in-reinforcement-learning-explained-2265864cf43c#2022-09-02">https://towardsdatascience.com/natural-policy-gradients-in-reinforcement-learning-explained-2265864cf43c#2022-09-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0175" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">传统的政策梯度方法存在固有的缺陷。自然梯度收敛得更快更好，形成了当代强化学习算法的基础。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a7bfe5b02ed5a9d24f1bfa317b218234.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vqAfnQ5T2rzi7Nzn"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自然策略梯度在统计流形上推进策略，确保相同黎曼距离的一致更新。[罗伯特·卢克曼在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片]</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="a478" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">策略梯度算法是现代强化学习的基础。其思想是，通过简单地跟随目标函数的梯度(即偏导数的向量)，我们最终会达到最优。这是一个聪明的方法:( I)直接优化政策(而不是学习间接价值函数),( ii)让奖励函数引导搜索。然而，政策梯度有<strong class="li iu">的根本缺陷</strong>。本文解释了自然渐变的概念，揭示了传统渐变的缺点以及如何弥补它们。</p><p id="7b8a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">虽然自然梯度的受欢迎程度已经被<a class="ae ky" rel="noopener" target="_blank" href="/trust-region-policy-optimization-trpo-explained-4b56bd206fc2"> TRPO </a>和<a class="ae ky" rel="noopener" target="_blank" href="/proximal-policy-optimization-ppo-explained-abed1952457b"> PPO </a>等算法超越，但掌握它们的基本原理对于理解这些当代RL算法至关重要。<strong class="li iu">自然政策梯度</strong>部署不同的思维方式，仅仅观察损失函数并不总是清晰的。</p><p id="3460" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">然而，对自然梯度的完整讨论是相当技术性的，需要许多冗长的推导。为了保持这篇文章的简洁，我主要关注于<strong class="li iu">推理和直觉</strong>，为更深入的推导提供外部参考。此外，假设对传统(普通)策略梯度和加强算法有扎实的理解。</p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/policy-gradients-in-reinforcement-learning-explained-ecec7df94245"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">强化学习中的策略梯度解释</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">了解所有关于基于似然比的政策梯度算法(加强):直觉，推导，和…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt ks mf"/></div></div></a></div><h1 id="0379" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">一阶政策梯度的问题</h1><p id="eb51" class="pw-post-body-paragraph lg lh it li b lj nm ju ll lm nn jx lo lp no lr ls lt np lv lw lx nq lz ma mb im bi translated">在传统的策略梯度方法中，<strong class="li iu">梯度∇只给出了权重更新</strong>的<em class="nr">方向</em>。它没有告诉我们在这个方向上要走多远。导数定义在一个无穷小的区间上，这意味着梯度只在局部<em class="nr">有效</em>，在函数的另一部分可能完全不同。</p><p id="b61d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">因此，我们在采样(使用当前策略)和更新(基于采样数据)之间迭代。每个新策略卷展都允许重新计算梯度并更新策略权重θ。</p><p id="7cfc" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">行为由步长α控制。这产生了以下众所周知的策略梯度更新函数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/0f452ca259da52230dadd302edcc7084.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*JGlxC5Spfo04JlYDxGtgPg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">传统的策略梯度更新函数，基于目标函数梯度∇_θJ(θ和步长α更新策略权重θ</p></figure><p id="8b38" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">更新过程中可能会出现两个常见问题:</p><ul class=""><li id="d4a2" class="nt nu it li b lj lk lm ln lp nv lt nw lx nx mb ny nz oa ob bi translated"><strong class="li iu">超调</strong>:更新错过了回报高峰，落在了次优政策区域。</li><li id="eced" class="nt nu it li b lj oc lm od lp oe lt of lx og mb ny nz oa ob bi translated"><strong class="li iu">下冲</strong>:在梯度方向上采取不必要的小步导致收敛缓慢。</li></ul><p id="1b66" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在监督学习问题中，由于数据是固定的，超调并不是<em class="nr">太大的问题。如果我们超过了，我们可以纠正下一个时代。但是，如果RL更新导致不良策略，未来的样本批次可能不会提供太多有意义的信息。有点戏剧性:<strong class="li iu">我们可能永远无法从一次糟糕的更新中恢复过来。非常小的学习率可能会解决这个问题，但会导致收敛缓慢。</strong></em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/d1ae47b64d3bab54d394a8a3b6e07080.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2WFI1_pFTwZoDs4PnwiVkA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="oi">超调的例子。如果进入梯度方向的步长太大(左)，更新可能会错过奖励峰值，并落在低梯度的次优区域(右)。【作者图片】</em></p></figure><p id="49e4" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">更新后，我们降落在一个平坦的，次优的地区。低梯度仅引起小的权重更新，并且将需要多次迭代才能再次逃逸。</p><blockquote class="oj ok ol"><p id="594d" class="lg lh nr li b lj lk ju ll lm ln jx lo om lq lr ls on lu lv lw oo ly lz ma mb im bi translated">这里一个有趣的观察是，当我们应该执行一个谨慎的更新时，我们执行了一个大的更新，反之亦然。正如我们将在后面看到的，自然渐变正好相反。</p></blockquote><p id="d6f1" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们做一个思维实验。为了进行适当大小的权重更新，我们可能会决定给参数变化设置一个<strong class="li iu">上限。假设我们在参数空间中定义一个最大距离作为约束。我们可以这样定义这个问题:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/cb3035330830bebb1d012433db81b2f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*FOrZNVJB3U-no586zUcGpQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一种权重更新方案，限制了旧参数和更新参数之间的欧几里德距离。</p></figure><p id="7082" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">其中| |δθ| |表示更新前后参数之间的欧几里德距离。</p><p id="2176" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这听起来很合理，因为它应该避免超调，同时也没有必要限制更新大小。不幸的是，<strong class="li iu">它并不像你预期的那样工作</strong>。例如，假设我们的策略是由θ_1=μ和θ_2=σ参数化的高斯控制，并且我们设置了上限ϵ=1.下图中的两个更新都满足约束！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/9365ebbe6924ef2375fe3413a85dba88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-GoXs5-XjqqYTlfyi48x-g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">正态分布对的比较。左边有μ_1=0，μ_2=1，σ_1=σ_2=0.3。右边有μ_1=0，μ_2=1，σ_1=σ_2=3.0。虽然两对之间的欧几里德距离是1，但是很明显右边的一对比左边的一对更相似。[图片由作者提供]</p></figure><p id="b5cc" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在这两种情况下，欧几里德距离都是1:sqrt[(1–0)+(0.3–0.3)]和sqrt[(1–0)+(3–3)]。然而，对分布(即随机策略)的影响是完全不同的。</p><p id="87a4" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">问题是给参数空间<strong class="li iu">封顶并不能有效地给我们操作的统计流形</strong>封顶。请注意，政策是概率分布，改变概率会改变预期回报。这是我们优化并想要控制的流形。</p><blockquote class="oj ok ol"><p id="89e3" class="lg lh nr li b lj lk ju ll lm ln jx lo om lq lr ls on lu lv lw oo ly lz ma mb im bi translated">我喜欢把统计流形想象成分布的“家族”。例如，正态分布的集合族(由μ和σ参数化)构成了一个流形。另一个例子是将神经网络视为输出值的分布。改变随机策略可以被视为在定义目标函数的流形上移动(由参数θ占据)。</p></blockquote><p id="45c1" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">参数cap仅在统计流形是线性的情况下才起作用，但这种情况很少发生。为了防止策略本身在更新期间改变太多，我们必须考虑<strong class="li iu">分布对参数变化</strong>有多敏感。传统的策略梯度算法没有考虑这种曲率。要做到这一点，我们需要进入<strong class="li iu">二阶导数</strong>的领域，这正是自然政策梯度所做的。</p><h1 id="f9a6" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">缩小政策之间的差异</h1><p id="8e63" class="pw-post-body-paragraph lg lh it li b lj nm ju ll lm nn jx lo lp no lr ls lt np lv lw lx nq lz ma mb im bi translated">我们发现，感兴趣的是<em class="nr">分布</em>(即由θ参数化的策略)之间的差异，而不是<em class="nr">参数</em> θ和θ_old本身之间的差异。幸运的是，存在多种距离来计算两个概率分布之间的<strong class="li iu">差。本文将使用文献中最常见的<strong class="li iu"> KL散度</strong>。从技术上来说，它不是一种度量(因为它是不对称的)，但可以这样认为(对于小的差异，它是近似对称的):</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/cd6d8b3f6213c6066768dbcb6211a8b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Dgp5BwAdfljgg76IfBtdQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">策略π和π_old之间的Kullback-Leibner散度(也称为“相对熵”)。它描述了两个概率分布之间的距离。</p></figure><blockquote class="oj ok ol"><p id="9ef0" class="lg lh nr li b lj lk ju ll lm ln jx lo om lq lr ls on lu lv lw oo ly lz ma mb im bi translated">在之前显示的正态分布示例中，KL散度分别为0.81661和0.023481[<a class="ae ky" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions" rel="noopener ugc nofollow" target="_blank">公式通过维基百科</a> ]</p></blockquote><p id="5a29" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在这一点上，引入KL散度和<strong class="li iu">费希尔信息矩阵</strong>之间的联系是很好的(后面我们会看到为什么)。费希尔信息矩阵是描述统计流形的<em class="nr">曲率</em>的黎曼度量，即流形对边缘参数变化的灵敏度。矩阵可以被视为对考虑曲率的距离的校正——想象一下在地球仪上而不是在平坦的地球上测量距离。</p><p id="8b70" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果我们局部地定义KL散度<em class="nr"/>，即δθ= 0，结果证明两者是等价的。在这种情况下，零阶和一阶导数变成0，可以被删除。二阶导数的<strong class="li iu">矩阵由Hessian矩阵表示，在这种情况下，它相当于Fisher信息矩阵:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/7a5f78992a2e68bac0fde1187a564aac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e4GL9wiB9qNrmainddXBow.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">局部地，KL散度等价于Fisher矩阵。这一结果有助于实际应用。</p></figure><p id="21df" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这个结果对于实际的实现将是至关重要的，但是现在让我们先确定一下。</p><blockquote class="oj ok ol"><p id="0272" class="lg lh nr li b lj lk ju ll lm ln jx lo om lq lr ls on lu lv lw oo ly lz ma mb im bi translated">如果Fisher矩阵是一个单位矩阵，那么流形上的距离就是欧氏距离。在这种情况下，传统政策梯度和自然政策梯度是等价的。实际上，这种情况很少见。</p></blockquote><p id="be20" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">与之前类似，我们对允许的更新变更设置了一个<strong class="li iu">约束。然而，这一次，我们将其应用于<em class="nr">策略</em>的KL散度，而不是<em class="nr">参数空间</em>的欧几里德距离。调整后的问题如下:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/24e7be4d7ac4fba05ea435dbe2795b2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*1_uQkwpodVlxOtdx6PM8Xg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一个权重更新方案，它限制了新旧策略之间的KL差异。注意，这个方案考虑的是分布之间的差异，而不是参数。</p></figure><p id="53d5" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">通过求解这个表达式，我们确保在参数空间中执行大的更新，同时确保<strong class="li iu">策略本身不会改变太多</strong>。然而，计算KL散度需要评估所有的状态-动作对，所以我们需要一些简化来处理实际的RL问题。</p><h1 id="2390" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">拉格朗日松弛和泰勒展开</h1><blockquote class="oj ok ol"><p id="6dbc" class="lg lh nr li b lj lk ju ll lm ln jx lo om lq lr ls on lu lv lw oo ly lz ma mb im bi translated">对于接下来的部分，可以在卡耐基梅隆的<a class="ae ky" href="https://www.andrew.cmu.edu/course/10-403/slides/S19_lecture13_NaturalPolicyGradients.pdf" rel="noopener ugc nofollow" target="_blank">演讲幻灯片中找到一个精彩而详细的推导(作者Katerina Fragkiadaki)。为了保持对直觉的关注，我只强调最突出的结果。</a></p></blockquote><p id="2b35" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们将找出这个问题的解决方法。首先，我们使用拉格朗日松弛法将散度约束转化为罚函数，得到一个更容易求解的表达式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/3a8450329e09fdf4c509af36772abe71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6TAqHTy5HEnTLR6q-eY7bg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过执行Langrangian松弛，我们得到一个惩罚而不是约束大的政策变化的表达式。这个表达式比较好解。</p></figure><p id="f368" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">鉴于典型的RL问题太大，无法计算所有状态和动作的散度D_KL，我们必须求助于近似方法。利用<strong class="li iu">泰勒展开</strong>——根据导数来近似函数——我们可以基于通过部署策略π_θ获得的样本轨迹来逼近KL散度。</p><p id="d429" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">上述拉格朗日松弛的泰勒展开看起来如下(为了便于标记，考虑θ=θ_ old+δθ):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/a26f32b8892bf12c51b965569be36923.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y5xJN-47CGD0WM035YlO0w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">逼近最佳权重更新方案的泰勒展开。展开取损失的一阶展开和KL散度的二阶展开。</p></figure><p id="8e2c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">简而言之，损失项J(θ)用<strong class="li iu">一阶泰勒展开</strong>(即梯度w.r.t. θ)来近似，类似于传统的政策梯度(本质上是局部线性化)。KL散度用<strong class="li iu">二阶泰勒展开</strong>来近似。当局部逼近KL散度(即δθ= 0)时，零阶和一阶差评估为0，因此我们可以消除它们。这里感兴趣的是二阶导数。</p><blockquote class="oj ok ol"><p id="119b" class="lg lh nr li b lj lk ju ll lm ln jx lo om lq lr ls on lu lv lw oo ly lz ma mb im bi translated">为什么损失项J(θ)没有二阶展开？首先，第二项相对于散度来说可以忽略不计。第二，费希尔矩阵是正定的，但是当也混合在损失项的海森中时，这可能不成立。</p></blockquote><p id="c31d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为了使表达式不那么吓人，我们可以(I)用Fisher信息矩阵代替二阶KL导数，以及(ii)删除所有不依赖于δθ的项。这给我们留下了一个稍微友好的表达:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/1250ac7ef03002f7230a937b5ca7af00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cjFHd_Og1zhGhygwUJsQTg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">权重更新方案的简化泰勒展开，代入费希尔矩阵，并删除不依赖于δθ的项</p></figure><p id="4103" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">撇开符号紧凑性不谈，为什么要用费希尔矩阵代替二阶导数呢？事实证明等价是非常方便的。海森矩阵是一个|θ|⋅ |θ|矩阵，每个元素都是二阶导数。完整的计算可能相当麻烦。然而，对于费希尔矩阵，我们有一个替代表达式，它是梯度的<strong class="li iu">外积。无论如何，由于我们已经需要传统策略梯度的这些值，因此计算开销大大减少:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/d9b1cf5133d231b0fbb2c44bf09be056.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GOpEegInB_I3O7Rz-p_9CA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">费希尔信息矩阵可以表示为政策梯度的外积。该表达式在局部等价于Hessian矩阵，但在计算上更有效地生成。</p></figure><p id="527b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">因此，如果我们能够像我们习惯的那样计算梯度，我们就拥有了执行权重更新所需的所有信息。还要注意，期望意味着我们可以使用样本。</p><p id="55bd" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">需要了解的信息相当多，所以让我们简要回顾一下到目前为止我们已经完成的工作:</p><ul class=""><li id="fbe1" class="nt nu it li b lj lk lm ln lp nv lt nw lx nx mb ny nz oa ob bi translated">为了防止政策偏离太远，我们对新旧政策之间的KL差异进行了限制。</li><li id="a61d" class="nt nu it li b lj oc lm od lp oe lt of lx og mb ny nz oa ob bi translated">使用<strong class="li iu">拉格朗日松弛</strong>，我们将约束转化为惩罚，给我们一个单一的(无约束的)表达式。</li><li id="83c1" class="nt nu it li b lj oc lm od lp oe lt of lx og mb ny nz oa ob bi translated">由于我们不能基于样本直接计算KL散度，我们使用<strong class="li iu">泰勒展开</strong>作为权重更新方案的近似。</li><li id="3753" class="nt nu it li b lj oc lm od lp oe lt of lx og mb ny nz oa ob bi translated">对于小的参数变化，使用<strong class="li iu">费希尔信息矩阵</strong>来近似计算KL散度，对此我们有一个现成的表达式。</li><li id="7c36" class="nt nu it li b lj oc lm od lp oe lt of lx og mb ny nz oa ob bi translated">整个近似是一个<strong class="li iu">局部结果</strong>，假设θ=θ_old。因此，整个原理只适用于小的政策变化。</li></ul><p id="96fb" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">现在，让我们看看如何解决这个问题。</p><h1 id="ec68" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">求解KL约束问题</h1><p id="8d7f" class="pw-post-body-paragraph lg lh it li b lj nm ju ll lm nn jx lo lp no lr ls lt np lv lw lx nq lz ma mb im bi translated">是时候回到拉格朗日松弛法的泰勒展开式了。我们如何求解这个表达式，即找到最优权重更新δθ？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/1250ac7ef03002f7230a937b5ca7af00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cjFHd_Og1zhGhygwUJsQTg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">权重更新方案的简化泰勒展开可以使用拉格朗日方法来解决</p></figure><p id="55e0" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">嗯，我们可以通过<strong class="li iu">将梯度w . r . t .δθ设置为零</strong>来找到所需的更新(朗格方法)。求解表达式(现在转换为最小化问题，假设θ=θ_old)得出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/fb128ab9fb94757cde08051e94f87b9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XCgqgZW3xxfimQcXN8ZB1Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过将导数w . r . t .δθ设置为0来求解松弛泰勒展开式</p></figure><p id="1a6d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">该解决方案可以被重新安排以找到权重更新δθ:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/d38f747079d17599ccf23d5b7340c62f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*f-yYdxrEuMWRmclwNpDfBw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">重新安排解决方案允许表达最佳权重更新</p></figure><p id="83d9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">注意-1/λ是一个常数，可以被吸收到学习速率α中。事实上，α可以通过分析推导出来。从最初的约束，我们知道KL散度应该至多是ϵ.对于固定的学习率α，我们不能保证α F(θ)^-1 ∇_θJ(θ)≤ϵ.从代数上来说，我们可以推断出一个动态学习速率α(t11 ),它确保(再次近似地)更新的大小等于ϵ.遵守这一约束会产生以下学习率:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/01eb6c2507cc80ea90b1d22a76a2f5dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rPCA_V3xdtO6UeEt_EzBKg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">动态学习速率α确保权重更新的KL-散度(通过近似)不超过散度阈值ϵ</p></figure><p id="92c8" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">最后，从重新排列中，我们提取<strong class="li iu"> <em class="nr">自然</em>策略梯度</strong>，这是针对流形的曲率校正的梯度:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/548ab2ddc918d325d53c39c9f626572d.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*zs5QUs9sAB5N4Qxfo_k0yQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自然策略梯度w.r.t .目标函数是标准梯度乘以逆Fisher矩阵，说明黎曼空间的曲率</p></figure><p id="041b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这个自然梯度在距离限制内给出了黎曼空间中最陡的下降方向，而不是传统假设的欧几里得空间。注意，与传统的策略梯度相比，<strong class="li iu">唯一的区别是</strong> <strong class="li iu">与逆Fisher矩阵相乘！</strong>事实上，如果费雪矩阵是一个单位矩阵——实际上很少是——传统的和自然的政策梯度是等价的。</p><p id="a042" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">最终的权重更新方案如下所示</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/4623db5d27e5356c8696300474f97821.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*faYD1e1i5iE_Eu2yyRhaPg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自然政策梯度的权重更新方案。动态学习率确保每次更新同等地改变分布。</p></figure><p id="e06d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这种方案的强大之处在于，它总是以相同的幅度改变策略，而不管分布的表示。</p><p id="7908" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">最终结果在两个方面不同于传统的政策梯度:</p><ul class=""><li id="8b22" class="nt nu it li b lj lk lm ln lp nv lt nw lx nx mb ny nz oa ob bi translated"><strong class="li iu">考虑到政策对局部变化的敏感性，梯度由逆向Fisher矩阵</strong>进行“修正”。由于矩阵是倒置的，在陡坡处(高灵敏度)更新趋于谨慎，而在平坦表面处(低灵敏度)更新趋于较大。传统的梯度方法(错误地)假设更新之间的欧几里德距离。</li><li id="926b" class="nt nu it li b lj oc lm od lp oe lt of lx og mb ny nz oa ob bi translated"><strong class="li iu">更新权重/步长α具有适应梯度和局部灵敏度的动态表达式</strong>，确保ϵ量级的策略改变，而不管参数化。在传统方法中，α是一个可能不适合的可调参数，通常设置为某个标准值，如0.1或0.01。</li></ul><p id="6f62" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">尽管背后的机制相当不同，但在表面上，传统政策梯度方法和自然政策梯度方法惊人地相似。</p><p id="8660" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">自然策略梯度算法的完整概要总结如下。注意，在实践中，我们总是对梯度和Fisher矩阵使用样本估计。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/19f2a260835d1ae54394e596e9d6d025.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_0YmeAExNUx9G3IF4xwVcQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自然政策梯度算法，来自<a class="ae ky" href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf" rel="noopener ugc nofollow" target="_blank">柏克莱的深度RL课程，作者约书亚·阿奇姆</a></p></figure><h1 id="2e2c" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">自然渐变的问题</h1><p id="0f37" class="pw-post-body-paragraph lg lh it li b lj nm ju ll lm nn jx lo lp no lr ls lt np lv lw lx nq lz ma mb im bi translated">自然梯度克服了传统方法的基本缺陷，考虑了目标函数定义的流形如何随参数更新而变化。具体来说，自然梯度允许逃离高原，谨慎地接近回报高峰。从理论上讲，自然政策梯度<strong class="li iu">应该比传统政策梯度</strong>收敛得更快更好。</p><p id="b8a1" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在它们最纯粹的形式中，<strong class="li iu">自然梯度算法通常是不实用的</strong>。这有许多原因。</p><p id="950a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">首先，泰勒展开提供了高达二阶的<em class="nr">局部近似</em>。由于这个原因，估计的Hessian可能不是正定的。在实践中，自然梯度法在数字上是脆弱的，并不总是产生稳定的结果。大量的数学推导可能看起来令人信服，但泰勒展开、样本近似和严格的局部有效性(假设θ = θ_old)会极大地影响现实世界的性能。</p><p id="7d8a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">第二，费希尔信息矩阵占据了|θ|⋅|θ|空间。考虑一个有10万个参数的神经网络，你可以想象笔记本电脑上的100亿个矩阵不会飞。此外，计算矩阵的逆矩阵是O(N)复杂度的运算，这相当繁琐。因此，对于深度RL方法，自然策略梯度通常会超过<strong class="li iu">内存和计算限制</strong>。</p><p id="e080" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">最后，我们习惯于与复杂的<strong class="li iu">一阶随机梯度优化器</strong>——如ADAM，它也考虑二阶效应——在广泛的问题上提供出色的结果。二阶优化方法(即自然梯度算法)没有利用这些优化器。</p><p id="1aad" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">诸如共轭梯度和克罗内克因子化近似曲率(K-FAC)的方法可以(部分地)解决上述问题。在实践中，诸如信赖域策略优化(<strong class="li iu"> TRPO </strong>)和特别是邻近策略优化(<strong class="li iu"> PPO </strong>)的方法已经在流行度上超过了自然梯度，尽管它们植根于相同的数学基础。</p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/trust-region-policy-optimization-trpo-explained-4b56bd206fc2"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">解释了信任区域策略优化(TRPO)</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">强化学习算法TRPO建立在自然策略梯度算法的基础上，确保更新保持…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="pe l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/proximal-policy-optimization-ppo-explained-abed1952457b"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">解释了最近策略优化(PPO)</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">从强化到连续控制中的go-to算法</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="pf l mq mr ms mo mt ks mf"/></div></div></a></div><h1 id="5590" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">结束语</h1><p id="ed50" class="pw-post-body-paragraph lg lh it li b lj nm ju ll lm nn jx lo lp no lr ls lt np lv lw lx nq lz ma mb im bi translated">当对比自然政策梯度和传统政策梯度时，差异看起来相当有限。最后，我们只在我们所熟悉的梯度上增加了一个倒置的费希尔矩阵——考虑了局部敏感性。尽管如此，我们优化的方式是非常不同的，考虑<strong class="li iu">策略距离而不是参数距离</strong>。通过确保策略在更新权重时不会偏离太远，我们可以执行更稳定和一致的更新。</p><p id="6b2e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">自然策略梯度伴随着一系列数字挑战<strong class="li iu"/>，尤其是在处理大规模优化时(例如，具有大量参数的神经网络)。此外，在理论基础上进行了大量的近似和简化；修行可能更不守规矩。对于现实世界的实现，最近的策略优化现在通常是首选的。</p><p id="6dd5" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">尽管如此，对自然梯度的<strong class="li iu">理解</strong>对于那些希望了解强化学习最新技术的人来说是非常重要的。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="d367" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="nr">喜欢这篇文章？你可能也会喜欢以下的RL作品:</em></p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/why-reinforcement-learning-doesnt-need-bellman-s-equation-c9c2e51a0b7"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">为什么强化学习不需要贝尔曼方程</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">强化学习中著名的贝尔曼方程和MDP公式的再评价</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="pg l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/a-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">TensorFlow 2.0中连续策略梯度的最小工作示例</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">一个简单的训练高斯演员网络的例子。定义自定义损失函数并应用梯度胶带…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="ph l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">TensorFlow 2.0中离散策略梯度的最小工作示例</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">一个训练离散演员网络的多兵种土匪例子。在梯度胶带功能的帮助下…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="pi l mq mr ms mo mt ks mf"/></div></div></a></div><h1 id="bb1d" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">进一步阅读</h1><p id="acd1" class="pw-post-body-paragraph lg lh it li b lj nm ju ll lm nn jx lo lp no lr ls lt np lv lw lx nq lz ma mb im bi translated">对于自然政策梯度的起源，我建议阅读Amari (1998年)和Kokade (2001年)的<strong class="li iu">基础论文</strong>，以及Martens (2020年)最近的反思。</p><ul class=""><li id="5e59" class="nt nu it li b lj lk lm ln lp nv lt nw lx nx mb ny nz oa ob bi translated">阿马里，S. I. (1998年)。<a class="ae ky" href="https://d1wqtxts1xzle7.cloudfront.net/32944066/Amari_-_Natural_Gradient_works_efficiently_in_learning-with-cover-page-v2.pdf?Expires=1662047457&amp;Signature=KiOeWpPHdAWivpd0vlFhsg4bbZt~AmV1GA5ZL9ZwUMdjDPzzLB7fMeFiaBOQ11wYGgW5MKXX-gGgGgqQms3aHjpGYivUeN~U0xeXBwceDoz6jh4~7i8G--7rdavlU0HVj1sxX1vrQUSk~qVXqbsjKm-wCtwuk9o6jXUfeJql1PGTv65T70akN6rWVPUb3mW7Xw8drbgBfDsUh1AWCrczpSuWUIZG4n~gCzmGzfqb~Jfj3zBy~WbzSLNFUgMoT3FsrI-fq0~gMkmyXKThnd~4gY00k445Y0HKQmHjNq5RO9~OCr9~kx1tTvks6Zd9BBjtUBmHPAywLiQzPHWPEAf0-A__&amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA" rel="noopener ugc nofollow" target="_blank">自然梯度在学习中很有效。</a> <em class="nr">神经计算</em>，<em class="nr"> 10 </em> (2)，251–276。</li><li id="cc60" class="nt nu it li b lj oc lm od lp oe lt of lx og mb ny nz oa ob bi translated">Kakade，S. M. (2001年)。<a class="ae ky" href="https://proceedings.neurips.cc/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf" rel="noopener ugc nofollow" target="_blank">天然的政策梯度。</a> <em class="nr">神经信息处理系统的进展</em>、<em class="nr"> 14 </em>。</li><li id="83ed" class="nt nu it li b lj oc lm od lp oe lt of lx og mb ny nz oa ob bi translated">Martens，J. (2020年)。<a class="ae ky" href="https://www.jmlr.org/papers/volume21/17-678/17-678.pdf" rel="noopener ugc nofollow" target="_blank">对自然梯度法的新见解和新观点。</a> <em class="nr">《机器学习研究杂志》</em>，<em class="nr"> 21 </em> (1)，5776–5851。</li></ul><p id="b650" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">就<strong class="li iu">讲座幻灯片</strong>而言，我发现以下几张特别有帮助。</p><ul class=""><li id="c8a1" class="nt nu it li b lj lk lm ln lp nv lt nw lx nx mb ny nz oa ob bi translated">高级政策梯度(CS 285)。加州大学伯克利分校。</li><li id="0423" class="nt nu it li b lj oc lm od lp oe lt of lx og mb ny nz oa ob bi translated">Achiam，J. (2017年)。<a class="ae ky" href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf" rel="noopener ugc nofollow" target="_blank">高级政策梯度方法。加州大学伯克利分校。</a></li><li id="f8c9" class="nt nu it li b lj oc lm od lp oe lt of lx og mb ny nz oa ob bi translated">自然政策梯度(CMU 10-403)。卡内基梅隆。</li></ul><p id="75ed" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">最后，以下<strong class="li iu">帖子</strong>从不同角度提供了很棒的解释。</p><ul class=""><li id="df1c" class="nt nu it li b lj lk lm ln lp nv lt nw lx nx mb ny nz oa ob bi translated">克里斯蒂娅(2018)。自然梯度下降。[ <a class="ae ky" href="https://agustinus.kristia.de/techblog/2018/03/14/natural-gradient/" rel="noopener ugc nofollow" target="_blank">链接</a></li><li id="4075" class="nt nu it li b lj oc lm od lp oe lt of lx og mb ny nz oa ob bi translated">自然渐变。【<a class="ae ky" href="https://julien-vitay.net/deeprl/NaturalGradient.html" rel="noopener ugc nofollow" target="_blank">链接</a></li><li id="75e7" class="nt nu it li b lj oc lm od lp oe lt of lx og mb ny nz oa ob bi translated">扬·彼得斯(2010年)。政策梯度方法。Scholarpedia，5(11):3698。【<a class="ae ky" href="http://www.scholarpedia.org/article/Policy_gradient_methods" rel="noopener ugc nofollow" target="_blank">链接</a></li><li id="1c9b" class="nt nu it li b lj oc lm od lp oe lt of lx og mb ny nz oa ob bi translated">OpenAI (2018)。信任区域策略优化。[ <a class="ae ky" href="https://spinningup.openai.com/en/latest/algorithms/trpo.html#id2" rel="noopener ugc nofollow" target="_blank">链接</a></li></ul></div></div>    
</body>
</html>