<html>
<head>
<title>Data Science Tutorials: Training an XGBoost using R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学教程:使用 R 训练 XGBoost</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-science-tutorials-training-an-xgboost-using-r-cf3c00b1425#2022-03-10">https://towardsdatascience.com/data-science-tutorials-training-an-xgboost-using-r-cf3c00b1425#2022-03-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4d83" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解如何使用 R 语言训练最强大的基于树的模型之一(XGBoost)</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/820436488f63f331bfe2fcf82790c872.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zcMI9hDVUvzphtZ_"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由 J. Kelly Brito  @unsplash.com 拍摄</p></figure><p id="bcb0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di">在我的数据科学教程的上一篇文章</span>中，我已经展示了<a class="ae ky" rel="noopener" target="_blank" href="/data-science-tutorials-training-a-random-forest-in-r-a883cc1bacd1?sk=20e3776b9cde63314d7aa1ad12e7e4de">如何使用 R </a>训练一个随机森林。虽然随机森林是非常强大的模型，但它们受到了 boosting 算法的挑战，成为“基于树的模型的冠军”。</p><p id="c185" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">一般来说，boosting 模型试图通过处理它们的错误来改进以前的树</strong> —这种机制试图通过创建改进的学习器来减少偏差，这些学习器将试图用过去错误的子集来概括模式。boosting 算法有几种实现，比如 LightGBM 或 XGBoost。在本文中，我们将关注 XGBoost — <strong class="lb iu">梯度增强算法的具体实现。</strong></p><p id="e58a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">XGBoost，或极端梯度增强，可用于回归或分类——在本文中，我们将使用回归示例。</p><p id="f17c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就像我的其他数据科学教程系列一样，我们将使用<a class="ae ky" href="https://www.kaggle.com/hmavrodiev/london-bike-sharing-dataset" rel="noopener ugc nofollow" target="_blank">伦敦自行车共享数据集</a>-该数据集包含伦敦自行车共享计划的共享自行车需求信息-我们有每天和每小时的汇总数据:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi me"><img src="../Images/1dfcb63cf23db0eeca38ae8bee7a1de5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/0*qbMyhONou3H8G1Eu.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">伦敦自行车共享数据集的第一行</p></figure><p id="fa07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">列<em class="mf"> cnt </em>包含新自行车份额的计数。预览中显示的其他变量与天气数据相关，包括温度、湿度、风速等。</p><p id="8fb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的用例中，我们希望使用当天的大气数据和元数据来预测骑自行车的次数，例如，特定的一天是假日还是周末。为了简单起见，我们不会在 post 期间执行任何功能工程。</p><p id="51dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下命令使我们能够使用 R 读取 csv 文件:</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="d748" class="ml mm it mh b gy mn mo l mp mq">london_bike &lt;- read.csv(‘./london_merged.csv’)</span></pre><p id="3e61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，我们将加载本教程所需的所有库:</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="2f0a" class="ml mm it mh b gy mn mo l mp mq">library(dplyr)<br/>library(xgboost) <br/>library(Metrics)<br/>library(ggplot2)</span></pre></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="b3b2" class="my mm it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">分为训练和测试</h1><p id="2cf6" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">让我们使用一个简单的函数来拆分数据训练和测试样本，留下 20%的数据作为性能评估的维持集:</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="5f07" class="ml mm it mh b gy mn mo l mp mq"># Splitting into train and test<br/>train_test_split &lt;- function(data, percentage) {<br/> <br/> data_with_row_id &lt;- data %&gt;% <br/> mutate(id = row_number())<br/> <br/> set.seed(1234)<br/> training_data &lt;- data_with_row_id %&gt;%<br/> sample_frac(percentage)<br/> test_data &lt;- anti_join(<br/> data_with_row_id,<br/> training_data,<br/> by=’id’<br/> )<br/> <br/> training_data$id &lt;- NULL<br/> test_data$id &lt;- NULL<br/> <br/> return (list(training_data, test_data))<br/>}</span><span id="6a25" class="ml mm it mh b gy nu mo l mp mq"># Keeping 80% for the training set<br/>training_data &lt;- train_test_split(london_bike, 0.8)[[1]]<br/>test_data &lt;- train_test_split(london_bike, 0.8)[[2]]</span></pre><p id="996c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们剩下 13.931 个时间位置<strong class="lb iu">(正如我们在预览中看到的，每行代表特定小时的数据)</strong>用于训练。</p><p id="7e98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果需要，还可以使用超时维持集，而不是随机拆分数据，使用连续时间段(数据集的最后几天)作为测试集。</p><p id="6ff2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了测试我们算法的目的(评估量化性能)，我们将使用 3.483 个时间位置。请记住，我们将使用均方根误差来评估我们的算法。</p><p id="ff05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">只剩下一步了——隔离特性和目标列。从特性开始——记住我们在本教程中不做特性工程，而是按原样使用变量<em class="mf"/>:</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="89a2" class="ml mm it mh b gy mn mo l mp mq"># Defining the features in X_train and X_test</span><span id="b019" class="ml mm it mh b gy nu mo l mp mq">X_train &lt;- training_data[,c('t1','t2','hum',<br/>                                  'wind_speed','weather_code',<br/>                                  'is_holiday','is_weekend',<br/>                                  'season')]<br/>X_test &lt;- test_data[,c('t1','t2','hum',<br/>                          'wind_speed','weather_code',<br/>                          'is_holiday','is_weekend',<br/>                          'season')]</span></pre><p id="44ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如问题陈述中所提到的，我们将希望预测在特定的小时和天将使用多少辆自行车—这意味着我们的目标将是包含相同值的列“<em class="mf"> cnt </em>”:</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="a081" class="ml mm it mh b gy mn mo l mp mq"># Defining the Target<br/>y_train &lt;- training_data$cnt</span><span id="3065" class="ml mm it mh b gy nu mo l mp mq">y_test &lt;- test_data$cnt</span></pre><p id="480e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">设置好一切之后，让我们使用 R 来训练 XGBoost 模型！</p></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="9e73" class="my mm it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">培训 XGBoost</h1><p id="6259" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">我们将使用 R <em class="mf">中的<em class="mf"> xgboost </em>库来训练模型。</em>在本例中，我们将在模型中使用 10 轮(该参数与随机森林模型中的<em class="mf">树数</em>有点“类似”):</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="bcc8" class="ml mm it mh b gy mn mo l mp mq">set.seed(1234)<br/>xgb &lt;- xgboost(data = as.matrix(X_train),<br/>               label = y_train,<br/>               nround = 10)</span></pre><p id="40bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意一些有趣的事情——虽然在 R 的大多数库中使用格式为<em class="mf"> y ~ x1 + x2 + … + xn，</em>的公式的概念，但是对于<em class="mf"> xgboost </em>库却不是这样。在<em class="mf">数据</em>参数中，我们陈述特性(X ),在<em class="mf">标签</em>中，我们陈述目标(y)——这是我们创建四个不同对象的主要原因，类似于 Python 的<em class="mf"> sklearn </em>中的一个对象。此外，XGBoost 期望数据标签上有一个矩阵，所以我们需要显式地转换我们的对象——我们可以在数据管道或函数本身中这样做。</p><p id="f374" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用默认参数，<em class="mf"> xgboost </em>函数输出每次迭代的 RMSE <em class="mf">(均方根误差)</em>度量(用于回归)的历史:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/10caa757a1d78f3a3cdfae1a368e97d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*B_LoQiCwo_hPW6kwBOoZyw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">XGBoost 迭代结果。</p></figure><p id="a37c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，在 10 次迭代之后，我们的 RMSE 仍然在下降——使用更高的<em class="mf"> nround </em>参数可能是个好主意，但是我们马上就要这么做。</p><p id="c67d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们也为 XGBoost 模型的执行计时:</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="2326" class="ml mm it mh b gy mn mo l mp mq"># Timing the execution<br/>system.time(<br/>            xgboost(data = as.matrix(X_train),<br/>            label = y_train,<br/>            nround = 10)<br/>)</span></pre><p id="d097" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个模型在 0.41 秒左右运行——比大多数装袋模型(如随机森林)快得多。众所周知，助推模型通常比装袋模型训练起来更快。</p><p id="2502" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们知道，我们的 RMSE 是大约 845 辆自行车。拒绝合作的人呢？我们可以借助来自<em class="mf"> Metrics </em>包的<em class="mf"> rmse </em>函数，在测试集上评估我们的 XGBoost:</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="3bd3" class="ml mm it mh b gy mn mo l mp mq"># Assessing performance on the test set<br/>rmse(<br/> y_test, <br/> predict(xgb, as.matrix(X_test))<br/>)</span></pre><p id="6211" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这段代码将计算<em class="mf"> y_test — </em> <strong class="lb iu">测试集示例</strong> —和<em class="mf"> predict(xgb，as.matrix(X_test)) — </em> <strong class="lb iu">之间的 RMSE，我们将基于 XGBoost 模型生成预测。</strong></p><p id="67cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，我们还将 X_test 数据帧转换为 matrix —如果删除此转换，将会出现错误，因为 XGBoost 实现需要 matrix 数据类型。</p><p id="65e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的测试集的 RMSE 是 891 bikes，类似于训练集。我们可以通过调整<em class="mf"> xgboost </em>的一些参数来改善这一点吗？也许吧！我们来测试一下！</p></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="a571" class="my mm it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">扩展 XGBoost 超参数</h1><p id="f7fe" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">目前，我的树只下降了 6 层——这是<em class="mf"> xgboost </em>函数的默认参数。让我们稍微调整一下，并增加更多的迭代。</p><p id="6abc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要向 XGBoost 模型添加一个新的超参数，只需在函数中给出一个新的参数，例如:</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="8bb2" class="ml mm it mh b gy mn mo l mp mq"># Adding more hyperparameters to the XGBoost model<br/>xgb_ext &lt;- xgboost(data = as.matrix(X_train),<br/>               label = y_train,<br/>               nround = 50,<br/>               max_depth=20)</span></pre><p id="e851" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">超级简单！我们刚刚给我们的助推模型增加了一个新参数<em class="mf"> max_depth </em>。您可以使用<em class="mf">为这个模型调整大量参数。xgboost </em>命令在 R 中，你可以看到它们全部，包括它们的描述。<strong class="lb iu">调整这些参数是提高性能和稳定性的关键。</strong></p><p id="05c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们在第一个<em class="mf">xgb boost</em>模型中所看到的，我们在性能方面仍有一些改进的空间——这一点似乎从我们的<em class="mf"> xgb_ext </em>模型的输出中得到<em class="mf">的证实:</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/0ff5f6871ec8ad0cef4a6f13b13bef5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*ICHY0Ejnl5lsyl8_Tsb9bQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">XGB_EXT 性能</p></figure><p id="bb51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，这最后 8 次迭代的 RMSE 比第一次<em class="mf"> xgb_model </em>低得多。更好的是，让我们在每次迭代中可视化 RMSE:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/f258cfc8410596ee7c14d7d3cda2c264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*84HFn5FSNkwRAR7xJ9c2sA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">XGBoost 迭代中的 RMSE</p></figure><p id="6947" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在第 20 <em class="mf">次</em>执行之后，我们的 RMSE 变平了——这种稳定是增强模型的正常行为，可能意味着您已经找到了这组特定超参数的误差<strong class="lb iu">的全局最小值。</strong></p><p id="5096" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们增加了迭代次数，那会如何影响执行时间？让我们看看。</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="29c6" class="ml mm it mh b gy mn mo l mp mq">system.time(xgboost(data = as.matrix(X_train),<br/>                    label = y_train,<br/>                    nround = 50,<br/>                    max_depth=20))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/65bb00d0716dafaa1847dad144223ddf.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*NjGGHBT12ZmkR2ehp9_Zwg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">扩展 XGBoost 的执行时间</p></figure><p id="ccf4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个<em class="mf"> xgboost </em>的执行时间比前一个多一点——大约 8.42 秒。这是正常的，因为我们正在构建更复杂的树(更深的树)和更多的迭代。</p><p id="2457" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好的，我们有一个较低的 RMSE 但是..在训练集中有一个好的结果可能是一种"<em class="mf">数据科学安慰剂"-</em>我们在看不见的数据上表现如何？</p><p id="76c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为最后一项测试，让我们了解一下我们在维持集上的性能:</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="cdad" class="ml mm it mh b gy mn mo l mp mq"># Assessing Performance of Extended XGBoost<br/>rmse(<br/> y_test, <br/> predict(xgb_ext, as.matrix(X_test))<br/>)</span></pre><p id="0b6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">测试集的 RMSE 是 1008 辆自行车！就像在经济学中一样，在数据科学中没有免费的午餐— <strong class="lb iu">我们能够在训练集中改进我们的算法，但我们最终进入了过度适应的领域，创建了一个高方差模型。</strong></p><p id="4542" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">发生这种情况是因为我们创建了更复杂的树</strong> —记住，基于树的模型在搜索纯节点时是贪婪的，我们越是让它们变得复杂，它们就越有可能过度拟合。<a class="ae ky" rel="noopener" target="_blank" href="/classification-decision-trees-easily-explained-f1064dde175e?sk=ccd305c31950f2e8c843377186e5e75f"> <em class="mf">(如果你对我刚刚说的话感到极度困惑，可以看看这篇帖子！)</em> </a></p><p id="ccf1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或许，我们现在应该尝试更低的最大深度参数，并保持 50 次迭代，以尝试在训练和测试误差之间取得更好的平衡— <strong class="lb iu">作为一个挑战，亲自尝试一下吧！</strong></p></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><p id="13fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢你花时间阅读这篇文章！如果你想看的话，我还写了一篇关于如何在 R 中训练装袋模型(随机森林)的教程:</p><div class="nz oa gp gr ob oc"><a rel="noopener follow" target="_blank" href="/data-science-tutorials-training-a-random-forest-in-r-a883cc1bacd1"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">数据科学教程—在 R 中训练随机森林</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">了解如何使用 R 库训练随机森林</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="om l on oo op ol oq ks oc"/></div></div></a></div><p id="db1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我还在<a class="ae ky" href="https://www.udemy.com/course/r-for-data-science-first-step-data-scientist/?referralCode=MEDIUMREADERS" rel="noopener ugc nofollow" target="_blank"> Udemy </a>上开设了一门课程，从零开始学习数据科学的概念，我希望你能在我身边！</p><p id="6868" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是一个小要点，您可以通过更改输入数据和要素将其用于您的项目:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="158e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mf">本文中使用的数据集受开放政府许可条款和条件的约束，可从 https://www . ka ggle . com/hmavrodiev/London-bike-sharing-dataset</em>获得</p></div></div>    
</body>
</html>