<html>
<head>
<title>Ensemble Averaging — Improve Machine Learning Performance by Voting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">集成平均—通过投票提高机器学习性能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ensemble-averaging-improve-machine-learning-performance-by-voting-246106c753ee#2022-03-24">https://towardsdatascience.com/ensemble-averaging-improve-machine-learning-performance-by-voting-246106c753ee#2022-03-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c128" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">整体平均利用多个模型预测来开发稳健的性能</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3d1c552b83752cd1bccdcbd3809e2791.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xc4nd-ocrtdhrfGf"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">帕克·约翰逊在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="f662" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">人多力量大。令人惊讶的是，在机器学习领域也可以做出同样的观察。整体平均是<strong class="ky ir">设计多个不同模型</strong>的技术，即线性回归或梯度增强树<strong class="ky ir">，并允许它们对最终预测</strong>形成意见。</p><p id="df14" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该技术可以通过以下三个步骤轻松应用:</p><ul class=""><li id="a3c9" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">开发多个预测模型，每个模型都能够做出自己的预测</li><li id="cdcc" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">使用同一组训练数据训练每个预测模型</li><li id="f716" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">通过使用每一个模型预测结果，并平均它们的值</li></ul><p id="5dfd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这看起来似乎很容易实现，与单一的独立预测模型相比，它真的会产生更好的结果吗？事实上，一组模型经常比任何一个单独的模型显示出更好的结果。为什么这样</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="3dc2" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">为什么系综平均有效？</h1><h2 id="38a1" class="nf mo iq bd mp ng nh dn mt ni nj dp mx lf nk nl mz lj nm nn nb ln no np nd nq bi translated">偏差和方差</h2><p id="6cf4" class="pw-post-body-paragraph kw kx iq ky b kz nr jr lb lc ns ju le lf nt lh li lj nu ll lm ln nv lp lq lr ij bi translated">为了理解它的工作原理，我们首先来看看机器学习模型的两个最重要的属性:<strong class="ky ir">偏差</strong>和<strong class="ky ir">方差</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/724115c226cdc367b2d362da39d21e77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UWZm0m5j7z1nIqnvLR7vzA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者的代码输出图</p></figure><p id="3aef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">偏差本质上是模型使用目标函数将给定输入映射到输出的<strong class="ky ir">假设。例如，简单的普通最小二乘(OLS)回归是一种具有高偏差的模型。具有高偏差的模型允许它自己容易地学习输入和输出之间的关系。然而，它们总是对目标函数做更多的假设，因此灵活性较低。所以<strong class="ky ir">偏高</strong>的机型往往会出现<strong class="ky ir">欠配</strong>的问题(左图)。</strong></p><p id="b558" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一方面，模型的<strong class="ky ir">方差</strong>测量模型在不同训练数据上的表现。决策树不对目标函数做任何假设。因此，它非常灵活，并且通常具有低偏差。然而，由于灵活性，它倾向于学习最初用于训练模型的所有噪声。因此，这些模型引入了另一个问题— <strong class="ky ir">高方差</strong>。这在另一方面经常导致<strong class="ky ir">过拟合</strong>的问题(右图)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/92fcd3133ecf87276c717f36ff964bd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DO4UndHtyP8Vj1dx"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">乔恩·泰森在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="a730" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">偏差-方差困境</strong>因此存在于机器学习的世界中，因为试图同时最小化偏差和方差存在冲突。越来越复杂的模型通常具有低偏差和高方差，而相对简单的模型通常具有高偏差和低方差。因此，困境描述了模型学习输入和输出之间的最佳关系，同时在原始训练集之外进行概括的难度。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h2 id="d912" class="nf mo iq bd mp ng nh dn mt ni nj dp mx lf nk nl mz lj nm nn nb ln no np nd nq bi translated">投票机概述</h2><p id="b877" class="pw-post-body-paragraph kw kx iq ky b kz nr jr lb lc ns ju le lf nt lh li lj nu ll lm ln nv lp lq lr ij bi translated">一群模型就像投票机一样工作。每个合奏成员<strong class="ky ir">通过使用他们的预测</strong>进行投票来平等地做出贡献。因此，当每个模型被集合时，它引入了多样性的概念。这种多样性将减少方差，并提高超越训练数据的概括能力。</p><p id="e147" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们假设下图中的八杯咖啡分别代表不同的预测模型。每一种都有不同的口味(有些更甜，有些微苦)。由于我们事先不了解顾客，所以我们无法确定我们用来招待顾客(预测结果)的咖啡是否合适。我们的主要目标是想出一种新的咖啡，味道好，更适合大多数人的口味。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/8083f0b5eee5b9885035edaf7f53ac59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5YnrejakAjNwpKB-"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@nate_dumlao?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">内森·杜姆劳</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="a1ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们在这里可以做的是，通过混合八杯咖啡，我们得到了一种新的咖啡混合物，它保留了每种咖啡的焦糖、巧克力和苦味。与例如纯浓咖啡相比，这种咖啡在理论上应该能够服务于更广泛的顾客。这是因为最初浓缩咖啡的苦味可能不适合一些顾客，但这种混合已经抵消了。</p><h2 id="e218" class="nf mo iq bd mp ng nh dn mt ni nj dp mx lf nk nl mz lj nm nn nb ln no np nd nq bi translated">多样性的定义？</h2><p id="0057" class="pw-post-body-paragraph kw kx iq ky b kz nr jr lb lc ns ju le lf nt lh li lj nu ll lm ln nv lp lq lr ij bi translated">然而，多样性的确切定义在任何地方都没有定义。然而,<strong class="ky ir">经验表明，在大多数情况下，集合的性能优于单个模型。由于数据科学是一个需要经验证据的领域，能够在大多数时间工作实际上比你想象的更有说服力。这意味着它能够<strong class="ky ir">解决偏差-方差困境</strong>，通过使用模型集合减少方差，而不增加偏差。</strong></p><p id="26df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从金融角度来看这个问题，这个概念与投资组合多样化非常相似。如果投资组合中的每只股票都是独立的或者相关性很低。非系统风险(每个模型的差异或限制)可以通过多样化来减轻，而每个单独组件的回报不会改变。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="557c" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">Python 演示</h1><p id="d921" class="pw-post-body-paragraph kw kx iq ky b kz nr jr lb lc ns ju le lf nt lh li lj nu ll lm ln nv lp lq lr ij bi translated">在本文中，我将演示集成平均在回归问题和分类问题上的实现。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/163f3fdf7da4d89499aa7745fb52eeb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5N41wKgmDuygZ0FP"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@cdr6934?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">克里斯里德</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><h2 id="670d" class="nf mo iq bd mp ng nh dn mt ni nj dp mx lf nk nl mz lj nm nn nb ln no np nd nq bi translated">分类</h2><p id="d8ed" class="pw-post-body-paragraph kw kx iq ky b kz nr jr lb lc ns ju le lf nt lh li lj nu ll lm ln nv lp lq lr ij bi translated">首先，我们从 Scikit Learn 导入一些必要的模块。</p><pre class="kg kh ki kj gt oa ob oc od aw oe bi"><span id="1352" class="nf mo iq ob b gy of og l oh oi">from sklearn.datasets import make_classification<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.naive_bayes import GaussianNB<br/>from sklearn.ensemble import VotingClassifier <br/>from sklearn.neighbors import KNeighborsClassifier</span></pre><p id="37a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们使用来自 Scitkit Learn 的 make_classification()来模拟具有 50 个特征和 10000 个样本的二元分类问题。</p><p id="eeae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">模拟数据被进一步分成训练集和测试集。</p><pre class="kg kh ki kj gt oa ob oc od aw oe bi"><span id="f676" class="nf mo iq ob b gy of og l oh oi">X, y = make_classification(n_samples=10000, n_features=50, n_redundant= 15, random_state=42)<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=43)</span></pre><p id="6a74" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们训练几个独立的模型来预测分类问题。在本文中，我们将使用决策树分类器，KNN 和朴素贝叶斯。</p><pre class="kg kh ki kj gt oa ob oc od aw oe bi"><span id="d6e5" class="nf mo iq ob b gy of og l oh oi">models = [('Decision Tree', DecisionTreeClassifier()),<br/>         ('KNN', KNeighborsClassifier()),<br/>         ('Naive Bayes', GaussianNB())]</span><span id="9d1b" class="nf mo iq ob b gy oj og l oh oi">for name, model in models:<br/>    model.fit(X_train, y_train)<br/>    <br/>    prediction = model.predict(X_test)<br/>    score = accuracy_score(y_test, prediction)<br/>    print('{} Model Accuracy: {}'.format(name,score))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/84f60ddadc207e25636aca70e0b64bd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*LA1ft0uhkt3wkLGStJYz7Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者输出的代码</p></figure><p id="8f94" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在拟合模型之后，我们可以观察到模型本身已经做得很好了。让我们看看使用 Scitkit Learn 的 VotingClassifier 实现集成技术后的结果。</p><pre class="kg kh ki kj gt oa ob oc od aw oe bi"><span id="dd10" class="nf mo iq ob b gy of og l oh oi">ensemble = VotingClassifier(estimators=models)<br/>ensemble.fit(X_train, y_train)</span><span id="282a" class="nf mo iq ob b gy oj og l oh oi">prediction = ensemble.predict(X_test)<br/>score = accuracy_score(y_test, prediction)<br/>print('Ensemble Model Accuracy: {}'.format(score))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/c4a2beb6acf1c416f0966b46926efe27.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*WakmiMFewS6oV8uqI0bGZQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者输出的代码</p></figure><p id="c09e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">VotingClassifier 通过使用多数规则投票选择预测的类别标签来实现三个模型的集成。例如，如果有两个模型预测实例为 A 类，而只有一个模型预测为 B 类，则预测的最终结果将是 A 类。</p><p id="a164" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从这个实验中，我们可以看到，在我们实现集成平均之后，预测性能有了显著的提高。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h2 id="d28b" class="nf mo iq bd mp ng nh dn mt ni nj dp mx lf nk nl mz lj nm nn nb ln no np nd nq bi translated">回归</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/a32d1fea311ea79c8341e2b2ba6f2620.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dODCCdslBjQgz7Bn"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@markuswinkler?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马库斯·温克勒</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="36ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该技术也可以应用于回归问题。类似于我们在分类问题中介绍的，我们首先从 Scitkit Learn 导入一些必要的模块。</p><pre class="kg kh ki kj gt oa ob oc od aw oe bi"><span id="b841" class="nf mo iq ob b gy of og l oh oi">import numpy as np<br/>from sklearn.datasets import make_classification<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import mean_squared_error<br/>from sklearn.datasets import make_regression<br/>from sklearn.svm import SVR<br/>from sklearn.tree import DecisionTreeRegressor<br/>from sklearn.neighbors import KNeighborsRegressor<br/>from sklearn.ensemble import VotingRegressor</span></pre><p id="6cbe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们这次用 make_regression()模拟一个 10 个特征，10000 个样本的回归问题。</p><p id="2749" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">模拟数据再次被进一步分成训练集和测试集。</p><pre class="kg kh ki kj gt oa ob oc od aw oe bi"><span id="737e" class="nf mo iq ob b gy of og l oh oi">X, y = make_regression(n_samples=10000, n_features=10, random_state=42)<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=43)</span></pre><p id="5da5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们训练支持向量回归机、决策树回归机、KNN 来预测回归问题。</p><pre class="kg kh ki kj gt oa ob oc od aw oe bi"><span id="f5c5" class="nf mo iq ob b gy of og l oh oi">models = [('Support Vector', SVR()),<br/>         ('Decision Tree', DecisionTreeRegressor()),<br/>         ('KNN', KNeighborsRegressor())]<br/>score = []</span><span id="5a46" class="nf mo iq ob b gy oj og l oh oi">for name, model in models:<br/>    model.fit(X_train, y_train)<br/>    <br/>    prediction = model.predict(X_test)<br/>    score = mean_squared_error(y_test, prediction, squared = False)<br/>    scores.append(score)<br/>    print('{} Model RMSE: {}'.format(name,score))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/e00938dda48c4fbabd7b03f40f4118d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*6D6Ons8zHT4oOsSOtHp7zQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者输出的代码</p></figure><p id="f8c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">像往常一样，我们现在使用 Scitkit Learn 的 VotingRegressor 实现集成技术。</p><pre class="kg kh ki kj gt oa ob oc od aw oe bi"><span id="5f0d" class="nf mo iq ob b gy of og l oh oi">ensemble = VotingRegressor(estimators=models)<br/>ensemble.fit(X_train, y_train)</span><span id="6361" class="nf mo iq ob b gy oj og l oh oi">prediction = ensemble.predict(X_test)<br/>score = mean_squared_error(y_test, prediction, squared = False)<br/>print('Ensemble Model RMSE: {}'.format(score))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/46e48dbc93fd4e39af70ea50a3c8458c.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*_rowpwicLjRBc6EUkhZDvg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者输出的代码</p></figure><p id="5c3b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">VotingRegressor 的工作方式与 VotingClassifier 非常相似，只是它通过平均各个预测来实现集成，从而形成最终预测。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="a79f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">额外的实用技巧</strong></p><p id="f2af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在 VotingRegressor 和 VotingClassifier 中，我们可以利用参数“权重”来改变集成结果。“权重”参数指定了每个模型在最终结果中的能力。例如，在这个回归问题中，我们可以看到 KNN 模型在预测结果方面比其同行相对更强。在这种情况下，我们可以为其对最终预测的贡献分配更强的权重。为此，我们只需将权重列表传递给 VotingRegressor 函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/9b8f5066870eb10a88075d93aa1e735a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DVVZv-BDBb6JKfI4"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kv" href="https://unsplash.com/@graphicnode?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">图形节点</a>拍摄</p></figure><p id="64bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">事实上，我们可以根据模型相对于其他模型的排名来分配它们的权重。通过这样做，我们让那些预测更好的人在最终结果的投票中有更大的发言权。</p><pre class="kg kh ki kj gt oa ob oc od aw oe bi"><span id="833e" class="nf mo iq ob b gy of og l oh oi">scores = [-x for x in scores]<br/>ranking = 1 + np.argsort(scores)<br/>print(ranking)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/f89fa2c8012ad033d8ac36a432167a1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:134/format:webp/1*qMvArhhN9axAouO6boA8sg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者输出的代码</p></figure><p id="b4c0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用模型中的 RMSE，我们实现了 np.argsort 来根据它们的等级对它们进行排序。由于 np.argsort 从一个较小的值开始排序，所以我们在排序之前先对分数取反。这样，RMSE 较低的模型将具有较高的权重。例如，KNN 模型的权重为 3 分(满分为 6 分)。</p><p id="782f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们再训练一次，看看我们的结果是否会更好。</p><pre class="kg kh ki kj gt oa ob oc od aw oe bi"><span id="e109" class="nf mo iq ob b gy of og l oh oi">ensemble = VotingRegressor(estimators=models, weights = ranking)<br/>ensemble.fit(X_train, y_train)</span><span id="ed55" class="nf mo iq ob b gy oj og l oh oi">prediction = ensemble.predict(X_test)<br/>score = mean_squared_error(y_test, prediction, squared = False)<br/>print('Ensemble Model RMSE: {}'.format(score))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/c939dd78cd774f18a2d8c2cc848e7fc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*Ncn-6bpbWhYrR-7YfFgdGA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者输出的代码</p></figure><p id="4b81" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不幸的是，与正常平均相比，我们的集合模型这次具有更高的 RMSE。您可能会问，我们如何知道设置哪个权重，以便为每个模型获得最合适的权重？</p><p id="98aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">幸运的是，我们可以使用堆叠回归器，它将<strong class="ky ir">从预测器</strong>中学习最佳线性组合，这能够以提高的预测精度再次达到最终输出。我将在我的下一篇文章<a class="ae kv" rel="noopener" target="_blank" href="/stacked-ensembles-improving-model-performance-on-a-higher-level-99ffc4ea5523">中详细阐述更多关于<strong class="ky ir"> <em class="os">堆叠回归变量</em> </strong>。</a></p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="f6c5" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">外卖</h1><p id="6bd9" class="pw-post-body-paragraph kw kx iq ky b kz nr jr lb lc ns ju le lf nt lh li lj nu ll lm ln nv lp lq lr ij bi translated">整体平均是一项伟大的技术，可以通过<strong class="ky ir">引入多样性以减少方差</strong>来提高模型性能。请注意，尽管这种技术经常奏效，但它并不像药丸那样奏效。有时，您可能会看到模型的集合比单个模型工作得更差。</p><p id="2788" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，鉴于它易于实现，它绝对值得被包含在您的机器学习管道中。</p><p id="4a53" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">再次感谢您在百忙之中抽出更多的时间来阅读这篇文章。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/1459bd45b58ca015845c08d3a26d3124.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HTh4RQT6h1Y9jXXq"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@kellysikkema?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">凯利·西克玛</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div></div>    
</body>
</html>