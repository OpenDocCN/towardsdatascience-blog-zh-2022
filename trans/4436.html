<html>
<head>
<title>Why Incremental ETL Is Right for You</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么增量ETL适合您</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-incremental-etl-is-right-for-you-121b4406bf03#2022-10-02">https://towardsdatascience.com/why-incremental-etl-is-right-for-you-121b4406bf03#2022-10-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="08e2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Databricks自动加载器的增量数据处理教程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/909ef402f25ecd32d96370aa882de8b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S8hX_TG2YfbNeWjMUp5u2w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">嘶！你在听吗？在<a class="ae ky" href="https://unsplash.com/s/photos/cat?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@madhatterzone?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Manja Vitolic </a>拍摄的照片</p></figure><p id="5476" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">企业环境中的数据通常会随着时间无限增长。例如:</p><ul class=""><li id="e1e2" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">网上商店的交易积累速度很快。</li><li id="f889" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">服务器上运行的应用程序的日志数据。</li><li id="100c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">数百万在线购物者的点击流数据。</li><li id="80a8" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">来自数百台互联设备的物联网传感器数据。</li></ul><p id="5d66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上述所有情况中，共同点是数据会随着时间的推移而积累和扩展，需要明智地处理。</p><h2 id="aaf0" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">什么是增量数据处理？</h2><blockquote class="nc nd ne"><p id="066e" class="kz la nf lb b lc ld ju le lf lg jx lh ng lj lk ll nh ln lo lp ni lr ls lt lu im bi translated">在高级别上，增量数据处理是将新的或新鲜的数据从源移动到目的地。</p></blockquote><p id="0ca0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">增量ETL是数据湖架构中最重要的模式之一。可靠地加载新数据降低了成本，并有助于高效地扩展数据管道。</p><p id="bbf7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如今，大多数数据管道都是从云存储的某种形式的文件摄取过程开始的。</p><p id="8243" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从历史上看，在数据湖中接收文件是一个复杂的过程，因为许多系统被配置为处理目录中的所有文件，或者工程师需要实现自定义逻辑来识别新数据。</p><p id="dba1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常定制逻辑相当复杂，事实上重新处理所有数据的方法相当昂贵。</p><p id="7bd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您可以有效地加载新数据，而不必花费大量金钱来读取所有数据或创建自己的处理框架，会怎么样？现在有可能了。</p><h2 id="dfbc" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">什么是Databricks自动加载器？</h2><p id="e2dd" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">一个现成的解决方案，允许您从云存储中增量加载新数据文件，而无需任何复杂的设置。</p><p id="2790" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在引擎盖下，Autoloader使用<a class="ae ky" href="https://docs.databricks.com/structured-streaming/index.html" rel="noopener ugc nofollow" target="_blank"> Spark结构化流</a> API，以便从名为<em class="nf"> cloudFiles的源类型中读取新数据。</em></p><p id="f02e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您熟悉Spark读写操作，那么自动加载器的实现非常直观。</p><p id="eede" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">仅使用以下4个参数，就可以使用自动加载器轻松地配置和设置增量数据处理流水线。</p><ul class=""><li id="8e7d" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">数据源</strong> —源数据的目录。自动加载器将自动接收到达此位置的文件。</li><li id="8fde" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">源文件格式</strong> — <em class="nf"> cloudFiles </em>应该是自动加载器摄取的默认格式，您也可以指定一个选项<em class="nf"> cloudFiles.format </em>来表示源文件本身的格式。</li><li id="049d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">表</strong> —接收源数据的目的表。可以使用<em class="nf">来指定。表()</em>自变量</li><li id="fc58" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">检查点目录</strong> —让自动加载器知道在哪里存储检查点的参数。检查点就像Spark流的簿记服务，它跟踪流的进度，也可以用来跟踪数据集的模式。</li></ul><h2 id="2653" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">如何使用自动加载器增量摄取数据？</h2><p id="e686" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">为了实现以下内容，您只需要一个Databricks工作区和一个集群来运行您的笔记本。</p><p id="5ffe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们熟悉一下我们将在本教程中使用的航班延误CSV数据示例。你可以在<a class="ae ky" href="https://github.com/xavier211192/Apache-Spark/tree/main/Incremental%20ETL" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>中找到3个不同日期的航班延误的CSV文件，以及一个跟踪笔记本。</p><p id="7ac0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集具有以下架构:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/6b7dc515b88bdd72b5460bbedaf401eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*pZlS7K70mx1X832XbapwBw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据集的模式:按作者分类的图像</p></figure><h2 id="0be4" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">场景</h2><p id="54f8" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">现在让我们想象一下，您是一名数据工程师，负责创建和维护一个存储航班延误的数据湖表。每天，源系统将前一天的集体航班延误上传到云存储目录中。您的任务是每天以最佳方式处理这些数据。</p><h2 id="b6ce" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">设计</h2><p id="5a77" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">下面的示意图准确地描述了实施过程。正如人们常说的，图片比文字更有说服力，在数据工程中，视觉表现通常比一整页文字更能让人们正确看待事物。</p><blockquote class="nc nd ne"><p id="b80d" class="kz la nf lb b lc ld ju le lf lg jx lh ng lj lk ll nh ln lo lp ni lr ls lt lu im bi translated">我喜欢把这个叫做数据工程的伪代码！</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/4abb2898d93a9eb228852de1d05fcd62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*5nhgUhHyJA_dZoamX1C3sg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自动加载器示意图概述:作者图片。</p></figure><p id="325b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上图所示，该解决方案可用于表示任何通用的文件接收场景，其中数据随着时间的推移积累在数据湖中以备将来使用。</p><h2 id="04d3" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">履行</h2><p id="14e1" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">为了跟随并实现本教程，您需要从<a class="ae ky" href="https://github.com/xavier211192/Apache-Spark/tree/main/Incremental%20ETL/data" rel="noopener ugc nofollow" target="_blank"> git repo </a>下载CSV文件，因为我们试图模拟源系统的行为。</p><p id="dd20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了保持示例的简单和易于复制，我们不会在本教程中使用任何类型的云存储解决方案，而是坚持使用内置的Databricks文件系统(dbfs)。</p><blockquote class="nc nd ne"><p id="215d" class="kz la nf lb b lc ld ju le lf lg jx lh ng lj lk ll nh ln lo lp ni lr ls lt lu im bi translated">理想情况下，在生产设置中，源系统将文件发送到某种登陆/暂存云存储目录。</p></blockquote><p id="3c8d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们把伪代码变成实际的实现。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="c5dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用上面的代码准备一个空的笔记本。ETL过程的第一步是从源目录中读取数据。语法与传统的spark read相同，只是稍有改动。</p><p id="5a01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">提取</strong> —我们以流的形式读取数据，因为Auto Loader利用了底层的Spark流。注意，我们需要指定<em class="nf"> cloudFiles </em>作为格式来暗示我们想要使用自动加载功能。如前所述，我们还需要提供一个检查点目录，作为簿记功能的存储位置。</p><p id="2706" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">转换</strong> —没有转换的ETL很无聊。Spark模式推理将我们数据集中的第一列作为<code class="fe ns nt nu nv b">timestamp</code>读取。因此，我们可以应用一个简单的转换将时间戳列转换为日期类型。</p><p id="87ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Load </strong> —对于加载阶段，我们只需调用Spark writeStream方法，并传递检查点位置和我们希望存储数据的目标表名。目标表是一个增量表。如果不存在，执行将创建一个名为DailyDelays的增量表。</p><p id="38c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在ETL代码已经准备好了，让我们开始展示。为了模拟源系统将文件上传到源目录，让我们使用工作区本身提供的文件上传选项上传我们的第一个CSV文件(可以在git repo中找到)。您可以在工作区菜单→数据→创建表格选项中找到它。记得在默认路径/FileStore/tables/后面添加一个后缀，如下所示，并上传第一个文件。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/0f2ffd5ddbe62058fe2f38d9615fcf00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*pukie1h_RE6GSVzlk-A-ig.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">上传CSV文件:作者图片</p></figure><p id="5d2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">文件上传后，运行带有ETL代码的单元，如果一切顺利，您将开始注意到流式查询被激活，如下所示。这意味着第一个CSV文件已经在处理中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/036c7a4e4df845a7ee638fb7fbd7aa1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*Iakb3-Wk8-geQ-9ruPfzyA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">流式查询:按作者排序的图像</p></figure><p id="f31f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要确认数据是否被成功接收，请在另一个笔记本中查询如下所示的目标增量表，您应该已经看到了一些奇妙的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/11fa05f507356984d70cce4736e1df9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NcJMGJbj4hP65q1CWKlKCA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">目标表:按作者分类的图像</p></figure><p id="31d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在将第二个CSV文件上传到同一个目录中，看看魔法是如何展开的。无需调整/更新代码的任何部分，流式查询立即选取这个新文件进行处理，并将其吸收到同一个目标表中。显然，您可以通过查询目标表来确认这一点。如前所述，每个文件只包含某一天的延迟。如果你现在在表格中发现两个不同的日期，自动加载器已经发挥了它的魔力。</p><p id="fed3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你仍然不能相信，让我们再做一次检查，上传第三个文件，几秒钟后，我们的目标表应该有3个不同日期的数据。</p><p id="44d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意:如果您想知道为什么执行代码的单元持续运行并且从不停止，这是因为它运行一个活动的流查询。Spark流查询有一个默认触发器，每500毫秒运行一次，这相当于在数据到达时连续处理数据。当您期望低延迟和接近实时的数据接收时，这非常有用。请注意，这意味着您的集群需要始终在线。</p><p id="cd01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，每种情况都是不同的，在我们的示例中，我们确实说过数据每天只到达一次，让这个查询一直运行没有任何意义，最好是每天运行一次，但仍然只处理前一天到达的新文件。</p><p id="9a5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这可以通过对我们现有的writeStream逻辑进行简单的更改来实现。通过引入trigger once方法，我们仍然可以利用这个强大的功能，以便查询只执行一次，然后自动停止。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自动加载器触发一次:作者图像</p></figure><h2 id="99d5" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">结论</h2><p id="f4e8" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">增量数据处理不可避免地成为大多数数据管道的首选方法，如果使用得当，自动加载器和Spark流可以在数据管道的所有阶段完美地实现这一目的。许多供应商认识到对最佳数据处理方法的内在需求，将其作为内置功能提供，这并不奇怪。</p><p id="2fd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它有几个优点:</p><ul class=""><li id="359e" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">处理速度更快，因为数据更少。</li><li id="c646" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">一致的性能，因为只处理更改的数据。</li><li id="a738" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">不用说，它直接影响处理数据所需的资源成本。</li></ul><p id="3670" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">去吧，给自动装载机一个机会，我可以保证你永远不会回头！</p><p id="ede1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CSV文件以及助手笔记本可以在<a class="ae ky" href="https://github.com/xavier211192/Apache-Spark/tree/main/Incremental%20ETL" rel="noopener ugc nofollow" target="_blank"> git repo </a>中找到。</p></div></div>    
</body>
</html>