<html>
<head>
<title>Inference Optimization for Convolutional Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积神经网络的推理优化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/inference-optimization-for-convolutional-neural-networks-e63b51b0b519#2022-05-19">https://towardsdatascience.com/inference-optimization-for-convolutional-neural-networks-e63b51b0b519#2022-05-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7503" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">量化和融合以实现更快的推理</strong></h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/0f3614d8bf02572335a056c240ec4e39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dj0uJny68QgvKwmn"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/es/@bradencollum?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">布拉登·科拉姆</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="cb20" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现代深度学习研究主要集中在为各种问题(如对象检测、分割、自我监督学习等)创建和改进新的、更好的和优化的解决方案。这正在帮助许多企业和初创公司开始为他们的产品使用更容易获得的人工智能解决方案。然而，当涉及到现实世界的应用时，尤其是当模型是为边缘设备开发时，我们面临着许多限制，如模型大小大和低延迟。</p><p id="5bc8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有许多方法可以让移动设备和其他边缘设备访问AI模型。一种选择是使用为移动设备设计的小型号(比如为移动设备设计的<a class="ae kv" href="https://arxiv.org/abs/1704.04861" rel="noopener ugc nofollow" target="_blank"> MobileNet </a>和<a class="ae kv" href="https://github.com/pytorch/ios-demo-app/tree/master/ObjectDetection" rel="noopener ugc nofollow" target="_blank"> Yolo)。其他方法包括推理级别的优化。后者包括模型剪枝、量化、模块融合等方法。在这篇博文中，我们将探讨卷积神经网络的量化和融合方法。我们将使用PyTorch的量化模块，并比较有量化和没有量化的模型的大小和延迟。</a></p><p id="5cb1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">博客概述:</p><ol class=""><li id="750a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">什么是量子化？</li><li id="e9e0" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">量化技术</li><li id="67d3" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">什么是模块融合？</li><li id="d061" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">PyTorch中的应用与比较</li></ol><h2 id="6829" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated"><strong class="ak">什么是量化？</strong></h2><p id="5e57" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">量化是一种在推理阶段加速深度学习模型的简单技术。这是一种压缩信息的方法。模型参数存储在浮点数中，模型操作使用这些浮点数进行计算。浮点运算具有很高的精度，但是，它们非常占用内存，计算量也很大。量化将32位浮点数转换为8位整数。它对8位整数执行部分或全部操作，这可以将模型大小和内存需求减少4倍。</p><p id="908c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，这是有代价的。为了减小模型的规模，提高执行时间，我们会牺牲一些精度。因此，在模型准确性和大小/延迟之间会有一个折衷。</p><p id="a937" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了进行量化，我们需要一个将浮点数映射为整数的映射函数。最常见和最简单的方法是线性变换。</p><p id="fd27" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Q(r) = round(1/S + Z)，其中</p><ul class=""><li id="7b33" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr ne ly lz ma bi translated">r是输入，</li><li id="e89c" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr ne ly lz ma bi translated">s是比例因子，即输入范围与量化输出范围之比。<br/>找到比例因子最简单的方法是使用输入和输出范围的最小值和最大值。但是PyTorch还有MSE最小化、熵最小化等其他方法。寻找比例因子的过程称为校准。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/5460b8bec6c0c4fc6fe05caa6d87ba00.png" data-original-src="https://miro.medium.com/v2/resize:fit:240/0*y0X_eYxHFBqf6A0A"/></div></figure><ul class=""><li id="5039" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr ne ly lz ma bi translated">z是零点，此参数有助于将零点从输入空间正确映射到输出空间。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/6411baafbca84c6a3cf3d103c6639a1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:254/format:webp/1*dK_CgZ8LOrXky5kNuBl7Vg.png"/></div></figure><p id="4b41" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PyTorch有一个观察器模块，可以用来校准模型。它收集输入值的统计数据，并计算S和Z参数。量化参数也可以通过张量或通道来计算。在第一种情况下，观察者将获取整个张量并从中提取统计数据，在第二种情况下，分别为每个通道计算S和Z参数。</p><h2 id="8a07" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated"><strong class="ak">量化技术</strong></h2><p id="ddec" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">在PyTorch中，有3种不同的方法来实现量化:</p><ol class=""><li id="b76a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">动态量化</strong> <br/>模型权重被量化，推理时激活被动态量化。这可以提高精度，因为对每个输入都进行了校准。这项技术在LSTM、GRU、RNN都有效</li><li id="520c" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">训练后静态量化</strong> <br/>在这种情况下，模型权重和激活是预先量化的。对验证数据进行校准。这比动态量化更快，但是，它可能需要不时地重新校准以保持稳健。</li><li id="588f" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">量化感知训练(QAT) </strong> <br/>这些技术旨在改善训练后的量化。它在训练损失中增加了量化误差。s和Z参数可以在训练期间学习。</li></ol><p id="ed92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本帖的续篇中，我们将对卷积神经网络使用训练后量化。</p><h2 id="ea59" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated"><strong class="ak">什么是模块融合？</strong></h2><p id="6284" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">在继续讨论PyTorch代码之前，让我们先了解另一种技术，这就是fusion。融合旨在将多个层合并为一层，这可以节省推理时间并减少内存访问。它将合并后的操作序列发送到一个低级别的库，该库可以一次性计算出来，而无需将中间表示返回给PyTorch。然而，这种技术也是有代价的。现在，当各层合并时，模型很难调试。融合只对以下图层组有效:[Conv，雷鲁]，[Conv，巴奇诺姆]，[Conv，巴奇诺姆，雷鲁]，[线性，雷鲁]。</p><h2 id="bd3c" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated"><strong class="ak">py torch中的应用和比较</strong></h2><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="af30" class="mg mh iq ni b gy nm nn l no np"># Import packages <br/>from torch import nn<br/>from torchsummary import summary<br/>import torch<br/>import os</span></pre><p id="a4a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，让我们创建一个简单的卷积神经网络。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="c0c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从上面的代码片段可以看出，我们已经创建了一个小型卷积网络，它有两个卷积层，后面是relu和maxpooling。最后，我们有两个完全连接的层和一个Softmax输出。</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="e3a6" class="mg mh iq ni b gy nm nn l no np">n = Net().cuda()<br/>summary(n, (3, 224, 224))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/e3c47b948eeacb9b785a8f6802a4e7e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*te0JxhtUdN2QjpP_zY8-nw.png"/></div></figure><p id="571b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">模型摘要显示，我们有大约7000万个参数，估计模型大小为294MB。</p><p id="51ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了创建同一模型的量化版本，我们将创建2个新属性来量化和反量化模型。接下来，在正向传递期间，我们将在softmax之前对网络输入进行量化和去量化。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="9438" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了运行eval()设置的量化模型，我们需要定义配置。Torch有两个用于量化的后端:用于支持AVX2或更高版本的x86 CPUs的“fbgemm”和用于ARM CPUs(移动/嵌入式设备)的“qnnpack”。</p><p id="cdf5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们需要使用<em class="nt">torch . quantization . prepare</em>来准备模型。这将运行一个观察器方法来收集输入的统计数据。并且<em class="nt">torch . quantization . convert</em>从观察者状态转换到量子化状态。</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="2b18" class="mg mh iq ni b gy nm nn l no np"># Define original and quantized models and prepae for evaluation <br/>net = Net()<br/>net.eval()<br/>net_quant = NetQuant()<br/>net_quant.eval()</span><span id="7ab0" class="mg mh iq ni b gy nu nn l no np"># Prepare model quantization and convert to quantized version<br/>net_quant.qconfig = torch.quantization.get_default_qconfig("fbgemm")<br/>torch.backends.quantized.engine = "fbgemm"<br/>net_quant = torch.quantization.prepare(net_quant.cpu(), inplace=False)<br/>net_quant = torch.quantization.convert(net_quant, inplace=False)</span></pre><p id="a9e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">检查模型尺寸</strong></p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="4c3a" class="mg mh iq ni b gy nm nn l no np"># Check model size<br/>def print_model_size(mdl):<br/>    torch.save(mdl.state_dict(), "tmp.pt")<br/>    size = round(os.path.getsize("tmp.pt")/1e6)<br/>    os.remove('tmp.pt')<br/>    return size</span><span id="7b2b" class="mg mh iq ni b gy nu nn l no np">net_size = print_model_size(net)<br/>quant_size = print_model_size(net_quant)</span><span id="1df9" class="mg mh iq ni b gy nu nn l no np">print(f'Size whitout quantization: {net_size} MB \n Size whit quantization: {quant_size} MB')<br/>print(f'Size ratio: {round(net_size/quant_size, 2)}')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/cd516a70b08998a4e6316edd053b8bd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*dvjRkT5rVBxu6Tjb31yHHg.png"/></div></figure><p id="7389" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们得到了比原来尺寸小4倍的模型。</p><p id="3662" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">检查模型延迟</strong></p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="e8d0" class="mg mh iq ni b gy nm nn l no np"># input for the model<br/>inpp = torch.rand(32, 3, 224, 224)<br/># compare the performance<br/>print("Floating point FP32")<br/>%timeit net(inpp)</span><span id="5116" class="mg mh iq ni b gy nu nn l no np">print("Quantized INT8")<br/>%timeit net_quant(inpp)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/0539598c4e9f6f9fe0d35609e3659a5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hSaP5eBfCvtk0gTWlM3Bww.png"/></div></div></figure><p id="0109" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">CPU上的原始模型运行速度为162 ms，而量化后的模型大约快1.7倍。</p><p id="d214" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">融合</strong></p><p id="f1b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，让我们实现融合以实现更多优化。正如我们在模型结构中看到的，只有3种方法来融合层:</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="753d" class="mg mh iq ni b gy nm nn l no np"># Perpare blocks for the fusion</span><span id="b4ee" class="mg mh iq ni b gy nu nn l no np">moduls_to_fuse =  [['conv1', 'relu1'], <br/>                   ['conv2', 'relu2'], <br/>                   ['fc1', 'relu3']]</span><span id="746c" class="mg mh iq ni b gy nu nn l no np">net_quant_fused = torch.quantization.fuse_modules(net_quant, moduls_to_fuse)</span><span id="0eef" class="mg mh iq ni b gy nu nn l no np">net_fused = torch.quantization.fuse_modules(net, moduls_to_fuse)</span></pre><p id="370c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们再次检查延迟:</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="f231" class="mg mh iq ni b gy nm nn l no np">print("Fused and quantized model latency")<br/>%timeit net_quant_fused(inpp)</span><span id="a3ba" class="mg mh iq ni b gy nu nn l no np">print("Fused model latency")<br/>%timeit net_fused(inpp)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/283317d415ab702d27c4e766e0092d18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a3-AO3RV_QHP3hjlsMkBeg.png"/></div></div></figure><p id="1c16" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们所看到的，通过使用带量化或不带量化的融合，我们节省了一些运行时间。然而，这并不像量化那样影响准确性。</p><h2 id="b5ad" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated">结论</h2><p id="5a79" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">优化推理模型不是一件容易的事情。PyTorch使得使用这些优化技术变得更加容易。但是，一定要记得在量化前后仔细检查模型的精度，确保你的模型不仅快而且准。</p><p id="4d3b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">在GitHub上找到所有代码:</strong><a class="ae kv" href="https://github.com/LilitYolyan/inference_optimization_cnn" rel="noopener ugc nofollow" target="_blank">https://github.com/LilitYolyan/inference_optimization_cnn</a></p><p id="bb13" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">检查参考资料:</strong></p><div class="ny nz gp gr oa ob"><a href="https://pytorch.org/blog/quantization-in-practice/" rel="noopener  ugc nofollow" target="_blank"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd ir gy z fp og fr fs oh fu fw ip bi translated">PyTorch中的实用量子化</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">量化是一种既便宜又简单的方法，可以让您的DNN运行得更快，并且对内存的要求更低。PyTorch提供了…</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">pytorch.org</p></div></div><div class="ok l"><div class="ol l om on oo ok op kp ob"/></div></div></a></div><div class="ny nz gp gr oa ob"><a href="https://pytorch.org/docs/stable/quantization.html" rel="noopener  ugc nofollow" target="_blank"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd ir gy z fp og fr fs oh fu fw ip bi translated">量化- PyTorch 1.11.0文档</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">警告量化处于测试阶段，可能会有变化。量化指的是执行计算的技术…</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">pytorch.org</p></div></div></div></a></div></div></div>    
</body>
</html>