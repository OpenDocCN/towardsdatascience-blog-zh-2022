<html>
<head>
<title>Understanding Mean Shift Clustering and Implementation with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解均值漂移聚类和Python实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-mean-shift-clustering-and-implementation-with-python-6d5809a2ac40#2022-02-22">https://towardsdatascience.com/understanding-mean-shift-clustering-and-implementation-with-python-6d5809a2ac40#2022-02-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1a05" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">无监督学习</h2><div class=""/><div class=""><h2 id="f062" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">在这篇文章中，我简要介绍了无监督学习方法的概念，均值漂移聚类，以及它在Python中的实现</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/794e1d73d8508a4b66bd126a451ad16a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xhr1jCyr5HHs_-lmOUtwfw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">均值偏移聚类(图片由作者提供)</p></figure><p id="7cce" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">均值漂移是一种非监督学习算法，主要用于聚类。它广泛用于现实世界的数据分析(例如，图像分割)，因为它是非参数的，并且不需要特征空间中任何预定义的聚类形状。</p><p id="7143" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">简单来说，“均值漂移”等于以迭代的方式“向均值漂移”。在该算法中，每个数据点都在逐步向“区域均值”移动，每个点的最终目的地位置代表它所属的聚类。</p><p id="a0c2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这篇文章中，我将简要介绍如何理解这个算法以及Python中的实现。希望这篇文章有帮助！</p><h2 id="0200" class="md me it bd mf mg mh dn mi mj mk dp ml lq mm mn mo lu mp mq mr ly ms mt mu iz bi translated">什么意思？</h2><p id="3fd3" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">因此，根据上面对均值漂移算法的简要描述，您可能已经注意到，在没有进一步解释的情况下，有几个术语仍然令人困惑。第一个是我们所指的“意思”。</p><p id="5d42" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果我们看一个具有以下两个特征的样本数据集，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi na"><img src="../Images/0e3c01f4fc3d0c503301a09976afc14e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eNzgrE4QWd6syOrvIOtRCQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">样本数据集的平均点(红色)。(图片由作者提供)</p></figure><p id="6973" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通过计算feature_1的平均值和feature_2的平均值，我们可以轻松定位整个数据集的“中间点”(上图中的红色点)。注意，这里的“平均点”分别由特征_1和特征_2的算术平均值来定义，因为它是基于对所有点的相等权重来计算的，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/0a2f796352072cb9f67545ca255f93a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*YPhJX3xR0pkeG-DEYOWT2g.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">算术平均值(图片由作者提供)</p></figure><p id="fc03" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">其中M表示平均值，n是样本大小，x_i是数据点的一个特征(特征1或特征2)。</p><p id="fc82" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">有时给每样东西同等的权重是不合理的，我们也可以计算加权平均值，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/1b18da725eb719a1b17cbe196800b5eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*4gMQJWAUIL2N2p_4hXTuZQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">加权平均值(图片由作者提供)</p></figure><p id="15a9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">其中w_i是x_i的权重，它与点之间的欧氏距离有关。均值漂移算法中最广泛使用的权函数是平坦权函数，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nd"><img src="../Images/fff7e412f7740249bd1bdf6db5609430.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QumuAJ5tzbGW_HoK8-MXIQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">平面权重函数(图片由作者提供)</p></figure><p id="c7f4" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">其中d是任意数据点到当前调查点的距离，R是以调查点为圆心的圆的半径。</p><p id="a2e5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">例如，如果我们想要计算点O周围区域的加权平均值，则可以将平坦加权函数理解为以O为中心、半径为r的硬边界圆。圆内的所有数据都将被计算，而圆外的任何数据点都将被忽略。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ne"><img src="../Images/dad6a89c19012b52f59608ab67db14f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VkssKqfSrKPbl6bHDen1Zw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">平坦加权函数的加权平均值(图片由作者提供)</p></figure><p id="cbe9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这有点像我们站在一个局部点(中心点O)上，看不到整个画面，但被限制在一个局部区域来计算平均值。在这里，我们看到红点是调查区域的加权平均值，我们发现它往往位于点密度较高的区域。</p><p id="7be9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们也可以用高斯函数代替平坦权函数，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/8630ec44c5a74525f26f5d7f8d8d05bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*nskm24IbnLPnELi7-cUmmg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">高斯权重函数(图片由作者提供)</p></figure><p id="b496" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">其中d仍然是中心点到任何其他点之间的距离，这里的σ是一个参数，用于调整权重随着距离的增加而降低的速度。</p><p id="690b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">其思想类似于平权函数，一个点离圆心越近，在均值计算中权重越高。为了实现的简单性和清楚的物理意义，均值漂移算法通常使用<strong class="lj jd">平权函数</strong>来计算均值。</p><p id="1f44" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们上面计算的局部平均点在某种程度上代表了在特定局部区域中具有最大<strong class="lj jd">点密度</strong>的位置。你可能已经注意到，上图中黑色圆圈<strong class="lj jd"> R </strong>的半径对于定义局部区域非常重要。实际上，半径是mean shift算法中唯一的参数，称为“<strong class="lj jd">带宽</strong>”。</p><p id="5c67" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">到目前为止，我想你已经理解了当我们研究给定带宽参数的一个数据点时意味着什么。</p><h2 id="d2ab" class="md me it bd mf mg mh dn mi mj mk dp ml lq mm mn mo lu mp mq mr ly ms mt mu iz bi translated">为什么转变？</h2><p id="aa95" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">我在理解均值漂移算法时遇到的一个最大的问题是“<em class="ng">为什么点要向均值漂移？？</em>“应用这种方法时，我们是否改变了原始数据集？</p><p id="9716" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">实际上，不，我们没有对原始数据做任何改变。“转移”只是描述了我们如何标记每个数据点的过程。相似的点逐渐“移动”到它们所属的聚类的质心，以便被如此标记。</p><p id="49b6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们看一个2-D玩具数据集，它被设计成有三个集群。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi na"><img src="../Images/c8ea5d315436788dc68390191d434de5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dt_3mx5JWSU3jO9QzFsfOw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">具有三个集群的玩具数据集(图片由作者提供)</p></figure><p id="2538" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在mean shift算法中，每个点在每一步中通过向其局部区域的加权平均值移动来试图找到其组。每个点的目的地将是该点所属的数据聚类的质心。然后，具有相同目的地点的所有数据点可以用相同的聚类来标记。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nh"><img src="../Images/0969a7996cda40f4c60441036c537572.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1RfjgZLu1qW0azt5165UPA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">玩具数据集，包含三个具有移动轨迹的集群(图片由作者提供)</p></figure><p id="78a0" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">上面的图显示了标注的结果，其中黑色的大圆圈表示每组数据点的最终目的地，箭头大致显示了一些移动路径的轨迹。</p><p id="2e54" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，您会看到数据并未更改，只是标记了其集群id。</p><h2 id="b341" class="md me it bd mf mg mh dn mi mj mk dp ml lq mm mn mo lu mp mq mr ly ms mt mu iz bi translated">怎么换挡？</h2><p id="971f" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">让我们以一个数据点为例来说明这种转变是如何进行的。假设我们有一个点位于右下方聚类的边缘(下图中的黑点)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ni"><img src="../Images/ae2cd49b40f4693c43d535da0f62906c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LHU_k77NnqQKttbAUT3nKA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">均值偏移中的一个点的步骤1(图片由作者提供)</p></figure><p id="a8d8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在带宽参数定义的局部区域内，计算<em class="ng"> feature_1 </em>和<em class="ng"> feature_2 </em>的平均值后，我们得到了平均点的位置(如上图红点所示)。然后圆心从黑点移到红点。</p><p id="6b92" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，在新的点位置重复上述过程，如下所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ni"><img src="../Images/b7d447dca2e3da6bcddb6edcb79d9845.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-GtfjXnce2bnqmNaRlx7Iw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">均值偏移中的一个点的步骤2(图片由作者提供)</p></figure><p id="2ed3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">经过足够长时间的迭代或黑色圆圈内的点数不再增加(收敛)后，原始数据点到达其目的地，即其所属聚类的质心。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ni"><img src="../Images/a7a90babb187ade5e0d50d91f3e851f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PEmmWKmXmu0GEh6Z85CFKw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">均值偏移中一个点的第N步(图片由作者提供)</p></figure><p id="4702" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">您可能已经注意到，如果对每个数据点都执行这一过程，计算将会非常多余。因此，在实践中，当这些移动的圆重叠时，只有包含最多点的圆被保留。</p><p id="4d85" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">由于均值漂移聚类只有一个参数，即带宽(或某些情况下的窗口大小)，因此在为其选择合适的值时需要格外小心。</p><p id="b804" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">你可能已经注意到，局部区域越大，局部平均点就越接近全局平均点。如果对于所研究的每个数据点，局部区域<strong class="lj jd">都非常大</strong>，那么所有的“局部平均点”实际上都位于“全局平均点”的位置。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/856775a2849a82f6900ef944c9923062.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*gahrB1WzAoyifLgLvwopaQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">超大带宽均值漂移(图片由作者提供)</p></figure><p id="2018" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">换句话说，超大局部区域设置可以使我们看不到数据集的局部结构。</p><p id="3ac2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果我们有一个非常小的带宽，将会有更多的无意义簇，因为我们将平均值计算限制在一个非常小的局部区域。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/407bbf41647ee310e68817dc7635ed2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*lXtEpfB0wdDq4Qx8WCQL4g.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">超小带宽的均值漂移(图片由作者提供)</p></figure><p id="8f85" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，在建模时，最好对带宽有个概念。然而，如果我们没有关于数据的先验知识，我们仍然可以根据从整个数据集随机采样的成对距离来估计带宽。</p><h2 id="f2e8" class="md me it bd mf mg mh dn mi mj mk dp ml lq mm mn mo lu mp mq mr ly ms mt mu iz bi translated">用Python实现</h2><p id="22e9" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">得益于sklearn包，均值漂移聚类的实现相对容易。下面的代码显示了如何估计带宽并使用估计的参数进行聚类。</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="80b2" class="md me it nl b gy np nq l nr ns">bandwidth = estimate_bandwidth(X, quantile=0.3, n_samples=300)</span><span id="68f3" class="md me it nl b gy nt nq l nr ns">ms = MeanShift(bandwidth=bandwidth)<br/>ms.fit(X)</span></pre><p id="281e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了从分类结果中提取数据点的标签，我们可以这样做，</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="df46" class="md me it nl b gy np nq l nr ns">labels = ms.labels_</span></pre><p id="9490" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">就是这样！希望文章有帮助！</p><p id="7736" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你喜欢看我的文章，请<a class="ae nu" href="https://jianan-lin.medium.com/subscribe" rel="noopener">订阅我的媒体账号</a>。</p><h2 id="0581" class="md me it bd mf mg mh dn mi mj mk dp ml lq mm mn mo lu mp mq mr ly ms mt mu iz bi translated">参考资料:</h2><div class="nv nw gp gr nx ny"><a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html#sphx-glr-auto-examples-cluster-plot-mean-shift-py" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd jd gy z fp od fr fs oe fu fw jc bi translated">均值漂移聚类算法的演示</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">参考:Dorin Comaniciu和Peter Meer，“均值漂移:面向特征空间分析的稳健方法”。IEEE…</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">scikit-learn.org</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om lb ny"/></div></div></a></div><div class="nv nw gp gr nx ny"><a href="https://en.wikipedia.org/wiki/Mean_shift" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd jd gy z fp od fr fs oe fu fw jc bi translated">均值漂移-维基百科</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">均值漂移是一种非参数特征空间数学分析技术，用于定位密度的最大值</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">en.wikipedia.org</p></div></div><div class="oh l"><div class="on l oj ok ol oh om lb ny"/></div></div></a></div></div></div>    
</body>
</html>