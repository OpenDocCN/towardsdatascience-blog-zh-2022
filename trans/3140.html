<html>
<head>
<title>Deep-dive on ML techniques for feature selection in Python — Part 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入探讨Python中特性选择的ML技术—第3部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-dive-on-ml-techniques-for-feature-selection-in-python-part-3-de2a7593247f#2022-07-10">https://towardsdatascience.com/deep-dive-on-ml-techniques-for-feature-selection-in-python-part-3-de2a7593247f#2022-07-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="533b" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">Python中AI驱动的特征选择！</h2><div class=""/><div class=""><h2 id="7b34" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">基于ML的特征选择系列文章的最后一部分，在这里我们将讨论像Borutapy和Borutashap这样的高级方法。此外，讨论如何组合多种方法的结果。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/ae171648714ce4b1ec10adfa5f721e0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZYbmP6Yj9hRjYcZu"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://unsplash.com/photos/bTRsbY5RLr4" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/bTRsbY5RLr4</a></p></figure><p id="0308" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated"><span class="l mc md me bm mf mg mh mi mj di">我</span>首先要感谢并祝贺那些完成了基于ML的特征选择方法系列博客最后部分的读者！<br/>如果您还没有看完前两部分，请仔细阅读:</p><p id="79ee" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" href="https://indraneeldb1993ds.medium.com/deep-dive-on-ml-techniques-for-feature-selection-in-python-part-1-3574269d5c69" rel="noopener"> <strong class="lh ja"> <em class="mk">深入探讨Python中用于特征选择的ML技术—第1部分</em> </strong> </a></p><p id="f9fa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" href="https://indraneeldb1993ds.medium.com/deep-dive-on-ml-techniques-for-feature-selection-in-python-part-2-c258f8a2ac43" rel="noopener"> <strong class="lh ja"> <em class="mk">深入探讨Python中用于特征选择的ML技术—第二部分</em> </strong> </a></p><p id="0666" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们已经在前面的章节中介绍了以下内容:</p><p id="ee83" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mk"> A)特征选择方法的类型(第一部分)<br/> B)相关性:皮尔逊、点双序列、克莱姆V(第一部分)<br/> C)证据和信息值的权重(第一部分)<br/> D)贝塔系数(第二部分)<br/> E)拉索回归(第二部分)<br/> F)递归特征选择和顺序特征选择器(第二部分)</em></p><p id="c7d8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们将重点关注以下内容:</p><p id="b9d5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mk"> A)博鲁塔派(第三部分)<br/> B)博鲁塔派(第三部分)<br/> C)把所有的东西结合在一起(第三部分)</em></p><p id="b4c5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如前所述，数据集的细节和整个代码(包括数据准备)可以在这个<a class="ae le" href="https://github.com/IDB-FOR-DATASCIENCE/ML-based-feature-selection.git" rel="noopener ugc nofollow" target="_blank"> Github repo </a>中找到。所以让我们开始吧！</p><h1 id="cff8" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">a)突变性</h1><p id="97bd" class="pw-post-body-paragraph lf lg iq lh b li nd ka lk ll ne kd ln lo nf lq lr ls ng lu lv lw nh ly lz ma ij bi translated">Boruta在2010年作为R ( <a class="ae le" href="https://www.jstatsoft.org/article/view/v036i11" rel="noopener ugc nofollow" target="_blank"> paper link </a>)的一个包首次亮相。它很快成为最流行和最先进的特征选择方法之一，它基于两个概念:</p><p id="68da" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> <em class="mk">概念1:阴影特征</em> </strong></p><p id="ee89" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在Boruta中，特性的选择是基于它们相对于随机版本的性能。主要的逻辑是，只有当一个特性的性能优于随机特性时，它才是有用的。以下是其背后的主要步骤:</p><ol class=""><li id="a815" class="ni nj iq lh b li lj ll lm lo nk ls nl lw nm ma nn no np nq bi translated">基于特征上的原始数据帧，通过随机洗牌产生另一个数据帧。置换特征被称为阴影特征。</li><li id="bbf5" class="ni nj iq lh b li nr ll ns lo nt ls nu lw nv ma nn no np nq bi translated">然后阴影数据帧与原始数据帧结合</li><li id="f5ad" class="ni nj iq lh b li nr ll ns lo nt ls nu lw nv ma nn no np nq bi translated">在此组合数据框架上拟合用户定义的模型</li><li id="565a" class="ni nj iq lh b li nr ll ns lo nt ls nu lw nv ma nn no np nq bi translated">获取所有特征的重要性(原始特征+阴影特征)</li><li id="f91b" class="ni nj iq lh b li nr ll ns lo nt ls nu lw nv ma nn no np nq bi translated">获得阴影特征中记录的最高特征重要性(从现在起我们称之为阈值)。python版本在这一步有一个小变化，我们将在后面讨论。</li><li id="9f37" class="ni nj iq lh b li nr ll ns lo nt ls nu lw nv ma nn no np nq bi translated">当原始特征的重要性高于这个阈值时，我们称之为“命中”</li></ol><p id="0577" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">将阴影要素的最大值作为选择要素的阈值有时非常保守，因此python包允许用户将阴影要素重要性的百分比设置为阈值。默认值100相当于R版本的Boruta所做的。</p><p id="c66e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> <em class="mk">概念二:二项分布</em> </strong></p><p id="524a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">下一个想法是基于多次重复前面提到的步骤，以使结果更加可靠。如果我们称每次迭代为试验，那么在试验中选择一个特性的概率是多少？鉴于我们事先不知道某个特征是否重要，概率是50%，并且由于每个独立试验可以给出二元结果(命中或未命中)，这一系列的<em class="mk"> n </em>试验遵循<a class="ae le" href="https://www.statisticshowto.com/probability-and-statistics/binomial-theorem/binomial-distribution-formula/" rel="noopener ugc nofollow" target="_blank">二项式分布。</a></p><p id="0f23" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果我们以50%的成功概率和5%的显著性水平绘制20次试验的二项式概率分布，概率曲线将看起来像:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nw"><img src="../Images/bdcdf93b318b208b2e6f0d92891831a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HV2ZyZwxPHq9qwfnhaCalw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="d81a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在Borutapy中，特性分为3个部分。这些部分基于分布的两个极端部分，称为尾部(由显著性水平决定):</p><ul class=""><li id="7bae" class="ni nj iq lh b li lj ll lm lo nk ls nl lw nm ma nx no np nq bi translated"><strong class="lh ja">拒绝</strong>(红色区域):这里的特征被认为是噪声，应该丢弃。在上面的图片中，如果一个特性的命中率低于20分之4，那么它就在这个区域。</li><li id="19e9" class="ni nj iq lh b li nr ll ns lo nt ls nu lw nv ma nx no np nq bi translated"><strong class="lh ja">优柔寡断</strong>(蓝色区域):此处的特征并不确定，因此，该模型无法自信地说放弃/保留。在上面的图片中，如果一个特性的点击次数少于16次，多于20次中的3次，那么它就在这个区域。</li><li id="a98e" class="ni nj iq lh b li nr ll ns lo nt ls nu lw nv ma nx no np nq bi translated"><strong class="lh ja">接受</strong>(绿色区域):这里的特征是一个重要的估计量。在我们上面的图片中，如果一个功能在20次点击中超过15次，那么它就在这个区域。</li></ul><p id="6d65" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如何计算BorutaPy的详细示例可以在<a class="ae le" rel="noopener" target="_blank" href="/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a#:~:text=Boruta%20is%20a%20pretty%20smart,and%20can%20be%20found%20here.">这里</a>找到。</p><p id="ed4e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"><em class="mk">boru tapy的Python实现</em> </strong></p><p id="e65a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" href="https://github.com/scikit-learn-contrib/boruta_py" rel="noopener ugc nofollow" target="_blank"> python </a>包比R包有一些改进:</p><ul class=""><li id="8389" class="ni nj iq lh b li lj ll lm lo nk ls nl lw nm ma nx no np nq bi translated"><em class="mk">由于scikit-learn，运行时间更快</em></li><li id="1798" class="ni nj iq lh b li nr ll ns lo nt ls nu lw nv ma nx no np nq bi translated"><em class="mk"> Scikit-learn like接口</em></li><li id="9b57" class="ni nj iq lh b li nr ll ns lo nt ls nu lw nv ma nx no np nq bi translated"><em class="mk">兼容scikit-learn的任何集成方法</em></li><li id="4eea" class="ni nj iq lh b li nr ll ns lo nt ls nu lw nv ma nx no np nq bi translated"><em class="mk">自动n _估算器选择</em></li><li id="8844" class="ni nj iq lh b li nr ll ns lo nt ls nu lw nv ma nx no np nq bi translated"><em class="mk">功能排名</em></li><li id="41e0" class="ni nj iq lh b li nr ll ns lo nt ls nu lw nv ma nx no np nq bi translated"><em class="mk">特征重要性源自基尼系数，而非随机森林R包的MDA。</em></li></ul><p id="20ff" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">另外，强烈建议我们使用修剪过的深度在3-7之间的树。关于python包的更多细节可以在这里找到<a class="ae le" href="https://github.com/scikit-learn-contrib/boruta_py" rel="noopener ugc nofollow" target="_blank">。功能<code class="fe ny nz oa ob b">borutapy_feature_selection</code>允许用户从3种流行的基于树的算法中选择:XG Boost，Random Forest和Light GBM。该函数从用户使用<code class="fe ny nz oa ob b">borutapy_estimator</code>参数指定的列表中设置任何估计值。如果没有提到，默认情况下XG boost模型是合适的。他们还可以改变许多其他特性，比如试验次数(参见下面代码中函数的输入部分)。</a></p><pre class="kp kq kr ks gt oc ob od oe aw of bi"><span id="f703" class="og mm iq ob b gy oh oi l oj ok">#7. Select features based on BorutaPy method</span><span id="3f2d" class="og mm iq ob b gy ol oi l oj ok"># BorutaPy:<br/>borutapy_estimator = "XGBoost"<br/>borutapy_trials = 10<br/>borutapy_green_blue = "both"</span><span id="6132" class="og mm iq ob b gy ol oi l oj ok">################################ Functions #############################################################</span><span id="ee3e" class="og mm iq ob b gy ol oi l oj ok">def borutapy_feature_selection(data, train_target,borutapy_estimator,borutapy_trials,borutapy_green_blue):<br/>    <br/>    #Inputs<br/>    # data - Input feature data <br/>    # train_target - Target variable training data<br/>    # borutapy_estimator - base model (default: XG Boost)<br/>    # borutapy_trials -  number of iteration<br/>    # borutapy_green_blue - choice for green and blue features</span><span id="76f6" class="og mm iq ob b gy ol oi l oj ok">## Initialize borutapy<br/>     <br/>    if borutapy_estimator == "RandomForest":<br/>        # Manual Change in Parameters - RandomForest<br/>        # Link to function parameters - <a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a><br/>        estimator_borutapy=RandomForestClassifier(n_jobs = -1,<br/>                                                  random_state=101,<br/>                                                  max_depth=7)<br/>    elif borutapy_estimator == "LightGBM":<br/>        # Manual Change in Parameters - LightGBM<br/>        # Link to function parameters - <a class="ae le" href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html" rel="noopener ugc nofollow" target="_blank">https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html</a><br/>        estimator_borutapy=lgb.LGBMClassifier(n_jobs = -1,<br/>                                              random_state=101,<br/>                                              max_depth=7)<br/>    else:<br/>        # Manual Change in Parameters - XGBoost<br/>        # Link to function parameters - <a class="ae le" href="https://xgboost.readthedocs.io/en/stable/parameter.html" rel="noopener ugc nofollow" target="_blank">https://xgboost.readthedocs.io/en/stable/parameter.html</a>       <br/>        estimator_borutapy = XGBClassifier(n_jobs = -1,<br/>                                           random_state=101,<br/>                                           max_depth=7)</span><span id="aa2a" class="og mm iq ob b gy ol oi l oj ok">## fit Borutapy<br/>    # Manual Change in Parameters - Borutapy<br/>    # Link to function parameters - <a class="ae le" href="https://github.com/scikit-learn-contrib/boruta_py" rel="noopener ugc nofollow" target="_blank">https://github.com/scikit-learn-contrib/boruta_py</a><br/>    borutapy = BorutaPy(estimator = estimator_borutapy,<br/>                        n_estimators = 'auto', <br/>                        max_iter = borutapy_trials)<br/>    borutapy.fit(np.array(data), np.array(train_target))<br/>    <br/>    ## print results<br/>    green_area = data.columns[borutapy.support_].to_list()<br/>    blue_area = data.columns[borutapy.support_weak_].to_list()<br/>    print('features in the green area:', green_area)<br/>    print('features in the blue area:', blue_area)</span><span id="7254" class="og mm iq ob b gy ol oi l oj ok">    if borutapy_green_blue == "both":<br/>        borutapy_top_features = green_area + blue_area<br/>    else:<br/>        borutapy_top_features = green_area<br/>        <br/>    borutapy_top_features_df =pd.DataFrame(borutapy_top_features,<br/>                                           columns = ['Feature'])<br/>    borutapy_top_features_df['Method'] = 'Borutapy'<br/>    <br/>    return borutapy_top_features_df,borutapy</span><span id="c475" class="og mm iq ob b gy ol oi l oj ok">################################ Calculate borutapy #############################################################</span><span id="9dee" class="og mm iq ob b gy ol oi l oj ok">borutapy_top_features_df,boruta = borutapy_feature_selection(train_features_v2, train_target,borutapy_estimator,borutapy_trials,borutapy_green_blue)</span><span id="2184" class="og mm iq ob b gy ol oi l oj ok">borutapy_top_features_df.head(n=20)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi om"><img src="../Images/598f36776b42bcb16de2f4e32bdd69de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Olm-8CNaS1JgPtnSpNa7g.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><h1 id="14f6" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">博鲁塔·SHAP</h1><p id="70f8" class="pw-post-body-paragraph lf lg iq lh b li nd ka lk ll ne kd ln lo nf lq lr ls ng lu lv lw nh ly lz ma ij bi translated">BorutaPy的致命弱点是它强烈依赖于可能有偏差的特征重要性的计算。这就是沙普利附加解释公司(SHAP)适合这个难题的地方。简单来说，SHAP值可以解释一个复杂的模型如何做出决策。它实际上是为每个观察值计算所有排列中每个特征的平均边际贡献。由于计算的附加性质，我们可以对每个观察值取这些边际贡献的平均值，以获得全局特征重要性。如何计算SHAP值的详细例子可以在找到。</p><p id="68a7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这种方法的唯一缺点是评估时间，因为需要计算许多排列。</p><p id="30a5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> <em class="mk"> Python实现的BorutaShap </em> </strong></p><p id="627f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">功能<code class="fe ny nz oa ob b">borutapy_feature_selection</code>允许用户从3种流行的基于树的算法中选择:XG Boost、随机森林和轻型GBM。该功能从用户使用<code class="fe ny nz oa ob b">borutapy_estimator</code>参数指定的列表中设置任何估计值。如果没有提到，默认情况下XG boost模型是合适的。他们还可以改变许多其他特性，比如试验次数(参见下面代码中函数的输入部分)。</p><pre class="kp kq kr ks gt oc ob od oe aw of bi"><span id="24e9" class="og mm iq ob b gy oh oi l oj ok">#8. Select features based on BorutaShap method</span><span id="636a" class="og mm iq ob b gy ol oi l oj ok"># BorutaShap:<br/>borutashap_estimator = "XGBoost"<br/>borutashap_trials = 10<br/>borutashap_green_blue = 'both'</span><span id="095c" class="og mm iq ob b gy ol oi l oj ok">################################ Functions #############################################################</span><span id="ff0d" class="og mm iq ob b gy ol oi l oj ok">def borutashap_feature_selection(data, train_target,borutashap_estimator,borutashap_trials,borutashap_green_blue):<br/>    <br/>    #Inputs<br/>    # data - Input feature data <br/>    # train_target - Target variable training data<br/>    # borutashap_estimator - base model (default: XG Boost)<br/>    # borutashap_trials -  number of iteration<br/>    # borutashap_green_blue - choice for green and blue features</span><span id="aa4f" class="og mm iq ob b gy ol oi l oj ok">## Initialize borutashap<br/>     <br/>    if borutashap_estimator == "RandomForest":<br/>        # Manual Change in Parameters - RandomForest<br/>        # Link to function parameters - <a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a><br/>        estimator_borutashap=RandomForestClassifier(n_jobs = -1,<br/>                                                    random_state=1,<br/>                                                    max_depth=7)<br/>    elif borutashap_estimator == "LightGBM":<br/>        # Manual Change in Parameters - LightGBM<br/>        # Link to function parameters - <a class="ae le" href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html" rel="noopener ugc nofollow" target="_blank">https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html</a><br/>        estimator_borutashap=lgb.LGBMClassifier(n_jobs = -1,<br/>                                                random_state=101,<br/>                                                max_depth=7)<br/>    else:<br/>        # Manual Change in Parameters - XGBoost<br/>        # Link to function parameters - <a class="ae le" href="https://xgboost.readthedocs.io/en/stable/parameter.html" rel="noopener ugc nofollow" target="_blank">https://xgboost.readthedocs.io/en/stable/parameter.html</a>       <br/>        estimator_borutashap=XGBClassifier(n_jobs = -1,<br/>                                           random_state=101,<br/>                                           max_depth=7)</span><span id="6ae5" class="og mm iq ob b gy ol oi l oj ok">## fit BorutaShap<br/>    # Manual Change in Parameters - BorutaShap<br/>    # Link to function parameters - <a class="ae le" href="https://github.com/scikit-learn-contrib/boruta_py" rel="noopener ugc nofollow" target="_blank">https://github.com/scikit-learn-contrib/boruta_py</a><br/>    borutashap = BorutaShap(model = estimator_borutashap,<br/>                            importance_measure = 'shap',<br/>                            classification = True)<br/>    borutashap.fit(X = data, y = train_target, <br/>                   n_trials = borutashap_trials)<br/>    <br/>    ## print results<br/>    %matplotlib inline<br/>    borutashap.plot(which_features = 'all')</span><span id="2e25" class="og mm iq ob b gy ol oi l oj ok">## print results<br/>    green_area = borutashap.accepted<br/>    blue_area = borutashap.tentative<br/>    print('features in the green area:', green_area)<br/>    print('features in the blue area:', blue_area)</span><span id="e837" class="og mm iq ob b gy ol oi l oj ok">    if borutashap_green_blue == "both":<br/>        borutashap_top_features = green_area + blue_area<br/>    else:<br/>        borutashap_top_features = green_area<br/>        <br/>    borutashap_top_features_df=pd.DataFrame(borutashap_top_features,<br/>                                            columns = ['Feature'])<br/>    borutashap_top_features_df['Method'] = 'Borutashap'</span><span id="c5fc" class="og mm iq ob b gy ol oi l oj ok">    return borutashap_top_features_df,borutashap</span><span id="a15c" class="og mm iq ob b gy ol oi l oj ok">################################ Calculate borutashap #############################################################</span><span id="6f67" class="og mm iq ob b gy ol oi l oj ok">borutashap_top_features_df,borutashap = borutashap_feature_selection(train_features_v2, train_target,borutashap_estimator,borutashap_trials,borutashap_green_blue)<br/>borutashap_top_features_df.head(n=20)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi on"><img src="../Images/8729d7f8e4a3c50a997095fc6e5accdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5L9gDdtWsUBtMQ11JcvJTQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="6c99" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">关于python包的更多细节可以在<a class="ae le" href="https://github.com/Ekeany/Boruta-Shap" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="6110" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated"><strong class="ak"> <em class="oo"> C)汇集一切</em> </strong></h1><blockquote class="op"><p id="d4ee" class="oq or iq bd os ot ou ov ow ox oy ma dk translated">永远不要把所有的鸡蛋放在一个篮子里</p></blockquote><p id="ffee" class="pw-post-body-paragraph lf lg iq lh b li oz ka lk ll pa kd ln lo pb lq lr ls pc lu lv lw pd ly lz ma ij bi translated">这句至理名言非常适合数据科学的大多数应用。大多数情况下，没有一个ML模型是没有缺陷的，但是每个模型都有自己的优点。根据我的经验，不要只依赖一种方法，如果可能的话，使用多种方法的组合总是一个好策略。这样，我们就不会受到任何方法的主要缺点的影响，并利用多种方法的优势。相同的逻辑可以应用于特征选择技术。现在我们已经学习了8种不同的方法，我建议读者使用它们的组合(或全部),并选择大多数方法选择的特征。下面是一个快速的python代码</p><pre class="kp kq kr ks gt oc ob od oe aw of bi"><span id="7984" class="og mm iq ob b gy oh oi l oj ok"># Methods Selected</span><span id="85a0" class="og mm iq ob b gy ol oi l oj ok">selected_method = [corr_top_features_df, woe_top_features_df,beta_top_features_df,lasso_top_features_df,<br/>                   rfe_top_features_df,sfs_top_features_df,borutapy_top_features_df,borutashap_top_features_df]</span><span id="8469" class="og mm iq ob b gy ol oi l oj ok"># Combining features from all the models<br/>master_df_feature_selection = pd.concat(selected_method, axis =0)<br/>number_of_methods = len(selected_method)<br/>selection_threshold = int(len(selected_method)/2)<br/>print('Selecting features which are picked by more than ', selection_threshold, ' methods')<br/>master_df_feature_selection_v2 = pd.DataFrame(master_df_feature_selection.groupby('Feature').size()).reset_index()<br/>master_df_feature_selection_v2.columns = ['Features', 'Count_Method']<br/>master_df_feature_selection_v3 = master_df_feature_selection_v2[master_df_feature_selection_v2['Count_Method']&gt;selection_threshold]<br/>final_features = master_df_feature_selection_v3['Features'].tolist()<br/>print('Final Features Selected: ',final_features)<br/>train_features_v2[final_features].hist(figsize = (14,14), xrot =  45)<br/>plt.show()<br/>master_df_feature_selection_v3.head(n=30)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/dafe73a276ea0ef31b79af02c80033d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*cZ_L8BSaQh070CA8ubPHog.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><h1 id="7d78" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">最后的话</h1><blockquote class="op"><p id="b554" class="oq or iq bd os ot ou ov ow ox oy ma dk translated">恭喜你！</p></blockquote><p id="d585" class="pw-post-body-paragraph lf lg iq lh b li oz ka lk ll pa kd ln lo pb lq lr ls pc lu lv lw pd ly lz ma ij bi translated">我们已经完成了基于ML的特征选择技术系列！我们深入探讨了8种主要方法，它们分布在不同的类别(过滤器、包装器和嵌入式)、计算难度和易于理解的程度。我们不仅学习了它背后的理论，还能够用python实现它们。我们还讨论了使用它们时的最佳实践。这个博客系列以及参考资料应该足以让任何人开始使用这些方法。</p><p id="7949" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这里的最后一块拼图是要记住，我们在这里只涉及了科学，但是特征选择也是一门艺术。通过实践，我们会做得更好，但我想用一条建议来结束这个系列:尽可能多地使用方法，并始终根据业务逻辑来验证结果。有时，即使模型不能挑选一些关键特性，但业务逻辑要求我们使用它，我也会建议添加它。</p><h1 id="e23f" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">参考材料</h1><ol class=""><li id="1812" class="ni nj iq lh b li nd ll ne lo pf ls pg lw ph ma nn no np nq bi translated"><a class="ae le" href="https://github.com/scikit-learn-contrib/boruta_py" rel="noopener ugc nofollow" target="_blank"> Borutapy python包</a></li><li id="4d29" class="ni nj iq lh b li nr ll ns lo nt ls nu lw nv ma nn no np nq bi translated"><a class="ae le" href="https://github.com/Ekeany/Boruta-Shap" rel="noopener ugc nofollow" target="_blank"> Borutshap python包</a></li></ol><h1 id="aaf3" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">我们连线吧！</h1><p id="875c" class="pw-post-body-paragraph lf lg iq lh b li nd ka lk ll ne kd ln lo nf lq lr ls ng lu lv lw nh ly lz ma ij bi translated">如果你和我一样，对AI、数据科学或经济学充满热情，请随时在<a class="ae le" href="http://www.linkedin.com/in/indraneel-dutta-baruah-ds" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae le" href="https://github.com/IDB-FOR-DATASCIENCE" rel="noopener ugc nofollow" target="_blank"> Github </a>和<a class="ae le" href="https://medium.com/@indraneeldb1993ds" rel="noopener"> Medium </a>上添加/关注我。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/b2aef855fb98eaafa75adbb9912f19cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*20vLbWVF7tdLix7W"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@wilhelmgunkel" rel="noopener ugc nofollow" target="_blank">威廉·冈克尔</a>在<a class="ae le" href="https://unsplash.com/photos/L04Kczg_Jvs" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div></div>    
</body>
</html>