<html>
<head>
<title>Let us Extract some Topics from Text Data — Part I: Latent Dirichlet Allocation (LDA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让我们从文本数据中提取一些主题—第一部分:潜在狄利克雷分配(LDA)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/let-us-extract-some-topics-from-text-data-part-i-latent-dirichlet-allocation-lda-e335ee3e5fa4#2022-11-03">https://towardsdatascience.com/let-us-extract-some-topics-from-text-data-part-i-latent-dirichlet-allocation-lda-e335ee3e5fa4#2022-11-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8b61" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Python的nltk、gensim、sklearn和pyLDAvis包了解主题建模需要什么及其实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/2cb6a019de91ac46386865d98b6b9d02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*tpnL4GKB39-idm68"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">来自<a class="ae ku" href="https://www.pexels.com/ko-kr/photo/6937933/" rel="noopener ugc nofollow" target="_blank">像素</a>的免费使用照片</p></figure><h1 id="f38a" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">介绍</h1><p id="6d5a" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated"><strong class="lp iu">主题建模</strong>是一种自然语言处理(NLP)任务，它利用无监督学习方法从我们处理的一些文本数据中提取出主要主题。这里的“无监督”一词意味着没有与主题标签相关联的训练数据。相反，算法试图直接从数据本身发现潜在的模式，在这种情况下，是主题。</p><p id="6a7a" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">有各种各样的算法广泛用于主题建模。在接下来的系列文章中，我打算一一介绍。在本文中，我们看看什么是<strong class="lp iu">潜在的狄利克雷分配(LDA) </strong>算法，它是如何工作的，以及如何使用多个Python包来实现它。</p><h1 id="91c8" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">什么是LDA？</h1><p id="0efa" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">LDA是一种用于主题建模的模型，由于与可能执行相同任务的极其复杂的神经网络相比，它相对简单和有效，所以经常被使用。</p><p id="814c" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我将我们处理的文本数据称为“语料库”,将每个文本观察称为“文档”。LDA的基本假设是每个文档可以由主题的分布来表示，而主题的分布又可以由一些词的分布来表示。请注意“分发”这个词。LDA假设每个文档不仅仅由一个主题组成，而是由几个主题组成。因此，LDA模型将分解每个主题对该文档的百分比贡献。例如，如果我们将主题的数量设置为5，那么文档A可以表示如下:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="a7d4" class="mt kw it mp b gy mu mv l mw mx">document_A = 0.2 x Topic1 + 0.1 x Topic2 + 0.1 x Topic3 + 0.5 x Topic4 + 0.1 x Topic5 </span></pre><p id="2ff9" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我不会深入这个算法的数学，但是对于那些感兴趣的人，请看看这个算法的最初发明者的这篇<a class="ae ku" href="https://ai.stanford.edu/~ang/papers/jair03-lda.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</p><p id="f533" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我要提醒读者的是，不要将潜在的狄利克雷分配与线性判别分析模型混淆，线性判别分析模型是一种分类算法，因为它们通常都缩写为LDA。</p><p id="0528" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">现在，让我们看一些实际的代码，看看如何实现LDA。</p><h1 id="3e7c" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">清理文本</h1><p id="3764" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">我们首先导入nltk包，并下载一个相关语料库的列表，这些语料库将用于删除停用词、词干化和词汇化。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="cc4d" class="mt kw it mp b gy mu mv l mw mx"><strong class="mp iu">import </strong>nltk</span><span id="775c" class="mt kw it mp b gy my mv l mw mx"><strong class="mp iu">from </strong>nltk.stem import *</span><span id="93f9" class="mt kw it mp b gy my mv l mw mx">nltk.download(‘punkt’) <strong class="mp iu"># For Stemming</strong></span><span id="2f68" class="mt kw it mp b gy my mv l mw mx">nltk.download(‘wordnet’) <strong class="mp iu"># For Lemmatization</strong></span><span id="3508" class="mt kw it mp b gy my mv l mw mx">nltk.download(‘stopwords’) <strong class="mp iu"># For Stopword Removal</strong></span><span id="547e" class="mt kw it mp b gy my mv l mw mx">nltk.download(‘omw-1.4’)</span></pre><p id="663c" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我们还在一个名为stop words的变量中存储了一个以后要使用的英语停用词列表。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="e551" class="mt kw it mp b gy mu mv l mw mx">stopwords = set(nltk.corpus.stopwords.words('english'))</span></pre><p id="50f4" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">是时候载入一些数据了。这里，我们使用开源的20个新闻组数据集，每个人都可以在sklearn包接口中使用该数据集。它使用的是<a class="ae ku" href="https://www.apache.org/licenses/LICENSE-2.0" rel="noopener ugc nofollow" target="_blank">Apache 2.0版许可证</a>。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="86a7" class="mt kw it mp b gy mu mv l mw mx"><strong class="mp iu">import</strong> pandas as <strong class="mp iu">pd</strong></span><span id="0be6" class="mt kw it mp b gy my mv l mw mx"><strong class="mp iu">import </strong>numpy as <strong class="mp iu">np</strong></span><span id="5a6c" class="mt kw it mp b gy my mv l mw mx">from sklearn.datasets import fetch_20newsgroups</span><span id="fe4d" class="mt kw it mp b gy my mv l mw mx">fetch20newsgroups = fetch_20newsgroups(subset='train')</span><span id="7a0b" class="mt kw it mp b gy my mv l mw mx"><strong class="mp iu"># Store in a pandas dataframe</strong></span><span id="c3ae" class="mt kw it mp b gy my mv l mw mx">df = pd.DataFrame(fetch20newsgroups.data, columns=['text'])</span></pre><p id="129a" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">接下来，我们做一些文本清理，如删除一些无关的单词或表达。我们删除网址，提及和散列标签。为此，我们导入正则表达式包，然后使用sub函数删除与指定正则表达式匹配的字符串部分。请记住，“\S”匹配除空白以外的单个字符。如果你看下面的代码，我们匹配的表达式包括“https”，“@”和“#”，它们对应于我们想要从文本中删除的字符串。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="4d2c" class="mt kw it mp b gy mu mv l mw mx"><strong class="mp iu"># Remove URLs</strong></span><span id="b01e" class="mt kw it mp b gy my mv l mw mx"><strong class="mp iu">import </strong>re # Import regular expression package</span><span id="b4bf" class="mt kw it mp b gy my mv l mw mx"><strong class="mp iu">def </strong>remove_url(text):<br/>    return re.sub(r'https?:\S*','',text)</span><span id="06ec" class="mt kw it mp b gy my mv l mw mx">df.text = df.text.apply(remove_url)</span><span id="90ba" class="mt kw it mp b gy my mv l mw mx"><strong class="mp iu"># Remove mentions and hashtags</strong></span><span id="250e" class="mt kw it mp b gy my mv l mw mx"><strong class="mp iu">import </strong>re</span><span id="186f" class="mt kw it mp b gy my mv l mw mx"><strong class="mp iu">def </strong>remove_mentions_and_tags(text):<br/>     text = re.sub(r'@\S*','',text)<br/>     return re.sub(r'#\S*','',text)</span><span id="5349" class="mt kw it mp b gy my mv l mw mx">df.text = df.text.apply(remove_mentions_and_tags)</span></pre><p id="c50f" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">现在，我们希望将我们拥有的文档标记化，并将它们转换成某种字典形式，可以用作gensim模型的输入。</p><h1 id="c2b2" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">预处理</h1><p id="fb2e" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">我们使用下面的自定义函数来预处理语料库中的文档。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="3805" class="mt kw it mp b gy mu mv l mw mx"><strong class="mp iu">def </strong>text_preprocessing(df):</span><span id="1424" class="mt kw it mp b gy my mv l mw mx">    corpus=[]<br/>    <br/>    lem = WordNetLemmatizer() <strong class="mp iu"># For Lemmatization</strong></span><span id="1383" class="mt kw it mp b gy my mv l mw mx">    for news in df['text']:<br/>        words=[w for w in nltk.tokenize.word_tokenize(news) if (w not in stopwords)] <strong class="mp iu"># word_tokenize function tokenizes text on each word by default</strong></span><span id="8974" class="mt kw it mp b gy my mv l mw mx">        words=[lem.lemmatize(w) for w in words if len(w)&gt;2]</span><span id="484b" class="mt kw it mp b gy my mv l mw mx">        corpus.append(words)</span><span id="68c5" class="mt kw it mp b gy my mv l mw mx">    return corpus</span><span id="a676" class="mt kw it mp b gy my mv l mw mx"><strong class="mp iu"># Apply this function on our data frame<br/></strong>corpus = text_preprocessing(df)</span></pre><p id="d505" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">从上面的代码中可以看出，我们循环遍历每个文档，并按顺序应用标记化和词汇化。记号化意味着将文档分解成称为记号的小分析单元，记号通常是单词。词汇化指的是将单词还原为其原始基本形式的过程，称为“词汇”。将同一个动词的多种形式根据它的时态改变成它的基本形式就是词汇化的一个例子。</p><p id="dab5" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我们利用Python中的gensim包来继续第3步和后续步骤。</p><p id="7cf1" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">使用以下命令安装gensim包。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="851d" class="mt kw it mp b gy mu mv l mw mx">!pip install -U gensim==3.8.3</span></pre><p id="febd" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我安装特定版本的gensim (3.8.3)的原因是因为一些不同的LDA实现(如LDA Mallet模型)在以后的版本中已经被删除。</p><p id="2d5e" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我们首先使用已经预处理过的语料库创建一个gensim dictionary对象，然后创建一个名为“bow_corpus”的变量，在其中存储单词包(bow)转换后的文档。由于gensim包接受输入的方式，这一步是必要的。然后，我们使用pickle包保存bow_corpus和字典供以后使用。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="a295" class="mt kw it mp b gy mu mv l mw mx">import gensim</span><span id="21a1" class="mt kw it mp b gy my mv l mw mx"># Transform to gensim dictionary<br/>dic = gensim.corpora.Dictionary(corpus) </span><span id="3f17" class="mt kw it mp b gy my mv l mw mx">bow_corpus = [dic.doc2bow(doc) for doc in corpus]</span><span id="03a6" class="mt kw it mp b gy my mv l mw mx">import pickle # Useful for storing big datasets</span><span id="9c6d" class="mt kw it mp b gy my mv l mw mx">pickle.dump(bow_corpus, open('corpus.pkl', 'wb'))</span><span id="415b" class="mt kw it mp b gy my mv l mw mx">dic.save('dictionary.gensim')</span></pre><h1 id="d3ed" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">实际模型</h1><p id="3f9b" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">接下来，我们使用gensim.models类中的LDAMulticore函数来实例化我们的LDA模型。有人可能会问LDAMulticore函数和基本的LDAmodel函数有什么区别？它们基本上是一样的，但是前者支持多进程，这可以节省你的运行时间，所以为什么不用它来代替基本的呢？注意，我们可以通过“workers”参数指定将参与多处理操作的处理器数量。我们还通过“num_topics”参数指定要从语料库中提取的主题数量。不幸的是，除非你事先有一些关于语料库的领域知识，否则没有办法找出多少主题是最佳值。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="5076" class="mt kw it mp b gy mu mv l mw mx">lda_model = gensim.models.LdaMulticore(bow_corpus,<br/>                                   num_topics = 4,<br/>                                    id2word = dic,<br/>                                      passes = 10,<br/>                                      workers = 2)</span><span id="e27e" class="mt kw it mp b gy my mv l mw mx">lda_model.save('model4.gensim')</span></pre><p id="68e6" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">一旦我们训练了LDA模型，我们查看从语料库中提取的每个主题中最重要的前十个单词。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="8a15" class="mt kw it mp b gy mu mv l mw mx"><strong class="mp iu"># We print words occuring in each of the topics as we iterate through them</strong></span><span id="bedf" class="mt kw it mp b gy my mv l mw mx">for idx, topic in lda_model.print_topics(num_words=10):    <br/>    print('Topic: {} \nWords: {}'.format(idx, topic))</span></pre><p id="d2dc" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">输出如下。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="36ca" class="mt kw it mp b gy mu mv l mw mx">Topic: 0  Words: 0.008*"The" + 0.007*"would" + 0.006*"one" + 0.006*"From" + 0.006*"people" + 0.005*"writes" + 0.005*"Subject" + 0.005*"Lines" + 0.005*"Organization" + 0.005*"article"</span><span id="815b" class="mt kw it mp b gy my mv l mw mx">Topic: 1  Words: 0.008*"The" + 0.007*"Subject" + 0.007*"From" + 0.007*"Lines" + 0.007*"Organization" + 0.004*"use" + 0.004*"file" + 0.003*"one" + 0.003*"would" + 0.003*"get"</span><span id="28e0" class="mt kw it mp b gy my mv l mw mx">Topic: 2  Words: 0.007*"From" + 0.007*"Organization" + 0.007*"Lines" + 0.006*"The" + 0.006*"Subject" + 0.005*"game" + 0.005*"University" + 0.005*"team" + 0.004*"year" + 0.003*"writes"</span><span id="6594" class="mt kw it mp b gy my mv l mw mx">Topic: 3  Words: 0.013*"MAXAXAXAXAXAXAXAXAXAXAXAXAXAXAX" + 0.009*"The" + 0.006*"From" + 0.006*"Subject" + 0.005*"Lines" + 0.005*"key" + 0.005*"Organization" + 0.004*"writes" + 0.004*"article" + 0.003*"Israel"</span></pre><p id="850f" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">可能题目2和“大学”这个词有点关系。除此之外，我们没有看到任何重要的东西。这里发生了什么事？请注意，有些词在所有主题中都出现得非常频繁。它们是包括主题、文章和组织的单词。这些词出现在每篇新闻文章的开头，列出了作者、所属单位和标题。这就是为什么他们主宰了每一个话题。换句话说，它们是没有信息的噪音，不会给我们构建的LDA模型增加多少价值。这说明了文本清理和预处理与你的自然语言处理项目的目标相一致的重要性！</p><h1 id="c583" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">估价</h1><p id="2454" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">正如我前面提到的，没有一个确定的方法来找出多少个主题是最佳的。一种方法是简单地观察每个主题中的一些关键词，看它们是否相互连贯。但这绝对不是一种客观或严谨的评价方式。一个稍微严谨一点的方法是使用连贯性评分。</p><p id="6195" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">这个分数衡量每个主题中高分单词之间的语义相似程度。现有的算法有C_v，C_p，C_uci，C_umass等。C_v和C_umass是两种应用较为广泛的方法。</p><p id="dd06" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">C_v的一致性分数范围从0到1，值越高，一致性越好。C_umass方法返回负值。这里的棘手之处在于，对于什么样的价值构成“良好的一致性”，没有明确的共识或解释。学者们已经做了一些尝试，例如<a class="ae ku" href="https://www.amazon.com/Doing-Computational-Social-Science-Introduction/dp/1526468190" rel="noopener ugc nofollow" target="_blank">约翰·麦克里维，他认为任何超过0.5分的C_v方法都是相当好的</a>。然而，如果你浏览一些数据科学的论坛，会有一些不同的观点。来自Stackoverflow的这篇<a class="ae ku" href="https://stackoverflow.com/questions/54762690/evaluation-of-topic-modeling-how-to-understand-a-coherence-value-c-v-of-0-4" rel="noopener ugc nofollow" target="_blank">帖子</a>给出了以下建议。</p><ul class=""><li id="4322" class="mz na it lp b lq mj lt mk lw nb ma nc me nd mi ne nf ng nh bi translated">0.3是不好的</li><li id="3acf" class="mz na it lp b lq ni lt nj lw nk ma nl me nm mi ne nf ng nh bi translated">0.4很低</li><li id="82ec" class="mz na it lp b lq ni lt nj lw nk ma nl me nm mi ne nf ng nh bi translated">0.55没问题</li><li id="2852" class="mz na it lp b lq ni lt nj lw nk ma nl me nm mi ne nf ng nh bi translated">0.65可能是最好的结果了</li><li id="f10a" class="mz na it lp b lq ni lt nj lw nk ma nl me nm mi ne nf ng nh bi translated">0.7就不错了</li><li id="9b05" class="mz na it lp b lq ni lt nj lw nk ma nl me nm mi ne nf ng nh bi translated">0.8不太可能</li><li id="5a5b" class="mz na it lp b lq ni lt nj lw nk ma nl me nm mi ne nf ng nh bi translated">0.9很可能是错的</li></ul><p id="ed4e" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">对于u_mass方法，它变得更加混乱。看看发生在这个<a class="ae ku" href="https://www.reddit.com/r/learnmachinelearning/comments/9bcr77/coherence_score_u_mass/" rel="noopener ugc nofollow" target="_blank"> Reddit论坛</a>的讨论。它说，当我们增加要提取的主题数量时，一致性分数可能会非常不稳定。我认为大多数数据科学用户提出的一般想法是，C_v方法通常比C_umass方法更可靠，尽管最终的裁决取决于用户。</p><p id="b7cb" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">尽管如此，我认为从所有这些关于一致性评估的信息中获得的关键信息是:</p><ul class=""><li id="65fe" class="mz na it lp b lq mj lt mk lw nb ma nc me nd mi ne nf ng nh bi translated">一致性分数不是评估主题质量的绝对标准。千万不要只依靠一种方法。使用连贯性分数作为基线，然后确保你浏览每个主题的文档及其关键词，以获得你“自己”对每个主题连贯性的感觉。</li><li id="97e6" class="mz na it lp b lq ni lt nj lw nk ma nl me nm mi ne nf ng nh bi translated">对于计算一致性得分的C_v方法，检查您的模型的得分是否不太高也不太低。0.5到0.7之间的任何值都是一个合适的范围。</li><li id="1a17" class="mz na it lp b lq ni lt nj lw nk ma nl me nm mi ne nf ng nh bi translated">对于计算一致性分数的C_umass方法，检查分数的绝对值是否合理地接近0。</li></ul><p id="acdd" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">让我们用C_v方法来评价我们的模型。gensim包的models类包含CoherenceModel，它可以方便地计算一致性分数。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="4fe1" class="mt kw it mp b gy mu mv l mw mx"><strong class="mp iu">from </strong>gensim.models <strong class="mp iu">import </strong>CoherenceModel</span><span id="6f29" class="mt kw it mp b gy my mv l mw mx"><strong class="mp iu"># instantiate topic coherence model</strong></span><span id="4195" class="mt kw it mp b gy my mv l mw mx">cm = CoherenceModel(model=lda_model, corpus=bow_corpus, texts=corpus, coherence='c_v')</span><span id="0c2f" class="mt kw it mp b gy my mv l mw mx"><strong class="mp iu"># get topic coherence score</strong></span><span id="40d0" class="mt kw it mp b gy my mv l mw mx">coherence_lda = cm.get_coherence()</span><span id="04b8" class="mt kw it mp b gy my mv l mw mx">print(coherence_lda)<br/>&gt;&gt; 0.49725706360481475</span></pre><p id="c65d" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">0.497不差但也不体面。我们想知道连贯分数是否随着从语料库中提取的主题数量而变化。我们从数字2到6迭代主题的数量，并将每个一致性分数存储在名为score的变量中，该变量被初始化为一个空列表。然后我们使用Python的matplotlib包将它可视化成一个线图。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="2d2f" class="mt kw it mp b gy mu mv l mw mx">import matplotlib.pyplot as plt</span><span id="a88d" class="mt kw it mp b gy my mv l mw mx">topics = []</span><span id="709a" class="mt kw it mp b gy my mv l mw mx">score = []</span><span id="7f92" class="mt kw it mp b gy my mv l mw mx">for i in range(2,7,1):<br/>     lda = gensim.models.LdaMulticore(corpus=bow_corpus, id2word=dic, iterations=10, num_topics=i, workers = 3, passes=10, random_state=42)</span><span id="c193" class="mt kw it mp b gy my mv l mw mx">     cm = CoherenceModel(model=lda, corpus=bow_corpus, texts=corpus, coherence='c_v')</span><span id="bd3d" class="mt kw it mp b gy my mv l mw mx">     topics.append(i) # Append number of topics modeled</span><span id="def2" class="mt kw it mp b gy my mv l mw mx">     score.append(cm.get_coherence()) # Append coherence scores to list</span><span id="6565" class="mt kw it mp b gy my mv l mw mx">plt.plot(topics, score)</span><span id="dc56" class="mt kw it mp b gy my mv l mw mx">plt.xlabel('# of Topics')</span><span id="60b4" class="mt kw it mp b gy my mv l mw mx">plt.ylabel('Coherence Score')</span><span id="9e16" class="mt kw it mp b gy my mv l mw mx">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/76f6119e29db234428ed7b475b185e82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*THmu40Y27ukC3GK9znnnnA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">来源:来自作者</p></figure><p id="6800" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我们看到，当有两个主题要提取时，连贯性得分最高。它随着话题数量的增加而不断减少。</p><p id="05c7" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我们现在基于两个主题建立新的LDA模型。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="4f64" class="mt kw it mp b gy mu mv l mw mx"><strong class="mp iu"># LDA model with two topics</strong><br/>lda_model2 = gensim.models.LdaMulticore(bow_corpus,<br/>                                    num_topics = 2,<br/>                                     id2word = dic,<br/>                                        passes = 8,<br/>                                       workers = 3)</span><span id="9227" class="mt kw it mp b gy my mv l mw mx">lda_model2.save('model2.gensim')<br/></span><span id="7517" class="mt kw it mp b gy my mv l mw mx"><strong class="mp iu"># We print words occuring in each of the topics as we iterate through them</strong></span><span id="c339" class="mt kw it mp b gy my mv l mw mx">for idx, topic in lda_model2.print_topics(num_words=20):</span><span id="ca50" class="mt kw it mp b gy my mv l mw mx">print('Topic: {} \nWords: {}'.format(idx, topic))</span></pre><p id="252b" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">结果输出:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="e715" class="mt kw it mp b gy mu mv l mw mx">Topic: 0  Words: 0.007*"The" + 0.005*"MAXAXAXAXAXAXAXAXAXAXAXAXAXAXAX" + 0.004*"From" + 0.004*"Subject" + 0.004*"Lines" + 0.004*"Organization" + 0.003*"one" + 0.002*"get" + 0.002*"year" + 0.002*"<strong class="mp iu">game</strong>" + 0.002*"like" + 0.002*"would" + 0.002*"time" + 0.002*"University" + 0.002*"writes" + 0.002*"<strong class="mp iu">team</strong>" + 0.002*"article" + 0.002*"dont" + 0.002*"This" + 0.002*"people" </span><span id="540f" class="mt kw it mp b gy my mv l mw mx">Topic: 1  Words: 0.009*"The" + 0.007*"From" + 0.007*"Subject" + 0.007*"Lines" + 0.007*"Organization" + 0.005*"would" + 0.005*"writes" + 0.005*"one" + 0.004*"article" + 0.003*"people" + 0.003*"<strong class="mp iu">know</strong>" + 0.003*"<strong class="mp iu">like</strong>" + 0.003*"University" + 0.003*"dont" + 0.003*"get" + 0.003*"<strong class="mp iu">think</strong>" + 0.002*"This" + 0.002*"<strong class="mp iu">use</strong>" + 0.002*"time" + 0.002*"<strong class="mp iu">say</strong>"</span></pre><p id="5be3" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">上面的输出显示，主题0与游戏和团队有些关系，而主题1与动作动词更相关，包括知道、喜欢、使用和说。由于我们没有删除那些频繁出现的代词或冠词，这些代词或冠词只是为了它们的语法目的，并没有给模型增加太多的意义或价值，所以噪音在这两个主题中仍然相当猖獗。然而，即使有了眼球，我们也能对每个主题的内容有一个稍微好一点的感觉，这通常意味着更好的连贯性。</p><h1 id="ccfb" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">可视化LDA结果</h1><p id="7ea4" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">使用另一个叫做pyLDAvis的软件包，我们可以可视化LDA结果。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="b008" class="mt kw it mp b gy mu mv l mw mx">!pip install pyldavis</span><span id="67c1" class="mt kw it mp b gy my mv l mw mx"><strong class="mp iu">import </strong>pyLDAvis</span><span id="65b4" class="mt kw it mp b gy my mv l mw mx"><strong class="mp iu">import </strong>pyLDAvis.gensim_models</span></pre><p id="2a76" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">请注意，pyLDAvis包有了更新，所以人们以前知道的pyLDAvis.gensim模块现在变成了<strong class="lp iu"> pyLDAvis.gensim_models </strong>而不是<strong class="lp iu">。</strong></p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="637a" class="mt kw it mp b gy mu mv l mw mx"><strong class="mp iu"># Loading the dictionary and corpus files we saved earlier</strong><br/>dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')</span><span id="7a6b" class="mt kw it mp b gy my mv l mw mx">corpus = pickle.load(open('corpus.pkl', 'rb'))</span><span id="bae2" class="mt kw it mp b gy my mv l mw mx"><strong class="mp iu"># Loading the num_of_topics = 2 model we saved earlier<br/></strong>lda = gensim.models.ldamodel.LdaModel.load('model2.gensim')</span><span id="e97c" class="mt kw it mp b gy my mv l mw mx">pyLDAvis.enable_notebook()</span><span id="2e19" class="mt kw it mp b gy my mv l mw mx">vis = pyLDAvis.gensim_models.prepare(lda, bow_corpus, dic, sort_topics=False)</span><span id="180a" class="mt kw it mp b gy my mv l mw mx">pyLDAvis.display(vis)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi no"><img src="../Images/4a26a880a27f57c59430b450fc6a6a9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aw67iBAoes36gtV2ATBb8Q.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">来源:来自作者</p></figure><p id="991e" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">在我上面截图的pyLDAvis仪表板的左侧，每个圆圈的面积代表了主题相对于语料库的重要性。此外，圆心之间的距离表示主题之间的相似性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi nt"><img src="../Images/2d1f0773e14dadfc81a7781cedf89f15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EcwPe0MOoqdsr74YJnZk6Q.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">来源:来自作者</p></figure><p id="3396" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">现在，在仪表板的右侧，正如您在右上方看到的，每个主题的前30个相关词以直方图显示，红色部分代表所选主题内的估计词频(显著性)，浅蓝色部分代表整体词频。</p><h1 id="88ff" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">奖金</h1><p id="1e25" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">LDA有各种版本。例如，LDA Mallet是LDA的另一个版本，它使用了<a class="ae ku" rel="noopener" target="_blank" href="/gibbs-sampling-8e4844560ae5">吉布斯采样</a>，与我在上面展示的普通LDA模型相比，它提高了性能。LDA Mallet模型的一个缺点是相对于它的原始对应物，它的计算效率较低，因此对于大型数据集，它可能不是最合适的。</p><p id="99ad" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">请看看这篇<a class="ae ku" href="https://neptune.ai/blog/pyldavis-topic-modelling-exploration-tool-that-every-nlp-data-scientist-should-know" rel="noopener ugc nofollow" target="_blank">文章</a>的最后一部分，了解更多关于LDA Mallet模型的信息。</p><h1 id="7cb8" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">结论</h1><p id="c16b" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">在本文中，我向您介绍了什么是主题建模，以及LDA模型可以做什么来执行主题建模。我还逐步向您介绍了如何清理和预处理文本数据，然后在其上构建LDA模型。最后，我阐述了如何评估您的LDA模型以及如何可视化结果。</p><p id="8e21" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">读者应该记住的一个要点是，预处理对增加主题连贯性比你想象的更重要。确保文本数据中没有太多噪声的一些方法是过滤掉极高或极低频率的标记，并排除特定的词性(PoS)标记的单词，例如in(例如，upon，except)和MD(例如，may，must)。当然，后者必须在词性标注之前，词性标注本身是一个独立的NLP任务，我将在另一篇文章中介绍。</p><p id="c794" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">如果你觉得这篇文章有帮助，请考虑通过以下链接注册medium来支持我: )</p><p id="c9ea" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">【joshnjuny.medium.com T4】</p><p id="c778" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">你不仅可以看到我，还可以看到其他作者写的这么多有用和有趣的文章和帖子！</p><h1 id="2ffd" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">关于作者</h1><p id="85a0" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated"><em class="nu">数据科学家。加州大学欧文分校信息学专业一年级博士生。</em></p><p id="4509" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated"><em class="nu">密歇根大学刑事司法行政记录系统(CJARS)经济学实验室的前研究领域专家，致力于统计报告生成、自动化数据质量审查、构建数据管道和数据标准化&amp;协调。Spotify前数据科学实习生。Inc .(纽约市)。</em></p><p id="bc51" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">他喜欢运动、健身、烹饪美味的亚洲食物、看kdramas和制作/表演音乐，最重要的是崇拜我们的主耶稣基督。结账他的 <a class="ae ku" href="http://seungjun-data-science.github.io" rel="noopener ugc nofollow" target="_blank"> <em class="nu">网站</em> </a> <em class="nu">！</em></p></div></div>    
</body>
</html>