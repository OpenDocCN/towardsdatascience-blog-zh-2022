<html>
<head>
<title>Random Forest for Lithology Classification From Well Log Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于测井数据岩性分类的随机森林</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/random-forest-for-lithology-classification-from-well-log-data-4a1380ef025b#2022-08-20">https://towardsdatascience.com/random-forest-for-lithology-classification-from-well-log-data-4a1380ef025b#2022-08-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="95f4" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习|岩石物理学</h2><div class=""/><div class=""><h2 id="3a21" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">用机器学习对地下进行分类</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/c085f6646f6a95190468f13606466a3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s7anbk4yj9CIv2fZAXCVVg.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">奈杰科希尔的照片:<a class="ae lh" href="https://www.pexels.com/photo/green-leafed-tree-338936/" rel="noopener ugc nofollow" target="_blank">https://www.pexels.com/photo/green-leafed-tree-338936/</a></p></figure><p id="54a9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">随机森林是一种非常流行的机器学习算法，可用于分类(我们试图预测类别或类)和回归(我们试图预测连续变量)。</p><p id="8d1c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在岩石物理学和地球科学中，我们可以使用这种算法根据测井测量预测岩性，或者根据岩心测量预测连续测井曲线，例如连续渗透率。</p><p id="9109" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在本文中，我们将重点关注随机森林在测井测量岩性预测中的应用。但首先，让我们看看随机森林是如何工作的。</p><h1 id="932d" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">随机森林分类简介</h1><p id="1397" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">在很高的层次上，随机森林本质上是决策树的集合。这些都是非常简单和直观的理解，我们经常在日常生活中使用它们，即使我们可能没有意识到。</p><p id="2d0d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这个简单的例子中，我们在散点图上有一系列双色(红色和橙色)的数据点。每个彩色系列代表一个不同的类别。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nb"><img src="../Images/adcea589e9a6a94599d6e2b9f9a59d72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CarG2Y7hnhFi-tUr-DNmlw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">一个简单的数据集被分成两类:橙色和蓝色。图片由作者提供。</p></figure><p id="96a1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们首先决定如何最好地分割我们的数据，并尝试分离类。这里我们选择x ≤ 3的值。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nc"><img src="../Images/82f37e2c01c17468fa0edf76e28230cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YzKZihT4XPxqYvq75vP6wA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">我们决策树中的第一个决策。散点图上的灰线代表我们的决策值3。然后，我们对此应用一个条件，并检查点数是否小于或等于3。图片由作者提供。</p></figure><p id="4945" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以看到，当我们这样做了，我们结束了两个橙色的点在左边，蓝色和橙色的混合点在右边。</p><p id="29c4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以根据许多其他条件继续分割数据，直到我们成功地分离出不同的类。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nc"><img src="../Images/355551aa10a5da17e495442c0ada0067.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OxNtHaCsz9EmYbHAJoL-Pg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">经过多次决策，我们终于有了一个完整的决策树，可以用来进行预测。图片由作者提供。</p></figure><p id="6c2e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如上所示，橙色点已经成功地从蓝色点中分离出来，我们有了最终的决策树。这现在形成了新数据的蓝图，新数据将遵循正确的路径，直到到达最终节点。</p><p id="5471" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，这个例子非常简单，我们只使用了二维空间，我们可以很容易地分离出类。然而，真实世界的数据可能是杂乱的，我们可能不会以纯粹的叶节点结束。我们可能有包含这两个类的节点，为了确定该节点的最终结果，我们可以使用多数投票的过程。这将在以后的文章中讨论。</p><p id="9b47" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们所做的是创建一个决策树。为了建立我们的森林，我们需要组合多个决策树。顾名思义，我们需要随机化它的一些部分。</p><p id="ed0a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们首先获取数据，并对其进行随机采样，这为我们提供了完整训练数据集的子集。这被称为<strong class="lk jd">引导</strong>我们的数据。</p><p id="b7f6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们还可以为每棵树随机选择不同的输入特征。</p><p id="d885" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这样会形成不同形状的树。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/1967ffe49db46a1cd35e6bce4ae20560.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2R7-1gX0iUqTA_Om.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">将多个决策树聚合在一起，以进行更可靠的预测。图片由作者提供。</p></figure><p id="54e6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，我们可以将新数据传递给这些树，这些数据将经过每个决策，并在最终节点结束，然后给出一个结果。</p><p id="ccc4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可能不会从所有的树中得到相同的结果，所以我们可以采取多数投票，换句话说，什么类被预测得最多，然后使用它作为我们的最终预测。</p><p id="bf6f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">组合多棵树的结果是一种强大的技术，可以为我们提供比单棵树更好的答案。这个过程被称为<strong class="lk jd">装袋</strong>。</p><p id="f629" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">既然我们对随机森林的作用有了基本的直觉，现在我们可以看看如何将它应用于一些测井数据进行岩性预测。</p><h1 id="5858" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">本教程中使用的数据</h1><p id="c7a6" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">我们在本教程中使用的数据集是一个训练数据集的子集，该数据集用作Xeek和FORCE 2020 <em class="ne"> (Bormann et al .，2020) </em>举办的机器学习竞赛的一部分。它是在挪威政府的NOLD 2.0许可下发布的，详细信息可以在这里找到:<a class="ae lh" href="https://data.norge.no/nlod/en/2.0/" rel="noopener ugc nofollow" target="_blank">挪威开放政府数据许可(NLOD) 2.0 </a>。</p><p id="85b7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">完整的数据集可通过以下链接获得:<a class="ae lh" href="https://doi.org/10.5281/zenodo.4351155" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.5281/zenodo.4351155</a>。</p><p id="e8e6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">竞赛的目的是利用测井测量从现有的标记数据预测岩性。完整的数据集包括来自挪威海的118口井。</p><h1 id="de8c" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">本教程的视频版本</h1><p id="75da" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">我还在我的YouTube频道上发布了这个教程的视频版本，它用另一个数据例子进行了更详细的描述。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nf ng l"/></div></figure><h1 id="2b4f" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">导入库和数据</h1><p id="d946" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">我们项目的第一步是导入我们将要使用的库。在本例中，我们将结合使用<a class="ae lh" href="http://pandas.pydata.org" rel="noopener ugc nofollow" target="_blank"> pandas </a>来加载和存储我们的数据，结合使用<a class="ae lh" href="https://matplotlib.org" rel="noopener ugc nofollow" target="_blank"> matplotlib </a>和<a class="ae lh" href="https://seaborn.pydata.org" rel="noopener ugc nofollow" target="_blank"> seaborn </a>来可视化我们的数据，并结合使用<a class="ae lh" href="https://github.com/ResidentMario/missingno" rel="noopener ugc nofollow" target="_blank"> missingno </a>来分析哪里可能有缺失值。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="e3d0" class="nm mf it ni b gy nn no l np nq">import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>import missingno as mno</span></pre><p id="35c5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，我们将使用<code class="fe nr ns nt ni b">pd.read_csv()</code>从CSV文件导入数据。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="c559" class="nm mf it ni b gy nn no l np nq">df = pd.read_csv('Data/Xeek_train_subset_clean.csv')</span></pre><p id="a540" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一旦数据被加载，我们可以检查我们的数据，看看我们有什么功能。</p><p id="a962" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用<code class="fe nr ns nt ni b">describe()</code>方法将允许我们查看数据集中数字特征的统计数据。我们很快会谈到分类特征。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="dafd" class="nm mf it ni b gy nn no l np nq">df.describe()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nu"><img src="../Images/ef85238ce5cbfb001983d38a6d0d4025.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hp8kraCz3feJTMBK8c1cwA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Xeek测井子集的内容。图片由作者提供。</p></figure><p id="3fc1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们得到的摘要为我们提供了数据的一般统计信息，包括最小值、最大值、百分位数等等。</p><p id="8b2b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们还可以使用<code class="fe nr ns nt ni b">.info()</code>方法来检查数据集中有多少非空值。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="f454" class="nm mf it ni b gy nn no l np nq">df.info()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/c81dfeedcdffbf1d4629afab45553091.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*UZY_dQbO9DU_aijU6n35Mw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">对测井数据集使用df.info()的结果。图片由作者提供。</p></figure><h2 id="8d42" class="nm mf it bd mg nw nx dn mk ny nz dp mo lr oa ob mq lv oc od ms lz oe of mu iz bi translated">快速查看岩性变量</h2><p id="60c6" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">在本教程中，我们将使用名为<code class="fe nr ns nt ni b">LITH</code>的特征，这是我们的地质岩性，用于训练和预测。</p><p id="ea5c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们首先检查一下在<code class="fe nr ns nt ni b">LITH</code>列中有多少唯一值。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="51ac" class="nm mf it ni b gy nn no l np nq">df['LITH'].nunique()</span></pre><p id="0b12" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">它返回:</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="e92b" class="nm mf it ni b gy nn no l np nq">11</span></pre><p id="2a44" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，我们将检查每个唯一值是什么:</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="8f00" class="nm mf it ni b gy nn no l np nq">df['LITH'].unique()</span></pre><p id="6878" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">它返回以下岩性列表:</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="7499" class="nm mf it ni b gy nn no l np nq">array(['Shale', 'Sandstone', 'Sandstone/Shale', 'Limestone', 'Tuff',<br/>       'Marl', 'Anhydrite', 'Dolomite', 'Chalk', 'Coal', 'Halite'],<br/>      dtype=object)</span></pre><p id="4378" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们可以使用<code class="fe nr ns nt ni b">value_counts()</code>方法收集每个岩性的数量。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="72e3" class="nm mf it ni b gy nn no l np nq">df['LITH'].value_counts()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/606b84492bb9ca66da79e02bbdd6f82d.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*iqiSetnZRD6RVwCKBqnSKQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">数据中的岩性计数。图片由作者提供。</p></figure><p id="fc9e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以看到，我们的数据集是不平衡的，在页岩、石灰岩和砂岩岩性中有更多的点。这是在进行预测时需要考虑的事情，但不在本文讨论范围之内。</p><h2 id="3595" class="nm mf it bd mg nw nx dn mk ny nz dp mo lr oa ob mq lv oc od ms lz oe of mu iz bi translated">可视化按岩性划分的数据</h2><p id="e6ac" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">由于我们的数据集中只有少量岩性，我们可以使用Seaborn的FacetGrid图来可视化两个变量的值分布:<code class="fe nr ns nt ni b">NPHI</code>和<code class="fe nr ns nt ni b">RHOB</code>。这两个特性通常用于岩石物理学，但所有其他列都可以替代使用。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="0490" class="nm mf it ni b gy nn no l np nq">g = sns.FacetGrid(df, col='LITH', col_wrap=4)<br/>g.map(sns.scatterplot, 'NPHI', 'RHOB', alpha=0.5)<br/>g.set(xlim=(-0.15, 1))<br/>g.set(ylim=(3, 1))</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oh"><img src="../Images/2b926be157b39627ca528733d23f708b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dnwiFHktjIEzSRcOAN5-6A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">FacetGrid显示了密度(RHOB) —中子孔隙度(NPHI)数据中的岩性分布。</p></figure><p id="ac05" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从上面可以看出，一些岩性具有较大的分布，如页岩数据，而硬石膏和石盐具有更明显的分布。</p><h1 id="63bb" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">处理缺失值</h1><p id="471a" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">在探索了我们的数据之后，下一步是处理我们缺失的值。可视化我们丢失的值的一个很好的方法是使用<a class="ae lh" href="https://github.com/ResidentMario/missingno" rel="noopener ugc nofollow" target="_blank"> missingno </a>库。</p><p id="49c7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在本教程中，我们将只使用条形图，但理想情况下，您应该查看所有图表，尝试并了解哪里以及为什么会有缺失值。</p><p id="ae5c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以像这样调用条形图。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="6879" class="nm mf it ni b gy nn no l np nq">mno.bar(df)</span></pre><p id="7dee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这将返回下面的图表。我们可以看到，与其他列(例如NPHI)相比，一些列的值的数量(嗯，DEPTH_MD)更大。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/9aeb58fe8b359f84aefbb0d539cc2e08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L2FWpkruKVcd0WW2GSS8Zw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">缺失没有显示数据集中缺失值总数的条形图。图片由作者提供。</p></figure><p id="b466" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了简单起见，我们将简单地使用列表删除来删除任何列中缺少值的行。还有更高级的方法，但是这个适合这个例子。</p><p id="a9f4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以使用<code class="fe nr ns nt ni b">df.dropna()</code>删除包含缺失值的行，如果我们设置参数<code class="fe nr ns nt ni b">inplace</code>等于<code class="fe nr ns nt ni b">True</code>，那么我们将删除原始数据帧中的行。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="a9a4" class="nm mf it ni b gy nn no l np nq">df.dropna(inplace=True)<br/>mno.bar(df)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/f74cc0bb92398533c5400497f713e0d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2dLnhNU3rHAdscITt_LmKQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">缺失使用pandas的dropna()方法删除缺失值后没有条形图。图片由作者提供。</p></figure><p id="889c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如上所示，我们已经将总价值计数从133，196减少到81，586，这是一个显著的减少。</p><h1 id="45c4" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">构建随机森林模型</h1><p id="a276" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">既然我们已经研究了数据并移除了缺失值，现在我们可以继续构建我们的随机森林分类模型。为此，我们需要从<a class="ae lh" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>库中导入一些模块。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="a4ec" class="nm mf it ni b gy nn no l np nq">from sklearn.model_selection import train_test_split<br/>from sklearn import metrics<br/>from sklearn.metrics import classification_report, confusion_matrix<br/>from sklearn.ensemble import RandomForestClassifier</span></pre><p id="184d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，我们需要将我们的数据分成哪些变量将用于训练模型(X)和我们的目标变量<code class="fe nr ns nt ni b">LITH</code> (y)。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="c481" class="nm mf it ni b gy nn no l np nq">X = df[['RDEP', 'RHOB', 'GR', 'NPHI', 'PEF', 'DTC']]<br/>y = df['LITH']</span></pre><p id="4b92" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">与任何机器学习模型一样，我们需要一个用来训练模型的数据集和一个用来测试和验证我们的模型是否有效的数据集。这可以通过使用<code class="fe nr ns nt ni b">train_test_split()</code>功能轻松实现。</p><p id="2d38" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们传入X和y变量，以及表示我们想要多大的测试数据集的参数。这是作为十进制值输入的，范围在0和1之间。在这种情况下，我们使用0.3，这意味着我们的测试数据集将是原始数据的30%，我们的训练数据集将是原始数据的70%。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="986f" class="nm mf it ni b gy nn no l np nq">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)</span></pre><p id="adec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">设置我们的分类器非常简单，我们创建一个名为<code class="fe nr ns nt ni b">clf</code>的变量，并将其赋给<code class="fe nr ns nt ni b">RandomForestClassifier</code>。我们也可以为我们的分类器传入参数，但是在这个例子中，我们将保持简单并使用默认值。</p><p id="4f3f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你可以在这里找到更多相关信息。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="f8c3" class="nm mf it ni b gy nn no l np nq">clf=RandomForestClassifier()</span></pre><p id="63d4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一旦模型被初始化，我们就可以通过调用<code class="fe nr ns nt ni b">fit</code>方法并传入<code class="fe nr ns nt ni b">X_train</code>和<code class="fe nr ns nt ni b">y_train</code>将它应用于我们的训练数据。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="5e10" class="nm mf it ni b gy nn no l np nq">clf.fit(X_train,y_train)</span></pre><h1 id="d08c" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">利用随机森林分类器预测岩性</h1><p id="99e4" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">在模型建立之后，我们可以对我们的测试数据进行预测，并评估它的表现如何。这是通过调用<code class="fe nr ns nt ni b">predict</code>方法并传入我们的测试数据集特性<code class="fe nr ns nt ni b">X_test</code>来完成的。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="5bf2" class="nm mf it ni b gy nn no l np nq">y_pred=clf.predict(X_test)</span></pre><p id="c2a4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一旦模型被用于预测，我们就可以开始查看一些关键指标。第一个是准确性，即每类的正确预测数除以预测总数。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="3b40" class="nm mf it ni b gy nn no l np nq">accuracy=metrics.accuracy_score(y_test, y_pred)<br/>accuracy</span></pre><p id="eb89" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">它返回:</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="6f05" class="nm mf it ni b gy nn no l np nq">0.91203628043798</span></pre><p id="241a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这是一个很好的值，但是，它不应该完全单独使用，因为它可能会对模型的实际性能产生错误的感觉。</p><p id="6255" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了获得更好的见解，我们可以生成一个分类报告，如下所示。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="a9c6" class="nm mf it ni b gy nn no l np nq">clf_rpt = classification_report(y_test, y_pred)<br/>print(clf_rpt)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/8c9ef5ef0bcbad5fa230c26a4da60036.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*OcwcBrjQEM8mNe6hoZ9KIw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">scikit-learn的分类报告。图片由作者提供。</p></figure><p id="adae" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> precision: </strong>提供了该类中有多少值被正确预测的指示。值介于0.0和1.0之间，1表示最好，0表示最差。</p><p id="46f1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> recall: </strong>提供了分类器能够找到该类的所有阳性案例的程度的度量。</p><p id="5231" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> f1-score: </strong>精度和召回率的加权调和平均值，生成1.0(好)和0.0(差)之间的值。</p><p id="e051" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> support: </strong>这是数据集中该类的实例总数。</p><h2 id="f8e9" class="nm mf it bd mg nw nx dn mk ny nz dp mo lr oa ob mq lv oc od ms lz oe of mu iz bi translated">混淆矩阵</h2><p id="05b8" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">另一种观察我们的模型表现如何的方法是使用混淆矩阵。这用于根据提供的标签的实际值绘制每个特征的预测结果(<code class="fe nr ns nt ni b">LITH</code>)。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="21de" class="nm mf it ni b gy nn no l np nq">cf_matrix = confusion_matrix(y_test, y_pred)<br/>cf_matrix</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/0633b33c5f759b76788aee922d330f91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*tlt68oQp6EjXrTfM9XWASQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">scikit-learn生成的混淆矩阵的原始示例。图片由作者提供。</p></figure><p id="e822" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以使用seaborn和热图绘制一个漂亮且易于理解的图表，而不是使用数字来可视化我们的混淆矩阵。首先，我们必须设置希望出现在x轴和y轴上的标签。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="c165" class="nm mf it ni b gy nn no l np nq">labels = ['Shale', 'Sandstone', 'Sandstone/Shale', 'Limestone', 'Tuff',<br/>       'Marl', 'Anhydrite', 'Dolomite', 'Chalk', 'Coal', 'Halite']<br/>labels.sort()</span></pre><p id="1090" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后我们可以创建我们的图形，并添加一些标题和轴标签。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="85b9" class="nm mf it ni b gy nn no l np nq">fig = plt.figure(figsize=(10,10))<br/>ax = sns.heatmap(cf_matrix, annot=True, cmap='Reds', fmt='.0f',<br/>                xticklabels=labels, <br/>                yticklabels = labels)</span><span id="02d2" class="nm mf it ni b gy om no l np nq">ax.set_title('Seaborn Confusion Matrix with labels\n\n');<br/>ax.set_xlabel('\nPredicted Values')<br/>ax.set_ylabel('Actual Values ');</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/abafa20c45fb0ef719902291a5b5a3a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d9Q6rmDMGPwIeJs05DYMpw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">我们的分类结果的混淆矩阵是使用Seaborn热图生成的。图片由作者提供。</p></figure><p id="0f3b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用我们混乱矩阵的可视化比看简单的数字要好得多。我们可以看到，数值越高，红色越深。因此，我们可以立即看到，我们的大多数数据点都属于页岩类别，这也表明这些值预测正确。</p><h1 id="4cd2" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">摘要</h1><p id="4a5b" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">随机森林是一种功能强大、易于理解、易于实现的机器学习方法。如上所述，它在岩性预测的第一次通过中提供了非常好的结果，但是，我们仍然需要在看不见的数据上验证我们的模型并调整参数。</p></div><div class="ab cl oo op hx oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="im in io ip iq"><p id="96e3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="ne">感谢阅读。在你走之前，你一定要订阅我的内容，把我的文章放到你的收件箱里。</em> <a class="ae lh" href="https://andymcdonaldgeo.medium.com/subscribe" rel="noopener"> <strong class="lk jd"> <em class="ne">你可以在这里做！</em></strong></a><strong class="lk jd"><em class="ne"/></strong><em class="ne">或者，您也可以</em> <a class="ae lh" href="https://fabulous-founder-2965.ck.page/2ca286e572" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> <em class="ne">注册我的简讯</em> </strong> </a> <em class="ne">免费将更多内容直接发送到您的收件箱。</em></p><p id="5d6a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其次，通过注册会员，你可以获得完整的媒介体验，并支持我和其他成千上万的作家。它每个月只花你5美元，你可以完全接触到所有令人惊叹的媒体文章，也有机会用你的写作赚钱。如果你用 <a class="ae lh" href="https://andymcdonaldgeo.medium.com/membership" rel="noopener"> <strong class="lk jd"> <em class="ne">我的链接</em></strong></a><strong class="lk jd"><em class="ne"/></strong><em class="ne">报名，你直接用你的一部分费用支持我，不会多花你多少钱。如果你这样做了，非常感谢你的支持！</em></p><h1 id="c95e" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">参考</h1><p id="f7e6" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">博尔曼，彼得，奥桑德，彼得，迪里布，法哈德，曼拉尔，投降，&amp;迪辛顿，彼得。(2020).机器学习竞赛FORCE 2020井测井和岩相数据集[数据集]。芝诺多。<a class="ae lh" href="http://doi.org/10.5281/zenodo.4351156" rel="noopener ugc nofollow" target="_blank">http://doi.org/10.5281/zenodo.4351156</a></p></div></div>    
</body>
</html>