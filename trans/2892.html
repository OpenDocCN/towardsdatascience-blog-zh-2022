<html>
<head>
<title>Seven Exploration Strategies In Reinforcement Learning You Should Know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你应该知道的强化学习中的七个探索策略</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/seven-exploration-strategies-in-reinforcement-learning-you-should-know-8eca7dec503b#2022-06-23">https://towardsdatascience.com/seven-exploration-strategies-in-reinforcement-learning-you-should-know-8eca7dec503b#2022-06-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0e56" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">纯探索和开发，ϵ-greedy，玻尔兹曼探索，乐观初始化，置信区间，知识梯度</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/dfee6f655430205f5040a06057728d6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ae-eOAY5PmwFfASD"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">霍利·曼达里奇在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="a87a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">探索-开发困境</strong>在日常生活中无处不在。一旦你找到一家你喜欢的餐馆，你可能会决定下半辈子都去同一家餐馆，利用你积极的经历。然而，探索新的场地也有一定的吸引力。是的，你可能会有一些痛苦的经历，但你也可能会找到一个新的最爱！在探索和利用之间找到适当的平衡至关重要。</p><p id="9f01" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在强化学习中，困境同样重要。对于中等现实的问题，问题的规模是巨大的，计算是昂贵的(如果不是，我们可以只是列举)。因此，我们想要学习好状态的精确值，而不是在低质量的状态上浪费宝贵的(计算)预算。与此同时，过于狭窄的焦点可能会让我们陷入局部最优。</p><p id="761c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们可以把这个问题形式化如下。给定某个<strong class="li iu">度量预算</strong>，我们的目标是以这样一种方式分配我们的度量，即我们识别最显著的<a class="ae ky" rel="noopener" target="_blank" href="/a-deep-dive-into-problem-states-498ad0746c98">状态</a>。根据大数定律，通过无限多的样本，我们将逼近一个状态的真实值。然而，有限的测量预算不能完全解决不确定性，样本均值可能偏离真实均值。换句话说，我们不确定我们是否真正确定了具有最高值的州。</p><p id="252c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="mc"> *在这个上下文中，一个测量值是决定样品的一个特定状态。这个概念可以很容易地扩展到(顺序)动作选择问题，例如在Q-learning和SARSA这样的算法中遇到的问题。</em></p><p id="5e63" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">出于本文的考虑，让我们假设状态空间是可枚举的，并且每个<strong class="li iu">度量都有一个显著的成本</strong>(计算的或金钱的)。想一想在你的<a class="ae ky" rel="noopener" target="_blank" href="/need-help-making-decisions-ask-your-digital-twin-6e4cf328cb0">数字孪生</a>环境中的冗长模拟或者<a class="ae ky" rel="noopener" target="_blank" href="/five-things-to-consider-for-reinforcement-learning-in-business-environments-23647a32f01f">真实世界</a>观察——足以明智地选择你的测量。</p><p id="043c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这个问题可以想象如下。假设我们从一个噪声函数中采样，状态<em class="mc"> x∈X </em>的测量值**在真实平均值<em class="mc"> μ_x </em>附近有一些不确定性(即标准偏差<em class="mc"> σ_x </em>)。通过绘制测量值，我们能够<strong class="li iu">构建样本均值</strong>——在本文中用a ~表示——最终应该收敛到真实均值。然而，有限的测量预算意味着我们无法详尽地测量每个州。</p><p id="5fe0" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="mc"> **为简单起见，我们假设可以直接确定要测量的状态。将本文中的策略扩展到顺序决策问题是很简单的，在顺序决策问题中，我们选择引导我们进入新状态的行动。</em></p><p id="a8dc" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">够了<em class="mc">解释</em>，让我们转移到<em class="mc">探索</em>攻略！</p><h1 id="df38" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">一.纯粹的剥削</h1><p id="aa08" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">在像<a class="ae ky" rel="noopener" target="_blank" href="/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff"> Q-learning和SARSA </a>这样的算法中，你通常会在所有Q值上看到一些最大化器。在这个简化的问题设置中，我们等效地选择具有<strong class="li iu">最高期望值</strong>的状态:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/86f9bff9217a112de97b283d6aa239b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*WsM7bkqkld694kKEdcN47w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">纯粹的剥削总是选择最好的已知状态。由于最初的估计往往很差，所以很容易陷入局部最优。</p></figure><p id="82b6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这个表达式暗示我们选择最好的状态，<strong class="li iu">给定我们当前估计的状态值</strong>。尤其是在早期，这些估计可能非常糟糕。在没有先验知识的情况下，我们甚至可能简单地将状态值初始化为0或另一个任意数。</p><p id="1639" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">考虑到利用的本质，值<strong class="li iu">的初始化非常重要</strong>(参见策略四)。假设你用零初始化你的表。然后，一个积极的观察就足以产生积极的回报估计，并观察相同的选项，直到时间结束。</p><p id="c90f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">即使我们在开发之前强制观察每个状态至少一次，最初的观察结果也可能与真实值相差甚远。因此，纯粹的开发倾向于对很少的状态给出非常精确的估计，通常会快速收敛到局部最优<strong class="li iu">而忽略状态空间的大部分。</strong></p><h1 id="2941" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">二。天真的探索</h1><p id="dc0c" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">纯粹探索的两极对立是完全随机地探索状态。我们给每个状态分配一个<strong class="li iu">相等的概率，并相应地采样:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/07b7b61f7948a38b8ea465c0126a128f.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*rjR7jJbi-sKmtLAlN5CeHg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">天真探索随机选择一个州，对每个州赋予相等的概率。因为它没有考虑我们所学的值，所以样本效率很低。</p></figure><p id="33a0" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这听起来很平均主义，但有一个问题。我们没有以任何方式考虑我们对状态值的<strong class="li iu">知识，这意味着我们可能对有希望的状态进行欠采样，而对差的状态进行过采样。通过很多很多的观察，这个策略应该是可行的，但是正如我们之前提到的——测量通常是昂贵的。</strong></p><h1 id="ce76" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">三。ε-贪婪的</h1><p id="01d1" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">前两种策略有明显的警告，但放在一起，它们往往会产生相当不错的结果。概率ϵ(通常为5%或10%)和使用剥削剩余的1-ϵ时间。因此，我们将大部分计算预算用于改进对有希望的测量的估计，同时加入<strong class="li iu">偶然随机测量</strong>以避免局部最优。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/d6a6459efb2502c51c7739a5f21eebfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Wx-vtm8RFb3XAi8_3Icrw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">ϵ-greedy方法在开发(通常是大部分时间)和利用之间进行平衡，不时地进行随机测量并避免局部最优。</p></figure><p id="a2a2" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">ϵ-greedy通常是RL实现的<strong class="li iu">默认度量策略</strong>，克服了纯探索和开发中的明显警告。ϵ的背景需要一些考虑，因为它决定了勘探和开发之间的平衡。我们也可以使它依赖于时间(例如，类似于<em class="mc"> ϵ_n=1/n </em>的东西)，在开始时产生一个随着测量次数减少的高探测率。谐波步长允许对时间行为进行更多控制，但代价是需要调整另一个参数。如果测量值很便宜，ε-贪婪政策通常可以很好地发挥作用，否则就是一种粗糙的方法。</p><h1 id="7b62" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">四。乐观初始化</h1><p id="40d9" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">假设我们不用0来初始化Q值，而是用+∞来代替。第一个观察用样本平均值代替了Q值，使其对未观察到的状态相对没有吸引力。因此，乐观初始化<strong class="li iu">强制每个状态至少被观察一次</strong>，因为它总是具有更高的预期回报。自然，初始值可以设置得更低(比如10，000，或者任何适合问题的值)来稍微缓和一下乐观情绪。</p><p id="cebb" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">初始化过程本身并不是一个测量策略，但是它会对其他策略的行为产生实质性的影响。如前所述，一个观察值可能不能很好地代表真实的平均值。</p><h1 id="5694" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">动词 （verb的缩写）玻尔兹曼探索</h1><p id="026f" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">ϵ-greedy的方法平衡了探索和开发，但是在探索的时候是随机的。我们应该<strong class="li iu">更频繁地测量“有希望的”状态</strong>(即，具有更高的期望值)，这样才不会在低估计值的状态上浪费我们的测量值，而这些状态成为未经雕琢的钻石的可能性极小。</p><p id="83d2" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">Boltzmann exploration提供了一种优雅的方式，以与我们的期望成比例的方式来集中我们的测量预算。对于每个状态，它使用<strong class="li iu"> softmax函数</strong>定义一个采样概率:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/7d952ceb11234c62ccb40d2d6a22cb9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*jNND_S3pksHd838XE1ONwA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Boltzmann exploration利用sofmax函数来确定对每个状态进行采样的概率，返回与样本均值成比例的概率</p></figure><blockquote class="ne nf ng"><p id="d12a" class="lg lh mc li b lj lk ju ll lm ln jx lo nh lq lr ls ni lu lv lw nj ly lz ma mb im bi translated">对于那些熟悉<a class="ae ky" rel="noopener" target="_blank" href="/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7">离散</a> <a class="ae ky" rel="noopener" target="_blank" href="/policy-gradients-in-reinforcement-learning-explained-ecec7df94245">政策梯度算法</a>的人来说，请注意，这里使用了相同的sofmax机制，根据预期回报为每个行动附加一个概率。</p></blockquote><p id="c77e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">温度<em class="mc"> ρ </em>可以固定在适合问题的水平，但是我们也可以根据执行的测量次数来改变它。通过逐渐降低温度，我们<strong class="li iu">最初鼓励勘探</strong>，随着时间的推移，倾向于开采。</p><p id="b89a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">玻尔兹曼方法微妙地平衡了勘探和开发。我们仍然允许采样避开局部最优，但是也考虑我们收集的知识来集中我们的测量。然而，一个主要的缺点是，在能够计算分母之前，我们必须<strong class="li iu">评估所有状态</strong>至少一次，这可能需要大量的工作。此外，温度<em class="mc"> ρ </em>引入了一个必须调整的额外参数。</p><h1 id="1eb1" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">不及物动词乐观置信区间</h1><p id="3091" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">到目前为止，我们只考虑了期望值的估计，而没有考虑这些估计中的<strong class="li iu">不确定性。如果您有一些模拟经验，您可能知道——遵循大数定律——均值的置信区间(即标准误差)随着观察次数的增加而减少。使用乐观置信区间，我们选择具有最高上限的状态:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/169393d74f5486decdc04c015ea128f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*opUQig1I3t7hRN4IOp1tHQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">当使用乐观置信区间时，我们选择置信区间上限最高的状态。随着间隔随着更多的观察而缩短，该策略鼓励访问具有高度不确定性的国家。</p></figure><p id="3a8f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">例如，如果我们假设观察值是正态分布的，我们可以设置一个包含95%概率的真实平均值的区间。我们进行的测量越多，间隔就变得越紧(注意，<em class="mc">样本</em>的方差没有减少，只是<em class="mc">均值</em>的方差减少)。</p><p id="2fec" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在测量一个状态至少两次后，我们可以计算它的标准偏差，从而得到它的平均值的<strong class="li iu">置信区间</strong>。真实的均值可能在区间内的任何地方(同样:假设分布假设成立，并且具有给定的概率)，那么为什么不考虑它可能在区间的顶部呢？</p><p id="dd63" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这正是<strong class="li iu">乐观置信区间</strong>所反映的。我们不是根据预期的平均值来选择州，而是根据平均值的第95个四分位数。实际上，我们选择了一种形式的上界，一种最好的情况。因此，我们的估计的不确定性被明确地考虑在内，当更频繁地测量一个状态时，间隔缩小。</p><p id="15c4" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">置信区间要求跟踪一个额外的变量(<strong class="li iu">标准偏差</strong>)并测量每个状态至少两次，但是在我们的决策中明确整合测量不确定性的吸引力是显而易见的。虽然这种方法通常工作得很好，但是术语<em class="mc"> z_α/2 </em>没有特别的含义，必须由用户来调整。该方法也不能保证收敛。让我们看看我们是否能在那上面展开一点。</p><h1 id="2326" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">七。知识梯度</h1><p id="9a9a" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">跟踪不确定性的概念可以通过形式化测量状态的<strong class="li iu">临界值</strong>来扩展。假设我们还有一个测量要做，我们应该采样哪个状态来最大化我们的学习效果？这是知识梯度法回答的问题。在极限情况下，这种方法收敛到最佳状态。</p><p id="4dc4" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">就本文的目的而言，深入讨论知识梯度太过牵强。它植根于<strong class="li iu">贝叶斯统计</strong>，其中我们假设状态的先验分布，并根据每次观察更新它们。在这种观点下，状态的均值和标准差是需要学习的随机变量。</p><p id="e661" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">类似于前述的置信区间，测量可能改变样本均值，使得状态成为<strong class="li iu">新的最佳选项</strong>。在那种情况下，我们实际上<em class="mc">学到了一些东西——我们进行了一项测量，改变了我们的决策。</em></p><p id="6b3f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">下图显示了<strong class="li iu">知识梯度</strong>的概念。在纯粹剥削的情况下，我们总是选择国家A，因为它有较高的样本均值。然而，假设我们的下一个测量来自状态B的阴影区域；其样本平均值将成为最高值。由于我们知道分布情况，我们可以给抽取的样本附加一个概率。这正是驱动知识梯度的机制。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/cd9376202c52702ba0f827e034ee8e6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hEu_UmrNxBW4VU2-rwls0g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">知识梯度算法图解。对于每个状态，该算法考虑测量将改变样本均值的样本的概率，使得它成为新的最佳状态。算法测量知识梯度最高的状态[图片由作者提供]</p></figure><p id="85ca" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这个过程本身并不一定很难实现，但是它确实需要相当多的符号——对于这篇概述文章来说太多了。我希望很快在一篇独立的文章中介绍它，现在可以参考鲍威尔T34弗雷泽的优秀教程。</p><p id="b61d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">现在跳过实际的数学，总体思想的轮廓如下:</p><ul class=""><li id="b1b3" class="nm nn it li b lj lk lm ln lp no lt np lx nq mb nr ns nt nu bi translated">跟踪观测值的<strong class="li iu">样本均值</strong>和<strong class="li iu">样本方差</strong>，构建贝叶斯模型</li><li id="54f0" class="nm nn it li b lj nv lm nw lp nx lt ny lx nz mb nr ns nt nu bi translated">计算表示测量状态 x相对于所有其他状态<em class="mc"> x'≠x </em>的相对值的度量</li><li id="24ea" class="nm nn it li b lj nv lm nw lp nx lt ny lx nz mb nr ns nt nu bi translated">将指标插入<strong class="li iu">累积密度函数</strong>和<strong class="li iu">概率密度函数</strong>，捕捉测量值将产生所有状态的最佳平均值的分布部分</li><li id="0a36" class="nm nn it li b lj nv lm nw lp nx lt ny lx nz mb nr ns nt nu bi translated">在此基础上，计算知识梯度以反映测量状态 <em class="mc"> x </em>的<strong class="li iu">边缘值</strong></li><li id="b124" class="nm nn it li b lj nv lm nw lp nx lt ny lx nz mb nr ns nt nu bi translated">选择知识梯度最高的州</li></ul><p id="e1a5" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">虽然这份名单上最复杂的，但它显然也是最不直观和最复杂的。如果测量预算真的是一个问题，它可能值得麻烦，否则更简单的方法可能就足够了。</p><p id="9337" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">下次您实现RL算法时，不要盲目采用标准的ε贪婪方法，而是考虑探索一些替代策略！</p><h1 id="2a98" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">TL；速度三角形定位法(dead reckoning)</h1><ul class=""><li id="b16c" class="nm nn it li b lj mv lm mw lp oa lt ob lx oc mb nr ns nt nu bi translated"><strong class="li iu">纯剥削</strong> —始终选择样本均值最高的州。</li><li id="115d" class="nm nn it li b lj nv lm nw lp nx lt ny lx nz mb nr ns nt nu bi translated"><strong class="li iu">纯探索</strong> —始终选择随机状态。</li><li id="800f" class="nm nn it li b lj nv lm nw lp nx lt ny lx nz mb nr ns nt nu bi translated"><strong class="li iu">ε-贪婪</strong>——纯粹的剥削，但是选择一个随机的行动(探索)带有一些概率ϵ.</li><li id="e900" class="nm nn it li b lj nv lm nw lp nx lt ny lx nz mb nr ns nt nu bi translated"><strong class="li iu">乐观初始化</strong> —以鼓励或强制探索的方式初始化未知状态的值，至少观察每个状态一次。</li><li id="4786" class="nm nn it li b lj nv lm nw lp nx lt ny lx nz mb nr ns nt nu bi translated"><strong class="li iu">玻尔兹曼探测</strong> —样本状态与其估计值成比例，针对可能随时间降低的探测温度进行校正。</li><li id="c44e" class="nm nn it li b lj nv lm nw lp nx lt ny lx nz mb nr ns nt nu bi translated"><strong class="li iu">乐观置信区间</strong> —将样本均值置信区间的上限作为确定测量值的代理，乐观地考虑测量值的不确定性。</li><li id="8010" class="nm nn it li b lj nv lm nw lp nx lt ny lx nz mb nr ns nt nu bi translated"><strong class="li iu">知识梯度</strong> —假设测量值的先验分布，并随时间更新。选择具有最高边际信息值的状态，即成为新的最佳状态的最高概率。</li></ul><h1 id="5162" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">参考</h1><p id="c277" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">鲍威尔。W. B .和Frazier，P. (2008年)。<em class="mc">优学。</em>运筹学教程。可用<a class="ae ky" href="https://pubsonline.informs.org/doi/epdf/10.1287/educ.1080.0039" rel="noopener ugc nofollow" target="_blank">在线</a>。</p></div></div>    
</body>
</html>