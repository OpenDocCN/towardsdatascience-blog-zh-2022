<html>
<head>
<title>Causal SHAP values: A possible improvement of SHAP values</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">因果SHAP值:SHAP值的一种可能的改进</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/casual-shap-values-a-possible-improvement-of-shap-values-4d4d62925b71#2022-09-02">https://towardsdatascience.com/casual-shap-values-a-possible-improvement-of-shap-values-4d4d62925b71#2022-09-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9ea1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">介绍和案例研究</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/400db3e5f9a2a34268ba823bd71eeed3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o3MBRr2wGclPs_AN-AkoEg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/photos/i--IN3cvEjg" rel="noopener ugc nofollow" target="_blank">图片由埃文·丹尼斯拍摄</a></p></figure><p id="48ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我在之前的<a class="ae kv" rel="noopener" target="_blank" href="/why-shap-values-might-not-be-perfect-cbc8056056be">帖子</a>中解释的那样，广泛用于机器学习可解释性的SHAP价值观框架不幸未能反映其结果中的因果结构。研究人员一直在提出可能的解决方案来消除这种限制。在这篇文章中，我将回顾其中一个被提议的选择，因果SHAP值(CSV)，并给出一个简单的例子和详细的计算来说明CSV和“传统的”SHAP值之间的区别。</p><h1 id="16be" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">传统SHAP价值观及其局限性</h1><p id="b65d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">让我们首先回顾一下SHAP值的定义，这是一种基于合作博弈论的方法，旨在解释一种机器学习模型，将特征重要性归因于特征对最终输出所做贡献的回报。对于给定的模型f，考虑j∈{1，2，…n}:=N的特征x_j的SHAP值由下式给出</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/8832a3e107dc8f00f61b3eaa80446846.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/1*9e3xSwzBsBsf3smD1tzCmQ.gif"/></div></figure><p id="ad43" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中π是选择为均匀分布的N和符号</p><p id="4166" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">We can notice immediately that the above computation is based on a marginal expectation, which, explains why SHAP values cannot reflect the causal relation of the model.</p><h1 id="bd2b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">CSVs: a possible improvement of SHAP</h1><h2 id="be41" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">Definition of CSVs</h2><p id="b06d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">Consider a very simple case where feature 1 is the direct cause of feature 2 and we would like to argue that we cannot give the two features the same weight when evaluating their contributions to the output of the model. By intuition, the cause should be considered more important than the effect when attributing feature importance. Such intuition leads to the causal SHAP values (CSVs) framework, proposed by <a class="ae kv" href="https://arxiv.org/abs/2011.01625" rel="noopener ugc nofollow" target="_blank"> Heskes等人</a>的排列，旨在修改当前的SHAP值框架，而不破坏其期望的属性，即效率、对称性、虚拟性和可加性。</p><p id="f870" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">CSV具有以下定义:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/0c4c2f480861485375d9d6498374383b.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/1*2skqLqpp-JOm5dYMUxwoiw.gif"/></div></figure><p id="a2b6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们立即注意到与前一个定义的不同(根据作者的偏好，达到一个常数):边际期望被由<a class="ae kv" href="https://plato.stanford.edu/entries/causal-models/do-calculus.html" rel="noopener ugc nofollow" target="_blank"> Pearl的do演算</a>实现的介入期望所取代。对于那些不熟悉这个概念的人，只需注意do(。)是一个数学运算符，允许我们在一个随意的模型中进行干预，通常由有向无环图(DAG)表示，这个“do”可以从字面上理解为“do it”。基于CSV的定义，让我们回头看看上面的例子:做特征1，特征2的值不再是自由的，并且因果将被介入期望考虑。</p><h2 id="19d5" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">与SHAP的区别</h2><p id="700e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">省略计算的细节，我们想提到的是，特征x_i的CSV可以分解成两部分:直接贡献和间接贡献。当特征变量X_i取特征x_i的值时，直接贡献是预测的变化，而间接贡献是由于干预do(X_i=x_i)引起的变化。从这个意义上说，CSV与传统CSV的区别在于这种间接贡献的额外信息。</p><p id="042a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下图给出了自行车租赁模型中的SHAP值和CSV:以一年中的时间和温度的cos值作为输入，模型预测自行车租赁计数。我们可以看到，在考虑一年中的时间对温度的影响时，CSV更重视cos_year值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/89d87b31fa1cf6fa4264231b4eaee0d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*ApYCO8CBnl4dy5TNrOFOoA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由<a class="ae kv" href="https://arxiv.org/abs/2011.01625" rel="noopener ugc nofollow" target="_blank">赫斯克斯等人</a>拍摄:自行车租赁模式的SHAP价值观</p></figure><p id="d7a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了结束这一部分，让我们继续，通过<strong class="ky ir">用介入期望</strong>代替模型的边际期望，使得特征的<strong class="ky ir">间接贡献与其直接贡献一起被考虑</strong>，偶然SHAP值可以被视为当前SHAP值的概括。</p><h1 id="9112" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">CSV的具体示例</h1><p id="f3a3" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">让我们通过考虑以下只有三个变量的简单模型来说明CSV是如何工作的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/b2a5e178640424e7627273b44393c61c.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/1*isN-nG29iwdklMQk2Bd4gQ.gif"/></div></figure><p id="e521" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们假设变量遵循由下面的DAG给出的偶然模型，其中我们假设x_1是x_2的父，x_3独立于其他两个。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/910feadd80a747ef040dd49c282c9518.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*4UP7STdlfaUL4jRNriOYVw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片:休闲模特</p></figure><p id="400b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，我们假设:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/b23e9ce731314e42f2ce3ea13b81dc3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/1*jwH2rtGhqhaLwXCgTxW20Q.gif"/></div></figure><p id="8da6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，很容易得到所有特征的SHAP值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/9ba004aebdd7c4444016053f03b9b921.png" data-original-src="https://miro.medium.com/v2/resize:fit:252/1*lPwpaBC9CMohNr0b2RN1fw.gif"/></div></figure><p id="9d1e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们现在转向CSV。以第一个特征为例。根据定义，CSVs被给定为{1，2，3}的所有排列的介入期望的总和。作为例子，我们将计算φ_ 1(1，2，3)。将线性函数代入φ的定义，我们得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/8b47b4cd7b863a0bbb221750c8492f14.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/1*8YNobLGYVobdaY4n9plmuw.gif"/></div></figure><p id="4cd2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据偶然模型，我们有E[X_2|do(x_1)]=α_1x_1，而E[X_3|do(x_1)]=E[X_3]这导致:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/736cbf65063b6d86412f981039a5931b.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/1*BkXkaFBQirylJJ-yTB798A.gif"/></div></figure><p id="af09" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">类似的计算给出了所有三个特征的CSV:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/65e0fa7e0db46793cad18fa39ec733ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/1*1tq-Q2T-65mOzjwNkkp5ng.gif"/></div></figure><p id="329c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们从结果中观察到，当模型具有一定的因果结构时，CSV最终具有不同的特征属性。由于在上面所示的DAG图中，X_1是X_2的父节点，所以当我们从x_2的SHAP值中移除一项时，由于它是由x_1而不是x_2本身引起的，所以在x_1的值中增加了一项作为其间接影响。</p><h1 id="c5e1" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">CSV的局限性</h1><p id="1045" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们最后想说，如果用CSV取代SHAP还为时过早。尽管设计巧妙，但CSV的局限性也很明显。总的来说，我们数据科学家对SHAP解释任何黑盒模型的能力感到兴奋，这些黑盒模型的随意结构在实践中很难获得，如果没有这些信息，我们将永远无法对do-calculus进行很好的估计。然而，即使只有一部分偶然的信息可用，它仍然有助于更好地理解模型。</p></div></div>    
</body>
</html>