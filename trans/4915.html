<html>
<head>
<title>Creating Your Own Logistic Regression Model from Scratch in R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在R中从头开始创建自己的逻辑回归模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-your-own-logistic-regression-model-from-scratch-in-r-ce719a49e10b#2022-11-01">https://towardsdatascience.com/creating-your-own-logistic-regression-model-from-scratch-in-r-ce719a49e10b#2022-11-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7a58" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在没有外部包的情况下用R构建二进制分类模型的初学者指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/bb372b594260dd92e5f9fe6a21ff40f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KeB2Wsj79Ac92UyO"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">米利安·耶西耶在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><blockquote class="kz la lb"><p id="3545" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这篇文章着重于从头开始开发一个逻辑回归模型。我们将使用虚拟数据来研究众所周知的判别模型(即逻辑回归)的性能，并随着数据量的增加反映典型判别模型的学习曲线的行为。数据集可以在这里找到<a class="ae ky" href="https://github.com/angeleastbengal/Data-Collection" rel="noopener ugc nofollow" target="_blank">。请注意，该数据是使用随机数生成器创建的，用于在概念上训练模型。</a></p></blockquote><h1 id="3c91" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">介绍</h1><p id="f06b" class="pw-post-body-paragraph lc ld it lf b lg mr ju li lj ms jx ll mt mu lo lp mv mw ls lt mx my lw lx ly im bi translated">逻辑回归直接将目标变量y对输入x的预测建模为定义为<strong class="lf iu"> p(y|x) </strong>的条件概率。与线性回归模型相比，在逻辑回归中，目标值通常被限制为0到1之间的值；我们需要使用一个激活函数(sigmoid)将我们的预测转换成一个有界值。</p><p id="0c9f" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">假设函数sigmoid应用于数据的线性函数时，将其转换为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/9ba827f28054ad6d611192f9c88cb663.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*wXPxgPS13EYPpNkHS5qJKA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd na">方程式1。</strong>说明了应用于线性函数的sigmoid变换。作者使用Markdown &amp; Latex制作的图片。</p></figure><p id="174e" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">我们现在可以将类别概率建模为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/88bf6edd4973f1dfa5fa037a25270c9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*cJ7ek8r1OjpgLdqIqyt7nA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd na">方程式2。</strong>使用逻辑函数说明了类别概率C。图片由作者使用Markdown &amp;乳胶制作。</p></figure><p id="43ab" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">我们现在可以将类别概率C=1或C=0建模为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/b459b91e53570ddd6c1fcc0627181a5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Sa09ekbtEw5VEAXtEcwDw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd na">方程式3。</strong>使用逻辑函数，说明类别概率C=1|X和C=0|X。图片由作者使用Markdown &amp; Latex制作。</p></figure><p id="b9d8" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">逻辑回归有一个线性决策边界；因此，使用最大似然函数，我们可以确定模型参数，即权重。<strong class="lf iu">注P(C|x) = y(x)，为简单起见，记为y’。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/0ed310933928182e32d3b41119c21bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*-VBrJlGkp0FniqoBUFo5Ow.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd na">方程式4。</strong>说明了损失函数。作者使用Markdown &amp; Latex制作的图片。</p></figure><p id="ff85" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">最大似然函数可以计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/cb3acd4afae77f0fc151e7c4626baae5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*1DmjhChs5JwUsDag1G4xrg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd na">方程式5。</strong>作者使用Markdown &amp; Latex制作的图片。</p></figure><p id="57d9" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">现在我们将使用虚拟数据来玩逻辑回归模型。</p><h1 id="bebf" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">1.在R中加载相关的库</h1><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="92fe" class="nk ma it ng b gy nl nm l nn no">#---------------------------------Loading Libraries---------------------------------<br/>library(mvtnorm)<br/>library(reshape2)<br/>library(ggplot2)<br/>library(corrplot)<br/>library(gridExtra)</span></pre><p id="d384" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">这些库将用于创建可视化和检查数据不平衡。</p><h1 id="4119" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">2.读取相关数据</h1><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="f2b8" class="nk ma it ng b gy nl nm l nn no">#---------------------------------Set Working Directory---------------------------------</span><span id="6b19" class="nk ma it ng b gy np nm l nn no">setwd("C:/Users/91905/LR/")</span><span id="756c" class="nk ma it ng b gy np nm l nn no">#---------------------------------Loading Training &amp; Test Data---------------------------------</span><span id="c0e3" class="nk ma it ng b gy np nm l nn no">train_data = read.csv("Train_Logistic_Model.csv", header=T)<br/>test_data = read.csv("Test_Logistic_Model.csv", header=T)</span><span id="87d2" class="nk ma it ng b gy np nm l nn no">#---------------------------------Set random seed (to produce reproducible results)---------------------------------<br/>set.seed(1234)</span><span id="fea3" class="nk ma it ng b gy np nm l nn no">#---------------------------------Create  training and testing labels and data---------------------------------<br/>train.len = dim(train_data)[1]<br/>train.data &lt;- train_data[1:2]<br/>train.label &lt;- train_data[,3]</span><span id="1d29" class="nk ma it ng b gy np nm l nn no">test.len = dim(test_data)[1]<br/>test.data &lt;- test_data[1:2]<br/>test.label &lt;- test_data[ ,3]</span><span id="27d7" class="nk ma it ng b gy np nm l nn no">#---------------------------------Defining Class labels---------------------------------<br/>c0 &lt;- '1'; c1 &lt;- '-1'</span></pre><h1 id="464e" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">3.创建图:训练和测试数据的独立变量散点图</h1><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="f331" class="nk ma it ng b gy nl nm l nn no">#------------------------------Function to define figure size---------------------------------<br/>fig &lt;- function(width, heigth){<br/>     options(repr.plot.width = width, repr.plot.height = heigth)<br/>}</span></pre><p id="4b5b" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">观察数据的分布。</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="19e1" class="nk ma it ng b gy nl nm l nn no"># — — — — — — — — — — — — — — — Creating a Copy of Training Data — — — — — — — — — — — — — — — — -<br/>data=train_data<br/>data[‘labels’]=lapply(train_data[‘y’], as.character)</span><span id="88cc" class="nk ma it ng b gy np nm l nn no">fig(18,8)<br/>plt1=ggplot(data=data, aes(x=x1, y=x2, color=labels)) + <br/>geom_point()+<br/> ggtitle (‘Scatter Plot of X1 and X2: Training Data’) +<br/> theme(plot.title = element_text(size = 10, hjust=0.5), legend.position=’top’)</span><span id="669e" class="nk ma it ng b gy np nm l nn no">data=test_data<br/>data[‘labels’]=lapply(test_data[‘y’], as.character)</span><span id="e4c5" class="nk ma it ng b gy np nm l nn no">fig(18,8)<br/>plt2=ggplot(data=data, aes(x=x1, y=x2, color=labels)) + <br/>geom_point()+<br/> ggtitle (‘Scatter Plot of X1 and X2: Test Data’) +<br/> theme(plot.title = element_text(size = 10, hjust=0.5), legend.position=’top’)</span><span id="4436" class="nk ma it ng b gy np nm l nn no">grid.arrange(plt1, plt2, ncol=2)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/07a3de6df8a8995bd6f3af791d9e70b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m4yYg4zq5ek7fR68kIKAyw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd na">图一。</strong>说明了训练和测试数据的分布。我们可以在上面的图中观察到数据是线性可分的。这是伪数据。真实世界的数据可能不像类似的分布，自变量的数量也不会限制为两个。Image credit —由作者使用r。</p></figure><h1 id="74c9" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">4.检查阶级不平衡</h1><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="9550" class="nk ma it ng b gy nl nm l nn no">#------------------------------Function to define figure size---------------------------------<br/>fig &lt;- function(width, heigth){<br/>     options(repr.plot.width = width, repr.plot.height = heigth)<br/>}</span></pre><p id="3a4d" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">看数据不平衡。我们检查来自训练和测试数据的前100行。</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="0fec" class="nk ma it ng b gy nl nm l nn no">library(‘dplyr’)</span><span id="e0e3" class="nk ma it ng b gy np nm l nn no">data_incr=100<br/>fig(8,4)</span><span id="238f" class="nk ma it ng b gy np nm l nn no"># — — — — — — — — — — — — — — — Creating a Copy of Training Data — — — — — — — — — — — — — — — — -<br/>data=train_data<br/>data[‘labels’]=lapply(train_data[‘y’], as.character)</span><span id="e83e" class="nk ma it ng b gy np nm l nn no"># — — — — — — — — — — — — — — — — — — — — — Looping 100 iterations (500/5) — — — — — — — — — — — — — — — — — — — <br/># — — — — — — — — — — — — — — — — — — — — — Since increment is 5 — — — — — — — — — — — — — — — — — — — <br/>for (i in 1:2)<br/> <br/> {</span><span id="02ac" class="nk ma it ng b gy np nm l nn no">interim=data[1:data_incr,]<br/> <br/> # — — — — — — — — — — — — — — — — — — — — — Count of Records by class balance — — — — — — — — — — — — — — — — — — — <br/> result&lt;-interim%&gt;%<br/> group_by(labels) %&gt;%<br/> summarise(Records = n())<br/> <br/> # — — — — — — — — — — — — — — — — — — — — — Plot — — — — — — — — — — — — — — — — — — — <br/> if (i==1)<br/> {<br/> plot1=ggplot(data=result, aes(x=labels, y=Records)) +<br/> geom_bar(stat=”identity”, fill=”steelblue”)+<br/> geom_text(aes(label=Records), vjust=-0.3, size=3.5)+<br/> ggtitle(“Distribution of Class (#Training Data=5) “)+<br/> theme(plot.title = element_text(size = 10, hjust=0.5), legend.position=’top’)<br/> }<br/> <br/> else<br/> {<br/> plot2=ggplot(data=result, aes(x=labels, y=Records)) +<br/> geom_bar(stat=”identity”, fill=”steelblue”)+<br/> geom_text(aes(label=Records), vjust=-0.3, size=3.5)+<br/> ggtitle(“Distribution of Class (#Training Data=10) “)+<br/> theme(plot.title = element_text(size = 10, hjust=0.5), legend.position=’top’)<br/> }<br/> <br/> <br/> data_incr=data_incr+5<br/> <br/> }<br/>grid.arrange(plot1, plot2, ncol=2)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/0275d438c8beb674115b0b6cd46b622e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1sDL8UGYDPCF8Pk9XGsskA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd na">图二。</strong>说明了二进制类的分布。正如我们可以看到的，我们的积极类在数据中占多数；因此，我们可以看到，数据在很大程度上是不平衡的。Credit —由作者使用r。</p></figure><h1 id="7c14" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">5.逻辑回归</h1><p id="e457" class="pw-post-body-paragraph lc ld it lf b lg mr ju li lj ms jx ll mt mu lo lp mv mw ls lt mx my lw lx ly im bi translated">概率判别模型使用广义线性模型来获得类别的后验概率，并旨在使用最大似然来学习参数。逻辑回归是一种概率判别模型，可用于基于分类的任务。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/06a1404f30a02680fbc2686dd3615f5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IG_9nqWDm8X26fCb3fWtIA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd na">图三。</strong>展示了设计逻辑回归模型的逐步方法。Credit —由作者使用markdown和latex开发。</p></figure><h2 id="29d5" class="nk ma it bd mb ns nt dn mf nu nv dp mj mt nw nx ml mv ny nz mn mx oa ob mp oc bi translated">5.1定义辅助功能</h2><h2 id="17b0" class="nk ma it bd mb ns nt dn mf nu nv dp mj mt nw nx ml mv ny nz mn mx oa ob mp oc bi translated">预测功能</h2><p id="784e" class="pw-post-body-paragraph lc ld it lf b lg mr ju li lj ms jx ll mt mu lo lp mv mw ls lt mx my lw lx ly im bi translated">使用概率分数返回-1或+1。这里使用的阈值是0.5，即如果一个类别的预测概率&gt; 0.5，那么该类别被标记为-1，否则为+1。</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="8818" class="nk ma it ng b gy nl nm l nn no">#-------------------------------Auxiliary function that predicts class labels-------------------------------</span><span id="b038" class="nk ma it ng b gy np nm l nn no">predict &lt;- function(w, X, c0, c1)<br/>{<br/>    sig &lt;- sigmoid(w, X)<br/>    <br/>    return(ifelse(sig&gt;0.5, c1, c0))<br/>    <br/>}</span></pre><h2 id="b011" class="nk ma it bd mb ns nt dn mf nu nv dp mj mt nw nx ml mv ny nz mn mx oa ob mp oc bi translated">成本函数</h2><p id="da6a" class="pw-post-body-paragraph lc ld it lf b lg mr ju li lj ms jx ll mt mu lo lp mv mw ls lt mx my lw lx ly im bi translated">计算成本的辅助功能。</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="f9b2" class="nk ma it ng b gy nl nm l nn no">#-------------------------------Auxiliary function to calculate cost function-------------------------------</span><span id="2de7" class="nk ma it ng b gy np nm l nn no">cost &lt;- function (w, X, T, c0)<br/>{<br/>    sig &lt;- sigmoid(w, X)<br/>    return(sum(ifelse(T==c0, 1-sig, sig)))<br/>    <br/>}</span></pre><h2 id="5442" class="nk ma it bd mb ns nt dn mf nu nv dp mj mt nw nx ml mv ny nz mn mx oa ob mp oc bi translated">乙状结肠功能</h2><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="c283" class="nk ma it ng b gy nl nm l nn no">#-------------------------------Auxiliary function to implement sigmoid function-------------------------------</span><span id="8e44" class="nk ma it ng b gy np nm l nn no">sigmoid &lt;- function(w, x)<br/>{<br/>    return(1.0/(1.0+exp(-w%*%t(cbind(1,x)))))    <br/>}</span></pre><h2 id="ac5a" class="nk ma it bd mb ns nt dn mf nu nv dp mj mt nw nx ml mv ny nz mn mx oa ob mp oc bi translated">5.1.4训练逻辑回归模型</h2><p id="0cd8" class="pw-post-body-paragraph lc ld it lf b lg mr ju li lj ms jx ll mt mu lo lp mv mw ls lt mx my lw lx ly im bi translated">该算法的工作原理如下。最初，设置参数。然后，在处理每个数据点Xn，Tn之后，参数向量被更新为:</p><p id="e51b" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">𝑤(𝜏+1):=𝑤𝜏−𝜂𝜏(𝑦𝑛−𝑡𝑛)(𝑥𝑛)其中，(𝑦𝑛−𝑡𝑛)(𝑥𝑛)是误差函数的梯度，𝜏是迭代次数，𝜂𝜏是特定于迭代的学习速率。</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="b1bc" class="nk ma it ng b gy nl nm l nn no">Logistic_Regression &lt;- function(train.data, train.label, test.data, test.label)<br/>{<br/>    <br/>    #-------------------------------------Initializations-----------------------------------------<br/>    train.len = nrow(train.data)<br/>    <br/>    #-------------------------------------Iterations-----------------------------------------<br/>    tau.max &lt;- train.len * 2<br/>    <br/>    #-------------------------------------Learning Rate-----------------------------------------<br/>    eta &lt;- 0.01<br/>    <br/>    #-------------------------------------Threshold On Cost Function to Terminate Iteration-----------------------------------<br/>    epsilon &lt;- 0.01<br/>    <br/>    #-------------------------------------Counter for Iteration-----------------------------------<br/>    tau &lt;- 1<br/>    <br/>    #-------------------------------------Boolean to check Terimination-----------------------------------<br/>    terminate &lt;- FALSE</span><span id="19ac" class="nk ma it ng b gy np nm l nn no">#-------------------------------------Type Conversion-----------------------------------<br/>    <br/>    #-------------------------------------Convert Training Data to Matrix-----------------------------------<br/>    X &lt;- as.matrix(train.data)<br/>    <br/>    #-------------------------------------Train Labels-----------------------------------<br/>    T &lt;- ifelse(train.label==c0,0,1)<br/>    <br/>    #-------------------------------------Declaring Weight Matrix-----------------------------------<br/>    #-------------------------------------Used to Store Estimated Coefficients-----------------------------------<br/>    #-------------------------------------Dimension of the Matrix = Iteration x Total Columns + 1-----------------------------<br/>    <br/>    W &lt;- matrix(,nrow=tau.max, ncol=(ncol(X)+1))<br/>    <br/>    #-------------------------------------Initializing Weights-----------------------------------<br/>    W[1,] &lt;- runif(ncol(W))</span><span id="6dbc" class="nk ma it ng b gy np nm l nn no">#-------------------------------------Project Data Using Sigmoid function-----------------------------------<br/>    #-------------------------------------Y includes the probability values-----------------------------------<br/>    Y &lt;- sigmoid(W[1,],X)<br/>    <br/>    #-------------------------------------Creating a data frame for storing Cost-----------------------------------<br/>    costs &lt;- data.frame('tau'=1:tau.max)<br/>    <br/>    #-------------------------------------Threshold On Cost Function to Terminate Iteration-----------------------------------<br/>    costs[1, 'cost'] &lt;- cost(W[1,],X,T, c0)<br/> <br/>  <br/>  #-------------------------------------Checking Termination of Iteration-----------------------------------<br/>  while(!terminate){<br/>  <br/>      #-------------------------------------Terminating Criterion----------------------------------<br/>      #-------------------------------------1. Tau &gt; or = Tau Max (Iteration 1 is done before)----------------------------------<br/>      #-------------------------------------Cost &lt;=minimum value called epsilon-----------------------------------<br/>      <br/>      terminate &lt;- tau &gt;= tau.max | cost(W[tau,],X,T, c0)&lt;=epsilon</span><span id="abce" class="nk ma it ng b gy np nm l nn no">#-------------------------------------Shuffling Data-----------------------------------<br/>      train.index &lt;- sample(1:train.len, train.len, replace = FALSE)<br/>      <br/>      #-------------------------------------Obtaing Indexes of Dependent and Independent Variable------------------------------<br/>      X &lt;- X[train.index,]<br/>      T &lt;- T[train.index]</span><span id="399e" class="nk ma it ng b gy np nm l nn no">#-------------------------------------Iterating for each data point-----------------------------------<br/>      for (i in 1:train.len){<br/>        <br/>        #------------------------------------Cross check termination criteria-----------------------------------<br/>        if (tau &gt;= tau.max | cost(W[tau,],X,T, c0) &lt;=epsilon) {terminate&lt;-TRUE;break}<br/>        <br/>        #-------------------------------------Predictions using Current Weights-----------------------------------<br/>        Y &lt;- sigmoid(W[tau,],X)</span><span id="c31a" class="nk ma it ng b gy np nm l nn no">#-------------------------------------Updating Weights-----------------------------------<br/>        #-------------------------------------Refer to the Formula above-----------------------------------<br/>        <br/>        W[(tau+1),] &lt;- W[tau,] - eta * (Y[i]-T[i]) * cbind(1, t(X[i,]))</span><span id="59e5" class="nk ma it ng b gy np nm l nn no">#-------------------------------------Calculate Cost-----------------------------------<br/>        costs[(tau+1), 'cost'] &lt;- cost(W[tau,],X,T, c0)</span><span id="e3f1" class="nk ma it ng b gy np nm l nn no"># #-------------------------------------Updating Iteration-----------------------------------<br/>        tau &lt;- tau + 1</span><span id="48ea" class="nk ma it ng b gy np nm l nn no"># #-------------------------------------Decrease Learning Rate-----------------------------------<br/>        eta = eta * 0.999<br/>      }<br/>      }<br/>     <br/>    #-------------------------------------Remove NAN from Cost vector if it stops early-----------------------------------<br/>      costs &lt;- costs[1:tau, ]</span><span id="7bb3" class="nk ma it ng b gy np nm l nn no">#-------------------------------------Final Weights-----------------------------------<br/>    # #-------------------------------------We use the last updated weight since it is most optimized---------------------<br/>      weights &lt;- W[tau,]</span><span id="2dc8" class="nk ma it ng b gy np nm l nn no">#-------------------------------------Calculating misclassification-----------------------------------<br/>    <br/>    train.predict&lt;-predict(weights,train.data,c0,c1)<br/>    test.predict&lt;-predict(weights,test.data,c0,c1)<br/>    <br/>      errors = matrix(,nrow=1, ncol=2)<br/>    <br/>      errors[,1] = (1-sum(train.label==train.predict)/nrow(train.data))<br/>      errors[,2] = (1-sum(test.label==test.predict)/nrow(test.data))<br/>  <br/>  return(errors)<br/>}</span></pre><p id="cd0c" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">逻辑回归，使用最大似然学习参数。这意味着在学习模型参数(权重)时，必须开发并最大化似然函数。然而，由于非线性方程系统没有解析解，所以使用迭代过程来寻找最优解。</p><p id="2924" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">随机梯度下降被应用于逻辑回归的训练目标，以学习参数和误差函数，从而最小化负对数似然。</p><h2 id="f121" class="nk ma it bd mb ns nt dn mf nu nv dp mj mt nw nx ml mv ny nz mn mx oa ob mp oc bi translated">5.2使用不同数据子集的训练模型</h2><p id="7c6d" class="pw-post-body-paragraph lc ld it lf b lg mr ju li lj ms jx ll mt mu lo lp mv mw ls lt mx my lw lx ly im bi translated">我们将在不同的数据子集上训练模型。这样做是为了在研究数据量对模型错误分类率的影响时考虑方差和偏差。</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="81e8" class="nk ma it ng b gy nl nm l nn no">#------------------------------------------Creating a dataframe to track Errors--------------------------------------</span><span id="7614" class="nk ma it ng b gy np nm l nn no">acc_train &lt;- data.frame('Points'=seq(5, train.len, 5), 'LR'=rep(0,(train.len/5)))<br/>acc_test &lt;- data.frame('Points'=seq(5, test.len, 5), 'LR'=rep(0,(test.len/5)))</span><span id="0208" class="nk ma it ng b gy np nm l nn no">data_incr=5</span><span id="03a0" class="nk ma it ng b gy np nm l nn no">#------------------------------------------Looping 100 iterations (500/5)--------------------------------------<br/>#------------------------------------------Since increment is 5--------------------------------------<br/>for (i in 1:(train.len/5))<br/>    <br/>    {<br/>        #---------------------------------Training on a subset and test on whole data-----------------------------<br/>        error_Logistic = Logistic_Regression(train.data[1:data_incr, ], train.label[1:data_incr], test.data, test.label)<br/>        <br/>        #------------------------------------------Creating accuarcy metrics--------------------------------------<br/>      <br/>        acc_train[i,'LR'] &lt;- round(error_Logistic[ ,1],2)</span><span id="5d6b" class="nk ma it ng b gy np nm l nn no">acc_test[i,'LR'] &lt;- round(error_Logistic[ ,2],2)<br/>        <br/>        #------------------------------------------Increment by 5--------------------------------------<br/>        data_incr = data_incr + 5<br/>}</span></pre><p id="8aba" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">模型的准确性可通过以下方式进行检验:</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="5e27" class="nk ma it ng b gy nl nm l nn no">head(acc_train)<br/>head(acc_test)</span></pre><h1 id="d46b" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">6.结论</h1><p id="55fa" class="pw-post-body-paragraph lc ld it lf b lg mr ju li lj ms jx ll mt mu lo lp mv mw ls lt mx my lw lx ly im bi translated">在处理每个数据点之后更新参数向量；因此，在逻辑回归中，迭代的次数取决于数据的大小。当处理较小的数据集时(即数据点的数量较少)，模型需要更多的训练数据来更新权重和决策边界。因此，当训练数据量很小时，它的准确性很差。</p></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><p id="12ea" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated"><em class="le">关于作者:高级分析专家和管理顾问，帮助公司通过对组织数据的商业、技术和数学的组合找到各种问题的解决方案。一个数据科学爱好者，在这里分享、学习、贡献；你可以和我在</em> <a class="ae ky" href="https://www.linkedin.com/in/angel-das-9532bb12a/" rel="noopener ugc nofollow" target="_blank"> <em class="le">上联系</em> </a> <em class="le">和</em> <a class="ae ky" href="https://twitter.com/dasangel07_andy" rel="noopener ugc nofollow" target="_blank"> <em class="le">上推特</em></a><em class="le">；</em></p></div></div>    
</body>
</html>