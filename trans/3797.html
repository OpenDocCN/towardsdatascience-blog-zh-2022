<html>
<head>
<title>How Number of Hidden Layers Affects the Quality of Autoencoder Latent Representation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">隐藏层数如何影响自动编码器潜在表示的质量</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-number-of-hidden-layers-affects-the-quality-of-autoencoder-latent-representation-181215c8e7d1#2022-08-23">https://towardsdatascience.com/how-number-of-hidden-layers-affects-the-quality-of-autoencoder-latent-representation-181215c8e7d1#2022-08-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8da5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">自动编码器中超参数调谐—第1部分</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0fc19b4fc3522c37fcf32e25826724e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*akj3m_YRGX2Jdju5eAVyNA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Clark Van Der Beken 在<a class="ae ky" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="62ab" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="cd33" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">你可能已经知道<a class="ae ky" href="https://rukshanpramoditha.medium.com/an-introduction-to-autoencoders-in-deep-learning-ab5a5861f81e#d259" rel="noopener">自动编码器潜在表示</a>包含了输入数据的最重要的特征，当它的维数明显低于输入数据的维数时。</p><p id="f4a4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">自动编码器潜在表示的质量取决于许多因素，例如<em class="ms">隐藏层数</em>、<em class="ms">每层中的节点数</em>、<a class="ae ky" href="https://rukshanpramoditha.medium.com/how-the-dimension-of-autoencoder-latent-vector-affects-the-quality-of-latent-representation-c98c38fbe3c3" rel="noopener">、<em class="ms">潜在向量的维数</em>、</a>、<a class="ae ky" rel="noopener" target="_blank" href="/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c">、<em class="ms">隐藏层中激活函数的类型</em>、</a>、<em class="ms">、</em>、<em class="ms">优化器的类型</em>、<em class="ms">学习速率</em>、<em class="ms">数量</em>、<a class="ae ky" rel="noopener" target="_blank" href="/creating-a-multilayer-perceptron-mlp-classifier-model-to-identify-handwritten-digits-9bac1b16fe10#7540">、技术上，这些因素被称为自动编码器模型</a><a class="ae ky" href="https://rukshanpramoditha.medium.com/parameters-vs-hyperparameters-what-is-the-difference-5f40e16e2e82" rel="noopener"> <em class="ms">超参数</em> </a>。</p><p id="7ed6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">获得这些超参数的最佳值称为<em class="ms">超参数调整</em>。机器学习中有不同的<a class="ae ky" rel="noopener" target="_blank" href="/python-implementation-of-grid-search-and-random-search-for-hyperparameter-optimization-2d6a82ebf75c">超参数调整技术</a>。一个简单的技术是手动调整一个超参数(这里是隐藏层数)，同时保持其他超参数值不变。</p><p id="29cb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">今天，在这一集里，我将向你展示隐藏层的数量是如何影响自动编码器潜在表现的质量的。</p><h1 id="8309" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">我们使用的数据集</h1><p id="fbad" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们将使用<a class="ae ky" href="https://rukshanpramoditha.medium.com/acquire-understand-and-prepare-the-mnist-dataset-3d71a84e07e7" rel="noopener"> <strong class="lt iu"> MNIST数据集</strong> </a>(见最后的<a class="ae ky" href="#aec6" rel="noopener ugc nofollow">引文</a>)在这里建立自动编码器模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mt mu l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="ak">获取并准备MNIST数据集</strong>(作者代码)</p></figure><h1 id="ca54" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">方法</h1><p id="2827" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们将建立三个具有不同架构的自动编码器模型，这些模型仅取决于隐藏层的数量，而模型中的其他超参数值保持不变。我将使用几种可视化技术来验证结果。</p><h1 id="65ff" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">具有一个隐藏层的自动编码器</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/e27be896d0b97c873212c89ba678da34.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*1OXK9Hjaod_-WpsAcXcJpg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="0c9f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">上述自动编码器架构可以编码如下。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mt mu l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="ak">用一个隐藏层定义自动编码器架构</strong>(作者代码)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/51b1054a2f25b4490b2447e4f59bd260.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*7AvMCeJhBZtDUpauCtCSbw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="07e0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，我们可以训练模型了。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mt mu l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/6d0a9b567b0f46349ab2b1cf1d519bd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*SmeoZldex0m9WkMxruG_NA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="ca01" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，我们可以看到编码后压缩的MNIST数字。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mt mu l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/f73918fda29f8a026805574a68dc04e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*nLeAnvK6zYSP7JJhMow6zw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mz">带有一个隐藏层的自动编码器的输出</strong>(图片由作者提供)</p></figure><p id="40f9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">此输出不够清晰，无法区分MNIST数据集中的每个数字，因为单个隐藏图层不足以捕捉MNIST数据中大多数复杂的非线性模式(关系)。</p><p id="e1db" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们可以将上面的输出与原始的MNIST数字进行比较。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mt mu l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/ff761dfc992b13cb3095f2c06d57c489.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*xGO8obEtupJgR7W4uc0m8g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mz">原MNIST数字</strong>(图片由作者提供)</p></figure><p id="27ff" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，我们可以在潜在空间中可视化测试MNIST数据，以查看具有一个隐藏层的自动编码器模型如何能够区分九个数字。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mt mu l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/a20f6b0fe2bb63d4573e66d2d06d81fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*tCw6v_3Cm8jZpf4_cFOpug.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mz">可视化潜在空间中的测试MNIST数据</strong>(图片由作者提供)</p></figure><p id="2465" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这些数字在潜在空间中没有清晰的独立簇。这意味着仅具有一个隐藏层的自动编码器模型不能清楚地区分测试MNIST数据中的九个数字。</p><p id="fa2f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">作为一个解决方案，我们可以增加自动编码器模型中的隐藏层数。</p><h1 id="d632" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">具有两个隐藏层的自动编码器</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/7abc3b58952adc6a9b251e19e4dd6ca3.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*RpBY4KtmbE9gE2ZqFpOldA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="f0a4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">上述自动编码器架构可以编码如下。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mt mu l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="ak">定义具有两个隐藏层的自动编码器架构</strong>(作者代码)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/020f1538c357e6be32bfd820723b7f50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*s9dmjk5p77aQCLrWySiFpA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="0acf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，我们可以像以前一样训练模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/d3a17658fef042e3945f25604121d5f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*D1HImQRW-pjfGIcWUAAhEg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="e387" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，我们可以看到编码后压缩的MNIST数字。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/2de4df0e1ad721464cd09232e64f3e02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*afTXagvbOIfVyr-BfLlKYA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mz">具有两个隐藏层的自动编码器的输出</strong>(图片由作者提供)</p></figure><p id="18c5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">此输出比前一个输出好得多，因为两个隐藏层可以捕获MNIST数据中大量复杂的非线性模式(关系)。但是，这个结果仍然离最初的MNIST数字相差甚远。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/ff761dfc992b13cb3095f2c06d57c489.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*xGO8obEtupJgR7W4uc0m8g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mz">原MNIST数字</strong>(图片由作者提供)</p></figure><p id="fde4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，我们可以在潜在空间中可视化测试MNIST数据，以查看具有两个隐藏层的自动编码器模型如何能够区分九个数字。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/4e33d0aaa36c578f2ee14fe07c0fe301.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*FsG6r8NDU713vn-MkZHZMw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mz">可视化潜在空间中的测试MNIST数据</strong>(图片由作者提供)</p></figure><p id="39e0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">与之前的输出相比，一些数字在潜在空间中有清晰的独立簇，而一些没有。这意味着具有两个隐藏层的自动编码器模型可以在一定程度上区分测试MNIST数据中的九个数字，但不是完美的！</p><p id="85a8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">作为一个解决方案，我们可以进一步增加自动编码器模型中的隐藏层数。</p><h1 id="d792" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">具有三个隐藏层的自动编码器</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/86b5b028dc18af57c92ab470fced30fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*JGOCPaUDE5L0BhzqMxaoDQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="d975" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">上述自动编码器架构可以编码如下。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mt mu l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="ak">定义具有三个隐藏层的自动编码器架构</strong>(作者代码)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/de80a09fafbbf6a7b5118584783529b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*49SL66xDRpKEDo4eP8uc0Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="8bc2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，我们可以像以前一样训练模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/dacc0a15217c11c1b84c9eafdb716b8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*B1VgNPKV3xURZPth0Wo5Sg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="7bca" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，我们可以看到编码后压缩的MNIST数字。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/a37145a7b2f3d2d42b92869134945d9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*O14OGOshqd03BJwq4xb-dw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mz">三层隐藏自动编码器的输出</strong>(图片由作者提供)</p></figure><p id="8f71" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">此输出比前两个输出好得多，因为三个隐藏层可以捕获MNIST数据中大多数复杂的非线性模式(关系)。除此之外，这个输出足够接近原来的MNIST数字，但它并不完美！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/ff761dfc992b13cb3095f2c06d57c489.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*xGO8obEtupJgR7W4uc0m8g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mz">原MNIST数字</strong>(图片由作者提供)</p></figure><p id="9627" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，我们可以在潜在空间中可视化测试MNIST数据，以查看具有三个隐藏层的自动编码器模型如何能够区分九个数字。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/00ebf70c431e5adec8cbe9ecf57c9397.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*NLEK4g7ILF--wbLPkOdUWA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mz">可视化潜在空间中的测试MNIST数据</strong>(图片由作者提供)</p></figure><p id="78d3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">与前两个输出相比，大部分数字在潜在空间中具有清晰的独立簇。这意味着具有三个隐藏层的自动编码器模型可以清楚地区分测试MNIST数据中的九个数字。</p><h1 id="64b0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="bd3a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">您可以尝试进一步增加自动编码器模型中的隐藏层数。当您这样做时，由于模型变得更加复杂，有许多隐藏层，这可能会导致<a class="ae ky" href="https://rukshanpramoditha.medium.com/list/addressing-overfitting-868959382d1d" rel="noopener">过拟合</a>，从而显著降低自动编码器潜在表示的质量。为了减轻自动编码器模型中的过拟合，您可以尝试不同的<a class="ae ky" href="https://rukshanpramoditha.medium.com/list/regularization-techniques-for-neural-networks-c4ad21cce618" rel="noopener">正则化技术</a>，如<a class="ae ky" href="https://rukshanpramoditha.medium.com/how-dropout-regularization-mitigates-overfitting-in-neural-networks-9dcc3e7102ff" rel="noopener"> <strong class="lt iu"> <em class="ms">辍学</em> </strong> </a>或<a class="ae ky" href="https://rukshanpramoditha.medium.com/using-early-stopping-to-reduce-overfitting-in-neural-networks-7f58180caf5b" rel="noopener"> <strong class="lt iu"> <em class="ms">提前停止</em> </strong> </a>。</p><p id="2a72" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这篇文章的结尾，我想用一张图片向您展示三种自动编码器模型的输出，并与原始的MNIST数据进行比较。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/dd7e6535947e51f512d08ed7d479dcd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*H6foM3oleWqjgaWforrNrA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mz">自动编码器输出一张图片</strong>(图片由作者提供)</p></figure><p id="8706" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最上面一行代表原始的MNIST数据。随着我们一个接一个地增加autoencoder模型中隐藏层的数量，autoencoder潜在表示的质量也会提高！我们可以考虑进一步增加隐藏层的数量，但这可能会导致我前面提到的过度拟合。</p></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><p id="af9c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">今天的帖子到此结束。</p><p id="783a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果您有任何问题或反馈，请告诉我。</p><h2 id="8889" class="nq la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated">阅读下一篇(推荐)</h2><ul class=""><li id="5f46" class="oc od it lt b lu lv lx ly ma oe me of mi og mm oh oi oj ok bi translated"><strong class="lt iu">深度学习中的自动编码器介绍</strong></li></ul><div class="ol om gp gr on oo"><a href="https://rukshanpramoditha.medium.com/an-introduction-to-autoencoders-in-deep-learning-ab5a5861f81e" rel="noopener follow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">深度学习中的自动编码器介绍</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">理解自动编码器背后的原理</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc ks oo"/></div></div></a></div><ul class=""><li id="c69c" class="oc od it lt b lu mn lx mo ma pd me pe mi pf mm oh oi oj ok bi translated"><strong class="lt iu">自动编码器潜在向量的维度如何影响潜在表示的质量</strong></li></ul><div class="ol om gp gr on oo"><a href="https://rukshanpramoditha.medium.com/how-the-dimension-of-autoencoder-latent-vector-affects-the-quality-of-latent-representation-c98c38fbe3c3" rel="noopener follow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">自动编码器潜在向量的维数如何影响潜在表示的质量</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">自动编码器中超参数调谐—第2部分</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="ox l"><div class="pg l oz pa pb ox pc ks oo"/></div></div></a></div><ul class=""><li id="89e6" class="oc od it lt b lu mn lx mo ma pd me pe mi pf mm oh oi oj ok bi translated"><strong class="lt iu">自动编码器如何在降维方面优于PCA</strong></li></ul><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/how-autoencoders-outperform-pca-in-dimensionality-reduction-1ae44c68b42f"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">自动编码器如何在降维方面优于PCA</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">使用非线性数据的自动编码器的维数减少</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="ph l oz pa pb ox pc ks oo"/></div></div></a></div><ul class=""><li id="03c1" class="oc od it lt b lu mn lx mo ma pd me pe mi pf mm oh oi oj ok bi translated"><strong class="lt iu">全集我的</strong> <a class="ae ky" href="https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75" rel="noopener"> <strong class="lt iu">神经网络与深度学习教程</strong></a><strong class="lt iu"/></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75"><div class="gh gi pi"><img src="../Images/96df37dbb13de047f5bc590b4bd54676.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*N9u-vmTMmi1Oq0W3Fh88yA.png"/></div></a><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mz">点击图片进入我的神经网络和深度学习课程</strong>(作者截图)</p></figure></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><h2 id="a005" class="nq la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated">支持我当作家</h2><p id="ccf5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我希望你喜欢阅读这篇文章。如果你愿意支持我成为一名作家，请考虑 <a class="ae ky" href="https://rukshanpramoditha.medium.com/membership" rel="noopener"> <strong class="lt iu"> <em class="ms">注册会员</em> </strong> </a> <em class="ms">以获得无限制的媒体访问权限。它只需要每月5美元，我会收到你的会员费的一部分。</em></p><div class="ol om gp gr on oo"><a href="https://rukshanpramoditha.medium.com/membership" rel="noopener follow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">通过我的推荐链接加入Medium</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">阅读Rukshan Pramoditha(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="ox l"><div class="pj l oz pa pb ox pc ks oo"/></div></div></a></div><p id="d216" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">非常感谢你一直以来的支持！下一篇文章再见。祝大家学习愉快！</p></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><h2 id="aec6" class="nq la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated">MNIST数据集信息</h2><ul class=""><li id="f186" class="oc od it lt b lu lv lx ly ma oe me of mi og mm oh oi oj ok bi translated"><strong class="lt iu">引用:</strong>邓，l，2012。用于机器学习研究的手写数字图像mnist数据库。<strong class="lt iu"> IEEE信号处理杂志</strong>，29(6)，第141–142页。</li><li id="8d0b" class="oc od it lt b lu pk lx pl ma pm me pn mi po mm oh oi oj ok bi translated"><strong class="lt iu">来源:</strong><a class="ae ky" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">http://yann.lecun.com/exdb/mnist/</a></li><li id="80ba" class="oc od it lt b lu pk lx pl ma pm me pn mi po mm oh oi oj ok bi translated"><strong class="lt iu">许可:</strong><em class="ms">Yann le Cun</em>(NYU库朗研究所)和<em class="ms"> Corinna Cortes </em>(纽约谷歌实验室)持有MNIST数据集的版权，该数据集在<em class="ms">知识共享署名-共享4.0国际许可</em>(<a class="ae ky" href="https://creativecommons.org/licenses/by-sa/4.0/" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">CC BY-SA</strong></a>)下可用。您可以在此了解有关不同数据集许可类型<a class="ae ky" href="https://rukshanpramoditha.medium.com/dataset-and-software-license-types-you-need-to-consider-d20965ca43dc#6ade" rel="noopener">的更多信息。</a></li></ul></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><p id="f04d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="pp pq ep" href="https://medium.com/u/f90a3bb1d400?source=post_page-----181215c8e7d1--------------------------------" rel="noopener" target="_blank">鲁克山普拉莫迪塔</a><br/><strong class="lt iu">2022–08–23</strong></p></div></div>    
</body>
</html>