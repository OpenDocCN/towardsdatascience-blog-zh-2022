<html>
<head>
<title>Characteristics of Word Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词嵌入的特征</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/characteristics-of-word-embeddings-59d8978b5c02#2022-07-26">https://towardsdatascience.com/characteristics-of-word-embeddings-59d8978b5c02#2022-07-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="d467" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/word-embeddings-primer" rel="noopener" target="_blank">单词嵌入入门</a></h2><div class=""/><div class=""><h2 id="2e01" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">反义词的问题</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/856f12bbbada0214c9603279cc7de168.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f1fJauc4JvhPr9RB9T2D1g.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@conscious_design" rel="noopener ugc nofollow" target="_blank">精心设计</a>在<a class="ae lh" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="301a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">本文是6ᵗʰ系列文章<strong class="lk jd">中关于单词嵌入的入门:<br/> </strong> 1。<a class="ae lh" href="https://medium.com/@jongim/a-primer-on-word-embeddings-95e3326a833a" rel="noopener">word 2 vec后面有什么</a> | 2。<a class="ae lh" href="https://medium.com/@jongim/words-into-vectors-a7ba23acaf3d" rel="noopener">单词成向量</a> | <br/> 3。<a class="ae lh" href="https://medium.com/@jongim/statistical-learning-theory-26753bdee66e" rel="noopener">统计学习理论</a> | 4。<a class="ae lh" href="https://medium.com/@jongim/the-word2vec-classifier-5656b04143da" rel="noopener">word 2 vec分类器</a> | <br/> 5。<a class="ae lh" href="https://medium.com/@jongim/the-word2vec-hyperparameters-e7b3be0d0c74" rel="noopener">word 2 vec超参数</a> | 6。<strong class="lk jd">单词嵌入的特征</strong></p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="4cab" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在上一篇文章<a class="ae lh" href="https://medium.com/@jongim/the-word2vec-hyperparameters-e7b3be0d0c74" rel="noopener"><strong class="lk jd">Word2vec超参数</strong> </a>中，我们通过了解word 2 vec算法的文本预处理建议、重新加权技术和优化设置，完成了对它的研究。</p><p id="296e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在本文中，我们将评估典型单词嵌入的优缺点，特别是来自Word2vec的那些。</p><h1 id="b628" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">词义和词义消歧</h1><p id="389b" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">Manning和Schütze的名为<em class="ni">自然语言处理统计基础</em>的书专门用了36页的章节来讨论词义消歧(Manning和Schütze，1999)。这是因为多义词，即具有多个含义的词的处理长期以来被认为是NLP中的一个重要问题。</p><p id="a8c5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">1998年，斯特蒂娜和高娜通过WordNet使用语义标记计算了当今美国英语布朗大学标准语料库(<a class="ae lh" href="https://en.wikipedia.org/wiki/Brown_Corpus" rel="noopener ugc nofollow" target="_blank">布朗语料库</a>)中每个单词的平均词义数量，从而量化了这个问题的程度。他们发现每个单词平均有5.8个意思。他们还量化了语料库中具有多种含义的单词的百分比，如下所示:</p><blockquote class="nj nk nl"><p id="d21b" class="li lj ni lk b ll lm kd ln lo lp kg lq nm ls lt lu nn lw lx ly no ma mb mc md im bi translated">虽然词典中的大多数单词都是单义的，但在演讲和文章中出现频率最高的却是多义词。例如，WordNet中超过80%的单词是单义的，但是在测试的语料库中几乎78%的实词有不止一个义项。<em class="it">(斯特蒂娜和高娜，1998) </em></p></blockquote><p id="f106" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，由Word2vec产生的单词嵌入即使对于多义词也能很好地运行。正如Neelakantan等人(2014年)所解释的，原因是“在适度高维空间中，一个向量可以一次相对‘接近’多个区域。”然而，由和Mooney在2010年以及黄等人在2012年发表的早期论文证明了当在单个词嵌入上使用聚类技术开发多个向量原型时对词向量表示的改进。</p><p id="d068" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">根据Ruas等人在2019年发表的一篇论文，该论文对多义词嵌入的历史发展进行了广泛的回顾，关于Word2vec的多协议类型嵌入的论文数量很少。事实上，只有两篇论文使用了Skip-gram，这两篇论文都发表于2014年，第一篇由Tian等人发表，第二篇由Neelakantan等人发表。Tian等人(2014年)设计了一种使用Skip-gram和分层softmax的期望最大化算法，Neelakantan等人(2014年)提出了一种Skip-gram的扩展，该扩展使用上下文词平均在SGNS训练期间区分多个义项。两种方法都显示出有希望的结果。</p><p id="1a83" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Ruas等人还讨论了其他研究人员的工作，这些工作整合了结构化信息，如来自词汇数据库的词性，如<a class="ae lh" href="https://wordnet.princeton.edu" rel="noopener ugc nofollow" target="_blank"> WordNet </a>以帮助每个词向量的意义表示。Ruas等人(2019)将其他研究人员的这些发现与他们自己使用词汇数据库为每个词义创建单独向量的方法进行了对比。</p><p id="05d9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">2015年，李和朱拉夫斯基断言，“为一个歧义词的每一个意义获得一个不同的表示可以导致更强大和更细粒度的向量空间表示模型。”然而，在他们自己的关于何时证明多义词嵌入合理的模型的结论中，他们指出:“我们的结果表明，简单地增加基线跳格嵌入的维度有时足以实现与使用多义嵌入相同的性能优势”(李和Jurafsky，2015)。尽管如此，Gladkova和Drozd在他们2016年关于评估单词嵌入的论文中，假设一词多义如此重要，以至于它是“房间里的大象”(Gladkova和Drozd，2016)。</p><p id="bc39" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">事实上，对多协议类型嵌入需求的一些模糊性来自于对如何衡量成功缺乏共识。Dubossarsky等人在2018年反对一般意义上的特定向量，因为它们的好处通常是使用单词相似性任务来衡量的，这可能导致假阳性结论(Dubossarsky等人，2018)。</p><h1 id="c161" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">单词嵌入向量的结构</h1><p id="2ee1" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">Word2vec的训练算法涉及一个对数线性函数，我们在本系列的第四篇文章<a class="ae lh" href="https://medium.com/@jongim/the-word2vec-classifier-5656b04143da" rel="noopener"> <strong class="lk jd">中介绍过Word2vec分类器</strong> </a>。虽然目标函数的最小值是非凸的，因为目标和上下文单词的表示<em class="ni"> t </em>和<em class="ni"> c </em>是共同学习的(Goldberg和Levy，2014)，但是在嵌入中观察到线性关系。这些线性关系是Arora等人2018年论文的主题，该论文考虑了词嵌入中词义的线性代数结构。Arora等人提供了理论和实验结果，表明一个多义词的向量本质上是独立意义向量的加权平均值(Arora等人，2018)。这一发现可能与本系列第一篇文章中介绍的向量数学有关，并以<em class="ni">国王男人+女人≈王后</em>为例进行了说明。</p><p id="d873" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">单词嵌入的线性关系也支持从向量中归纳单词含义的能力(Arora等人，2018；穆等，2016)。它们还可能有助于尝试找到一种统计测试，以确定何时需要多义词嵌入的临界值(Oomoto等人，2017年)。</p><p id="c2ce" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">多义词向量的另一个特征是它们的大小。在本系列的第二篇文章中，<a class="ae lh" href="https://medium.com/@jongim/words-into-vectors-a7ba23acaf3d" rel="noopener"> <strong class="lk jd">单词成向量</strong> </a>，当描述为什么<em class="ni">同相似度</em>优于其他距离度量时，我们注意到更频繁的单词往往具有更大幅度的向量，或<em class="ni"> L </em>范数。这种趋势在向量空间模型中通常是真实的，但是当应用有限的上下文窗口(如Word2vec所使用的)时，还有其他因素。</p><p id="0022" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Schakel和Wilson在2015年的论文中证明，对于固定的上下文窗口，向量大小实际上随着频率而增加，但对于出现在多个上下文中的单词，向量大小往往会缩小，因为多义性将向量值拉向多个方向。基于这一观察，Schakel和Wilson讨论了一个单词的“重要性”，他们使用向量幅度与单词频率的比率来衡量与其他以相同频率出现的单词的关系。一般来说，多义词往往有较小的向量幅度，给定其整体词频(Schakel和Wilson，2015)。</p><h1 id="115c" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">反义词和单词嵌入</h1><p id="9de4" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">即使单词嵌入相当好地完成了单词相似性任务和类似于<em class="ni">国王男人+女人≈王后</em>的类比，它们也有缺点。对于同义词，单词嵌入通常执行正确，但是对于多义词，执行不太好。</p><p id="ac42" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">此外，单词嵌入不区分反义词。他们对待反义词更像同义词，因为反义词往往出现在相同的语境中，因为他们的对立面(雷辛格和穆尼，2010；Turney和Pantel，2010年)。例如，以下对立短语通常在附近文本的内容中差别很小:</p><p id="5cdd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="ni">何</em><strong class="lk jd">T5】确认 </strong> <em class="ni">预约。<br/>何</em> <strong class="lk jd"> <em class="ni">取消了</em> </strong> <em class="ni">的预订。</em></p><p id="f2ac" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">大多数区分反义词的方法依赖于词汇数据库。然而，一些研究者，最著名的是Samenko等人，已经找到了在标准单词嵌入本身中寻找反义词信息痕迹的方法。</p><p id="ff3a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">克服单词嵌入中的反义词的挑战的技术在继续发展。例如，2021年由<a class="ae lh" href="https://www.qalaxia.com/" rel="noopener ugc nofollow" target="_blank"> Qalaxia </a>发布的下图显示了一个扩展的更大的类比测试集(<a class="ae lh" href="https://vecto.space/projects/BATS/" rel="noopener ugc nofollow" target="_blank"> BATS </a>)反义词测试集的令人印象深刻的结果:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/b554b85f66f6ab8fc087d3f300896213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NFSeBBnNmQTfi5brA86W5Q.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd nq">反义词在扩展蝙蝠反义词测试集</strong> <br/>上的表现(作者<a class="ae lh" href="https://www.linkedin.com/posts/qalaxia_nlp-ai4good-innovation-activity-6827121691450318848-v154" rel="noopener ugc nofollow" target="_blank">卡拉夏</a>，2021，经许可转载)</p></figure><p id="56ea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Samenko等人对反义词这一主题进行了综述(Samenko等人，2020)。</p><h1 id="de6c" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">超越Word2vec</h1><p id="65e9" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">自2013年Word2vec发布以来，研究人员积极地继续开发其他算法来改善单词嵌入。像Word2vec一样“肤浅”且值得更详细讨论的两个重要算法是GloVe和fastText。</p><p id="6fdc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">手套是由斯坦福大学的一组研究人员在2014年发明的。GloVe代表“全局向量”，它基于全局单词-单词共现统计，使用类似于本系列第二篇文章中讨论的降维技术的技术，<a class="ae lh" href="https://medium.com/@jongim/words-into-vectors-a7ba23acaf3d" rel="noopener"> <strong class="lk jd">单词到向量</strong> </a>，其中数据被收集，然后降维。GloVe的技术产生的单词嵌入具有与Word2vec的嵌入类似的属性，并且在测试时，它产生的结果略好于word 2 vec(Pennington等人，2014)。</p><p id="fbe0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">继GloVe之后，<a class="ae lh" href="https://fasttext.cc" rel="noopener ugc nofollow" target="_blank"> fastText </a>于2016年在Word2vec的创作者Mikolov和脸书的AI研究团队(Bojanowski等人，2016；Joulin等人，2016)。fastText不仅将Word2vec的训练扩展到单词，还扩展到字母组(称为<em class="ni">字符n-grams </em>)，这允许对训练词汇中不存在的单词进行更准确的建模。</p><p id="aca3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Word2vec、GloVe和fastText是用于创建单词嵌入的三种主要的浅层神经网络算法。</p><p id="7aa6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在GloVe和fastText的开发之后，使用上下文化单词表示的深度学习系统的研究取得了进展。目前，AllenNLP <a class="ae lh" href="http://allennlp.org/elmo" rel="noopener ugc nofollow" target="_blank"> ELMo </a>、OpenAI的<a class="ae lh" href="http://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank"> GPT </a>和Google的<a class="ae lh" href="http://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT </a>是三个使用<em class="ni">上下文嵌入</em>的流行深度学习系统(Ethayarajh，2019；Ethayarajh，2020)。在这些系统中，单词嵌入是基于上下文动态更新的，例如周围的句子和其他上下文线索。为了完成他们的NLP任务，这三个系统使用深度学习技术，如一种称为长短期记忆(LSTM)的递归神经网络(RNN)和变形金刚(<a class="ae lh" href="https://medium.com/@gauravghati/comparison-between-bert-gpt-2-and-elmo-9ad140cd1cda" rel="noopener"> Ghati，2020 </a>)。</p><p id="3316" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">随着时间的推移，埃尔莫，GPT和伯特已经变得非常准确，并导致了重大的自然语言处理应用程序的改进。然而，因为它们是深度神经网络，所以它们是计算密集型的。</p><h1 id="5718" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">摘要</h1><p id="7f38" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">在本文中，我们回顾了由浅层神经网络算法(如Word2vec、GloVe和fastText)生成的单词嵌入的一些特征。这些算法通常工作良好，但受到多义词和反义词的挑战，改进浅层神经网络的研究正在进行中。我们还了解了三个更受欢迎的使用上下文嵌入的深度学习系统，以及它们的优点和缺点。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="ad7c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这个系列文章中，<strong class="lk jd">关于单词嵌入的初级读本</strong>，我们研究了单词嵌入是如何包含单词在实际使用时的关系信息的，以及这些关系是如何用数学方法表达的。我们揭示了单词嵌入如何发挥作用并为计算应用程序提供价值，例如搜索引擎和消息应用程序的自动完成功能中的单词预测。</p><p id="a629" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们了解到，创建单词嵌入所涉及的数据收集和处理深深植根于统计理论。虽然NLP中使用的数据不一定是随机的或参数分布的，并且使用了重新加权和压缩技术来将数据转换为实用的信息，但概率和统计理论是NLP不可或缺的一部分。</p><p id="4d51" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们还了解了浅层神经网络在创建单词嵌入方面的能力。通过详细研究Word2vec学习算法，我们发现并不仅仅是Word2vec的新颖算法导致了它的显著成功。Word2vec特殊的数据处理技术对其有效性和广泛采用至关重要。</p><p id="346d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们了解了单词反义词和具有多重含义的单词给NLP带来的挑战，以及NLP如何通过利用深度学习技术做出响应。</p><p id="82d3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">单词嵌入的应用非常广泛:搜索引擎、情感分析、语言翻译、推荐引擎、文本预测、聊天机器人等等。随着如此多的人类交流在网上发生，通常以数字或图像的形式，但更多的是以文本和语音的形式，改进和更好地利用单词嵌入的探索将会继续。我想我们会对未来几年的发展印象深刻。</p><p id="00e6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">具有可操作价值的信息(知识)通常以文字的形式在人们之间交流，而书面文本通常是我们尽力总结这些信息的方式— <em class="ni">走向数据科学</em>就是这样一个知识库！</p><p id="af7c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">虽然在实践层面上，单词嵌入是统计导出的值，以表示应用中的单词，但在更深的层面上，它们使用语言中单词的关系来阐明意义。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="3e14" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这篇文章是6ᵗʰ系列文章<strong class="lk jd">中关于单词嵌入的初级读本:<br/> </strong> 1。<a class="ae lh" href="https://medium.com/@jongim/a-primer-on-word-embeddings-95e3326a833a" rel="noopener">word 2 vec背后有什么</a> | 2。<a class="ae lh" href="https://medium.com/@jongim/words-into-vectors-a7ba23acaf3d" rel="noopener">单词成向量</a> | <br/> 3。<a class="ae lh" href="https://medium.com/@jongim/statistical-learning-theory-26753bdee66e" rel="noopener">统计学习理论</a> | 4。<a class="ae lh" href="https://medium.com/@jongim/the-word2vec-classifier-5656b04143da" rel="noopener">word 2 vec分类器</a> | <br/> 5。<a class="ae lh" href="https://medium.com/@jongim/the-word2vec-hyperparameters-e7b3be0d0c74" rel="noopener">word 2 vec超参数</a> | 6。单词嵌入的特征</p><p id="5226" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">关于这个主题的更多信息:</strong>我推荐的一个了解更多关于单词嵌入属性的资源是一篇在ICML 2019年获得最佳论文荣誉奖的论文:Allen，c .和Hospedales，T. (2019)。类比解释:理解单词嵌入。在<em class="ni">机器学习国际会议上</em>。可从<a class="ae lh" href="https://arxiv.org/abs/1901.09813" rel="noopener ugc nofollow" target="_blank"> arXiv:1901.09813 </a>获得。</p><p id="ab52" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你想了解更多关于<em class="ni">语境嵌入</em>，Ghati的文章《<a class="ae lh" href="https://medium.com/@gauravghati/comparison-between-bert-gpt-2-and-elmo-9ad140cd1cda" rel="noopener">伯特、GPT-2、埃尔莫</a>之间的比较》(<em class="ni">中)</em>是一个很好的介绍。</p><p id="ef61" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果您想开始使用Word2vec算法，Chia的“<a class="ae lh" rel="noopener" target="_blank" href="/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281">使用NumPy和Google Sheets的Word2Vec实现指南</a>”包括一个可下载的Python脚本，它很容易设置，并且有一些图表有助于描述向量和矩阵数学，尽管实现非常基础。或者，<a class="ae lh" href="https://medium.com/search?q=word2vec+pytorch" rel="noopener"> PyTorch版本的Word2vec </a>可以在<em class="ni">上找到，走向数据科学，</em>和<a class="ae lh" href="https://www.tensorflow.org/tutorials/representation/word2vec" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>都有教程。</p><p id="91c9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，如果您想要从更大的语料库中创建可用的单词嵌入，或者使用一组预训练的嵌入启动应用程序，<a class="ae lh" href="https://radimrehurek.com/gensim/models/word2vec.html" rel="noopener ugc nofollow" target="_blank"> Gensim </a>具有Word2vec和fastText的完整实现，并使用Python接口在C中进行了优化。您还可以从Word2vec、fastText和GloVe online中找到标准的预训练嵌入。</p><h1 id="8c4d" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">参考</h1><p id="75f8" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">Arora，s .，Li，y .，Liang，y .，Ma，t .，和Risteski，A. (2018年)。词义的线性代数结构及其在多义词中的应用。计算语言学协会汇刊，6:483–495。可在<a class="ae lh" href="https://arxiv.org/abs/1601.03764v6" rel="noopener ugc nofollow" target="_blank"> arXiv:1601.03764v6 </a>获得。</p><p id="ae61" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Bojanowski，p .，Grave，e .，Joulin，a .，和Mikolov，T. (2016年)。用子词信息丰富词向量。可在<a class="ae lh" href="https://arxiv.org/abs/1607.04606" rel="noopener ugc nofollow" target="_blank"> arXiv:1607.04606 </a>获得。</p><p id="6256" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Chia，D. (2018)。使用NumPy和Google Sheets的Word2Vec实现指南。<em class="ni">走向数据科学。</em></p><p id="f0cb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Dubossarsky、e . Grossman和d . wein shall(2018年)。多义词研究中的控制和评价集。<em class="ni">自然语言处理经验方法会议论文集</em>，第1732-1740页。计算语言学协会。可在doi<a class="ae lh" href="https://www.researchgate.net/deref/http%3A%2F%2Fdx.doi.org%2F10.18653%2Fv1%2FD18-1200" rel="noopener ugc nofollow" target="_blank">10.18653/v1/D18–1200</a>处获得。</p><p id="3c9f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Ethayarajh，K. (2019)。语境化的单词表征有多语境化？比较伯特、埃尔莫和GPT-2嵌入的几何学。<em class="ni">2019自然语言处理经验方法会议暨第九届国际自然语言处理联合会议论文集</em>，第55–65页。<a class="ae lh" href="https://www.aclweb.org/anthology/D19-1006.pdf" rel="noopener ugc nofollow" target="_blank"> PDF </a>。</p><p id="6d7b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Ethayarajh，K. (2020年)。伯特、埃尔莫和GPT-2:语境化的词汇表征有多语境化？ <em class="ni"> Github </em>。</p><p id="8182" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Ghati，G. (2020)。伯特、GPT-2和埃尔莫的比较。<em class="ni">中等</em>。</p><p id="60e4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Gladkova，a .和Drozd，A. (2016)。单词嵌入的内在评估:我们能做得更好吗？<em class="ni">评估自然语言处理矢量空间表示的第一次研讨会会议记录</em>，第36–42页。计算语言学协会。可在doi<a class="ae lh" href="http://dx.doi.org/10.18653/v1/W16-2507" rel="noopener ugc nofollow" target="_blank">10.18653/v1/W16–2507</a>处获得。</p><p id="3c9c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Goldberg和o . Levy(2014年)。word2vec解释:推导Mikolov等人的负采样单词嵌入法。可从<a class="ae lh" href="https://arxiv.org/abs/1402.3722v1" rel="noopener ugc nofollow" target="_blank"> arXiv:1402.3722v1 </a>获得。</p><p id="190a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">黄(e .)、索赫尔(r .)、曼宁(c .)和吴(a .)(2012年)。通过全局上下文和多个单词原型改进单词表示。计算语言学协会第50届年会会议记录，第873-882页。<a class="ae lh" href="https://www.aclweb.org/anthology/P12-1092/" rel="noopener ugc nofollow" target="_blank"> PDF </a>。</p><p id="94ea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Joulin，a .，Grave，e .，Bojanowski，p .，和Mikolov，T. (2016年)。高效文本分类的窍门。可在<a class="ae lh" href="https://arxiv.org/abs/1607.01759" rel="noopener ugc nofollow" target="_blank"> arXiv:1607.01759 </a>获得。</p><p id="ad65" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">李和朱拉夫斯基博士(2015年)。多义嵌入能提高自然语言理解吗？<em class="ni">2015年自然语言处理经验方法会议论文集</em>，第1722–1732页。可在doi<a class="ae lh" href="http://dx.doi.org/10.18653/v1/D15-1200" rel="noopener ugc nofollow" target="_blank">10.18653/v1/D15–1200</a>处获得。</p><p id="e14c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">曼宁和舒策(1999年)。<em class="ni">统计自然语言处理基础</em>。麻省剑桥:麻省理工学院出版社。</p><p id="8607" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Mu，j .，Bhat，s .，和Viswanath，P. (2016年)。多义的几何学。可从<a class="ae lh" href="https://arxiv.org/abs/1610.07569v1" rel="noopener ugc nofollow" target="_blank"> arXiv:1610.07569v1 </a>获得。</p><p id="8e25" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Neelakantan，a .，Shankar，j .，Passos，a .，和McCallum，A. (2014年)。向量空间中每个词多重嵌入的有效非参数估计。<em class="ni">2014年自然语言处理经验方法会议论文集</em>，第1059–1069页。计算语言学协会。可在doi<a class="ae lh" href="http://dx.doi.org/10.3115/v1/D14-1113" rel="noopener ugc nofollow" target="_blank">10.3115/v1/D14–113</a>处获得。</p><p id="b97a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Oomoto，k .，Oikawa，h .，Yamamoto，e .，Yoshida，m .，Okabe，m .，Umemura，K. (2017)。词义分布式表征中的多义词检测。可从<a class="ae lh" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwjmmPe6qdjtAhUB2qQKHRXOACMQFjAAegQIBBAC&amp;url=https%3A%2F%2Farxiv.org%2Fpdf%2F1709.08858&amp;usg=AOvVaw0VLUlF9f89DQZlyPEIaFG3" rel="noopener ugc nofollow" target="_blank"> arXiv:1709.08858v1 </a>获得。</p><p id="e610" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Pennington、r . Socher和c . Manning(2014年)。GloVe:单词表示的全局向量。<em class="ni">2014年自然语言处理经验方法会议论文集</em>，第1532-1543页。<a class="ae lh" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwj_r9_nsartAhUMC-wKHXu4BqYQFjABegQIAxAC&amp;url=https%3A%2F%2Fnlp.stanford.edu%2Fpubs%2Fglove.pdf&amp;usg=AOvVaw3XPTcwWcbYOXnahjvpeDTu" rel="noopener ugc nofollow" target="_blank"> PDF </a>。</p><p id="7722" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">雷辛格和穆尼(2010年)。词义的多原型向量空间模型。人类语言技术会议录:计算语言学协会北美分会会议，第109-117页。<a class="ae lh" href="https://www.aclweb.org/anthology/N10-1013/" rel="noopener ugc nofollow" target="_blank"> PDF </a>。</p><p id="2e74" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Ruas、w . gro sky和a . Aizawa(2019年)。通过词义消歧过程实现的多义嵌入。<em class="ni">专家系统与应用</em>，136:288–303。在doi<a class="ae lh" href="https://doi.org/10.1016/j.eswa.2019.06.026" rel="noopener ugc nofollow" target="_blank">10.1016/j . eswa . 2019 . 06 . 026</a>有售。</p><p id="0e0a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Samenko，I .，Tikhonov，a .，Yamshchikov，I. (2020)。同义词和反义词:嵌入冲突。可从<a class="ae lh" href="https://arxiv.org/abs/2004.12835v1" rel="noopener ugc nofollow" target="_blank"> arXiv:2004.12835v1 </a>获得。</p><p id="54ad" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">a . m . j . schakel和b . j . Wilson(2015年)。使用单词的分布式表示来测量单词的重要性。可从<a class="ae lh" href="https://arxiv.org/abs/1508.02297" rel="noopener ugc nofollow" target="_blank"> arXiv:1508.02297v1 </a>获得。</p><p id="01bf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">斯蒂纳和高娜(1998年)。基于完整句子上下文的通用词义消歧方法。<em class="ni"/><em class="ni">自然语言处理杂志</em>，5卷2期:47–74页。</p><p id="ca8b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">特尼博士和潘特尔博士(2010年)。从频率到意义:语义学的向量空间模型。<em class="ni">人工智能研究杂志</em>，37:141–188。<a class="ae lh" href="https://www.researchgate.net/publication/45904528_From_Frequency_to_Meaning_Vector_Space_Models_of_Semantics" rel="noopener ugc nofollow" target="_blank"> PDF </a>。</p><p id="796c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">*除非另有说明，数字和图像均由作者提供。</p></div></div>    
</body>
</html>