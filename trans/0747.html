<html>
<head>
<title>Linear Model the Machine Learning Way</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性模型机器学习方式</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-model-the-machine-learning-way-c33f9db9ac37#2022-03-02">https://towardsdatascience.com/linear-model-the-machine-learning-way-c33f9db9ac37#2022-03-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ea4b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">当机器学习遇到计量经济学，反之亦然</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8b4a050a296faf00fdb8b51dd668efc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bgc8e2v5et-pE4QQNmJD1Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://www.pexels.com/@souvenirpixels?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kz">詹姆斯·惠勒</strong> </a>发自<a class="ae ky" href="https://www.pexels.com/photo/photo-of-pathway-surrounded-by-fir-trees-1578750/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kz"> Pexels </strong> </a></p></figure><h1 id="a2c3" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">动机</h1><p id="4cb2" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated"><strong class="lu iu">普通最小二乘模型(OLS)是机器学习(ML)的核心构建模块。OLS在社会科学中也随处可见。我有经济学背景，最初我对ML教科书解决OLS问题的方式有点困惑。在这篇博文中，我解释了经济学方法和ML方法，以及为什么两者都有意义。</strong></p><p id="6da4" class="pw-post-body-paragraph ls lt it lu b lv mo ju lx ly mp jx ma mb mq md me mf mr mh mi mj ms ml mm mn im bi translated"><strong class="lu iu">TL；DR:在高维设置中，不要求一个巨大矩阵的逆，用梯度下降。</strong></p><p id="f262" class="pw-post-body-paragraph ls lt it lu b lv mo ju lx ly mp jx ma mb mq md me mf mr mh mi mj ms ml mm mn im bi">—</p><h1 id="e28a" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">一.理论</h1><h2 id="9d8e" class="mt lb it bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated">经济学之路</h2><p id="93a4" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">让我们考虑一个简单的线性模型的形式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/8c72fb532ae1e22e7ffa9a9a8f7ee598.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/format:webp/1*mNz920zaDgdCrwP7mP1ptw.png"/></div></figure><p id="4898" class="pw-post-body-paragraph ls lt it lu b lv mo ju lx ly mp jx ma mb mq md me mf mr mh mi mj ms ml mm mn im bi translated">其中y是因变量的向量；x列中包含独立变量的矩阵；β是我们想知道的向量。和e是随机噪声项。<strong class="lu iu">线性模型的OLS估计器在于最小化数据和(线性)模型之间的均方误差</strong>。就不告诉你数学细节了。底线是这个问题有一个精确的公式。OLS估计量为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/4d80c943f4921fca0f7fa59f773febbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/format:webp/1*NBxt8Rdus8b9YFwR9auJdQ.png"/></div></figure><p id="5977" class="pw-post-body-paragraph ls lt it lu b lv mo ju lx ly mp jx ma mb mq md me mf mr mh mi mj ms ml mm mn im bi translated">“经济学方法”在于直接使用这个公式，做这个看似无害的矩阵求逆。从理论的角度来看，这个公式是惊人的，因为我们可以用它来推导OLS的理论性质。我们也可以用这个公式来计算β的OLS估计量的置信区间。</p><p id="6942" class="pw-post-body-paragraph ls lt it lu b lv mo ju lx ly mp jx ma mb mq md me mf mr mh mi mj ms ml mm mn im bi translated">此外，请注意，在社会科学应用程序中，没有人真正关心您如何获得beta。人们对β系数的符号更感兴趣。例如，以β的第一个系数为例，它不是截距。我们称这个系数为β_ 1。然后，如果y和x_1都以水平表示，那么x_1增加一个单位会导致y增加一个β_ 1。线性模型被用作调查x和y之间因果关系的工具。线性模型能够预测未来结果的程度不是最终目标。</p><h2 id="0d73" class="mt lb it bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated">机器学习的方式</h2><p id="c29a" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">ML方法在于使用递归算法来求解β。为什么不直接使用上面描述的可爱公式来计算β的估计值呢？<strong class="lu iu">症结在于X’X的矩阵求逆。</strong>虽然对“小”数据集(没有太多解释变量和/或观察值)可行，但X’X的直接求逆可能会使您的笔记本电脑崩溃，或者在处理数千个解释变量和/或数十亿个观察值时速度非常慢。</p><p id="0a81" class="pw-post-body-paragraph ls lt it lu b lv mo ju lx ly mp jx ma mb mq md me mf mr mh mi mj ms ml mm mn im bi translated">ML的方式是用<a class="ae ky" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">渐变下降</a> (GD)。GD的想法很直观<strong class="lu iu">。</strong>f在给定点的梯度告诉我们f在该点最大增加的方向。因此，向相反的方向移动(减去梯度)可能是找到局部最小值的好主意。</p><p id="9f7a" class="pw-post-body-paragraph ls lt it lu b lv mo ju lx ly mp jx ma mb mq md me mf mr mh mi mj ms ml mm mn im bi translated">GD算法重复应用这个过程，直到找到最小值(希望是全局的)。从β的初始猜测开始，使用以下公式更新猜测:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/250e47aada2ffa3e0b088201a4fc60d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*wLbbEeAWMQY4EQXseGoCTg.png"/></div></figure><p id="de0e" class="pw-post-body-paragraph ls lt it lu b lv mo ju lx ly mp jx ma mb mq md me mf mr mh mi mj ms ml mm mn im bi translated">其中α是一个小值(“学习率”)，右手边的最后一项是使用当前对β的猜测时损失函数的梯度。这里损失函数的梯度简单地是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/82b230bd915d1d1603cf60e3ac6f66fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*pc4NnLI39KzT4E_fW5hwWQ.png"/></div></figure><h1 id="94a7" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">二。应用</h1><p id="0386" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">现在让我们使用模拟数据来比较这两种方法。我使用模拟数据，因为它允许我直接比较β的估计值和真实值。我使用Julia，它是一种在科学计算应用中日益流行的“快速Python”。</p><h2 id="07a3" class="mt lb it bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated">经济学之路</h2><p id="9b55" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">在下面的代码块中，我定义了一些模拟数据，并固定了一个要估计的随机系数β。按照“经济学方法”，对β的估计是一个简单的矩阵求逆。如图所示，β中的所有系数都位于45度线上，表明OLS公式有效。到目前为止还没什么大惊喜。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Julia线性模型:矩阵求逆</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/cef7b4ff65698d6562fc968d42cb2e5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*7qZR7slsyl4NM4LEi8uLtQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:(X'X)^-1X'y在行动</p></figure><h2 id="e8df" class="mt lb it bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated">机器学习的方式</h2><p id="a4b4" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">现在我们切换到机器学习的方式。在下面的代码块中，我对50次迭代应用梯度下降，并绘制估计的β与真实的β。最初，因为我以随机猜测开始这个过程，所以真实值和估计值是正交的，如蓝色水平线所示。随着迭代方案向前推进，点云开始逆时针旋转。<strong class="lu iu">经过50次迭代，所有点都极其接近45度线，说明我们找到了一个非常接近真值的值。</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">带Julia的线性模型:梯度下降100d</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/1a952998cfc73b4c96a7d4a4e5a276bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*Fp1wxmoFkTZpppIEX8d-uA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者:梯度下降100d案例</p></figure><p id="e96a" class="pw-post-body-paragraph ls lt it lu b lv mo ju lx ly mp jx ma mb mq md me mf mr mh mi mj ms ml mm mn im bi translated">如果我们把注意力限制在二维情况，另一种可视化是可能的。我们可以为梯度下降的每一步绘制β估计的坐标。下面的代码块可以做到这一点。如下图所示，在每次迭代中，beta的估计值越来越接近真实值(小星号)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">带有Julia的线性模型:梯度下降2d</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/9b60c511db8602ff36d0a4df9920bb9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*v49upsnaKayC6lissPUrpA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:梯度下降2d案例</p></figure><h2 id="2162" class="mt lb it bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated">“真正的”机器学习方式</h2><p id="a4b1" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">在上一节中，样本中的所有观察值都用于计算梯度。对于非常大的数据集，计算梯度可能不可行(或者因为相关成本而不可取)。“真正的”机器学习方法在于仅使用观察值的子集来计算每一步的梯度。下面的代码块实现了这个想法。如下图所示，β使用随机梯度下降的路径用十字表示。不是非常平滑地下降到真实值，而是十字所选择的路径更加“模糊”。然而，模糊路径收敛到真实值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">带Julia的线性模型:随机梯度下降2d</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/0aa8b1bd4b9a5391fdcb80312cef6b00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*xrrFyqJh6SBa8SOf5g49DA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:SGD与GD算法</p></figure><h1 id="373c" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">结论</h1><p id="7187" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">OLS分析公式是推导理论属性的黄金标准，在处理合理大小的数据时非常好。在大数据环境中，梯度下降是必由之路。如果你接受过经济学培训，这篇博文可以被看作是对机器学习中使用的一些工具的温和介绍。如果你接受过机器学习的培训，我希望这篇文章能帮助你理解社会科学领域的人们是如何使用线性模型的。</p></div></div>    
</body>
</html>