<html>
<head>
<title>Apache Spark for Data Science — How to Work with Spark RDDs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark for Data Science —如何使用Spark RDDs</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/apache-spark-for-data-science-how-to-work-with-spark-rdds-ff4172ff01f9#2022-04-14">https://towardsdatascience.com/apache-spark-for-data-science-how-to-work-with-spark-rdds-ff4172ff01f9#2022-04-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2180" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><strong class="ak"> Spark基于弹性分布式数据集(RDD)——确保你知道如何使用它们</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/40092dab4c194aa12c5b72b4852b214f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pPXlC-DR7ncVigeV"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@kaps3666?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> NANDKUMAR PATEL </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="cb7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">rdd，或<em class="lv">弹性分布式数据集</em>是Apache Spark中的核心对象。它们是Spark用于快速有效的MapReduce操作的主要抽象。顾名思义，这些数据集是弹性的(容错的)和分布式的(可以分布在集群的不同节点上)。</p><p id="aad9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">谈到Spark中的rdd，有很多东西需要学习，我今天不打算介绍。如果你对理论和内部工作原理感兴趣，请参考<a class="ae ky" href="https://www.xenonstack.com/blog/rdd-in-spark/" rel="noopener ugc nofollow" target="_blank"> XenonStack </a>的这篇介绍性文章。</p><p id="9d1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，我们今天要做什么？我们将亲自处理Spark RDDs。我们将编写一个完整的Spark脚本来处理<a class="ae ky" href="https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv" rel="noopener ugc nofollow" target="_blank">虹膜数据集</a>的值。更准确地说，我们将计算不同物种的萼片长度属性的平均值。这感觉就像每周去杂货店买一辆赛车，但是在这个过程中你会学到很多东西。</p><p id="1fcf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您需要在Python中安装Apache Spark的参考资料，请不要犹豫:</p><div class="lw lx gp gr ly lz"><a rel="noopener follow" target="_blank" href="/apache-spark-for-data-science-how-to-install-and-get-started-with-pyspark-6367a1ea3de8"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">Apache Spark for Data Science —如何安装和使用PySpark</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">在本地安装PySpark并加载您的第一个数据集—只需要5分钟。</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">towardsdatascience.com</p></div></div><div class="mi l"><div class="mj l mk ml mm mi mn ks lz"/></div></div></a></div><p id="410b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">和往常一样，我以视频格式讲述了相同的主题，如果你喜欢的话:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mo mp l"/></div></figure></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="d7c6" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">如何读取一个文本文件作为一个火花RDD</h1><p id="4419" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">和往常一样，您必须做的第一件事是初始化一个新的Spark会话。使用以下代码片段作为本地环境的参考:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="57d2" class="nz my it nv b gy oa ob l oc od">from pyspark import SparkConf, SparkContext</span><span id="baaf" class="nz my it nv b gy oe ob l oc od">conf = SparkConf().setMaster("local").setAppName("IrisSLMeans")<br/>sc = SparkContext(conf=conf)</span></pre><p id="63a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从这里，使用<code class="fe of og oh nv b">textFile()</code>方法从磁盘中读取一个文本文件。记得把<code class="fe of og oh nv b">file://</code>放在路径前面，否则Spark就找不到它了:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="976b" class="nz my it nv b gy oa ob l oc od">iris = sc.textFile("file:///Users/dradecic/Desktop/iris.csv")<br/>iris.collect()[:10]</span></pre><p id="21c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe of og oh nv b">collect()</code>操作方法用于从RDD中获取结果——在本例中，它打印前十行:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/8a8dc6a3082d2344576f477f4aa7c4ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qguHMNjKxzAdsqK6GlDcMA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1 —用Spark读取文本文件(图片由作者提供)</p></figure><p id="bff1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">问题</strong> —文本文件包含一个我们不需要的标题行。当只使用Spark RDDs时，没有明显的方法来消除它。您可以做的是:</p><ol class=""><li id="a072" class="oj ok it lb b lc ld lf lg li ol lm om lq on lu oo op oq or bi translated">通过调用<code class="fe of og oh nv b">iris.first()</code>提取标题行。</li><li id="e45f" class="oj ok it lb b lc os lf ot li ou lm ov lq ow lu oo op oq or bi translated">标题行现在是一个普通的Python字符串——我们需要将其转换成Spark RDD。使用<code class="fe of og oh nv b">parallelize()</code>方法将本地Python集合分发到RDD。</li><li id="0fc6" class="oj ok it lb b lc os lf ot li ou lm ov lq ow lu oo op oq or bi translated">使用<code class="fe of og oh nv b">subtract()</code>方法，从数据集中减去标题。</li></ol><p id="68b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是它在代码中的样子:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="4cb6" class="nz my it nv b gy oa ob l oc od">iris_header = iris.first()<br/>iris_header = sc.parallelize([iris_header])<br/>iris = iris.subtract(iris_header)<br/>iris.collect()[:10]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/e9236e213e3d8a92d212fef2ea45bccc.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*PzRcgbmgeXVem4f3MWZbww.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2 —删除Spark中的标题行(图片由作者提供)</p></figure><p id="b3b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是我们可以解决的问题。Spark将整行视为一个字符串。本质上，我们有一个包含150个字符串的RDD。接下来让我们看看如何解析这些值。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="d536" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">如何用Spark解析逗号分隔的值</h1><p id="fcad" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">我们的RDD的值由逗号分隔。要获得单个值(就像在Pandas中一样)，我们必须在逗号符号上分割线。从那里，我们将提取并返回物种和萼片长度值作为一个元组。</p><p id="eec4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">描述的逻辑将存储在一个名为<code class="fe of og oh nv b">parse_input()</code>的Python函数中:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="41ee" class="nz my it nv b gy oa ob l oc od">def parse_input(line: str) -&gt; tuple:<br/>    cols = line.split(",")<br/>    sepal_length = float(cols[0])<br/>    species = str(cols[-1])<br/>    return (species, sepal_length)</span></pre><p id="d1bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在火花的美丽变得可见。我们将调用<code class="fe of og oh nv b">map()</code>方法，并将我们的函数作为参数传递:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="25f5" class="nz my it nv b gy oa ob l oc od">iris_parsed = iris.map(parse_input)<br/>iris_parsed.collect()[:10]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/c552eb4d666360459373f3c5b736b50f.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*gzD5fhUpBpftQE1LoHexCw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3 —在Spark中使用自定义map()函数(图片由作者提供)</p></figure><p id="625c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">RDD现在看起来有点不同——每个值都是一个元组，包含一个字符串和一个浮点数。这就是我们计算每种花的平均值所需要的。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="a6d8" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">Python中的基本Spark MapReduce任务</h1><p id="82f0" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">在Spark中，您经常会发现map和reduce任务被链接在一行代码中。对初学者来说可能会很困惑，所以我就不多说了。</p><p id="116b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最终目标是将萼片长度值与物种总数相加。例如，我们希望看到<code class="fe of og oh nv b">("Setosa", (250.3, 50))</code>，这意味着50朵Setosa物种的花的萼片总长度为250.3。</p><p id="b654" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">说起来容易做起来难。</p><p id="851d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一步是将<code class="fe of og oh nv b">x</code>转换为<code class="fe of og oh nv b">(x, 1)</code>。这样，我们可以在reduce任务中跟踪每个物种的总萼片长度和总花数。使用Python的<code class="fe of og oh nv b">lambda</code>函数完成任务:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="5f66" class="nz my it nv b gy oa ob l oc od">iris_sl_totals = iris_parsed.mapValues(lambda x: (x, 1))<br/>iris_sl_totals.collect()[:10]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/7ab8d5e75a9580552025225410f4c682.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*PPfSHWMXS7ndGysqsGYwew.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4 —在Spark中使用mapValues()函数(图片由作者提供)</p></figure><p id="c745" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在到了棘手的部分。减少操作需要将萼片长度测量值和计数相加。为此，使用<code class="fe of og oh nv b">reduceByKey()</code>方法并指定另一个<code class="fe of og oh nv b">lambda</code>函数:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="2868" class="nz my it nv b gy oa ob l oc od">iris_sl_totals = iris_parsed.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))<br/>iris_sl_totals.collect()</span></pre><p id="c096" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以看到萼片长度和计数的汇总结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/a9393851c061fc78bb21a2db938784a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dg2M3dIz5p40sJzpoERBMw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5 — MapReduce结果(图片由作者提供)</p></figure><p id="ad1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要计算平均值，只需再次调用<code class="fe of og oh nv b">mapValues()</code>并用总萼片长度除以计数。我还将结果四舍五入到小数点后两位:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="805d" class="nz my it nv b gy oa ob l oc od">iris_sl_means = iris_sl_totals.mapValues(lambda x: round(x[0] / x[1], 2))<br/>result = iris_sl_means.collect()<br/>result</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/969c853df482678915c14d7f176b9bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NKrD5C116jFI45AuZtRU1g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6——每个物种的萼片长度平均值(图片由作者提供)</p></figure><p id="f70c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以访问这个结果对象来漂亮地打印这些值:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="d64b" class="nz my it nv b gy oa ob l oc od">for val in result:<br/>    print(f"Iris species {val[0]} has an average sepal length of {val[1]}")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/b69daf1e6d14ce1afb683734d0a5fbdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iNeWssIWtxx-PafwXQ3Lzw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7——每个物种漂亮印刷的萼片长度平均值(图片由作者提供)</p></figure><p id="6cac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是Spark和Python中基本的MapReduce操作。接下来，我们将把整个逻辑封装到一个Python脚本中。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="bc80" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">编写并执行Spark脚本</h1><p id="3323" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">创建一个新的Python脚本——我将我的脚本命名为<code class="fe of og oh nv b">iris_sl_means.py</code>,并粘贴上一节中的代码——没有调用<code class="fe of og oh nv b">collect()</code>:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="9184" class="nz my it nv b gy oa ob l oc od">from pyspark import SparkConf, SparkContext</span><span id="99b2" class="nz my it nv b gy oe ob l oc od">conf = SparkConf().setMaster("local").setAppName("IrisSLMeans")<br/>sc = SparkContext(conf=conf)<br/></span><span id="74c7" class="nz my it nv b gy oe ob l oc od">def parse_input(line: str) -&gt; tuple:<br/>    cols = line.split(",")<br/>    sepal_length = float(cols[0])<br/>    species = str(cols[-1])<br/>    return (species, sepal_length)<br/></span><span id="2487" class="nz my it nv b gy oe ob l oc od">if __name__ == "__main__":<br/>    # 1. Read the text file<br/>    iris = sc.textFile("file:///Users/dradecic/Desktop/iris.csv")<br/>    <br/>    # 2. Remove the header row<br/>    iris_header = iris.first()<br/>    iris_header = sc.parallelize([iris_header])<br/>    iris = iris.subtract(iris_header)<br/>    <br/>    # 3. Parse the input<br/>    iris_parsed = iris.map(parse_input)<br/>    <br/>    # 4. Calculate totals - sum of all sepal_length values per flower species<br/>    iris_sl_totals = iris_parsed.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))<br/>    <br/>    # 5. Calculate means - Divide the total by the number of instances<br/>    iris_sl_means = iris_sl_totals.mapValues(lambda x: round(x[0] / x[1], 2))<br/>    <br/>    # 6. Wrap into a result<br/>    result = iris_sl_means.collect()<br/>    <br/>    # Print<br/>    for val in result:<br/>        print(f"Iris species {val[0]} has an average sepal length of {val[1]}")</span></pre><p id="fa81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从终端运行脚本:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="e93a" class="nz my it nv b gy oa ob l oc od">spark-submit iris_sl_means.py</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/b22426c62262e7945d3f507b948d6797.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vs9LruOOUG9b9DQ_3yfcSA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图8 —从shell运行Spark脚本(图片由作者提供)</p></figure><p id="668a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我忘了配置Spark现在显示信息日志消息，所以为不必要的详细输出道歉。然而，您可以在笔记本中看到与前面相同的结果。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="ddea" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">Python中Spark RDDs概述</h1><p id="5c21" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">今天，您已经学习了Spark和Python中rdd的基础知识。RDDs不是一天就能学会的概念。它们背后有很多理论，如果你不习惯在一行中看到这么多lambda函数，语法可能会让人不知所措。</p><p id="e8b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，你可以在笔记本上摆弄rdd，随时给<code class="fe of og oh nv b">collect()</code>打电话。通过这样做，您可以看到每次函数调用后发生了什么。希望这足以让你明白其中的逻辑。</p><p id="4182" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下面的文章中，您将学习如何用Spark和Python解决经典的字数问题，敬请关注。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><p id="15c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">喜欢这篇文章吗？成为</em> <a class="ae ky" href="https://medium.com/@radecicdario/membership" rel="noopener"> <em class="lv">中等会员</em> </a> <em class="lv">继续无限制学习。如果你使用下面的链接，我会收到你的一部分会员费，不需要你额外付费。</em></p><div class="lw lx gp gr ly lz"><a href="https://medium.com/@radecicdario/membership" rel="noopener follow" target="_blank"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">通过我的推荐链接加入Medium-Dario rade ci</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">medium.com</p></div></div><div class="mi l"><div class="pe l mk ml mm mi mn ks lz"/></div></div></a></div><h2 id="c24a" class="nz my it bd mz pf pg dn nd ph pi dp nh li pj pk nj lm pl pm nl lq pn po nn pp bi translated">推荐阅读</h2><ul class=""><li id="8a3b" class="oj ok it lb b lc np lf nq li pq lm pr lq ps lu pt op oq or bi translated"><a class="ae ky" href="https://betterdatascience.com/best-data-science-prerequisite-books/" rel="noopener ugc nofollow" target="_blank">学习数据科学先决条件(数学、统计和编程)的5本最佳书籍</a></li><li id="eb92" class="oj ok it lb b lc os lf ot li ou lm ov lq ow lu pt op oq or bi translated"><a class="ae ky" href="https://betterdatascience.com/top-books-to-learn-data-science/" rel="noopener ugc nofollow" target="_blank">2022年学习数据科学的前5本书</a></li><li id="129a" class="oj ok it lb b lc os lf ot li ou lm ov lq ow lu pt op oq or bi translated"><a class="ae ky" href="https://betterdatascience.com/python-list-print/" rel="noopener ugc nofollow" target="_blank">用Python打印列表的7种方法</a></li></ul><h2 id="b456" class="nz my it bd mz pf pg dn nd ph pi dp nh li pj pk nj lm pl pm nl lq pn po nn pp bi translated">保持联系</h2><ul class=""><li id="e6de" class="oj ok it lb b lc np lf nq li pq lm pr lq ps lu pt op oq or bi translated">雇用我作为一名技术作家</li><li id="a5b3" class="oj ok it lb b lc os lf ot li ou lm ov lq ow lu pt op oq or bi translated">订阅<a class="ae ky" href="https://www.youtube.com/c/BetterDataScience" rel="noopener ugc nofollow" target="_blank"> YouTube </a></li><li id="a198" class="oj ok it lb b lc os lf ot li ou lm ov lq ow lu pt op oq or bi translated">在<a class="ae ky" href="https://www.linkedin.com/in/darioradecic/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上连接</li></ul></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><p id="4656" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">原载于2022年4月14日</em><a class="ae ky" href="https://betterdatascience.com/apache-spark-rdd-basics/" rel="noopener ugc nofollow" target="_blank"><em class="lv">【https://betterdatascience.com】</em></a><em class="lv">。</em></p></div></div>    
</body>
</html>