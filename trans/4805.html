<html>
<head>
<title>Choosing a good value for PCA Dimensionality Reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为PCA降维选择一个好的值</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/choosing-a-good-value-for-pca-dimensionality-reduction-a763f2edc8d#2022-10-25">https://towardsdatascience.com/choosing-a-good-value-for-pca-dimensionality-reduction-a763f2edc8d#2022-10-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7f1b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">降低数据维数的快速代码</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/05eea9d1079c68b684e92a10e3506394.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7nR6ByDUjMAvlk3GNFEYPw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@mihaly_koles?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">米赫利·克勒斯</a>在<a class="ae ky" href="https://unsplash.com/s/photos/points?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="e5b3" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">介绍</h1><p id="fae5" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">如果你曾经面对一个数据集，比如说，有100个特征，你可能会考虑减少它的维数。</p><p id="8922" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">首先，因为我们真的很难创造出有那么多属性的引人注目的可视化效果。很难知道什么对情节真正重要。</p><p id="2f32" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">其次，因为有一个被称为<a class="ae ky" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener ugc nofollow" target="_blank"> <em class="mz">维度诅咒</em> </a>的概念，它告诉我们，包含太多变量的数据集往往会变得稀疏(太多零)，这使得机器学习模型更难估计可靠的结果。随着变量数量的增加，对观测数据的需求呈指数增长。</p><p id="8c6e" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">因此，数据科学家经常使用主成分分析[PCA]来减少数据集的特征数量。</p><h1 id="4a7d" class="lg lh it bd li lj na ll lm ln nb lp lq jz nc ka ls kc nd kd lu kf ne kg lw lx bi translated">主成分分析</h1><p id="39c5" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">PCA是一种数学变换，它将数据点从其当前维度投影到称为分量的点向量。每个组成部分的目标都是理解真实数据的最大差异，尽可能减少信息损失。</p><div class="nf ng gp gr nh ni"><a rel="noopener follow" target="_blank" href="/pca-beyond-the-dimensionality-reduction-e352eb0bdf52"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd iu gy z fp nn fr fs no fu fw is bi translated">主成分分析:超越降维</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">了解如何使用PCA算法来寻找一起变化的变量</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">towardsdatascience.com</p></div></div><div class="nr l"><div class="ns l nt nu nv nr nw ks ni"/></div></div></a></div><h1 id="1ae6" class="lg lh it bd li lj na ll lm ln nb lp lq jz nc ka ls kc nd kd lu kf ne kg lw lx bi translated">运行PCA</h1><p id="b49b" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">让我们快速创建一个数据集并运行PCA。以下是包含100个变量的数据集的代码。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="f26b" class="oc lh it ny b gy od oe l of og">from sklearn.datasets import make_regression<br/>from sklearn.decomposition import PCA<br/>import pandas as pd<br/>import matplotlib.pyplot as plt</span><span id="3121" class="oc lh it ny b gy oh oe l of og"># Dataset<br/>X, y = make_regression(n_features=100, n_informative=40, random_state=12)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/3ad4b67663afeb088a891491c42e42f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bacYnDbM-O43VzywMibhAA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">高维数据集。图片由作者提供。</p></figure><p id="a651" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">因为它是用Scikit Learn的<code class="fe oj ok ol ny b">make_regression</code>创建的，所以它已经处于类似的规模，所以我们在本文中不使用缩放技术。</p><p id="1fa5" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">现在，如果我们想要绘制最近创建的数据，这将是一项艰巨的任务。我们需要知道哪些是最重要的变量，然而，比较太多的维度会给我们带来很多麻烦。因此，解决这个问题的一个方法是将数据减少到仅仅2维，这样我们就可以用一个普通的2D图形来绘制它们。就这么办吧。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="4579" class="oc lh it ny b gy od oe l of og">#Reduce Dimensionality<br/>pca = PCA(n_components=2)</span><span id="e589" class="oc lh it ny b gy oh oe l of og"># Fit<br/>pca_2d = pca.fit_transform(X)<br/>df_2d = pd.DataFrame(pca_2d, columns=['PC1', 'PC2'])</span></pre><p id="bebc" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">绘制2D图形。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="435c" class="oc lh it ny b gy od oe l of og"># Plot 2D PCA<br/>plt.figure(figsize=(10,6))<br/>plt.scatter(x= df_2d.PC1, y = df_2d.PC2)<br/>plt.title('Principal Components 2D', size=15);</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/23b04d7c5c049652f54a3853c3c0e439.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*Ob_TsbBRLZaD-snSqCJP3w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PCA减少后的2D图形。图片由作者提供。</p></figure><p id="bdad" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">这是我们缩减到二维的数据。</p><h1 id="2e85" class="lg lh it bd li lj na ll lm ln nb lp lq jz nc ka ls kc nd kd lu kf ne kg lw lx bi translated">选择维度的数量</h1><p id="8105" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">为了选择维度的数量，您有几个选项。当然，第一种方法是像刚才那样输入一个整数。我们简化到二维。但是，这是最好的决定吗？</p><p id="7b26" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">对于可视化来说，可能是的，因为它使我们能够绘制数据。但是，让我们看看我们通过这种减少获得了多少差异。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="7933" class="oc lh it ny b gy od oe l of og"># Variance comprehended in 2D<br/>pca.explained_variance_ratio_</span><span id="5eaf" class="oc lh it ny b gy oh oe l of og"><strong class="ny iu">[OUT]:</strong><br/>array([0.03874607, 0.03712152])</span></pre><p id="ec2c" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们只获得了总方差的7.58%。不是我们数据的一个很好的代表，对吗？仅用这种方法将数据拟合到模型中并不是一个好的选择。</p><p id="dd31" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">这就是为什么<code class="fe oj ok ol ny b">PCA()</code>类可以选择从数据中插入您想要的百分比来创建组件。换句话说，例如，您可以询问获取数据集中95%方差的维度数量。</p><p id="0dd0" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">这是我们如何做到这一点。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="3345" class="oc lh it ny b gy od oe l of og">#Reduce Dimensionality<br/>pca95 = PCA(n_components=0.95)</span><span id="3c0b" class="oc lh it ny b gy oh oe l of og"># Fit<br/>transformed = pca95.fit_transform(X)</span></pre><p id="452d" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">正如在前面的代码片段中看到的，如果我们使用属性<code class="fe oj ok ol ny b">explained_variance_ratio_</code>，就有可能看到每个主成分捕获的数量。利用这些数据，我们可以绘制一个柱状图。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="f8dc" class="oc lh it ny b gy od oe l of og"># Plot<br/>plt.figure(figsize=(15,6))<br/>plt.bar(x= [str(i) for i in range(1,pca95.n_components_+1)], height= pca95.explained_variance_ratio_,)<br/>plt.title('Principal Components with 95% data variance', size=15);</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/f193eb4311930a5b4de7acc455d392d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6tv6GDX5HfsM8TBE3Hkjlg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">60个主成分将获得95%的方差。图片由作者提供。</p></figure><p id="a9d7" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">在这里，我们可以看到方差在许多变量中分布良好。不存在方差超过4%的组件，因此需要60台电脑来弥补我们数据集中高达95%的方差。</p><h1 id="49bf" class="lg lh it bd li lj na ll lm ln nb lp lq jz nc ka ls kc nd kd lu kf ne kg lw lx bi translated">在你走之前</h1><p id="5f18" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">我以前写过关于PCA的文章，重温这个概念和这个漂亮的变压器总是好的。</p><p id="6974" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们可以使用PCA来帮助我们解决复杂的问题，因此知道如何选择正确的组件数量也同样重要。</p><p id="5d27" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">如果你喜欢这个内容，关注我的博客或者在<a class="ae ky" href="https://www.linkedin.com/in/gurezende/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>找到我。</p><div class="nf ng gp gr nh ni"><a href="http://gustavorsantos.medium.com/" rel="noopener follow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd iu gy z fp nn fr fs no fu fw is bi translated">古斯塔沃·桑托斯-中等</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">阅读古斯塔夫·桑托斯在媒介上的作品。数据科学家。我从数据中提取见解，以帮助个人和公司…</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">gustavorsantos.medium.com</p></div></div><div class="nr l"><div class="oo l nt nu nv nr nw ks ni"/></div></div></a></div><p id="e721" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">Git Hub代码:<a class="ae ky" href="https://github.com/gurezende/Studying/blob/master/Python/sklearn/PCA_Python.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/gure zende/studining/blob/master/Python/sk learn/PCA _ Python . ipynb</a></p><h1 id="be86" class="lg lh it bd li lj na ll lm ln nb lp lq jz nc ka ls kc nd kd lu kf ne kg lw lx bi translated">参考</h1><div class="nf ng gp gr nh ni"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener  ugc nofollow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd iu gy z fp nn fr fs no fu fw is bi translated">sklearn.decomposition.PCA</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">主成分分析。使用数据的奇异值分解进行线性降维…</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">scikit-learn.org</p></div></div><div class="nr l"><div class="op l nt nu nv nr nw ks ni"/></div></div></a></div><div class="nf ng gp gr nh ni"><a href="https://medium.com/@monishatemp20/principal-component-analysis-pca-1c285cce3c26" rel="noopener follow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd iu gy z fp nn fr fs no fu fw is bi translated">主成分分析</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">主成分分析</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">主成分分析medium.com</p></div></div><div class="nr l"><div class="oq l nt nu nv nr nw ks ni"/></div></div></a></div><p id="04db" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">Aurélien Géron，2019。<a class="ae ky" href="https://tinyurl.com/2s3tsh9x" rel="noopener ugc nofollow" target="_blank"> <em class="mz">用Scikit-Learn动手机器学习，Keras &amp; TensorFlow </em> </a>。第二版，奥赖利。</p></div></div>    
</body>
</html>