<html>
<head>
<title>Transposed Convolutional Neural Networks — How to Increase the Resolution of Your Image</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">转置卷积神经网络——如何提高图像分辨率</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transposed-convolutional-neural-networks-how-to-increase-the-resolution-of-your-image-d1ec27700c6a#2022-06-06">https://towardsdatascience.com/transposed-convolutional-neural-networks-how-to-increase-the-resolution-of-your-image-d1ec27700c6a#2022-06-06</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="9f93" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph">神经网络</h2><div class=""/><div class=""><h2 id="af46" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">用一个简单的Python例子详细解释转置卷积</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/04cee6b82131f9a8f18280d10bb540c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UB-iZfdZvSCnGBy4_FS5zw.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">转置卷积神经网络。图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><h1 id="2133" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">介绍</h1><p id="4c25" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">卷积神经网络彻底改变了图像分类和对象检测领域。但是你听说过<strong class="md je">转置卷积、</strong>吗，你知道如何使用它们吗？</p><p id="4d6c" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">在本文中，我将解释什么是转置卷积，它们与常规卷积相比如何，并向您展示如何构建一个简单的神经网络，利用它们来提升图像分辨率。</p><h1 id="1e07" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">内容</h1><ul class=""><li id="c3f4" class="nc nd iu md b me mf mh mi mk ne mo nf ms ng mw nh ni nj nk bi translated">机器学习算法领域内的转置卷积</li><li id="a13c" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">什么是转置卷积？</li><li id="cdeb" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">转置卷积有什么用？</li><li id="71ba" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">在Keras/Tensorflow中使用转置卷积构建神经网络的完整Python示例</li></ul><h1 id="f1ff" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">机器学习算法领域内的转置卷积</h1><p id="f46e" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">我已经根据机器学习算法的性质和它们被设计用来做的工作对它们进行了分类。您可以在下面的图表中看到这种分类。</p><p id="462b" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">虽然不可能完美地做到这一点，因为一些算法可以被分配到多个类别，但尝试带来一些结构使我们能够可视化这些不同的算法如何连接和比较。</p><p id="92ad" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated"><strong class="md je">图表是交互式的</strong>，所以你可以通过点击<strong class="md je">来探索它👇在不同的类别上</strong>揭示更多。不出所料，你会在卷积神经网络分支下找到转置卷积网络。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="nq nr l"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">机器学习算法分类。由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>创建的互动图表。</p></figure><p id="5e0b" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated"><strong class="md je"> <em class="ns">如果你喜欢数据科学和机器学习</em> </strong> <em class="ns">，请</em> <a class="ae li" href="https://bit.ly/3sItbfx" rel="noopener ugc nofollow" target="_blank"> <em class="ns">订阅</em> </a> <em class="ns">获取我的新文章邮件。如果你不是中等会员，可以在这里加入</em><a class="ae li" href="https://bit.ly/36Mozgu" rel="noopener ugc nofollow" target="_blank"><em class="ns"/></a><em class="ns">。</em></p><h1 id="80cd" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">什么是转置卷积？</h1><blockquote class="nt nu nv"><p id="49cf" class="mb mc ns md b me mx ke mg mh my kh mj nw mz mm mn nx na mq mr ny nb mu mv mw in bi translated">注意，在一些文献中，转置卷积也被称为解卷积或卷积步长卷积。</p></blockquote><p id="2abc" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">为了理解转置卷积，让我们首先提醒自己什么是正则卷积。</p><h2 id="67c5" class="nz lk iu bd ll oa ob dn lp oc od dp lt mk oe of lv mo og oh lx ms oi oj lz ja bi translated">盘旋</h2><p id="cc3c" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">卷积有三个部分:输入(例如，2D图像)、滤波器(又称为内核)和输出(又称为卷积特征)。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ok"><img src="../Images/caabf4329017f2811cde291871608432.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J6zxbsfIn8Tx89tdz0ESrA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">卷积。对输入数据应用过滤器的迭代过程中的第一次计算。图片由<a class="ae li" href="https://medium.com/@solclover" rel="noopener">作者</a>提供。</p></figure><p id="134b" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">卷积过程是迭代的。首先，对输入图像的一部分应用滤镜，并记录输出值。然后，当<strong class="md je">步距=1 </strong>时，滤波器移动一个位置，或者当步距设置为一个更高的数字时，滤波器移动多个位置，重复相同的过程，直到卷积特征完成。</p><p id="88ca" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">下面的gif图像说明了对5x5输入应用3x3过滤器的过程。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ol"><img src="../Images/8af173c3c284c5fbe69aa89b826693ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*OVeduMtpBhsRKZBrc6u5Eg.gif"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">卷积在起作用。Gif图片由<a class="ae li" href="https://medium.com/@solclover" rel="noopener">作者</a>提供。</p></figure><h2 id="ed39" class="nz lk iu bd ll oa ob dn lp oc od dp lt mk oe of lv mo og oh lx ms oi oj lz ja bi translated"><strong class="ak">转置卷积</strong></h2><p id="e372" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">转置卷积的目标与常规卷积相反，即<strong class="md je">将输入特征图上采样为所需的更大尺寸的输出特征图</strong>。</p><p id="299d" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">为了实现这一点，转置卷积会经历一个迭代过程，即通过过滤器将输入要素地图中的条目相乘，然后将它们相加。请注意，我们还会在每一步中移动指定数量的位置(<strong class="md je">步距</strong>)。</p><p id="f923" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">下面的gif说明了转置卷积是如何工作的。该示例使用步长1，通过2x2滤波器从2x2输入变为3x3输出。</p><ul class=""><li id="2bf0" class="nc nd iu md b me mx mh my mk om mo on ms oo mw nh ni nj nk bi translated">取第一个输入条目，并将其乘以滤波器矩阵。暂时存储结果。</li><li id="5185" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">然后，取第二个输入条目，并将其乘以滤波器矩阵。暂时存储结果。对输入矩阵的其余部分继续此过程。</li><li id="482f" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">最后将所有的部分输出相加得到最终结果。</li></ul><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj op"><img src="../Images/f0f1818676f10844e8eaaf007d4d5450.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*YwVviBiy2qAp0CwS5CDwmA.gif"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">转置卷积的作用。Gif图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="7f60" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">值得注意的是，在转置卷积运算期间，我们实际上是在生成额外的数据，因为我们将特征图从较小的尺寸向上采样到较大的尺寸。</p><p id="e9a5" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">然而，这种运算并不完全是卷积的逆运算。这是因为在卷积过程中总是会丢失一些信息，这意味着我们永远无法通过应用转置卷积来精确地重建相同的数据。</p><p id="706f" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">最后，我们可以试验过滤器的大小或步幅，以获得所需的输出特征地图大小。例如，我们可以将步幅从1增加到2，以避免部分重叠，并产生4x4输出(见下图)。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj oq"><img src="../Images/ac4dd482ce620d1742a40ef0c47c67e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kv5m8-VXHZ5RzHu70Jt_BQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">跨距=2的转置卷积。图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><h2 id="4b5d" class="nz lk iu bd ll oa ob dn lp oc od dp lt mk oe of lv mo og oh lx ms oi oj lz ja bi translated">转置卷积有什么用？</h2><p id="efd3" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">转置卷积对于<strong class="md je">语义分割</strong>和<strong class="md je">生成对抗网络(GANs) </strong>中的数据生成至关重要。一个更直接的例子是训练神经网络来提高图像分辨率。我们现在就要建立一个这样的网络。</p><div class="kt ku kv kw gu ab cb"><figure class="or kx os ot ou ov ow paragraph-image"><a href="https://solclover.com/membership"><img src="../Images/63320331b74bd98eea6402472b4209ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*qkXay39OnVc2IosW6rkxtw.png"/></a></figure><figure class="or kx os ot ou ov ow paragraph-image"><a href="https://www.linkedin.com/in/saulius-dobilas/"><img src="../Images/60fb21d1cb2701bfb6b71f61c99403e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*vabxOXtQ4T034N_mscHSmQ.png"/></a></figure></div><h1 id="b439" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">利用Keras/Tensorflow的完整Python示例</h1><h2 id="9499" class="nz lk iu bd ll oa ob dn lp oc od dp lt mk oe of lv mo og oh lx ms oi oj lz ja bi translated">设置</h2><p id="12e7" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">我们需要获得以下数据和库:</p><ul class=""><li id="c2c4" class="nc nd iu md b me mx mh my mk om mo on ms oo mw nh ni nj nk bi translated">加州理工学院101图像数据集(<a class="ae li" href="https://data.caltech.edu/records/20086" rel="noopener ugc nofollow" target="_blank">来源</a>)</li></ul><blockquote class="nt nu nv"><p id="3493" class="mb mc ns md b me mx ke mg mh my kh mj nw mz mm mn nx na mq mr ny nb mu mv mw in bi translated"><strong class="md je">数据许可:</strong> <a class="ae li" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank">归属4.0国际(CC BY 4.0) </a></p><p id="9a45" class="mb mc ns md b me mx ke mg mh my kh mj nw mz mm mn nx na mq mr ny nb mu mv mw in bi translated"><strong class="md je">参考</strong>:李，女，男，安德雷托，男，兰扎托，文学硕士，&amp;p .(2022)。加州理工101(版本1.0)[数据集]。CaltechDATA。<a class="ae li" href="https://doi.org/10.22002/D1.20086" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.22002/D1.20086</a></p></blockquote><ul class=""><li id="a049" class="nc nd iu md b me mx mh my mk om mo on ms oo mw nh ni nj nk bi translated">用于数据操作的<a class="ae li" href="https://pandas.pydata.org/docs/" rel="noopener ugc nofollow" target="_blank">熊猫</a>和<a class="ae li" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"> Numpy </a></li><li id="04be" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><a class="ae li" href="https://pypi.org/project/opencv-python/" rel="noopener ugc nofollow" target="_blank"> Open-CV </a>、<a class="ae li" href="https://matplotlib.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> Matplotlib </a>和<a class="ae li" href="https://graphviz.org/" rel="noopener ugc nofollow" target="_blank"> Graphviz </a>用于摄取和显示图像并显示模型图</li><li id="3cd2" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><a class="ae li" href="https://www.tensorflow.org/api_docs/python/tf" rel="noopener ugc nofollow" target="_blank"> Tensorflow/Keras </a>用于构建神经网络</li><li id="f745" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><a class="ae li" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank">Scikit-学习库</a>用于拆分数据(<a class="ae li" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" rel="noopener ugc nofollow" target="_blank"> train_test_split </a>)</li></ul><p id="beb9" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">让我们导入库:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ox nr l"/></div></figure><p id="ee3a" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">上面的代码打印了我在这个例子中使用的包版本:</p><pre class="kt ku kv kw gu oy oz pa pb aw pc bi"><span id="b391" class="nz lk iu oz b gz pd pe l pf pg">Tensorflow/Keras: 2.7.0<br/>pandas: 1.3.4<br/>numpy: 1.21.4<br/>sklearn: 1.0.1<br/>OpenCV: 4.5.5<br/>matplotlib: 3.5.1<br/>graphviz: 0.19.1</span></pre><p id="debd" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">接下来，我们下载、保存和摄取加州理工学院101图像数据集。注意，在这个例子中，我将只使用熊猫的图片(Category = "panda ")，而不是101个类别的完整列表。</p><p id="87d5" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">同时，我准备数据并以两种不同的分辨率保存图像:</p><ul class=""><li id="366a" class="nc nd iu md b me mx mh my mk om mo on ms oo mw nh ni nj nk bi translated">64 x 64像素，这将是我们的低分辨率输入数据。</li><li id="bc31" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">256 x 256像素，这将是我们的高分辨率目标数据。</li></ul><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ox nr l"/></div></figure><p id="977a" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">上面的代码打印出我们数据的形状，是<strong class="md je">【样本，行，列，通道】</strong>。</p><pre class="kt ku kv kw gu oy oz pa pb aw pc bi"><span id="b5eb" class="nz lk iu oz b gz pd pe l pf pg">Shape of whole data_lowres:  (38, 64, 64, 3)<br/>Shape of whole data_hires:  (38, 256, 256, 3)<br/>Shape of X_train:  (30, 64, 64, 3)<br/>Shape of Y_train:  (30, 256, 256, 3)<br/>Shape of X_test:  (8, 64, 64, 3)<br/>Shape of Y_test:  (8, 256, 256, 3)</span></pre><p id="1ea4" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">为了更好地理解我们正在处理的数据，让我们显示一些低分辨率的图像，我们将使用它们作为输入。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ox nr l"/></div></figure><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ph"><img src="../Images/16b238b22b54880b8d67a373d28f20cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pxGIVL7_u8d0Ews3F46Pfg.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">训练数据中的低分辨率图像。来自<a class="ae li" href="https://data.caltech.edu/records/20086" rel="noopener ugc nofollow" target="_blank">加州理工学院101 </a>的原始图像数据。由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>合成的图片。</p></figure><p id="e38b" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">和一些高分辨率图像作为我们模型中的目标。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ox nr l"/></div></figure><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pi"><img src="../Images/d1cc9dd5a7cff2e2d8364fb993255942.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l6RyDzGUJI6bFWHAe5OdIw.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">目标数据中更高分辨率的图像。来自<a class="ae li" href="https://data.caltech.edu/records/20086" rel="noopener ugc nofollow" target="_blank">加州理工101 </a>的原始图像数据。组合图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><h1 id="fdac" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">训练和评估转置卷积神经网络</h1><p id="b5c2" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">这个模型非常简单，包含一个输入层、两个转置卷积层和一个充当输出的最终卷积层。您可以按照代码中的注释来理解每个部分的作用。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ox nr l"/></div></figure><p id="b322" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">这是模型图:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pj"><img src="../Images/2ea9fe408e092f91c02bae010cfe3bd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P4oTIW6SilUpSsNWxWWa-Q.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">转置卷积网络图。图片作者<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>。</p></figure><p id="97bf" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">接下来，我训练模型超过100个时期。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ox nr l"/></div></figure><p id="cdad" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">一旦训练完成，我们可以使用该模型将低分辨率图像预测(升级)到更高的分辨率。让我们看几个例子，一个来自训练集，另一个来自测试集。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ox nr l"/></div></figure><p id="4f99" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">显示来自训练集的图像比较:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ox nr l"/></div></figure><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pk"><img src="../Images/346799fa84e156e107e4055b483ae56b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vi0fHbU9z8pqtoCWuu6EYA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">来自列车组的示例。来自<a class="ae li" href="https://data.caltech.edu/records/20086" rel="noopener ugc nofollow" target="_blank">加州理工学院101 </a>的原始图像数据。由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>合成的图片。</p></figure><p id="37ce" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">显示测试集中的图像比较:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ox nr l"/></div></figure><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pl"><img src="../Images/92baba150c38bac2d5ab0f813dfa2525.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-frDuURiiNIe_x562AkFKg.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">来自测试集的示例。来自<a class="ae li" href="https://data.caltech.edu/records/20086" rel="noopener ugc nofollow" target="_blank">加州理工学院101 </a>的原始图像数据。由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>合成的图片。</p></figure><p id="1abf" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">我们可以看到，在上面的两个例子中，我们都成功地提高了图像分辨率。即，单个像素在建模图像中不太明显。</p><p id="4a02" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">然而，我们确实损失了一些锐度，这与我们的目标(256 x 256)图像相比是显而易见的。人们可以试验模型参数以获得更好的结果，因为我的模型决不是优化的。</p><h1 id="0e4f" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">结束语</h1><p id="3afb" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">我真诚地希望你喜欢阅读这篇文章，并获得一些新的知识。</p><p id="fc94" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">你可以在我的<a class="ae li" href="https://github.com/SolClover/Art051_NN_Transposed_CNN" rel="noopener ugc nofollow" target="_blank"> <strong class="md je"> GitHub资源库</strong> </a>中找到完整的Jupyter笔记本代码。<strong class="md je"> </strong>您可以随意使用它来构建自己的转置卷积神经网络，如果您有任何问题或建议，请随时联系我们。</p><p id="bc19" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">如果您想在我发表新文章时得到通知，例如关于<strong class="md je">语义分割</strong>或<strong class="md je"> GANs </strong>的文章，您可以<a class="ae li" href="https://bit.ly/3uJnQFT" rel="noopener ugc nofollow" target="_blank">在此</a>订阅。</p><p id="23bd" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">还有，可以随意查阅我的其他神经网络文章:<a class="ae li" rel="noopener" target="_blank" href="/feed-forward-neural-networks-how-to-successfully-build-them-in-python-74503409d99a">前馈</a>、<a class="ae li" rel="noopener" target="_blank" href="/deep-feed-forward-neural-networks-and-the-advantage-of-relu-activation-function-ff881e58a635">深度前馈</a>、<a class="ae li" rel="noopener" target="_blank" href="/convolutional-neural-networks-explained-how-to-successfully-classify-images-in-python-df829d4ba761#b32e-f8a4ab5107c5"> CNN </a>、<a class="ae li" rel="noopener" target="_blank" href="/rnn-recurrent-neural-networks-how-to-successfully-model-sequential-data-in-python-5a0b9e494f92"> RNN </a>、<a class="ae li" rel="noopener" target="_blank" href="/lstm-recurrent-neural-networks-how-to-teach-a-network-to-remember-the-past-55e54c2ff22e"> LSTM </a>、<a class="ae li" rel="noopener" target="_blank" href="/gru-recurrent-neural-networks-a-smart-way-to-predict-sequences-in-python-80864e4fe9f6"> GRU </a>、<a class="ae li" rel="noopener" target="_blank" href="/autoencoders-ae-a-smart-way-to-process-your-data-using-unsupervised-neural-networks-9661f93a8509#5a19-bd0fb45c9472"> AE </a>、<a class="ae li" rel="noopener" target="_blank" href="/denoising-autoencoders-dae-how-to-use-neural-networks-to-clean-up-your-data-cd9c19bc6915"> DAE </a>、<a class="ae li" rel="noopener" target="_blank" href="/sparse-autoencoder-neural-networks-how-to-utilise-sparsity-for-robust-information-encoding-6aa9ff542bc9"> SAE </a>、<a class="ae li" rel="noopener" target="_blank" href="/vae-variational-autoencoders-how-to-employ-neural-networks-to-generate-new-images-bdeb216ed2c0"> VAE </a>。</p><p id="52af" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">干杯！<br/>索尔·多比拉斯<strong class="md je"/></p><div class="pm pn gq gs po pp"><a href="https://bit.ly/3J6StZI" rel="noopener  ugc nofollow" target="_blank"><div class="pq ab fp"><div class="pr ab ps cl cj pt"><h2 class="bd je gz z fq pu fs ft pv fv fx jd bi translated">通过我的推荐链接加入Medium索尔·多比拉斯</h2><div class="pw l"><h3 class="bd b gz z fq pu fs ft pv fv fx dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="px l"><p class="bd b dl z fq pu fs ft pv fv fx dk translated">solclover.com</p></div></div><div class="py l"><div class="pz l qa qb qc py qd lc pp"/></div></div></a></div></div></div>    
</body>
</html>