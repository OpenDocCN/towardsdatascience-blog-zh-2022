<html>
<head>
<title>A Deep Dive into Dimensionality Reduction with PCA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入探讨使用主成分分析进行降维</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-deep-dive-into-dimensionality-reduction-with-pca-bc6f026ba95e#2022-07-13">https://towardsdatascience.com/a-deep-dive-into-dimensionality-reduction-with-pca-bc6f026ba95e#2022-07-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f1f2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个简单而强大的降维算法的数学深度挖掘</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fd3151bf711a9eef551ffaa141a8d893.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Y43Uk6miOeg5w5Gv"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">美国地质勘探局在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="e8cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你不熟悉PCA，它本质上是一种将高维数据集转换为低维数据集的算法。主成分分析(PCA)是一个非常强大的工具，尤其是在处理大型高维数据集时。在这篇博文中，我试图解释PCA算法是如何工作的，并说明它与奇异值分解(SVD)的联系，奇异值分解是线性代数中一种重要的矩阵分解。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="2745" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在深入数学直觉之前，我们先来看看算法有多简单！</p><p id="e5cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> PCA算法</strong></p><ol class=""><li id="f89a" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">将数据矩阵X居中，使所有列的平均值为0</li><li id="3293" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">计算XᵀX</li><li id="4a1c" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">获得XᵀX的特征值，并按降序排列</li><li id="37f3" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">找出XᵀX.的特征向量，特征向量代表变换向量的权重/负载</li><li id="3321" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">将原始数据X转换成PCA形式Z</li></ol><p id="2752" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一次学这个的时候(没有任何证明)，好像太神奇了。通过获取看似任意的矩阵XᵀX的特征值和特征向量，我们可以找到数据的有效的低维表示。让我们看看为什么这个简单的算法有效。</p><h1 id="6ad8" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated"><strong class="ak">方差最大化</strong></h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/8e1397a7ba79d330bb552efcdb309e0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VZOf95M_-mCrtzAL"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">罗伯特·斯汤普在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="168c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在执行降维时，我们希望保留尽可能多的信息。这个目标可以通过最大化变换数据集的方差来优化。</p><blockquote class="nj nk nl"><p id="c9cf" class="kz la nm lb b lc ld ju le lf lg jx lh nn lj lk ll no ln lo lp np lr ls lt lu im bi translated">一个令人困惑的问题是为什么我们要最大化方差。高方差可能会让你联想到高方差模型，这通常是不好的，是过度拟合的迹象。在这种情况下，PCA不进行任何类型的预测或估计。当我们说模型(回归或分类)具有高方差时，这意味着它对坏的训练数据过度敏感。在我们执行降维的情况下，所讨论的“方差”是该数据集中特征的方差。</p></blockquote><p id="7572" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里的差异表示数据集中存在的有用信息。如果某个特征的方差为0(该特征的所有值都相同)，那么它对我们的分析就没有用了。</p><p id="b758" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于PCA是用线性变换来表示的，所以我们要做的是找到一个权重W的矩阵，它可以变换我们的原始数据集。</p><p id="137d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于由<em class="nm"> n </em>行和<em class="nm"> k </em>特征组成的数据集<em class="nm"> X </em>，我们希望将其转换为具有精简的<em class="nm"> r </em>特征集的转换数据集<em class="nm"> Z </em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/b2b29fc5a2ec7cb4b09ec5f848c4e162.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*G8Rv7D8Th7q8Slj9tV1Jug.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用权重矩阵W将X转换为Z</p></figure><p id="f080" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">利用这一点，权重矩阵W自然将具有维度<em class="nm"> k × r </em>。既然我们已经公式化了我们的问题，让我们看看优化标准。</p><p id="a944" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如前所述，我们希望最大化转换数据集z的方差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/89fdc87288e6aea7596fdf0b7d2f5662.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*ioRzU_mRXrBEEUChHpBpCQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">给定均值= 0时变换数据集方差的数学表达式</p></figure><p id="11ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用上面的方差定义，我们可以导出优化标准。因为我们的原始数据集位于中心，所以特征μ的平均值将为0。这导致了WᵀXᵀXW.的优化目标</p><p id="fa87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了简化优化，我们可以将权重矩阵W分成单独的权重向量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/0bb63189e23ea00ad88b34518ab458a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*a9NIIBi3xzhWubeEwbCceg.png"/></div></figure><p id="8c69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nm"> z_i </em>是指第I个特征，是一个大小为<em class="nm"> n </em>的行向量。对于原始数据集中的<em class="nm"> k </em>要素，第I个权重向量<em class="nm"> w_i </em>的大小为<em class="nm"> k </em>。为了优化整个矩阵<em class="nm"> W </em>，我们可以对从1到r的每个向量<em class="nm"> w_i </em>重复优化过程。</p><p id="b2a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“欺骗”优化的一种方式是让<em class="nm"> w_i </em>变得非常大。使用这种方法将导致转换数据集的方差大于原始数据集。因为这不是我们想要的，我们限制w_i的值，使得向量|| <em class="nm"> w_i </em> ||的范数为1。这使得转换数据集的上限不大于原始数据集。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/96846c775d2390c99ac0ae004978ff69.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*XaUB9eSeGGH30Z5rk54oxw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第I个向量的优化目标</p></figure><p id="be8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个最优的<em class="nm"> w_i </em>就是XᵀX <a class="ae ky" href="https://en.wikipedia.org/wiki/Rayleigh_quotient" rel="noopener ugc nofollow" target="_blank">(这里证明比较复杂)</a>的特征向量。有趣的是，w_iᵀXᵀXw_i值取决于与特征向量相关的特征值。如果我们把最大特征值关联的特征向量作为第一特征<em class="nm"> w_1 </em>，它会给我们最好的可能结果。</p><p id="415f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过按降序取特征值，对随后的权重向量重复这一过程。</p><p id="aef7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是因为<em class="nm"> XᵀX </em>是<em class="nm">t39】的对称矩阵。对称矩阵是特殊的，因为它的特征向量构成了标准正交基。这导致我们的新特征彼此正交，这具有新特征z_i不相关且独立的优点。</em></p><p id="9702" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于每个秩为n的矩阵，我们可以将其分解成n个独立的正交向量，但这将给我们与原始问题完全相同的维数。这里PCA所做的是丢弃具有较低方差(低特征值)的特征。这些复合要素在整个数据集内变化不大，因此它们被确定为不太重要，可以丢弃，对其余数据的影响最小。</p><p id="9831" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">奇异值分解</strong></p><p id="b7ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理解PCA的另一种方法是线性代数。利用线性代数，我们可以从“矩阵分解透镜”来观察PCA。在这种情况下，正在进行的矩阵分解是奇异值分解(SVD)。</p><p id="6e64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">什么是奇异值分解？</strong></p><p id="0a83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SVD是一种通用的矩阵分解算法，它将一个矩阵分解为3个独立的矩阵。SVD的妙处在于它既可以用于正方形矩阵，也可以用于矩形矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/66935ae261a3cfa5e85517bf970c3b19.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*Wn0eCdMdAtsBBrwayPDHYA.png"/></div></figure><p id="deac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这三个矩阵是</p><ul class=""><li id="4b3d" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu nv mi mj mk bi translated">u:aaᵀ的特征向量</li><li id="ae3d" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu nv mi mj mk bi translated">σ:奇异值矩阵。这些是AᵀA和AAᵀ特征值的平方根</li><li id="c101" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu nv mi mj mk bi translated">转置后AᵀA的Vᵀ:特征向量。</li></ul><p id="a21b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">奇异值分解的工作方式是利用对称矩阵AᵀA和AAᵀ的某些性质来获得分解的特征向量。通过这种因式分解，我们可以将矩阵X表示为多个外积/秩1矩阵的和。这里，秩1矩阵的重要性由奇异值σ_i表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/614b8b14c1085d880f43b53629ac69ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2bqBqcHKEFz3voDGvMwdzw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作为外积和的奇异值分解。σ_1对应于最大奇异值</p></figure><p id="ad65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来看看这个方法的实际应用！我们可以使用奇异值分解来分解MNIST数据集中的样本。因为数据来自8×8矩阵，所以总共有8个分量组成图像。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/bb95f17a58c5c70e79ac908b324f7bba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*4gZYy5ssLHbS8XErZBCtYg.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者用SVD — GIF压缩图像</p></figure><p id="63ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们观察到图像中的大多数重要细节可以在3个分量之后观察到。这些分量具有最大的奇异值，并且是最重要的。因此，我们可以安全地丢弃具有低奇异值的矩阵，以减少x中的信息。这样的一个应用是图像压缩。我们可以选择存储第一个<em class="nm"> r </em>矩阵，而不是存储一个巨大的n × n图像。因为每个矩阵由2个n大小的向量组成，所以空间的大小将是<em class="nm"> 2nr </em>而不是<em class="nm"> n </em>。</p><p id="4d71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这与PCA非常相似，我们采用第一个<em class="nm"> r </em>最重要的特征。关键区别在于，在SVD的这种应用中，矩阵乘法<strong class="lb iu">的结果具有相同的维数，但它被压缩成更易于存储的向量。</strong></p><h1 id="1702" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated"><strong class="ak">把所有的东西放在一起</strong></h1><p id="07c4" class="pw-post-body-paragraph kz la it lb b lc ny ju le lf nz jx lh li oa lk ll lm ob lo lp lq oc ls lt lu im bi translated">现在我们对SVD做什么有了一些直觉，让我们更仔细地看看SVD公式，并做一些重新排列。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/4b6c047ae7d5b181b1f94e7c11216e5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*aEBU7bzaaobU1IzxRDNmmQ.png"/></div></figure><p id="3b6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为v是对称矩阵AᵀA的特征向量矩阵，所以它将是正交基，并且VᵀV= <em class="nm"> I </em>。后乘以V将得到第二行。这里我们看到左手边非常熟悉。</p><p id="5648" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据我们的SVD分解，v由XᵀX的特征向量表示，该特征向量恰好是先前在PCA中导出的权重向量。矩阵乘法XV是与来自PCA的XW完全相同的结果。</p><p id="fc63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这允许我们减少V的维数，如果我们减少矩阵V中的列数，我们就减少了x V(转换后的数据集)中的特征数。</p><p id="b1de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这说明PCA可以解释为矩阵分解的中间步骤！</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="6ce1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">代码实现</strong></p><p id="2c17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们使用相同的MNIST数据集来比较不同的主成分分析方法！</p><p id="f969" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们从预处理步骤开始，使数据集居中，并确保所有特征的平均值为0。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据预处理</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="ac66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于第二个实现，我们将使用博客文章开头的原始算法。首先，我们通过矩阵乘法计算协方差矩阵。然后用<code class="fe og oh oi oj b">np.linalg.eigh</code>函数求出特征向量和特征值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="1c13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于第三个实施方案，我们使用<code class="fe og oh oi oj b">np.linalg.svd</code>函数直接计算特征向量矩阵v。与之前的实施方案相比，这为我们节省了几行排序和矩阵乘法代码。</p><p id="612c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在你知道了！以3种不同方式执行PCA的数学直觉和代码实现。检查代码并运行它，让自己相信它是可行的！</p><div class="ok ol gp gr om on"><a href="https://github.com/reoneo97/medium-notebooks/blob/master/pca-svd.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd iu gy z fp os fr fs ot fu fw is bi translated">中型笔记本电脑/PCA-SVD . ipynb at master reoneo 97/中型笔记本电脑</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">关于规范化的媒体文章库。为reoneo 97/中型笔记本电脑的发展做出贡献</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">github.com</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb ks on"/></div></div></a></div></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="499b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你喜欢这篇文章，请在Medium上关注我！<br/>在LinkedIn上连接:<a class="ae ky" href="https://www.linkedin.com/in/reo-neo/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/reo-neo/</a></p><p id="c8da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考文献</strong></p><p id="9511" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1] G. Strang，<em class="nm">线性代数导论</em>，第四。马萨诸塞州韦尔斯利学院:韦尔斯利-剑桥出版社，2009年。</p></div></div>    
</body>
</html>