<html>
<head>
<title>Hands-On Introduction to Reinforcement Learning in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python强化学习实践介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hands-on-introduction-to-reinforcement-learning-in-python-da07f7aaca88#2022-07-07">https://towardsdatascience.com/hands-on-introduction-to-reinforcement-learning-in-python-da07f7aaca88#2022-07-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2508" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过教机器人走迷宫来理解奖励</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a579c7076c9472a565b371bfa4af83a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gMgU8hLuZL_6MSHs"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/es/@brett_jordan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">布雷特·乔丹</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="b6ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">传统机器学习的最大障碍之一是，大多数有监督和无监督的机器学习算法需要大量数据才能在现实世界的用例中有用。即使这样，如果没有人类的监督和反馈，人工智能也无法学习。如果人工智能可以从头开始学习会怎样？</p><p id="9f9e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为最著名的例子之一，谷歌的DeepMind构建了<a class="ae kv" href="https://www.deepmind.com/research/highlighted-research/alphago" rel="noopener ugc nofollow" target="_blank"> AlphaGo </a>，它能够击败历史上最好的围棋选手Lee Sedol。为了学习最佳策略，它使用了深度学习和强化学习的结合——就像在，通过与自己对弈成千上万的围棋游戏。李·塞多尔甚至说，</p><blockquote class="ls lt lu"><p id="9fef" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">我认为AlphaGo是基于概率计算的，它只是一台机器。但是当我看到这个举动的时候，我改变了主意。毫无疑问，AlphaGo很有创造力。</p></blockquote><p id="e270" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">强化学习消除了对大量数据的需要，并且还优化了它可能在广泛的环境中接收的高度变化的数据。它密切模拟了人类的学习方式(甚至可以像人类一样找到非常令人惊讶的策略)。</p><p id="d8a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">更简单地说，强化学习算法由代理和环境组成。代理为环境的每个状态计算一些奖励或惩罚的概率。这个循环是这样工作的:给一个代理一个状态，代理向环境发送一个动作，环境发送一个状态和回报。</p><p id="2088" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们试着编码一个机器人，它将试着在尽可能少的移动中通过一个6×6的迷宫。首先，让我们从创建代理和环境类开始。</p><h1 id="f202" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">环境和代理</h1><p id="e019" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">我们希望我们的代理能够根据以前的一些经验来决定做什么。它需要能够基于一组给定的操作做出决策并执行一些操作。避免拟人化的定义代理是什么，以更严格地定义你的代理将有什么样的方法和功能——在强化学习中，代理不能控制的任何东西都是环境的一部分。</p><p id="17c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">环境是代理可以与之交互的代理之外的任何事物，包括系统的状态。它不一定是您想象的完整环境，只要包括当代理做出选择时真正改变的东西。环境还包括你用来计算奖励的算法。</p><p id="118e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在名为<code class="fe mw mx my mz b">environment.py</code>的文件中，创建这个类:</p><pre class="kg kh ki kj gt na mz nb nc aw nd bi"><span id="309c" class="ne ma iq mz b gy nf ng l nh ni">import numpy as np</span><span id="d1d9" class="ne ma iq mz b gy nj ng l nh ni">ACTIONS = {'U': (-1, 0), 'D': (1, 0), 'L': (0, -1), 'R': (0, 1)}</span><span id="d35d" class="ne ma iq mz b gy nj ng l nh ni">class Maze(object):<br/>    def __init__(self):<br/>        # start with defining your maze<br/>        self.maze = np.zeroes((6, 6))<br/>        self.maze[0, 0] = 2<br/>        self.maze[5, :5] = 1<br/>        self.maze[:4, 5] = 1<br/>        self.maze[2, 2:] = 1<br/>        self.maze[3, 2] = 1</span><span id="08e8" class="ne ma iq mz b gy nj ng l nh ni">        self.robot_position = (0, 0) # current robot position<br/>        self.steps = 0 # contains num steps robot took</span><span id="f484" class="ne ma iq mz b gy nj ng l nh ni">        self.allowed_states = None # for now, this is none<br/>        self.construct_allowed_states() # not implemented yet</span></pre><p id="5f30" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据我们编写的代码，下面是我们的迷宫的样子(在我们的代码中，1代表墙壁，2代表机器人的位置):</p><pre class="kg kh ki kj gt na mz nb nc aw nd bi"><span id="e649" class="ne ma iq mz b gy nf ng l nh ni">R 0 0 0 0 X<br/>0 0 0 0 0 X<br/>0 0 X X X X<br/>0 0 X 0 0 X<br/>0 0 0 0 0 0 <br/>X X X X X 0 &lt;- here's the end</span></pre><p id="ea41" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我们需要存储在环境中的核心信息。根据这些信息，我们可以在以后创建函数来更新给定动作的机器人位置，给予奖励，甚至打印迷宫的当前状态。</p><p id="cfdf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可能还注意到，我们添加了一个<code class="fe mw mx my mz b">allowed_states</code>变量，并在其后调用了一个<code class="fe mw mx my mz b">construct_allowed_states()</code>函数。<code class="fe mw mx my mz b">allowed_states</code>将很快拥有一本字典，将机器人所处的每一个可能的位置映射到机器人可以从那个位置到达的可能位置的列表。<code class="fe mw mx my mz b">construct_allowed_states()</code>将构建此地图。</p><p id="4f9a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还创建了一个名为<code class="fe mw mx my mz b">ACTIONS</code>的全局变量，它实际上只是一个可能的移动及其相关翻译的列表(我们甚至可以省略方向标签，但它们是为了便于阅读和代码调试)。我们将在构建允许的州地图时使用它。为此，让我们添加以下方法:</p><pre class="kg kh ki kj gt na mz nb nc aw nd bi"><span id="15ab" class="ne ma iq mz b gy nf ng l nh ni">def is_allowed_move(self, state, action):<br/>    y, x = state<br/>    y += ACTIONS[action][0]<br/>    x += ACTIONS[action][1]</span><span id="8a34" class="ne ma iq mz b gy nj ng l nh ni">    # moving off the board<br/>    if y &lt; 0 or x &lt; 0 or y &gt; 5 or x &gt; 5:<br/>         return False</span><span id="8db1" class="ne ma iq mz b gy nj ng l nh ni">    # moving into start position or empty space<br/>    if self.maze[y, x] == 0 or self.maze[y, x] == 2:<br/>        return True<br/>    else:<br/>        return False</span><span id="27bd" class="ne ma iq mz b gy nj ng l nh ni">def construct_allowed_states(self):<br/>    allowed_states = {}<br/>    for y, row in enumerate(self.maze):<br/>        for x, col in enumerate(row):<br/>            # iterate through all valid spaces<br/>            if self.maze[(y,x)] != 1:<br/>                allowed_states[(y,x)] = []<br/>                for action in ACTIONS:<br/>                    if self.is_allowed_move((y, x), action):<br/>                        allowed_states[(y,x)].append(action)</span><span id="cd34" class="ne ma iq mz b gy nj ng l nh ni">    self.allowed_states = allowed_states</span><span id="bc46" class="ne ma iq mz b gy nj ng l nh ni">def update_maze(self, action):<br/>    y, x = self.robot_position<br/>    self.maze[y, x] = 0 # set the current position to empty<br/>    y += ACTIONS[action][0]<br/>    x += ACTIONS[action][1]<br/>    self.robot_position = (y, x)<br/>    self.maze[y, x] = 2<br/>    self.steps += 1</span></pre><p id="f04e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这允许我们在迷宫实例化时快速生成状态到允许动作的地图，然后在机器人每次移动时更新状态。</p><p id="78bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还应该在环境中创建一种方法来检查机器人是否在迷宫的尽头:</p><pre class="kg kh ki kj gt na mz nb nc aw nd bi"><span id="d1d4" class="ne ma iq mz b gy nf ng l nh ni">def is_game_over(self):<br/>    if self.robot_position == (5, 5):<br/>        return True<br/>    return False</span></pre><p id="f5bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们准备为我们的代理开始上课。在名为<code class="fe mw mx my mz b">agent.py</code>的文件中，创建一个新类:</p><pre class="kg kh ki kj gt na mz nb nc aw nd bi"><span id="a6d9" class="ne ma iq mz b gy nf ng l nh ni">import numpy as np</span><span id="9997" class="ne ma iq mz b gy nj ng l nh ni">ACTIONS = {'U': (-1, 0), 'D': (1, 0), 'L': (0, -1), 'R': (0, 1)}</span><span id="e27a" class="ne ma iq mz b gy nj ng l nh ni">class Agent(object):<br/>    def __init__(self, states, alpha=0.15, random_factor=0.2):<br/>        self.state_history = [((0, 0), 0)] # state, reward<br/>        self.alpha = alpha<br/>        self.random_factor = random_factor<br/>        <br/>        # start the rewards table<br/>        self.G = {}<br/>        self.init_reward(states)</span></pre><p id="b94c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，很多内容看起来都不太熟悉，但这是一个介绍奖励算法的好时机，我们将使用该算法来培训我们的代理。</p><h1 id="0ea5" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">奖励</h1><p id="0a33" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">所有代理人的目标都是回报最大化。就像任何机器学习算法一样，奖励将采取数字的形式，根据某种算法而变化。代理将尝试估计其每个行动选择的损失，然后采取行动，然后从环境中获得行动的<em class="lv">真实</em>回报，然后调整其对该特定行动的未来预测。</p><p id="cdda" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将为我们的环境提出一个非常简单的奖励政策:机器人每走一步，我们罚-1分(因为我们想要最快的解决方案，而不是任何解决方案)，然后当机器人到达终点时，奖励0分。因此，采取20个步骤的解决方案将奖励代理总共-20分，而采取10个步骤的解决方案将奖励代理-10分。我们奖励政策的关键是保持简单——我们不想过度监管我们的代理。</p><p id="6fa2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们将它编码到我们的环境中。将这些方法添加到您的<code class="fe mw mx my mz b">Maze</code>类中:</p><pre class="kg kh ki kj gt na mz nb nc aw nd bi"><span id="2aa2" class="ne ma iq mz b gy nf ng l nh ni">def give_reward(self):<br/>    if self.robot_position == (5, 5):<br/>        return 0<br/>    else:<br/>        return -1</span><span id="0bea" class="ne ma iq mz b gy nj ng l nh ni">def get_state_and_reward(self):<br/>    return self.robot_position, self.give_reward()</span></pre><p id="a5f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样！</p><p id="30e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好吧，但是有一个问题——代理人怎么可能<em class="lv">预测</em>每一次行动会得到的回报？</p><h2 id="bfb2" class="ne ma iq bd mb nk nl dn mf nm nn dp mj lf no np ml lj nq nr mn ln ns nt mp nu bi translated">情节剧</h2><p id="1c26" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">这里的目标是创建一个函数，在一集<em class="lv">中</em>(在我们的例子中，一集是一场游戏)为每个州的预期未来奖励建模。这些奖励随着代理经历更多的情节或游戏而不断调整，直到它收敛于环境给定的每个状态的“真实”奖励。例如，我们可能有这样一个状态表:</p><pre class="kg kh ki kj gt na mz nb nc aw nd bi"><span id="2dba" class="ne ma iq mz b gy nf ng l nh ni">+--------+-----------------+<br/>| State  | Expected Reward |<br/>+--------+-----------------+<br/>| (0, 0) | -9              |<br/>| (1, 0) | -8              |<br/>| ...    | ...             |<br/>| (X, Y) | G               |<br/>+--------+-----------------+</span></pre><p id="2107" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中G是一个状态(X，Y)的给定期望报酬。但是我们的机器人将从一个随机化的状态表开始，因为它实际上还不知道任何给定状态的预期回报，并将试图收敛到每个状态的G。</p><p id="64ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的学习公式是<code class="fe mw mx my mz b"><em class="lv">G_state = G_state + </em>α(target — <em class="lv">G_state</em>)</code> <em class="lv">。在实践中，在一集结束时，机器人已经记住了它所有的状态和相应的奖励。它也知道自己当前的<em class="lv"> G </em>表。使用这个公式，机器人将根据这个简单的公式更新<em class="lv"> G </em>表中的每一行。</em></p><p id="b700" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们来分解一下。我们实际上是在给定状态下，增加了实际奖励(<em class="lv">目标</em>)和我们最初预期奖励之间的差额百分比。你可以把这个差额想象成<em class="lv">损失</em>。这个百分比被称为alpha或α，熟悉传统机器学习模型的人会将其视为学习率。百分比越大，它可能越快地向目标回报靠拢，但是它越有可能超过或高估真正的目标。对于我们的代理，我们将默认学习率设置为0.15。</p><p id="3b4e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">实现成功的奖励算法有多种方式，这都取决于环境及其复杂性。例如，AlphaGo使用深度q学习，这种学习实现了神经网络，可以根据过去有益举动的随机样本来帮助预测预期回报。</p><p id="40d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们编码我们的算法。首先，我们需要一个函数来初始化我们的<code class="fe mw mx my mz b">Agent</code>类中的随机状态表，这个函数在代理初始化时被调用:</p><pre class="kg kh ki kj gt na mz nb nc aw nd bi"><span id="f899" class="ne ma iq mz b gy nf ng l nh ni">def init_reward(self, states):<br/>    for i, row in enumerate(states):<br/>        for j, col in enumerate(row):<br/>            self.G[(j,i)] = np.random.uniform(high=1.0, low=0.1)</span></pre><p id="9812" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将<em class="lv"> G </em>的随机值初始化为总是大于0.1，因为我们不希望它将任何状态初始化为0，因为这是我们的目标(如果一个状态最终从0开始，代理将永远不会从该状态中学习)。</p><p id="c08f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其次，我们需要一种方法，允许代理在一集结束时“学习”G 的新值，给定该集的状态和奖励对(由环境给定)。将这个添加到<code class="fe mw mx my mz b">Agent</code>类中:</p><pre class="kg kh ki kj gt na mz nb nc aw nd bi"><span id="73ed" class="ne ma iq mz b gy nf ng l nh ni">def update_state_history(self, state, reward):<br/>    self.state_history.append((state, reward))</span><span id="0f79" class="ne ma iq mz b gy nj ng l nh ni">def learn(self):<br/>    target = 0 # we know the "ideal" reward<br/>    a = self.alpha</span><span id="4743" class="ne ma iq mz b gy nj ng l nh ni">    for state, reward in reversed(self.state_history):<br/>        self.G[state] = self.G[state]+ a * (target - self.G[state])</span><span id="a9d1" class="ne ma iq mz b gy nj ng l nh ni">    self.state_history = [] # reset the state_history<br/>    self.random_factor = -= 10e-5 # decrease random_factor</span></pre><p id="152d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你会注意到我们在最后也把<code class="fe mw mx my mz b">random_factor</code>缩小了一点。我们来谈谈那是什么。</p><h2 id="8235" class="ne ma iq bd mb nk nl dn mf nm nn dp mj lf no np ml lj nq nr mn ln ns nt mp nu bi translated">探索与利用</h2><p id="6ae1" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">现在，代理人<em class="lv">可以</em>总是采取它认为会导致最大回报的行动。然而，如果一个行动，代理人估计会有最低的回报，最终却有最高的回报呢？如果一个特定行为的回报随着时间的推移会有回报呢？作为人类，我们能够估计长期回报(“如果我今天不买这部新手机，我就能在未来攒钱买车”)。我们如何为我们的代理复制这一点？</p><p id="baa7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是通常所说的探索与利用的困境。一个总是利用(比如，总是采取它预测会有最高回报的行动)的代理人可能永远不会找到解决问题的更好的办法。然而，一个总是探索的代理(总是随机选择一个选项来看它通向哪里)将会花很长时间来优化自己。因此，大多数奖励算法将结合使用探索和利用。这是我们的<code class="fe mw mx my mz b">Agent</code>课程中的<code class="fe mw mx my mz b">random_factor</code>超参数——在学习过程的开始，代理将探索20%的时间。随着时间的推移，我们可能会减少这个数字，因为随着代理的学习，它可以更好地优化利用，我们可以更快地收敛到一个解决方案。在更复杂的环境中，您可以选择保持相当高的探索率。</p><p id="2289" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们知道了我们的机器人会如何选择一个动作，让我们把它编码到我们的<code class="fe mw mx my mz b">Agent</code>类中。</p><pre class="kg kh ki kj gt na mz nb nc aw nd bi"><span id="9857" class="ne ma iq mz b gy nf ng l nh ni">def choose_action(self, state, allowed_moves):<br/>    next_move = None</span><span id="9bb1" class="ne ma iq mz b gy nj ng l nh ni">    n = np.random.random()<br/>    if n &lt; self.random_factor:<br/>        next_move = np.random.choice(allowed_moves)<br/>    else:<br/>        maxG = -10e15 # some really small random number<br/>        for action in allowed_moves:<br/>            new_state = tuple)[sum(x) for x in zip(state, ACTIONS[action])])<br/>            if self.G[new_state] &gt;= maxG:<br/>                next_move = action<br/>                maxG = self.G[new_state]</span><span id="4cb5" class="ne ma iq mz b gy nj ng l nh ni">    return next_move</span></pre><p id="c728" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们根据我们的<code class="fe mw mx my mz b">random_factor </code>概率随机选择探索或利用。如果我们选择探索，我们从给定的允许移动列表中随机选择我们的下一步移动(传递给函数)。如果我们选择利用，我们循环遍历可能的状态(给定allowed_moves列表)，然后从<em class="lv"> G </em>中找到期望值最大的一个。</p><p id="8fc6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">完美！我们已经完成了代理和环境的代码，但是我们所做的只是创建类。我们还没有检查每一集的工作流程，也没有让代理人学习的方式和时间。</p><h1 id="10c8" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">把所有的放在一起</h1><p id="0659" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">在我们代码的开始，在创建了我们的代理和迷宫之后，我们需要随机初始化<em class="lv"> G </em>。对于我们想玩的每一个游戏，我们的代理应该从环境中获得当前的状态奖励对(记住，除了最后一个方块，每个方块都是-1，应该返回0)，然后用它选择的动作更新环境。它将从环境中接收一个新的状态-奖励对，并在选择下一个动作之前记住更新的状态和奖励。</p><p id="04b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在一集结束后(迷宫无论走了多少步都完成了)，代理应该查看该游戏的状态历史，并使用我们的学习算法更新其<em class="lv"> G </em>表。让我们用伪代码描述一下我们需要做什么:</p><pre class="kg kh ki kj gt na mz nb nc aw nd bi"><span id="84eb" class="ne ma iq mz b gy nf ng l nh ni">Initialize G randomly<br/>Repeat for number of episodes<br/> While game is not over<br/>  Get state and reward from env<br/>  Select action<br/>  Update env<br/>  Get updated state and reward<br/>  Store new state and reward in memory<br/> Replay memory of previous episode to update G</span></pre><p id="9065" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以在以下要点中看到这一点的完整实现:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="91d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这段代码中，我们要求代理玩游戏5000次。我还添加了一些代码来绘制机器人在5000次游戏中每次完成迷宫所需的步数。尝试以不同的学习速率或随机因素运行代码几次，比较机器人收敛到10步解决迷宫需要多长时间。</p><p id="447b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一个挑战是尝试打印出机器人完成迷宫的步骤。在<code class="fe mw mx my mz b">Maze</code>类中的方法<code class="fe mw mx my mz b">print_maze()</code>中有一些起始代码(下面显示了完整的类)，但是您需要添加代码，从代理接收state_history并在打印函数中格式化它，比方说，作为R，用于每一步。这将允许您查看机器人最终决定的步骤-这可能很有趣，因为在我们的迷宫中有多条十步的路线。</p><p id="2773" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">环境和代理的完整代码如下。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nv nw l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="1a2e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lv">我要感谢菲尔·泰伯的精彩课程《动态强化学习》。</em></p></div><div class="ab cl nx ny hu nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="ij ik il im in"><p id="c53a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> Neha Desaraju是德克萨斯大学奥斯丁分校学习计算机科学的学生。你可以在网上</strong><a class="ae kv" href="https://estaudere.github.io" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">estau dere . github . io</strong></a><strong class="ky ir">找到她。</strong></p></div></div>    
</body>
</html>