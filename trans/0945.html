<html>
<head>
<title>The practical guide for Object Detection with YOLOv5 algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">YOLOv5 算法目标检测实用指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-practical-guide-for-object-detection-with-yolov5-algorithm-74c04aac4843#2022-03-14">https://towardsdatascience.com/the-practical-guide-for-object-detection-with-yolov5-algorithm-74c04aac4843#2022-03-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5a2c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">详细教程解释了如何在您自己的自定义数据集上有效地训练对象检测算法 YOLOv5。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9e7a9b4634eeaf39a1dc6cab1a7b5a2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z1OsXK10Fhu_76n-0WXFuA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者标签，国家科学基金会图片，<a class="ae kv" href="http://www.nsf.gov/" rel="noopener ugc nofollow" target="_blank">http://www.nsf.gov/</a></p></figure><h1 id="2cc9" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">简介</strong></h1><p id="bd27" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">识别图像中的物体被认为是人脑的一项普通任务，尽管对机器来说并不那么简单。照片中物体的识别和定位是一项被称为“物体检测”的计算机视觉任务，在过去几年中出现了几种算法来解决这个问题。迄今为止最流行的实时对象检测算法之一是 YOLO(你只看一次)，最初由 Redmond 等人提出。阿尔[1]。</p><p id="c8e6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在本教程中，您将学习使用 Ultralytics 开发的最新 YOLOv5 实现对自定义数据集执行端到端对象检测项目[2]。我们将使用迁移学习技术来训练我们自己的模型，评估它的性能，使用它进行推理，甚至将其转换为其他文件格式，如 ONNX 和 TensorRT。</p><p id="5e41" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">本教程面向具有目标检测算法理论背景的人，他们寻求实际的实施指导。为了您的方便，下面提供了一个易于使用的 Jupiter 笔记本，并附有完整的代码。</p><h1 id="5547" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">数据处理</strong></h1><p id="fcc8" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated"><em class="mp">数据集创建</em></p><p id="d4a7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在本教程中，我从网上手动标记了大约 250 张企鹅的图片和视频，生成了自己的企鹅数据集。我花了几个小时使用 Roboflow 平台，该平台对公众用户友好且免费[3]。为了实现健壮的 YOLOv5 模型，建议每类训练 1500 个以上的图像，每类训练 10，000 个以上的实例。还建议添加高达 10%的背景图像，以减少假阳性错误。由于我的数据集非常小，我将使用迁移学习技术缩小训练过程。</p><p id="cbc9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="mp"> YOLO 标签格式</em></p><p id="a52c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">大多数注释平台支持以 YOLO 标签格式导出，为每个图像提供一个注释文本文件。每个文本文件包含图像中每个对象的一个边界框(BBox)注释。注释被标准化为图像大小，并且在 0 到 1 的范围内。它们以下列格式表示:</p><p id="fb9c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">&lt; object-class-ID&gt; <x center=""> <y center=""> <box width=""> <box height=""/></box></y></x></p><p id="8031" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果图像中有两个对象，YOLO 注释文本文件的内容可能如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/4e54afd81c03e03c22a6c7e3287c123e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H9bFZ0TOhyIVLqI1ozhP6Q.png"/></div></div></figure><p id="cbbb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="mp">数据目录结构</em></p><p id="4bc9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了符合 Ultralytics 目录结构，数据以下列结构提供:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/c5199adf7b62838a5ba5e3c3a11502a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J2UTo9Z2hJCeaTwB1d_aNw.png"/></div></div></figure><p id="f497" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了方便起见，我在笔记本上提供了一个自动创建这些目录的功能，只需将你的数据复制到正确的文件夹中。</p><h1 id="535f" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">配置文件</strong></h1><p id="10f2" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">培训的配置分为三个 YAML 文件，它们随回购协议本身一起提供。我们将根据任务定制这些文件，以满足我们的需求。</p><ol class=""><li id="96ab" class="ms mt iq lq b lr mk lu ml lx mu mb mv mf mw mj mx my mz na bi translated"><strong class="lq ir">数据配置文件</strong>描述了数据集参数。由于我们是在自定义的企鹅数据集上训练，我们将编辑这个文件并提供:训练、验证和测试(可选)数据集的路径；类的数量(NC)；和类名，顺序与它们的索引相同。在本教程中，我们只有一个名为“企鹅”的类。我们将自定义数据配置文件<strong class="lq ir"> </strong>命名为‘penguin _ data . YAML ’,并将其放在‘data’目录下。这个 YAML 文件内容如下:</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/0234730e9aa53e428e7aeefd459e6b05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Igc2asE5sIIqUPY_K9Kexw.png"/></div></div></figure><p id="6e56" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">2.<strong class="lq ir">模型配置文件</strong>规定了模型架构。Ultralytics 支持几种 YOLOv5 架构，命名为 P5 型号，主要因其参数大小而异:YOLOv5n (nano)、YOLOv5s (small)、YOLOv5m (medium)、YOLOv5l (large)、YOLOv5x (extra large)。这些架构适合于用 640*640 像素图像尺寸进行训练。附加系列，针对 1280*1280 的较大图像尺寸的训练进行了优化，称为 P6 (YOLOv5n6、YOLOv5s6、YOLOv5m6、YOLOv5l6、YOLOv5x6)。P6 模型包括一个额外的输出层，用于检测较大的对象。他们从更高分辨率的训练中受益最多，并产生更好的结果[4]。</p><p id="1e1a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">Ultralytics 为上述每个体系结构提供内置的模型配置文件，这些文件位于“models”目录下。如果您是从零开始训练，选择具有所需架构的模型配置<strong class="lq ir"> </strong> YAML 文件(在本教程中为“YOLOv5s6.yaml”)，然后只需将类的数量(nc)参数编辑为自定义数据中正确的类的数量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/78d01365a30511b3b002abb2ec7338e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xDDDWWnx-w_hrP7uVZEdpw.png"/></div></div></figure><p id="5bce" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">当训练像本教程中一样从预训练的权重初始化时，不需要编辑模型配置文件，因为模型将使用预训练的权重提取。</p><p id="fb2c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">3.<strong class="lq ir">超参数配置文件</strong>定义了训练的超参数，包括学习率、动量、损耗、增量等。Ultralytics 在“data/hyp/hyp.scratch.yaml”目录下提供了一个默认的超参数文件。正如我们将在本教程中所做的那样，我们主要建议从默认的超参数开始训练，以建立一个性能基线。</p><p id="e3b7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">YAML 配置文件嵌套在以下目录中:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/804c96d1574fd8ae8a569e22370e359d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ncGGlkWVMaWcB8ccGBSTaA.png"/></div></div></figure><h1 id="920d" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">培训</strong></h1><p id="0b62" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">为了本教程的简单性，我们将训练小参数大小的模型 YOLOv5s6，尽管可以使用更大的模型来改善结果。对于不同的情况，可以考虑不同的训练方法，这里我们将介绍最常用的技术。</p><p id="8c2f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">从零开始训练</strong></p><p id="6dda" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">当拥有足够大的数据集时，模型将从零开始训练中受益最多。权重是通过向权重参数传递空字符串(“”)来随机初始化的。训练由以下命令引发:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/c15b92c71d4c1e20c778fda9711dfa23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mzPShrF4E6qo3B8O4gArVg.png"/></div></div></figure><ul class=""><li id="92ed" class="ms mt iq lq b lr mk lu ml lx mu mb mv mf mw mj nf my mz na bi translated">批次—批次大小(-1 表示自动批次大小)。使用硬件允许的最大批量。</li><li id="e136" class="ms mt iq lq b lr ng lu nh lx ni mb nj mf nk mj nf my mz na bi translated">时期—时期的数量。</li><li id="51f7" class="ms mt iq lq b lr ng lu nh lx ni mb nj mf nk mj nf my mz na bi translated">数据—数据配置文件的路径。</li><li id="baa7" class="ms mt iq lq b lr ng lu nh lx ni mb nj mf nk mj nf my mz na bi translated">cfg 模型配置文件的路径。</li><li id="3e90" class="ms mt iq lq b lr ng lu nh lx ni mb nj mf nk mj nf my mz na bi translated">权重-初始权重的路径。</li><li id="2838" class="ms mt iq lq b lr ng lu nh lx ni mb nj mf nk mj nf my mz na bi translated">缓存—缓存图像以加快训练速度。</li><li id="1896" class="ms mt iq lq b lr ng lu nh lx ni mb nj mf nk mj nf my mz na bi translated">img —以像素为单位的图像大小(默认为 640)。</li></ul><p id="fc09" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">迁移学习</strong></p><p id="3b92" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="mp">预训练模型热启动:</em></p><p id="0706" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">由于我的企鹅数据集相对较小(约 250 张图片)，迁移学习有望产生比从头开始训练更好的结果。Ultralytic 的默认模型是在 COCO 数据集上预先训练的，尽管也支持其他预先训练的模型(VOC、Argoverse、VisDrone、GlobalWheat、xView、Objects365、SKU-110K)。COCO 是一个对象检测数据集，包含来自日常场景的图像。它包含 80 个类，包括相关的“鸟”类，但没有“企鹅”类。通过将模型的名称传递给“weights”参数，我们的模型将使用来自预训练 COCO 模型的权重进行初始化。将自动下载预训练模型。</p><p id="5057" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="mp">特征提取</em></p><p id="9b65" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">模型由两个主要部分组成:作为特征提取器的主干层和计算输出预测的头部层。为了进一步补偿较小的数据集大小，我们将使用与预训练 COCO 模型相同的主干，并且只训练模型的头部。YOLOv5s6 主干由 12 层组成，它们将由‘冻结’参数固定。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/2df31098b3d21ab1c6684d4dbc3d29ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vxmSR1FjjfgyyDrHjdWzDA.png"/></div></div></figure><ul class=""><li id="6d0a" class="ms mt iq lq b lr mk lu ml lx mu mb mv mf mw mj nf my mz na bi translated">权重-初始权重的路径。COCO 模型会自动下载。</li><li id="7de9" class="ms mt iq lq b lr ng lu nh lx ni mb nj mf nk mj nf my mz na bi translated">冻结(freeze )-要冻结的层数</li><li id="50e0" class="ms mt iq lq b lr ng lu nh lx ni mb nj mf nk mj nf my mz na bi translated">项目—项目的名称</li><li id="fb40" class="ms mt iq lq b lr ng lu nh lx ni mb nj mf nk mj nf my mz na bi translated">名称-运行的名称</li></ul><p id="8c76" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果提供了“项目”和“名称”参数，结果会自动保存在那里。否则，它们被保存到“运行/训练”目录。我们可以查看保存到 results.png 文件的指标和损失:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/4fefe1ff8fc407589f618345545673d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GJORca4_qs2QoEfqGbnbvQ.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/bcfb1ae7cabdbe470bf868d482dca111.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XQeZp8enQ-Xu-99aCn8caQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">“特征提取”训练的结果|作者图片</p></figure><p id="abf0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了更好地理解结果，让我们总结一下 YOLOv5 损失和指标。YOLO 损失函数由三部分组成:</p><ol class=""><li id="8928" class="ms mt iq lq b lr mk lu ml lx mu mb mv mf mw mj mx my mz na bi translated"><strong class="lq ir"> box_loss </strong> —包围盒回归损失(均方误差)。</li><li id="0aab" class="ms mt iq lq b lr ng lu nh lx ni mb nj mf nk mj mx my mz na bi translated"><strong class="lq ir"> obj_loss </strong> —物体存在的置信度就是物体损失。</li><li id="99f8" class="ms mt iq lq b lr ng lu nh lx ni mb nj mf nk mj mx my mz na bi translated"><strong class="lq ir"> cls_loss </strong> —分类损失(交叉熵)。</li></ol><p id="8376" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因为我们的数据只有一个类别，所以没有类别错误识别，并且分类错误始终为零。</p><p id="3644" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">精度</strong>测量 bbox 预测正确的程度(真阳性/(真阳性+假阳性))，而<strong class="lq ir">召回</strong>测量真 bbox 预测正确的程度(真阳性/(真阳性+假阴性))。“mAP_0.5”是 IoU(联合交集)阈值为 0.5 时的平均精度(<strong class="lq ir"> mAP </strong>)。“mAP_0.5:0.95”是不同 IoU 阈值的平均 mAP，范围为 0.5 至 0.95。你可以在参考文献[5]中了解更多。</p><p id="7eb7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="mp">微调</em></p><p id="f291" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">训练的最后一个可选步骤是微调，包括解除冻结我们上面获得的整个模型，并以非常低的学习率根据我们的数据重新训练它。通过逐步调整预训练特征以适应新数据，这有可能实现有意义的改进。可以在超参数-配置文件中调整学习率参数。对于教程演示，我们将采用内置的“hyp.finetune.yaml”文件中定义的超参数，它的学习速率比默认的要小得多。权重将使用前一步骤中保存的权重进行初始化。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="b73b" class="nt kx iq np b gy nu nv l nw nx">python train.py --hyp 'hyp.finetune.yaml' --batch 16 --epochs 100 --data 'data/penguins_data.yaml' --weights 'runs_penguins/feature_extraction/weights/best.pt' --project 'runs_penguins' --name 'fine-tuning' --cache</span></pre><ul class=""><li id="db51" class="ms mt iq lq b lr mk lu ml lx mu mb mv mf mw mj nf my mz na bi translated">超级-超级参数配置文件的路径</li></ul><p id="8a3b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">正如我们在下面看到的，在微调阶段，指标和损耗仍在改善。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/e20fa18d48972df5c3c5fd9301d9be92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WFRgRjpN9caEjcMmw34dyw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">“微调”训练的结果|作者提供的图片</p></figure><h1 id="b4b1" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">验证</strong></h1><p id="e886" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">为了评估我们的模型，我们将利用验证脚本。性能可以通过训练、验证或测试数据集分割来评估，由“任务”参数控制。这里，正在评估测试数据集分割:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/1d04a165a848ff521e29a969e6c96eda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zfeJAMU7ILzHekx_rC2Syw.png"/></div></div></figure><p id="666a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们还可以获得精确召回曲线，它会在每次验证时自动保存。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/a47740f4c55fced4dc65f31be18f3130.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OgAGKah5NOGdrDEbwgpImQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">精确度—测试数据的召回曲线分割|作者图片</p></figure><h1 id="41a8" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">推论</strong></h1><p id="6658" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">一旦我们获得了令人满意的训练性能，我们的模型就可以进行推理了。根据推断，我们可以通过应用测试时间增强(TTA)来进一步提高预测的准确性:每个图像都被增强(水平翻转和 3 个不同的分辨率)，最终的预测是所有这些增强的集合。如果我们紧抓每秒帧数(FPS)速率，我们将不得不放弃 TTA，因为它的推论是 2-3 倍长。</p><p id="140d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">用于推断的输入可以是图像、视频、目录、网络摄像头、流甚至 youtube 链接。在下面的检测命令中，测试数据用于推断。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/90c412c72a0146e697f1bf353b213e42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*92CIezUMH4XwBuffPZthEg.png"/></div></div></figure><ul class=""><li id="24e1" class="ms mt iq lq b lr mk lu ml lx mu mb mv mf mw mj nf my mz na bi translated">源—输入路径(0 表示网络摄像头)</li><li id="c8f5" class="ms mt iq lq b lr ng lu nh lx ni mb nj mf nk mj nf my mz na bi translated">权重-权重路径</li><li id="d01b" class="ms mt iq lq b lr ng lu nh lx ni mb nj mf nk mj nf my mz na bi translated">img —用于推断的图像大小，以像素为单位</li><li id="2a60" class="ms mt iq lq b lr ng lu nh lx ni mb nj mf nk mj nf my mz na bi translated">conf —置信度阈值</li><li id="f444" class="ms mt iq lq b lr ng lu nh lx ni mb nj mf nk mj nf my mz na bi translated">iou—NMS 的 IoU 阈值(非最大抑制)</li><li id="48de" class="ms mt iq lq b lr ng lu nh lx ni mb nj mf nk mj nf my mz na bi translated">增强——增强推理(TTA)</li></ul><p id="1cd9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">推理结果会自动保存到定义的文件夹中。让我们回顾一下测试预测的一个例子:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/ef7be956a86cd8492e92ae9c8b8e42e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Ejlh-TKWVtEbV16RYFixg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">推理结果|作者图片</p></figure><h1 id="88e1" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">导出为其他文件格式</h1><p id="4d47" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">至此，我们的模型已经完成，并保存为带有。pt '文件扩展名。模型可以导出为其他文件格式，如 ONNX 和 TensorRT。ONNX 是一种中间的机器学习文件格式，用于在不同的机器学习框架之间进行转换[6]。TensorRT 是 NVIDIA 开发的一个用于优化机器学习模型的库，以在 NVIDIA 图形处理单元(GPU)上实现更快的推理[7]。</p><p id="5be6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">“export.py”脚本用于通过将类型格式应用于“include”参数，将 PyTorch 模型转换为 ONNX、TensorRT engine 或其他格式。以下命令用于将我们的企鹅模型导出到 ONNX 和 TensorRT。这些新的文件格式与 PyTorch 模型保存在同一个“权重”文件夹下。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/4534a12148ad6ac6f3b8d71f613d69d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cbv0fNYEIVfPe9ECWewbUg.png"/></div></div></figure><p id="a3a3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi">_________________________________________________________________</p><h1 id="0bb2" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">感谢您的阅读！</h1><p id="2eda" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated"><strong class="lq ir">想了解更多信息？</strong></p><ul class=""><li id="f2db" class="ms mt iq lq b lr mk lu ml lx mu mb mv mf mw mj nf my mz na bi translated"><a class="ae kv" href="https://medium.com/@lihigurarie" rel="noopener"> <strong class="lq ir">探索</strong> </a>我写的附加文章</li><li id="d318" class="ms mt iq lq b lr ng lu nh lx ni mb nj mf nk mj nf my mz na bi translated"><a class="ae kv" href="https://medium.com/@lihigurarie/subscribe" rel="noopener"> <strong class="lq ir">订阅</strong> </a> <strong class="lq ir"> </strong>在我发布文章时获得通知</li><li id="c8ed" class="ms mt iq lq b lr ng lu nh lx ni mb nj mf nk mj nf my mz na bi translated">关注我的<a class="ae kv" href="https://www.linkedin.com/in/lihi-gur-arie/" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir"> Linkedin </strong> </a></li></ul><p id="562d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">本教程的完整代码在第一个参考资料[0]中提供:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="4206" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> <em class="mp">参考文献</em> </strong></p><p id="aec1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[0]完整代码:<a class="ae kv" href="https://gist.github.com/Lihi-Gur-Arie/41f014bcfbe8b8e1e965fa11a6251e04" rel="noopener ugc nofollow" target="_blank">https://gist . github . com/Lihi-Gur-Arie/41f 014 BCF be 8 b 8e 1e 965 fa 11 a 6251 e 04</a></p><p id="e700" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[1]<a class="ae kv" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1506.02640</a></p><p id="2d62" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><a class="ae kv" href="https://github.com/ultralytics/yolov5" rel="noopener ugc nofollow" target="_blank">https://github.com/ultralytics/yolov5</a></p><p id="5097" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><a class="ae kv" href="https://roboflow.com/" rel="noopener ugc nofollow" target="_blank">https://roboflow.com/</a></p><p id="14ac" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><a class="ae kv" href="https://zenodo.org/record/4679653#.YfFLX3UzaV4" rel="noopener ugc nofollow" target="_blank">https://zenodo.org/record/4679653#.YfFLX3UzaV4</a></p><p id="8d09" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><a class="ae kv" href="https://blog.paperspace.com/mean-average-precision/" rel="noopener ugc nofollow" target="_blank">https://blog.paperspace.com/mean-average-precision/</a></p><p id="16ad" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><a class="ae kv" href="https://onnx.ai/get-started.html" rel="noopener ugc nofollow" target="_blank">https://onnx.ai/get-started.html</a></p><p id="1ee4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><a class="ae kv" href="https://developer.nvidia.com/tensorrt" rel="noopener ugc nofollow" target="_blank">https://developer.nvidia.com/tensorrt</a></p></div></div>    
</body>
</html>