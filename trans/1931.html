<html>
<head>
<title>Principal Component Analysis from the ground up with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于Python的主成分分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-analysis-from-the-ground-up-with-python-754399f88923#2022-05-03">https://towardsdatascience.com/principal-component-analysis-from-the-ground-up-with-python-754399f88923#2022-05-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><h1 id="2eb9" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">0.介绍</h1><blockquote class="ko"><p id="5080" class="kp kq it bd kr ks kt ku kv kw kx ky dk translated">假设你有一堆数据点，你想从中找出模式。主成分分析是一个工具，可以帮助你做到这一点。它会找到数据中最重要的特征，并减少数据的维数。这意味着它需要大量的数据点，并将它们转化为更容易处理的少量数据点。</p></blockquote><p id="e987" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ky im bi translated">在统计学中，主成分分析(PCA)是一种用于降低数据维度的技术。这是一种线性变换形式，其中数据被变换到一个新的坐标系，使得数据在新轴上的任何投影的最大方差出现在第一个轴上，第二个最大方差出现在第二个轴上，依此类推。这种变换由数据的协方差矩阵的特征向量来定义，这些特征向量被称为主分量。换句话说，PCA是一种找到数据变化最大的方向并将数据投射到这些方向上的方法。</p><p id="f55c" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">PCA是一种用于数据分析的强大工具，并被用于各种领域，例如机器学习、图像分析和信号处理。在本文中，我们将对主成分分析做一个简单的介绍，包括它背后的数学概述，以及主成分分析的一些应用。</p><h1 id="c954" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">1.有用的库</h1><p id="8e69" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk md lm ln lo me lq lr ls mf lu lv ky im bi translated">NumPy是Python中科学计算的基础库。它用于:</p><p id="9c6c" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">1.数组</p><p id="99c7" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">2.矩阵</p><p id="3ade" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">3.线性代数</p><p id="f800" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">4.随机数生成</p><p id="d214" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">还有更多！</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi mg"><img src="../Images/6d820b71ab391f875442060a9be21612.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rnQsxMDxtDCLF92xUZgmmA.png"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">Numpy橱窗。作者图片由<a class="ae mw" href="https://twitter.com/carbon_app" rel="noopener ugc nofollow" target="_blank"> @carbon_app </a>创作</p></figure><p id="af78" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated"><strong class="lb iu"> Scikit-learn </strong>是一个免费的Python机器学习库。它用于:</p><p id="6279" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">1.分类</p><p id="d256" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">2.回归</p><p id="ce49" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">3.使聚集</p><p id="939b" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">4.降维</p><p id="7f65" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">5.型号选择</p><p id="bc0a" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">6.预处理</p><p id="9715" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">还有更多！</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi mx"><img src="../Images/44d371adb5663510884ff3b44872244b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Yk5wI5wa0ouSbfiQJRUAg.png"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">sci kit-学习展示。图片作者由<a class="ae mw" href="https://twitter.com/carbon_app" rel="noopener ugc nofollow" target="_blank"> @carbon_app </a>创作</p></figure><h1 id="a867" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">2.主成分分析</h1><p id="cb0a" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk md lm ln lo me lq lr ls mf lu lv ky im bi translated">主成分分析(PCA)是一种用于确定数据集底层结构的统计技术。它通过识别一组相互正交(即垂直)的新轴来实现这一点，并最好地解释数据中的差异。第一个轴解释了最大的方差。第二个轴代表第二大方差，依此类推。</p><p id="a2de" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">主成分分析常用于降维，即减少数据集中变量的数量，同时尽可能保留变量的过程。这是通过将每个数据点仅投影到前几个主成分上来实现的。</p><p id="7ae1" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">数学上，PCA是计算数据协方差矩阵的特征向量的过程。协方差矩阵是描述数据集中每个变量的方差的矩阵。特征向量是数据中代表最大方差的方向。方差是数据点围绕平均值的变化量。</p><p id="8800" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">有许多不同的方法来计算数据集的主成分。一种流行的方法叫做奇异值分解(SVD)。SVD是一种矩阵分解技术，它将一个矩阵分解为三个矩阵:</p><ul class=""><li id="cb14" class="my mz it lb b lc lw lg lx lk na lo nb ls nc ky nd ne nf ng bi translated">左奇异矩阵包含协方差矩阵的特征向量。</li><li id="b8c2" class="my mz it lb b lc nh lg ni lk nj lo nk ls nl ky nd ne nf ng bi translated">右奇异矩阵包含数据矩阵的特征向量。</li><li id="8b46" class="my mz it lb b lc nh lg ni lk nj lo nk ls nl ky nd ne nf ng bi translated">对角矩阵包含协方差矩阵的特征值。</li></ul><p id="e90a" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">SVD用于计算数据集的主成分，因为它在计算上是高效的，并且因为它可以用于解决与PCA相关的许多问题。</p><p id="96b4" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">PCA程序可总结如下:</p><ol class=""><li id="8f16" class="my mz it lb b lc lw lg lx lk na lo nb ls nc ky nm ne nf ng bi translated">将数据居中(即，从每个数据点减去每个变量的平均值)。这是必要的，因为PCA是一个方差最大化的过程，并且将数据居中可以确保第一主成分解释最大可能的方差。</li></ol><p id="3aba" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">1.计算数据的协方差矩阵。</p><p id="a426" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">2.计算协方差矩阵的特征向量和特征值。</p><p id="1012" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">3.按特征值降序排列特征向量。</p><p id="57a8" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">4.选择前k个特征向量，其中k是所需主成分的数量。</p><p id="64b7" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">5.计算数据在所选特征向量上的投影。</p><h2 id="1386" class="nn jr it bd js no np dn jw nq nr dp ka lk ns nt ke lo nu nv ki ls nw nx km ny bi translated">Python实现</h2><p id="1317" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk md lm ln lo me lq lr ls mf lu lv ky im bi translated">您可以将这些行复制/粘贴到您喜欢的IDE中。如果不知道选哪个，我推荐PyCharm。</p><figure class="mh mi mj mk gt ml"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="abc9" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">下面是我们在上面代码中所做工作的逐步解释:</p><ol class=""><li id="b3a7" class="my mz it lb b lc lw lg lx lk na lo nb ls nc ky nm ne nf ng bi translated">我们生成一些数据。</li></ol><p id="525c" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">请注意，我们没有将数据居中，因为平均值已经是0。</p><ol class=""><li id="47c6" class="my mz it lb b lc lw lg lx lk na lo nb ls nc ky nm ne nf ng bi translated"><strong class="lb iu">我们计算数据的实际协方差矩阵。</strong></li><li id="eb9b" class="my mz it lb b lc nh lg ni lk nj lo nk ls nl ky nm ne nf ng bi translated"><strong class="lb iu">我们计算协方差矩阵的特征向量和特征值。</strong></li><li id="8344" class="my mz it lb b lc nh lg ni lk nj lo nk ls nl ky nm ne nf ng bi translated"><strong class="lb iu">我们按照特征值降序排列特征向量。</strong></li><li id="5093" class="my mz it lb b lc nh lg ni lk nj lo nk ls nl ky nm ne nf ng bi translated"><strong class="lb iu">我们选择前两个特征向量，并计算数据在所选特征向量上的投影。</strong></li></ol><p id="6d2c" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">看看我们如何在第35行计算解释的方差比。等于[ <strong class="lb iu"> 0.93134786 </strong>，<strong class="lb iu"> 0.06865214 </strong> ]。这意味着第一个因素单独解释了我们数据中约93%的方差，第二个因素仅增加了6.9%。累积和等于100%，这意味着两个主成分解释了我们数据中的所有方差。</p><p id="dea9" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">在某些情况下，您可能希望只保留解释数据中大部分差异的几个组成部分，因为在现实世界的应用程序中，您通常会有太多的有噪声的要素，这些要素对模型的预测能力没有什么帮助。</p><p id="7b7e" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">6 <strong class="lb iu">。我们将我们的解决方案与scikit-learn </strong>进行比较</p><p id="c346" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">7.<strong class="lb iu">我们绘制数据。</strong></p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ob"><img src="../Images/89e7d89a17ef595b96e266533ea3c5c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NQ3_HJlWIy6w58DpYSzR8g.png"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">图一。作者图片</p></figure><p id="a31d" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">在图1中，特征向量(主成分)被可视化为按照长度排序的直线。每条线都标有编号，并根据组件类型进行颜色编码。这些成分按照它们在数据中解释的差异量排序。如前所述，两者相互正交。</p><p id="d7a0" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">8.<strong class="lb iu">我们从数据中去除最有价值的成分，看看数据会发生什么。</strong></p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi oc"><img src="../Images/9cd810a68ad9144f121a163806a7c72d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pq1_ymyP_l3IZW2ioOgdMQ.png"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">图二。作者图片</p></figure><p id="ffad" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">这是不言自明的。然后所有数据点被投影到第二个轴上。</p><p id="b8ba" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">9.<strong class="lb iu">我们从数据中移除第二大信息量，看看数据会发生什么。</strong></p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi od"><img src="../Images/162447829b668b828940a9cbe969c3e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-F9p16spdx0MwCRUljYKOw.png"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">图3。作者图片</p></figure><p id="162d" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">因此，所有数据点都被投影到解释最大差异的第一个轴上。</p><p id="af7b" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">10.<strong class="lb iu">只是为了好玩，我们比较线性回归。</strong></p><p id="2b0e" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">主成分分析(PCA)和线性回归之间有一些相似之处。这两种方法都可以找到数据中的线性关系。然而，有一些重要的区别。最重要的区别是，PCA找到的是使数据方差最大化的方向，而线性回归找到的是使模型误差最小化的方向(图4)。</p><p id="5229" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">重要的是要记住，PCA的目的不是找到线性回归模型的最佳预测变量，而是找到数据的底层结构，以降低其维数。相反，线性回归是一种从预测变量的线性组合预测定量响应变量的技术。线性方程的系数是代表每个预测变量对响应变量的影响的回归系数。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi oe"><img src="../Images/acda8fe1537e1124f29864716d5ce0a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DgU_3HwZSs5nyg72V3x7Ow.png"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">图4。作者图片</p></figure><h1 id="0f29" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">4.结论</h1><p id="8837" class="pw-post-body-paragraph kz la it lb b lc mb le lf lg mc li lj lk md lm ln lo me lq lr ls mf lu lv ky im bi translated">从代码中我们可以看出，PCA是一个强大的工具，可以用来发现数据集的底层结构。这也是一种计算效率很高的降维方法。</p><p id="cd1f" class="pw-post-body-paragraph kz la it lb b lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls ma lu lv ky im bi translated">在本文中，我们看到了如何使用主成分分析来发现数据集的底层结构。我们还看到了如何使用PCA进行降维，以及如何选择要保留的组件数量。最后，我们看到了PCA与线性回归的关系。</p></div></div>    
</body>
</html>