<html>
<head>
<title>Regularization for Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习的正则化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/regularization-for-machine-learning-67c37b132d61#2022-05-16">https://towardsdatascience.com/regularization-for-machine-learning-67c37b132d61#2022-05-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/0d7d7d9ad4209295a76d7a67b16de140.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3_8Shbns7XYRRT_c"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">杰斯温·托马斯在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="fd6c" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">为什么它是最重要的技术之一，以及如何使用它</h2></div><h1 id="5273" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">这是什么？</h1><p id="6e79" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">正则化是一种通过使损失函数考虑特征重要性来校准机器学习模型的技术。</p><p id="ef87" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">直观地说，这意味着我们迫使我们的模型对那些在预测目标变量时不太重要的特征给予较小的权重，而对那些更重要的特征给予较大的权重。</p><h1 id="5113" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">为什么有用？</h1><p id="9e96" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在机器学习中，可能会有“过度拟合”的趋势，这取决于我们拥有的数据和我们使用的模型。也就是说，我们的模型可能“过于努力”来适应训练数据，然后它将无法在以后进行归纳(以在新数据上很好地工作)。</p><p id="4aa3" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这是一个与好模型相比过度拟合的直观示例，也是一个拟合不足的示例:</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mr"><img src="../Images/457b74a64f7c194114d39b440e52f22e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8dXH4HZeSMs7XcyxoyHdEA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片作者(改编自<a class="ae jg" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html" rel="noopener ugc nofollow" target="_blank"> sklearn示例</a>)</p></figure><p id="b05e" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">当我们没有太多数据时，当我们必须拥有许多特征时，或者当我们的模型太复杂时，这可能会发生。</p><blockquote class="mw mx my"><p id="e804" class="lq lr mz ls b lt mm kk lv lw mn kn ly na mo mb mc nb mp mf mg nc mq mj mk ml im bi translated">经验法则:数据越少，就越应该保持模型简单(参数越少，特性越少)</p></blockquote><p id="f68d" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">最基本的正则化技术是<em class="mz"> L1 </em>和<em class="mz"> L2 </em>。我们将详细了解它们是如何工作的，然后对其他技术进行概述。</p><h1 id="d85f" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">L1对L2</h1><p id="4f00" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">L1和L2提到了这些正则化(也分别称为套索和岭回归)所使用的规范类型，以及回归中添加到损失函数的项。它们之间的差别很微妙，但却很重要。</p><p id="5621" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">首先，让我们简要回顾一下回归方程:</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/475305d2a2e735713d7dee3de6817cc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:214/format:webp/1*HfGl-wJ6fYZQNnWLQ-K_rA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">回归方程式</p></figure><p id="3fb1" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">其中，ŷ是我们的预测值，x是特征，β和α是模型的截距。在回归模型中，我们的工作是找到最佳的β和α。我们所说的最好是什么意思？将损失函数最小化的那些:</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/f8d91983af2ae620e683061808c16c01.png" data-original-src="https://miro.medium.com/v2/resize:fit:210/format:webp/1*Vyovw_XmDeOS-1K6_VCl3A.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">基本损失函数</p></figure><p id="55d8" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">其中n是观测值的数量，ŷ是预测值，y是目标变量的实际值。</p><p id="50c3" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">损失函数简单地计算预测值(ŷ)和实际值(y)之间的差值的平方，并对它们求和。</p><p id="2045" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">现在让我们看看这个函数是如何进行L1回归的:</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/edda7b29d673fd114b72726e363d328a.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*sBD1HBfx_Mph1H1v3drV8A.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">带L1正则项的损失函数</p></figure><p id="cfbb" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">新术语(绿色，以防您没有注意到)有两个主要部分:lambda，它是正则化参数(它越大，我们的正则化就越强)和|β|，它是回归参数的绝对值。本质上，它所做的是增加高参数的成本，因此为了给参数分配更大的值，必须通过显著提高预测质量来补偿。</p><p id="859f" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">另一方面，L2回归是这样计算损失的:</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/886993c3e83a161c0d76d715404fd7fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*RaQQ7bMlBCh-8NdA0PoEMQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">带有L2正则项的损失函数</p></figure><p id="a9a0" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">区别很微妙:我们取β的平方，而不是绝对值。</p><p id="1f7e" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在L1，如果各个特征对整个模型的贡献不足，βj可以等于0。然而，在L2，β可以很小，但不等于0。</p><p id="fe1d" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在L1和L2之间做出选择会有什么影响？</p><p id="ce3c" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">首先，因为L1可以赋予权重等于0，这实质上意味着我们可以摆脱不太重要的功能，使我们的模型更简单，数据摄取过程更容易。另一方面，L2在计算上花费较少(因为L2范数有一个封闭形式的解)。</p><p id="7655" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">顺便说一下，如果你想把两者结合起来，你有<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html" rel="noopener ugc nofollow" target="_blank">弹性网</a>选项，它增加了两个正则化项，并允许你给一个或另一个更多的权重。</p><p id="be70" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">你应该使用多少正则化？这将取决于您的数据集和模型。我建议你做一些超参数调整，通过测试正则化参数的多个值，看看哪一个产生最好的误差。让我们看看如何使用Python来实现这一点。</p><h1 id="29b3" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">如何使用它</h1><p id="78ef" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">Lasso、Ridge和ElasticNet回归都可以在scikit-learn上获得，只需几行代码就可以轻松使用。</p><p id="04c8" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我添加了一些代码，可以帮助您实现它们，还可以评估正则化强度的最佳值，在scikit-learn中称为<em class="mz"> alpha </em>(但相当于我们之前示例中的λ。</p><p id="346c" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我使用了使用sk learn . datasets . make _ regression生成的假数据，以及训练数据的标准名称(<em class="mz"> X_train </em>和<em class="mz"> y_train </em>)，但是您可以很容易地根据您的数据和代码调整它:</p><pre class="ms mt mu mv gt nh ni nj nk aw nl bi"><span id="9a4d" class="nm kz jj ni b gy nn no l np nq">from sklearn.linear_model import Lasso, Ridge, ElasticNet</span><span id="7096" class="nm kz jj ni b gy nr no l np nq">alphas = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]</span><span id="db63" class="nm kz jj ni b gy nr no l np nq"># LASSO<br/>errors = []<br/>for alpha in alphas:<br/>    clf = <strong class="ni jk">Lasso(alpha=alpha)</strong><br/>    clf.fit(X_train, y_train)<br/>    y_hat = clf.predict(X_test)<br/>    mse = mean_squared_error(y_test, y_hat)<br/>    errors.append(mse)<br/>plt.plot(np.log10(alphas), errors)<br/>plt.show()</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/5f66f24bb17ac82f6bff4581ff3f34e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*OnlP7R-gxb3JF_qjLJVZQg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Lasso回归的MSE与log(alpha )(图片由作者提供)</p></figure><p id="7cb2" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我绘制<em class="mz"> log(alpha) </em>而不是<em class="mz"> alpha </em>的原因是因为它的比例呈指数变化，所以在对数比例中可视化变得更容易。</p><p id="2a6f" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">当然，Ridge和ElasticNet也可以类似的方式使用:</p><pre class="ms mt mu mv gt nh ni nj nk aw nl bi"><span id="386a" class="nm kz jj ni b gy nn no l np nq"># RIDGE<br/>errors = []<br/>for alpha in alphas:<br/>    clf = <strong class="ni jk">Ridge(alpha=alpha)</strong><br/>    clf.fit(X_train, y_train)<br/>    y_hat = clf.predict(X_test)<br/>    mse = mean_squared_error(y_test, y_hat)<br/>    errors.append(mse)<br/>plt.plot(np.log10(alphas), errors)<br/>plt.show()</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/b89e7dc243f7a08dc8c26b43bdb356aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*qaSpYJn44uVMSfDyTorNGw.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">岭回归的MSE与log(alpha )(图片由作者提供)</p></figure><pre class="ms mt mu mv gt nh ni nj nk aw nl bi"><span id="6b88" class="nm kz jj ni b gy nn no l np nq"># ElasticNet<br/>errors = []<br/>for alpha in alphas:<br/>    clf = <strong class="ni jk">ElasticNet(alpha=alpha)</strong><br/>    clf.fit(X_train, y_train)<br/>    y_hat = clf.predict(X_test)<br/>    mse = mean_squared_error(y_test, y_hat)<br/>    errors.append(mse)<br/>plt.plot(np.log10(alphas), errors)<br/>plt.show()</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/1105573b33aa16bde72338040d42b508.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*wnSOfi2d-svCATh2pxxiSQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">弹性网络回归的MSE与log(alpha )(图片由作者提供)</p></figure><p id="b6d1" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">从这些图中，您可以找到将最小化模型误差的回归类型和alpha。</p><h1 id="7890" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">其他技术</h1><h2 id="e9e7" class="nm kz jj bd la nt nu dn le nv nw dp li lz nx ny lk md nz oa lm mh ob oc lo od bi translated">提前停止</h2><p id="6985" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">早期停止可用于迭代方法，如梯度下降，它包括在达到某个目标后停止训练过程。这个目标可以是某个精度度量的给定值，或者只是迭代次数。换句话说，你是在告诉模型，如果它表现得足够好，或者如果它走得太远，就停止训练过程。它常用于神经网络和梯度增强方法。</p><p id="f1cb" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">早期停止起到正则化的作用，因为它降低了模型超参数将针对特定训练数据进行优化而不能很好地泛化的风险。</p><h2 id="5fff" class="nm kz jj bd la nt nu dn le nv nw dp li lz nx ny lk md nz oa lm mh ob oc lo od bi translated">拒绝传统社会的人</h2><p id="9628" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">Dropout是训练神经网络时随机丢弃节点的技术(尽管它可以适用于其他方法)。在每个时期，一定百分比的节点没有被使用，这给过程增加了一些噪声，并迫使模型更好地适应和概括。在XGBoost中,“DART”booster还建议在训练过程中随机删除树，以减少过度拟合。</p></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><p id="911f" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如果你喜欢这篇文章，你可能也会喜欢这些:</p><div class="is it gp gr iu ol"><a rel="noopener follow" target="_blank" href="/a-new-better-version-of-the-k-nearest-neighbors-algorithm-12af81391682"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd jk gy z fp oq fr fs or fu fw ji bi translated">K-最近邻算法的一个新的更好的版本</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">通过使用正则化来改善KNN的结果</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">towardsdatascience.com</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz ja ol"/></div></div></a></div><div class="is it gp gr iu ol"><a rel="noopener follow" target="_blank" href="/hypothesis-testing-a23852264d09"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd jk gy z fp oq fr fs or fu fw ji bi translated">假设检验</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">它是什么以及如何在Python中实现它</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">towardsdatascience.com</p></div></div><div class="ou l"><div class="pa l ow ox oy ou oz ja ol"/></div></div></a></div></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><blockquote class="mw mx my"><p id="56c1" class="lq lr mz ls b lt mm kk lv lw mn kn ly na mo mb mc nb mp mf mg nc mq mj mk ml im bi translated">如果你想进一步讨论，请随时通过LinkedIn 联系我，这将是我的荣幸(老实说)。</p></blockquote></div></div>    
</body>
</html>