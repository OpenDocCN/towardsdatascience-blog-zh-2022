<html>
<head>
<title>Bayesian Deep Learning &amp; Estimating Uncertainty</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯深度学习&amp;估计不确定性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-deep-learning-estimating-uncertainty-9907f5208cc0#2022-07-13">https://towardsdatascience.com/bayesian-deep-learning-estimating-uncertainty-9907f5208cc0#2022-07-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7994" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">天气数据；任意和认知的不确定性</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/49fe45c9279f1c2d7e33e342a315fbb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5-WJR9iLcZNh3R6oIX8wgQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">湿度是温度的函数。更多细节见正文。(来源:作者)</p></figure><p id="7fc2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">深度神经网络(DNNs)的性能依赖于从大量数据中逐步构建和提取特征的能力。尽管这些深度模型通常是自适应的，但是性能取决于数据分布。鲁棒性在各种应用中是重要的，例如计算机视觉任务，例如自动驾驶，因为室外环境可能自然变化。在这一点上，我们希望知道预测正确的可能性有多大，这可以通过在模型中加入不确定性估计来实现。</p><p id="1f02" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本帖中，我们将使用公开可用的真实<a class="ae lu" href="https://www.kaggle.com/datasets/muthuj7/weather-dataset?sort=votes" rel="noopener ugc nofollow" target="_blank">天气数据</a>，并构建简单的线性和非线性模型，不仅找到最佳模型，还找到不确定性估计。你可以从这篇文章中学到/回顾到什么—</p><ol class=""><li id="6200" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">使用<a class="ae lu" href="https://www.tensorflow.org/probability" rel="noopener ugc nofollow" target="_blank">张量流概率</a>库开始贝叶斯深度学习。</li><li id="1a19" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">两种不同类型的不确定性估计:<em class="mj">任意的</em>和<em class="mj">认知的</em>不确定性。</li><li id="7351" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">网络权重有多确定(<em class="mj">认知</em>不确定性)，什么是变分后验概率？</li></ol><p id="fe78" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">参考资料中给出的链接提供了完整的笔记本。让我们开始吧，不要耽搁。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h2 id="1de4" class="mr ms it bd mt mu mv dn mw mx my dp mz lh na nb nc ll nd ne nf lp ng nh ni nj bi translated">1.加载和预处理数据:</h2><p id="6f0d" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">我使用的是从2006年到2016年的10年间测量的<a class="ae lu" href="https://www.kaggle.com/datasets/muthuj7/weather-dataset" rel="noopener ugc nofollow" target="_blank">天气数据</a>(在CC0许可下可公开获得)。为简单起见，我将只使用温度和湿度值，我们的目标是发现湿度如何作为温度的函数而变化，并建立一个模型，该模型不仅预测行为，还为我们提供模型(&amp;数据)不确定性信息。选择了所需的列后，我们的数据框架如下所示—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/ea8f67f6ea65a05a85d04f494c2ce7ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D1Hxc4I95Y9Tcyq1YjMVYA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们数据的前几个条目</p></figure><p id="9205" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们所看到的，在10年的时间里，每小时都进行测量，产生了超过95，000个数据点。为了进一步简化，我们对数据帧进行了重新采样，以降低输入数据的频率，我选择了3天的时间间隔，而不是1小时的时间间隔，结果得到了1340个数据点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/3fd0340db6d87ac46affa1d38c0152e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KpHW4eL7V2QsVy6TE8uUsA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">以3天而不是1小时的频率重新采样数据。</p></figure><p id="9494" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在可以在散点图中绘制湿度与温度的函数关系，如下所示——</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/d22d6342fb632aa0ff6246837b55c3f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u7W34NR4DIbCCi2kFxGpWA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图:1。湿度是温度的函数。</p></figure><p id="1a1a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们看到湿度(相对)在大约0.4到1之间变化，温度在大约-10到大约30摄氏度之间变化，似乎有一种逆线性关系。我们将数据分成训练测试集，并将温度值标准化。这样，我们就为下一步做好了准备，建立模型并量化不确定性。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h2 id="26c3" class="mr ms it bd mt mu mv dn mw mx my dp mz lh na nb nc ll nd ne nf lp ng nh ni nj bi translated">2.线性回归:</h2><p id="f287" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">在处理任何不确定性之前，让我们先做一个简单的回归。简单的确定性回归给出了一个点估计，即对于一个输入值，我们得到一个预测值。对于这项任务，我们假设X数据与Y数据呈线性关系，噪声项呈正态分布。我们的模型通过最小化均方误差(MSE损失)来学习，本质上是在我们的统计建模假设下最大化数据的可能性。我们可以很容易地在TensorFlow中建立这样一个回归网络，并检查如下预测—</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码块1:具有单一单元的密集层的简单线性回归。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/6a9cf8d87c9f5d0c532591ee83f1fc0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hl3CT57QCYmxu2G7xXUVbQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:使用TensorFlow &amp; Python的线性回归模型和预测。</p></figure></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h2 id="4402" class="mr ms it bd mt mu mv dn mw mx my dp mz lh na nb nc ll nd ne nf lp ng nh ni nj bi translated">任意的和认知的不确定性:</h2><p id="080a" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">在我们深入研究概率回归之前，让我们简单讨论一下两种常见的不确定性:任意的和认知的不确定性。随机不确定性捕获了观测中固有的噪声，导致即使我们有更多的数据也无法减少的不确定性。另一方面，认知不确定性说明了模型参数的不确定性，如果获得更多数据，这种不确定性可以减少。认知不确定性是指决策者的无知(在这种情况下是深度神经网络)，而不是任何潜在的随机/随机过程。</p><h2 id="f761" class="mr ms it bd mt mu mv dn mw mx my dp mz lh na nb nc ll nd ne nf lp ng nh ni nj bi translated">3.1.任意不确定性:可训练均值</h2><p id="0442" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">使用张量流概率库为回归任务模拟随机不确定性相当容易。在这里，我们的想法是捕捉数据中的固有噪声，并开始训练一个模型，该模型返回一个正态分布(与确定性线性回归和点估计非常不同)，具有可训练的平均值，但标准偏差=1(参见代码块2)。这个想法是，在训练之后，我们的模型将能够复制原始的数据分布。为了使用密集层的输出来模拟正态分布，我使用了TensorFlow Probability的<code class="fe nu nv nw nx b">DistributionLambda</code>层，它返回一个分布对象。这就是为什么我们使用负对数似然作为损失函数，而不是MSE损失。训练后，可以从学习到的分布中取样，并绘制平均&amp;标准偏差，如下所示—</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码块2:表示输入数据分布的模型(具有可训练的平均值，但方差固定)。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2d2cea19174e9dbf02ff33aba91d357d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jXjcTw1wlLkk40rf4ITRIg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:绘制了从学习的分布(左)生成的样本以及相应的平均值和标准偏差。</p></figure><p id="5d20" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于只有平均值是可训练的，并且我们将标准偏差(stddev)固定为1，我们看到平均值的2σ线相距太远。</p><p id="2bc8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这可以通过在之前的模型中添加stddev作为可学习参数来简单解决。现在，我们将使用两个单元(用于平均值和标准偏差)而不是一个只有一个单元的密集层。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h2 id="e1bd" class="mr ms it bd mt mu mv dn mw mx my dp mz lh na nb nc ll nd ne nf lp ng nh ni nj bi translated">3.2.任意不确定性:可训练的均值和方差；</h2><p id="27c5" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">为了在先前模型中添加可训练方差，我们可以使用具有<code class="fe nu nv nw nx b">DistrbutionLambda</code>层的先前码块，但是在TF概率库中，我们具有能够直接包括先前密集层输出的随机性的<code class="fe nu nv nw nx b"><a class="ae lu" href="https://www.tensorflow.org/probability/api_docs/python/tfp/layers/IndependentNormal" rel="noopener ugc nofollow" target="_blank">IndependentNormal</a></code>层。让我们来看看这个模型—</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码块3:类似于代码块2，但是这里的模型具有可训练的均值和方差。</p></figure><p id="588f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe nu nv nw nx b">IndependentNormal</code>层中的<code class="fe nu nv nw nx b">event_shape</code>参数告诉我们所需的参数数量，对<code class="fe nu nv nw nx b">event_shape=1</code>来说是2；这就是为什么<code class="fe nu nv nw nx b">Dense</code>层有2个单位(均值和方差)。为了指定密集层中的单元数量，我们使用了<code class="fe nu nv nw nx b">IndependentNormal</code>层的静态方法<code class="fe nu nv nw nx b">params_size</code>。</p><p id="6e92" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">绘制学习分布的样本和相应的均值和标准差，如下所示—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/07e40e849413ba9785650bec6c120f8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7jsmJinaTTs6HoSl9tXQgA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4:与图3相同，但是现在我们的模型学习了原始数据分布的平均值和标准偏差。</p></figure><p id="938d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不仅是线性模型，我们还可以添加一个具有非线性激活函数的密集层来包含非线性。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码块4:用非线性密集层扩展代码块3。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/49fe45c9279f1c2d7e33e342a315fbb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5-WJR9iLcZNh3R6oIX8wgQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5:与图4相同，但是代替线性模型，我们假设非线性模型来表示数据分布。</p></figure></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h2 id="9f01" class="mr ms it bd mt mu mv dn mw mx my dp mz lh na nb nc ll nd ne nf lp ng nh ni nj bi translated">4.认知不确定性:</h2><p id="c4c0" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">到目前为止，我们试图通过在模型的最后一层添加一些分布来模拟数据的一般随机性(即潜在噪声)，即最终预测是分布中的随机变量。</p><p id="0f80" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">认知不确定性捕捉了DNN模型参数的不确定性。为了捕捉神经网络中认知的不确定性(<em class="mj"> NN </em>)，我们将先验分布置于其权重之上，例如，高斯先验分布:<em class="mj">W</em>∞N(0，<em class="mj"> I </em>)。将DNN权重表示为概率分布而不是点估计的想法是在“<a class="ae lu" href="https://arxiv.org/pdf/1505.05424.pdf" rel="noopener ugc nofollow" target="_blank">神经网络</a>中的权重不确定性”论文中提出的。我们从权重的一些先验分布开始，并随着网络看到更多数据而更新，以获得后验分布。对于多元正态分布，有可能得到后验的精确形式。一般来说，不可能确定后验概率的解析形式，因此变分法就变得有用了。</p><p id="61dc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">给定训练数据，<em class="mj">P</em>(<em class="mj">w</em>|<em class="mj">D</em>)的情况下，神经网络的贝叶斯推理计算权重的后验分布。<em class="mj">变分贝叶斯</em>方法使用称为<em class="mj">变分后验</em>的第二个函数来近似后验分布。该函数具有已知的函数形式，因此避免了精确确定后验概率P  ( <em class="mj"> w </em> | <em class="mj"> D </em>)的需要。为了避免选择差的变分后验概率的风险，这个近似函数，即变分后验概率由参数θ来参数化，参数θ被调整以使得该函数尽可能地近似原始后验概率(P( <em class="mj"> w </em> | <em class="mj"> D </em>))。变分学习找到权重分布的参数<em class="mj">θ</em><em class="mj">q</em>(<em class="mj">w</em>|<em class="mj">θ</em>)，该参数最小化权重的Kullback-Leibler (KL)散度与真实贝叶斯后验概率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/346063b69146ef5a656ff2d9941279a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MYZ9Z83GPmd9VnPQCcrhOg.png"/></div></div></figure><p id="e782" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最小化KL散度将我们引向证据下限(ELBO)。关于ELBO的更多信息，你可以查看我在参考资料中列出的笔记本，也可以查看关于<a class="ae lu" href="https://medium.com/towards-data-science/latent-variables-expectation-maximization-algorithm-fb15c4e0f32c" rel="noopener">期望最大化(EM)算法</a>的详细帖子。</p><p id="c178" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们在张量流中实现变分后验的方法是通过<code class="fe nu nv nw nx b"><a class="ae lu" href="https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DenseVariational" rel="noopener ugc nofollow" target="_blank">DenseVariationalLayer</a></code>层，文档定义如下—</p><blockquote class="nz oa ob"><p id="ddad" class="ky kz mj la b lb lc ju ld le lf jx lg oc li lj lk od lm ln lo oe lq lr ls lt im bi translated">这一层使用变分推理来拟合分布在<code class="fe nu nv nw nx b">kernel</code>矩阵和<code class="fe nu nv nw nx b">bias</code>项上的“替代物”,这些项以类似于<code class="fe nu nv nw nx b"><a class="ae lu" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense" rel="noopener ugc nofollow" target="_blank">tf.keras.layers.Dense</a></code>的方式使用。</p></blockquote><p id="ded0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们使用这一层之前，我们需要实现固定的先验分布和可训练的后验分布，这将在<code class="fe nu nv nw nx b">DenseVariationalLayer</code>中使用。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码块5:考虑简单回归网络中的权重不确定性。</p></figure><p id="b449" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看在代码块5中发生了什么:<em class="mj">先验</em>:我们将先验权重定义为具有对角协方差矩阵的多元正态分布(<em class="mj"> N </em> (0，1))，作为一个<code class="fe nu nv nw nx b">sequential</code>，并且没有可训练的参数。<em class="mj">后验:</em>这里权重是可训练的(<code class="fe nu nv nw nx b">VariableLayer</code> ) &amp;遵循具有全协方差矩阵的多元正态分布(<code class="fe nu nv nw nx b">MultivariateNormalTril</code>)。<em class="mj">模型</em>:最后我们使用<code class="fe nu nv nw nx b">DenseVariational</code>层定义一个模型，并使用先验和后验定义。KL散度根据训练样本的数量进行加权，你可以在笔记本上找到原因。在我们用MSE损失训练这个模型之后，我们可以绘制回归线如下—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/4acf02ae88fc28c40369edc0dfd7ccdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Cu8ndRI79f3HsvKjTSFOg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6:认知的不确定性:这里，每条线代表从后验分布中随机抽取的不同模型参数。</p></figure><p id="fc15" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每次我们调用这个模型，我们都会得到一个稍微不同的结果，你可以从那5行中看到。不同的斜率表明我们的模型不确定温度和湿度的线性相关性。<code class="fe nu nv nw nx b">DenseVariational</code>本质上定义了模型的集合，在这种情况下，我们可以从这5个不同的调用中获得平均值。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h2 id="02d3" class="mr ms it bd mt mu mv dn mw mx my dp mz lh na nb nc ll nd ne nf lp ng nh ni nj bi translated">5.任意+认知的不确定性:</h2><p id="7f51" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">最后，我们将创建一个模型，它可以结合随机和认知的不确定性，这是很容易做到的，我们只需要添加一个<code class="fe nu nv nw nx b">IndependentNormal</code>层到以前的模型(代码块5)来考虑标签(湿度)分布的随机性质。让我们看看下面的代码块—</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="2b1d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦经过训练，我们就可以像以前一样从模型中为不同的呼叫(认知不确定性)进行采样，如下所示</p><pre class="kj kk kl km gt og nx oh oi aw oj bi"><span id="2b4a" class="mr ms it nx b gy ok ol l om on">for _ in range(2):</span><span id="99d6" class="mr ms it nx b gy oo ol l om on">   y_model = model_non_lin_al_ep(X_test_scaled)</span><span id="3fe4" class="mr ms it nx b gy oo ol l om on">   y_hat = y_model.mean()</span><span id="3cd2" class="mr ms it nx b gy oo ol l om on">   y_hat_m2sd = y_hat — 2 * y_model.stddev()</span><span id="959e" class="mr ms it nx b gy oo ol l om on">   y_hat_p2sd = y_hat + 2 * y_model.stddev()</span></pre><p id="56a8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">绘制这两个调用的均值和标准差如下所示—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/06655b9eb0f82845d528c138c387be2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LhkL4JDe9BTSXxt7RVg50g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7:与之前的图相同，但是这里我们有任意的和认知的不确定性:两条不同的线代表对模型的两个不同的调用(代表认知的不确定性)。</p></figure></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="a8ba" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们学习了贝叶斯神经网络(BNN)的基本构建模块，特别是与认知不确定性相关的模块。如果我们将数据集表示为X = {x1，…，xN }，Y = {y1，…，yN }，那么贝叶斯推断用于计算权重p(W|X，Y)的后验概率。这里我们通过一个简单的例子，我们知道确切的后验分布，只需要找到最佳参数。虽然认知的不确定性可以减少给定更多的数据，随机的不确定性是不可减少的。使用真实世界的天气数据，我们开发了一个简单的模型，适用于超越点估计和预测不确定性的回归任务。我希望这里用来建立这些模型的概念能给我们提供关于不确定性的信息，这将是有用的，你可以根据你的任务修改它们。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h2 id="c461" class="mr ms it bd mt mu mv dn mw mx my dp mz lh na nb nc ll nd ne nf lp ng nh ni nj bi translated">参考资料:</h2><p id="a2fa" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">[1] ' <a class="ae lu" href="https://link.springer.com/article/10.1007/s10994-021-05946-3" rel="noopener ugc nofollow" target="_blank">'机器学习中的任意和认知不确定性</a> ': E. Hüllermeier，W. Waegeman</p><p id="ba27" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2] ' <a class="ae lu" href="https://arxiv.org/abs/1703.04977" rel="noopener ugc nofollow" target="_blank">在计算机视觉的贝叶斯深度学习中我们需要哪些不确定性</a>？':a .肯德尔，Y. Gal。</p><p id="4553" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[3] <a class="ae lu" href="https://www.tensorflow.org/probability/examples/Probabilistic_Layers_Regression" rel="noopener ugc nofollow" target="_blank"> TFP层:概率回归</a>。</p><p id="eed2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[4] ' <a class="ae lu" href="https://arxiv.org/pdf/1505.05424.pdf" rel="noopener ugc nofollow" target="_blank">神经网络中的权重不确定性</a> ': C. Bundell等人</p><p id="b817" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[4]代码、概念和数学笔记本:<a class="ae lu" href="https://github.com/suvoooo/Learn-TensorFlow/blob/master/TF-Proba/Bayesian_uncertainty.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub链接</a></p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="b220" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="mj">如果你对更深入的基础机器学习概念感兴趣，可以考虑加盟Medium使用</em> </strong> <a class="ae lu" href="https://saptashwa.medium.com/membership" rel="noopener"> <strong class="la iu"> <em class="mj">我的链接</em> </strong> </a> <strong class="la iu"> <em class="mj">。你不用额外付钱，但我会得到一点佣金。感谢大家！！</em> </strong></p></div></div>    
</body>
</html>