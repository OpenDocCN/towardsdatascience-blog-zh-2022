<html>
<head>
<title>How Neural Network Works — with Worked Example (Neural Network Series) — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络如何工作—实例(神经网络系列)—第2部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feed-forward-neural-network-with-example-neural-network-series-part-2-eeca7a081ef5#2022-05-09">https://towardsdatascience.com/feed-forward-neural-network-with-example-neural-network-series-part-2-eeca7a081ef5#2022-05-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="038b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">本文将构建到本系列的前一部分。我们将涵盖前馈神经网络(FF-NN)，集中讨论神经网络所做的计算。</p><p id="2aa8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">本系列的前一部分:<a class="ae ko" rel="noopener" target="_blank" href="/the-basics-of-neural-networks-neural-network-series-part-1-4419e343b2b">神经网络的基础知识(神经网络系列)——第一部分</a></p><div class="kp kq gp gr kr ks"><a rel="noopener follow" target="_blank" href="/the-basics-of-neural-networks-neural-network-series-part-1-4419e343b2b"><div class="kt ab fo"><div class="ku ab kv cl cj kw"><h2 class="bd iu gy z fp kx fr fs ky fu fw is bi translated">神经网络基础(神经网络系列)——第一部分</h2><div class="kz l"><h3 class="bd b gy z fp kx fr fs ky fu fw dk translated">神经网络</h3></div><div class="la l"><p class="bd b dl z fp kx fr fs ky fu fw dk translated">(神经网络系列)——第一部分神经Networkstowardsdatascience.com</p></div></div><div class="lb l"><div class="lc l ld le lf lb lg lh ks"/></div></div></a></div></div><div class="ab cl li lj hx lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="im in io ip iq"><h1 id="2b1c" class="lp lq it bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">神经网络设计(概述)</h1><p id="03c5" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">神经网络是由许多层叠的神经元组成的系统。第一层，输入层(我们称之为第0层)除了传递输入值之外不执行任何计算。因此，在计算神经网络的层数时，我们忽略输入层。因此，下面的图1是一个两层网络。</p><p id="3110" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输出层计算网络的最终输出。输入层和输出层之间的层称为隐藏层。下面图1中的神经网络被描述为3–4–1网络，输入层有3个单元，输入层有4个单元，输出值为1。</p><p id="f207" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">神经网络的层数决定了网络的深度。基于此，具有多个隐含层的神经网络被称为深度神经网络。</p><p id="83b6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过层数和每层中神经元的数量来解释神经网络的设计，通常被称为神经网络的<strong class="js iu">架构。我们在本系列中交替使用这些术语(神经网络设计和神经网络架构)。</strong></p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ms"><img src="../Images/6a251b70049d636538677bf3e7dbacc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4SauDfA57nXN7Mwz59rL8A.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">图1:一个有3个输入特征的神经网络，一个有4个节点的隐藏层，一个单值输出。节点是密集连接的——每个节点都连接到上一层的所有神经元。每个连接都有权重，表示任意两个节点之间的连接强度(来源:作者)。</p></figure></div><div class="ab cl li lj hx lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="im in io ip iq"><h1 id="24f3" class="lp lq it bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">前馈神经网络</h1><p id="e382" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">前馈网络，也称为前向传递，逼近某个函数<br/> y=f( <strong class="js iu"> x </strong> |θ)的输入值<strong class="js iu"> x </strong>和已知输出y。网络学习θ中最逼近函数f的参数，以建立良好的映射ŷ=f( <strong class="js iu"> x </strong> |θ)。ŷ是一个预测的模型。在NN中，<strong class="js iu"> w </strong>，<strong class="js iu">b</strong>∈θ——也就是我们在模型训练时优化的参数是2个权重(<strong class="js iu"> w </strong>)和偏差(<strong class="js iu"> b </strong>)。</p><p id="4230" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">前馈神经网络的关键特征</strong> —前馈网络只允许信息单向流动(无反馈回路或连接)。</p><p id="5bf4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">定义:多层感知器(MLP) <br/> </strong> MLP是前馈NN的特例。在MLP中，所有节点都是密集连接的，也就是说，每个神经元/节点都连接到上一层的所有节点。事实上，图1中的神经网络是一个多层感知器。</p></div><div class="ab cl li lj hx lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="im in io ip iq"><h1 id="b7e2" class="lp lq it bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">前馈神经网络(FF-NN)-示例</h1><p id="3cf1" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">本节将展示如何执行由FF-NN完成的计算。本节要掌握的基本概念是描述不同参数和变量的符号，以及实际计算是如何进行的。我们将使用图1所示的神经网络结构。</p><h2 id="addb" class="nh lq it bd lr ni nj dn lv nk nl dp lz kb nm nn md kf no np mh kj nq nr ml ns bi translated">1.建筑和符号</h2><p id="12c4" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">让我们重新绘制图1，以显示神经网络的基本变量和参数。注意，在这个图中(下面的图2)，节点之间的其他连接被删除了，只是为了使图不那么混乱；否则，这个神经网络是一个多层感知器(任何两个相邻层中的所有节点都是互连的)。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nt"><img src="../Images/e33e8d2577233e5893eb1762eb8fd088.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*We2ySc706dfHSzRnRH666Q.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">图2:带有变量和参数的3–4–1神经网络我们需要在给定输入值的情况下计算输出ŷ(来源:作者)。</p></figure><p id="c9fd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中:</p><ul class=""><li id="3f01" class="nu nv it js b jt ju jx jy kb nw kf nx kj ny kn nz oa ob oc bi translated">x⁰ᵢ-iᵗʰ输入值。输入层(层0)的特征I的值，</li><li id="c7fb" class="nu nv it js b jt od jx oe kb of kf og kj oh kn nz oa ob oc bi translated">wˡⱼᵢ <strong class="js iu"> </strong> —权重来自第l-1层的神经元I到当前第l层的神经元j，</li><li id="9d46" class="nu nv it js b jt od jx oe kb of kf og kj oh kn nz oa ob oc bi translated">fˡⱼ —层l中单元j的输出。这成为下一层(层l+1)中单元的输入，</li><li id="a782" class="nu nv it js b jt od jx oe kb of kf og kj oh kn nz oa ob oc bi translated">第一层中jᵗʰ神经元的zˡⱼ加权输入，</li><li id="0100" class="nu nv it js b jt od jx oe kb of kf og kj oh kn nz oa ob oc bi translated">bˡⱼ<strong class="js iu"/>——l层jᵗʰ神经元上的偏置，</li><li id="db9f" class="nu nv it js b jt od jx oe kb of kf og kj oh kn nz oa ob oc bi translated">nˡ——第一层神经元的数量，</li><li id="0e7b" class="nu nv it js b jt od jx oe kb of kf og kj oh kn nz oa ob oc bi translated">f <strong class="js iu"> ˡ </strong> ⱼ <strong class="js iu"/></li><li id="2b4e" class="nu nv it js b jt od jx oe kb of kf og kj oh kn nz oa ob oc bi translated">l —给定的层。对于，l =0，1，…，L。在我们的例子中，神经网络为2层，因此L=2(记得我们说过不计算输入层)</li></ul><p id="c2e8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以下是一些解释参数和变量的示例:</p><ul class=""><li id="f2ef" class="nu nv it js b jt ju jx jy kb nw kf nx kj ny kn nz oa ob oc bi translated">x⁰₂ —第二个输入值，</li><li id="ebde" class="nu nv it js b jt od jx oe kb of kf og kj oh kn nz oa ob oc bi translated">w₄₃——从第0层中的神经元3到第1层中的神经元4的连接的权重，</li><li id="acab" class="nu nv it js b jt od jx oe kb of kf og kj oh kn nz oa ob oc bi translated">z ₃ —第1层中单元3的加权输入，</li><li id="f425" class="nu nv it js b jt od jx oe kb of kf og kj oh kn nz oa ob oc bi translated">g(z₃)-在z ₃.上应用激活函数g</li><li id="6fd6" class="nu nv it js b jt od jx oe kb of kf og kj oh kn nz oa ob oc bi translated">每层中的单元数-输入层为3个单元(n⁰=3)，隐藏层有4个单元(n =4)，最后一层为一个神经元(n =1)，以及</li><li id="ee17" class="nu nv it js b jt od jx oe kb of kf og kj oh kn nz oa ob oc bi translated">b .₁——第2层(本例中为输出层)中神经元1的偏置。</li></ul><h2 id="9197" class="nh lq it bd lr ni nj dn lv nk nl dp lz kb nm nn md kf no np mh kj nq nr ml ns bi translated">2.数据</h2><p id="6a48" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">我们将在本例中使用的数据包含<strong class="js iu"> 3个特征G1、G2和G2 </strong>(顺便说一下，这就是我们在架构的输入层选择3个神经元的原因)<strong class="js iu">目标为通过</strong>(我们称之为<code class="fe oi oj ok ol b">y</code>)，分别为0或1，表示失败和通过。这意味着我们正在处理一个二元分类问题。</p><p id="c3b5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://github.com/kipronokoech/MediumDotCom/blob/main/data/student-mat-pass-or-fail.csv" rel="noopener ugc nofollow" target="_blank">原始数据</a>包含30个特征和395行(数据点)，但是我们将只使用3个特征来有效地显示必要的计算。我们将在本系列的后面处理整个数据集。</p><p id="3b76" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">同样，为了使概念易于理解，我们将展示如何在单次正向传递中通过网络传递单个数据点(训练示例)。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi om"><img src="../Images/a00b14522ca573c10853d90b60351897.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SV5SLiXScknnZW6fm7U27g.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">在我们的示例中，我们将考虑索引2处的训练示例(来源:作者)。</p></figure><h2 id="e7aa" class="nh lq it bd lr ni nj dn lv nk nl dp lz kb nm nn md kf no np mh kj nq nr ml ns bi translated">3.参数初始化</h2><p id="7e44" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">我们将用0到1之间的值随机初始化权重，而bias将总是用0初始化。我们将在后面的系列文章中对此进行更多的讨论，请记住，权重最好初始化为0–1，而不是零，偏差最好在开始时保持为0。</p><h2 id="e63a" class="nh lq it bd lr ni nj dn lv nk nl dp lz kb nm nn md kf no np mh kj nq nr ml ns bi translated">4.隐含层神经元的计算</h2><p id="9190" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">对于隐藏层中的第一个神经元，我们需要计算f ₁，这意味着我们需要三个权重w ₁₁、w ₁₂和w ₁₃.的初始值</p><p id="aab7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们初始化他们如下w ₁₁=0.3，w ₁₂=0.8和w ₁₃=0.62.如前所述，我们将设置偏差，b ₁=0.让我们引入一个称为整流线性单位(ReLU)的激活函数，我们将使用它作为函数g(这只是一个任意的选择)。如前所述，我们将在后面讨论更多关于激活函数的内容，但是现在，我们可以定义ReLU并使用它。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi on"><img src="../Images/6f8efb90e28e91cd549bf83ddf11854f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*0TfkguocqjqfoTQI4l9kDw.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">ReLU激活功能。</p></figure><p id="a822" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi oo"><img src="../Images/35bab4499cae99cf305b36a46ae1997e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tTQwOZH6NBngsCLtXXPPxQ.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">对层=1的第一个神经元的计算。</p></figure><p id="f598" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以用同样的方法计算f ₂，f ₃和f ₄，假设</p><ul class=""><li id="41d2" class="nu nv it js b jt ju jx jy kb nw kf nx kj ny kn nz oa ob oc bi translated">w·₂₁=0.9，w·₂₂=0.1，w·₂₃=0.1和b·₂=0，计算f·₂，</li><li id="0127" class="nu nv it js b jt od jx oe kb of kf og kj oh kn nz oa ob oc bi translated">w·₃₁=0.7、w·₃₂=0.2、w·₃₃=0.4和b·₃=0，用于f·₃的计算，</li><li id="88e7" class="nu nv it js b jt od jx oe kb of kf og kj oh kn nz oa ob oc bi translated">w·₄₁=0.01，w·₄₂=0.5，w·₄₃=0.2和b·₄=0，计算f·₄.</li></ul><p id="cc69" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">确认₂=8.1，₃=10.5和₄=6.07.</p><h2 id="b07e" class="nh lq it bd lr ni nj dn lv nk nl dp lz kb nm nn md kf no np mh kj nq nr ml ns bi translated">5.最后一层的计算</h2><p id="195b" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">在最后一层，让我们初始化参数(权重)和偏差:w ₁₁=0.58，w ₁₂=0.1，w ₁₃=0.1，w ₁₄=0.42，和b ₁=0.</p><p id="8426" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">关于该层中的激活函数-由于我们正在处理二元分类，我们可以使用sigmoid/logistic函数，因为该函数输出0到1之间的值，因此可以解释为类的预测概率。Sigmoid函数定义为:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi op"><img src="../Images/d024ecec7521992113e7857cf3123144.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*a8Uo2_9zbhL41CiUlgJh_Q.png"/></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">Sigmoid函数</p></figure><p id="2256" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi oq"><img src="../Images/71d9e21dedfed760d18aa485af6ca6ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HpH_ERp8IM_SmdYocn4G0g.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">网络最后一层的计算。</p></figure><p id="4b53" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于最后一层的输出是由Sigmoid生成的(范围从0到1)，所以结果可以解释为一个概率。y ₀=0.999997502意味着通过的可能性几乎是1。从数据来看，真值为1，意味着正向传递使预测正确。然而，这不仅仅是一个巧合，因为没有训练。权重是随机生成的，所有偏差值都设置为0。</p></div><div class="ab cl li lj hx lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="im in io ip iq"><p id="cd6f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我希望这篇文章为理解神经网络中发生的计算打下良好的基础。这是对系列文章上一篇文章<a class="ae ko" rel="noopener" target="_blank" href="/the-basics-of-neural-networks-neural-network-series-part-1-4419e343b2b">的补充。本系列的下一篇文章</a><a class="ae ko" rel="noopener" target="_blank" href="/how-neural-networks-actually-work-python-implementation-simplified-a1167b4f54fe">在这里</a> <strong class="js iu">。</strong></p><p id="d982" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请<a class="ae ko" href="https://medium.com/@kiprono_65591/membership" rel="noopener">以每月5美元的价格注册medium会员资格</a>，以便能够阅读我和其他作者在medium上的所有文章。</p><p id="f8d6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你也可以<a class="ae ko" href="https://medium.com/subscribe/@kiprono_65591" rel="noopener">订阅，以便在我发表文章时将我的文章发送到你的邮箱</a>。</p><p id="9830" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">感谢您的阅读，欢迎下次光临！！！</p></div></div>    
</body>
</html>