<html>
<head>
<title>Implementing Gradient Descent in Python from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始在 Python 中实现梯度下降</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-gradient-descent-in-python-from-scratch-760a8556c31f#2022-02-18">https://towardsdatascience.com/implementing-gradient-descent-in-python-from-scratch-760a8556c31f#2022-02-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d187" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从头开始用代码实现梯度下降算法，了解它的工作原理。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/58c9e8aaac3d4e402ea9bb79f3bdba7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*QZXjuWZdGLnYioL58vtYfA.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated"><strong class="bd kr">作者图片(使用 python 中的 matplotlib 创建)</strong></p></figure><p id="bd1a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">机器学习模型可能有几个特征，但是一些特征可能比其他特征对输出有更高的影响。例如，如果模型预测公寓价格，公寓的位置对输出的影响可能比公寓建筑的层数更大。因此，我们提出了权重的概念。每个特征都与一个权重(一个数字)相关联，即特征对输出的影响越大，与之相关联的权重就越大。但是，如何决定应该给每个特征分配多大的权重呢？这就是梯度下降的用武之地。</p><p id="1dd9" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">梯度下降是一种优化算法，可以帮助您找到模型的最佳权重。它通过尝试各种权重并找到最适合模型的权重来实现，即最小化成本函数。成本函数可以定义为实际产量和预测产量之间的差异。因此，成本函数越小，模型的预测输出就越接近实际输出。成本函数在数学上可以定义为:</p><pre class="kg kh ki kj gt lo lp lq lr aw ls bi"><span id="18f9" class="lt lu iq lp b gy lv lw l lx ly"><strong class="lp ir">𝑦=𝛽+θnXn,</strong> <br/>where x is the parameters(can go from 1 to n), 𝛽 is the             bias and θ is the weight</span></pre><p id="d64a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">而另一方面，梯度下降的学习率被表示为<strong class="ku ir"> α。</strong>学习率是每个梯度所走的步的大小。虽然较大的学习率可能会给我们的<strong class="ku ir"> 𝛽 </strong>和<strong class="ku ir"> θ </strong>的优化值较差，但学习率也可能太小，这需要大量增加获得收敛点(𝛽和θ的最佳值点)所需的迭代次数。该算法给出α <strong class="ku ir">、</strong> 𝛽和θ的值作为输出。</p><p id="db13" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">如果你想更详细的了解梯度下降和代价函数，我推荐<a class="ae lz" rel="noopener" target="_blank" href="/machine-leaning-cost-function-and-gradient-descend-75821535b2ef">这篇文章</a>。</p><p id="a877" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">现在我们知道了什么是梯度下降以及它是如何工作的，让我们开始用 python 实现它。但是，在我们开始相同的代码逻辑之前，让我们先来看看我们将要使用的数据和为了我们的目的将要导入的库。</p><p id="5d55" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">对于这个实现，我们将使用广告数据集。这是一个数据集，它给出了不同产品在电视、广播和报纸上销售后的总销售额。使用我们的算法，我们可以找出哪种媒体对我们的销售表现最好，并相应地为所有媒体分配权重。该数据集可从以下链接下载:</p><blockquote class="ma mb mc"><p id="af80" class="ks kt md ku b kv kw jr kx ky kz ju la me lc ld le mf lg lh li mg lk ll lm ln ij bi translated"><a class="ae lz" href="https://www.kaggle.com/sazid28/advertising.csv" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/sazid28/advertising.csv</a></p></blockquote><p id="9963" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">转到导入，我们将使用库 pandas 和 numpy，它们将用于读取数据以及对数据执行数学函数。我们还将使用 matplotlib 和 seaborn 来绘制我们的发现，并以图形方式解释结果。一旦我们导入了库，我们将使用 pandas 来读取我们的数据集，并打印前 5 列以确保数据被正确读取。一旦我们读取了数据，我们将为我们的梯度下降曲线设置 X &amp; Y 变量。</p><pre class="kg kh ki kj gt lo lp lq lr aw ls bi"><span id="57c1" class="lt lu iq lp b gy lv lw l lx ly">import pandas as pd</span><span id="2137" class="lt lu iq lp b gy mh lw l lx ly">import numpy as np</span><span id="7c35" class="lt lu iq lp b gy mh lw l lx ly">import matplotlib.pyplot as plt</span><span id="b953" class="lt lu iq lp b gy mh lw l lx ly">import seaborn as sn</span><span id="860b" class="lt lu iq lp b gy mh lw l lx ly">df=pd.read_csv('Advertising.csv')</span><span id="d2b4" class="lt lu iq lp b gy mh lw l lx ly">df.head()</span></pre><p id="90d2" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">输出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mi"><img src="../Images/955e36e643966b3604dcea7f11e9a9f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*avEBTNmC-LFjjxRNAQ1cBQ.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">按作者分类的图像(数据集的前 5 行)</p></figure><p id="1342" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">x 代表电视、广播和报纸，而 Y 代表我们的销售。由于所有这些销售可能在不同的规模，然后我们正常化我们的 X &amp; Y 变量。</p><pre class="kg kh ki kj gt lo lp lq lr aw ls bi"><span id="195a" class="lt lu iq lp b gy lv lw l lx ly">X=df[[‘TV’,’Radio’,’Newspaper’]]</span><span id="7abf" class="lt lu iq lp b gy mh lw l lx ly">Y=df[‘Sales’]</span><span id="a563" class="lt lu iq lp b gy mh lw l lx ly">Y=np.array((Y-Y.mean())/Y.std())</span><span id="fcf1" class="lt lu iq lp b gy mh lw l lx ly">X=X.apply(lambda rec:(rec-rec.mean())/rec.std(),axis=0)</span></pre><p id="95e9" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">一旦我们有了一个标准化的数据集，我们就可以开始定义我们的算法。要实现梯度下降算法，我们需要遵循 4 个步骤:</p><ol class=""><li id="3453" class="mn mo iq ku b kv kw ky kz lb mp lf mq lj mr ln ms mt mu mv bi translated">随机初始化偏差和权重θ</li><li id="c90f" class="mn mo iq ku b kv mw ky mx lb my lf mz lj na ln ms mt mu mv bi translated">给定偏差和权重，计算 Y 的预测值</li><li id="0bea" class="mn mo iq ku b kv mw ky mx lb my lf mz lj na ln ms mt mu mv bi translated">根据 Y 的预测值和实际值计算成本函数</li><li id="52a9" class="mn mo iq ku b kv mw ky mx lb my lf mz lj na ln ms mt mu mv bi translated">计算梯度和权重</li></ol><p id="50fa" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">首先，我们将为偏差和权重取一个随机值，它可能实际上接近最佳偏差和权重，也可能离得很远。</p><pre class="kg kh ki kj gt lo lp lq lr aw ls bi"><span id="a1ae" class="lt lu iq lp b gy lv lw l lx ly">import random</span><span id="c8b6" class="lt lu iq lp b gy mh lw l lx ly">def initialize(dim):</span><span id="de68" class="lt lu iq lp b gy mh lw l lx ly">b=random.random()</span><span id="2b64" class="lt lu iq lp b gy mh lw l lx ly">theta=np.random.rand(dim)</span><span id="7782" class="lt lu iq lp b gy mh lw l lx ly">return b,theta</span><span id="f5ec" class="lt lu iq lp b gy mh lw l lx ly">b,theta=initialize(3)</span><span id="c9cf" class="lt lu iq lp b gy mh lw l lx ly">print(“Bias: “,b,”Weights: “,theta)</span></pre><p id="dfe5" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">输出:</p><pre class="kg kh ki kj gt lo lp lq lr aw ls bi"><span id="f991" class="lt lu iq lp b gy lv lw l lx ly">Bias: 0.4439513186463464 Weights: [0.92396489 0.05155633 0.06354297]</span></pre><p id="a15a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这里，我们创建了一个名为 initialise 的函数，它为我们提供了一些偏差和权重的随机值。我们使用随机库来给出符合我们需要的随机数。下一步是使用这些权重和偏差计算输出(Y)。</p><pre class="kg kh ki kj gt lo lp lq lr aw ls bi"><span id="8d7a" class="lt lu iq lp b gy lv lw l lx ly">def predict_Y(b,theta,X):</span><span id="6e3a" class="lt lu iq lp b gy mh lw l lx ly">return b + np.dot(X,theta)</span><span id="b0cd" class="lt lu iq lp b gy mh lw l lx ly">Y_hat=predict_Y(b,theta,X)</span><span id="c7e9" class="lt lu iq lp b gy mh lw l lx ly">Y_hat[0:10]</span></pre><p id="3a59" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">输出:</p><pre class="kg kh ki kj gt lo lp lq lr aw ls bi"><span id="e436" class="lt lu iq lp b gy lv lw l lx ly">array([ 3.53610908, 0.53826004, 1.4045012 , 2.27511933, 2.25046604, 1.6023777 , -0.36009639, -0.37804669, -2.20360351, 0.69993066])</span></pre><p id="fad4" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">Y_hat 是预测的输出值，而 Y 将是实际值。这两者之间的差异将给出我们的成本函数。这将在我们下一个函数中计算。</p><pre class="kg kh ki kj gt lo lp lq lr aw ls bi"><span id="da96" class="lt lu iq lp b gy lv lw l lx ly">import math</span><span id="1def" class="lt lu iq lp b gy mh lw l lx ly">def get_cost(Y,Y_hat):</span><span id="210d" class="lt lu iq lp b gy mh lw l lx ly">Y_resd=Y-Y_hat</span><span id="eba4" class="lt lu iq lp b gy mh lw l lx ly">return np.sum(np.dot(Y_resd.T,Y_resd))/len(Y-Y_resd)</span><span id="0450" class="lt lu iq lp b gy mh lw l lx ly">Y_hat=predict_Y(b,theta,X)</span><span id="e0a2" class="lt lu iq lp b gy mh lw l lx ly">get_cost(Y,Y_hat)</span></pre><p id="d683" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">输出:</p><pre class="kg kh ki kj gt lo lp lq lr aw ls bi"><span id="445c" class="lt lu iq lp b gy lv lw l lx ly">0.5253558445651124</span></pre><p id="413c" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这是我们的成本函数，我们的目的是尽可能地减少这个，以获得最准确的预测。为了获得更新的偏差和权重，我们使用梯度下降公式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nb"><img src="../Images/ecfdc38ef0edeac651b8e753373052f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OVgmDqccd2uFMMbNsFZIkQ.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图片作者(更新的 theta 公式)，</p></figure><p id="9aa5" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">传递给该函数的参数有</p><ol class=""><li id="1471" class="mn mo iq ku b kv kw ky kz lb mp lf mq lj mr ln ms mt mu mv bi translated">x，y:输入和输出变量</li><li id="beed" class="mn mo iq ku b kv mw ky mx lb my lf mz lj na ln ms mt mu mv bi translated">y_hat:带有当前偏差和权重的预测值</li><li id="14dc" class="mn mo iq ku b kv mw ky mx lb my lf mz lj na ln ms mt mu mv bi translated">b_0，θ_ 0:电流偏置和权重</li><li id="2b92" class="mn mo iq ku b kv mw ky mx lb my lf mz lj na ln ms mt mu mv bi translated">学习率:调整更新步长的学习率</li></ol><pre class="kg kh ki kj gt lo lp lq lr aw ls bi"><span id="b30f" class="lt lu iq lp b gy lv lw l lx ly">def update_theta(x,y,y_hat,b_0,theta_o,learning_rate):</span><span id="fe6c" class="lt lu iq lp b gy mh lw l lx ly">db=(np.sum(y_hat-y)*2)/len(y)</span><span id="5b03" class="lt lu iq lp b gy mh lw l lx ly">dw=(np.dot((y_hat-y),x)*2)/len(y)</span><span id="ce16" class="lt lu iq lp b gy mh lw l lx ly">b_1=b_0-learning_rate*db</span><span id="a283" class="lt lu iq lp b gy mh lw l lx ly">theta_1=theta_o-learning_rate*dw</span><span id="4161" class="lt lu iq lp b gy mh lw l lx ly">return b_1,theta_1</span><span id="0e0f" class="lt lu iq lp b gy mh lw l lx ly">print("After initialization -Bias: ",b,"theta: ",theta)</span><span id="8947" class="lt lu iq lp b gy mh lw l lx ly">Y_hat=predict_Y(b,theta,X)</span><span id="df18" class="lt lu iq lp b gy mh lw l lx ly">b,theta=update_theta(X,Y,Y_hat,b,theta,0.01)</span><span id="e90b" class="lt lu iq lp b gy mh lw l lx ly">print("After first update -Bias: ",b,"theta: ",theta)</span><span id="1ea3" class="lt lu iq lp b gy mh lw l lx ly">get_cost(Y,Y_hat)</span></pre><p id="9c09" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">输出:</p><pre class="kg kh ki kj gt lo lp lq lr aw ls bi"><span id="0c8b" class="lt lu iq lp b gy lv lw l lx ly">After initialization -Bias:  0.4733071000028529 theta:  [0.56745402 0.43024717 0.0722811 ]<br/>After first update -Bias:  0.46384095800279584 theta:  [0.57117721 0.43202382 0.07171437]</span><span id="0b85" class="lt lu iq lp b gy mh lw l lx ly">0.37245638135702513</span></pre><p id="9a02" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">如我们所见，新的偏差和权重降低了成本函数。目前我们将学习率定为 0.001，我们也将尝试 0.01，看看它是否能成为更好、更优的学习率。</p><p id="4b72" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">现在我们已经创建了所有需要的函数，我们可以创建一个主梯度下降函数，它运行它们一个特定的迭代次数，并为我们找到最佳的偏差和权重。对于这个实现，我们运行这个函数 200 次迭代。</p><pre class="kg kh ki kj gt lo lp lq lr aw ls bi"><span id="09d0" class="lt lu iq lp b gy lv lw l lx ly">def run_gradient_descent(X,</span><span id="78b4" class="lt lu iq lp b gy mh lw l lx ly">Y,</span><span id="547f" class="lt lu iq lp b gy mh lw l lx ly">alpha,</span><span id="4816" class="lt lu iq lp b gy mh lw l lx ly">num_iterations):</span><span id="edb4" class="lt lu iq lp b gy mh lw l lx ly">b,theta=initialize(X.shape[1])</span><span id="2229" class="lt lu iq lp b gy mh lw l lx ly">iter_num=0</span><span id="c7e0" class="lt lu iq lp b gy mh lw l lx ly">gd_iterations_df=pd.DataFrame(columns=[‘iteration’,’cost’])</span><span id="52d6" class="lt lu iq lp b gy mh lw l lx ly">result_idx=0</span><span id="7162" class="lt lu iq lp b gy mh lw l lx ly">for each_iter in range(num_iterations):</span><span id="7b82" class="lt lu iq lp b gy mh lw l lx ly">Y_hat=predict_Y(b,theta,X)</span><span id="29bb" class="lt lu iq lp b gy mh lw l lx ly">this_cost=get_cost(Y,Y_hat)</span><span id="f27c" class="lt lu iq lp b gy mh lw l lx ly">prev_b=b</span><span id="594b" class="lt lu iq lp b gy mh lw l lx ly">prev_theta=theta</span><span id="a7c2" class="lt lu iq lp b gy mh lw l lx ly">b,theta=update_theta(X,Y,Y_hat,prev_b,prev_theta,alpha)</span><span id="a5a5" class="lt lu iq lp b gy mh lw l lx ly">if(iter_num%10==0):</span><span id="6457" class="lt lu iq lp b gy mh lw l lx ly">gd_iterations_df.loc[result_idx]=[iter_num,this_cost]</span><span id="c2e5" class="lt lu iq lp b gy mh lw l lx ly">result_idx=result_idx+1</span><span id="81f3" class="lt lu iq lp b gy mh lw l lx ly">iter_num +=1</span><span id="ac37" class="lt lu iq lp b gy mh lw l lx ly">print(“Final Estimate of b and theta : “,b,theta)</span><span id="1ee4" class="lt lu iq lp b gy mh lw l lx ly">return gd_iterations_df,b,theta</span><span id="cc6b" class="lt lu iq lp b gy mh lw l lx ly">gd_iterations_df,b,theta=run_gradient_descent(X,Y,alpha=0.001,num_iterations=200)</span></pre><p id="643c" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">输出:</p><pre class="kg kh ki kj gt lo lp lq lr aw ls bi"><span id="7b3a" class="lt lu iq lp b gy lv lw l lx ly">Final Estimate of b and theta : 0.31516040347417285 [0.39731522 0.31571747 0.61334115]</span></pre><p id="2f2d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">theta 是 3 个数字的列表，因为我们有 3 个输入，电视，广播和报纸。如果我们打印每次迭代的成本函数，我们可以看到成本函数的减少。我们也可以绘制迭代的成本函数来查看结果。</p><pre class="kg kh ki kj gt lo lp lq lr aw ls bi"><span id="b294" class="lt lu iq lp b gy lv lw l lx ly">gd_iterations_df[0:10]</span></pre><p id="db1c" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">输出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/ffa9b315f4c9256a59887873d619f380.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*iuRzVL_9kUS0v2s7bh_hSw.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">按作者分类的图像(迭代的输出)</p></figure><pre class="kg kh ki kj gt lo lp lq lr aw ls bi"><span id="8fdc" class="lt lu iq lp b gy lv lw l lx ly">%matplotlib inline</span><span id="5f68" class="lt lu iq lp b gy mh lw l lx ly">plt.plot(gd_iterations_df[‘iteration’],gd_iterations_df[‘cost’])</span><span id="63a9" class="lt lu iq lp b gy mh lw l lx ly">plt.xlabel(“Number of iterations”)</span><span id="79f0" class="lt lu iq lp b gy mh lw l lx ly">plt.ylabel(“Cost or MSE”)</span></pre><p id="9592" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">输出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/b62cf3d74c1daa806046e7ef415628ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*6aHxZEebP5Kp78wAFVV1Rw.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">按作者分类的图像(成本函数与迭代)</p></figure><p id="cfd4" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">正如我们可以看到的，成本函数随着迭代次数的增加而减少，但我们仍然没有达到收敛。现在，让我们尝试使用α=0.01 进行 2000 次迭代，并将其与α=0.001 进行比较，找出哪个学习率对于该数据集更好。</p><pre class="kg kh ki kj gt lo lp lq lr aw ls bi"><span id="b2b6" class="lt lu iq lp b gy lv lw l lx ly">alpha_df_1,b,theta=run_gradient_descent(X,Y,alpha=0.01,num_iterations=2000)</span><span id="3ae2" class="lt lu iq lp b gy mh lw l lx ly">alpha_df_2,b,theta=run_gradient_descent(X,Y,alpha=0.001,num_iterations=2000)</span><span id="1740" class="lt lu iq lp b gy mh lw l lx ly">plt.plot(alpha_df_1[‘iteration’],alpha_df_1[‘cost’],label=”alpha=0.01")</span><span id="2240" class="lt lu iq lp b gy mh lw l lx ly">plt.plot(alpha_df_2[‘iteration’],alpha_df_2[‘cost’],label=”alpha=0.001")</span><span id="c855" class="lt lu iq lp b gy mh lw l lx ly">plt.legend()</span><span id="835c" class="lt lu iq lp b gy mh lw l lx ly">plt.ylabel(‘cost’)</span><span id="96ab" class="lt lu iq lp b gy mh lw l lx ly">plt.xlabel(‘Number of iterations’)</span><span id="68ea" class="lt lu iq lp b gy mh lw l lx ly">plt.title(‘Cost Vs. Iterations for different alpha values’)</span></pre><p id="8eba" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">输出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/9fa15589b088b4ad7a9594269a74b091.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*xlZPvGn3smfQK3TDMPWxcQ.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><p id="d76e" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">可以看出，0.01 是更优的学习速率，因为它比 0.001 收敛得更快。0.01 在 100 大关附近收敛，而 0.001 需要 1000 次迭代才能达到收敛。</p><p id="47e0" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">因此，我们成功地在 python 上构建了梯度下降算法。记住，学习率的最佳值对于每个数据集都是不同的。</p><p id="3e3f" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">希望你今天学到了新的有意义的东西。</p><p id="7685" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">谢谢你。</p></div></div>    
</body>
</html>