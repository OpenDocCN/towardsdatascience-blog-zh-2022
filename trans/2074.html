<html>
<head>
<title>PySpark Data Skew in 5 Minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark数据在5分钟内出现偏差</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-skew-in-pyspark-783d529a9dd7#2022-05-10">https://towardsdatascience.com/data-skew-in-pyspark-783d529a9dd7#2022-05-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ecc8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">正是您需要的，仅此而已</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2795cdf33dad27056278a7ab9f06e21f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*66EyphcA23oEmkLR"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">约翰·巴克托在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="9787" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有很多关于数据偏斜的过于复杂的帖子，这是一个看似简单的话题。在这篇文章中，我们将在5分钟内讲述必要的基础知识。</p><p id="187e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章的主要来源是<a class="ae kv" href="https://www.amazon.com/Spark-Definitive-Guide-Processing-Simple-ebook/dp/B079P71JHY" rel="noopener ugc nofollow" target="_blank"> Spark:权威指南</a>，这里是<a class="ae kv" href="https://github.com/mberk06/DS_academic_papers/blob/master/39_spark_partitions.py" rel="noopener ugc nofollow" target="_blank">代码</a>。</p><p id="36b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们开始吧…</p><h1 id="8a0a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">什么是数据不对称？</h1><p id="10df" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在spark中，数据被分成行块，然后存储在worker节点上，如图1所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/b2f072620579355ff0ca17cbb9a0b044.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6O9_mcCTGCFlBRiCQkwzsw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:数据分区如何存储在spark中的例子。图片作者。</p></figure><p id="a12b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个单独的数据“块”被称为一个<strong class="ky ir">分区</strong>，一个给定的工作者可以拥有任意数量、任意大小的分区。但是，最好将数据均匀分布，以便每个工人都有等量的数据要处理。</p><p id="eb9c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">当工人之间的数据不平衡时，我们称之为“偏斜数据”</strong></p><p id="5d49" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们的机器有8个worker节点，而不是上面显示的3个，那么一个完美分布的数据集在每个节点上会有相同数量的行，如图2左侧所示。另一方面，具有不对称的数据集在一些内核上有大量数据，而在其他内核上只有很少的数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/fc4b04f8e670214f4d114b04c71b2957.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ECBAapN6VdobAvZCHaRBw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:均匀(左)与不均匀(右)数据偏斜。图片作者。</p></figure><h1 id="282c" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">我为什么在乎？</h1><p id="6c7f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">很好，我们知道什么是不对称，但它对我们的应用有什么影响？数据不对称的一些常见结果是…</p><ul class=""><li id="a049" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated"><strong class="ky ir">运行缓慢的阶段/任务:</strong>某些操作会花费很长时间，因为给定的工作人员正在处理太多的数据。</li><li id="eeb5" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><strong class="ky ir">将数据溢出到磁盘</strong>:如果数据不适合工人的内存，它将被写到磁盘<a class="ae kv" href="https://medium.com/road-to-data-engineering/spark-performance-optimization-series-2-spill-685126e9d21f" rel="noopener">，这将花费更长的时间</a>。</li><li id="f989" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><strong class="ky ir">内存不足错误</strong>:如果worker用完了磁盘空间，就会抛出一个错误。</li></ul><blockquote class="nf"><p id="e582" class="ng nh iq bd ni nj nk nl nm nn no lr dk translated">数据不对称意味着计算和内存资源的利用率不均衡。</p></blockquote><h1 id="155c" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw np jx me jz nq ka mg kc nr kd mi mj bi translated">如何判断我的数据是否有偏差？</h1><p id="5e0f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如果你正在经历上述的一些症状，并且预期你的数据是偏斜的，你可以使用下面的方法来诊断。</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="5ec3" class="nx lt iq nt b gy ny nz l oa ob">import pyspark.sql.functions as F<br/>df.groupBy(F.spark_partition_id()).count().show()</span></pre><p id="d315" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上述代码确定了对数据帧进行分区的键。这个键可以是数据集中的一组列，默认的spark <a class="ae kv" href="https://stackoverflow.com/questions/31424396/how-does-hashpartitioner-work" rel="noopener ugc nofollow" target="_blank"> HashPartitioner </a>，或者一个<a class="ae kv" href="https://blog.clairvoyantsoft.com/custom-partitioning-spark-datasets-25cbd4e2d818" rel="noopener ugc nofollow" target="_blank">自定义HashPartitioner </a>。</p><p id="a0a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们来看看输出…</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/465ea9db7fe8306b34c44a0a51e30b9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*CMnMw4wp745AT8unFG1M5g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3:每个spark_partition_id的行数。图片作者。</p></figure><p id="693f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在图3中，我们可以看到创建的演示数据没有出现偏差——每个分区中的所有行数都是相同的。太好了，但是如果我想看到每个分区中的数据呢？</p><p id="3258" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为此，我们将访问底层RDD，并按分区提取数据…</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="3603" class="nx lt iq nt b gy ny nz l oa ob">df.rdd.glom().collect()</span></pre><p id="34e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe od oe of nt b">.glom()</code>返回列表列表。第一个轴对应于给定的分区，第二个轴对应于该分区中的<code class="fe od oe of nt b">Row()</code>对象。在图4中，我们打印了每个分区中的前2个<code class="fe od oe of nt b">Row()</code>对象——打印8个分区中的所有125个<code class="fe od oe of nt b">Row()</code>对象并不容易阅读。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/945e20d46eab59ed4d48c8c0e5a48e5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AQy966guLwov0em8ZN9EXg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4:df . rdd . glom()的输出。collect()被截断到每个分区的前2行。图片作者。</p></figure><p id="8ea1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">很酷，对吧？</p><p id="af3f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">提醒一句，当使用<code class="fe od oe of nt b">.glom()</code>方法时，你很容易让你的记忆超载。如果您正在处理大型数据集，请确保进行缩减采样，以便您收集的任何内容都可以放入RAM中。</p><h1 id="2273" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">如何纠正数据倾斜？</h1><p id="f1bc" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">尽管有上面的例子，但在现实世界中，完美的数据分布是罕见的。通常在读取数据时，我们会从预分区文件或ETL管道中提取数据，这些文件或管道可能不会自动很好地分发。</p><p id="db19" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要解决这个问题，有两个主要的解决方案…</p><h2 id="a6b5" class="nx lt iq bd lu oh oi dn ly oj ok dp mc lf ol om me lj on oo mg ln op oq mi or bi translated">1.按列重新分区</h2><p id="83b0" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">第一种解决方案是根据脚本中的转换对数据进行逻辑上的重新分区。简而言之，如果您正在分组或连接，通过groupBy/join列进行分区可以提高<a class="ae kv" href="https://sparkbyexamples.com/spark/spark-shuffle-partitions/#:~:text=The%20Spark%20SQL%20shuffle%20is,sql." rel="noopener ugc nofollow" target="_blank"> shuffle </a>的效率。</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="1243" class="nx lt iq nt b gy ny nz l oa ob">df = df.repartition(&lt;n_partitions&gt;, '&lt;col_1&gt;', '&lt;col_2&gt;',...)</span></pre><h2 id="cc16" class="nx lt iq bd lu oh oi dn ly oj ok dp mc lf ol om me lj on oo mg ln op oq mi or bi translated">2.盐</h2><p id="eac7" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如果您不确定哪些列会导致应用程序的工作负载均匀，您可以使用随机salt在内核之间均匀分布数据。我们所做的就是用一个随机值创建一个列，用该列进行分区…</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="89a8" class="nx lt iq nt b gy ny nz l oa ob">import pyspark.sql.functions as F</span><span id="909d" class="nx lt iq nt b gy os nz l oa ob">df = df.withColumn('salt', F.rand())<br/>df = df.repartition(8, 'salt')</span></pre><p id="a890" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了检查我们的盐是否有效，我们可以使用与上面相同的groupBy</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="6476" class="nx lt iq nt b gy ny nz l oa ob">df.groupBy(F.spark_partition_id()).count().show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/f143aed7a382acc571f7190f062ed7eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*4Vd4vpgpe5NbiV-njtMErw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图salted keys的分布示例。图片作者。</p></figure><p id="b036" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如您所见，每个分区的行数有一些变化，但是我们的键分布相当均匀。</p><h1 id="589d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="48ac" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在，所有其他复杂的帖子都有宝贵的信息——正确划分数据可以对应用程序性能产生巨大影响。然而，有了上面的信息，你(希望)有一个框架来搜索你的解决方案。</p><p id="af94" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">也就是说，这里有一些更复杂的快速提示:</p><ul class=""><li id="e249" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">你可以在Spark UI中查看任务速度的分布。这很有帮助！</li><li id="e767" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">通常最好让分区的数量是工作线程数量的倍数。</li><li id="5302" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">这里有一个关于spark操作层级的<a class="ae kv" href="https://stackoverflow.com/questions/42263270/what-is-the-concept-of-application-job-stage-and-task-in-spark?rq=1" rel="noopener ugc nofollow" target="_blank">快速概述</a>。</li><li id="a7b7" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">洗牌会导致数据被重新分区。一个好的分区将最小化程序所需的数据移动量。</li><li id="2f64" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">如果你想要一个关于分区大小的超级实用和高级的资源，查看<a class="ae kv" href="https://www.youtube.com/watch?v=_ArCesElWp8&amp;ab_channel=Databricks" rel="noopener ugc nofollow" target="_blank">这个视频</a>。</li></ul></div><div class="ab cl ou ov hu ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="ij ik il im in"><p id="8d43" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="pb">感谢阅读！我会再写12篇文章，把学术研究带到DS行业。查看我的评论，链接到这篇文章的主要来源和一些有用的资源。</em></p></div></div>    
</body>
</html>