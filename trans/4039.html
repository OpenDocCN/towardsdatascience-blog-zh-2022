<html>
<head>
<title>Transformers in Action: Attention Is All You Need</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变形金刚在行动:注意力是你所需要的</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transformers-in-action-attention-is-all-you-need-ac10338a023a#2022-09-08">https://towardsdatascience.com/transformers-in-action-attention-is-all-you-need-ac10338a023a#2022-09-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="552a" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">变形金刚(电影名)</h2><div class=""/><div class=""><h2 id="140d" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">简短的调查、说明和实施</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/e74ed2bb673783b9e827f3a2c537b73f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FvC1sLeE6c_K5v0iGsiYvQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图一。人工智能生成的艺术品。提示:童话小屋风格的家的街景。<a class="ae le" href="https://huggingface.co/spaces/stabilityai/stable-diffusion" rel="noopener ugc nofollow" target="_blank">稳定扩散</a>生成的照片。<a class="ae le" href="https://gist.github.com/soran-ghaderi/105084c714f5bfc406dc1b4064fc9510" rel="noopener ugc nofollow" target="_blank">链接到完整提示</a>。</p></figure><h1 id="ac5c" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">目录</h1><p id="d61f" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">1.<a class="ae le" href="#9dba" rel="noopener ugc nofollow">简介</a></p><p id="5f19" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">2.<a class="ae le" href="#dc42" rel="noopener ugc nofollow">快速回顾注意力</a></p><p id="2b17" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">3.<a class="ae le" href="#f828" rel="noopener ugc nofollow">变压器架构</a> <br/> 3.1。<a class="ae le" href="#ccd2" rel="noopener ugc nofollow">编码器和解码器组件</a> <br/> 3.1.1。<a class="ae le" href="#761f" rel="noopener ugc nofollow">编码器</a>3 . 1 . 2<br/>。<a class="ae le" href="#0176" rel="noopener ugc nofollow">解码器</a>3.2<br/>。<a class="ae le" href="#dc94" rel="noopener ugc nofollow">3.3变压器</a>中的模块<br/>。<a class="ae le" href="#5cb5" rel="noopener ugc nofollow">注意模块</a> <br/> 3.3.1。<a class="ae le" href="#fcad" rel="noopener ugc nofollow">3 . 3 . 2缩放点积注意事项</a> <br/>。<a class="ae le" href="#d417" rel="noopener ugc nofollow">多头关注</a> <br/> 3.4。3 . 4 . 1<a class="ae le" href="#095c" rel="noopener ugc nofollow">注意变压器</a>中的变型。<a class="ae le" href="#80ae" rel="noopener ugc nofollow">自我关注</a> <br/> 3.4.2。<a class="ae le" href="#825c" rel="noopener ugc nofollow">被掩盖的自我注意(自回归或因果注意)</a> <br/> 3.4.3。<a class="ae le" href="#36c1" rel="noopener ugc nofollow">交叉关注</a> <br/> 3.5。<a class="ae le" href="#36df" rel="noopener ugc nofollow">逐位FFN </a> <br/> 3.6。<a class="ae le" href="#01a4" rel="noopener ugc nofollow">残差连接和归一化</a> <br/> 3.7。<a class="ae le" href="#1e8d" rel="noopener ugc nofollow">位置编码</a> <br/> 3.7.1。<a class="ae le" href="#845d" rel="noopener ugc nofollow">绝对位置信息</a> <br/> 3.7.2。<a class="ae le" href="#77d4" rel="noopener ugc nofollow">相对位置信息</a></p><p id="bd00" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">4.<a class="ae le" href="#c283" rel="noopener ugc nofollow">使用自我关注背后的动机</a></p><p id="f18e" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">5.<a class="ae le" href="#16a4" rel="noopener ugc nofollow">研究前沿</a></p><p id="8215" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">6.<a class="ae le" href="#ce9b" rel="noopener ugc nofollow">问题</a></p><p id="206f" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">7.<a class="ae le" href="#acde" rel="noopener ugc nofollow">总结</a></p><p id="0efc" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">8.<a class="ae le" href="#3a54" rel="noopener ugc nofollow"> TransformerX库</a></p><p id="6b6d" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">9.<a class="ae le" href="#da58" rel="noopener ugc nofollow">参考文献</a></p></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><h1 id="9dba" class="lf lg iq bd lh li nf lk ll lm ng lo lp kf nh kg lr ki ni kj lt kl nj km lv lw bi translated">1.介绍</h1><p id="5880" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">作为人工智能研究过程中的一个成功前沿，<strong class="lz ja"> Transformers </strong>被认为是一种新颖的深度前馈人工神经网络架构，它利用了自我注意机制，可以处理输入序列项之间的长程相关性。由于在行业和学术研究中取得了巨大成功，自Vaswani等人[3]于2017年提出以来，研究人员已经提出了丰富的变压器架构(也称为X-formers)，并已在许多领域得到采用，如自然语言处理(NLP)、计算机视觉(CV)、音频和语音处理、化学和生命科学；他们可以在前面提到的学科中取得SOTA的成绩。在本文中，我通过底层数学、python代码实现和不同层的可视化解释了transformer架构。端到端的例子可以在GitHub 上的<a class="ae le" href="https://github.com/tensorops/TransformerX" rel="noopener ugc nofollow" target="_blank"> TransformerX库库中找到。</a></p></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><h1 id="dc42" class="lf lg iq bd lh li nf lk ll lm ng lo lp kf nh kg lr ki ni kj lt kl nj km lv lw bi translated">2.注意力的快速回顾</h1><p id="b22c" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">诸如注意机制和与编码器-解码器模型相关的术语等低级概念是变形金刚的基本思想。因此，我提供了这些方法的简要总结。</p><p id="a817" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">注意是一种认知资源的分配方案，具有有限的处理能力<strong class="lz ja"/>【1】。</p><p id="f8a7" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">Bahdanau等人[2]提出的注意力背后的一般思想是，当翻译每一步中的单词时，它搜索位于输入序列中不同位置的最相关的信息。在下一步中，它为源令牌(单词)wrt生成翻译。1)这些相关位置的上下文向量，以及2)先前同时生成的单词。</p><p id="3645" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">它们可以根据几个标准分为不同的类别，例如:</p><ul class=""><li id="98de" class="nk nl iq lz b ma mt md mu mg nm mk nn mo no ms np nq nr ns bi translated"><strong class="lz ja">注意的柔软度:</strong> <br/> 1。软2。硬3。本地4。全球的</li><li id="7f37" class="nk nl iq lz b ma nt md nu mg nv mk nw mo nx ms np nq nr ns bi translated"><strong class="lz ja">输入特征的形式:</strong> <br/> 1。逐项2。位置方面</li><li id="5e0d" class="nk nl iq lz b ma nt md nu mg nv mk nw mo nx ms np nq nr ns bi translated"><strong class="lz ja">输入表示:<br/> </strong> 1。共同关注2。自我关注3。与众不同的注意力。分层注意</li><li id="c57e" class="nk nl iq lz b ma nt md nu mg nv mk nw mo nx ms np nq nr ns bi translated"><strong class="lz ja">输出表示:</strong> <br/> 1。多头2。单输出3。多维的</li></ul><blockquote class="ny nz oa"><p id="49ae" class="lx ly ob lz b ma mt ka mc md mu kd mf oc mv mi mj od mw mm mn oe mx mq mr ms ij bi translated">如果你觉得注意力机制处于未知领域，我推荐你阅读下面这篇文章:</p></blockquote><div class="of og gp gr oh oi"><a rel="noopener follow" target="_blank" href="/rethinking-thinking-how-do-attention-mechanisms-actually-work-a6f67d313f99"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd ja gy z fp on fr fs oo fu fw iz bi translated">反思思维:注意力机制实际上是如何工作的？</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">大脑、数学和DL——2022年的研究前沿</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">towardsdatascience.com</p></div></div><div class="or l"><div class="os l ot ou ov or ow ky oi"/></div></div></a></div></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><h1 id="f828" class="lf lg iq bd lh li nf lk ll lm ng lo lp kf nh kg lr ki ni kj lt kl nj km lv lw bi translated">3.变压器架构</h1><p id="7fe3" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">基本变换器[3]架构由两个主要构建模块组成，即编码器和解码器模块。编码器从输入表示序列(𝒙₁ <em class="ob">，…，</em> 𝒙ₙ)生成嵌入向量𝒁 = (𝒛₁ <em class="ob">，…，</em> 𝒛ₙ)，并将其传递给解码器以生成输出序列(𝒚₁ <em class="ob">，…，</em> 𝒚ₘ).)在每一步产生输出之前，𝒁向量被输入解码器，因此模型是自回归的。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ox"><img src="../Images/ce0a81c31d663a684961b25f6eb3ef81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nqEy4i4sQPhYV0E2n436fQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图二。变压器架构。照片由<a class="ae le" href="https://www.linkedin.com/in/soran-ghaderi/" rel="noopener ugc nofollow" target="_blank">作者</a>拍摄。</p></figure><h2 id="ccd2" class="oy lg iq bd lh oz pa dn ll pb pc dp lp mg pd pe lr mk pf pg lt mo ph pi lv iw bi translated">3.1.编码器和解码器组件</h2><p id="f09e" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">与序列到序列模型类似，转换器使用编码器-解码器架构。</p><h2 id="761f" class="oy lg iq bd lh oz pa dn ll pb pc dp lp mg pd pe lr mk pf pg lt mo ph pi lv iw bi translated">3.1.1.编码器</h2><p id="4686" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">编码器只是多个组件或层的堆叠——𝑵<em class="ob"/>在原始论文中是6——它们本身是一组两个子层，即多头自关注块和简单的位置式FC FFN(全连接前馈网络)。为了实现更深的模型，研究人员通过包裹两个子层中的每一个，然后进行层归一化，来练习剩余连接。因此，每个子层的输出都是<em class="ob">layer norm(</em>𝒙<em class="ob">+sublayer(</em>𝒙<em class="ob">)</em>，<em class="ob"> Sublayer( </em> 𝒙 <em class="ob"> ) </em>是在自身内部实现的函数。所有子层以及嵌入的输出维度是𝒅 <em class="ob"> _model = 512 </em>。</p><p id="1611" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">变压器编码器模块的实现:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="pj pk l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Transformer编码器实现(<a class="ae le" href="https://discord.gg/7BF9KUnBNT" rel="noopener ugc nofollow" target="_blank"> TransformerX Discord </a>服务器讨论)。由<a class="ae le" href="https://github.com/soran-ghaderi" rel="noopener ugc nofollow" target="_blank">作者</a>编写的代码。</p></figure><h2 id="0176" class="oy lg iq bd lh oz pa dn ll pb pc dp lp mg pd pe lr mk pf pg lt mo ph pi lv iw bi translated">3.1.2.解码器</h2><p id="a397" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">除了编码器中使用的子层之外，解码器对编码器组件的输出应用多头关注。像编码器一样，残差连接被附加到子层，然后进行层归一化。为了保证位置𝒊的预测可以仅依赖于先前已知的位置这一事实，将另一个修改应用于自关注子层，以防止位置关注其他位置，同时将输出嵌入偏移一个位置。</p><p id="e836" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">变压器解码器模块的实现:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="pj pk l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Transformer解码器实现(<a class="ae le" href="https://discord.gg/7BF9KUnBNT" rel="noopener ugc nofollow" target="_blank"> TransformerX Discord服务器</a>供讨论)。代码由<a class="ae le" href="https://github.com/soran-ghaderi" rel="noopener ugc nofollow" target="_blank">作者</a>编写。</p></figure><h2 id="dc94" class="oy lg iq bd lh oz pa dn ll pb pc dp lp mg pd pe lr mk pf pg lt mo ph pi lv iw bi translated">3.2.变压器中的模块</h2><p id="a6e4" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">接下来，我将讨论组成原始transformer架构的基本组件。</p><ol class=""><li id="5262" class="nk nl iq lz b ma mt md mu mg nm mk nn mo no ms pl nq nr ns bi translated">注意模块</li><li id="297f" class="nk nl iq lz b ma nt md nu mg nv mk nw mo nx ms pl nq nr ns bi translated">位置式前馈网络</li><li id="1d69" class="nk nl iq lz b ma nt md nu mg nv mk nw mo nx ms pl nq nr ns bi translated">剩余连接和规范化</li><li id="fc58" class="nk nl iq lz b ma nt md nu mg nv mk nw mo nx ms pl nq nr ns bi translated">位置编码</li></ol><h2 id="5cb5" class="oy lg iq bd lh oz pa dn ll pb pc dp lp mg pd pe lr mk pf pg lt mo ph pi lv iw bi translated">3.3.注意模块</h2><p id="a883" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">该转换器集成了来自信息检索的查询键值(QKV)概念和注意机制</p><ul class=""><li id="5ab0" class="nk nl iq lz b ma mt md mu mg nm mk nn mo no ms np nq nr ns bi translated">标度点积注意力</li><li id="6120" class="nk nl iq lz b ma nt md nu mg nv mk nw mo nx ms np nq nr ns bi translated">多头注意力</li></ul><h2 id="fcad" class="oy lg iq bd lh oz pa dn ll pb pc dp lp mg pd pe lr mk pf pg lt mo ph pi lv iw bi translated">3.3.1.标度点积注意力</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pm"><img src="../Images/e9d7c88cf7b87ffe4bf1d1b7ed208503.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uya6_ec79IIXOkvPbGOQ-g.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图3。成比例的点积注意力。照片由<a class="ae le" href="https://www.linkedin.com/in/soran-ghaderi/" rel="noopener ugc nofollow" target="_blank">作者</a>拍摄。</p></figure><p id="bb1f" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">缩放的点积注意力被公式化为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/2f43086767e2c5e1fff126d628cb6a5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*XE8ws8z4KXW5vWXp1r2UbA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">情商。一</p></figure><p id="38f2" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">其中𝑲 ∈ ℝ^𝑀×𝐷𝑘、𝑸 ∈ ℝ^ 𝑵 ×𝐷𝑘和𝑽 ∈ ℝ^ 𝑴×𝐷𝑣是表示矩阵。键(或值)和查询的长度分别由𝑴和𝑵表示，它们的维度由𝐷𝑘和𝐷𝑣.表示方程中的矩阵𝑨。1通常被称为注意力矩阵。他们使用点积注意力而不是加法注意力的原因是由于矩阵乘法优化技术，在实践中的速度和空间效率，加法注意力使用具有单个隐藏层的前馈网络来计算兼容性函数。尽管如此，对于大𝐷𝑘值的点积有一个很大的缺点，它会将softmax函数的梯度推到极小的梯度。为了抑制softmax函数的梯度消失问题，键和查询的点积除以𝐷𝑘的平方根，由于这个事实，它被称为缩放的点积。</p><p id="2746" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">点积注意模块的实现:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="pj pk l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用TransformerX的缩放点积实现(<a class="ae le" href="https://discord.gg/7BF9KUnBNT" rel="noopener ugc nofollow" target="_blank"> Discord社区</a>讨论)。代码作者<a class="ae le" href="https://github.com/soran-ghaderi" rel="noopener ugc nofollow" target="_blank">作者</a>。</p></figure><h2 id="d417" class="oy lg iq bd lh oz pa dn ll pb pc dp lp mg pd pe lr mk pf pg lt mo ph pi lv iw bi translated">3.3.2.多头注意力</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi po"><img src="../Images/ed3c3f869bfb282f2f4ae56ffb9a2fdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*52nWUfJA13oeLks4mdmwng.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图4。a .多头关注。b .多头注意中张量运算的端到端流程。照片由<a class="ae le" href="https://www.linkedin.com/in/soran-ghaderi/" rel="noopener ugc nofollow" target="_blank">作者</a>拍摄。</p></figure><p id="60f2" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">引入多个注意头而不是单个注意函数，Transformer分别用不同的、学习到的线性投影<em class="ob"> h </em>次将𝐷𝑚-dimensional原始查询、键和值线性投影到𝐷𝑘、𝐷𝑘和𝐷𝑣维度；通过它，注意力函数(等式。1)在这些投影上可以并行执行，产生𝐷𝑣-dimensional输出值。该模型然后将它们连接起来，并产生一个𝐷𝑚-dimensional表示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/f0e9e4dee693746e8dd07c5e00684a27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*18Y0lEiEd7qSV22FkrRF7Q.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">情商。2</p></figure><p id="567d" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">在哪里</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/8ec504e386a42cbd0d645eeb20ac41f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*dBYfpBIzd6pOmexdAD4e5g.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">情商。3</p></figure><p id="c84b" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">这些投影是𝑾𝑸ᵢ ∈ ℝ^d_model×dk、𝑾𝑲ᵢ ∈ ℝ^d_model×dk、𝑾𝑽ᵢ ∈ ℝ^d_model×dv和𝑾𝒐 ∈ ℝ^h*dv×d_model矩阵。</p><p id="2f31" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">这个过程使得变换器能够共同关注不同的表示子空间和位置。为了使它更具体，对于一个特定的形容词，一个头可能捕捉形容词的强度，而另一个头可能关注它的消极和积极。</p><p id="de6b" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">多头注意力的实现</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="pj pk l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用transformer x(<a class="ae le" href="https://discord.gg/7BF9KUnBNT" rel="noopener ugc nofollow" target="_blank">Discord community</a>)的多头注意力实现。代码由<a class="ae le" href="https://github.com/soran-ghaderi" rel="noopener ugc nofollow" target="_blank">作者</a>编写。</p></figure><p id="63d1" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">可以看出，多头注意力具有三个确定张量维度的超参数:</p><ul class=""><li id="1de8" class="nk nl iq lz b ma mt md mu mg nm mk nn mo no ms np nq nr ns bi translated">注意力头的数量</li><li id="abb9" class="nk nl iq lz b ma nt md nu mg nv mk nw mo nx ms np nq nr ns bi translated">模型大小(嵌入大小):嵌入向量的长度。</li><li id="ae7e" class="nk nl iq lz b ma nt md nu mg nv mk nw mo nx ms np nq nr ns bi translated">查询、键和值大小:输出查询、键和值矩阵的线性图层使用的查询、键和值权重大小</li></ul><h2 id="095c" class="oy lg iq bd lh oz pa dn ll pb pc dp lp mg pd pe lr mk pf pg lt mo ph pi lv iw bi translated">3.4.变压器中的注意变量</h2><p id="fdc4" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在最初的Transformer论文中使用了三种不同的方式来使用注意力，这三种方式在将键、查询和值提供给注意力函数的方式上是不同的。</p><ul class=""><li id="d0e1" class="nk nl iq lz b ma mt md mu mg nm mk nn mo no ms np nq nr ns bi translated">自我关注</li><li id="1ac1" class="nk nl iq lz b ma nt md nu mg nv mk nw mo nx ms np nq nr ns bi translated">隐蔽的自我注意(自回归或因果注意)</li><li id="6a8f" class="nk nl iq lz b ma nt md nu mg nv mk nw mo nx ms np nq nr ns bi translated">交叉注意</li></ul><h2 id="80ae" class="oy lg iq bd lh oz pa dn ll pb pc dp lp mg pd pe lr mk pf pg lt mo ph pi lv iw bi translated">3.4.1.自我关注</h2><p id="8f5d" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">所有的关键字、查询和值向量来自相同的序列，在变换器的情况下，编码器的前一步输出，允许编码器的每个位置同时关注它自己的前一层中的所有位置，即𝑸 = 𝑲 = 𝑽 = 𝑿(前一编码器输出)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/182753402687cc7bbb4c453b7c86c0c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*80qCY9Kgs3-u4skZIZKr-Q.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图5。自我注意张量运算。照片由<a class="ae le" href="https://www.linkedin.com/in/soran-ghaderi/" rel="noopener ugc nofollow" target="_blank">作者</a>拍摄。</p></figure><h2 id="825c" class="oy lg iq bd lh oz pa dn ll pb pc dp lp mg pd pe lr mk pf pg lt mo ph pi lv iw bi translated">3.4.2.隐蔽的自我注意(自回归或因果注意)</h2><p id="ff59" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">尽管有编码器层，但在解码器的自我关注中，查询被限制在它们之前的键值对位置以及它们的当前位置，以便保持自回归特性。这可以通过屏蔽无效位置并将其设置为负无穷大来实现，即𝑨𝒊𝒋=∞。如果𝒊 &lt; 𝒋.</p><h2 id="36c1" class="oy lg iq bd lh oz pa dn ll pb pc dp lp mg pd pe lr mk pf pg lt mo ph pi lv iw bi translated">3.4.3. Cross-attention</h2><p id="786a" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">This type of attention obtains its queries from the previous decoder layer whereas the keys and values are acquired from the encoder yields. This is basically the attention used in the encoder-decoder attention mechanisms in sequence-to-sequence models. In other words, cross-attention combines two different embedding sequences with the exact dimensions which derive its queries from one sequence and its keys and values from the other. Let’s assume <em class="ob"> S1 </em>和<em class="ob"> S2 </em>是两个嵌入序列，交叉注意力从<em class="ob"> S1 </em>获得其键和值，并从<em class="ob"> S2 </em>获得其查询，然后计算注意力得分并产生长度为<em class="ob"> S2 </em>的结果序列。在转换器的情况下，键和值来自编码器，查询来自前一步骤的解码器输出。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/857b86932ceaea10019ebb083b453215.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*MS-bxl8ckthsH8xS1sEv1A.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图6。交叉注意张量运算。照片由<a class="ae le" href="https://www.linkedin.com/in/soran-ghaderi/" rel="noopener ugc nofollow" target="_blank">作者</a>拍摄。</p></figure><p id="6d9a" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">值得一提的是，两个输入嵌入序列可以是不同的形式(即文本、图像、音频等。).</p><h2 id="36df" class="oy lg iq bd lh oz pa dn ll pb pc dp lp mg pd pe lr mk pf pg lt mo ph pi lv iw bi translated">3.5.位置明智的FFN</h2><p id="a373" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在编码器和解码器中的每个子层之上，以完全相同的方式单独地将位置式全连接前馈网络应用于每个位置，然而，参数在层与层之间是不同的。它是一对线性层，中间有一个ReLU激活函数；它等同于内核大小为1的两层卷积。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/bcc918c0ccdb0a12a00ea47270e5f832.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*PoZ43HErrH0ymPgz6Y6OyQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">情商。四</p></figure><p id="e584" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">其中<em class="ob"> x </em>为前一层的输出，𝑾₁ ∈ ℝ^𝐷_ <em class="ob">模型</em> × 𝐷𝑓，𝑾₂ ∈ ℝ^𝐷𝑓 × 𝐷_ <em class="ob">模型</em>，𝒃₁ ∈ ℝ^𝐷𝑓，T7模型为可训练矩阵，内层</p><p id="94f2" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">实施基于位置的FFN:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="pj pk l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">逐位FFN实现(<a class="ae le" href="https://discord.gg/7BF9KUnBNT" rel="noopener ugc nofollow" target="_blank">变压器x不和谐社区</a>)。由<a class="ae le" href="https://github.com/soran-ghaderi" rel="noopener ugc nofollow" target="_blank">作者</a>编写的代码。</p></figure><h2 id="01a4" class="oy lg iq bd lh oz pa dn ll pb pc dp lp mg pd pe lr mk pf pg lt mo ph pi lv iw bi translated">3.6.剩余连接和规范化</h2><p id="5812" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">用剩余连接包装每个模块支持更深层次的架构，同时避免梯度消失/爆炸。因此，转换器使用模块周围的剩余连接，然后进行层规范化。它可以表述如下:</p><ul class=""><li id="eec2" class="nk nl iq lz b ma mt md mu mg nm mk nn mo no ms np nq nr ns bi translated">𝒙′=layernorm(selfattention(𝑿)+𝑿)</li><li id="71ce" class="nk nl iq lz b ma nt md nu mg nv mk nw mo nx ms np nq nr ns bi translated">𝒙=layernorm(ffn(𝒙<em class="ob">'</em>)+𝒙<em class="ob">'</em>)</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/6da62939d7037778be8ea02be499cb98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*UGeJm5_PL6jzxReDtg9XEA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图7。剩余连接和层标准化。照片由<a class="ae le" href="https://www.linkedin.com/in/soran-ghaderi/" rel="noopener ugc nofollow" target="_blank">作者</a>拍摄。</p></figure><p id="08b3" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">剩余连接和归一化的实现:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="pj pk l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">剩余连接和归一化实现(<a class="ae le" href="https://discord.gg/7BF9KUnBNT" rel="noopener ugc nofollow" target="_blank"> TransformerX Discord </a>服务器讨论)。由<a class="ae le" href="https://github.com/soran-ghaderi" rel="noopener ugc nofollow" target="_blank">作者</a>编写的代码。</p></figure><h2 id="1e8d" class="oy lg iq bd lh oz pa dn ll pb pc dp lp mg pd pe lr mk pf pg lt mo ph pi lv iw bi translated">3.7.位置编码</h2><p id="87ea" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">Transfomer的研究人员使用了一个有趣的想法，将有序感注入到输入令牌中，因为它没有递归或卷积。绝对和相对位置信息可用于暗示输入的序列顺序，其可以被学习或固定。矩阵之间的求和过程需要相同大小的矩阵，因此，位置编码维数与输入嵌入维数相同。它们被注入编码器和解码器模块底部的输入编码中。Vaswani等人[3]在正弦和余弦函数的帮助下使用固定位置编码，然而，他们试验了相对位置编码，并意识到在他们的情况下，它产生几乎相同的结果[4]。设𝑿是包含n个<em class="ob"> d </em>维嵌入的表征的输入表示。位置编码产生𝑿 + 𝑷，其中𝑷是相同大小的位置嵌入矩阵。第I行<em class="ob"> (2 </em> 𝒋 <em class="ob">)第</em>或<em class="ob"> (2 </em> 𝒋 <em class="ob"> +1)第</em>列的元素为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/e80130f2d28dade0a55f1f66c5e81cd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*5w__IR-n58nhqVPKqIoZtw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">情商。5</p></figure><p id="dc8b" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">和</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/9ca51570eb220197aa47aac1dbf211e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*PlKM-bzZkDdb3K3GdITE4A.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">情商。七</p></figure><p id="6509" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">在位置嵌入矩阵P中，行表示记号在序列中的位置，列表示不同的位置编码维度。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="pj pk l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">位置编码实现(<a class="ae le" href="https://discord.gg/7BF9KUnBNT" rel="noopener ugc nofollow" target="_blank">transformer x Discord community</a>)。由<a class="ae le" href="https://github.com/soran-ghaderi" rel="noopener ugc nofollow" target="_blank">作者</a>编写的代码。</p></figure><p id="4bab" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">在下面的可视化中，我描述了矩阵𝑷中4列之间的差异。请注意不同列的不同频率。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi px"><img src="../Images/b798c4da97ac2472afbac94ff9a4074d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PqmTkGXJlMFSk861g0hSiQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图8。位置编码。照片由<a class="ae le" href="https://www.linkedin.com/in/soran-ghaderi/" rel="noopener ugc nofollow" target="_blank">作者</a>拍摄。</p></figure><h2 id="845d" class="oy lg iq bd lh oz pa dn ll pb pc dp lp mg pd pe lr mk pf pg lt mo ph pi lv iw bi translated">3.7.1.绝对位置信息</h2><p id="f7b3" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在位置编码类型中，频率根据元素的位置交替变化。举例来说，看看下面的二进制编码；最没有价值的位置(右侧)上的数字波动更频繁，而具有更有价值的位置的其他数字相对于它们的位置具有较少的波动，即最有价值的位置更稳定。</p><pre class="kp kq kr ks gt py pz qa qb aw qc bi"><span id="db7a" class="oy lg iq pz b gy qd qe l qf qg">0 <strong class="pz ja">-&gt;</strong> 000<br/>1 <strong class="pz ja">-&gt;</strong> 001<br/>2 <strong class="pz ja">-&gt;</strong> 010<br/>3 <strong class="pz ja">-&gt;</strong> 011<br/>4 <strong class="pz ja">-&gt;</strong> 100<br/>5 <strong class="pz ja">-&gt;</strong> 101<br/>6 <strong class="pz ja">-&gt;</strong> 110<br/>7 <strong class="pz ja">-&gt;</strong> 111</span></pre><h2 id="77d4" class="oy lg iq bd lh oz pa dn ll pb pc dp lp mg pd pe lr mk pf pg lt mo ph pi lv iw bi translated">3.7.2.相对位置信息</h2><p id="473a" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">除了上面的位置编码，另一种方法是学习通过相对位置来参加。对于任何固定位置的𝛿，𝛿+𝒊的位置编码可以通过将其线性投影到位置𝒊 <em class="ob">得到。</em>让<em class="ob">ψ</em>=1/(10000^(2𝒋/d))，任何一对情商。4和eq。对于任何固定的偏移𝛿:，可以线性投影到𝛿+𝒊的位置</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/43afbf0cac830c1b6e4db8a4ed428ae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*-CHwD7ZLCyjebwFbAOVGVA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">情商。七</p></figure></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><h1 id="c283" class="lf lg iq bd lh li nf lk ll lm ng lo lp kf nh kg lr ki ni kj lt kl nj km lv lw bi translated">4.使用自我关注背后的动机</h1><p id="4ef5" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在“注意力是你所需要的全部”[3]论文中，研究人员在将自我注意力与卷积层和递归层进行比较时考虑了多个标准。这些需求可以分为三大类:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qi"><img src="../Images/77f99fdd961cc8f844c1f8039141836d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ft6JA7mlvdQCrFoUGxFgig.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">表1。每层的计算复杂度、每层中发生的最小顺序操作以及最大路径长度。其中<em class="qj"> n表示序列长度，d表示表示的维数，k表示卷积的核大小，r表示受限自我关注中的邻域大小。表来自[3]。</em></p></figure><ul class=""><li id="408c" class="nk nl iq lz b ma mt md mu mg nm mk nn mo no ms np nq nr ns bi translated"><strong class="lz ja">计算复杂度</strong>:每层计算复杂度的总量</li><li id="33ec" class="nk nl iq lz b ma nt md nu mg nv mk nw mo nx ms np nq nr ns bi translated"><strong class="lz ja">并行化</strong>:计算可以并行到什么程度</li><li id="cebd" class="nk nl iq lz b ma nt md nu mg nv mk nw mo nx ms np nq nr ns bi translated"><strong class="lz ja">学习长程相关性</strong>:处理网络中长程相关性的能力</li><li id="cc13" class="nk nl iq lz b ma nt md nu mg nv mk nw mo nx ms np nq nr ns bi translated"><strong class="lz ja">可解释性</strong>:检查学习到的分布的能力和关注输入的语义和句法特征的能力</li></ul><p id="58d8" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">表1说明了当序列长度<em class="ob"> n </em>小于表示维度<em class="ob"> d </em>时，自我注意在计算复杂度上优于递归层，这在SOTA翻译模型中是常见的情况，如单词片段[5]和字节对[6]表示。<strong class="lz ja">受限自关注</strong>是普通自关注的一个更复杂的版本，当涉及到非常长的输入序列中的计算复杂性时，它仅使用来自相应输出位置周围的输入序列的有限数量的大小为<em class="ob"> r </em>的邻居。此外，当使用连续内核时，它可以击败需要几个Conv层的卷积层，其复杂性为𝑶 <em class="ob"> (n/k) </em>，对于扩张卷积，其复杂性为𝑶<em class="ob">(log _ k(n))</em>【7】。这种层的堆叠又延长了网络中任意两个位置之间的最长路径。卷积层的计算开销通常是递归层的k倍。值得一提的是，可分离卷积的复杂度要低得多，然而，在最好的情况下，它相当于自关注层和前馈层的组合。</p></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><h1 id="16a4" class="lf lg iq bd lh li nf lk ll lm ng lo lp kf nh kg lr ki ni kj lt kl nj km lv lw bi translated">5.研究前沿</h1><p id="3a33" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">最近的变体试图通过进一步探索改进架构的不同路线来提高原始作品的性能，即:</p><ul class=""><li id="257d" class="nk nl iq lz b ma mt md mu mg nm mk nn mo no ms np nq nr ns bi translated"><strong class="lz ja">效率</strong>:自我注意力在处理长序列时会导致计算和记忆的复杂性，这促使研究人员通过引入轻量级注意力解决方案(例如稀疏注意力变体)和分治方法(例如递归和分层机制)来解决这个问题。</li><li id="c7bc" class="nk nl iq lz b ma nt md nu mg nv mk nw mo nx ms np nq nr ns bi translated"><strong class="lz ja">一般化</strong>:由于对输入数据的结构偏差不敏感，转换器需要大量的数据进行训练，因此需要引入结构偏差或正则化、对大规模未标记数据进行预训练等工作。来克服这个障碍。</li><li id="e35b" class="nk nl iq lz b ma nt md nu mg nv mk nw mo nx ms np nq nr ns bi translated"><strong class="lz ja">适应</strong>:由于变形金刚有能力被各种领域采用，研究人员试图将它们与特定的下游任务整合在一起。</li></ul></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><h1 id="ce9b" class="lf lg iq bd lh li nf lk ll lm ng lo lp kf nh kg lr ki ni kj lt kl nj km lv lw bi translated">6.问题</h1><p id="7e27" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在这一部分，我邀请你思考以下问题，并尝试在评论中给出答案。</p><ol class=""><li id="3538" class="nk nl iq lz b ma mt md mu mg nm mk nn mo no ms pl nq nr ns bi translated">如果你在《变形金刚》中用加法注意力取代比例点积注意力，会发生什么？</li><li id="ee2c" class="nk nl iq lz b ma nt md nu mg nv mk nw mo nx ms pl nq nr ns bi translated">如果我们想使用转换器进行语言建模，我们应该使用编码器、解码器还是两者都用？</li><li id="b4ee" class="nk nl iq lz b ma nt md nu mg nv mk nw mo nx ms pl nq nr ns bi translated">如果变压器的输入过长，会发生什么情况？我们该如何应对？</li><li id="13be" class="nk nl iq lz b ma nt md nu mg nv mk nw mo nx ms pl nq nr ns bi translated">我们能做些什么来提高变压器的计算和存储效率？</li></ol><p id="f9f2" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">说到这里，请留下评论，描述您认为哪些部分令人困惑或含糊不清。这篇文章对你有什么影响，你还想知道关于媒体写作的哪些话题。</p><p id="728a" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">我们可以在<a class="ae le" href="https://discord.gg/7BF9KUnBNT" rel="noopener ugc nofollow" target="_blank">不和谐服务器</a>上进一步讨论它们。</p></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><h1 id="acde" class="lf lg iq bd lh li nf lk ll lm ng lo lp kf nh kg lr ki ni kj lt kl nj km lv lw bi translated">7.摘要</h1><p id="972a" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在本文中，您了解了Transformer架构及其实现，并看到了它在不同领域带来的重大突破，如机器翻译、计算机视觉以及其他一些学科，同时降低了它们的复杂性，并使它们更具可解释性。转换器的另一个基本组件是不同头的并行化能力，因为它完全使用多头自关注，而不是使用递归或卷积层。现在，您已经熟悉了变压器的主要组件。</p><p id="5bcb" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">我希望这篇文章对你有所帮助。如果你有，请分享到你最喜欢的社交媒体频道，这样其他人也可以找到它。</p><p id="00f4" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">我写关于机器学习和其他技术主题的最新研究。如果你对其中任何一个感兴趣，请查看并<a class="ae le" href="https://soran-ghaderi.medium.com/" rel="noopener">跟随我</a>。</p></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><h1 id="3a54" class="lf lg iq bd lh li nf lk ll lm ng lo lp kf nh kg lr ki ni kj lt kl nj km lv lw bi translated">8.TransformerX库</h1><p id="d05e" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated"><a class="ae le" href="https://github.com/tensorops/TransformerX" rel="noopener ugc nofollow" target="_blank"><strong class="lz ja">transformer x</strong></a><strong class="lz ja"/>是一个python库，为研究人员、学生和专业人士提供开发、培训和评估transformer所需的构建模块，并顺利集成到Tensorflow中(我们将很快添加对Pytorch和JAX的支持)。我们正在积极努力增加更多的功能。(我们最近发布了它的第一个版本，我非常感谢给我们一个🌟在Github上🌹)</p><p id="853c" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">我想请你在<a class="ae le" href="https://github.com/soran-ghaderi" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上关注我，如果你想为一个尖端的深度学习库(TransformerX)做贡献，请随时联系我，我们期待着你的来信。我们将指导您完成第一笔捐款的每一步。</p><p id="65b9" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">你也可以在<a class="ae le" href="https://discord.gg/7BF9KUnBNT" rel="noopener ugc nofollow" target="_blank"> TransformerX Discord服务器</a>和<a class="ae le" href="https://twitter.com/tensorops" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上加入我们，我们会保持联系。</p><div class="of og gp gr oh oi"><a href="https://www.linkedin.com/in/soran-ghaderi/" rel="noopener  ugc nofollow" target="_blank"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd ja gy z fp on fr fs oo fu fw iz bi translated">Soran Ghaderi -自雇-自雇| LinkedIn</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">寻找一个非常适合的机器学习研究职位，我打算扩展我在机器学习方面的技能…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">www.linkedin.com</p></div></div><div class="or l"><div class="qk l ot ou ov or ow ky oi"/></div></div></a></div></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><h1 id="da58" class="lf lg iq bd lh li nf lk ll lm ng lo lp kf nh kg lr ki ni kj lt kl nj km lv lw bi translated">9.参考</h1><p id="c096" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">[1] J. R. Anderson，2005，《认知心理学及其启示》，沃斯出版社，2005年。<br/> [2] D. Bahdanau，K. Cho，Y. Bengio，联合学习对齐和翻译的神经机器翻译，载:ICLR。<br/>[3]阿什什·瓦斯瓦尼、诺姆·沙泽尔、尼基·帕马尔、雅各布·乌兹科雷特、Llion Jones、艾丹·戈麦斯、祖卡斯·凯泽和伊利亚·波洛舒欣。你需要的只是关注。NeurIPS，2017。<br/> [4]乔纳斯·戈林、迈克尔·奥利、大卫·格兰吉尔、丹尼斯·亚拉茨和扬恩·多芬。卷积序列到序列学习。arXiv预印本arXiv:1705.03122v2，2017。<br/>【5】吴永辉、迈克·舒斯特、、郭维乐、穆罕默德·诺鲁齐、沃尔夫冈·马切里、马克西姆·克里昆、、秦高、克劳斯·马切里等.谷歌的神经机器翻译系统:弥合人类与机器翻译之间的鸿沟。arXiv预印本arXiv:1609.08144，2016。<br/>[6]丹尼·布里兹、安娜·戈尔迪、吴明堂和郭诉乐。神经机器翻译架构的大规模探索。更正，abs/1703.03906，2017。<br/>[7]纳尔·卡尔希布伦纳、拉塞·埃斯佩霍尔特、卡伦·西蒙扬、亚伦·范登欧德、亚历克斯·格雷夫斯和科拉伊·卡武克库奥卢。线性时间内的神经机器翻译。arXiv预印本arXiv:1610.10099v2，2017。</p></div></div>    
</body>
</html>