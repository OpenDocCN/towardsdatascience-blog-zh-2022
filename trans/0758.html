<html>
<head>
<title>Demystifying the Lottery Ticket Hypothesis in Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭秘深度学习中的彩票假说</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/demystifying-the-lottery-ticket-hypothesis-in-deep-learning-158570b62674#2022-03-03">https://towardsdatascience.com/demystifying-the-lottery-ticket-hypothesis-in-deep-learning-158570b62674#2022-03-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="51ee" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">为什么彩票是训练神经网络的下一件大事</h2></div><p id="344f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">训练神经网络是昂贵的。据计算，OpenAI 的 GPT-3 使用市场上成本最低的云 GPU 的<a class="ae lb" href="https://lambdalabs.com/blog/demystifying-gpt-3/" rel="noopener ugc nofollow" target="_blank">培训成本为 460 万美元</a>。难怪<a class="ae lb" href="https://arxiv.org/abs/1803.03635" rel="noopener ugc nofollow" target="_blank">弗兰克尔和卡宾的 2019 年彩票假说</a>开始了一场研究淘金热，引起了脸书和微软等顶级学术头脑和科技巨头的关注。在论文中，他们证明了<strong class="kh ir">中奖(彩票)</strong>的存在:一个神经网络的子网络可以被<em class="lc">训练产生与原始网络</em>一样好的性能，并且规模小得多。在这篇文章中，我将阐述这是如何工作的，为什么它是革命性的，以及研究的现状。</p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><h1 id="872c" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">为什么是彩票？</h1><p id="7be8" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">传统观点认为，神经网络最好在训练后修剪，而不是在开始时。通过修剪权重、神经元或其他组件，得到的神经网络<strong class="kh ir">更小、更快，并且在推理过程中消耗更少的资源</strong>。如果处理得当，网络规模可能会大幅缩小，但精度不会受到影响。</p><p id="9c56" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过颠倒传统智慧，我们可以考虑<strong class="kh ir"> <em class="lc">我们是否可以在训练</em> </strong>之前修剪网络并获得相同的结果。换句话说，<em class="lc">来自删减组件的信息是网络学习所必需的吗，即使不代表它的学习？</em></p><p id="d926" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">彩票假说集中于修剪权重，并提供了经验证据，即某些修剪后的子网络可以从一开始就被训练，以实现与整个网络相似的性能。怎么会？迭代幅度修剪。</p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><h1 id="6029" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated"><strong class="ak">迭代幅度修剪</strong></h1><p id="5898" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">当历史上尝试过这样的任务时，修剪后的网络权重会随机重新初始化，性能会迅速下降。</p><p id="db3c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里的关键区别是权重被返回到它们的初始<strong class="kh ir">状态。当训练时，在相同的训练时间内，在高水平的修剪下，结果匹配原始性能。</strong></p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mh"><img src="../Images/3a975c2653d1e11d991205029d5dab2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yGm5OxVhop2ZYUFI"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">照片由<a class="ae lb" href="https://unsplash.com/@tegethoff?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马克·泰格霍夫</a>在<a class="ae lb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="a603" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这表明这些彩票作为特定子网和初始权重的交集而存在。可以说，他们“赢得了彩票”，因为该架构和这些权重的匹配表现得与整个网络一样好。这适用于更大的型号吗？</p><p id="2b35" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于更大的模型，同样的方法并不适用。在观察对噪声的敏感度时，弗兰克尔和卡宾复制了经过修剪的网络，并对不同排序的数据进行了训练。当<strong class="kh ir">线性模式连通性</strong>存在时，IMP 成功，这是一种非常罕见的现象，其中多个网络收敛到相同的局部最小值。对于小型网络，这种情况很自然。对于大型网络来说，则不然。那怎么办呢？</p><p id="3758" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以较小的学习速率开始导致 IMP 适用于大模型，因为对来自数据的初始噪声的敏感性降低了。学习率可以随着时间的推移而增加。另一个发现是，在稍后的训练迭代中，将我们修剪过的神经网络的权重恢复到它们的值，而不是第一次迭代，同样有效。例如，1000 次迭代训练中第 10 次迭代的权重。</p><p id="601d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些结果在不同的架构中保持稳定，如变压器、LSTMs、CNN 和强化学习架构。</p><p id="6b47" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然这篇论文<strong class="kh ir">证明了这些彩票</strong>的存在，但是<strong class="kh ir">还没有提供识别它们的方法</strong>。因此，淘金热在寻找他们的属性和他们是否可以在训练前确定。他们也启发了早期修剪的启发式工作，因为我们当前的启发式工作集中在训练后的修剪。</p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><h1 id="afa2" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">当前研究</h1><p id="8cde" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated"><a class="ae lb" href="https://arxiv.org/abs/1906.02773" rel="noopener ugc nofollow" target="_blank">一票全赢</a> (2019)显示<strong class="kh ir">彩票编码信息对数据类型和优化器</strong>不变。他们能够成功地在基于不同数据类型(如 VGG 到 ImageNet)的网络之间传输彩票，并获得成功。</p><p id="0dfc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个关键指标是网络训练数据的相对规模。<em class="lc">如果彩票票源在比目的地网络更大的数据集上被训练，则其表现更好</em>；否则，类似或更糟。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mx"><img src="../Images/16f0b4660f5754b5a295d56930ed3d31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jVBVTOTQ3AjRZGFF"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">照片由<a class="ae lb" href="https://unsplash.com/@moritz_photography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">莫里茨·金德勒</a>在<a class="ae lb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="dbdb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://arxiv.org/abs/1909.11957" rel="noopener ugc nofollow" target="_blank">抽早鸟票</a> (2019):本文旨在证明在训练中可以早早发现彩票。每次训练迭代，他们都会计算一个剪枝掩码。如果上一次迭代和这次迭代的掩码距离<em class="lc">(使用汉明距离)</em>低于某个阈值，网络停止修剪。</p><p id="63b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://arxiv.org/abs/2006.05467" rel="noopener ugc nofollow" target="_blank">通过迭代保存突触流</a> (2020)在没有任何数据的情况下修剪神经网络:本文重点讨论在没有数据的情况下初始化<em class="lc">时计算修剪。它在初始化时优于现有的最先进的剪枝算法。该技术侧重于最大化<strong class="kh ir">临界压缩</strong>、<em class="lc">在不影响性能的情况下可能发生的最大修剪。为了做到这一点，作者旨在防止整层被修剪。网络通过对保留层进行积极评分并在每次网络修剪时重新评估分数来实现这一点。</em></em></p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><h1 id="5a3d" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">结束语</h1><p id="710c" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">在神经结构中存在小的子网络，这些子网络可以被训练得和整个神经网络一样好，这为有效的训练打开了一个可能性的世界。在这个过程中，研究人员正在学习很多关于神经网络如何学习以及学习需要什么的知识。谁知道呢？很快有一天，我们可能能够在 训练之前<strong class="kh ir"> <em class="lc">修剪我们的网络，节省时间、计算和能量。</em></strong></p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi my"><img src="../Images/0ccc9ff9348850a31a39457dd2a7a3d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9_NWVjR3bcaefsNq"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">照片由<a class="ae lb" href="https://unsplash.com/@emilianovittoriosi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">埃米利亚诺·维托里奥西</a>在<a class="ae lb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div></div>    
</body>
</html>