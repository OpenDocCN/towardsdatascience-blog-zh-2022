<html>
<head>
<title>Beautifully Illustrated: NLP Models from RNN to Transformer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">精美的插图:从RNN到变形金刚的NLP模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beautifully-illustrated-nlp-models-from-rnn-to-transformer-80d69faf2109#2022-10-11">https://towardsdatascience.com/beautifully-illustrated-nlp-models-from-rnn-to-transformer-80d69faf2109#2022-10-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ffe8" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">深度学习</h2><div class=""/><div class=""><h2 id="d792" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">用工作图解释他们复杂的数学公式</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/064d03c0d72c80fb6c1c5c90adeec121.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HiyGwoGOMI5Ao_fd"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">鲁拜图·阿扎德在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="a696" class="ln lo it lj b gy lp lq l lr ls"><strong class="lj jd">Table of Contents</strong></span><span id="c013" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj jd">· </strong><a class="ae lh" href="#6cf1" rel="noopener ugc nofollow"><strong class="lj jd">Recurrent Neural Networks (RNN)</strong></a><br/>  ∘ <a class="ae lh" href="#8dde" rel="noopener ugc nofollow">Vanilla RNN</a><br/>  ∘ <a class="ae lh" href="#ed57" rel="noopener ugc nofollow">Long Short-term Memory (LSTM)</a><br/>  ∘ <a class="ae lh" href="#1247" rel="noopener ugc nofollow">Gated Recurrent Unit (GRU)</a></span><span id="bdcd" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj jd">· </strong><a class="ae lh" href="#866f" rel="noopener ugc nofollow"><strong class="lj jd">RNN Architectures</strong></a></span><span id="94e3" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj jd">· </strong><a class="ae lh" href="#b5e5" rel="noopener ugc nofollow"><strong class="lj jd">Attention</strong></a><br/>  ∘ <a class="ae lh" href="#e999" rel="noopener ugc nofollow">Seq2seq with Attention</a><br/>  ∘ <a class="ae lh" href="#7efc" rel="noopener ugc nofollow">Self-attention</a><br/>  ∘ <a class="ae lh" href="#2fbd" rel="noopener ugc nofollow">Multi-head Attention</a></span><span id="b362" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj jd">· </strong><a class="ae lh" href="#8704" rel="noopener ugc nofollow"><strong class="lj jd">Transformer</strong></a><br/>  ∘ <a class="ae lh" href="#5dd0" rel="noopener ugc nofollow">Step 1. Adding Positional Encoding to Word Embeddings</a><br/>  ∘ <a class="ae lh" href="#1e51" rel="noopener ugc nofollow">Step 2. Encoder: Multi-head Attention and Feed Forward</a><br/>  ∘ <a class="ae lh" href="#b51d" rel="noopener ugc nofollow">Step 3. Decoder: (Masked) Multi-head Attention and Feed Forward</a><br/>  ∘ <a class="ae lh" href="#7edd" rel="noopener ugc nofollow">Step 4. Classifier</a></span><span id="76bb" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj jd">· </strong><a class="ae lh" href="#f7d9" rel="noopener ugc nofollow"><strong class="lj jd">Wrapping Up</strong></a></span></pre><p id="cb1b" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi mq translated"><span class="l mr ms mt bm mu mv mw mx my di"> N </span>自然语言处理(NLP)是深度学习中一个具有挑战性的问题，因为计算机不理解如何处理原始单词。为了使用计算机的能力，我们需要在将单词输入模型之前将其转换为向量。由此产生的向量被称为<a class="ae lh" href="https://levelup.gitconnected.com/glove-and-fasttext-clearly-explained-extracting-features-from-text-data-1d227ab017b2" rel="noopener ugc nofollow" target="_blank"> <strong class="lw jd">单词嵌入</strong> </a>。</p><p id="e9aa" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">这些嵌入可以用于解决期望的任务，例如情感分类、文本生成、命名实体识别或机器翻译。它们以一种聪明的方式被处理，使得模型对于某些任务的性能变得与人的能力不相上下。</p><p id="a9bc" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">那么，我听到你问，如何处理单词嵌入？如何为这种数据建立一个自然的模型？</p><p id="1e7c" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">让我们首先试着熟悉我们将要使用的符号。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mz"><img src="../Images/d9d6e21e6892650969ca3c8043320385.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vK9U01IouUdyBypGHORKhg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">我们将在这个故事中使用的符号|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><h1 id="6cf1" class="na lo it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">递归神经网络(RNN)</h1><p id="1ca4" class="pw-post-body-paragraph lu lv it lw b lx nr kd lz ma ns kg mc md nt mf mg mh nu mj mk ml nv mn mo mp im bi translated">当你读这句话的时候，你并不是从零开始思考每一个新单词。你保持以前单词的信息来理解你当前正在看的单词。基于这种行为，递归神经网络(RNN)诞生了。</p><p id="4f64" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">在这一部分的故事中，我们将重点介绍RNN细胞及其改进。稍后，我们将看到细胞是如何排列在一起的。</p><h2 id="8dde" class="ln lo it bd nb nw nx dn nf ny nz dp nj md oa ob nl mh oc od nn ml oe of np iz bi translated">香草RNN</h2><p id="6b4b" class="pw-post-body-paragraph lu lv it lw b lx nr kd lz ma ns kg mc md nt mf mg mh nu mj mk ml nv mn mo mp im bi translated">传统的RNN由重复的单元组成，每个单元依次接收嵌入<em class="og"> xₜ </em>的输入，并通过隐藏状态<em class="og"> hₜ₋₁ </em>记住过去的序列。隐藏状态被更新到<em class="og"> hₜ </em>并被转发到下一个单元格，或者——取决于任务——可用于输出预测。下图显示了RNN细胞的内部工作原理。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oh"><img src="../Images/a0432643106949eacd9365e081fa78ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xwv9096KN2jKD-gCeWRw3Q.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">香草RNN细胞|图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="f892" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated"><strong class="lw jd">优势:</strong></p><ul class=""><li id="5c4a" class="oi oj it lw b lx ly ma mb md ok mh ol ml om mp on oo op oq bi translated">以有意义的方式说明订单和先前的输入。</li></ul><p id="d23b" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated"><strong class="lw jd">缺点:</strong></p><ul class=""><li id="420a" class="oi oj it lw b lx ly ma mb md ok mh ol ml om mp on oo op oq bi translated">每一步的预测都依赖于之前的预测，因此很难将RNN运算并行化。</li><li id="e506" class="oi oj it lw b lx or ma os md ot mh ou ml ov mp on oo op oq bi translated">处理长序列会产生爆炸或消失的梯度。</li></ul><h2 id="ed57" class="ln lo it bd nb nw nx dn nf ny nz dp nj md oa ob nl mh oc od nn ml oe of np iz bi translated">长短期记忆(LSTM)</h2><p id="6cbe" class="pw-post-body-paragraph lu lv it lw b lx nr kd lz ma ns kg mc md nt mf mg mh nu mj mk ml nv mn mo mp im bi translated">减轻爆炸或消失梯度问题的一种方法是使用门控rnn，其被设计成选择性地保留信息，并且能够学习长期依赖性。有两种受欢迎的门控RNNs:长短期记忆(LSTM)和门控循环单位(GRU)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/48bf89eef61745a2ead49196d717fc43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e9j04GkLU2TyCA37Sj0log.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">LSTM手机|图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="9af0" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">为了避免长期依赖问题，LSTM配备了一个单元状态<em class="og"> Cₜ </em>，这有点像高速公路，所以信息很容易不加修改地通过它。</p><p id="78a2" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">为了有选择地保留信息，LSTM也有三个关口:</p><ol class=""><li id="cf84" class="oi oj it lw b lx ly ma mb md ok mh ol ml om mp ox oo op oq bi translated">忘记门→查看<em class="og"> hₜ₋₁ </em>和<em class="og"> xₜ </em>，并输出由0和1之间的数字组成的向量<em class="og"> fₜ </em>，该向量告诉我们要从单元格状态<em class="og"> Cₜ₋₁ </em>中丢弃什么信息。</li><li id="5106" class="oi oj it lw b lx or ma os md ot mh ou ml ov mp ox oo op oq bi translated">输入门→类似于遗忘门，但这一次输出<em class="og"> iₜ </em>用于根据虚拟单元状态<em class="og"> ćₜ </em>决定我们要在单元状态中存储什么新信息。</li><li id="6706" class="oi oj it lw b lx or ma os md ot mh ou ml ov mp ox oo op oq bi translated">输出门→类似于遗忘门，但输出<em class="og"> oₜ </em>用于过滤更新的单元状态<em class="og"> Cₜ </em>成为新的隐藏状态<em class="og"> hₜ </em>。</li></ol><h2 id="1247" class="ln lo it bd nb nw nx dn nf ny nz dp nj md oa ob nl mh oc od nn ml oe of np iz bi translated">门控循环单元(GRU)</h2><p id="f93d" class="pw-post-body-paragraph lu lv it lw b lx nr kd lz ma ns kg mc md nt mf mg mh nu mj mk ml nv mn mo mp im bi translated">LSTM相当复杂。GRU提供了与LSTM相似的性能，但复杂性更低(重量更轻)。它合并了单元格状态和隐藏状态。它还将遗忘门和输入门合并成一个“更新门”。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oh"><img src="../Images/6ae876f5dbbcf682b88b0bebaa34e724.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5zS9mmaUZEaCmtd5iiTDmQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">GRU手机|图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="ef15" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">为了进一步解释，GRU有两个大门:</p><ol class=""><li id="0110" class="oi oj it lw b lx ly ma mb md ok mh ol ml om mp ox oo op oq bi translated">reset gate →查看<em class="og"> hₜ₋₁ </em>和<em class="og"> xₜ </em>，输出一个由0和1之间的数字组成的矢量<em class="og"> rₜ </em>，该矢量决定需要忽略多少过去的信息<em class="og"> hₜ₋₁ </em>。</li><li id="8391" class="oi oj it lw b lx or ma os md ot mh ou ml ov mp ox oo op oq bi translated">更新门→根据<em class="og"> rₜ </em>决定我们要在新的隐藏状态<em class="og"> hₜ </em>中存储什么信息或者丢弃什么信息。</li></ol><h1 id="866f" class="na lo it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">RNN建筑</h1><p id="7f33" class="pw-post-body-paragraph lu lv it lw b lx nr kd lz ma ns kg mc md nt mf mg mh nu mj mk ml nv mn mo mp im bi translated">现在你已经理解了RNN细胞是如何工作的，你可以按如下顺序链接它的拷贝。这里，<em class="og"> yₜ </em>是预测的输出概率，表示为</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/76ed690ea3d072a0f06e36cdaa4a70e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/format:webp/1*TFg2Huw7yRFlSOP4aEZhaw.png"/></div></figure><p id="45f9" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">对于一些重量<em class="og"> W </em>，偏置<em class="og"> b </em>，以及激活功能<em class="og"> g </em>。如您所见，架构和激活功能取决于任务。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/0847f330f62186815788ce3f44159b82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ElW6ysYMDd0q0xduLXacHA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">RNN建筑取决于任务|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="96ea" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">特别是，上面最后一个机器翻译的例子是我们所说的<strong class="lw jd"> seq2seq </strong>模型，其中输入序列<em class="og"> x₁ </em>、<em class="og"> x₂ </em>、…、<em class="og"> xₜ </em>被翻译成输出序列<em class="og"> y₁ </em>、<em class="og"> y₂ </em>、…、<em class="og"> yₘ </em>。第一组<em class="og"> t </em> RNNs称为<strong class="lw jd">编码器</strong>，最后一组<em class="og"> m </em> RNNs称为<strong class="lw jd">解码器</strong>。</p><p id="9df8" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">RNN单元也可以翻转或垂直堆叠，如下所示。这种架构通常只由两到三层组成。这可以提高模型性能，但代价是计算量更大。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/41303f18787200084bfcf2068a03bb0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GTXV-m0laGGp0YSuW8_Syg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">RNN变体，这样的架构通常只由两三层组成|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><h1 id="b5e5" class="na lo it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">注意力</h1><p id="7d3b" class="pw-post-body-paragraph lu lv it lw b lx nr kd lz ma ns kg mc md nt mf mg mh nu mj mk ml nv mn mo mp im bi translated">Seq2seq被限制为使用编码器最末端的表示来被解码器解释。解码器只看到来自最后一个编码器RNN单元的输入序列的一个表示。然而，输入序列的不同部分在输出序列的每个生成步骤中可能更有用。这就是注意力的概念。</p><p id="4f7c" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated"><strong class="lw jd">优点:</strong></p><ul class=""><li id="79f9" class="oi oj it lw b lx ly ma mb md ok mh ol ml om mp on oo op oq bi translated">考虑适当的编码表示，而不管输入标记的位置。</li></ul><p id="a35e" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated"><strong class="lw jd">缺点:</strong></p><ul class=""><li id="76fa" class="oi oj it lw b lx ly ma mb md ok mh ol ml om mp on oo op oq bi translated">另一个计算步骤包括学习权重。</li></ul><h2 id="e999" class="ln lo it bd nb nw nx dn nf ny nz dp nj md oa ob nl mh oc od nn ml oe of np iz bi translated">Seq2seq注意</h2><p id="8437" class="pw-post-body-paragraph lu lv it lw b lx nr kd lz ma ns kg mc md nt mf mg mh nu mj mk ml nv mn mo mp im bi translated">每个解码器令牌将查看每个编码器令牌，并通过其隐藏状态来决定哪些令牌需要更多关注。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/bf435ecbd7cd6baa537387d4dfc92408.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kZQDwfXsJDaqSMwva2AACg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">seq2seq工作图有关注|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="5dc0" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">在seq2seq中融入注意力有三个步骤:</p><ol class=""><li id="50ce" class="oi oj it lw b lx ly ma mb md ok mh ol ml om mp ox oo op oq bi translated">从一对解码器和编码器隐藏状态(<em class="og"> sᵢ </em>、<em class="og"> hⱼ </em>)计算标量<strong class="lw jd">注意力分数</strong>，其代表编码器令牌<em class="og"> j </em>与解码器令牌<em class="og"> i </em>的“相关性”。</li><li id="d79a" class="oi oj it lw b lx or ma os md ot mh ou ml ov mp ox oo op oq bi translated">所有注意力分数通过softmax传递，以产生<strong class="lw jd">注意力权重</strong>，其形成解码器和编码器令牌对的相关性的概率分布。</li><li id="317f" class="oi oj it lw b lx or ma os md ot mh ou ml ov mp ox oo op oq bi translated">计算具有注意力权重的编码器隐藏状态的<strong class="lw jd">加权和</strong>,并将其馈入下一个解码器单元。</li></ol><p id="c83a" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">在我们的例子中，得分函数是缩放的点积</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/370733a64dca28484b51553657ad6c36.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*Z94Gw-E0-nN-72od2syjvw.png"/></div></figure><p id="48d7" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">其中<em class="og"> dₕ </em>为<em class="og"> hⱼ </em>(和<em class="og"> sᵢ </em>)的尺寸。</p><h2 id="7efc" class="ln lo it bd nb nw nx dn nf ny nz dp nj md oa ob nl mh oc od nn ml oe of np iz bi translated">自我关注</h2><p id="e601" class="pw-post-body-paragraph lu lv it lw b lx nr kd lz ma ns kg mc md nt mf mg mh nu mj mk ml nv mn mo mp im bi translated">让我们完全抛弃seq2seq，只关注注意力。一种流行的关注是自我关注，其中不是从解码器到编码器令牌寻找相关性，而是从一组令牌中的每个令牌到同一组中的所有其他令牌寻找相关性。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/5214aae5e76cf293a060aa21f584fd5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dz73vwh45PR1JWGVVWvIKQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">自我关注|图片工作图<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="59e3" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">自我注意使用注意函数创建基于输入标记对之间的相似性的加权表示，这提供了知道其元素之间的关系的输入序列的丰富表示。</p><p id="fa47" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">普通注意力和自我注意力有三个主要区别:</p><ol class=""><li id="682f" class="oi oj it lw b lx ly ma mb md ok mh ol ml om mp ox oo op oq bi translated">由于自我关注中没有解码器令牌，一个“查询”向量<em class="og">qᵢ</em>——与输入嵌入<em class="og"> xᵢ </em>线性相关——取而代之。</li><li id="891e" class="oi oj it lw b lx or ma os md ot mh ou ml ov mp ox oo op oq bi translated">关注度得分是从这一对(<em class="og"> qᵢ </em>、<em class="og"> kⱼ </em>)中计算出来的，其中<em class="og"> kⱼ </em>是一个“关键”向量，与<em class="og"> qᵢ </em>的维度相同，也与<em class="og"> xⱼ </em>线性相关。</li><li id="4a9c" class="oi oj it lw b lx or ma os md ot mh ou ml ov mp ox oo op oq bi translated">自我关注用注意力权重乘以一个新的“价值”向量<em class="og"> vⱼ </em>，而不是像普通注意力一样，再用注意力权重乘以<em class="og"> kⱼ </em>。请注意，<em class="og"> vⱼ </em>的尺寸可能与<em class="og"> kⱼ </em>的尺寸不同，并且再次与<em class="og"> xⱼ </em>线性相关。</li></ol><h2 id="2fbd" class="ln lo it bd nb nw nx dn nf ny nz dp nj md oa ob nl mh oc od nn ml oe of np iz bi translated">多头注意力</h2><p id="fa6a" class="pw-post-body-paragraph lu lv it lw b lx nr kd lz ma ns kg mc md nt mf mg mh nu mj mk ml nv mn mo mp im bi translated">注意力可以并行运行几次，产生我们所说的多头注意力。独立的注意力输出然后被连接并线性转换成期望的维度。</p><p id="03b8" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">多个注意头背后的动机是它们允许不同地注意输入序列的部分(例如，长期依赖性对短期依赖性)，因此它可以提高单一注意的性能。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pe"><img src="../Images/7715af846c9dd2ab34034199f2a36bcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a1MADmZSwhSzglDFLqVwMQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">n种注意力机制的多头注意力工作图|图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="e3f7" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">由于在故事的下一部分事情会变得更加疯狂，在上面的图表中，我们对一切进行了矢量化，以便多头注意力的输入和输出由一个带单箭头的矩阵表示。</p><p id="c398" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">自我注意也可以与任何类型的注意交换，或者通过任何修改来实现，只要其变量的维度是适当的。</p><h1 id="8704" class="na lo it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">变压器</h1><p id="7fd2" class="pw-post-body-paragraph lu lv it lw b lx nr kd lz ma ns kg mc md nt mf mg mh nu mj mk ml nv mn mo mp im bi translated">给定输入嵌入<em class="og"> X </em>和输出嵌入<em class="og"> Y </em>，一般来说，使用一个接一个堆叠的<em class="og"> N个</em>编码器连接到一个接一个堆叠的<em class="og"> N个</em>解码器来构建变压器。<a class="ae lh" href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="og">没有递归或卷积，你所需要的是</em> </a>中的每个编码器和解码器。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/c1c95e80f7a24ab3179528ebb79d6e31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iakslgPQZt2wtOkQwh-HPg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">具有N个编码器和解码器的变压器的工作图|图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a>提供</p></figure><p id="eac9" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated"><strong class="lw jd">优点:</strong></p><ul class=""><li id="f38c" class="oi oj it lw b lx ly ma mb md ok mh ol ml om mp on oo op oq bi translated">输入记号的更好表示，其中记号表示基于使用自我注意的特定相邻记号。</li><li id="6bc2" class="oi oj it lw b lx or ma os md ot mh ou ml ov mp on oo op oq bi translated">(并行)处理所有输入令牌，而不是受到顺序处理(RNNs)的内存问题的限制。</li></ul><p id="7ab1" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated"><strong class="lw jd">缺点:</strong></p><ul class=""><li id="329d" class="oi oj it lw b lx ly ma mb md ok mh ol ml om mp on oo op oq bi translated">计算密集型。</li><li id="f663" class="oi oj it lw b lx or ma os md ot mh ou ml ov mp on oo op oq bi translated">需要大量数据(使用预训练模型缓解)。</li></ul><p id="232f" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">让我们来看看变压器是如何工作的！</p><h2 id="5dd0" class="ln lo it bd nb nw nx dn nf ny nz dp nj md oa ob nl mh oc od nn ml oe of np iz bi translated">第一步。向单词嵌入添加位置编码</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/3a85dc8f90cd7711b0e75d197427d4c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VeXw4xT2zljgRy42n5E_qQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">向单词嵌入添加位置编码|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="aec8" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">由于变换器不包含递归和卷积，为了使模型利用序列的顺序，我们必须注入一些关于序列中记号的相对或绝对位置的信息。</p><p id="a271" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">因此，我们必须通过“位置编码”的方式让模型明确地知道记号的位置:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/c32b03d35953468a226f93fc96e8fe85.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*yOm-aNJ58_FjrA0QZzq8mA.png"/></div></figure><p id="0c58" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">其中<em class="og"> i </em>是令牌的位置(令牌#0、令牌#1等等)，<em class="og"> j </em>是编码的列号，<em class="og"> dₓ </em>是编码的维数(与输入嵌入的维数<em class="og"> X </em>相同)。</p><p id="dea5" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">对于编码维度为512的前2048个标记，您可以将位置编码矩阵<em class="og"> P </em>可视化如下。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ph"><img src="../Images/f658f63ed925ba13617850e8448bc781.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qm5VlYm6eVsWXYiHoOVkUQ.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">编码维度为512的前2048个标记的位置编码|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="5827" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">在seq2seq模型中，输出嵌入<em class="og"> Y </em>右移，第一个标记是“句首”&lt; bos &gt;。位置编码然后被添加到<em class="og"> X </em>和<em class="og"> Y </em>，然后分别被馈送到第一编码器和解码器。</p><h2 id="1e51" class="ln lo it bd nb nw nx dn nf ny nz dp nj md oa ob nl mh oc od nn ml oe of np iz bi translated">第二步。编码器:多头关注和前馈</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/a887c6a18cbf8a126b48b39b33ef8467.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CpIYYkuN8XTpoDo1bDa5uA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">变形金刚中的编码器|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="5b97" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">编码器由两个模块组成:</p><ol class=""><li id="b1bd" class="oi oj it lw b lx ly ma mb md ok mh ol ml om mp ox oo op oq bi translated">输入的嵌入<em class="og">X</em>——加上位置编码<em class="og">P</em>——送入<strong class="lw jd">多头关注</strong>。剩余连接被用于多头注意力，这意味着我们将单词嵌入(或前一编码器的输出)加到注意力的输出上。然后将结果归一化。</li><li id="bd69" class="oi oj it lw b lx or ma os md ot mh ou ml ov mp ox oo op oq bi translated">归一化后，信息通过一个<strong class="lw jd">前馈</strong>神经网络，该网络由两层组成，分别具有ReLU和线性激活函数。同样，我们在这个块上使用剩余连接和标准化。</li></ol><blockquote class="pi pj pk"><p id="e7d8" class="lu lv og lw b lx ly kd lz ma mb kg mc pl me mf mg pm mi mj mk pn mm mn mo mp im bi translated">要了解更多关于残差连接和图层规范化的信息，您可以进入这个故事:</p></blockquote><div class="po pp gp gr pq pr"><a rel="noopener follow" target="_blank" href="/5-most-well-known-cnn-architectures-visualized-af76f1f0065e"><div class="ps ab fo"><div class="pt ab pu cl cj pv"><h2 class="bd jd gy z fp pw fr fs px fu fw jc bi translated">5种流行的CNN架构得到清晰的解释和可视化</h2><div class="py l"><h3 class="bd b gy z fp pw fr fs px fu fw dk translated">为什么盗梦空间看起来像三叉戟？！</h3></div><div class="pz l"><p class="bd b dl z fp pw fr fs px fu fw dk translated">towardsdatascience.com</p></div></div><div class="qa l"><div class="qb l qc qd qe qa qf lb pr"/></div></div></a></div><p id="527e" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">编码器内部的计算到此结束。输出可以被馈送到另一个编码器或最后一个编码器之后的每个解码器的特定部分。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qg"><img src="../Images/c55c853904cf74c2ee0618e9e4c2e7b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IDyRoTUFEDqaJDenNcJgsg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">变形金刚编码器和解码器|图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a>提供</p></figure><h2 id="b51d" class="ln lo it bd nb nw nx dn nf ny nz dp nj md oa ob nl mh oc od nn ml oe of np iz bi translated">第三步。解码器:(屏蔽)多头注意，前馈</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/24680dab6d80a9f44d055e5a2481bef8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1DRMkBZST7DP4bhpN3Yf4w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">《变形金刚》中的解码器|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="050b" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">解码器由三个模块组成:</p><ol class=""><li id="cbcc" class="oi oj it lw b lx ly ma mb md ok mh ol ml om mp ox oo op oq bi translated">输出嵌入<em class="og">Y</em>——右移加位置编码<em class="og"> P </em>后——送入<strong class="lw jd">多头关注</strong>。注意，我们屏蔽掉(设置为softmax输入中与后续位置连接相对应的所有值。</li><li id="91dd" class="oi oj it lw b lx or ma os md ot mh ou ml ov mp ox oo op oq bi translated">然后信息通过另一个<strong class="lw jd">多头注意力</strong>——现在没有屏蔽——作为查询向量。键向量和值向量来自最后一个编码器的输出。这允许解码器中的每个位置关注输入序列中的所有位置。</li><li id="b9ba" class="oi oj it lw b lx or ma os md ot mh ou ml ov mp ox oo op oq bi translated">结果通过一个<strong class="lw jd">前馈</strong>神经网络传递，该网络具有类似编码器的激活功能。</li></ol><p id="0d5d" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">注意，所有解码器块也采用残差连接和层归一化。结合输出嵌入偏移一个位置的事实，屏蔽确保位置<em class="og"> i </em>的预测可以仅依赖于小于<em class="og"> i </em>的位置处的已知输出。</p><p id="5405" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">这就是解码器内部计算的结束。输出可以被馈送到另一个解码器或最后一个解码器之后的分类器。</p><h2 id="7edd" class="ln lo it bd nb nw nx dn nf ny nz dp nj md oa ob nl mh oc od nn ml oe of np iz bi translated">第四步。分类者</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/0e527b9ee660edde8a04ce3c632ce945.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PB-_607pn18tzsVPNLnBtw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">变形金刚|图片中的分类器作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="8b44" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">这是最后一步，非常简单。我们使用学习线性变换和softmax将解码器输出转换为预测的下一个令牌概率。</p></div><div class="ab cl qh qi hx qj" role="separator"><span class="qk bw bk ql qm qn"/><span class="qk bw bk ql qm qn"/><span class="qk bw bk ql qm"/></div><div class="im in io ip iq"><p id="e701" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">《变形金刚》在自然语言处理方面取得了巨大的成功。许多预先训练好的模型，如GPT-2、GPT-3、BERT、XLNet和RoBERTa，展示了transformers执行各种NLP相关任务的能力，如机器翻译、文档摘要、文档生成、命名实体识别和视频理解。</p><p id="a0c2" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">变压器还被应用于<a class="ae lh" href="https://arxiv.org/pdf/2010.11929v2.pdf" rel="noopener ugc nofollow" target="_blank">图像处理</a>，与卷积神经网络相比，其结果颇具竞争力。</p><h1 id="f7d9" class="na lo it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">包扎</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qo"><img src="../Images/743863ce0f70e88b1a3a48108490ec4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xp5uqZoPVbYjtPTf"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@jeremythomasphoto?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">杰瑞米·托马斯</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="ceae" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">NLP相关的任务很难。有许多方法可以解决这些问题。最直观的一个是RNN，尽管它的操作很难并行化，并且当暴露于长序列时，它遭受爆炸或消失梯度。别担心，LSTM和GRU会来救你的！</p><p id="781e" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">排列RNN细胞的方法有很多种，其中一种叫做seq2seq。在seq2seq中，解码器只看到来自最后一个编码器RNN单元的输入序列的一个表示。这激发了注意力的使用，输入序列的不同部分可以在输出序列的每个生成步骤中得到不同的注意。</p><p id="7a7b" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">随着自我关注的发展，RNN细胞可以被完全抛弃。称为多头注意力的自我注意力束以及前馈神经网络形成了转换器，构建了最先进的NLP模型，如GPT-3、BERT等，以出色的性能处理许多NLP任务。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qp"><img src="../Images/e1a6e3674ab93bcb99796285f9d0175c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*6HsoGpmIb1oibJc_JWbqJA.gif"/></div></div></figure></div><div class="ab cl qh qi hx qj" role="separator"><span class="qk bw bk ql qm qn"/><span class="qk bw bk ql qm qn"/><span class="qk bw bk ql qm"/></div><div class="im in io ip iq"><p id="4859" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">🔥你好！如果你喜欢这个故事，想支持我这个作家，可以考虑 <a class="ae lh" href="https://dwiuzila.medium.com/membership" rel="noopener"> <strong class="lw jd"> <em class="og">成为会员</em> </strong> </a> <em class="og">。每月只需5美元，你就可以无限制地阅读媒体上的所有报道。如果你注册使用我的链接，我会赚一小笔佣金。</em></p><p id="bd58" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">🔖<em class="og">想了解更多关于经典机器学习模型如何工作以及如何优化其参数的信息？或者MLOps大型项目的例子？有史以来最优秀的文章呢？继续阅读:</em></p><div class="po pp gp gr pq"><div role="button" tabindex="0" class="ab bv gv cb fp qq qr bn qs lb ex"><div class="qt l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qu qv fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qu qv fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----80d69faf2109--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="qy qz gw l"><h2 class="bd jd wa oi fp wb fr fs px fu fw jc bi translated">从零开始的机器学习</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wc au wd we wf sr wg an eh ei wh wi wj el em eo de bk ep" href="https://dwiuzila.medium.com/list/machine-learning-from-scratch-b35db8650093?source=post_page-----80d69faf2109--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wk l fo"><span class="bd b dl z dk">8 stories</span></div></div></div><div class="rl dh rm fp ab rn fo di"><div class="di rd bv re rf"><div class="dh l"><img alt="" class="dh" src="../Images/4b97f3062e4883b24589972b2dc45d7e.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*CWNoicci28F2TUQc-vKijw.png"/></div></div><div class="di rd bv rg rh ri"><div class="dh l"><img alt="" class="dh" src="../Images/b1f7021514ba57a443fe0db4b7001b26.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*wSRsSHYnIiGJFAqC"/></div></div><div class="di bv rj rk ri"><div class="dh l"><img alt="" class="dh" src="../Images/deb73e42c79667024a46c2c8902b81fa.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HVEz7KwzO0tv1Q4d"/></div></div></div></div></div><div class="po pp gp gr pq"><div role="button" tabindex="0" class="ab bv gv cb fp qq qr bn qs lb ex"><div class="qt l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qu qv fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qu qv fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----80d69faf2109--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="qy qz gw l"><h2 class="bd jd wa oi fp wb fr fs px fu fw jc bi translated">高级优化方法</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wc au wd we wf sr wg an eh ei wh wi wj el em eo de bk ep" href="https://dwiuzila.medium.com/list/advanced-optimization-methods-26e264a361e4?source=post_page-----80d69faf2109--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wk l fo"><span class="bd b dl z dk">7 stories</span></div></div></div><div class="rl dh rm fp ab rn fo di"><div class="di rd bv re rf"><div class="dh l"><img alt="" class="dh" src="../Images/15b3188b0f29894c2bcf3d0965515f44.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*BVMamoNudzn9UlAE"/></div></div><div class="di rd bv rg rh ri"><div class="dh l"><img alt="" class="dh" src="../Images/3249ba2cf680952e2ccdff36d8ebf4a7.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*C1fv3HJdh1RBspwN"/></div></div><div class="di bv rj rk ri"><div class="dh l"><img alt="" class="dh" src="../Images/a73f0494533d8a08b01c2b899373d2b9.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*QZvzgiM2VnhYyx8M"/></div></div></div></div></div><div class="po pp gp gr pq"><div role="button" tabindex="0" class="ab bv gv cb fp qq qr bn qs lb ex"><div class="qt l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qu qv fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qu qv fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----80d69faf2109--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="qy qz gw l"><h2 class="bd jd wa oi fp wb fr fs px fu fw jc bi translated">MLOps大型项目</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wc au wd we wf sr wg an eh ei wh wi wj el em eo de bk ep" href="https://dwiuzila.medium.com/list/mlops-megaproject-6a3bf86e45e4?source=post_page-----80d69faf2109--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wk l fo"><span class="bd b dl z dk">6 stories</span></div></div></div><div class="rl dh rm fp ab rn fo di"><div class="di rd bv re rf"><div class="dh l"><img alt="" class="dh" src="../Images/41b5d7dd3997969f3680648ada22fd7f.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*EBS8CP_UnStLesXoAvjeAQ.png"/></div></div><div class="di rd bv rg rh ri"><div class="dh l"><img alt="" class="dh" src="../Images/41befac52d90334c64eef7fc5c4b4bde.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*XLpRKnIMcJzBzCwvXrLvsw.png"/></div></div><div class="di bv rj rk ri"><div class="dh l"><img alt="" class="dh" src="../Images/80908ef475e97fbc42efe3fae0dfcff5.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*K_gzBmjv-ZHlU0Q6HeXclQ.jpeg"/></div></div></div></div></div><div class="po pp gp gr pq"><div role="button" tabindex="0" class="ab bv gv cb fp qq qr bn qs lb ex"><div class="qt l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qu qv fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qu qv fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----80d69faf2109--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="qy qz gw l"><h2 class="bd jd wa oi fp wb fr fs px fu fw jc bi translated">我最好的故事</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wc au wd we wf sr wg an eh ei wh wi wj el em eo de bk ep" href="https://dwiuzila.medium.com/list/my-best-stories-d8243ae80aa0?source=post_page-----80d69faf2109--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wk l fo"><span class="bd b dl z dk">24 stories</span></div></div></div><div class="rl dh rm fp ab rn fo di"><div class="di rd bv re rf"><div class="dh l"><img alt="" class="dh" src="../Images/0c862c3dee2d867d6996a970dd38360d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*K1SQZ1rzr4cb-lSi"/></div></div><div class="di rd bv rg rh ri"><div class="dh l"><img alt="" class="dh" src="../Images/392d63d181090365a63dc9060573bcff.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*hSKy6kKorAfHjHOK"/></div></div><div class="di bv rj rk ri"><div class="dh l"><img alt="" class="dh" src="../Images/f51725806220b60eccf5d4c385c700e9.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HiyGwoGOMI5Ao_fd"/></div></div></div></div></div><div class="po pp gp gr pq"><div role="button" tabindex="0" class="ab bv gv cb fp qq qr bn qs lb ex"><div class="qt l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qu qv fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qu qv fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated">艾伯斯·乌兹拉</p></div></div><div class="qy qz gw l"><h2 class="bd jd wa oi fp wb fr fs px fu fw jc bi translated">R中的数据科学</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wc au wd we wf sr wg an eh ei wh wi wj el em eo de bk ep" href="https://dwiuzila.medium.com/list/data-science-in-r-0a8179814b50?source=post_page-----80d69faf2109--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wk l fo"><span class="bd b dl z dk">7 stories</span></div></div></div><div class="rl dh rm fp ab rn fo di"><div class="di rd bv re rf"><div class="dh l"><img alt="" class="dh" src="../Images/e52e43bf7f22bfc0889cc794dcf734dd.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*10B3radiyQGAp-QA"/></div></div><div class="di rd bv rg rh ri"><div class="dh l"><img alt="" class="dh" src="../Images/945fa9100c2a00b46f8aca3d3975f288.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*o6A863Vdwq7ThlmW"/></div></div><div class="di bv rj rk ri"><div class="dh l"><img alt="" class="dh" src="../Images/3ca9e4b148297dbc4e7da0a180cf9c99.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*ekmX89TW6N8Bi8bL"/></div></div></div></div></div></div><div class="ab cl qh qi hx qj" role="separator"><span class="qk bw bk ql qm qn"/><span class="qk bw bk ql qm qn"/><span class="qk bw bk ql qm"/></div><div class="im in io ip iq"><p id="889e" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">[1]阿希什·瓦斯瓦尼、诺姆·沙泽尔、尼基·帕尔马、雅各布·乌兹科雷特、利永·琼斯、艾丹·戈麦斯、卢卡兹·凯泽、伊利亚·波洛舒欣(2017): <em class="og">注意力是你所需要的全部</em>。NeurIPS 2017。[ <a class="ae lh" href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> pdf </a></p><p id="395e" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">[2]克里斯托弗·奥拉(2015): <em class="og">了解LSTM网络</em>。[ <a class="ae lh" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">网址</a></p><p id="cb56" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">[3]悟空·莫汉达斯(2022): <em class="og">粉底——用ML制作</em>。[ <a class="ae lh" href="https://madewithml.com/courses/foundations/" rel="noopener ugc nofollow" target="_blank">网址</a></p><p id="caee" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">[4]杰伊·阿拉玛(2018): <em class="og">图解变形金刚</em>。[ <a class="ae lh" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">网址</a></p><p id="4f9f" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">[5]莱娜·沃伊塔(2022): <em class="og"> Seq2seq及注意事项</em>。[ <a class="ae lh" href="https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html" rel="noopener ugc nofollow" target="_blank">网址</a></p></div></div>    
</body>
</html>