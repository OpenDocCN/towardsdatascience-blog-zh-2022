<html>
<head>
<title>What Are Vision Transformers And How Are They Important For General Purpose Learning?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是视觉变形金刚，它们对通用学习有何重要性？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-are-vision-transformers-and-how-are-they-important-for-general-purpose-learning-edd008545e9e#2022-05-23">https://towardsdatascience.com/what-are-vision-transformers-and-how-are-they-important-for-general-purpose-learning-edd008545e9e#2022-05-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0470" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><strong class="ak">探索概念并尝试示例应用</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7a77683196a7de49c727ac14829ee85f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yo3QejZBlGLf5r2W0GnRPQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:行动中的视觉变形金刚(图片由作者提供)</p></figure><p id="d566" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在过去的几年里，人工智能领域取得了重大进展。生成模型是视觉领域中最成功的，然而，它们是为高度专业化的任务而构建的。每当任务改变时，这些专门的学习模型需要重建或再培训。因此，对通用学习模型的兴趣正在增加。其中一种模型叫做变形金刚。在本文中，我们简要讨论:</p><ul class=""><li id="54a7" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">什么是变压器？</li><li id="3837" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">什么是视觉转换器(ViT)？</li><li id="280d" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">ViTs的各种应用有哪些？</li><li id="5d4f" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">vit如何用于通用学习？</li></ul><p id="55fc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 1。</strong> <strong class="la iu">正文</strong>变形金刚</p><p id="a543" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">转换器的概念起源于自然语言处理(NLP)应用，其任务是理解文本并得出有意义的结论。Transformer模型已经实现了高性能，并且由于其简单性和可推广性，已经成为NLP领域中事实上的标准。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/3ab76a5d838f551fd0962155895744aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*prnQfLgRAanmt2jm0F8dxg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:用于语言翻译的转换器示例(图片由作者提供)</p></figure><p id="2eb1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在文本处理应用程序中，数据由来自一组固定词汇表的大量单词组成。在典型的变压器架构中，执行以下一系列步骤:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="b97d" class="mo mp it mk b gy mq mr l ms mt">► <em class="mu">Text is split into a set of words called tokens.</em></span><span id="614f" class="mo mp it mk b gy mv mr l ms mt"><em class="mu">► Each token is converted into an encoded/embedded vector (e.g. word2vec)</em></span><span id="07f0" class="mo mp it mk b gy mv mr l ms mt"><em class="mu">► The position of the word in the sequence is encoded using position embedding [1] and integrated with the word embedding.</em></span><span id="e37e" class="mo mp it mk b gy mv mr l ms mt"><em class="mu">► The embeddings are fed into the Transformer encoder.</em></span><span id="22b5" class="mo mp it mk b gy mv mr l ms mt"><em class="mu">► The encoder has a Multi-Layer Self-Attention Network (MSP) which assigns weights to tokens based on their relative importance in the sentence, hence embedding the context.</em></span><span id="92f1" class="mo mp it mk b gy mv mr l ms mt"><em class="mu">► A Multi-layer Perceptron (MLP) network follows the MSP and encodes the output from the attention network.</em></span><span id="dd79" class="mo mp it mk b gy mv mr l ms mt"><em class="mu">► There are multiple MSP and MLP blocks along with Norm layers inside the encoder.</em></span><span id="1770" class="mo mp it mk b gy mv mr l ms mt"><em class="mu">► A final MLP-Head Layer is added outside the encoder network which provides the logits. Logits can be converted to probabilities by applying an activation layer (e.g. softmax).</em></span></pre><p id="350a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们所看到的，变压器网络的架构是通用的，因为编码器输出不是为特定任务(例如分类)构建的，而是通过添加相应的MLP头来提供可用于多种应用的通用编码。这就是为什么变形金刚在迁移学习中是有用的，并且有希望实现通用的学习目标。</p><p id="204d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 2。</strong> <strong class="la iu">【视觉变形金刚(ViT) </strong></p><p id="b1e0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Vision Transformer (ViT)的概念是Transformer原始概念的扩展，后者在本文前面被描述为text transformer。这只是在图像域中的变换器的应用，在实现中稍作修改，以便处理不同的数据形态。更具体地说，ViT使用不同的方法进行标记化和嵌入。然而，通用架构保持不变。输入图像被分割成一组图像块，称为<em class="mu">视觉标记</em>。视觉标记被嵌入到一组固定维度的编码向量中。图像中碎片的位置与编码矢量一起被嵌入，并被馈入变换器编码器网络，该网络基本上与负责处理文本输入的网络相同。在图3 [2]中可以看到运行中的ViT架构示例。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/f65fef578f2416dddd7e91885a1ac47c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*_c8SqxPMY_dsApyvDJ8HtA.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:用于图像分类的视觉转换器的演示(来源:<a class="ae mx" href="https://arxiv.org/pdf/2010.11929" rel="noopener ugc nofollow" target="_blank">谷歌研究</a></p></figure><p id="2a47" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">ViT编码器中有多个模块，每个模块由三个主要处理元件组成:<em class="mu">层范数</em>、<em class="mu">多头注意力网络(MSP) </em>和<em class="mu">多层感知器(MLP) </em>。<em class="mu">层规范</em>保持训练过程在轨道上，让模型适应训练图像之间的变化。<em class="mu"> MSP </em>是一个网络，负责从给定的嵌入式视觉标记生成注意力地图。这些注意力地图有助于网络聚焦于图像中最重要的区域，例如物体。注意力图的概念与传统计算机视觉文献中的概念相同(例如，显著性图和阿尔法抠图)。</p><p id="d218" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="mu"> MLP </em>是一个两层分类网络，末端是GELU ( <em class="mu">高斯误差线性单元</em>)。最后一个MLP块，也称为<em class="mu"> MLP头</em>，用作变压器的输出。在此输出上应用<em class="mu"> softmax </em>可以提供分类标签(即，如果应用是图像分类)。</p><p id="34c1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 3。</strong>应用<strong class="la iu">应用</strong></p><p id="3184" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于其通用性，ViTs的应用几乎涵盖了视觉的所有方面。这包括图像分类、图像到文本/文本到图像生成、视觉推理、联想学习和多模态学习。在本节中，我们将通过实际例子来试验<em class="mu"> ViTs </em>最常见和最成功的应用。</p><p id="5fd3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 3.1。</strong> <strong class="la iu">【图像分类】(图像- &gt;标签)</strong></p><p id="f956" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">图像分类是视觉中最常见的问题。图像分类任务的最新技术是基于CNN ( <em class="mu">卷积神经网络</em>)的方法。<em class="mu">vit</em>在小型到中型数据集上没有产生可比较的性能，然而，它们在非常大的数据集上已经胜过<em class="mu">CNN</em>[3]。这是因为<em class="mu">CNN</em>比<em class="mu"> ViTs </em>更有效地对图像中的局部信息进行编码，这是由于局部受限感受野的应用。</p><p id="6658" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们试着用<em class="mu"> ViT </em>做实验。我们首先加载一个已经在imagenet上训练过的预训练模型，然后将其应用于从互联网上收集的一组随机图像。我们为给定的图像挑选五个最可能的标签。结果可以在图4中看到。结果可能不是超级完美的；然而，他们很好地记住，网络不是在这些图像上训练的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/f00cbaa2147328be664754ae5f68e5b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IEHAfSFmiq1jezyWq21B9A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4:ViT的图像分类输出(作者提供的图像)</p></figure><p id="6560" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 3.2。</strong> <strong class="la iu">图像字幕(图像- &gt;句子)</strong></p><p id="d693" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过生成描述图像内容的标题而不是一个单词的标签，可以实现更高级形式的图像分类。随着<em class="mu"> ViTs </em>的使用，这已经成为可能。<em class="mu"> ViTs </em>学习给定数据形态的一般表示，而不是一组粗糙的标签，因此，可以为给定图像生成描述性文本。我们将使用在<em class="mu"> COCO </em>数据集上训练的<em class="mu">ViT</em>【4】<em class="mu"/>的实现。这种字幕的结果可以从图5中看出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/ab32bd143ab5dd506886e3b4b1e60431.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kdt-P3ow2VkRpqfg1wx-zA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5:使用ViT生成图片标题(图片由作者提供)</p></figure><p id="e0bb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 3.3。</strong> <strong class="la iu">对比语言-图像预训练(图像&lt; - &gt;文本片段)</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/7ffe84786e613fbbb99eebb7690f6643.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uXQpVZ1EvU1-xA_wd4FucQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6:具有对比语言-图像预训练的ViT架构(来源:<a class="ae mx" href="https://arxiv.org/abs/2103.00020" rel="noopener ugc nofollow" target="_blank"> Google Research </a></p></figure><p id="3925" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">ViTs 的一个更高级的应用是学习图像和文本之间的联系。这是一个需要真正概括能力的应用程序，因为它需要文本和图像的抽象表示。这可以通过为文本片段和图像训练两个独立的变换编码器来实现。然后，可以通过构建余弦相似性矩阵来比较编码图像和文本特征各自的相似性。在CLIP(对比语言图像预训练)[5]中提出了这种变换器模型的相关实现。为了进一步理解这一点，我们取了五个样本图像，并为每个图像编写了一个小的文本片段。然后利用预先训练好的ViT模型，将文本片段和图像分别编码成一组文本和图像特征向量。我们计算文本和图像特征之间的余弦相似度。这将产生如图7所示的输出。可以看出，正确的图像-文本对之间的相似度最高。这种学习方式相对来说更普遍，是迁移学习的一种形式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/1dad79b89bc29c255a0ba334ee51d39f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*966Y5CARF90InJeiMPUBFA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7:图文关联学习的ViT结果(图片由作者提供)</p></figure><p id="cf3d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 4。</strong> <strong class="la iu">最后备注</strong></p><p id="7b37" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们解释了文本转换器和图像转换器的概念。然后，我们通过构建和实现实际的例子，探索了变压器模型的一些主要应用。如果您想更深入地研究并获得代码，那么您可以从git存储库中访问相应的python笔记本和帮助代码。<a class="ae mx" href="https://github.com/azad-academy/vision-transformers" rel="noopener ugc nofollow" target="_blank">https://github.com/azad-academy/vision-transformers</a></p><p id="0d62" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在接下来的文章中，我们将更深入地研究迁移学习和变压器网络的一些最新进展。</p><p id="fc4f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 5。</strong> <strong class="la iu">参考文献</strong></p><p id="ebd9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[1] Mehreen Saeed，Transformer模型中位置编码的温和介绍，<a class="ae mx" href="https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/A-Gentle-Introduction-To-Positional-Encoding-In-Transformer-Models-part-1/</a>，2022年</p><p id="da34" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2] Alexey Dosovitskiy，Lucas Beyer，Alexander，Dirk Weissenborn，Xiaohua Zhai，Thomas Unterthiner，Mostafa Dehghani，Matthias Minderer，Georg Heigold，Sylvain Gelly，Jakob Uszkoreit，Neil Houlsby，“一幅图像相当于16x16个字:大规模图像识别的变形金刚”，ICLR，2021年</p><p id="b505" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[3]陈湘宁、谢卓瑞、龚柏青，“当视觉变形金刚在没有预训练或强大数据增强的情况下胜过ResNets时<a class="ae mx" href="https://arxiv.org/abs/2106.01548" rel="noopener ugc nofollow" target="_blank">”，2022年</a></p><p id="678d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[4] Saahil等人，CaTr:变形金刚的图像字幕，<a class="ae mx" href="https://github.com/saahiluppal/catr" rel="noopener ugc nofollow" target="_blank">https://github.com/saahiluppal/catr</a>，2020年</p><p id="45eb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[5] <a class="ae mx" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Radford%2C+A" rel="noopener ugc nofollow" target="_blank">亚历克·拉德福德</a>，<a class="ae mx" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kim%2C+J+W" rel="noopener ugc nofollow" target="_blank">琼·金旭</a>，<a class="ae mx" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hallacy%2C+C" rel="noopener ugc nofollow" target="_blank">克里斯·哈勒西</a>，<a class="ae mx" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ramesh%2C+A" rel="noopener ugc nofollow" target="_blank">阿迪蒂亚·拉梅什</a>，<a class="ae mx" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Goh%2C+G" rel="noopener ugc nofollow" target="_blank">加布里埃尔·戈</a>，<a class="ae mx" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Agarwal%2C+S" rel="noopener ugc nofollow" target="_blank">桑迪尼·阿加瓦尔</a>，<a class="ae mx" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sastry%2C+G" rel="noopener ugc nofollow" target="_blank">吉里什·萨斯特里</a>，<a class="ae mx" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Askell%2C+A" rel="noopener ugc nofollow" target="_blank">阿曼达·阿斯克尔</a>，<a class="ae mx" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mishkin%2C+P" rel="noopener ugc nofollow" target="_blank">帕梅拉·米什金</a>，<a class="ae mx" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Clark%2C+J" rel="noopener ugc nofollow" target="_blank">杰克·克拉克</a>，<a class="ae mx" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Krueger%2C+G" rel="noopener ugc nofollow" target="_blank">格雷琴·克鲁格</a></p></div></div>    
</body>
</html>