<html>
<head>
<title>Natural Language Process for Judicial Sentences with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 实现司法判决的自然语言处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/natural-language-process-for-judicial-sentences-with-python-bb60a6d3cc0b#2022-12-02">https://towardsdatascience.com/natural-language-process-for-judicial-sentences-with-python-bb60a6d3cc0b#2022-12-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/a672adc5bf31df2e710172a35efb40c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*F9uGneWezzfOy-mt.jpg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/</a></p></figure><div class=""/><div class=""><h2 id="62c0" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">第 6 部分:文档嵌入</h2></div><p id="0906" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本系列的最新文章中，我们经常提到，为了用作数学模型的输入(许多 NLP 应用程序的最终目标)，文本数据需要以某种方式转换成数字表示。我们已经看到了其中的一些，如 TF-IDF 矩阵或词袋。然而，在捕捉单词或文档的含义时，这些技术并不“有用”。</p><p id="83b5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是文档嵌入进入游戏的时候。文档嵌入的一般思想是以这样一种方式用向量表示文档，即一旦在向量空间中表示，向量之间的数学距离就表示它们所表示的文档之间的相似性。</p><p id="ad8d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在深入研究文档嵌入及其在我们分析中的应用之前，我们首先需要介绍它的起源，可以追溯到单词嵌入的更细粒度的概念。</p><h2 id="ed10" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">单词嵌入</h2><p id="ff95" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">使用单词嵌入技术，我们将向量定义为单词的数字表示，这样，具有相似含义的单词具有相似的向量表示。</p><p id="e0f3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个想法是，我们需要用数字来表示单词的意思，这样我们就可以进行如下活动:</p><ul class=""><li id="abbb" class="ms mt jj la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated">ML 模型训练</li><li id="e9f8" class="ms mt jj la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">单词所在语料库中潜在模式的可视化</li><li id="9c96" class="ms mt jj la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">预测所分析单词周围的单词</li></ul><p id="2170" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">和许多其他人。</p><p id="1719" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一种流行的单词嵌入技术是 Word2Vec，在 2013 年的这篇<a class="ae jg" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中介绍。Word2Vec 旨在将单词的含义(包括同义词、类比等)捕获到矢量表示中。这背后的思想是，例如，在向量空间中,“巴黎”的数字表示应该比“伦敦”的数字表示更接近“法国”的数字表示。</p><p id="54d5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Word2Vec 基于两种算法:</p><ul class=""><li id="77bf" class="ms mt jj la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated">连续单词包(CBOW)→它使用滑动窗口预测给定上下文的单词，即前一个和下一个单词。</li><li id="b527" class="ms mt jj la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">Skip-gram →它以相反的方向工作，使用一个单词作为输入来预测它周围的上下文。尽管它比 CBOW 慢，但在处理生僻字时效果更好。</li></ul><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ng"><img src="../Images/41b39b2c69b02d8bed4dd0c904b6cb17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pIAuAlAFp2qvN6iE2SG1LA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:https://arxiv.org/pdf/1301.3781.pdf<a class="ae jg" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="9dc5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">单词嵌入对于许多上下文来说肯定是有用的，但是，这可能还不够。事实上，可能有一些与单词相关的含义不仅严格依赖于它们的上一个或下一个单词，而且依赖于它们所引用的整个文档。也就是说，Word2Vec 算法可能很难捕捉到这样一个事实，即两个同音异义词在不同的文档中可能有非常不同的含义(取决于它们的上下文)。</p><p id="ee76" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是引入文档嵌入的原因。对于文档嵌入，我们有一个单词嵌入的离散近似，它有助于捕捉整个上下文，并旨在给出更有意义的向量表示。最流行的算法之一是 Doc2Vec，由 Quoc Le 和 Tomas Mikolov 于 2014 年在这篇<a class="ae jg" href="https://arxiv.org/pdf/1405.4053.pdf" rel="noopener ugc nofollow" target="_blank">文章</a>中介绍。</p><h2 id="d2f8" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">文档嵌入</h2><p id="b14d" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">Doc2Vec 模型用于创建一组单词的矢量化表示，这些单词作为一个整体，而不是单个单词。</p><p id="a0a0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">至于 Word2Vec，Doc2Vec 算法有两种变体:</p><ul class=""><li id="ce3d" class="ms mt jj la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated">分布式内存模型(DM)→类似于 Word2Vec 算法中的 CBOW 变体，DM 模型在其单词输入中还结合了一个段落矩阵，该矩阵可以被认为是另一个单词。“分布式记忆”这个名称表明了这样一个事实，即段落矩阵充当记忆单个单词不能捕捉的当前上下文中缺少的内容的记忆。</li></ul><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/f69cec99e0896b670bc7c026c43123ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*HFWZB-Y9GmmXEXte4c2TNA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:https://arxiv.org/pdf/1405.4053v2.pdf<a class="ae jg" href="https://arxiv.org/pdf/1405.4053v2.pdf" rel="noopener ugc nofollow" target="_blank"/></p></figure><ul class=""><li id="78fd" class="ms mt jj la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated">分布式单词包(DBOW)→类似于 Word2Vec 的 skip-gram 变体，但它不是使用单个单词来预测其周围的上下文，而是使用段落矩阵(与上面解释的概念相同)。</li></ul><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/8f6136f4d246ee342cf85263d9f2c599.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*DD7t7HCg-YtndnJ9b-v9YA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:<a class="ae jg" href="https://arxiv.org/pdf/1405.4053v2.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1405.4053v2.pdf</a></p></figure><h2 id="8995" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">用 Python 实现</h2><p id="57f1" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">我将使用<code class="fe nn no np nq b">gensim</code>中可用的<code class="fe nn no np nq b">Doc2Vec</code>模块。更具体地说，我将使用 DBOW 变体。</p><p id="ba9d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另外，出于可视化的目的，我将通过 t-SNE 算法来表示嵌入的文档。</p><p id="049c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，让我们开始创建文档矩阵 X，我们将在其上训练 Doc2Vec 算法。</p><pre class="nh ni nj nk gt nr nq ns bn nt nu bi"><span id="6395" class="nv lv jj nq b be nw nx l ny nz">df_factor = pd.read_pickle('data/df_factor.pkl')<br/><br/>from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer<br/>documents = df_factor.Tokens.apply(str).tolist()<br/># hash vectorizer instance<br/>hvec = HashingVectorizer(lowercase=False, analyzer=lambda l:l, n_features=2**12)<br/><br/># features matrix X<br/>X = hvec.fit_transform(documents)</span></pre><p id="a69e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们导入所有模块并训练我们的算法:</p><pre class="nh ni nj nk gt nr nq ns bn nt nu bi"><span id="9134" class="nv lv jj nq b be nw nx l ny nz">from gensim.models import Doc2Vec<br/>from gensim.models.doc2vec import FAST_VERSION<br/>from gensim.models.doc2vec import TaggedDocument<br/><br/>corpus = []<br/>for docid, document in enumerate(documents):<br/>    corpus.append(TaggedDocument(document.split(), tags=["{0:0&gt;4}".format(docid)]))<br/><br/>d2v_model = Doc2Vec(size=300,window=5,hs=0,sample=0.000001,negative=5, min_count=10, <br/>                    workers=-1, iter=5000, dm=0, dbow_words=1)<br/><br/>d2v_model.build_vocab(corpus)<br/><br/>d2v_model.train(corpus, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)</span></pre><p id="91a1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们检索与文档“0001”最相似的 5 个文档:</p><pre class="nh ni nj nk gt nr nq ns bn nt nu bi"><span id="a83b" class="nv lv jj nq b be nw nx l ny nz">target_doc = '0001'<br/># retrieve the 5 most similar documents<br/>d2v_model.docvecs.most_similar(target_doc, topn=5)</span></pre><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/bf2b581df9c6d192ef491e9383020b89.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*xrvTNO-z7SoRefGHnLbDsQ.png"/></div></figure><p id="04f7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个数组代表(target_doc，相似性得分)。</p><p id="9b54" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我还想可视化我的矢量化文档。为此，我将使用<a class="ae jg" href="https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/" rel="noopener ugc nofollow" target="_blank">T-分布式随机邻居嵌入(t-SNE) </a>算法，这是一种非线性降维技术(在这里阅读关于降维的更多信息<a class="ae jg" href="https://medium.com/towards-data-science/understanding-the-link-between-pca-and-eigenvectors-468435649d57" rel="noopener">)。</a></p><pre class="nh ni nj nk gt nr nq ns bn nt nu bi"><span id="84f9" class="nv lv jj nq b be nw nx l ny nz">#t-SNE embedding algorithm<br/><br/>from sklearn.model_selection import train_test_split<br/><br/># test set size of 20% of the data and the random seed 123 for replicability<br/>X_train, X_test = train_test_split(X.toarray(), test_size=0.2, random_state=123)<br/><br/>print("X_train size:", len(X_train))<br/>print("X_test size:", len(X_test), "\n")<br/><br/>from sklearn.manifold import TSNE<br/><br/>tsne = TSNE(verbose=1, perplexity=5)<br/>X_embedded = tsne.fit_transform(X_train)</span></pre><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/3ec154dd3c7106f31f6e6f7be6e99971.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*Y0I91XCRW3m04uaZemmuCw.png"/></div></figure><p id="4d91" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们导入可视化软件包来绘制缩减的维度:</p><pre class="nh ni nj nk gt nr nq ns bn nt nu bi"><span id="71ed" class="nv lv jj nq b be nw nx l ny nz">from matplotlib import pyplot as plt<br/>import seaborn as sns<br/><br/># sns settings<br/>sns.set(rc={'figure.figsize':(15,15)})<br/><br/># colors<br/>palette = sns.color_palette("bright", 1)<br/><br/># plot<br/>sns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)<br/><br/>plt.title("t-SNE Press Articles")<br/># plt.savefig("plots/t-sne_covid19.png")<br/>plt.show()</span></pre><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oc"><img src="../Images/fdaf656a91a781c4aa2977c8121cd0bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t2bLWKTUCoxDTQxzmbPnkg.png"/></div></div></figure><p id="4b8b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这张图片的边缘有一些有趣的集群，这意味着这些文档之间可能有一些共同的讨论趋势。用潜在主题分析来扩展这种分析也可能是有用的，以查看那些聚类是否实际上代表潜在主题。</p><p id="9721" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在下一篇文章中，我们将进一步研究低维数据，以执行聚类分析。我们还将通过命名实体来查看文章的词汇组成。</p><p id="3a2a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以请继续关注第 7 部分！</p><h1 id="ae97" class="od lv jj bd lw oe of og lz oh oi oj mc kp ok kq mf ks ol kt mi kv om kw ml on bi translated">参考</h1><ul class=""><li id="9b78" class="ms mt jj la b lb mn le mo lh oo ll op lp oq lt mx my mz na bi translated"><a class="ae jg" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> NLTK::自然语言工具包</a></li><li id="493b" class="ms mt jj la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">Python 中的 spaCy 工业级自然语言处理</li><li id="aea7" class="ms mt jj la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated"><a class="ae jg" href="https://www.justice.gov/news" rel="noopener ugc nofollow" target="_blank">司法新闻| DOJ |司法部</a></li><li id="c56a" class="ms mt jj la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated"><a class="ae jg" href="https://www.kaggle.com/datasets/jbencina/department-of-justice-20092018-press-releases" rel="noopener ugc nofollow" target="_blank">司法部 2009-2018 年新闻发布| Kaggle </a></li><li id="cca0" class="ms mt jj la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated"><a class="ae jg" href="https://en.wikipedia.org/wiki/Distributional_semantics" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Distributional_semantics</a></li><li id="ed40" class="ms mt jj la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated"><a class="ae jg" href="https://aurelieherbelot.net/research/distributional-semantics-intro/" rel="noopener ugc nofollow" target="_blank">https://aurelieherbelot . net/research/distributional-semantics-intro/</a></li><li id="aab1" class="ms mt jj la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated"><a class="ae jg" href="https://arxiv.org/pdf/1405.4053v2.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1405.4053v2.pdf</a></li><li id="b58a" class="ms mt jj la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated"><a class="ae jg" href="https://arxiv.org/pdf/1707.02377.pdf" rel="noopener ugc nofollow" target="_blank">arxiv.org 1707.02377.pdf</a></li><li id="fdf0" class="ms mt jj la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated"><a class="ae jg" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1301.3781.pdf</a></li></ul></div></div>    
</body>
</html>