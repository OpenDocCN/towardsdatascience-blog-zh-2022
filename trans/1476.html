<html>
<head>
<title>Taking Reinforcement Learning Algorithms to Real World Robotics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将强化学习算法应用于真实世界的机器人</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/taking-reinforcement-learning-algorithms-to-real-world-robotics-background-needs-challenges-7b7f3edcad0c#2022-04-11">https://towardsdatascience.com/taking-reinforcement-learning-algorithms-to-real-world-robotics-background-needs-challenges-7b7f3edcad0c#2022-04-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e2dc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">背景、需求、挑战和展望</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/959511c885bb914aa6b2e1d4e7527626.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Oi0SzksoFWxNjYlw"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@possessedphotography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">附身摄影</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="9c21" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">RL证明了它在游戏、商业ML和机器人方面的成功，它已经演变成人工智能的瑞士军刀。这篇文章探索了RL目前可以做什么，为什么我们需要机器人学的RL，什么样的挑战和未来的工作会是什么样子。</p><h1 id="138b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">背景</h1><p id="487b" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">强化学习(RL)指的是一种通过试错进行学习的算法范式。RL代理在基于奖励的系统中学习。代理人采取行动，并因成功而获得奖励或因失败而受到惩罚。因此，代理人通过最大化回报成功地学会了执行任务。这类似于人类和动物的学习方式(帕夫洛的狗有印象吗？).</p><p id="93cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当计算机程序AlphaGo击败世界著名的职业围棋选手Lee Sedol时，RL引起了我们的注意。这一点很特别，因为当时的其他计算机程序很难达到业余水平，也很难在有障碍的情况下击败职业选手。AlphaGo获得了业余棋手的比赛数据，从而很好地理解了比赛，成功地设计出了击败职业棋手的策略。几年后，当AlphaZero(又是一个计算机程序)自学下国际象棋、五子棋和围棋，并且每次都击败了世界冠军计算机程序时，RL让我们更加惊讶。</p><p id="09be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在商业上，RL比一些现有的ML框架工作得更好——例如，YouTube正在推动使用RL进行推荐。网飞使用RL展示更相关、更吸引人、更个性化的艺术作品，让用户粘在屏幕上。脸书将RL用于个性化推送通知[1]。甚至你看到的广告种类也是由一种RL算法决定的，这种算法可以最大化点击率(CRT) [2，3]。RL已经用于股票交易，其中RL代理决定是否持有、出售或购买股票。IBM有一个最复杂、最成功的基于RL的交易平台。</p><p id="161b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们来谈谈现实世界的机器人——为什么我们需要RL，以及它的表现如何。</p><h1 id="8e4e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">需要</strong></h1><p id="e805" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">由于机器人无处不在，我们的制造过程变得高效且有利可图。机器人有能力在有组织的环境中以高准确度和精确度执行平凡的、重复的任务。然而，这些制造工艺对其环境的变化并不稳健。这似乎违反直觉，但即使是高度自动化的生产线也可能需要数天的专业劳动来适应新产品规格。</p><p id="e62a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，仅仅是为任务编码和为装配线规划机器运动就是一个漫长而乏味的过程。行业中有效的解决方案通常是为特定任务而设计的。对于从传送带上捡起一包冰淇淋的简单任务，甚至包的高度和宽度都在算法中进行了编码。产品规格的微小变化需要改变算法，这可能需要改变生产线的处理速度、机械臂的运动参数、机械手的抓力或压力等。</p><p id="cea3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果机器人在稍微偏离过程参数的情况下就有这么多问题，你能想象它们在我们家、办公室或街道上的非结构化环境中的情况吗？这些挑战也是为什么我们没有机器人在家工作和在街上驾驶我们的汽车。直到最近，随着亚马逊仓库机器人、Roombas和自动驾驶汽车公司测试他们的算法，我们无法想象机器人在固定的装配线结构之外自由工作。</p><p id="b647" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果机器人可以通过从经验中学习，即使用RL方法，来学习处理这些挑战，这不是很好吗？在这种情况下，我们可以让机器人留在现实世界中，在那里它尝试各种事情，并利用这些学到的经验学习执行任务。机器人可以学习更稳健地完成制造任务，并学习新的任务，如驾驶或抓取我们不知道如何精确编程的各种对象。毕竟，RL已经在其他智能领域展现出了前景。RL的优势在于在没有系统模型的情况下找到解决方案。当我们不知道如何执行一项任务，但知道完成的任务是什么样子时，RL可以找到解决方案。</p><h1 id="54b3" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">挑战</h1><p id="0df4" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">尽管RL在现实世界机器人中的效用以及RL已被证明的能力，但是RL的使用对于现实世界的机器人任务来说是相当具有挑战性的。机器人应用中RL的一个众所周知的例子是OpenAI的手解决一个规则立方体。大量的工作、复杂的模拟软件和计算能力使这个项目获得了成功。虽然被认为是成功的，但OpenAI的手可以解决复杂的魔方问题，只有20%的成功率[4]。</p><p id="75ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们来谈谈在真实世界的机器人上使用RL的挑战，以及是什么使得它难以实现可靠的结果。这些大多摘自论文[5]和我自己的经历:</p><p id="b7e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">样本效率&amp;安全学习</strong></p><p id="c848" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管RL算法通过反复试验来学习，但它们可能需要很长时间来完成。在现实世界中，保持机器人实时运行直到学习发生可能会花费大量的时间。此外，在没有人类监督的情况下，为机器人设计最佳和最安全的方式来与环境长期互动是具有挑战性的，也是潜在不安全的。</p><p id="7408" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">可靠稳定的学习</strong></p><p id="345c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">众所周知，RL算法很难训练。此外，学习对超参数的选择非常敏感，甚至对使用什么随机种子来初始化和训练算法也非常敏感。</p><p id="f86d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">模拟的使用</strong></p><p id="b3e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">“样本效率和安全学习”的挑战可以通过“简单地”首先训练机器人进行模拟，并在现实世界中微调算法来解决。模拟可以加快学习所需的真实世界时间。不幸的是，事情没有这么简单。在最糟糕的情况下，创建一个模拟会比在现实世界中训练机器人花费更多的时间。目前的模拟在视觉上效果很好，但当涉及到计算机器人和物体之间的真实作用力时，他们还有很长的路要走。</p><p id="f300" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">巨大的探索空间</strong></p><p id="d4be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">试错学习的时候，探索空间是巨大的。例如，要学会翻转煎饼，机器人必须探索无限的行动空间——上、下、右、左，随时抓握。发现一个准确的顺序，达到锅，抓住它，然后移动它，使煎饼翻转比它看起来更复杂。尽管向人类学习可能是克服这一挑战的方法之一，但这并不总是可能的。</p><p id="80a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">泛化</strong></p><p id="5fdd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在一个环境中学习一项技能，并在其他环境中使用该技能的一部分，可以极大地减少机器人的训练时间和探索空间。对环境、技能或任务实现这样的概括仍然是一个未解决的任务。</p><p id="1f94" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">不断变化的世界和不断变化的机器人参数</strong></p><p id="2648" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现实世界是高度复杂的，我们还没有传感器来精确测量每个机器人世界的互动。此外，它总是不同于实验测试设置[6]。如果RL算法没有接收到所有的信息，那么期望总是成功是不公平的。此外，机器人的内部控制参数可能会随着时间的推移而改变，并且它们可能会因同一品牌的机器人而异。因此，一次性训练一个机器人，并期望所有其他机器人执行相同的任务(对这些参数敏感)可能行不通！</p><h1 id="2013" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">观点</h1><p id="601d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">RL的最终目标是让学习对机器人来说像对人类一样可扩展和自然。该领域的突破可以引领我们进入一个新时代，机器人可以自己学习任何任务。虽然RL越来越受欢迎，有大量的研究和资金，但最终目标似乎还很远。</p><p id="76d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，我们仍然可以通过将RL代理与已经建立的数学证明相结合来解决现实世界机器人中看似具有挑战性的任务——无论是对于控制理论还是物理。通过正确的问题表述和用我们已经知道的关于世界的知识来证明RL，可以获得快速而有用的结果。我将很快写一篇文章，介绍如何制定用于端到端解决方案的RL问题，以及如何降低它们的复杂性。</p><p id="f64a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我看来，从头到尾解决我们目前知道解决方案的基本问题，比如从头开始控制机器人手臂的运动，并不是RL最擅长的地方。定义问题，让RL只需要学习我们无法建模或公式化的东西，这将是更快地将RL融入现实世界机器人的最有效方法之一——直到该领域的研究和算法赶上来。</p><p id="b297" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">参考文献</strong></p><p id="7377" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1] <a class="ae kv" href="https://www.anyscale.com/blog/enterprise-applications-of-reinforcement-learning-recommenders-and-simulation-modeling" rel="noopener ugc nofollow" target="_blank">强化学习的企业应用:推荐器和仿真建模</a>，本·洛里卡，2020。</p><p id="2197" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] <a class="ae kv" href="https://bdtechtalks.com/2021/02/22/reinforcement-learning-ad-optimization/" rel="noopener ugc nofollow" target="_blank">强化学习如何选择你看到的广告</a>，本·迪克森，2021。</p><p id="8bac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3] <a class="ae kv" href="https://mobiledevmemo.com/bayesian-bandits-behind-the-scenes-of-facebooks-spend-allocation-decisioning/" rel="noopener ugc nofollow" target="_blank">《贝氏强盗:脸书支出分配决策的幕后》</a>，Eric Benjamin Seufert，2020年。</p><p id="58ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4] <a class="ae kv" href="https://openai.com/blog/solving-rubiks-cube/" rel="noopener ugc nofollow" target="_blank">用机器手解魔方</a>，OpenAI博客，2019。</p><p id="0939" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[5] <a class="ae kv" href="https://arxiv.org/abs/2102.02915" rel="noopener ugc nofollow" target="_blank">如何用深度强化学习训练你的机器人:我们学到的教训。</a> Ibarz J等人，国际机器人研究杂志。2021;40(4–5):698–721.</p><p id="b32b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[6]<a class="ae kv" href="http://ai.googleblog.com/2021/06/the-importance-of-ab-testing-in-robotics.html" rel="noopener ugc nofollow" target="_blank">A/B测试在机器人中的重要性</a> s，谷歌人工智能博客，2021。</p></div></div>    
</body>
</html>