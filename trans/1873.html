<html>
<head>
<title>Diffusion Models Made Easy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">扩散模型变得简单</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/diffusion-models-made-easy-8414298ce4da#2022-05-02">https://towardsdatascience.com/diffusion-models-made-easy-8414298ce4da#2022-05-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ff62" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><strong class="ak">了解去噪扩散概率模型的基础知识</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fd2dffde263f8aab647508655a6cf5ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MLRitrnUdOy6rPtZfNwO4w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:去噪扩散概率模型的过程(图片由作者提供)</p></figure><p id="3248" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 1 </strong>。<strong class="la iu">简介</strong></p><p id="0940" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在最近的过去，我谈到了GANs和VAEs作为两个重要的生成模型，已经取得了很大的成功和认可。gan非常适合多种应用，但是它们很难训练，并且由于一些挑战，例如模式崩溃和消失梯度，它们的输出缺乏多样性。虽然VAEs具有最坚实的理论基础，但是，在VAEs中，良好的损失函数的建模是一个挑战，这使得它们的输出是次优的。</p><p id="50c7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">还有另一套技术源自概率似然估计方法，并从物理现象中获得灵感；这就是所谓的扩散模型。扩散模型背后的中心思想来自气体分子的热力学，由此分子从高密度向低密度区域扩散。这种运动在物理学文献中经常被称为熵增加或热寂。在信息论中，这等同于由于噪声的逐渐介入而导致的信息丢失。</p><p id="b9e0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">扩散建模中的关键概念是，如果我们可以建立一个学习模型，该模型可以学习由于噪声导致的信息的系统衰减，那么应该可以逆转该过程，从而从噪声中恢复信息。这个概念类似于VAEs，它试图通过首先将数据投影到潜在空间，然后将其恢复到初始状态来优化目标函数。然而，该系统的目的不是学习数据分布，而是在<em class="lu">马尔可夫链</em>中模拟一系列噪声分布，并通过以分层方式撤销/去噪数据来“解码”数据。</p><p id="84bf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 2。</strong> <strong class="la iu">去噪扩散模型</strong></p><p id="911a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">去噪扩散模型的思想由来已久。它源于扩散图概念，这是机器学习文献中使用的降维技术之一。它还借用了概率方法中的概念，如已经在许多应用中使用的<em class="lu">马尔可夫链</em>。最初的去噪扩散方法是在<em class="lu"> Sohl-Dickstein等人</em>中提出的。[1].</p><p id="57a5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">去噪扩散建模是两步过程:正向扩散过程和反向过程或重建。在前向扩散过程中，连续引入高斯噪声，直到数据变成全噪声。反向/重建过程通过使用神经网络模型学习条件概率密度来消除噪声。这种过程的示例描述可以在图1中看到。</p><p id="35ed" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 3。</strong> <strong class="la iu">前进过程</strong></p><p id="a78a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以将前向扩散过程正式定义为<em class="lu">马尔可夫链</em>，因此，与VAEs中的编码器不同，它不需要训练。从初始数据点开始，我们为<strong class="la iu"> <em class="lu"> T </em> </strong>连续步骤添加高斯噪声，并获得一组噪声样本。<strong class="la iu"> </strong>时间<strong class="la iu"> <em class="lu"> t </em> </strong>的概率密度预测仅依赖于时间<strong class="la iu"> <em class="lu"> t-1 </em> </strong>的直接前趋，因此条件概率密度可以计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/542e88e0e6a3ed31cc10860669168351.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*A2QwjXNBM7D6sc_dp4ZgtQ.png"/></div></figure><p id="7380" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">整个过程的完整分布可以计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/bda1deb0a2b2d010873581bde64f0b5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*xDyrQt39UD5yUP6yPFHd-A.png"/></div></figure><p id="98e0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，密度函数的均值和方差取决于参数βτ，该参数是一个超参数，其值可以在整个过程中取为常数，也可以在连续步骤中逐渐改变。对于微分参数值分配，可能存在可用于模拟行为的函数范围(例如，sigmoid、tanh、linear等。).</p><p id="f548" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上面的推导足以预测连续状态，然而，如果我们想要在任何给定的时间间隔<strong class="la iu"> <em class="lu"> t </em> </strong>采样，而不经过所有中间步骤，因此，允许有效的实现，那么我们可以通过将超参数替换为ατ = 1 — βτ <strong class="la iu">来重新制定上面的等式。上面的重新表述变成了:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/5421d6c078d8fcfa7be8e6b44a6e8734.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*65edPVjlzVeRgaZr9J-9hA.png"/></div></figure><p id="63cd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了在时间步长<strong class="la iu"> <em class="lu"> t </em> </strong>产生样本，并且在时间步长<strong class="la iu"><em class="lu"/></strong>t-1可用概率密度估计，我们可以采用热力学中的另一个概念，称为，<em class="lu">朗之万动力学</em>。根据<em class="lu">随机梯度朗之万动力学</em>【2】我们可以仅通过<em class="lu">马尔可夫链</em>更新中密度函数的梯度来采样系统的新状态。基于前一时间点<strong class="la iu"> <em class="lu"> t-1 </em> </strong>的步长<strong class="la iu"> ε </strong>的新数据点在时间<strong class="la iu"> <em class="lu"> t </em> </strong>的采样可以计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/a7a354dd770dfa394adb54820ecc1619.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*zBhrM5T5qBSSHKV-GOAVLg.png"/></div></figure><p id="e2cf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 4。</strong>重建<strong class="la iu">重建</strong></p><p id="a733" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">相反的过程要求在给定系统当前状态的情况下，在较早的时间步估计概率密度。这意味着当<em class="lu">T =</em><strong class="la iu">T5】T</strong>时，估计<strong class="la iu"> q </strong> (χτ-1 | χτ),从而从各向同性高斯噪声中生成数据样本。然而，与正向过程不同，从当前状态对先前状态的估计需要所有先前梯度的知识，在没有能够预测这种估计的学习模型的情况下，我们无法获得这些知识。因此，我们必须训练一个神经网络模型，该神经网络模型基于学习到的权重<strong class="la iu"> θ </strong>和在时间<strong class="la iu">t<em class="lu">t</em>T15】的当前状态来估计<strong class="la iu">ψθ</strong>(χτ-1 |χτ)。这可以估计如下:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/05e1415d02252000d5066198146650d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*Y3tBRo4IjZiw2eErWoxOOg.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/8906b0149d8fb14d53320a234c9838d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*f3Ml765JJFEezFzCLASx6Q.png"/></div></figure><p id="826d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">平均函数的参数化由Ho<em class="lu">提出。等</em> [3]并可计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/bc965e3ee998be2210ffa2b35e4655f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*acHG_WFTWCXL6GCQyevE4g.png"/></div></figure><p id="c433" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作者在<em class="lu">何。et al. </em> [3]建议使用固定方差函数，如σθ=βτ。在时间<strong class="la iu"> <em class="lu"> t-1 </em> </strong>的样本可以计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/a5f64e2833c5684a66351998cd23d498.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*g_U_cCHGcaxqxPHaQFh7Zw.png"/></div></figure><p id="36af" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 5。</strong> <strong class="la iu">训练和结果</strong></p><p id="97d3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 5.1。</strong> <strong class="la iu">模型的构建</strong></p><p id="1340" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">用于扩散模型的训练中的模型遵循与VAE网络相似的模式，然而，与其他网络架构相比，它通常保持更简单和直接。输入层的输入大小与数据维度的输入大小相同。根据网络要求的深度，可以有多个隐藏层。中间层是具有各自激活功能的线性层。最终层的大小再次与原始输入层的大小相同，从而重建原始数据。在<em class="lu">去噪扩散网络</em>中，最终层由两个独立的输出组成，每个输出分别用于预测概率密度的均值和方差。</p><p id="c83c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 5.2。</strong> <strong class="la iu">损失函数的计算</strong></p><p id="5c9f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">网络模型的目标是优化以下损失函数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi md"><img src="../Images/92a791e08d1bab6ad1ca095b997982ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*9uGr_zaQ0HcYIXKXPSTXJA.png"/></div></figure><p id="6bfb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Sohl-Dickstein等人【1】提出了这种损失函数的简化形式，它用两个高斯分布和一组熵之间的KL散度的线性组合来表示损失。这简化了计算，并且使得实现损失函数变得容易。损失函数于是变成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi me"><img src="../Images/4dd27ad3fad1d31a4f309afb41fc35ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*IG_OQ3Pl_bNOH6rS_3UDpA.png"/></div></figure><p id="d278" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu"> Ho等人</em>【3】在损失函数中提出了进一步的简化和改进，其中均值的参数化如前一节所述用于正向过程。因此，损失函数变成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/8cbf53013df74d186caf06bcc3deb48e.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*w6l7323DpgucTITj5wGlfw.png"/></div></figure><p id="950b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 5.3。</strong>结果<strong class="la iu">结果</strong></p><p id="c49e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过遵循<em class="lu">马尔可夫链</em>添加高斯噪声的正向过程的结果可以在下图中看到。时间步长的总数是100，而该图显示了来自生成的序列集的10个样本。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mg"><img src="../Images/436cc60e51bd8a86accb407fa7d08dd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vEOa0hQr4p9MREobt7U_sQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:S曲线合成数据集前向扩散过程的结果(图片由作者提供)</p></figure><p id="ac51" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下图显示了反向扩散过程的结果。最终输出的质量取决于超参数的调整和训练时期的数量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mg"><img src="../Images/0e34dbbf67932c701fee90617d0ad421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d7geBWXutJpl-Fr3MzxYfw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:从各向同性高斯噪声中重建数据的结果(图片由作者提供)</p></figure><div class="kj kk kl km gt ab cb"><figure class="mh kn mi mj mk ml mm paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/4cbed0c684a691167fb5982088e3f804.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*vuWRw-ZAE8L9kX2qPuq92Q.gif"/></div></figure><figure class="mh kn mi mj mk ml mm paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/a1d4999b934b650dcdf77fe54f8ecfe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*3DRc8L5jl0Q6d3HXwC-d6w.gif"/></div></figure><figure class="mh kn mi mj mk ml mm paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/8653be4005f8044575f973010a7fb6c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*wiP5nfbtihXJf2snade2cg.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk mn di mo mp translated">图4:去噪扩散模型在3个不同数据集上的结果:瑞士卷、月亮和S曲线(图片由作者提供)</p></figure></div><p id="fb7d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 6。</strong> <strong class="la iu">结论</strong></p><p id="d607" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇文章中，我们讨论了扩散模型的基础，以及它们的实现。尽管扩散模型在计算上比其他深度网络架构更昂贵，但是它们在某些应用中表现得更好。例如，最近在文本和图像合成任务中的应用，扩散模型的表现优于其他架构[4]。更多的实现细节和代码可以在下面的github资源库中找到:<a class="ae mq" href="https://github.com/azad-academy/denoising-diffusion-model.git" rel="noopener ugc nofollow" target="_blank">https://github . com/Azad-academy/noking-diffusion-model . git</a></p><p id="1a20" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">订阅并关注更多更新:<a class="ae mq" href="https://azad-wolf.medium.com/" rel="noopener">azad-wolf.medium.com/</a></p><p id="1b0f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">参考文献</strong></p><p id="56e9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[1] <a class="ae mq" href="https://arxiv.org/pdf/1503.03585" rel="noopener ugc nofollow" target="_blank"> Sohl-Dickstein，j .、Weiss，E. A .、Maheswaranathan，n .&amp;Ganguli，S. (2015)。使用非平衡热力学的深度无监督学习。arXiv预印本arXiv:1503.03585。</a></p><p id="1036" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2]马克斯·韦林和伊惠德。<a class="ae mq" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.226.363" rel="noopener ugc nofollow" target="_blank">“通过随机梯度朗之万动力学进行贝叶斯学习。”</a> ICML 2011。</p><p id="8828" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[3] <a class="ae mq" href="https://arxiv.org/pdf/2006.11239" rel="noopener ugc nofollow" target="_blank">何，j，贾恩，a .，&amp;阿贝耳，P. (2020)。<em class="lu">去噪扩散概率模型</em>。预印本arXiv:2006.11239。</a></p><p id="2963" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[4] <a class="ae mq" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dhariwal%2C+P" rel="noopener ugc nofollow" target="_blank">普拉富拉·达瑞瓦尔</a>，<a class="ae mq" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nichol%2C+A" rel="noopener ugc nofollow" target="_blank">亚历克斯·尼科尔</a>，<a class="ae mq" href="https://arxiv.org/abs/2105.05233" rel="noopener ugc nofollow" target="_blank"> <em class="lu">扩散模型在图像合成上击败甘斯</em>，arXiv: 2105.05233 </a></p></div></div>    
</body>
</html>