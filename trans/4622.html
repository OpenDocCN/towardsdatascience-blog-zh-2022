<html>
<head>
<title>Graph Neural Networks as gradient flows</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">作为梯度流的图形神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/graph-neural-networks-as-gradient-flows-4dae41fb2e8a#2022-10-14">https://towardsdatascience.com/graph-neural-networks-as-gradient-flows-4dae41fb2e8a#2022-10-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="b202" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">基普夫&amp;韦林反击战</h2><div class=""/><div class=""><h2 id="1a05" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">在一些简单的约束下，图形神经网络可以被导出为梯度流，该梯度流最小化描述特征空间中吸引力和排斥力的可学习能量。这种形式主义允许将gnn解释为物理系统，并揭示了图形频率和通道混合频谱之间的相互作用如何决定节点特征的演变，并控制动力学是“平滑”还是“锐化”它还导致了一个令人惊讶的结论:即使是非常简单的具有共享权重的图卷积模型也不一定会遭受过度平滑，并且在嗜异性设置中可能是有效的。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/e9e7dbb3235a7aa91da10d427f31310d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3i5xOiGIAq9odMn4TR3q7Q.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片:Shutterstock。</p></figure><p id="d777" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">本文与Francesco Di Giovanni、James Rowbottom、Ben Chamberlain和Thomas Markovich合著，基于F. Di Giovanni、J. Rowbottom等人的论文</em> <a class="ae mb" href="https://arxiv.org/pdf/2206.10991.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ma">【图神经网络作为梯度流</em></a><em class="ma">【2022】，arXiv:2206.10991。请看一段</em><a class="ae mb" href="https://youtu.be/pG_akmY02Bo" rel="noopener ugc nofollow" target="_blank"><em class="ma">Francesco在</em> </a><a class="ae mb" href="https://www.sci.unich.it/geodeep2022/" rel="noopener ugc nofollow" target="_blank"> <em class="ma">首届意大利暑期学校关于几何深度学习</em> </a> <em class="ma">的演讲录音以及我们之前关于</em> <a class="ae mb" rel="noopener" target="_blank" href="/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774?sk=cf541fa43f94587bfa81454a98533e00"> <em class="ma">神经扩散</em></a><em class="ma"/><a class="ae mb" rel="noopener" target="_blank" href="/neural-sheaf-diffusion-for-deep-learning-on-graphs-bfa200e6afa6?sk=0b2f814a1180a64460699f6a3277053a"><em class="ma">细胞束</em> </a> <em class="ma">和</em> <a class="ae mb" rel="noopener" target="_blank" href="/graph-neural-networks-beyond-weisfeiler-lehman-and-vanilla-message-passing-bc8605fa59a?sk=3a058b427798d601ebb2d618fca89a8f"> <em class="ma">物理启发图ML</em></a></p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="e8b5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated">raph神经网络经常被批评为<em class="ma">过度平滑</em>(多个消息传递层逐渐产生接近常数的节点特征的趋势【1-2】)以及在<em class="ma">异嗜性</em>设置中的糟糕性能(当相邻节点具有不同标签时【3】)。应对这些现象的尝试最近导致了<a class="ae mb" rel="noopener" target="_blank" href="/a-new-computational-fabric-for-graph-neural-networks-280ea7e3ed1a?sk=3d4185067ef3c1cf81793deada08e18f">更复杂的信息传递方案</a>的“寒武纪大爆发”，其中一些求助于非常奇特的结构，如<a class="ae mb" rel="noopener" target="_blank" href="/neural-sheaf-diffusion-for-deep-learning-on-graphs-bfa200e6afa6?sk=0b2f814a1180a64460699f6a3277053a">蜂窝滑轮</a>【4】。</p><p id="c365" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">最重要的是，图形神经网络面临深度学习的普遍问题:可解释性差。总的来说，机器学习社区对GNNs有一种爱恨交加的关系:有时最新和最伟大的架构提供了最先进的结果，而在其他情况下，简单的旧方法(如图形卷积模型[5]或甚至节点式多层感知器)几乎一样好。对于某些模型在什么时候以及为什么表现良好，并没有清晰的认识，一种多少有些不健康的“民间传说”从一张纸重复到另一张纸上。</p><p id="e75b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">在最近的出版物</span>中【6】，我们认为GNNs是离散的粒子动力系统，它在微分方程(称为<em class="ma">梯度流)</em>下演化，使一些能量最小化。这种受物理学启发的方法，我们称之为<em class="ma">梯度流框架(GRAFF) </em>，提供了对现有GNN建筑的更好理解(导致一些令人惊讶的结论！)以及派生新的更有效的方法的原则性方法。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="5fb4" class="ms mt iq bd mu mv mw mx my mz na nb nc kf nd kg ne ki nf kj ng kl nh km ni nj bi translated"><strong class="ak">梯度流</strong></h1><p id="41bc" class="pw-post-body-paragraph le lf iq lg b lh nk ka lj lk nl kd lm ln nm lp lq lr nn lt lu lv no lx ly lz ij bi translated">考虑一个由<em class="ma">演化方程</em>支配的动力系统</p><p id="cfd7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">ẋ</strong>(<em class="ma">t</em>)=<strong class="lg ja">f</strong>(<strong class="lg ja">x</strong>(<em class="ma">t</em>))</p><p id="27db" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">从某个初始状态<strong class="lg ja"> X </strong> (0)开始，运行时间<em class="ma"> t </em> &gt; 0。这个微分方程的结构(在我们的符号中是<strong class="lg ja"> F </strong>)取决于底层的物理系统。例如，矩阵<strong class="lg ja"> X </strong>的行可以表示以矢量场<strong class="lg ja">F</strong>T10】给出的速度移动的粒子系统的空间坐标。如果粒子带电，它们相互施加<a class="ae mb" href="https://en.wikipedia.org/wiki/Coulomb%27s_law" rel="noopener ugc nofollow" target="_blank">静电力</a>，导致<em class="ma">耦合的</em>常微分方程系统，其中不同行的<strong class="lg ja"> X </strong>之间存在相关性。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/f93d10eb50c8cc759e3681a62a6393f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/0*Z-S2jRmaUvod1zX1"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">模拟流体的复杂粒子动力系统。图片:<a class="ae mb" href="http://david.li/fluid/" rel="noopener ugc nofollow" target="_blank">李大卫</a>。</p></figure><p id="addf" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在深度学习中，人们可以将微分方程系统的输入演变视为深度神经网络的连续版本，这是一种称为“神经微分方程”的方法[7–8]。然后，就有可能在用于求解方程的数值方案的时间步骤和神经网络的层之间画出一条平行线。</p><p id="59e1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">梯度流</em>【9】是特殊类型的演化方程的形式</p><p id="9551" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">f</em>(<strong class="lg ja">x</strong>(<em class="ma">t</em>)= –∇ℰ(<strong class="lg ja">x</strong>(<em class="ma">t</em>)，</p><p id="9b9b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">ℰ是某种能量函数。梯度流使得ℰ在演化过程中单调下降[10]。许多物理系统都可以用一些能量来表征，这些能量被它们的动力学最小化了。ℰ的知识提供了对系统的更好的理解，并且经常允许一个人对动力学的主要影响和它的渐近行为作出陈述。</p><p id="fe76" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">一个</span>原型梯度流是<em class="ma">扩散方程</em>，它控制着物理介质中的热传播。扩散方程的正式推导可以追溯到19世纪约瑟夫·傅立叶关于<em class="ma">热分析理论</em>【11】的论文。这种方程在物理学中有很多应用，也已经用于图像处理、计算机视觉[12]和<a class="ae mb" rel="noopener" target="_blank" href="/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774?sk=cf541fa43f94587bfa81454a98533e00">graph ML</a>【13】。</p><p id="4085" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在其最简单的形式中，𝒢图上的扩散方程由下式给出</p><p id="4937" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">ẋ</strong>(<em class="ma">t</em>)=–<strong class="lg ja">δx</strong>(<em class="ma">t</em>)，</p><p id="663b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">其中<strong class="lg ja"> X </strong>为<em class="ma"> n </em> × <em class="ma"> d </em>节点特征矩阵<strong class="lg ja">δ</strong>为<em class="ma"> n </em> × <em class="ma"> n </em>图拉普拉斯[14]。这是一个耦合的ode系统:图中的每个节点(<strong class="lg ja"> X </strong>的行)都与其邻居相互作用。</p><p id="c73c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">扩散方程原来是<em class="ma">狄利克雷能量</em>的梯度流</p><p id="e084" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">ℰᴰᴵᴿ(<strong class="lg ja">x</strong>)= trace(<strong class="lg ja">x</strong>ᵀ<strong class="lg ja">δx</strong>)=∑<em class="ma">ᵤᵥ||</em>(∇<strong class="lg ja">x</strong>)<em class="ma">ᵤᵥ</em>| |，</p><p id="e122" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">其中∇ <strong class="lg ja"> X </strong>表示在图的每条边上定义的特征的梯度。狄利克雷能量测量图[15]上特征的<em class="ma">平滑度</em>。在极限<em class="ma"> t </em> →∞时，扩散会丢失输入特征中包含的信息，因为所有节点都变得“相同”[16]——这种现象在GNN文献中通常被称为“过度平滑”</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/32aa18a89891f14ae3589a3716519fa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*1g6veSDkVY31JC-xbEgARg.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">流形上的热扩散方程。</p></figure><h1 id="f81a" class="ms mt iq bd mu mv nr mx my mz ns nb nc kf nt kg ne ki nu kj ng kl nv km ni nj bi translated"><strong class="ak">卷积图神经网络</strong></h1><p id="5a7d" class="pw-post-body-paragraph le lf iq lg b lh nk ka lj lk nl kd lm ln nm lp lq lr nn lt lu lv no lx ly lz ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di"> T </span>平滑节点特征的简单扩散方程在图形ML问题中可能不太有用【17】，其中图形神经网络提供了更多的灵活性和能力。人们可以把GNN想象成一个由参量演化方程支配的更一般的动力系统</p><p id="84b4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">ẋ</strong>(<em class="ma">t</em>)=gnn(𝒢，<strong class="lg ja">x</strong>(<em class="ma">t</em>)；<strong class="lg ja"> θ </strong> ( <em class="ma"> t </em>)，</p><p id="ffb2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">使用欧拉方法[18]离散，固定时间步长0<em class="ma">&lt;τ&lt;</em>1<em class="ma"/>为</p><p id="5f80" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">x</strong>(<em class="ma">t</em>+<em class="ma">τ</em>)=<strong class="lg ja">x</strong>(<em class="ma">t</em>)+<em class="ma">τ</em>gnn(𝒢，<strong class="lg ja">x</strong>(<em class="ma">t</em>)；<strong class="lg ja"> θ </strong> ( <em class="ma"> t </em>))。</p><p id="4451" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">每次迭代对应于一个GNN层，该层通常可以有一组不同的参数<strong class="lg ja"> θ </strong> ( <em class="ma"> t </em>)。</p><p id="a3aa" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如何实现上述等式中的函数“GNN”是不同图形神经网络架构之间的关键区别。<em class="ma">卷积</em>类型的gnn[19]将共享线性变换应用于特征(可学习的<em class="ma">d</em>×<em class="ma">d</em>‘信道混合’矩阵<strong class="lg ja"> W </strong>和<strong class="lg ja">ω</strong>)，随后沿边缘传播(通过归一化邻接矩阵<strong class="lg ja"/>‘扩散’)，并可能进行元素式非线性激活<em class="ma"> σ </em>:</p><p id="3240" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">X</strong>(<em class="ma">t</em>+<em class="ma">τ</em>)=<strong class="lg ja">X</strong>(<em class="ma">t</em>)+<em class="ma">▽</em>(—<strong class="lg ja">X</strong>(<em class="ma">t</em>)<strong class="lg ja">ω</strong>(<em class="ma">t</em>)+<strong class="lg ja">āX</strong>(<em class="ma">t</em>)【T24</p><p id="dac2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">选择<strong class="lg ja">ω</strong>=<strong class="lg ja">W</strong>=<strong class="lg ja">I</strong>和<em class="ma"> σ= </em> id恢复了我们之前看到的离散化图形扩散方程。由Thomas Kipf和Max Welling [5]介绍的最简单的图卷积模型之一GCN的残差版本是上述模型的特例，其中<strong class="lg ja">ω</strong>= 0。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="9463" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated">我们的例子展示了GNN建筑的通常设计过程，从参数化一个演化方程开始。我们认为，另一种方法，即参数化<em class="ma">能量</em>并导出GNN作为其离散梯度流，提供了更好的可解释性，并导致更有效的架构。这是我们GRAFF方法的精髓:我们对一类可以写成梯度流的gnn感兴趣</p><p id="d793" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">ẋ</strong>(<em class="ma">t</em>)=−∇ℰ(𝒢，<strong class="lg ja">x</strong>(<em class="ma">t</em>)；<strong class="lg ja"> θ </strong> ( <em class="ma"> t </em>)，</p><p id="e61d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">一些参量能量ℰ(.；<strong class="lg ja"> θ </strong>，参数<strong class="lg ja"> θ </strong>通过任务损失函数的反向传播学习。</p><p id="6535" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">能量和演化方程之间的联系提出了理解和设计GNNs的双重水平:从能量到演化方程，我们导出了更有原则和更有效的图形卷积模型版本，作为Dirichlet型参数能量族的梯度流。我们表明，这种模型具有避免过度平滑的理论保证和处理嗜异性数据的能力。</p><p id="1cb8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">相反，从演化方程到能量，我们证明了许多现有的非线性图卷积模型，尽管不是梯度流，仍然可以在我们的框架内进行分析。特别地，我们证明了它们的演化方程降低了前面提到的狄利克雷型能量。</p><h1 id="11f8" class="ms mt iq bd mu mv nr mx my mz ns nb nc kf nt kg ne ki nu kj ng kl nv km ni nj bi translated"><strong class="ak">从能量到演化方程</strong></h1><p id="b7fa" class="pw-post-body-paragraph le lf iq lg b lh nk ka lj lk nl kd lm ln nm lp lq lr nn lt lu lv no lx ly lz ij bi translated">让我们考虑一族二次能量</p><p id="e06f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">ℰᶿ(<strong class="lg ja">x</strong>)=∑<em class="ma">ᵤ</em>⟨<strong class="lg ja">x</strong><em class="ma">ᵤ</em>，<strong class="lg ja">ωx</strong><em class="ma">ᵤ</em>⟩∑<em class="ma">ᵤᵥāᵤᵥ</em>⟨<strong class="lg ja">x</strong><em class="ma"/>，<strong class="lg ja"> Wx </strong></p><p id="d1db" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">由矩阵<strong class="lg ja">ω</strong>和<strong class="lg ja"> W </strong>参数化。我们称ℰᶿ为“广义参数狄利克雷能量”，因为经典ℰᴰᴵᴿ是它的特例[20]。</p><p id="97d8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">物理解释。</strong> ℰᶿ是与<em class="ma">粒子系统</em>(图中的节点，其在特征空间中的位置由矩阵<strong class="lg ja"> X </strong>表示)相关联的能量，可以解释如下:第一项表示作用于所有粒子的‘外部’能量。第二项考虑了沿着图的边缘的<em class="ma">成对相互作用</em>，因此代表了粒子系统的“内部”能量。成对交互由通道混合矩阵<strong class="lg ja"> W </strong>控制。第二能量项的最小化使得相邻节点<em class="ma">的特征<strong class="lg ja">x</strong>ᵤ</em>、<strong class="lg ja">x</strong>t20】ᵥ沿着对应于正特征值的特征向量<strong class="lg ja"> W </strong>吸引。同样的，<strong class="lg ja"> W </strong>的负本征值诱发<em class="ma">斥力</em>【21】。</p><p id="b28e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">梯度流动。</strong>ℰᶿ的梯度流量由下式给出</p><p id="1b82" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">ẋ</strong>(<em class="ma">t</em>)= −∇ℰ(<strong class="lg ja">x</strong>(<em class="ma">t</em>)=-<strong class="lg ja">x</strong>(<em class="ma">t</em>)(<strong class="lg ja">ω+ω</strong>ᵀ)/2+<strong class="lg ja">+x</strong>(<em class="ma">t</em>)(<strong class="lg ja">w+w</strong>ᵀ)/2.</p><p id="2978" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">由于<strong class="lg ja">ω</strong>和<strong class="lg ja"> W </strong>以对称形式出现在梯度流中，在不失一般性的情况下，我们可以假设它们是<em class="ma">对称的</em>:</p><p id="aef7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">ẋ</strong>(<em class="ma">t</em>)=-<strong class="lg ja">x</strong>(<em class="ma">t</em>)<strong class="lg ja">ω</strong>+<strong class="lg ja">āx</strong>(<em class="ma">t</em>)<strong class="lg ja">w</strong>。</p><p id="ebb2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">当以固定时间步长<em class="ma"> τ、</em>离散化时，该演化方程成为具有<em class="ma">共享对称权重</em>的图卷积模型</p><p id="ab1e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">X</strong>(<em class="ma">t</em>+<em class="ma">τ</em>)=<strong class="lg ja">X</strong>(<em class="ma">t</em>)+<em class="ma">τ</em>(-<strong class="lg ja">X</strong>(<em class="ma">t</em>)<strong class="lg ja">ω</strong>+<strong class="lg ja">+X</strong>(<em class="ma">t</em></p><p id="9f2d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">由于<strong class="lg ja">ω</strong>和<strong class="lg ja"> W </strong>独立于层<em class="ma"> t </em>。逐层共享权重将一个<em class="ma"> L </em>层卷积GNN中的参数总数从2<em class="ma">d</em>T10】L减少到<em class="ma"> d </em>。重要的是，对称约束不会削弱GNN的表现力[22]。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nw"><img src="../Images/0c4cd39474c4f511f1ac6755d6156ef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RQWqdzG2jVEQwCo4C4XgjA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">格拉夫诱导的排斥和吸引动力学的例子:节点位置代表它们的特征坐标，颜色是标签。斥力沿着特征空间的x轴(W<strong class="bd nx">的负特征向量</strong>的方向)发生，引力沿着y轴(W<strong class="bd nx">的正特征向量</strong>发生。</p></figure><p id="e2e1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">动力学中的显性效应。</strong>我们对GNN粒子系统的物理解释允许分析其梯度流诱发的动力学。吸引力使边缘梯度最小化，产生平滑效应(“模糊”)，放大了图中节点特征的低频[23]。另一方面，排斥力增加了边缘梯度，产生了增强高频的“锐化”效应。</p><p id="1fd7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这可以通过监测沿归一化解的狄利克雷能量来定量表示:如果在极限ℰᴰᴵᴿ(<strong class="lg ja">x</strong>(<em class="ma">t</em>)/| |<strong class="lg ja">x</strong>(<em class="ma">t</em>)| |)趋向于零，我们就有了<em class="ma">低频主导的</em> (LFD)动力学。在收敛到图的拉普拉斯最大特征值的情况下，我们称动力学<em class="ma">高频</em> <em class="ma">支配</em>(HFD)【24】。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ny"><img src="../Images/9c8e1160bcb82bca3ca7d73d3801f9dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BVAQyZnAXYNB_QAk-dZ-eg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">嗜同性(左)和嗜异性(右)图表示例。不同类型的节点用颜色编码。</p></figure><p id="d979" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated">多项研究表明，LFD过程对于同形图中的节点分类是成功的。直观地，在这样的图中，邻居包含相似的信息，并且平滑过程聚集邻居，同时平均掉噪声。在<em class="ma">异嗜性</em>情况下，平滑通常是有害的，因为高频成分可能包含更多相关信息【27】。因此，一个理想的GNN模型必须通过能够诱导LFD或HFD动力学来适应这两种相反的情况。</p><p id="659b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们表明，基于扩散的模型，如连续GNN [28]，PDE-GCN-D [29]，和<a class="ae mb" rel="noopener" target="_blank" href="/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774?sk=cf541fa43f94587bfa81454a98533e00">图神经扩散</a> (GRAND) [13]无法诱导HFD动力学，因此可能会在嗜异性设置中遭受可预见的痛苦[30]。其中一个原因是，GRAND等方案不允许通道混合:GRAND在节点特征的每个通道上独立地执行非线性扩散，这可以被解释为一种图形注意力[31]的形式。</p><p id="2597" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">另一方面，</span>根据<strong class="lg ja">w</strong>【32】的光谱，通过离散化我们的参数能量ℰᶿ的梯度流获得的格拉夫模型可以学习成为LFD(主要是边缘方向吸引)或HFD(主要是边缘方向排斥)。这使得格拉夫既适用于嗜同性也适用于嗜异性。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nz"><img src="../Images/202b8714ff720571ad7881b2e861473c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eL8mxYAo90ZWvTbSpRP7WQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">具有受控同质性的合成Cora数据集上的节点分类实验。GCN [5]通过利用节点特征的局部相似性，在高同质性的情况下表现良好；当同质性低时，这变成导致性能恶化的缺点。多层感知器(MLP)通过独立考虑每个节点的特征而忽略了图的结构。这使得MLP对低同性免疫，但也无法利用高同性。我们受物理学启发的格拉夫模型在两种情况下都工作得很好，因为它能够诱导低频和高频主导的动力学。图改编自[6]。</p></figure><p id="650f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">格拉夫系统动态的驱动因素是拉普拉斯图{ <em class="ma"> λₗ </em> }的频谱和通道混合矩阵{<em class="ma">ₖ</em>}【33】的频谱的相互作用。<strong class="lg ja"> W </strong>的正特征值(<em class="ma">ₖ&gt;t13】0)放大低频(<em class="ma">λₗ</em>t29】1)，负特征值(<em class="ma">ₖ&gt;t17】0)放大高频(<em class="ma">λₗ</em>t31】1)[33 *]。</em></em></p><p id="8b17" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如果<strong class="lg ja"> W </strong>的负特征值比正特征值【35】大得多，那么排斥力和高频占主导地位。在相反的情况下，低频占主导地位，导致极限<em class="ma">t</em>→∞1，36】中的过度平滑。</p><p id="a801" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">此外，负特征值{ <em class="ma"> ₖ &lt; </em> 0}的影响等同于翻转边的符号；这是为嗜异性数据集[37]提出的一种启发式方法，最近在我们关于神经层扩散T26的NeurIPS论文[4]中也被赋予了代数拓扑动机。</p><h1 id="b5a9" class="ms mt iq bd mu mv nr mx my mz ns nb nc kf nt kg ne ki nu kj ng kl nv km ni nj bi translated">基普夫&amp;韦林公司</h1><p id="b23f" class="pw-post-body-paragraph le lf iq lg b lh nk ka lj lk nl kd lm ln nm lp lq lr nn lt lu lv no lx ly lz ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">我们</span>应该强调的是，虽然GRAFF是一个允许将gnn导出为任何能量泛函的离散梯度流的通用框架，但是从我们的二次能量ℰᶿ中产生的模型是标准的卷积型gnn。梯度流形式主义为他们的行为以及如何做出正确的架构选择提供了重要的见解。</p><p id="1f7d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们的分析的一个结果是，如果信道混合矩阵具有足够负的特征值，卷积GNNs可以处理图异构。这与图形ML文献中经常重复的相反的“民间传说”陈述相矛盾。</p><p id="f661" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然而，有一个重要的细微差别:由于我们使用欧拉离散化，我们总是有一个<em class="ma">残差</em> GCN模型。这个架构特征被证明是至关重要的:我们证明了一个没有剩余连接的模型(即迭代公式为<strong class="lg ja">X</strong>(<em class="ma">t</em>+<em class="ma">τ</em>)=<em class="ma">τ</em><strong class="lg ja">āX</strong>(<em class="ma">t</em>)<strong class="lg ja">W</strong>最初由Kipf和Welling [5]提出)只能诱导独立于<strong class="lg ja">W<strong class="lg ja">的谱的LFD动力学</strong></strong></p><p id="f9c7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated">其次，我们的模型是残差GCNs的特定版本，其中信道混合矩阵是对称的，并且跨层共享。这也与graph ML社区中的常见做法相矛盾:通常，多层GCN模型每层都有不同的通道混合矩阵，并且还包括非线性激活[39]。我们通过实现GRAFF模型来证明这是不必要的，其中演化方程是线性的，唯一的非线性是在特征编码器和解码器中。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oa"><img src="../Images/4b63b690f199d44f0f9152d26413523d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jziuX2oesHyNOG2ggifQlA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">不同同质性水平数据集上的节点分类精度。GRAFF及其非线性版本(GRAFF-NL)实现了与最近一些专门为嗜异性设置设计的方法(如GGCN和GPRGNN)相当或更好的结果，同时在计算复杂性和参数数量方面明显更轻便。表来自[6]。</p></figure><p id="91a6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">考虑到我们的理论和实验结果，简单的图卷积证明了简单的图卷积是正确的:我们得出结论，简单的图卷积在许多实际情况下足够强大，其性能与复杂得多的GNN模型相当，甚至更好。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ob"><img src="../Images/147e2c2e5418507a9d0a9aa550aa613c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eZjgYf9siKqFqcwIvyWgqg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">简单图卷积模型反击。原始迷因:米哈伊尔高尔金。</p></figure><h1 id="1430" class="ms mt iq bd mu mv nr mx my mz ns nb nc kf nt kg ne ki nu kj ng kl nv km ni nj bi translated">从演化方程到能量</h1><p id="2318" class="pw-post-body-paragraph le lf iq lg b lh nk ka lj lk nl kd lm ln nm lp lq lr nn lt lu lv no lx ly lz ij bi translated">梯度流形式主义为GNNs的设计带来了一种新的思维模式:我们不是将演化方程参数化，而是将梯度流最小化的能量参数化[41]。我们已经证明，通过这种方式，人们可以做出更明智的设计选择，并为GCNs等“旧”模型带来新的生命。</p><p id="2d65" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">人们可能想知道如何处理现有的进化方程不是梯度流的gnn？GRAFF在这种情况下仍然提供了重要的见解:例如，我们表明，如果权重是对称的，使用非线性激活的更一般的卷积模型仍然会降低能量ℰᶿ[42]——再次注意，从通用近似的角度来看，这不是限制性的[22],事实上为在图形神经网络中使用对称权重提供了更具理论原则性的理由。</p><p id="1b06" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们的分析概括了现有的结果[2，4],这些结果监测了用ReLU激活的Kipf-Welling GCNs中的狄利克雷能量ℰᴰᴵᴿ:我们考虑了更广泛的一类能量以及无限类非线性激活函数。我们相信，这种方法可以提供一种新的视角，通过考虑哪种能量泛函沿着学习的节点特征减少来分析新旧GNNs的学习动力学。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="48a5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated">更广泛地说，GRAFF是一种受T2物理学启发的图形学习方法，我们最近已经看到了几个这样的例子，包括我们自己的作品。这种方法的主要思想是使用物理系统作为学习的隐喻，并通过系统的状态来参数化解的空间。在某种意义上，这是对<a class="ae mb" href="https://en.wikipedia.org/wiki/Physics-informed_neural_networks" rel="noopener ugc nofollow" target="_blank">物理信息神经网络</a> (PINN)的补充，后者将物理定律作为其归纳偏差，以便更好地逼近现实生活中的物理系统。</p><p id="e4f6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">受物理学启发的方法的一个重要方面是，它们带来了不同于graph ML中传统使用的工具。例如，GNNs的表达能力通常通过诉诸消息传递和<a class="ae mb" rel="noopener" target="_blank" href="/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49?sk=5c2a28ccd38db3a7b6f80f161e825a5a"> Weisfeiler-Lehman图同构测试</a>【45】之间的等价性来分析。后者通常限制太多，因为它假设信息传播在输入图上完成，而在实施某种形式的图重布线的实际gnn中，情况往往不是这样[46]。</p><p id="0464" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">通过研究动力系统的渐近行为(例如，扩散方程[4]的极限)，人们可以表达GNNs的分离能力，而无需求助于WL形式。这种技术自然允许图形重新布线，这假定了对基础微分方程的不同离散化的解释。</p><p id="77ac" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated">我们的工作表明，物理启发的方法甚至可以在以前研究得很好的图形学习模型中提供新的见解，并将鼓励在这个方向上的更多研究。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="ad45" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[1] K. Oono和t .铃木，<a class="ae mb" href="https://openreview.net/pdf?id=S1ldO2EFPr" rel="noopener ugc nofollow" target="_blank">图神经网络对节点分类的表达能力呈指数级丧失</a> (2020) ICLR。</p><p id="1297" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[2]蔡春华，王永元，.关于图神经网络过光滑问题的一个注记.(2020)arXiv:2006.13318 .</p><p id="d7e2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[3] J. Zhu等，<a class="ae mb" href="https://proceedings.neurips.cc/paper/2020/file/58ae23d878a47004366189884c2f8440-Paper.pdf" rel="noopener ugc nofollow" target="_blank">超越图神经网络中的同向性:当前的局限与有效设计</a> (2020) <em class="ma"> NeurIPS </em>。</p><p id="046a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[4] C .博德纳尔，f .迪·乔瓦尼等，神经束扩散:GNNs中异嗜性和过度光滑的拓扑透视(2022) <em class="ma"> NeurIPS </em>。参见附带的<a class="ae mb" rel="noopener" target="_blank" href="/neural-sheaf-diffusion-for-deep-learning-on-graphs-bfa200e6afa6?sk=0b2f814a1180a64460699f6a3277053a">博文</a>。</p><p id="efd8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[5] T. Kipf和M. Welling，<a class="ae mb" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank">带图卷积网络的半监督分类</a> (2017)，<em class="ma"> ICLR </em>。</p><p id="d828" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[6] F. Di Giovanni，J. Rowbottom等人，<a class="ae mb" href="https://arxiv.org/pdf/2206.10991.pdf" rel="noopener ugc nofollow" target="_blank">将神经网络图化为梯度流</a> (2022)，arXiv:2206.10991。</p><p id="7a94" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[7] E. Haber和L. Ruthotto，深度神经网络的稳定架构(2017) <em class="ma">逆问题</em> 34 (1)。</p><p id="bcb7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[8] R. Chen等，<a class="ae mb" href="https://papers.nips.cc/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf" rel="noopener ugc nofollow" target="_blank">神经常微分方程</a> (2019) <em class="ma"> NeurIPS。</em></p><p id="cce8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[9]梯度流可被视为最陡下降法的连续版本，该方法最初由A.-L .柯西、<a class="ae mb" href="http://sites.mathdoc.fr/cgi-bin/oeitem?id=OE_CAUCHY_1_10_399_1" rel="noopener ugc nofollow" target="_blank">méthode générale pour la résolution des systèmes d 'éequations simultanées</a>(1847)<em class="ma">Comptes Rendus de l ' académie des Sciences</em>提出，用于解方程组(该方法出现在术语<em class="ma">梯度</em>之前，后者由Horace Lamb 于2004年提出<a class="ae mb" href="https://mathshistory.st-andrews.ac.uk/Miller/mathword/g/" rel="noopener ugc nofollow" target="_blank">术语“流”来自流体力学，粒子系统的一个重要设置。</a></p><p id="8f5b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[10]梯度流单调减少底层能量的事实源于∂/∂<em class="ma">t</em>ℰ(<strong class="lg ja">x</strong>(<em class="ma">t</em>)= –||∇ℰ(<strong class="lg ja">x</strong>(<em class="ma">t</em>))| |。</p><p id="9a08" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[11] J .傅立叶，<a class="ae mb" href="https://archive.org/details/bub_gb_TDQJAAAAIAAJ" rel="noopener ugc nofollow" target="_blank"><em class="ma"/></a><em class="ma"/>(1822)。傅立叶在1807-1811年间发展了后来成为同名的分析。这项工作在接下来的十年里没有被公开，因为他的职责是伊泽尔省的省长，这是一个由拿破仑亲自任命的高级职位(他在埃及的战役中就认识傅立叶)。</p><p id="7607" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[12] P .佩罗娜和j .马利克，<a class="ae mb" href="https://authors.library.caltech.edu/6498/1/PERieeetpami90.pdf" rel="noopener ugc nofollow" target="_blank">使用各向异性扩散的尺度空间和边缘检测</a> (1990)，<em class="ma">PAMI</em>12(7):629–639。</p><p id="a31e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[13]例如，参见我们的论文B. Chamberlain，J. Rowbottom等人，<a class="ae mb" href="https://arxiv.org/abs/2106.10934" rel="noopener ugc nofollow" target="_blank">GRAND:Graph Neural Diffusion</a>(2021)<em class="ma">ICML</em>，附带的博文<a class="ae mb" rel="noopener" target="_blank" href="/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774?sk=cf541fa43f94587bfa81454a98533e00"/>，以及其中的许多参考文献。</p><p id="7839" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[14]我们使用对称归一化图拉普拉斯算子<strong class="lg ja">δ</strong>= I–<strong class="lg ja">ā</strong>，其中<strong class="lg ja">ā= d</strong>ᐨᐟ<strong class="lg ja">ad</strong>ᐨᐟ是归一化邻接矩阵。我们的分析可以针对拉普拉斯算子的其他变体进行重复，参见[6]中的附录A.2。</p><p id="667c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[15]ℰᴰᴵᴿ的二次型表达式允许导出图傅立叶基(图拉普拉斯的正交特征向量)为<strong class="lg ja">φ</strong>= arg min<strong class="lg ja">ₓ</strong>trace(<strong class="lg ja">x</strong>ᵀ<strong class="lg ja">δx</strong>s . t .<strong class="lg ja">x</strong>ᵀ<strong class="lg ja">x</strong>=<strong class="lg ja">I</strong>，这可以解释为狄利克雷能量意义上的“最光滑正交基”。</p><p id="77f4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[16]更正确的说法是<strong class="lg ja"> X </strong> (0)在ker(<strong class="lg ja">δ</strong>)上的投影。对于归一化的拉普拉斯算子，扩散方程的极限是节点度数的向量。对于非规格化的拉普拉斯<strong class="lg ja">δ</strong>= I-<strong class="lg ja">A</strong>，<strong class="lg ja"> </strong>结果是一个常数向量。</p><p id="37fc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[17]固定扩散<em class="ma">在某些情况下</em>是有用的。比如D. Zhou和b . schlkopf提出的标签传播(label propagation)，离散空间上的正则化(2005)，<em class="ma">联合模式识别研讨会</em>，是一个带边界条件(已知节点标签)的扩散方程，是很多直推式图ML问题的很好的基线。在最近的一篇论文中，E. Rossi等人，<a class="ae mb" href="https://arxiv.org/abs/2111.12128" rel="noopener ugc nofollow" target="_blank">关于在具有缺失节点特征的图上学习的特征传播的不合理有效性</a> (2021)，arXiv:2111.12128表明<a class="ae mb" rel="noopener" target="_blank" href="/learning-on-graphs-with-missing-features-dd34be61b06?sk=ca026cec16a3ba6bfdbae660bc7065bf">特征传播</a>在具有缺失特征的同形图上有效地工作。</p><p id="5b71" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[18]欧拉离散化(也称为<em class="ma">显式</em>或<em class="ma">前向</em>方案)用前向时差、<strong class="lg ja">ẋ</strong>(<em class="ma">t</em>)≈(<strong class="lg ja">x</strong>(<em class="ma">t</em>+<em class="ma">τ</em>)–<strong class="lg ja">x</strong>(<em class="ma">t</em>)/<em class="ma">代替时间导数这总是导致<em class="ma">剩余</em>架构。</em></p><p id="2acc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[19]我们在这里采用了我们书中的“GNN风味”术语，M. M. Bronstein等人，<a class="ae mb" href="https://arxiv.org/abs/2104.13478" rel="noopener ugc nofollow" target="_blank">几何深度学习:网格、组、图、测地线和规范</a> (2021) arXiv:2104.13478。在这种情况下，<em class="ma">卷积</em>类型的gnn可以写成以下形式:<strong class="lg ja">X</strong>(<em class="ma">t</em>+<em class="ma">τ</em>)=<strong class="lg ja">AX</strong>(<em class="ma">t</em>)<strong class="lg ja"/>)(更新由与特征无关的矩阵完成)， <strong class="lg ja"> </strong> <em class="ma">注意力</em>为<strong class="lg ja">x</strong>(<em class="ma">t</em>+<em class="ma">τ</em>)=<strong class="lg ja">a</strong>(<strong class="lg ja">x</strong>(<em class="ma">t</em>)<strong class="lg ja">x</strong>(<em class="ma">t</em>)<strong class="lg ja"/>(特征相关矩阵值函数)<em class="ma">消息传递</em></p><p id="06b1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[20] ℰᶿ = ℰᴰᴵᴿ当<strong class="lg ja">ω</strong>=<strong class="lg ja">w</strong>=<strong class="lg ja">I</strong>。在[6]的附录B.5中，我们还表明ℰᶿ可以认为是一个离散版的谐波能量ℰ(<em class="ma">x</em>)=∫⟨∇<em class="ma">x</em>(<em class="ma">u</em>),∇<em class="ma">x</em>(<em class="ma">u</em>)⟩<em class="ma">ₕ</em>d<em class="ma">u</em>一个流形嵌入<em class="ma">x</em>:ℳ→ℝ</p><p id="abee" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[21]参见[6]中的公式10。直观上注意如果<strong class="lg ja"> x </strong> <em class="ma"> ᵤ </em>和<strong class="lg ja"> x </strong> <em class="ma"> ᵥ </em>是相邻节点特征并且都是满足<strong class="lg ja">wx</strong><em class="ma">ᵤ</em>=<em class="ma"/><strong class="lg ja">x</strong><em class="ma">ᵤ</em>和<strong class="lg ja"> Wx </strong> <em class="ma">的特征向量 <strong class="lg ja">wx</strong><em class="ma">ᵥ</em>⟩=<em class="ma"/>⟨<strong class="lg ja">x</strong><em class="ma">ᵤ</em>，<strong class="lg ja"> x </strong> <em class="ma"> ᵥ </em> ⟩和我们看到，如果<em class="ma"> </em> &gt; 0那么<strong class="lg ja"> x </strong> <em class="ma"> ᵤ </em>和<strong class="lg ja"> <em class="ma"> </em> &lt; 0反之亦然。</strong></em></p><p id="81a7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[22] S. X. Hu，S. Zagoruyko和N. Komodakis，探索深度神经网络中的权重对称性(2019) <em class="ma">计算机视觉和图像理解</em>，187表明具有对称权重的神经网络是通用近似器。</p><p id="0d74" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[23]本文中的“频率”是指归一化图拉普拉斯的频谱分解，提供了傅立叶基础的类比。节点特征可以写成拉普拉斯正交本征向量的线性组合，对应的特征值在0 ≤ <em class="ma"> λₗ </em> ≤ 2的范围内。第零特征值对应于恒定的特征向量(使用谐波分析术语的‘DC’)；对应于较大特征值的特征向量不太光滑，并且具有较高的狄利克雷能量。</p><p id="2749" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[24]见[6]中的定义3.1。注意，LFD/HFD动力学的定义是通用的，不要求演化方程是梯度流。</p><p id="3905" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[25] J .克利茨佩拉等，<a class="ae mb" href="https://proceedings.neurips.cc/paper/2019/file/23c894276a2c5a16470e6a31f4618d73-Paper.pdf" rel="noopener ugc nofollow" target="_blank">扩散改善图形学习</a> (2019) <em class="ma"> NeurIPS </em>。</p><p id="8c5e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[26]吴等，<a class="ae mb" href="https://arxiv.org/abs/1902.07153" rel="noopener ugc nofollow" target="_blank">简化图神经网络</a> (2019) <em class="ma">。</em></p><p id="424e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[27]经典例子是分离二部图的<strong class="lg ja">∈</strong>的最大特征向量。参见D. Bo等人，超越图卷积网络中的低频信息(2021) <em class="ma"> AAAI </em>。</p><p id="f91d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[28] L.-P. Xhonneux等人，<a class="ae mb" href="http://proceedings.mlr.press/v119/xhonneux20a/xhonneux20a.pdf" rel="noopener ugc nofollow" target="_blank">连续图神经网络</a> (2020) <em class="ma"> ICML </em>。</p><p id="2342" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[29] M. Eliasof、E. Haber和E. Treister。PDE-GCN:由偏微分方程驱动的图形神经网络的新架构(2021) <em class="ma"> NeurIPS </em>。</p><p id="c417" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[30]文献[6]中的定理3.3。</p><p id="9f71" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[31]p . veli kovi等人，<a class="ae mb" href="https://arxiv.org/abs/1710.10903" rel="noopener ugc nofollow" target="_blank">图形注意网络</a> (2018) <em class="ma"> ICLR </em>。</p><p id="3c59" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[32]文献[6]中的定理3.2。</p><p id="f7fb" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[33]文献[6]中的定理4.1。</p><p id="f803" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[34]我们表明，可以根据图形和通道混合频率明确地写出每一层的学习节点表示，从而得出每一个主导效应。这种对动力系统的“剖析”也强调了<strong class="lg ja"> W </strong>的光谱和特征向量比条目本身更重要。</p><p id="b262" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[35]在[6]中的方程13的意义上。</p><p id="4b3f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[36] H. Nt和T. Maehara，重新审视图形神经网络:我们所拥有的只是低通滤波器(2019) arXiv:1812.08434v4。</p><p id="a962" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[37]d . Bo等人在《超越图形卷积网络中的低频信息》(2021)<em class="ma"/>和Y. Yan等人在《同一枚硬币的两面:图形卷积神经网络中的异嗜性和过度平滑》(2021) arXiv:2102.06462中使用了异嗜性数据集的负边缘加权。</p><p id="4150" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[38]文献[6]中的定理4.3。</p><p id="3011" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[39]没有非线性激活(<em class="ma">σ</em>= id)an<em class="ma">m</em>-层gcn<strong class="lg ja">x</strong>(<em class="ma">t</em>+<em class="ma">τ</em>)=<em class="ma">σ</em>(<em class="ma">σ</em>(<strong class="lg ja">ā</strong>σ(<strong class="lg ja">ax</strong>(<em class="ma">t</em>)<strong class="lg ja">w 可以写成单层<strong class="lg ja">x</strong>(<em class="ma">t</em>+<em class="ma">)=<em class="ma">τ</em><strong class="lg ja">ā</strong><em class="ma">ᵐ</em><strong class="lg ja">x</strong>(<em class="ma">t</em>)<strong class="lg ja">w</strong>带<em class="ma"> m </em> -hop扩散<strong class="lg ja">ā</strong><em class="ma">ᵐ</em>通过吸收 这个特性在[26]中被用于一个被称为简化GCN (SGCN)的建筑。这种方法的优点是扩散特征<strong class="lg ja">ā</strong><em class="ma">ᵐ</em><strong class="lg ja">x</strong>可以预先计算，允许将gcn缩放到非常大的图，如我们的论文Frasca等人的<a class="ae mb" href="https://grlplus.github.io/papers/77.pdf" rel="noopener ugc nofollow" target="_blank">sign:scalable inception graph neural networks</a>(2020)<em class="ma">icml workshop on graph re presentation learning and beyond</em>以及附带的<a class="ae mb" rel="noopener" target="_blank" href="/simple-scalable-graph-neural-networks-7eb04f366d07?sk=2127a086dd3e91b06235ef0f74016e64">博客帖子</a>所示。注意，这种简化不会发生在GCN的残差版本中，如[6]中的方程(34)所示。</em></strong></p><p id="7091" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[40]在[6]中的方程17。</p><p id="03a6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[41]我们注意到,“最小化”在这里指的是能量沿着演化方程的行为(即，在通过网络的正向传递期间),并且不同于用于学习的行为(即，通过反向传播最小化关于模型参数的一些特定任务损失函数)。</p><p id="c714" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[42]文献[6]中的定理5.1。在我们最近的工作中，Dirichlet能量的指数衰减被用于形式化超光滑现象[43]。</p><p id="242a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[43] B. P. Chamberlain等人，<a class="ae mb" href="https://arxiv.org/pdf/2110.09443.pdf" rel="noopener ugc nofollow" target="_blank"> Beltrami流和图形上的神经扩散</a> (2021) NeurIPS。</p><p id="74ab" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[44] T. K. Rusch等人，<a class="ae mb" href="https://proceedings.mlr.press/v162/rusch22a/rusch22a.pdf" rel="noopener ugc nofollow" target="_blank">图耦合振荡器网络</a> (2022) <em class="ma"> ICML </em>。</p><p id="2d95" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[45] B. Weisfeiler和A. Lehman，将图简化为标准形式和其中出现的代数(1968)<em class="ma">Nauchno-techniceskaya informatisia</em>2(9):12–16。WL与用内射聚合器传递消息的等价性在例如K. Xu等人的文章中示出。<a class="ae mb" href="https://arxiv.org/abs/1810.00826" rel="noopener ugc nofollow" target="_blank">图神经网络有多强大？</a> (2019) <em class="ma"> ICLR </em>。参见我们的<a class="ae mb" rel="noopener" target="_blank" href="/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49?sk=5c2a28ccd38db3a7b6f80f161e825a5a">博客文章</a>。</p><p id="9ee6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[46]传统的gnn使用输入图来传播信息，通过这种方式获得反映图的结构和其上的特征的表示。然而，由于“瓶颈”导致来自太多节点的信息被“挤压”到单个节点表示中，一些图对信息传播不太友好，这种现象被称为“过度压制”。现代GNN实现通过将输入图与计算图解耦(或为计算目的对其进行优化)来处理这种现象，这是一套被称为“图重布线”的技术。在我们最近的论文J. Topping，F. Di Giovanni等人的<a class="ae mb" href="https://arxiv.org/pdf/2111.14522.pdf" rel="noopener ugc nofollow" target="_blank">通过曲率理解图的过度挤压和瓶颈</a>(2022)<em class="ma">ICLR</em>中，我们将重布线与图曲率和Ricci流联系起来。参见我们的<a class="ae mb" rel="noopener" target="_blank" href="/over-squashing-bottlenecks-and-graph-ricci-curvature-c238b7169e16?sk=f5cf01cbd57b4fee8fb3a89d447e56a9">博客文章</a>。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="3007" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">我们非常感谢大卫·艾纳德、尼尔斯·哈默拉、托马斯·基普夫、伊曼纽·罗西和康斯坦丁·鲁施对这篇文章的校对。关于图形深度学习的其他文章，请参见Michael在《走向数据科学》中的</em> <a class="ae mb" rel="noopener" target="_blank" href="https://towardsdatascience.com/graph-deep-learning/home"> <em class="ma">其他帖子</em> </a> <em class="ma">，</em> <a class="ae mb" href="https://michael-bronstein.medium.com/subscribe" rel="noopener"> <em class="ma">订阅他的帖子和</em> </a><a class="ae mb" href="https://www.youtube.com/c/MichaelBronsteinGDL" rel="noopener ugc nofollow" target="_blank"> <em class="ma"> YouTube频道</em> </a> <em class="ma">，获取</em> <a class="ae mb" href="https://michael-bronstein.medium.com/membership" rel="noopener"> <em class="ma">中等会员</em> </a> <em class="ma">，或关注他的</em> <a class="ae mb" href="https://twitter.com/mmbronstein" rel="noopener ugc nofollow" target="_blank"> <em class="ma"/></a></p></div></div>    
</body>
</html>