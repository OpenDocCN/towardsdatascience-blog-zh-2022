<html>
<head>
<title>Why Is Cross Entropy Equal to KL-Divergence?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么交叉熵等于 KL-散度？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-is-cross-entropy-equal-to-kl-divergence-d4d2ec413864#2022-02-06">https://towardsdatascience.com/why-is-cross-entropy-equal-to-kl-divergence-d4d2ec413864#2022-02-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div class="gh gi jq"><img src="../Images/89b482d7872aaf4dd7f31c2ea18553e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*UOqItmSOFQGS3JwL-R2wHQ.png"/></div><p class="jx jy gj gh gi jz ka bd b be z dk translated">图 1:从正态分布中抽取的两个概率分布(图片由作者提供)</p></figure><p id="f659" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">尽管最初的概念建议使用 KL-divergence，但在构建生成性对抗网络[1]时，通常在损失函数中使用交叉熵。这经常给这个领域的新手造成困惑。在本文中，我们将介绍熵、交叉熵和 Kullback-Leibler 散度[2]的概念，并了解如何使它们近似相等。</p><p id="011b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">当我们有一个以上的概率分布时，熵和 KL-散度的概念就起作用了，我们想比较它们彼此之间的公平程度。我们希望有一些基础来决定为什么最小化交叉熵而不是 KL 散度会产生相同的输出。让我们从正态分布中抽取两个概率分布<strong class="kd iu"> <em class="kz"> p </em> </strong>和<strong class="kd iu"> <em class="kz"> q </em> </strong>。如图 1 所示，这两种分布是不同的，但是它们都是从正态分布中采样的。</p><p id="65a5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> 1。</strong>熵<strong class="kd iu">熵</strong></p><p id="1ca5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">熵是系统不确定性的度量。直观地说，它是从系统中消除不确定性所需的信息量。系统各种状态的概率分布<strong class="kd iu"> <em class="kz"> p </em> </strong>的熵可以计算如下:</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="gh gi la"><img src="../Images/4b60c576bf3e47c395fe6e0432a6b9da.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*0myjCZM_AN2VY23XKlL-Jg.png"/></div></figure><p id="3607" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> 2。</strong> <strong class="kd iu">交叉熵</strong></p><p id="0e8b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">术语交叉熵指的是存在于两个概率分布之间的信息量。在这种情况下，分布<strong class="kd iu"> <em class="kz"> p </em> </strong>和<strong class="kd iu"> <em class="kz"> q </em> </strong>的交叉熵可以表述如下:</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="gh gi la"><img src="../Images/105c97bab438f3f259cb2f4d1bab8769.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*YEmtm52EjxEQBtygOLLNZQ.png"/></div></figure><p id="0bd0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> 3。</strong> <strong class="kd iu"> KL-Divergence </strong></p><p id="71b0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">两个概率分布之间的散度是它们之间存在的距离的度量。概率分布<strong class="kd iu"> <em class="kz"> p </em> </strong>和<strong class="kd iu"> <em class="kz"> q </em> </strong>的 KL 散度可以通过以下等式来测量:</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="gh gi la"><img src="../Images/56733abc0060fadd3b733f496cb9a05b.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*rsQ8hLAEL567E7vHptDzsw.png"/></div></figure><p id="fd93" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">其中等式右侧的第一项是分布<strong class="kd iu"><em class="kz">【p】</em></strong>的熵，第二项是根据<strong class="kd iu"> <em class="kz"> p </em> </strong>的分布<strong class="kd iu"> <em class="kz"> q </em> </strong>的期望。在大多数实际应用中，<strong class="kd iu"> <em class="kz"> p </em> </strong>是实际数据/测量值，而<strong class="kd iu"> <em class="kz"> q </em> </strong>是假设分布。对于 GANs，<strong class="kd iu"> <em class="kz"> p </em> </strong>是真实图像的概率分布，而<strong class="kd iu"> <em class="kz"> q </em> </strong> <em class="kz"> </em>是伪图像的概率分布。</p><p id="5dc7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> 4。</strong> <strong class="kd iu">验证</strong></p><p id="d752" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在让我们验证 KL-divergence 确实与使用交叉熵进行分布<strong class="kd iu"> <em class="kz"> p </em> </strong>和<strong class="kd iu"> <em class="kz"> q </em> </strong>相同。我们在 python 中分别计算了熵、交叉熵和 KL 散度。</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi lf"><img src="../Images/3136a2ddb5484b287438c10edf183c0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VwLRAmH9uwJWIj8ogJh7EQ.png"/></div></div></figure><figure class="lb lc ld le gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi lk"><img src="../Images/e3afa39cc62293078a755b70826fb028.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZV6lXlHtNSf9JQPlJ8tJQQ.png"/></div></div></figure><p id="dc4f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后我们如下比较这两个量:</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi ll"><img src="../Images/f3c9ab85195f4e64fed28ffd24c60ed9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9cIbsvHO3xGYfhtvWsPs6A.png"/></div></div></figure><p id="5900" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">右手边的第二项，即分布<strong class="kd iu"> <em class="kz"> p </em> </strong>的熵，可以被认为是一个常数，因此，我们可以得出结论，最小化交叉熵代替 KL-散度导致相同的输出，因此可以近似等于它。</p><p id="72ba" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">换句话说，我们的目标是在优化终止时达到分布<strong class="kd iu"> <em class="kz"> p </em> </strong>的不确定性水平，并且由于所有机器学习优化都是在受控数据集上执行的，该数据集在实验期间预计不会改变，因此，我们期望<strong class="kd iu"> <em class="kz"> p </em> </strong>的熵保持恒定。</p><p id="0835" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> 5。</strong> <strong class="kd iu">结论</strong></p><p id="9c49" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在本文中，我们学习了熵、交叉熵和 kl 散度的概念。然后我们回答了为什么这两个术语在深度学习应用中经常互换使用。我们还用 python 实现并验证了这些概念。完整代码请参考 github 资源库。<a class="ae lm" href="https://github.com/azad-academy/kl_cross_entropy.git" rel="noopener ugc nofollow" target="_blank">https://github.com/azad-academy/kl_cross_entropy.git</a></p><p id="526f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">参考资料:</p><p id="b8dc" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">[1] Goodfellow，I .等人，生成对抗网。神经信息处理系统进展。第 2672-2680 页，2014 年</p><p id="cec1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">[2]<a class="ae lm" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/kull back % E2 % 80% 93 lei bler _ divergence</a></p></div></div>    
</body>
</html>