<html>
<head>
<title>Understanding Dense Passage Retrieval (DPR) System</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解密集段落检索(DPR)系统</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-dense-passage-retrieval-dpr-system-bce5aee4fd40#2022-09-19">https://towardsdatascience.com/understanding-dense-passage-retrieval-dpr-system-bce5aee4fd40#2022-09-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e5bc" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">研究论文演练</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/18f23b124658dcb4d96502f9e76b65ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YldDy9zCXfqM_ae2NKi0qA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://unsplash.com/photos/AoqgGAqrLpU" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="6baa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇博客中，我们将深入研究论文《开放领域问答的密集段落检索T2》。我们将尝试理解它的理论方面，并做一些实践，建立我们自己的基于BERT的DPR模型。</p><p id="98a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">开放领域问答系统非常依赖高效的段落检索方法。这一步有助于选择回答任何问题的相关候选上下文。开放领域问答系统通常遵循两步流水线:</strong></p><ol class=""><li id="85a4" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">上下文检索器:</strong> <em class="me">上下文检索器</em>负责获取与问题相关并可能包含答案的一小部分段落。</li><li id="69ae" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu ma mb mc md bi translated"><strong class="lb iu">机器阅读器:</strong> <em class="me">机器阅读器</em>负责从这些段落集中识别正确答案。</li></ol><p id="d2c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="me">在这篇博客中，我们主要讨论管道的上下文检索部分的改进。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/1b4a7a977ec28da5ff6fdc0a97c48e10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cTvVzRX2HfWHBcfQoPS0vQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">开放域问答管道|作者图片</p></figure><p id="228a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">传统系统将TF-IDF和BM25的一些逻辑建模到它们的检索器中，<em class="me">这通常工作得很好，但问题是“我们能做得更好吗”？</em>我就不赘述<strong class="lb iu"> TF-IDF </strong>和<strong class="lb iu"> BM25 </strong>如何工作的细节了，可以随意查看一下<a class="ae ky" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank">这个</a>和<a class="ae ky" href="https://en.wikipedia.org/wiki/Okapi_BM25" rel="noopener ugc nofollow" target="_blank">这个</a>的情况。简而言之，它们在两个文本片段的加权单词袋表示法之间执行某种稀疏向量相似性。这种系统的一个明显的局限性是不能检索单词不匹配的上下文。最近，随着围绕自然语言处理中与词袋表示相反的<strong class="lb iu">语义</strong>建模的大肆宣传和影响，来自脸书·艾、华盛顿大学和普林斯顿大学的这项工作表明，基于密集向量表示可以有效地实现检索，并且还被视为大大超过传统技术。</p><p id="bf95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者提出了一种方法，通过简单的<strong class="lb iu">双编码器框架</strong>，利用少量的问题和黄金段落对来学习这些密集表示，也称为嵌入。让我们直接看一个论文中提到的例子来理解我们到底在试图解决什么——<em class="me">考虑一下“谁是指环王中的坏人？”这个问题，这可以从上下文“萨拉·贝克因在《指环王》三部曲中塑造反派索隆而闻名。”</em>基于术语的系统将难以检索这样的上下文，而密集检索系统将能够更好地将短语“坏人”与单词“恶棍”相匹配，从而获取正确的上下文。随着我们在博客中的进展，我们将会谈到这个方法的细节，但是让我们先看看一些结果。</p><p id="efa8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图显示了<strong class="lb iu"> top-k准确性</strong>的趋势，我们对在不同大小的真实数据集上训练的模型的k值进行了缩放。因此，这里的top-k准确性意味着，对于给定的查询(q ),有多少得分最高的段落是正确的，并且可以提供正确的答案。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ml"><img src="../Images/7249b27d8d46a808d98784480c3d1e08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aQNXDalXFayzGI9pJnRnCA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">与BM25相比的DPR Top-k精度性能|图片来自<a class="ae ky" href="https://arxiv.org/abs/2004.04906" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="ad76" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以清楚地看到，与BM25技术相比，仅用10k个问题和段落黄金对的训练样本，所提出的方法就可以在top-k准确度数字上获得几乎7-9%的提高。这里,“k”的值可以从非常小的5到非常大的100。令人惊讶的是，如果你对一个给定的问题只有10-20段感兴趣，那么即使只有1k黄金样本的训练也比BM25技术在top-k准确率上提高了近2-4%。</p><h1 id="e53e" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">密集通道检索器(DPR)</h1><p id="a861" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">密集段落检索器(DPR)负责基于段落和问题的高质量低维连续表示之间的相似性来检索与所问问题相关的段落。此外，由于整个系统必须合理地快速服务于用户的请求，所以包含这些表示的索引被预先计算和维护。现在，在推理期间，对于出现的任何新的查询/问题，我们可以有效地检索一些前k个段落，然后只在这个较小的子集上运行我们的阅读器组件。论文中的作者使用了<a class="ae ky" href="https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/" rel="noopener ugc nofollow" target="_blank">脸书人工智能相似性搜索(FAISS) </a>，这是一个允许我们快速<em class="me">(近似最近邻居)</em>搜索彼此相似的多媒体文档的库。这里,“k”的大小取决于几个因素，如流水线中的预期延迟、可用计算、召回等。但是一般来说，10-50之间的任何k值都可以达到这个目的。</p><p id="662c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">问题和段落嵌入之间的相似性通过计算它们之间的点积来表示。作者也试验了其他类似的方法，但最终选择了点积，因为它简单。相似性得分<em class="me">(或距离越小)</em>越高，文章就越与问题相关。<em class="me">数学表示如下— </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/9764f61b01496ff204ec613ffcb47869.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*BR3ERNloVbwn3Bzg-Gl39A.png"/></div></figure><p id="7a83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里q，p，Eq，Ep分别是问题文本，段落文本，输出问题表征的<a class="ae ky" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT </a>模型，输出段落表征的BERT模型。作者使用<em class="me">CLS标记的<em class="me"> 768维</em>表示作为输入文本片段的最终表示。</em></p><p id="695a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了训练模型，数据集被表示为<em class="me"> D={ &lt; q1，p1，p2，p3..pn &gt;，&lt; q2，p2，p1，p3..pn &gt; …}，</em>这里的气是第I个问题，每个问题都配有一个正例和一些反例。为了简单起见，上面每个例子的第一个索引是对问题“气”的肯定回答，其余的是否定回答。并且他们将损失函数优化为正通过的负对数似然。<em class="me">下图显示了相同- </em>的数学表示</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/190a4ecfc49f8bc770f5c3c35da5f91a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5cVhgBR1m-nDlXck6oJx_g.png"/></div></div></figure><p id="ef80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，qi，pi(+)，pi(-)分别是第I个问题，第I个相关的段落(正样本)，第I个不相关的段落(负样本)。<em class="me">优化的目标是最大化qi和pi(+)之间的相似性，并降低不相关的qi和pi(-)之间的相似性</em>。</p><p id="3e0b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常在检索场景中，获得正面例子是简单的(你可以通过给出问题和相关段落的训练数据获得)，而人们可能必须考虑有效地选择负面例子。作者在论文中实验了三种否定生成方法:(1) <strong class="lb iu"> Random </strong>:从语料库中随机抽取任意段落；(2) <strong class="lb iu"> BM25 </strong>:由BM25返回的顶部段落，其可能不一定包含答案，但是匹配大多数问题标记(3) <strong class="lb iu">黄金</strong>:与训练集中出现的其他问题配对的正面段落<em class="me">(具体地说是出现在相同小批量中的那些)</em>。发现第3点和第2点的组合效果最好。</p><p id="3b88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">至此，我们总结了对所提出模型的理论理解。现在我们开始建造我们自己的东西—</p><h1 id="3e51" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">代码模板</h1><p id="28a8" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">我们将使用<a class="ae ky" rel="noopener ugc nofollow" target="_blank" href="/Simple Transformers is a Natural Language Processing (NLP) library designed to simplify the usage of Transformer models without having to compromise on utility. It is built on the amazing work of Hugging Face and their Transformers library."> Simple Transformers python库</a>来设置实现DPR模型的模板。简单变压器旨在简化变压器模型的使用，而不必牺牲效用。它建立在令人惊叹的拥抱脸及其<a class="ae ky" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变形金刚</a>库的基础上。它支持NLP中的各种任务，可以在这里随意查看整个列表<a class="ae ky" href="https://simpletransformers.ai/docs/usage/#task-specific-models" rel="noopener ugc nofollow" target="_blank"/>。好的，那么<em class="me">你可以使用下面的命令来安装它- </em></p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="c83b" class="nq mn it nm b gy nr ns l nt nu">&gt;&gt; pip install simpletransformers</span></pre><p id="31ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来我们要做的是定义我们将用来训练模型的所有配置。该库为您提供了使用数据类或python字典定义配置的选项。为了这篇博客，我们将使用dataclass: RetrievalArgs。也可以从这个<a class="ae ky" href="https://simpletransformers.ai/docs/usage/#task-specific-models" rel="noopener ugc nofollow" target="_blank">源</a>中随意检查字典选项。</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="fa3a" class="nq mn it nm b gy nr ns l nt nu">from simpletransformers.retrieval import RetrievalModel, RetrievalArgs<br/>import torch</span><span id="5068" class="nq mn it nm b gy nv ns l nt nu">### loading pre-trained weights of passage and question encoder ###<br/>Eq = "bert-base-uncased"<br/>Ep = "bert-base-uncased"</span><span id="4fb4" class="nq mn it nm b gy nv ns l nt nu">model_args = RetrievalArgs()<br/>#model_args.retrieve_n_docs<br/>#model_args.hard_negatives<br/>#model_args.max_seq_length<br/>#model_args.num_train_epochs<br/>#model_args.train_batch_size<br/>#model_args.learning_rate</span></pre><p id="1113" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，Eq和Ep持有将用于编码问题和段落的模型。您还可以从一些公开的预先训练好的DPR编码器开始，用于上下文和段落<em class="me">(一个例子是上下文:“Facebook/DPR-question _ encoder-single-NQ-base”，段落:“Facebook/DPR-CTX _ encoder-single-NQ-base”)。</em>当“hard_negatives”设置为True时，有助于模型在批内否定的基础上学习使用BM25等技术生成的否定示例。如上所述，该论文还提出了批内底片的概念，并且还基于BM25或类似的方法提取底片样本。上面提到的所有参数都是不言自明的。该库提供了用于生成这些硬底片的代码片段。请在这里随意检查<a class="ae ky" href="https://simpletransformers.ai/docs/retrieval-specifics/" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="823d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们加载数据集，为了简单起见，我将手动定义一些数据点(<a class="ae ky" href="https://simpletransformers.ai/docs/retrieval-data-formats/" rel="noopener ugc nofollow" target="_blank">源</a>)。但是这将使您了解如何转换您的实际数据以适应管道。</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="7d24" class="nq mn it nm b gy nr ns l nt nu">import pandas as pd</span><span id="652b" class="nq mn it nm b gy nv ns l nt nu">train_data = [<br/>    {<br/>        "query_text": "Who is the protaganist of Dune?",<br/>        "gold_passage": 'Dune is set in the distant future amidst a feudal interstellar society in which various noble houses control planetary fiefs. It tells the story of young Paul Atreides, whose family accepts the stewardship of the planet Arrakis. While the planet is an inhospitable and sparsely populated desert wasteland, it is the only source of melange, or "spice", a drug that extends life and enhances mental abilities. Melange is also necessary for space navigation, which requires a kind of multidimensional awareness and foresight that only the drug provides. As melange can only be produced on Arrakis, control of the planet is a coveted and dangerous undertaking.',<br/>    },<br/>    {<br/>        "query_text": "Who is the author of Dune?",<br/>        "gold_passage": "Dune is a 1965 science fiction novel by American author Frank Herbert, originally published as two separate serials in Analog magazine.",<br/>    }<br/>    ...<br/>]</span><span id="9acb" class="nq mn it nm b gy nv ns l nt nu">train = pd.DataFrame(<br/>    train_data<br/>)</span></pre><p id="9c6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了这个，你就可以继续训练DPR模型了，但是如果你已经将hard_negatives设置为True，你就必须为每个数据点设置另一个key (hard_negative ),格式如上所述。</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="c998" class="nq mn it nm b gy nr ns l nt nu">cuda_available = torch.cuda.is_available()</span><span id="8b02" class="nq mn it nm b gy nv ns l nt nu">model = RetrievalModel(<br/>    model_type = "dpr",<br/>    context_encoder_name = Ep,<br/>    query_encoder_name = Eq,<br/>    args = model_args,<br/>    use_cuda = cuda_available<br/>)</span><span id="4da7" class="nq mn it nm b gy nv ns l nt nu">model.train_model(train, eval_data = eval, \<br/>                output_dir = 'model/', \<br/>                show_running_loss = True)</span><span id="bcd6" class="nq mn it nm b gy nv ns l nt nu">model.eval_model(test, verbose=True)</span></pre><p id="5817" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将所有必要的参数传递给我们的模型，并通过指定保存模型的输出目录来进行训练。“eval”和“test”数据帧的格式也与“train”完全相同。这里，train、eval和test是pandas数据帧，包含2-3列— <code class="fe nw nx ny nm b">query_text</code>、<code class="fe nw nx ny nm b">gold_passage</code>、<code class="fe nw nx ny nm b">hard_negative</code>(可选)</p><ul class=""><li id="17dc" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nz mb mc md bi translated"><code class="fe nw nx ny nm b">query_text</code>:查询/问题文本序列</li><li id="f148" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu nz mb mc md bi translated"><code class="fe nw nx ny nm b">gold_passage</code>:黄金段落文字序列</li><li id="a33f" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu nz mb mc md bi translated"><code class="fe nw nx ny nm b">hard_negative</code>:BM25中的硬否定段落文本序列(可选)</li></ul><h1 id="84a2" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">用训练好的DPR模型进行推理</h1><p id="87fb" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">一旦训练完成，你已经保存了你的模型。现在，您可以将问题传递给模型，指定要返回的文档数量，这样就可以了。</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="11c8" class="nq mn it nm b gy nr ns l nt nu">questions = [<br/>    'What year did Dune release?'<br/>]</span><span id="ac55" class="nq mn it nm b gy nv ns l nt nu">predicted_passages, _, _, _ = model.predict(questions, retrieve_n_docs=2)</span></pre><h1 id="a54d" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">总结想法</h1><p id="f71e" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">看到新兴技术的采用以及它在这种实际用例中的应用是非常有趣的。在本文中，我们讨论了什么是DPR，它是如何工作的，以及它的用途。我们也使用简单的Transformers python库实现了它。请随意查看其他规范，您可以使用它们来训练一个高效的模型。我希望你喜欢这篇文章。此外，如果你喜欢看视频而不是阅读文本，你可以在<a class="ae ky" href="https://youtu.be/YOunnfYEZfA" rel="noopener ugc nofollow" target="_blank">这个</a>查看视频论文解释。尽管你在那里找不到这里独有的实现；)</p><p id="02a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另外，如果你愿意支持我成为一名作家，可以考虑注册<a class="ae ky" href="https://prakhar-mishra.medium.com/membership" rel="noopener">成为一名媒体成员</a>。每月只需5美元，你就可以无限制地使用Medium。</p></div></div>    
</body>
</html>