<html>
<head>
<title>Simple Explanation on How Decision Tree Algorithm Makes Decisions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">浅谈决策树算法如何决策</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/simple-explanation-on-how-decision-tree-algorithm-makes-decisions-34f56be344e9#2022-02-22">https://towardsdatascience.com/simple-explanation-on-how-decision-tree-algorithm-makes-decisions-34f56be344e9#2022-02-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/07b97f1c859e5729427df6d745b3dfd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4QwcsMjXUii59DVa"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">阿达什·库姆穆尔在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""/><div class=""><h2 id="8c3c" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">决策树算法背后的直觉</h2></div><p id="69bf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">决策树是一种非常流行的机器学习算法。它适用于线性和非线性数据。此外，它还可以用于分类和回归。有了Python和R中的优秀库和包，任何人都可以轻松地使用决策树和其他机器学习算法，甚至不知道它是如何工作的。但是了解算法的直觉或机制有助于决定在哪里使用它。</p><p id="d57a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇文章中，我将尝试给你一个关于决策树算法如何工作的直觉。</p><blockquote class="lu lv lw"><p id="fa5c" class="ky kz lx la b lb lc kk ld le lf kn lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated"><strong class="la jk">本文将包括:</strong></p></blockquote><p id="2ec2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">决策树算法工作流程的高层介绍:</strong>如果这个高层介绍不是那么清楚，也不用担心。接下来的部分是一个例子的详细解释。那就更清楚了。</p><p id="6eab" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">决策树中决策背后的一个例子和数学计算:</strong>这部分将展示决策树算法利用数学计算做出的真实决策。</p><h2 id="db0c" class="mb mc jj bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">决策树介绍</h2><p id="e55d" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">决策树算法基于对特征条件的判定来工作。节点是对属性的条件或测试，分支代表测试的结果，叶节点是基于条件的决策。</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mz"><img src="../Images/d65cf9520355b2967b3c3b4b4c11bafb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ECd5efKxuSR-CZMIx0kUZA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="88bb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如您在图中看到的，它从一个根条件开始，基于该根条件的决策，我们得到三个分支，C1、C2和C3。它不一定是三个分支。分支可能更多，这取决于根节点特性中的类的数量。一根树枝最后变成了一片叶子。叶节点意味着最终的决定或预测。根节点的C1和C3以条件1和条件2结束。</p><p id="f699" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">条件1和条件2是两种不同的功能。我们将根据条件1和条件3中的类别进一步拆分数据。上图显示条件1和条件3各有两个特征。和往常一样，类别可以不止这些。这是一个关于决策树如何进行预测的高级概念。我们将更详细地了解我们实际上是如何为根节点和其他条件选择特性的。</p><p id="821f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在深入研究之前，需要明确以下一些术语:</p><p id="c578" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">根节点:</strong>正如我已经讨论过的，根节点是决策树的起点。这是开始拆分数据的第一个功能。</p><p id="9ccc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">决策节点:</strong>看上面的‘条件1’和‘条件3’节点。我们在基于根节点分裂之后得到那些节点，并且我们基于决策节点进一步分裂。</p><p id="8bfb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">叶节点:</strong>这些是决策树做出最终决策的最终节点，不能再进一步拆分。在上图中，决策2和决策3是叶节点。</p><h2 id="11dc" class="mb mc jj bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">决策树中决策背后的示例和数学计算</h2><p id="7d50" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">这是我为这个例子创建的一个合成数据集。该数据集有一个二进制类和三个变量N1、N2和N3。这三个变量都是绝对的。我采用了所有的分类变量，因为这样更容易解释。<strong class="la jk">一旦你有了直觉，也就更容易理解连续变量的用途。</strong></p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/94b415a4136358b88f98ab8b0cb669bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*lBWUZY2g6wCvk_Qpq0Vi7w.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="adf1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们有数据集。根据前面的讨论，我们需要一个<strong class="la jk">根节点</strong>，它基于我们将要开始拆分的数据。但是我们有三个变量。<strong class="la jk">如何决定根节点？我是否应该简单地从N1开始，因为它在数据集中排在第一位？</strong></p><p id="a686" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不完全是。为了理解如何做出关于哪个特征应该是根节点的正确决策，您需要了解信息增益。</p><p id="5e37" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">基于称为<strong class="la jk">信息增益</strong>的统计测量来选择根节点或第一测试属性。</p><p id="dace" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总体而言，测试属性的选择取决于“纯度测量”。这些是纯度指标:</p><ol class=""><li id="92d7" class="nf ng jj la b lb lc le lf lh nh ll ni lp nj lt nk nl nm nn bi translated">信息增益</li><li id="87e6" class="nf ng jj la b lb no le np lh nq ll nr lp ns lt nk nl nm nn bi translated">增益比</li><li id="6d30" class="nf ng jj la b lb no le np lh nq ll nr lp ns lt nk nl nm nn bi translated">基尼指数</li></ol><p id="c1f4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将详细讨论“信息增益”。</p><blockquote class="lu lv lw"><p id="918a" class="ky kz lx la b lb lc kk ld le lf kn lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated"><strong class="la jk">信息增益</strong></p></blockquote><p id="f924" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">想象一下10个朋友在一起。他们中的5个人想看电影，5个人想出去。在这种情况下做决定真的很难。如果他们中有8到9个人想看电影，而只有1到2个人想出去，那就更容易做出决定。对吗？最好的情况是他们10个人都想要同样的东西。决策非常容易。一点都不混乱！。</p><p id="1588" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样在决策树中，如果所有的类都属于yes或no，则数据集是最纯的。另一方面，如果类的50%属于是，另50%属于否，那么数据集是极不纯的。因为很难做决定。决定很不确定！</p><p id="e835" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">信息增益有助于衡量某一特征的不确定性的减少。<strong class="la jk">这也有助于决定哪一个特征适合作为根节点。</strong></p><p id="7494" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">信息增益的计算取决于<strong class="la jk">信息</strong>。那也叫<strong class="la jk">熵</strong>。</p><p id="e4a5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">信息的公式:</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/29e141dac1be515ccb7a860412060948.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*OH8e8U7Y4Cap3u89izsOUg.png"/></div></figure><p id="006b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看一个例子。</p><p id="9851" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我将研究上面的虚拟数据集，找出哪个要素将成为该数据集的根结点。</p><p id="d7ed" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，让我们计算一下虚拟数据集的信息。这个数据集总共有14行数据，8个是类和6个否类。</p><p id="d1cf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以，信息是:</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/93f61366e6a7ac5f13d34d860f078d51.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*SIyCcI91PIuXwGIS2kyk0g.png"/></div></figure><p id="28f8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">信息出来是0.985。<strong class="la jk">这是母熵。</strong></p><p id="b43d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，我们需要逐一计算三个变量N1、N2和N3的信息。</p><p id="bbe4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">从N1开始:</strong></p><p id="15a3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们需要通过分割N1数据集来计算分类样本所需的信息量:</p><p id="656d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，让我们分析一下N1的数据。有:</p><p id="bed7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">4 '雨天',其中2 'Y '和2 'N '类</p><p id="b36f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">4 'Sunny ',其中2 'Y's和2 'N's class</p><p id="dbe0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">6 '多云',其中4'Y '和2'N '级</p><p id="275c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">基于N1对数据集进行分类所需的信息如下:</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/eca7e9f77d1f1b729be840980ed15a96.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*uGBLoEJgIUGuLewlD9hemQ.png"/></div></figure><p id="8625" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它给出0.964。</p><p id="26a6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">Info _ gain(N1)</strong>= 0.985–0.964 = 0.02</p><p id="1429" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，如果N1成为根节点<strong class="la jk"/>，熵将减少0.02。</p><p id="cc67" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同理，我们可以计算出Info(DN2)和Info (DN3)分别为0.972和0.983。</p><p id="f248" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以，N2和N3的信息增益是:</p><p id="1988" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">信息_增益(N2)</strong>= 0.985–0.972 = 0.012</p><p id="99a5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">信息_增益(N3)</strong>= 0.985–0.983 = 0.001</p><p id="bf48" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们将N2或N3作为我们的根节点，熵<strong class="la jk">将分别减少0.012和0.001。</strong></p><blockquote class="lu lv lw"><p id="3d75" class="ky kz lx la b lb lc kk ld le lf kn lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">根据上面的计算，信息增益最高的属性是N1。所以，N1将是根。</p></blockquote><p id="8fb2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这个阶段，根节点已经固定，决策树将如下所示:</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/942d1ed1470109cb87057ac794b0fbfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*SS_SaFm-O-f6sbfiVd5icA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="9341" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们选择了根节点，并根据根节点分割数据。图中显示了数据与相应类别的精确划分。</p><p id="1e41" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">接下来呢？</strong></p><p id="29b8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如你所见，当“下雨”或“晴天”时，熵值非常高。因为Y和N的数量相同。但是“多云”时熵更低。所以，在‘多云’的时候，类可以是Y。我们可以根据N2和N3再次分割数据。为此，我们必须计算每个子节点“雨天”、“多云”和“晴天”的信息增益，以决定下一个特征出现在哪里。如您所见，对于每个部门，我们现在都有更小的数据集。</p><p id="b406" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我不会在这里展示任何进一步的计算。本文的目的是给出一个关于决策树算法如何工作的概念。</p><h2 id="042b" class="mb mc jj bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">哪里停止分裂</h2><p id="5760" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">我们致力于根据特征分割数据。这个虚拟数据集只有3个要素。但是真实世界的数据集可能有更多的特征需要处理。如果我们有大量的特征，并且我们想要使用所有的特征来构建树，树将变得太大。这可能导致过拟合和长计算时间。</p><p id="d0d2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了处理这个问题，在scikit-learn库的决策树函数中有一个<strong class="la jk"> max_depth </strong>参数。如果最大深度参数很大，树会更大。所以，我们可以选择树的深度。<strong class="la jk">尽管你可以使用任何你选择的特征选择方法来为算法选择更少的特征。</strong></p><p id="089c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">还有一个参数，<strong class="la jk"> max_features </strong>。参数的名称说明了它的作用。您可以指定要用于树的最大要素数。</p><p id="0be6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其他参数请参见<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><h2 id="88d7" class="mb mc jj bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">决策树算法的主要优势</h2><ol class=""><li id="652d" class="nf ng jj la b lb mu le mv lh nx ll ny lp nz lt nk nl nm nn bi translated">从我们处理的例子中可以看出，在某种程度上，它给了你一个关于预测如何发生的清晰的可视化。因此，更容易向利益相关者或客户解释。</li><li id="a58a" class="nf ng jj la b lb no le np lh nq ll nr lp ns lt nk nl nm nn bi translated">不需要特征缩放。</li><li id="7251" class="nf ng jj la b lb no le np lh nq ll nr lp ns lt nk nl nm nn bi translated">因变量和自变量之间的非线性关系不会影响决策树算法的性能。</li><li id="ca0f" class="nf ng jj la b lb no le np lh nq ll nr lp ns lt nk nl nm nn bi translated">决策树算法可以自动处理异常值。</li><li id="cf70" class="nf ng jj la b lb no le np lh nq ll nr lp ns lt nk nl nm nn bi translated">自动处理缺失值。</li></ol><h2 id="3420" class="mb mc jj bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">决策树算法的主要缺点</h2><ol class=""><li id="0ae1" class="nf ng jj la b lb mu le mv lh nx ll ny lp nz lt nk nl nm nn bi translated">如前所述，存在过度拟合的主要风险。这也可能导致高方差，从而导致预测中的许多错误。</li><li id="43fd" class="nf ng jj la b lb no le np lh nq ll nr lp ns lt nk nl nm nn bi translated">决策树算法很容易不稳定。数据中的一点点噪声或添加一个额外的数据可能会使整个树不稳定，或者可能会重新构建树。</li><li id="fe26" class="nf ng jj la b lb no le np lh nq ll nr lp ns lt nk nl nm nn bi translated">决策树算法不适合大型数据集。如示例部分所述，如果数据集太大，算法可能会变得太复杂，并可能导致过拟合问题。</li></ol><h2 id="af68" class="mb mc jj bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">结论</h2><p id="2b4f" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">Python和R对于所有流行的机器学习算法都有非常好的包。所以，现在工作很轻松。但是了解背后的机制会让你更好地为你的项目选择正确的机器学习算法。</p><h2 id="e4bb" class="mb mc jj bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">更多阅读</h2><div class="is it gp gr iu oa"><a rel="noopener follow" target="_blank" href="/multiclass-classification-algorithm-from-scratch-with-a-project-in-python-step-by-step-guide-485a83c79992"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd jk gy z fp of fr fs og fu fw ji bi translated">使用Python从零开始的多类分类算法:分步指南</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">本文介绍两种方法:梯度下降法和优化函数法</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="ok l ol om on oj oo ja oa"/></div></div></a></div><div class="is it gp gr iu oa"><a rel="noopener follow" target="_blank" href="/details-of-violinplot-and-relplot-in-seaborn-30c63de23a15"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd jk gy z fp of fr fs og fu fw ji bi translated">Seaborn的Violinplot和Relplot的细节</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">充分发挥潜力在Seaborn使用小提琴和Relplots</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="op l ol om on oj oo ja oa"/></div></div></a></div><div class="is it gp gr iu oa"><a rel="noopener follow" target="_blank" href="/a-complete-k-mean-clustering-algorithm-from-scratch-in-python-step-by-step-guide-1eb05cdcd461"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd jk gy z fp of fr fs og fu fw ji bi translated">Python中从头开始的完整K均值聚类算法:分步指南</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">还有，如何使用K均值聚类算法对图像进行降维</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="oq l ol om on oj oo ja oa"/></div></div></a></div><div class="is it gp gr iu oa"><a rel="noopener follow" target="_blank" href="/stochastic-gradient-descent-explanation-and-complete-implementation-from-scratch-a2c6a02f28bd"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd jk gy z fp of fr fs og fu fw ji bi translated">随机梯度下降:从头开始的解释和完整实现</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">使用单个感知器</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="or l ol om on oj oo ja oa"/></div></div></a></div><div class="is it gp gr iu oa"><a rel="noopener follow" target="_blank" href="/a-complete-anomaly-detection-algorithm-from-scratch-in-python-step-by-step-guide-4c115e65d54e"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd jk gy z fp of fr fs og fu fw ji bi translated">Python中从头开始的完整异常检测算法:分步指南</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">基于概率的异常检测算法</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="os l ol om on oj oo ja oa"/></div></div></a></div><div class="is it gp gr iu oa"><a href="https://pub.towardsai.net/dissecting-1-way-anova-and-ancova-with-examples-in-r-a3a7da83d742" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd jk gy z fp of fr fs og fu fw ji bi translated">用R中的例子剖析单向方差分析和协方差分析</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">通过分析方差得出的均值差异</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">pub.towardsai.net</p></div></div><div class="oj l"><div class="ot l ol om on oj oo ja oa"/></div></div></a></div><div class="is it gp gr iu oa"><a rel="noopener follow" target="_blank" href="/a-complete-sentiment-analysis-algorithm-in-python-with-amazon-product-review-data-step-by-step-2680d2e2c23b"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd jk gy z fp of fr fs og fu fw ji bi translated">一个完整的带有亚马逊产品评论数据的Python情感分析算法:一步一步</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">使用Python的Scikit_learn库的NLP项目</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="ou l ol om on oj oo ja oa"/></div></div></a></div></div></div>    
</body>
</html>