<html>
<head>
<title>Getting started with Delta Lake &amp; Spark in AWS— The Easy Way</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在AWS中开始使用Delta Lake &amp; Spark简单的方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/getting-started-with-delta-lake-spark-in-aws-the-easy-way-9215f2970c58#2022-08-31">https://towardsdatascience.com/getting-started-with-delta-lake-spark-in-aws-the-easy-way-9215f2970c58#2022-08-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4c53" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在AWS中配置EC2上的Apache Spark和Delta Lake的分步教程，以及Python中的代码示例</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3fbfe71000293e6dffe66adfa37651e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6etsFJJy0RL6h9xj"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">约书亚·索蒂诺在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="fb35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您从事过数据湖或湖屋解决方案的工程设计，那么您可能已经针对数据湖平台的可伸缩存储层采用了(或听说过)解耦和分布式计算框架。尽管这种计算框架的列表在不断增长，但Apache Spark一直在不断发展，并证明了它在大数据处理领域的稳健性。随着Apache Spark越来越成功，许多供应商也在提供各种风格的基于Apache Spark的解决方案(如Databricks、Amazon EMR)。此外，在解决现有数据湖解决方案的<a class="ae ky" href="https://www.databricks.com/discover/data-lakes/introduction#:~:text=all%20at%20once.-,Data%20lake%20challenges,-Despite%20their%20pros" rel="noopener ugc nofollow" target="_blank">酸性限制方面出现了激增，在这种情况下，您可能听说过像D </a><a class="ae ky" href="https://delta.io/" rel="noopener ugc nofollow" target="_blank">埃尔塔湖</a>、胡迪、冰山这样的解决方案。所有这些看起来都很吸引人，但是对于初学者来说，有时会有点不知所措。有一种可能性是，您可能希望从小处着手，在ACID兼容的数据湖中利用Apache Spark的潜力(例如，通过在AWS中的VM (EC2)中正确配置它)。或者，您可能希望解决特定的用例，这些用例不需要基于供应商的产品，或者不需要大量实例来进行大规模分布式处理。此类特定用例的一些示例可能是:</p><ul class=""><li id="b5f5" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">您发现您现有的ETL过程没有类似SQL的处理能力。例如，假设您正在使用Python，您主要依赖Python原生数据结构(如列表、字典)及其模块(如csv、pandas)来实现所需的转换。您认为，能够运行SQL表达式并与更抽象和可扩展的数据结构(如dataframes)互换以实现您想要的转换，可以加速和简化您的开发，例如，而不是求助于多行代码来从登录目录读取数据并根据列的值将其拆分到多个目标目录；您可以通过Spark SQL中的几行代码方便地实现它。</li><li id="8871" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">你发现<a class="ae ky" href="https://aws.amazon.com/blogs/aws/s3-glacier-select/" rel="noopener ugc nofollow" target="_blank"> S3选择</a>是相当有限的，并且想要一个更好的解决方案来运行除雅典娜之外的S3上的SQL。</li><li id="2fca" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">如果您的数据速度接近实时，并且在任何时间点，正在处理的数据量可以在资源范围内，即虚拟机的CPU和RAM，因此它不需要分布式计算。</li><li id="c182" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">您希望构建基于Apache Spark和Delta Lake的解决方案原型，通过从小处着手(即在您选择的EC2实例中运行)来实现其业务价值。您不希望为您的初始原型运行一个成熟的基于供应商的解决方案。如果你想学习这些新技术，同样的启发法也适用。</li></ul><p id="e4fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果以上任何一个条款引起了您的共鸣，那么您可能会发现这篇文章非常有帮助，因为它以简单/容易的方式解释了如何在AWS中的EC2实例上开始使用Apache Spark和Delta Lake。</p><h2 id="da30" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">先决条件</h2><p id="2eaa" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">要跟进，您将需要以下内容:</p><ul class=""><li id="e0c5" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">AWS帐户</li><li id="96e7" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">EC2实例(可以是任何大小，但建议至少是具有2个vCPUs的实例)配置如下:<br/> - Python(理想情况下是&gt; 3.8)(可选地配置了虚拟环境)<br/> - JDK(理想情况下是Oracle的，但OpenJDK也工作得很好。我特别发现Oracle JDK 11对于Apache Spark 3来说相当可靠</li><li id="3938" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">S3桶(您将在那里为您的三角洲湖水合/存储数据)</li><li id="d4a6" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">附加到EC2实例的IAM角色，允许对S3存储桶进行读/写操作</li></ul><h2 id="5a20" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">步伐</h2><p id="270a" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">第一步是在您的(虚拟)环境中安装PySpark。在撰写本文时，我发现pyspark 3.2.2在与Delta Lake dependencies结合使用时相当稳定。所以我将在本文中使用它。</p><p id="2c75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您使用pip在您的环境中安装依赖项，请运行以下命令:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="fd3c" class="mj mk it ni b gy nm nn l no np">pip install pyspark==3.2.2</span></pre><p id="57b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果一切顺利，PySpark模块将安装在您的Python环境中！现在是时候使用它了。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="7b84" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是代码片段中发生的事情的概要分析:</p><ul class=""><li id="b4f5" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">第1行—我们从pyspark.sql模块导入SparkSession类</li><li id="956b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">第2行—我们指定Spark工作所需的依赖关系，例如允许Spark与AWS(在我们的例子中为S3)交互、使用Delta Lake核心等</li><li id="f429" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">第3行——我们实例化了SparkSession对象，该对象标记为在脚本中使用Spark的入口点。在创建对象时发生了很多事情，您可能希望仔细阅读spark文档以了解每一个的基本原理，但有几个是关键的:<br/> - spark.sql.extensions:扩展Spark会话对象以使用Delta Lake功能<br/> - spark.jars.packages:以逗号分隔的jar Maven坐标列表以包含在驱动程序和执行器类路径中<br/>-Spark . SQL . sources . partitionoverwritemode:允许更好地/动态写入分区数据，即不删除现有分区，只插入当前数据帧中的分区</li></ul><p id="3520" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">提示:如果您使用的是公司HTTP/HTTPS代理，您可以添加以下配置来允许遍历代理，从而允许Spark从Maven repo下载指定的jars包:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="7534" class="mj mk it ni b gy nm nn l no np">.config("spark.driver.extraJavaOptions", "-Duser.timezone=UTC -Dhttp.proxyHost=proxy_hostname -Dhttp.proxyPort=port -Dhttps.proxyHost=proxy_hostname -Dhttps.proxyPort=port")</span></pre><p id="f784" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦完成，Spark会话对象就可以使用了。</p><h2 id="8ca0" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">从S3读取数据</h2><p id="4407" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">让我们使用一个样本数据集。对于本文，我选择了来自新西兰统计局网站的<a class="ae ky" href="https://stats.govt.nz/large-datasets/csv-files-for-download/" rel="noopener ugc nofollow" target="_blank">就业数据(请参考本文末尾关于数据集许可方面的参考资料部分)。</a></p><p id="5215" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我也将CSV文件上传到我的帐户中的一个桶中。</p><p id="3d81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是如何在Spark中读取带头文件的CSV文件:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="d8ac" class="mj mk it ni b gy nm nn l no np">df = spark.read.option("recursiveFileLookup", "true").option("header","true").csv("s3://your_bucket/test/machine-readable-business-employment-data-mar-2022-quarter.csv")</span></pre><p id="f47c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要注意的是，由于Spark遵循惰性执行模型，所以在这个阶段它不会在内存中加载任何数据。仅当您在Spark中执行某个操作时(例如。count()，。show()函数)，然后它会将所需的数据加载到内存中进行处理。</p><p id="2bc7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们先看一下数据是什么样子的:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="9966" class="mj mk it ni b gy nm nn l no np">df.show()</span></pre><p id="f57e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/5d296ac654b643fefe2a2f37fcd87674.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M66mEuRHmI097m83zYkVMw.png"/></div></div></figure><h2 id="6ae2" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">基于SQL的数据转换</h2><p id="282b" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">Spark最强大的特性之一是能够对数据运行SQL查询。这大大加速和简化了ETL，因为您可以通过SQL快速表达非常强大的数据转换逻辑，而不是编写冗长/复杂的代码来实现相同的功能。</p><p id="8e9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要运行SQL查询，您需要在数据上创建一个“临时视图”。此视图未注册到集成目录/hive-metastore(例如Glue ),并且将仅位于当前会话的本地。</p><p id="06be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要创建临时视图:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="6aa3" class="mj mk it ni b gy nm nn l no np">df.createOrReplaceTempView("employment_tbl")</span></pre><p id="ddcc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完成后，您可以对S3上的数据运行SQL查询。你可以通过使用spark.sql()函数来完成:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="79ba" class="mj mk it ni b gy nm nn l no np">spark.sql("select Series_title_2,count(*) as count from employment_tbl group by Series_title_2 order by 2 desc").show(truncate=False)</span></pre><p id="80d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/75d6b9facaecef8adce7596f77656ef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BkXxvMNP9nFc_ZVERsHxIw.png"/></div></div></figure><p id="f2ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们运行一个group by SQL查询来计算唯一值在series_title_2列中出现的次数。</p><p id="7eab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您还可以根据以下转换逻辑创建新的数据帧:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="4229" class="mj mk it ni b gy nm nn l no np">df_groupby = spark.sql("select Series_title_2,count(*) as count from employment_tbl group by Series_title_2 order by 2 desc")</span></pre><h2 id="5a4a" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">将增量格式的数据写入S3</h2><p id="b95a" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">让我们将这个转换后的数据帧以delta格式写入S3，以实现我们的“delta-lake”</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="60f8" class="mj mk it ni b gy nm nn l no np">df_groupby.write.format("delta").save("s3://your_bucket/test/sample_data/")</span></pre><p id="1235" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完成后，您可以从S3(例如通过管理控制台)验证数据是否以增量格式写入:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/b5ac816183fda929489add87ffe7c08a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8e2ySkwAn4MCl72l2ltjZg.png"/></div></div></figure><p id="2848" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想在Spark中读取delta格式的数据:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="b288" class="mj mk it ni b gy nm nn l no np">df = spark.read.format("delta").load("s3://your_bucket/test/sample_data/")</span></pre><p id="89cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想看数据:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="b2b5" class="mj mk it ni b gy nm nn l no np">df.show()</span></pre><p id="0c16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是它看起来的样子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/e913b03322398a2edc4d59f87b421ae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*4l52ddpWhEvA-0QJ5DuN3w.png"/></div></figure><h2 id="aea6" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">对数据执行更新</h2><p id="2faf" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">像Delta Lake这样的技术的独特主张之一是能够对data-lake上的数据执行符合ACID的更新/删除。传统上，在受Hadoop影响的大数据范式中，这是一个严重的限制，数据工程师不得不求助于各种变通办法来实现数据集中所需的更新/删除效果。但是现在，这个功能在Delta Lake(和胡迪的Iceberg)中得到了原生支持。</p><p id="47ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设您想要删除series_title_2 = '其他服务'的所有记录。您可以这样做的方法之一如下。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="b427" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的代码片段中，我们在表数据以增量格式存储的路径上创建了对增量表的引用。然后，我们指定了如何更新数据的逻辑，即将Series_title_2列值从“其他服务”更改为“其他”。执行时，这将直接将更新应用于S3上的底层增量文件。</p><p id="6df8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了验证更新是否发生，让我们读回dataframe中的数据并运行一些SQL查询:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="e213" class="mj mk it ni b gy nm nn l no np">df = spark.read.format("delta").load("s3://your_bucket/test/delta_format/")</span><span id="d89d" class="mj mk it ni b gy nw nn l no np">df.createOrReplaceTempView("employment_tbl")</span><span id="6662" class="mj mk it ni b gy nw nn l no np">spark.sql("select count(*) from employment_tbl where Series_title_2 = 'Other Services'").show()</span></pre><p id="7c6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个计数是零:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/ef46f4b164e562bc530dcb32dd226b63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fJo5Xv4zxpZKzFAiutVHAw.png"/></div></div></figure><p id="b490" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们运行另一个执行Group By的SQL查询来验证“Other Services”不存在，并且已被替换为“Others”:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/268abde0081ae8287a8c6abb6c127778.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kB7s5SYgkApEQKhPL-8Pbw.png"/></div></div></figure><p id="b579" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，数据帧中倒数第四个条目是“Others”，如果您将它与我们之前运行的group by query的输出进行比较，您会发现它是“Other Services”。</p><p id="56f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用类似的方法，您也可以删除数据。此外，我没有使用SQL表达式来执行更新和删除，但是您也可以轻松地做到这一点。</p><p id="78fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，如果您一直遵循本文，那么您已经完成了整个工作流程，即读取原始数据(CSV格式)、进行一些转换、以delta格式写入数据、更新数据，然后再次读取delta格式的数据。这是通过Spark可以实现的所有高级转换的基础。</p><p id="e6d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">值得指出的是，本文中的方法确实有一些限制:</p><ul class=""><li id="551a" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">该设置未与集中式Hive Metastore或目录(如Glue)集成。因此，如果您运行CREATE TABLE语句，这些语句将不会在中央目录中注册，因此其他Spark实例或工具(如Athena)将无法访问这些表。虽然你可以自己配置与Glue catalog集成，但是在这种情况下，EMR确实提供了一些好处，因为它提供了与Glue的开箱即用集成。</li><li id="5f85" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">该设置不是分布式的，即它不利用多个虚拟机来执行分布式计算。</li></ul><p id="92d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">符合ACID标准的datalake解决方案，如Delta Lake和胡迪等，以及像Databricks这样提供基于它的商业解决方案的相关公司，在数据行业正获得相当好的发展势头，因此了解这项技术将是一项很好的投资。最好的学习方法是通过实践，如果你发现了一个有价值的用例，就从小处着手，做实验，打破常规，构建东西，然后进化！</p><h2 id="8eb9" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">参考</h2><ul class=""><li id="3510" class="lv lw it lb b lc nc lf nd li nz lm oa lq ob lu ma mb mc md bi translated">三角洲湖泊:<a class="ae ky" href="https://delta.io/" rel="noopener ugc nofollow" target="_blank">https://delta.io/</a></li><li id="ccb0" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">文章中使用的数据集:<a class="ae ky" href="https://stats.govt.nz/assets/Uploads/Business-employment-data/Business-employment-data-March-2022-quarter/Download-data/business-employment-data-march-2022-quarter-csv.zip" rel="noopener ugc nofollow" target="_blank">https://stats . govt . NZ/assets/Uploads/Business-employment-data/Business-employment-data-March-2022-quarter/Download-data/Business-employment-data-March-2022-quarter-CSV . zip</a><br/>(<a class="ae ky" href="https://stats.govt.nz/large-datasets/csv-files-for-download/#:~:text=Unless%20indicated%20otherwise%2C%20all%20content%20on%20stats.govt.nz%20is%20licensed%20for%20re%2Duse%20under%20a%20Creative%20Commons%204.0%20International%20Licence" rel="noopener ugc nofollow" target="_blank">根据知识共享4.0国际许可授权重用</a>)</li></ul></div></div>    
</body>
</html>