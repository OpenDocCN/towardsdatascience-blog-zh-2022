<html>
<head>
<title>FuzzyTM: A Python package for Fuzzy Topic Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">FuzzyTM:一个用于模糊主题模型的 Python 包</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fuzzytm-a-python-package-for-fuzzy-topic-models-fd3c3f0ae060#2022-02-21">https://towardsdatascience.com/fuzzytm-a-python-package-for-fuzzy-topic-models-fd3c3f0ae060#2022-02-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/d3e438a8d2d5ee6d7de018412f27710c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HWMhV3yBlEEgE-Mz"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">莎伦·麦卡琴在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="7abb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我以前的文章中，我展示了如何开始使用 Python 包 OCTIS，这是一个比较和优化最先进的主题建模算法的包。<a class="ae kc" rel="noopener" target="_blank" href="/a-beginners-guide-to-octis-optimizing-and-comparing-topic-models-is-simple-590554ec9ba6">第一篇</a>展示如何入门 OCTIS，<a class="ae kc" rel="noopener" target="_blank" href="/a-beginners-guide-to-octis-vol-2-optimizing-topic-models-1214e58be1e5">第二篇</a>重点介绍模型优化。我和我的团队开始与 OCTIS 合作的原因是，我们开发了一种新的主题建模算法，称为 FLSA-W [1]，并希望看到它与现有的最先进的技术相比表现如何。基于对各种开放数据集的比较，我们发现它在大多数情况下在一致性(c_v)、多样性和可解释性方面优于其他模型(如 LDA、ProdLDA、NeuralLDA、NMF 和 LSI)。我还不能分享这些结果，因为我们已经将这项工作提交给一个会议，正在等待接受。与此同时，我们还开发了一个 Python 包，<a class="ae kc" href="https://pypi.org/project/FuzzyTM/" rel="noopener ugc nofollow" target="_blank"> FuzzyTM </a>，其中包含 FLSA-W 和另外两个基于模糊逻辑的主题建模算法(FLSA 和 FLSA-V)。这篇文章将简要描述模糊主题模型和 FLSA-W 的基本原理，然后演示如何开始使用 FuzzyTM(如果你想开始训练一个模型，只需转到“开始使用 FuzzyTM”)。在以后的帖子中，我将更详细地解释各种算法是如何工作的，并使用 OCTIS 将它们与现有算法进行比较。</p><h1 id="187f" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">模糊主题模型</h1><p id="fd93" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">注意，虽然有 50 种深浅不同的主题建模算法，但它们都分别返回两个矩阵<strong class="kf ir"> P(W_i|T_k) </strong>和<strong class="kf ir"> P(T_k|D_j) </strong>、<strong class="kf ir"> </strong>给定主题的单词概率和给定文档的主题概率。</p><p id="7479" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2018 年，模糊潜在语义分析被提出[2]，并在一致性方面优于 LDA。FLSA 利用贝叶斯定理、奇异值分解(SVD)和矩阵乘法求<strong class="kf ir"> P(W_i|T_k) </strong>和<strong class="kf ir"> P(T_k|D_j) </strong>。</p><p id="6566" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于主题建模，我们从文本语料库开始。在 Python 中，这被存储为字符串列表的列表，其中每个列表代表一个文档，每个字符串是文档中的一个单词。</p><p id="a9fd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们从定义下列量开始:</p><p id="cf4f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">M —数据集中唯一单词的数量</p><p id="028f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">N —数据集中文档的数量</p><p id="835d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">C —主题的数量</p><p id="9539" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">S —奇异值分解维数</p><p id="e6db" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="me"> i </em> —单词索引，<em class="me"> i </em> ∈ {1，2，3，…，M}</p><p id="7ad1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="me"> j — </em>文档索引，<em class="me"> j </em> ∈ {1，2，3，…，N}</p><p id="7ba8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="me"> k </em> —主题索引，<em class="me"> k </em> ∈ {1，2，3，…，C}</p><p id="38cd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么，在 FLSA 获得<strong class="kf ir"> P(W_i|T_k) </strong>和<strong class="kf ir"> P(T_k|D_j) </strong>的步骤如下:</p><ol class=""><li id="1646" class="mf mg iq kf b kg kh kk kl ko mh ks mi kw mj la mk ml mm mn bi translated"><strong class="kf ir">获取本地术语权重</strong> ( <em class="me"> MxN </em> ) —指示单词<em class="me"> i </em>在文档<em class="me"> j. </em>中出现的频率的文档术语矩阵</li><li id="9ec0" class="mf mg iq kf b kg mo kk mp ko mq ks mr kw ms la mk ml mm mn bi translated"><strong class="kf ir">获取全局术语权重</strong> ( <em class="me"> MxN </em> ) —在该步骤中，一个文档中的单词的出现与其他文档中的单词的出现相关。</li><li id="36e2" class="mf mg iq kf b kg mo kk mp ko mq ks mr kw ms la mk ml mm mn bi translated">在全局术语权重 ( <em class="me"> NxS </em>)上从<strong class="kf ir"> SVD 获得<strong class="kf ir">U</strong>—SVD 用于降维，见<a class="ae kc" href="https://medium.com/towards-data-science/svd-8c2f72e264f" rel="noopener">本帖</a>对 SVD 的直观解释。</strong></li><li id="8249" class="mf mg iq kf b kg mo kk mp ko mq ks mr kw ms la mk ml mm mn bi translated">对<strong class="kf ir"> U^T </strong>使用模糊聚类得到<br/><strong class="kf ir">p(t|d)^t</strong>(<em class="me">mxc</em>)—最常用的方法是模糊 c 均值聚类，但 FuzzyTM 中有各种算法。</li><li id="0579" class="mf mg iq kf b kg mo kk mp ko mq ks mr kw ms la mk ml mm mn bi translated">使用基于贝叶斯定理的矩阵乘法，使用<strong class="kf ir"> P(T|D)^T </strong>和 P(D_j)得到<strong class="kf ir"> P(W_i|T_k) </strong>。</li></ol><p id="5877" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在 FLSA，SVD 的<strong class="kf ir"> U </strong>矩阵被用作聚类的输入，这意味着文档正在被聚类。由于主题模型经常被用于寻找对应于主题的单词，所以采用奇异值分解的<strong class="kf ir"> V^T </strong>进行聚类似乎更有意义，因为现在单词正在被聚类。在 FLSA-W(现在“W”有意义了，希望)中，单词而不是文档被聚集在一起。</p><h1 id="240e" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">FuzzyTM 入门</h1><p id="d16d" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">FuzzyTM 是模块化构建的，因此每个算法的步骤都是父类中不同的方法，每个算法都是调用父类中方法的子类。新手可以用最少的工作训练各种主题模型，而研究人员可以修改每个步骤并添加更多的功能。</p><p id="58fd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们开始使用 OCTIS 包中的数据集训练模型。首先，我们使用以下代码安装 FuzzyTM:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="6b42" class="nc lc iq my b gy nd ne l nf ng">pip install FuzzyTM</span></pre><p id="b57a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其次，我们导入数据集:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="b9a3" class="nc lc iq my b gy nd ne l nf ng">from octis.dataset.dataset import Dataset<br/>dataset = Dataset()<br/>dataset.fetch_dataset('DBLP')<br/>data = dataset._Dataset__corpus</span></pre><p id="c142" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看这个数据集是什么样子的:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="271b" class="nc lc iq my b gy nd ne l nf ng">print(data[0:5])<br/>&gt;&gt;&gt; [['fast', 'cut', 'protocol', 'agent', 'coordination'],<br/> ['retrieval', 'base', 'class', 'svm'],<br/> ['semantic', 'annotation', 'personal', 'video', 'content', 'image'],<br/> ['semantic', 'repository', 'modeling', 'image', 'database'],<br/> ['global', 'local', 'scheme', 'imbalanced', 'point', 'matching']]</span></pre><p id="d04e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们准备导入 FuzzyTM:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="d03d" class="nc lc iq my b gy nd ne l nf ng">from FuzzyTM import FLSA_W</span></pre><p id="0112" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将模型初始化如下(` num_words '的缺省值是 20，但为了清楚起见，这里我只给出了 10 个单词):</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="6412" class="nc lc iq my b gy nd ne l nf ng">flsaW = FLSA_W(input_file = data, num_topics=10, num_words=10)</span></pre><p id="fc6a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我们得到<strong class="kf ir"> P(W_i|T_k) </strong>和<strong class="kf ir"> P(T_k|D_j) </strong>如下:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="33de" class="nc lc iq my b gy nd ne l nf ng">pwgt, ptgd = flsaW.get_matrices()</span></pre><p id="2615" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们可以开始看主题了:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="0353" class="nc lc iq my b gy nd ne l nf ng">topics = flsaW.show_topics(representation='words')</span><span id="8b32" class="nc lc iq my b gy nh ne l nf ng">print(topics)<br/>&gt;&gt;&gt; [['machine', 'decision', 'set', 'evaluation', 'tree', 'performance', 'constraint', 'stream', 'process', 'pattern'], ['face', 'robust', 'tracking', 'error', 'code', 'filter', 'shape', 'detection', 'recognition', 'color'], ['generalization', 'neighbor', 'predict', 'sensitive', 'computation', 'topic', 'link', 'recursive', 'virtual', 'construction'], ['language', 'logic', 'data', 'web', 'mining', 'rule', 'processing', 'discovery', 'query', 'datum'], ['factorization', 'regularization', 'people', 'measurement', 'parametric', 'progressive', 'dimensionality', 'histogram', 'selective', 'correct'], ['active', 'spatial', 'optimal', 'view', 'level', 'modeling', 'combine', 'hierarchical', 'dimensional', 'space'], ['correspondence', 'calibration', 'compress', 'curve', 'geometry', 'track', 'background', 'appearance', 'deformable', 'light'], ['heuristic', 'computational', 'update','preference', 'qualitative', 'mechanism', 'engine', 'functional', 'join', 'relation'], ['graphic', 'configuration', 'hypothesis', 'walk', 'relaxation', 'family', 'composite', 'factor', 'string', 'pass'], ['theorem', 'independence', 'discourse', 'electronic', 'auction', 'composition', 'diagram', 'version', 'hard', 'create']]</span></pre><p id="48f1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从这个输出中我们可以识别出一些主题:第一个主题似乎是关于通用机器学习的，第二个主题是关于图像识别的，第四个主题是关于自然语言处理的。</p><p id="a26b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们来看看分数评估指标:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="c65f" class="nc lc iq my b gy nd ne l nf ng">#Get coherence value<br/>flsaW.get_coherence_value(input_file = data, topics = topics)<br/>&gt;&gt;&gt; 0.34180921613509696</span><span id="c1b6" class="nc lc iq my b gy nh ne l nf ng">#Get diversity score<br/>flsaW.get_diversity_score(topics = topics)<br/>&gt;&gt;&gt; 1.0</span><span id="263e" class="nc lc iq my b gy nh ne l nf ng">#Get interpretability score<br/>flsaW.get_interpretability_score(input_file = data, topics = topics)<br/>&gt;&gt;&gt; 0.34180921613509696</span></pre><p id="d6a1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于主题模型的输出由各种主题组成，其中每个主题都是单词的集合，因此主题模型的质量应该关注每个主题内的单词质量(主题内质量)和不同主题之间的差异(主题间质量)。连贯性分数捕捉每个主题内的单词相互支持的程度，多样性分数显示每个主题的多样性(主题之间是否有单词重叠)。然后，可解释性得分结合了这两个指标，并被计算为一致性和多样性之间的乘积。</p><p id="7514" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从结果可以看出，FLSA-W 具有完美的多样性。这并不奇怪，因为它明确地将单词聚集在一起。不过，与大多数现有算法相比，这是一个很大的改进。</p><p id="7bc0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然 0.3418 的一致性分数看起来相当低，但是比较实验结果将显示，在大多数设置中，FLSA-W 具有比其他算法更高的一致性分数。</p><h1 id="e7e1" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">结论</h1><p id="7a37" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">在这篇文章中，我简要解释了主题模型 FLSA 和 FLSA-W 是如何工作的。然后，我演示了只用两行代码就可以训练 FLSA-W，以及如何分析主题。除了训练主题模型之外，FuzzyTM 还包含一种方法，用于基于训练的主题模型获得新文档的主题嵌入。这对于下游任务(如文本分类)的文档嵌入非常有用。在以后的文章中，我将更详细地描述这些算法，并将 FLSA-W 与现有算法进行比较。更多详情请看我的 Github 页面:<a class="ae kc" href="https://github.com/ERijck/FuzzyTM" rel="noopener ugc nofollow" target="_blank">https://github.com/ERijck/FuzzyTM</a>。</p><p id="2458" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[1] Rijcken，e .，Scheepers，f .，Mosteiro，p .，Zervanou，k .，Spruit，m .，&amp; Kaymak，U. (2021 年 12 月)。模糊话题模型和 LDA 在可解释性方面的比较研究。在<em class="me"> 2021 IEEE 计算智能系列研讨会(SSCI) </em>。IEEE。</p><p id="ecfb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]a .卡拉米，a .甘戈帕迪亚，a .周，b .，&amp;哈拉齐，H. (2018)。健康和医学语料库中的模糊方法主题发现。<em class="me">国际模糊系统杂志</em>，<em class="me"> 20 </em> (4)，1334–1345。</p></div></div>    
</body>
</html>