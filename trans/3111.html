<html>
<head>
<title>How Does Back-Propagation Work in Neural Networks?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">反向传播在神经网络中是如何工作的？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-does-back-propagation-work-in-neural-networks-with-worked-example-bc59dfb97f48#2022-07-08">https://towardsdatascience.com/how-does-back-propagation-work-in-neural-networks-with-worked-example-bc59dfb97f48#2022-07-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3a84" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用一个例子演示背景如何在神经网络中工作</h2></div><p id="aca7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">神经网络在训练阶段通过参数(权重和偏差)的迭代调整来学习。开始时，参数由随机生成的权重初始化，偏差设置为零。随后，数据通过网络向前传递，以获得模型输出。最后，进行反向传播。模型训练过程通常需要前向传递、反向传播和参数更新的几次迭代。</p><p id="2feb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本文将关注反向传播如何在前向传递之后更新参数(我们已经在<a class="ae le" rel="noopener" target="_blank" href="/how-neural-networks-actually-work-python-implementation-part-2-simplified-80db0351db45">的前一篇文章</a>中介绍了前向传播)。我们将研究一个简单而详细的反向传播的例子。在我们继续之前，让我们看看我们将在这篇文章中使用的数据和架构。</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="eea1" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">数据和架构</h1><p id="a5f5" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">本文中使用的数据集包含三个特性，目标类只有两个值— <code class="fe mj mk ml mm b">1</code>表示通过，而<code class="fe mj mk ml mm b">0</code>表示失败。目标是将数据点分类到这两个类别中的任何一个，这是一种二元分类的情况。为了让这个例子容易理解，我们将在这篇文章中只使用一个训练例子。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi mn"><img src="../Images/990b400ba8c9e6f63dd0d3620c76f2c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rLUL1hmN8E53lqGuei-jyw.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated"><strong class="bd nd">图1: </strong>我们将使用的数据和神经网络架构。我们的训练示例突出显示了其对应的实际值1。这个3–4–1 NN是一个密集连接的网络——当前层中的每个节点都连接到前一层中除输入层以外的所有神经元。然而，我们已经消除了一些连接，使图形不那么杂乱。<a class="ae le" rel="noopener" target="_blank" href="/how-neural-networks-actually-work-python-implementation-part-2-simplified-80db0351db45">向前传递产生0.521的输出</a>(来源:作者)。</p></figure><p id="ab8d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">理解:正向传递允许信息沿一个方向流动—从输入层到输出层，而反向传播则相反—允许数据从输出层反向流动，同时更新参数(权重和偏差)。</p><p id="0aed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">定义:</strong>反向传播是一种监督学习方法，由神经网络用来更新参数，使网络的预测更加准确。参数优化过程是使用一种叫做<strong class="kk iu">梯度下降</strong>的优化算法实现的(这个概念在你阅读时会非常清楚)。</p><p id="19d9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正向传递产生目标(<code class="fe mj mk ml mm b">y</code>)的损失预测(<code class="fe mj mk ml mm b">yhat</code>)，该预测由成本函数(E)捕获，该成本函数定义为:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/7d0aefdb2030e2081fc9f192c0931638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*lek0XcQZ9o5zP2h1Fvk3mQ.png"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated"><strong class="bd nd">等式1: </strong>成本函数</p></figure><p id="02f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<code class="fe mj mk ml mm b">m</code>为训练样本数，<code class="fe mj mk ml mm b">L</code>为模型预测<code class="fe mj mk ml mm b">yhat</code>而非实际值<code class="fe mj mk ml mm b">y</code>时产生的误差/损失。目标是最小化成本<code class="fe mj mk ml mm b">E</code>。这是通过相对于(wrt)参数(权重和参数)对E求微分并在梯度的相反方向上调整参数来实现的(这就是为什么优化算法被称为<strong class="kk iu">梯度下降</strong>)。</p><p id="0ca5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本帖中，我们考虑在<code class="fe mj mk ml mm b">1</code>训练示例(<code class="fe mj mk ml mm b">m=1</code>)上的反向传播。考虑到这一点，<code class="fe mj mk ml mm b">E</code>减少到</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nf"><img src="../Images/309872765e940549e7d1836b8df23b2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*PUup-qL0FKkcsFysflqpKg.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated"><strong class="bd nd">等式2: </strong>一个训练示例的成本函数。</p></figure></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="1843" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">选择损失函数L</h1><p id="4095" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">损失函数<code class="fe mj mk ml mm b">L</code>是基于手头的任务定义的。对于分类问题，<a class="ae le" rel="noopener" target="_blank" href="/cross-entropy-loss-function-f38c4ec8643e">交叉熵(也称为对数损失)</a>和铰链损失是合适的损失函数，而均方误差(MSE)和平均绝对误差(MAE)是回归任务的合适损失函数。</p><p id="7433" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">二元交叉熵损失是一个适合我们二元分类任务的函数——数据有两类，<code class="fe mj mk ml mm b">0</code>或<code class="fe mj mk ml mm b">1</code>。二元交叉熵损失函数可以应用于图1 的<em class="ng">中的前向传递示例，如下所示</em></p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nh"><img src="../Images/122af02a066f6ac2821de6375644f45d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nQEY3IY3Vxc1y-qa0p439Q.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">等式3:应用于我们例子的二元交叉熵损失。</p></figure><p id="e7e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe mj mk ml mm b">t=1</code>是真值标签，<code class="fe mj mk ml mm b">yhat</code> =0.521是模型的输出，<code class="fe mj mk ml mm b">ln</code>是自然对数——以2为底的对数。</p><p id="9670" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以通过下面的链接了解更多关于交叉熵损失函数的信息。</p><div class="ni nj gp gr nk nl"><a rel="noopener follow" target="_blank" href="/cross-entropy-loss-function-f38c4ec8643e"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd iu gy z fp nq fr fs nr fu fw is bi translated">交叉熵损失函数</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">在大多数分类问题中用于优化机器学习模型的损失函数…</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">towardsdatascience.com</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz mx nl"/></div></div></a></div><p id="14d2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">既然我们现在理解了神经网络结构和我们将使用的成本函数，我们可以直接讨论反向传播的步骤。</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="2df4" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">数据和参数</h1><p id="276c" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">下表显示了<code class="fe mj mk ml mm b">3–4–1</code> NN所有层的数据。在<code class="fe mj mk ml mm b">3</code>-神经元输入端，显示的值来自我们提供给模型用于训练的数据。第二/隐藏层包含我们希望更新的权重(<code class="fe mj mk ml mm b">w</code>)和偏差(<code class="fe mj mk ml mm b">b</code>)以及在正向传递期间每个<code class="fe mj mk ml mm b">4</code>神经元的输出(<code class="fe mj mk ml mm b">f</code>)。输出包含参数(<code class="fe mj mk ml mm b">w</code>和<code class="fe mj mk ml mm b">b</code>)和模型的输出(<code class="fe mj mk ml mm b">yhat</code> ) —该值实际上是模型训练的每次迭代的模型预测。一次顺传后，<code class="fe mj mk ml mm b"> yhat=0.521</code>。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi oa"><img src="../Images/6079fdfdb5c3e474d111a28ef9a1c4bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1qyuVuE3_YL4f2Tji3FbWA.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated"><strong class="bd nd">图2 </strong>数据和参数初始化(来源:作者)</p></figure></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="e982" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">A.更新方程和损失函数</h1><p id="0ee0" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated"><strong class="kk iu">重要提示:</strong>回想一下上一节:<code class="fe mj mk ml mm b">E(θ)=L(y, yhat)</code>其中<code class="fe mj mk ml mm b">θ</code>是我们的参数——权重和偏差。也就是说，<code class="fe mj mk ml mm b">E</code>是<code class="fe mj mk ml mm b">y</code>和<code class="fe mj mk ml mm b">yhat</code>和<code class="fe mj mk ml mm b">yhat=g(wx+b)</code>的函数，= &gt; <code class="fe mj mk ml mm b">yhat</code>是<code class="fe mj mk ml mm b">w</code>和<code class="fe mj mk ml mm b">b</code>的函数。<code class="fe mj mk ml mm b">x</code>是数据变量，<code class="fe mj mk ml mm b">g</code>是激活函数。实际上，<code class="fe mj mk ml mm b">E</code>是函数<code class="fe mj mk ml mm b">w</code>和<code class="fe mj mk ml mm b">b</code>，因此可以对这些参数进行微分。</p><p id="019f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每一层的参数用以下公式更新</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/64a79ef200f5c3fc8e684076fa0c9399.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*Uk3aWfezY1s_q4saG2aVcw.png"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">等式4:更新等式</p></figure><p id="1074" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<code class="fe mj mk ml mm b">t</code>是学习步骤，<code class="fe mj mk ml mm b">ϵ</code>是学习速率——由用户设置的超参数。它决定了权重和偏差的更新速率。我们会用<code class="fe mj mk ml mm b">ϵ=0.5</code>(任意选择)。</p><p id="7c9b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据等式<code class="fe mj mk ml mm b">4</code>，更新量变为</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/d842a9316c5fbdcc5ad95076ad906ac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*ZLGRCLf8hVyMrXmN4-Vh7Q.png"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">公式5:更新金额</p></figure><p id="d11b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如前所述，由于我们正在处理二元分类，我们将使用二元交叉熵损失函数，定义为:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi od"><img src="../Images/e0a841969cf7d027fc7f66631724fb98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P4njQhmdIzUs7Ub8bdsj8g.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">等式6:二元交叉熵损失</p></figure><p id="a253" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将在所有层中使用Sigmoid激活</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/7b3ac14843546a68867f9962afae0c9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*V4ujqkSKYHwJ-F6Ht4nuTA.png"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">等式7: Sigmoid函数。</p></figure><p id="c9f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<code class="fe mj mk ml mm b">z=wx+b</code>是神经元的加权输入加上偏置。</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="d685" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">B.更新输出隐藏层的参数</h1><p id="2808" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">与正向传递不同，反向推进从输出层向后工作到层<code class="fe mj mk ml mm b">1</code>。我们需要针对所有层的参数计算导数/梯度。为此，我们必须理解<strong class="kk iu">微分链法则</strong>。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi of"><img src="../Images/78d88480603ae5588c4036787514b7ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BVP0TYyJlYOWfJ1kKhXBzg.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">微分链式法则</p></figure><p id="7de5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们以更新<code class="fe mj mk ml mm b">w²₁₁</code>和<code class="fe mj mk ml mm b">b²₁</code>为例。我们将遵循如下所示的路线。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi og"><img src="../Images/7e48718c3d808632ca1b5865f2e454b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T1KehvR7zxmrh7l_0k6Cuw.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated"><strong class="bd nd">图3: </strong>反向传播信息流(来源:作者)。</p></figure><h2 id="3185" class="oh ln it bd lo oi oj dn ls ok ol dp lw kr om on ly kv oo op ma kz oq or mc os bi translated">B1。计算重量的导数</h2><p id="0aab" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">根据微分的链式法则，我们有</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ot"><img src="../Images/9dd963c3b1278e2c20329b989414a7ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RCnwOg77FfprniULrZqJIw.png"/></div></div></figure><p id="f1e3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">记住:</strong>当计算上述关于w·₁₁的导数时，所有其他参数都被视为常数，即w·₁₂、w·₁₃、w·₁₄和b·₁.常数的导数是<code class="fe mj mk ml mm b">0</code>，这就是为什么在上面的导数中删除了一些值。</p><p id="0b6f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来是Sigmoid函数的导数(参见<a class="ae le" rel="noopener" target="_blank" href="/derivative-of-sigmoid-and-cross-entropy-functions-5169525e6705">本文</a></p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ou"><img src="../Images/34d6e842749dc8012084971dc63a249d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OAdAsIA8AJnlV6pcqe8l4w.png"/></div></div></figure><p id="4168" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，交叉熵损失函数的导数(<a class="ae le" rel="noopener" target="_blank" href="/derivative-of-sigmoid-and-cross-entropy-functions-5169525e6705">参考资料</a></p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ov"><img src="../Images/ba9edef1d63a1472e752a4b5082dec9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x_mVnYVQPuL99JD70YzaNw.png"/></div></div></figure><p id="a21c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出层上其他三个权重的导数如下(您可以确认这一点)</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ow"><img src="../Images/9b4b1aa427b66774814387c09c97d3ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EgjX3dioGZHqqVv2pG0LaQ.png"/></div></div></figure><h2 id="3383" class="oh ln it bd lo oi oj dn ls ok ol dp lw kr om on ly kv oo op ma kz oq or mc os bi translated">B2。计算偏差的导数</h2><p id="e46b" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">我们需要计算</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/9aa721c20036e1013d9dee29e2741882.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*JMlexdPyMhFDX5xcQoQxUQ.png"/></div></figure><p id="5ab9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从前面的章节中，我们已经计算了<code class="fe mj mk ml mm b">∂E</code>和<code class="fe mj mk ml mm b"> ∂yhat</code>，剩下的是</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi oy"><img src="../Images/b659b9cddfed1d57758aa5b32b59ad16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3OT4Hh5aeAOzQBuN8he1Vw.png"/></div></div></figure><p id="c467" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用与之前相同的论点，即除了b ₁之外的所有其他变量都被视为常数，因此当微分时它们减少<code class="fe mj mk ml mm b">0</code>。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi oz"><img src="../Images/c47ecd14254655e9980cd75f9787dcef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8-56RlHi0Fmo2P3kb5xLnw.png"/></div></div></figure><p id="5488" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">到目前为止，我们已经计算了关于输出-输入层的所有参数的梯度。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/a2799f1e67e0ba7fc5710a7823a77895.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*2Xsfd00re5-7Fr8-RErPSg.png"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated"><strong class="bd nd">图4: </strong>输出-输入层的梯度<strong class="bd nd"> </strong>(来源:作者)。</p></figure><p id="0337" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此时，我们已经准备好更新输出-输入层的所有权重和偏差。</p><h2 id="aeb4" class="oh ln it bd lo oi oj dn ls ok ol dp lw kr om on ly kv oo op ma kz oq or mc os bi translated">B3。更新输出隐藏层的参数</h2><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi pb"><img src="../Images/54c1174e5504cf58136d182f8b955313.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZL5B6U36Nm4dyAO_Eb5q_Q.png"/></div></div></figure><p id="2b58" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请用同样的方法计算其余的，并在下表中确认它们</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/7efbbcc04e74d57ca812b77ad5a5cf59.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*qMXuW4Dew_qBjvr7jmRe3g.png"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated"><strong class="bd nd">图5: </strong>隐藏输出层的更新参数<strong class="bd nd"> </strong>(来源:作者)</p></figure></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="fa62" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">C.更新隐藏输入层的参数</h1><p id="076d" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">和以前一样，我们需要E关于这些层上所有权重和偏差的导数。我们总共有<code class="fe mj mk ml mm b">4x3=12</code>个权重要更新，还有<code class="fe mj mk ml mm b">4</code>个偏差。例如，让我们研究西₄₃和₂.请参见下图中的路线。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi og"><img src="../Images/7375b769fbf32ac3b0e2b71ae27f6aae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r35Q3DzwhCe-q_1n6GN7ww.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated"><strong class="bd nd">图6: </strong>隐藏输入层的反向传播信息(来源:作者)。</p></figure><h2 id="4416" class="oh ln it bd lo oi oj dn ls ok ol dp lw kr om on ly kv oo op ma kz oq or mc os bi translated">C1。重量梯度</h2><p id="db3c" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">对于重量，我们需要计算导数(如果下面的等式令人生畏，请遵循图<code class="fe mj mk ml mm b">6</code>中的路线)</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/65b3ea016d6c91f4dcb4618f387559c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*wui89fk86xznNES2XJgEtg.png"/></div></figure></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><p id="fd2b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们讨论上述每个衍生工具时，请注意以下要点:</p><ul class=""><li id="9b0a" class="pe pf it kk b kl km ko kp kr pg kv ph kz pi ld pj pk pl pm bi translated"><strong class="kk iu">在模型输出</strong>(当找到<code class="fe mj mk ml mm b">E</code>相对于<code class="fe mj mk ml mm b">yhat</code>的导数时)，我们实际上是在对损失函数进行微分。</li><li id="c07d" class="pe pf it kk b kl pn ko po kr pp kv pq kz pr ld pj pk pl pm bi translated"><strong class="kk iu">在层输出</strong> ( <code class="fe mj mk ml mm b">f</code>)(这里我们对wrt <code class="fe mj mk ml mm b">z</code>进行微分)，我们找到激活函数的导数。</li><li id="8a03" class="pe pf it kk b kl pn ko po kr pp kv pq kz pr ld pj pk pl pm bi translated">在上述两种情况下，对给定神经元的权重或偏差进行微分会产生相同的结果。</li><li id="dc25" class="pe pf it kk b kl pn ko po kr pp kv pq kz pr ld pj pk pl pm bi translated"><strong class="kk iu">加权输入</strong> ( <code class="fe mj mk ml mm b">z</code>)与我们希望更新的参数(<code class="fe mj mk ml mm b">w</code>或<code class="fe mj mk ml mm b">b</code>)不同。在这种情况下，<em class="ng">除了感兴趣的参数</em>外，所有参数保持不变。</li></ul></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><p id="5d3f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">进行与第<code class="fe mj mk ml mm b">B</code>节相同的过程，我们得到:</p><ul class=""><li id="5e97" class="pe pf it kk b kl km ko kp kr pg kv ph kz pi ld pj pk pl pm bi translated">层的加权输入<code class="fe mj mk ml mm b">1</code></li></ul><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ps"><img src="../Images/7a5fedd912f2dcc0383314f9b9959c36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jxnbCeogJ5jDUERcL4w7AQ.png"/></div></div></figure><ul class=""><li id="a78b" class="pe pf it kk b kl km ko kp kr pg kv ph kz pi ld pj pk pl pm bi translated">应用于第一层的Sigmoid激活函数的导数</li></ul><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/276835c68163c50d6ab589edfb3ab814.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*zgPE1RmaXmumktAOIbLeng.png"/></div></figure><ul class=""><li id="8810" class="pe pf it kk b kl km ko kp kr pg kv ph kz pi ld pj pk pl pm bi translated">输出层的加权输入。<code class="fe mj mk ml mm b">f</code>-值是隐藏层的输出。</li></ul><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi pu"><img src="../Images/98fa03025c41045630c9e2eb7b7ff3bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HeZOFLzcdEbL7z7Jxb4oiw.png"/></div></div></figure><ul class=""><li id="3490" class="pe pf it kk b kl km ko kp kr pg kv ph kz pi ld pj pk pl pm bi translated">应用于最后一层输出的激活函数。</li></ul><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi pv"><img src="../Images/b75e988fc982c05179519163a3a74d75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3E8T-k5LDP7R1BR8_-7qtg.png"/></div></div></figure><ul class=""><li id="109d" class="pe pf it kk b kl km ko kp kr pg kv ph kz pi ld pj pk pl pm bi translated">二元交叉熵损失函数的导数wrt到<code class="fe mj mk ml mm b">yhat</code>。</li></ul><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi pw"><img src="../Images/c3f3dbcad664d5e79e5646631757045e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gN7lA6--q43GCvtRqU1h2g.png"/></div></div></figure><p id="0e82" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们可以把所有这些放在一起</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi px"><img src="../Images/9cf10ce6c58eb9c4909f857c46489559.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dt0HykPaOlWxB1W1YQpelw.png"/></div></div></figure><h2 id="6105" class="oh ln it bd lo oi oj dn ls ok ol dp lw kr om on ly kv oo op ma kz oq or mc os bi translated">C2。偏差梯度</h2><p id="eea8" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">使用和以前一样的概念，检查一下，对于₂，我们有</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi py"><img src="../Images/a2628c9ff91cd2a6c02ebaf573c5da6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xlLf09kfld9WEhC8zWtnzw.png"/></div></div></figure><p id="7e59" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">隐藏输入的所有梯度值如下表所示</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/27eea3edc4a6fd31323847cc89282df1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*I-IgdZauo70qbD6KPrZvlw.png"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated"><strong class="bd nd">图7: </strong>隐藏输入层的渐变(来源:作者)。</p></figure><p id="eb5f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此时，我们已经准备好计算隐藏输入端的更新参数。</p><h2 id="ca79" class="oh ln it bd lo oi oj dn ls ok ol dp lw kr om on ly kv oo op ma kz oq or mc os bi translated">C3。更新隐藏输入的参数</h2><p id="8521" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">让我们回到更新方程，继续更新₁₃和₃</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/520ecbc4b3afb82e854d31fecd81c307.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*NMf2UujOoGNxlMPGfMb2GQ.png"/></div></figure><p id="aae7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么，我们需要更新多少参数呢？</p><p id="6283" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们在隐藏输入层有<code class="fe mj mk ml mm b">4x3=12</code>权重和<code class="fe mj mk ml mm b"> 4x1=4</code>偏差，在输出隐藏层有<code class="fe mj mk ml mm b">4x1=4 </code>权重和<code class="fe mj mk ml mm b">1</code>偏差。总共有21个参数。它们被称为<strong class="kk iu">可训练参数</strong>。</p><p id="ee46" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">隐藏输入层的所有更新参数如下所示</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/0dba6902b8edbbdba60aab68aa1caf0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*5836Cny0_UmfJnsb_Xx22A.png"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated"><strong class="bd nd">图8: </strong>隐藏输入层的更新参数(来源:作者)。</p></figure><p id="262b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在已经使用误差反向传播获得了<strong class="kk iu">图8 </strong>和<strong class="kk iu">图5 </strong>中所有层的更新参数。用这些更新的参数运行一个正向传递产生一个模型预测，从<code class="fe mj mk ml mm b">0.521</code>上升到<code class="fe mj mk ml mm b">0.648</code>的<code class="fe mj mk ml mm b">yhat</code>。这意味着模型正在学习——经过两次迭代训练后，接近<code class="fe mj mk ml mm b">1</code>的真实值。其他迭代产生<code class="fe mj mk ml mm b">0.758</code>、<code class="fe mj mk ml mm b">0.836</code>、<code class="fe mj mk ml mm b">0.881</code>、<code class="fe mj mk ml mm b"> 0.908</code>、<code class="fe mj mk ml mm b">0.925</code>、…(在下一篇文章中，我们将为许多训练示例和迭代实现反向传播和正向传递，您将会看到这一点)。</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="4854" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">定义</h1><ol class=""><li id="fc90" class="pe pf it kk b kl me ko mf kr qc kv qd kz qe ld qf pk pl pm bi translated"><strong class="kk iu">历元</strong> —一个历元是指整个数据集通过网络一次。这包括正向传递和反向传播的一个实例。</li><li id="f9ef" class="pe pf it kk b kl pn ko po kr pp kv pq kz pr ld qf pk pl pm bi translated"><strong class="kk iu"> Bath size </strong>是同时通过网络的训练样本数。在我们的例子中，我们有一个训练的例子。在我们有一个大数据集的情况下，数据可以通过网络批量传递。</li><li id="a116" class="pe pf it kk b kl pn ko po kr pp kv pq kz pr ld qf pk pl pm bi translated"><strong class="kk iu">迭代次数</strong> —使用设定为批量的训练样本，一次迭代等于一次通过。一次传递是正向传递和反向传播。</li></ol><p id="4c05" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">例:<br/> </strong>如果我们有<strong class="kk iu"> 2000个训练例</strong>，设置<strong class="kk iu">批量为20个</strong>，那么i <em class="ng"> t需要100次迭代才能完成1个历元</em>。</p><h1 id="e3cb" class="lm ln it bd lo lp qg lr ls lt qh lv lw jz qi ka ly kc qj kd ma kf qk kg mc md bi translated">结论</h1><p id="e648" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">在本文中，我们通过一个例子讨论了反向传播。我们已经看到如何使用微分链规则来获得不同方程的梯度——损失函数、激活函数、加权方程和层输出方程。我们还讨论了如何使用损失函数的导数来更新每一层的参数。在下一篇文章(附后)中，我们用Python实现了这里学到的概念。</p><div class="ni nj gp gr nk nl"><a rel="noopener follow" target="_blank" href="/how-back-propagation-works-a-python-implementation-21004d3b47c6"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd iu gy z fp nq fr fs nr fu fw is bi translated">反向传播如何工作——Python实现？</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">在Python中实现反向传播</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">towardsdatascience.com</p></div></div><div class="nu l"><div class="ql l nw nx ny nu nz mx nl"/></div></div></a></div></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><p id="034c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请以每月5美元的价格注册成为medium会员，以便能够阅读我和其他作者在Medium上的所有文章。</p><p id="e1c1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你也可以<a class="ae le" href="https://medium.com/subscribe/@kiprono_65591" rel="noopener">订阅，当我发表文章时，你可以把我的文章发到你的邮箱里</a>。</p><p id="1b74" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢您的阅读，下次再见！！！</p><h1 id="f3ee" class="lm ln it bd lo lp qg lr ls lt qh lv lw jz qi ka ly kc qj kd ma kf qk kg mc md bi translated">你可能会发现这些文章很重要</h1><ul class=""><li id="2c19" class="pe pf it kk b kl me ko mf kr qc kv qd kz qe ld pj pk pl pm bi translated"><a class="ae le" rel="noopener" target="_blank" href="/how-neural-networks-actually-work-python-implementation-part-2-simplified-80db0351db45">在Python中实现前向传播</a></li><li id="be7c" class="pe pf it kk b kl pn ko po kr pp kv pq kz pr ld pj pk pl pm bi translated"><a class="ae le" rel="noopener" target="_blank" href="/cross-entropy-loss-function-f38c4ec8643e">交叉熵损失函数</a></li></ul></div></div>    
</body>
</html>