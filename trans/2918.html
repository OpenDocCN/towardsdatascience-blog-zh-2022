<html>
<head>
<title>Machine Translation Evaluation with Cometinho</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Cometinho进行机器翻译评估</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-translation-evaluation-with-cometinho-c89880731409#2022-06-24">https://towardsdatascience.com/machine-translation-evaluation-with-cometinho-c89880731409#2022-06-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9f90" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">着眼于性能，减少模型大小、节省计算时间和金钱的实用建议</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/32ff57571da92d6132177e5a536db233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7lyvBAv9nQAOQZWb"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">什洛莫·沙莱夫在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="2aa6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">欧洲机器翻译协会(EAMT)会议是机器翻译研究人员、用户和翻译人员聚集在一起讨论行业最新进展的场所。去那里看看欧洲大陆在机器翻译开发和采用方面的进展真的很有意思。在这篇文章中，我想分享今年最佳论文奖的一些想法。它的标题是“搜索COMETINHO:可能的小度量”，来自葡萄牙里斯本Unbabel公司的研究实验室，该公司提供使用机器翻译和人工翻译的翻译服务。你可以在ACL选集里找到这篇论文的在线版本。</p><p id="9f94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文的目的是展示如何在实际场景中使用文献中的几种技术来加速大型模型的执行并节省计算量，同时降低翻译质量的损失。这篇文章的重点是简化机器翻译(MT)自动评估的模型Comet，但是其中描述的许多点都很通用，足以让任何处理大型和缓慢模型的人感兴趣，因为金钱或时间的原因需要节省计算。</p><h1 id="dc17" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">彗星是什么？</h1><p id="da7d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">最初在这篇<a class="ae ky" href="https://aclanthology.org/2020.emnlp-main.213/" rel="noopener ugc nofollow" target="_blank">论文</a>中描述的Comet是基于<a class="ae ky" href="https://ai.facebook.com/blog/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision/" rel="noopener ugc nofollow" target="_blank"> XML-R </a>的机器翻译评估模型。大型语言模型用于分别对源、假设和参考句子进行编码，以便将它们中的每一个简化为单个向量。然后，这些向量被组合以在输出中产生代表假设质量分数的单个实数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/2b264f748860a2b9564d734355bfa46d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ReB7XVrMTTcXpHbgjAmXMw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">彗星拓扑，有一种比萨斜塔的感觉。图片作者。</p></figure><div class="mt mu gp gr mv mw"><a rel="noopener follow" target="_blank" href="/xlm-cross-lingual-language-model-33c1fd1adf82"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">XLM:跨语言语言模型</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">了解基于变压器的自监督架构</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="ng l nh ni nj nf nk ks mw"/></div></div></a></div><p id="c3fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在WMT的<a class="ae ky" href="https://www.statmt.org/wmt21/pdf/2021.wmt-1.73.pdf" rel="noopener ugc nofollow" target="_blank"> metrics共享任务</a>中，Comet因与人类判断的相关性而排名靠前。然而，它是一个具有500M参数的相当大的模型，并且运行缓慢。当我们需要经常运行它时，这尤其是一个问题，例如以最小的风险<a class="ae ky" href="https://aclanthology.org/N04-1022.pdf" rel="noopener ugc nofollow" target="_blank">解码</a>或<a class="ae ky" href="https://aclanthology.org/P16-1159/" rel="noopener ugc nofollow" target="_blank">训练</a>。</p><h1 id="80fa" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">代码加速</h1><p id="bd17" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">Cometinho论文中获得的第一对加速是关于纯代码修改的。它们是，长度排序和缓存。</p><p id="5bb0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">深度学习模型以小批量运行，以更好地利用GPU并行化能力。对于文本和其他可变长度的输入，小批量将具有其中最长序列的长度。其他序列用零填充，为填充位置执行的所有计算基本上都被浪费了。一种有效且众所周知的减少计算量的方法是在开始解码过程之前，根据句子的长度对句子进行排序。长度排序总是能提高速度，而且实现起来又快又容易。因此，当以批处理模式执行推理时，应该<strong class="lb iu">总是</strong>使用，并且所有最著名的深度学习工具都实现了它。</p><p id="f760" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Comet编码其三元组的方式使缓存成为可能:源、假设和引用被编码成三个独立的向量。这样，就有可能存储代表一个句子的向量，并在它再次出现时检索它。你是问多久发生一次吗？假设您正在使用Comet来比较两个不同的MT模型。两个模型通过完全相同的测试集运行，因此所有的源句子和参考句子至少出现<strong class="lb iu">两次。</strong>另一个例子是，当Comet用于最小贝叶斯风险解码时，为相同的源句子计算许多假设，并且再次节省了大量资源。这个想法是，如果你要为一个速度很重要的任务设计一个深度学习模型，在可能的情况下，尝试以一种可以利用缓存来节省重要资源的方式来设计它。</p><p id="484b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">长度排序和缓存的结合在对一个系统评分时带来了高达39%的速度提升，在对8个系统评分时带来了高达65%的速度提升，因此缓存更加有效。</p><h1 id="e573" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">层修剪</h1><p id="c051" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">接下来是实际缩小模型的部分，本文用两种方法进行了实验。第一种是从模型中物理移除整个层，第二种是减少一些子层的大小。</p><p id="9696" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在第一种情况下，一个层被认为是一个完整的变换器层，包括自关注、前馈、剩余连接和层归一化。</p><p id="158b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文的情况下，很容易从编码器模型的顶部删除层，因为池层不直接使用最后一个编码器层的输出。相反，他们使用相同的方法来生成带有<a class="ae ky" href="https://arxiv.org/abs/1802.05365" rel="noopener ugc nofollow" target="_blank"> Elmo </a>的嵌入向量，并对各层的输出进行加权平均。权重都是正的，并归一化为1.0，在训练期间学习，之后保持不变。对权重的快速分析表明，最顶层的权重最低，因此这些层的贡献最少*。然后作者决定只移除它们，这在推断过程中不会造成伤害，因为编码器输出总是许多层之间的平均值。结果显示，删除最上面的4层时，性能<strong class="lb iu">略有提高</strong>，删除5层时，性能没有下降。考虑到该模型由25层组成，这是20%的免费参数缩减(排除仍然代表参数计数的大部分但不影响推断那么多的嵌入权重)。</p><p id="e686" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二种方法是从每个自我关注层中去掉两个头(总共16个头)。此外，它们还将前馈大小从4096减小到3072(原始大小的75%)。由此产生的模型称为Prune-Comet，其结果与Comet相当，但推理时间加快了37%。</p><p id="fc5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用<a class="ae ky" href="https://textpruner.readthedocs.io/en/ latest" rel="noopener ugc nofollow" target="_blank"> TextPruner </a>执行修剪，这是一个开源工具，为许多预训练的变压器实现层修剪。如果这是你的任务，就查一下。</p><div class="mt mu gp gr mv mw"><a rel="noopener follow" target="_blank" href="/neural-network-pruning-101-af816aaea61"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">神经网络修剪101</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">你需要知道的是不要迷路</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="nl l nh ni nj nf nk ks mw"/></div></div></a></div><p id="782a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">*权重成比例地降低向量的大小，但这只是故事的一部分。Kobayashi和他的同事的这篇论文表明，矢量的原始大小也必须被考虑。这也很直观，因为外部权重也可以重新平衡不平衡的向量。</p><h1 id="6630" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">知识蒸馏</h1><p id="0e2c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">虽然层修剪有助于以较小的退化减少参数计数，但是模型仍然太大而不能有效地使用。事实上，它的大尺寸极大地影响了计算能力和时间，特别是对于需要重复MT评估计算的应用，例如最小贝叶斯风险解码。为了实现显著的参数缩减，需要<a class="ae ky" href="https://arxiv.org/abs/1503.02531" rel="noopener ugc nofollow" target="_blank">知识提炼</a>。</p><div class="mt mu gp gr mv mw"><a rel="noopener follow" target="_blank" href="/knowledge-distillation-simplified-dd4973dbc764"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">知识蒸馏:简体</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">窥探师生网络的世界</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="nm l nh ni nj nf nk ks mw"/></div></div></a></div><p id="3276" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过知识提炼，使用原始模型的输出作为学习目标来训练一个新的更小的模型。对于这个特定的任务，作者使用OPUS语料库的15个语言对，用两个不同质量的机器翻译模型翻译其2500万个句子对，并使用5个Comet模型的集合来评估翻译。由于这些是没有经过人工评估的机器翻译训练数据，所以没有用来训练Comet。</p><p id="5011" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">学生模型保持了与教师模型相同的深度学习架构，但它要小得多:12层，12个注意头，前馈大小为1576，嵌入向量大小为384。它的参数计数是119M参数，而不是原来的582M。</p><p id="3b59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最终结果显示，模型大小减少约80%，质量下降10%，速度提高53%。确实需要新的训练，但是在实际的应用场景中，训练只做一次，而推理要做上千次甚至更多。</p><h1 id="d5f1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="8b3e" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">Cometinho的论文具有实际意义，因为它展示了如何应用模型简化技术，这些技术可以很容易地转移到不同的应用领域。</p><p id="7fbe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你应该带什么回家？首先，有许多方法可以在性能损失很小甚至没有损失的情况下节省计算，并且它们可以在不同的域之间重用。尽可能使用它们。</p><p id="226a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二，从某种意义上来说，模型应该被设计成可缩减的。在本文的示例中，Elmo类层平均值允许层修剪。</p><p id="a4a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第三，顺便说一下，如果他们也显示从零开始训练基线模型到缩小尺寸的结果，而不是使用这里提出的技术，这篇论文会更完整。证明减少现有模型比训练更小的模型更好总是更好的。</p><p id="ca68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你对这个话题感兴趣，你的下一步可以是研究量化作为进一步的技术，以减少模型的大小和提高推理时间。</p><div class="mt mu gp gr mv mw"><a rel="noopener follow" target="_blank" href="/pick-your-deep-learning-tool-d01fcfb86845"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">挑选你的深度学习工具</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">为什么您的工具可以依赖于您组织的团队结构</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="nn l nh ni nj nf nk ks mw"/></div></div></a></div><div class="mt mu gp gr mv mw"><a rel="noopener follow" target="_blank" href="/massive-pretraining-for-bilingual-machine-translation-3e26bfd85432"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">双语机器翻译的大量预训练</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">通过mBART，一个编码器-解码器语言模型，打开了多语言有趣的视角的导游…</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="no l nh ni nj nf nk ks mw"/></div></div></a></div><div class="mt mu gp gr mv mw"><a rel="noopener follow" target="_blank" href="/dynamically-add-arguments-to-argparse-python-patterns-a439121abc39"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">向Argparse | Python模式动态添加参数</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">如何使用argparse.ArgumentParser根据用户输入指定不同的参数。</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="np l nh ni nj nf nk ks mw"/></div></div></a></div></div></div>    
</body>
</html>