<html>
<head>
<title>How to Build a Neural Network from Zero</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何从零开始构建神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-neural-network-from-scratch-8f03c5c50adc#2022-02-02">https://towardsdatascience.com/building-a-neural-network-from-scratch-8f03c5c50adc#2022-02-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d626" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">没有框架，只有Python</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b36491082e6a81de8bd52680a3d6c12b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PcQSGHKjP0s62VDq"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">布雷特·乔丹在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><blockquote class="kw kx ky"><p id="b230" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">“我不能创造的东西，我不理解”(理查德·费曼)</p></blockquote><p id="dfa0" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">这句话是在<a class="ae kv" href="https://en.wikipedia.org/wiki/Richard_Feynman" rel="noopener ugc nofollow" target="_blank">理查德·费曼</a>去世时的<a class="ae kv" href="https://i.imgur.com/2yF4l.jpg" rel="noopener ugc nofollow" target="_blank">黑板</a>上发现的。我一直认为引用费曼的话有些老套。然而，当一句引语完美地表达了你的感受时，诉诸陈词滥调可能是合理的。</p><p id="c678" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">让我解释一下:我知道有几十篇关于本文主题的优秀文章、教程和视频。我们需要另一个吗？简单说说为什么我认为我的观点与众不同。本人51岁，无论是编程，微积分，线性代数，还是工科背景都是<em class="lb">零</em>。大约12个月前，我开始自学编程和人工神经网络。在大量关于深度学习的教程和课程之后，其中大多数都利用了现有的框架，如PyTorch、sci-kit learn和TensorFlow，我仍然觉得我对某些概念的理解有些不对劲。当我对一件事理解不透彻的时候，我总是做同样的事情:从头开始构建。因此，我决定不借助任何框架来实现一个简单的人工神经网络(ANN或NN)。这篇文章说明了我的尝试的结果。也许，有些人会发现这是徒劳的。希望其他人会发现它很有帮助——甚至可能学到一些东西。</p><p id="a956" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">(请注意，这不是对神经网络的介绍，我也不会解释支撑算法的数学。我假设我的读者熟悉深度神经网络背后的基本概念，例如反向传播。如果你不是，你可以做得比在YouTube上看<a class="ae kv" href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi" rel="noopener ugc nofollow" target="_blank"> 3blue1brown的系列更糟</a>。</p><p id="dc23" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">最后，请注意，我欢迎批评和建议。如果你发现了一个错误，请随时<a class="ae kv" href="http://sassoli.medium.com" rel="noopener">联系</a>！</p><p id="5a16" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">有兴趣的可以在<a class="ae kv" href="https://github.com/bsassoli/NN_from_zero" rel="noopener ugc nofollow" target="_blank">这个GitHub repo </a>里找代码。)</p><h1 id="d40f" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">入门指南</h1><p id="a76b" class="pw-post-body-paragraph kz la iq lc b ld mr jr lf lg ms ju li lw mt ll lm lx mu lp lq ly mv lt lu lv ij bi translated">首先，我们导入一些库。由于我们不会使用任何深度学习或机器学习框架，所以导入的数量有限。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">导入库</p></figure><p id="f2cf" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">我决定使用<a class="ae kv" href="https://www.openml.org/d/554" rel="noopener ugc nofollow" target="_blank"> MNIST_784数据集</a>来训练和测试这个网络。如果你正在阅读这篇文章，你可能知道，<a class="ae kv" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank"> MNIST </a>被认为是深度学习的“你好，世界”。这是一个由70.000幅手写数字灰度图像组成的数据集；用作者的话说:</p><blockquote class="kw kx ky"><p id="733a" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">“对于那些希望在真实世界数据上尝试学习技术和模式识别方法，同时在预处理和格式化方面花费最少精力的人来说，这是一个很好的数据库。”</p></blockquote><p id="7078" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">我们可以下载MNIST作为一个<code class="fe my mz na nb b">python</code>字典(使用例如<code class="fe my mz na nb b">scikit-learn</code>)，如下所示(我们将数据集本身存储在<code class="fe my mz na nb b">data</code>，将标签存储在<code class="fe my mz na nb b">labels</code>:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">装载MNIST</p></figure></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><p id="a95c" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">让我们来探索数据集。我们随机选取一个数据点来了解图像及其形状:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">探索MNIST数据点</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="ab gu cl nj"><img src="../Images/1e74a7945ae510db211babc12fb1b315.png" data-original-src="https://miro.medium.com/v2/format:webp/1*MBxnGRHAP0biKdB9ox0QIQ.png"/></div></figure><p id="ed39" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">所以我们知道我们的图像是一个形状为784的向量:一个扁平的28 x 28的矩阵。如果我们将数据集分成具有例如60.000个数据点的训练集和具有10.000个数据点的测试集，我们将得到如下结果:<em class="lb">输入层大小= m*n，</em>其中<em class="lb"> m </em>是样本数，<em class="lb"> n </em>是特征数。</p><p id="c96b" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">因此，网络的输入图层大小为(60.000 x 784)。假设我们将训练数据集表示为以行表示要素，以列表示示例，那么我们将以类似如下的内容结束:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/e6e460853ccdafcbb4872a10924b2c4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G9EmC698E1EoeqMk80CPUQ.png"/></div></div></figure><p id="1613" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">让我们首先考虑一个简单的全连接神经网络，只有一个隐藏层。假设隐藏层有512个节点。第一个问题是:我们如何控制从输入层到隐藏层的映射？换句话说，<strong class="lc ir">权重矩阵</strong>的大小应该是多少？输入层中的每个节点都将连接到隐藏层中的每个输入。因此，我们将需要784 x 512 = 401，408个权重或可训练参数。并且由于到隐藏层的每个连接将向每个加权和添加一个偏差，我们将需要512个偏差。作为<strong class="lc ir">一般规则</strong>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/3849ab00852add3e008d1614e00b52b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YtsoNpBq2FHwT7jc3qABiA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">权重矩阵的形状规则</p></figure><p id="2cd5" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">(获得维度权限总是让我抓狂:<a class="ae kv" href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/Rz47X/getting-your-matrix-dimensions-right" rel="noopener ugc nofollow" target="_blank">这个</a>，吴恩达提供的，是我迷路时用的。)最后，这是我们得到的具有由4个神经元组成的单一深层的NN:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图层的形状</p></figure><p id="bfdd" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">(但是记住:我们需要给隐藏层的每个节点加上<em class="lb">偏差</em>。)</p><h1 id="5be5" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">搭建舞台</h1><p id="032a" class="pw-post-body-paragraph kz la iq lc b ld mr jr lf lg ms ju li lw mt ll lm lx mu lp lq ly mv lt lu lv ij bi translated">先来定义一些常用的<a class="ae kv" href="https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ir">激活函数</strong> </a>:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">激活</p></figure><p id="7378" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">在本文中，我将只使用<a class="ae kv" href="https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ir">整流线性单元(ReLU) </strong> </a>函数，但是NN应该足够灵活以适应其他选择。当我们实现神经网络时，我们会记住这一点。</p><p id="9f52" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">对于<strong class="lc ir">输出层，</strong>我们将使用<a class="ae kv" href="https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d" rel="noopener"> <strong class="lc ir"> softmax </strong> </a> <strong class="lc ir">，</strong>鉴于这是一个多类分类问题。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Softmax</p></figure><p id="b283" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">在训练我们的网络之前，我们需要一些预处理函数:</p><ol class=""><li id="66fb" class="nm nn iq lc b ld le lg lh lw no lx np ly nq lv nr ns nt nu bi translated"><em class="lb">归一化</em>功能将输入缩放至[0，1]范围，并且</li><li id="bbfe" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv nr ns nt nu bi translated">一个<em class="lb"> one-hot-encode </em>函数，它将把标签数组从一个<em class="lb"> n </em>大小的向量(其中<em class="lb"> n </em>是样本数)变成一个<em class="lb"> n x m </em>数组(其中<em class="lb"> m </em>是可能的输出数)。</li></ol><p id="9d95" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">缩放特性有多种方式:在我们的例子中，我们将使用<em class="lb">最小-最大缩放</em>(我们也可以将每个特性除以一个特性可能取值的数量——在我们的例子中是255):</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">缩放比例</p></figure><p id="cbe3" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">为了对标签执行一次热编码，我们可以创建一个单位矩阵(即对角线上由1和其他地方的0组成的矩阵)，其形状与我们的标签向量相同，然后用标签向量本身对其进行索引，如下所示:</p><pre class="kg kh ki kj gt oa nb ob oc aw od bi"><span id="95b1" class="oe ma iq nb b gy of og l oh oi">label                            encoded<br/>-------  -------------------------------<br/>      5  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]<br/>      0  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]<br/>      4  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]<br/>      1  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]<br/>      9  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]<br/>      2  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]<br/>      1  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]<br/>      3  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]<br/>      1  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]<br/>      4  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]</span></pre><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">标签的一键编码</p></figure><p id="5a1d" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">因为原始数据可以以不同的方式组织，所以我决定，作为一种设计选择，将适当的整形和一次性编码留给用户:也就是说，将由他们来预处理训练数据集和测试数据集，以便将特性作为行传递，将示例作为列传递，并将标签传递给<code class="fe my mz na nb b">one_hot_encode</code>函数。相反，缩放将由网络本身来执行。</p><p id="a2f9" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">最后，我们将需要激活函数的<strong class="lc ir">导数</strong>来执行<strong class="lc ir">梯度下降</strong>:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">派生物</p></figure><h1 id="d57b" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">实现神经网络</h1><p id="6d47" class="pw-post-body-paragraph kz la iq lc b ld mr jr lf lg ms ju li lw mt ll lm lx mu lp lq ly mv lt lu lv ij bi translated">我们现在已经拥有了深度神经网络所需的一切，我们将把它实现为一个<code class="fe my mz na nb b">class</code>。所以我们需要思考:</p><ul class=""><li id="3852" class="nm nn iq lc b ld le lg lh lw no lx np ly nq lv oj ns nt nu bi translated">一个<code class="fe my mz na nb b"><strong class="lc ir">init</strong></code>的方法；</li><li id="d17c" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv oj ns nt nu bi translated">一种<strong class="lc ir">正向传播</strong>方法；</li><li id="fce6" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv oj ns nt nu bi translated">如何实现<strong class="lc ir">反向传播</strong>；</li><li id="cd3f" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv oj ns nt nu bi translated">如何<strong class="lc ir">训练</strong>网络；</li><li id="c0bd" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv oj ns nt nu bi translated">如何计算<strong class="lc ir">成本函数</strong>；</li><li id="f4b8" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv oj ns nt nu bi translated">方法进行<strong class="lc ir">预测，</strong>，最后:</li><li id="589c" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv oj ns nt nu bi translated">一种<strong class="lc ir">实现准确性等指标</strong>的方法。</li></ul><p id="b9c9" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">我的思考过程大致如下。我们可以将以下参数传递给构造函数:</p><ul class=""><li id="233b" class="nm nn iq lc b ld le lg lh lw no lx np ly nq lv oj ns nt nu bi translated">带有相应标签的训练集</li><li id="db78" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv oj ns nt nu bi translated">带有<em class="lb">标签的测试装置</em></li><li id="7e47" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv oj ns nt nu bi translated">激活功能选项(字符串形式)</li><li id="7d26" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv oj ns nt nu bi translated">我们需要预测的班级数量</li><li id="af8b" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv oj ns nt nu bi translated">架构——我们用列表来表示，其中列表的每个元素对应于一个<em class="lb">深层</em>,并指示该层中神经元的<em class="lb">数量。</em></li></ul><p id="fcfc" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">然后，我们使用字典来存储这些层，并用字符串来命名它们，以便于跟踪(这也是我从<a class="ae kv" href="https://www.coursera.org/learn/machine-learning?utm_source=gg&amp;utm_medium=sem&amp;utm_campaign=07-StanfordML-ROW&amp;utm_content=B2C&amp;campaignid=2070742271&amp;adgroupid=80109820241&amp;device=c&amp;keyword=machine%20learning%20mooc&amp;matchtype=b&amp;network=g&amp;devicemodel=&amp;adpostion=&amp;creativeid=516962315003&amp;hide_mobile_promo&amp;gclid=Cj0KCQiA_8OPBhDtARIsAKQu0gaXbQX1Gl2TcnOc5pDn7BJvioZ8XIH1m3teUKxKeJbZqy6XBKa8WHoaAlTpEALw_wcB" rel="noopener ugc nofollow" target="_blank">吴恩达的机器学习课程</a>中学到的技巧)。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">神经网络__init__方法。</p></figure><p id="e091" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">注意一堆<code class="fe my mz na nb b">assert</code>语句。我再强调这一点也不为过:如果你不想花时间去调试地狱，考虑一下如何测试你的网络，并在你的方法中随意加入需要满足的条件。</p><p id="d815" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">当构造函数被调用来实例化一个NN时，它会执行一系列操作:</p><ol class=""><li id="56e9" class="nm nn iq lc b ld le lg lh lw no lx np ly nq lv nr ns nt nu bi translated">它对训练集X中的特征进行归一化；</li><li id="7c1e" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv nr ns nt nu bi translated">它在<code class="fe my mz na nb b">architecture</code>列表的开头添加了一个大小为<em class="lb"> m </em>的层，其中<em class="lb"> m </em>是特征的数量(它从第一个训练示例的形状中推断出来的)，并在末尾添加了另一个大小为<em class="lb"> n </em>(类的数量)的层；</li><li id="22cc" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv nr ns nt nu bi translated">它初始化一个名为参数的空<code class="fe my mz na nb b">dict</code>来存储偏差和权重。</li></ol><p id="bddf" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">一旦我们有了<code class="fe my mz na nb b">init</code>，我们就可以考虑初始化这些参数的方法。为此，我们进行如下操作:</p><ul class=""><li id="2489" class="nm nn iq lc b ld le lg lh lw no lx np ly nq lv oj ns nt nu bi translated">我们迭代<em class="lb">深</em>层的数量(因此，从1开始，到<em class="lb"> L </em> -1结束，其中<em class="lb"> L </em>是<code class="fe my mz na nb b">architecture</code>列表的长度)。</li><li id="b839" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv oj ns nt nu bi translated">对于每一层，我们向<code class="fe my mz na nb b">parameters</code>字典中添加一个用于权重的键值对和另一个用于偏差的键值对</li><li id="6133" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv oj ns nt nu bi translated">为了简单起见，我们通过从形状的正态分布样本中随机选取来初始化<strong class="lc ir">权重</strong>:<em class="lb">I层中的节点数，</em>I层中的节点数，<em class="lb"> i+1) </em>并将它们缩放100倍。更好的选择可能是使权重初始化依赖于所选择的激活函数(例如，参见<a class="ae kv" href="https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank">本文</a>)</li><li id="fe7e" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv oj ns nt nu bi translated"><strong class="lc ir">偏差</strong>是零向量:层中的每个节点一个。我们使用关键字<em class="lb"> w{i} </em>和带有关键字<em class="lb"> b{i}的每个偏置向量将每个权重矩阵存储在字典中。</em></li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">参数初始化</p></figure><h1 id="bbf6" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">前馈、反向传播和拟合</h1><h2 id="3bfb" class="oe ma iq bd mb ok ol dn mf om on dp mj lw oo op ml lx oq or mn ly os ot mp ou bi translated">前馈</h2><p id="9ede" class="pw-post-body-paragraph kz la iq lc b ld mr jr lf lg ms ju li lw mt ll lm lx mu lp lq ly mv lt lu lv ij bi translated">现在我们来看看<strong class="lc ir">正向传播</strong>和<strong class="lc ir">反向传播</strong>的方法。</p><p id="4106" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">我希望这两种方法都不接受任何参数作为输入(当然，除了类本身),并且就地修改模型。为什么？因为我们将把数据传递给<strong class="lc ir"> fit </strong>方法，并且对向前和向后传递的调用将在那里发生。(然而，我们确实需要每次向前传递返回一个<strong class="lc ir">成本</strong>值，我们将在梯度下降期间拟合模型时使用该值。)</p><p id="1a9e" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">因此，下面是我们如何实现<code class="fe my mz na nb b">forward</code>方法。您可能记得我们初始化了两个字典:一个用于层，一个用于参数(我们使用上面的<code class="fe my mz na nb b">initialize_parameters</code>方法填充)。对于除输入和输出层之外的每个层<em class="lb">中的每个节点，我们将需要一个输入(输入的加权和)和一个激活输出。我们可以在我们的<code class="fe my mz na nb b">layers</code>字典中为模型架构中的每一个<em class="lb"> i </em>深层分别存储为<em class="lb"> z{i} </em>和<em class="lb"> a{i} </em>。因此:</em></p><ul class=""><li id="5623" class="nm nn iq lc b ld le lg lh lw no lx np ly nq lv oj ns nt nu bi translated">我们迭代范围(1，架构的长度-1)；</li><li id="5d94" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv oj ns nt nu bi translated">为了获得该范围内每个<em class="lb"> i </em>的<code class="fe my mz na nb b">layers[zi]</code>,我们计算<code class="fe my mz na nb b">parameters[wi]</code>和<code class="fe my mz na nb b">layers[a-i]</code>的点积，再加上<code class="fe my mz na nb b">parameters[bi];</code></li><li id="b33e" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv oj ns nt nu bi translated">最后，为了获得激活输出<code class="fe my mz na nb b">layers[ai]</code>，我们将选择的激活函数应用于<code class="fe my mz na nb b">layers[zi]</code>。我们存储层的值，因为当我们执行反向传播时需要它们。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">正向传播</p></figure><p id="bec6" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">作为一个实现说明:我被成本函数计算中的溢出逼疯了，直到我意识到这是由于可能被零除。因此在成本计算中为0.00000001。</p><h2 id="f804" class="oe ma iq bd mb ok ol dn mf om on dp mj lw oo op ml lx oq or mn ly os ot mp ou bi translated">反向传播</h2><p id="1cf2" class="pw-post-body-paragraph kz la iq lc b ld mr jr lf lg ms ju li lw mt ll lm lx mu lp lq ly mv lt lu lv ij bi translated">反向传播算法的实现在结构上相对简单。它与前向传递的算法非常相似(这里我不打算讨论反向传播算法背后的数学，因为这已经超出了本文的范围)。我们初始化一个空的<code class="fe my mz na nb b">derivatives</code>字典来存储渐变，并从右到左遍历各层。对于每一层，我们计算<code class="fe my mz na nb b">dZ</code>，并使用它来计算该层的<code class="fe my mz na nb b">dW</code>和<code class="fe my mz na nb b">dB</code>(分别更新权重和偏差)。这就是我们之前定义的<code class="fe my mz na nb b">layers</code>字典派上用场的地方，因为我们需要访问每一层的输入<code class="fe my mz na nb b">z</code>和激活<code class="fe my mz na nb b">a</code>。首先，让我们计算最后(输出)层的梯度。</p><p id="3230" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">对于<strong class="lc ir">最外层的</strong>层来说，<code class="fe my mz na nb b">dz</code>仅仅是<code class="fe my mz na nb b">output — ground truths</code>的结果。(注:<code class="fe my mz na nb b">dz</code>是什么形状？有多少标签就有多少行，有多少训练示例就有多少列——因此，当我们用存储在<code class="fe my mz na nb b">layers</code>中的值打点它时，我们需要转置它。请记住这一点。)</p><p id="a941" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">然后我们可以如下计算<code class="fe my mz na nb b">dW</code>和<code class="fe my mz na nb b">db</code>:</p><ol class=""><li id="85e9" class="nm nn iq lc b ld le lg lh lw no lx np ly nq lv nr ns nt nu bi translated">权重的梯度是<code class="fe my mz na nb b">dz</code>的点积和<em class="lb">从<em class="lb">先前</em>层转置</em>的激活(见上述注释)(实例数量的平均值)</li><li id="7e6f" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv nr ns nt nu bi translated">偏差的梯度只是平均的<code class="fe my mz na nb b">dz</code>(注意，我们需要<code class="fe my mz na nb b">keepdims=True</code>，否则，数组将被挤压成形状(1)，作为秩1数组，这可能导致<a class="ae kv" href="https://www.kdnuggets.com/2020/07/numpy-handle-dimensions.html" rel="noopener ugc nofollow" target="_blank">不明确的结果</a>。</li><li id="4c18" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv nr ns nt nu bi translated">我们将该层激活的渐变存储在变量<code class="fe my mz na nb b">dAPrev</code>中。</li></ol><p id="ee71" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">然后我们在剩余的层上循环，除了这次我们将<code class="fe my mz na nb b">dz</code>计算为层的激活函数与<code class="fe my mz na nb b">n+1th</code>层的激活梯度<code class="fe my mz na nb b">dAPrev</code>的乘积。</p><p id="9586" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">最后，我们归还<code class="fe my mz na nb b">derivatives</code>字典。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">反向投影</p></figure><h2 id="944f" class="oe ma iq bd mb ok ol dn mf om on dp mj lw oo op ml lx oq or mn ly os ot mp ou bi translated">拟合、精确度和预测</h2><p id="2a48" class="pw-post-body-paragraph kz la iq lc b ld mr jr lf lg ms ju li lw mt ll lm lx mu lp lq ly mv lt lu lv ij bi translated">我们现在把它们放在一起训练网络。我们将一个<strong class="lc ir">学习速率</strong>和多个<strong class="lc ir">时期</strong>传递给<code class="fe my mz na nb b">fit</code>方法。这将通过调用<code class="fe my mz na nb b">initialize_parameters</code>来训练模型，并且对于每个时期运行一个正向传递，随后是返回梯度的反向传播。这些将依次用于更新可训练参数。在培训期间，费用将存储在一个列表中，以供将来参考。训练集和测试集(这里定义为<code class="fe my mz na nb b">accuracy</code>)上的性能也将被存储和显示。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">适合模型。</p></figure><p id="9376" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">最后两部分是<code class="fe my mz na nb b">predict</code>和<code class="fe my mz na nb b">accuracies</code>方法。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">预测和准确性。</p></figure><p id="ee33" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">我还定义了一堆绘图函数，我在这里省略了，但是如果你感兴趣，你可以在Github repo上找到它们。</p><h2 id="7e21" class="oe ma iq bd mb ok ol dn mf om on dp mj lw oo op ml lx oq or mn ly os ot mp ou bi translated">训练网络和结果</h2><p id="30f1" class="pw-post-body-paragraph kz la iq lc b ld mr jr lf lg ms ju li lw mt ll lm lx mu lp lq ly mv lt lu lv ij bi translated">最后，我们可以训练和测试我们的网络！让我们将数据分成一个训练集和一个测试集，并用ReLu执行训练(但是我们可以使用任何其他已定义的激活函数)。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="fbe5" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">然后，我们可以用适当的初始化参数实例化一个NN:在这里，我选择了一个分别具有128个和32个神经元的2层架构，我将以0.03的学习率跨200个时期对其进行训练。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="a304" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">让我们看看结果！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="ab gu cl nj"><img src="../Images/ae4c3c3df25a2764aa78744f82ec46a7.png" data-original-src="https://miro.medium.com/v2/format:webp/1*8fk_nfaA8v7aUBMs67QHgQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="ab gu cl nj"><img src="../Images/c218fa23e7f4f4ef6b97ba4eb6c1aabd.png" data-original-src="https://miro.medium.com/v2/format:webp/1*NG8Ao6Aj-E-ekpvvb3ckJg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="ab gu cl nj"><img src="../Images/4cd1d8b2847d3388937c895806877322.png" data-original-src="https://miro.medium.com/v2/format:webp/1*5J6XUuLwTNk6tfnnDMZO5A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="5f44" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">这个好像一点都不差！</p><p id="46db" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">我希望你在路上学到了一些东西。</p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h2 id="954f" class="oe ma iq bd mb ok ol dn mf om on dp mj lw oo op ml lx oq or mn ly os ot mp ou bi translated">参考</h2><p id="43f9" class="pw-post-body-paragraph kz la iq lc b ld mr jr lf lg ms ju li lw mt ll lm lx mu lp lq ly mv lt lu lv ij bi translated">[1]许可:Yann LeCun和Corinna Cortes拥有MNIST数据集的版权，该数据集是原始NIST数据集的衍生作品。MNIST数据集是根据<a class="ae kv" href="https://creativecommons.org/licenses/by-sa/3.0/" rel="noopener ugc nofollow" target="_blank">知识共享署名-同样分享3.0许可条款提供的。</a></p></div></div>    
</body>
</html>