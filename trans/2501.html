<html>
<head>
<title>Word2Vec Neural Network from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Word2Vec神经网络从无到有</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/skip-gram-neural-network-from-scratch-485f2e688238#2022-05-31">https://towardsdatascience.com/skip-gram-neural-network-from-scratch-485f2e688238#2022-05-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/112b7c26c01026d8fe74255ded9e611e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ga5JJ2qad8e35TCU"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">由<a class="ae jd" href="https://unsplash.com/@borodinanadi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">纳迪·博罗迪纳</a>在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><div class=""/><div class=""><h2 id="aeaf" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">使用Skip-Gram和CBOW深入研究非上下文NLP</h2></div><p id="142c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">自然语言处理领域具有挑战性有几个原因，但最主要也是最琐碎的原因是处理<strong class="kx jh">数据</strong>，因为<strong class="kx jh">不是数字，而是文本。</strong></p><p id="0339" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，需要一种从文本到数字值的转换，这种转换可以尽可能地保留单词的含义，而不会丢失单词在人的头脑中唤起的任何东西。</p><p id="be45" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种转换被称为<strong class="kx jh">单词嵌入</strong>。我们可以进行更低层次的分析，创建一个字符嵌入，或者更高层次的分析，创建一个句子嵌入。这取决于我们选择了哪些记号(我们的原子元素)。</p><p id="3fe0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">迄今为止，在众多尝试中，最好的尝试是<strong class="kx jh">将单词映射成数字向量</strong>。目标是让<strong class="kx jh">相似的单词在向量空间</strong>中紧密地放置在一起。我们可以用余弦相似度来度量向量的相似度。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lr"><img src="../Images/fdc5d2023d75756ba220fbc68d96aa53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T1TdC6Pj0wjIdN5h7mcCfQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><p id="432f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们看看上面图片的代码示例。</p><figure class="ls lt lu lv gt is"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="2c32" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你可以注意到<em class="ly">啤酒</em>和<em class="ly">葡萄酒</em>之间的相似度很高，这有道理吧？</p><p id="3100" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本文中，我们将处理非上下文嵌入方法，其中单词的<strong class="kx jh">表示，因此<strong class="kx jh">不依赖于上下文</strong>。</strong></p><p id="2587" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">例如，即使以下句子中的单词<em class="ly"> second </em>具有不同的含义，使用这些嵌入方法，其矢量表示将总是相同的。</p><p id="b17f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><code class="fe lz ma mb mc b">This is the second time that I try it</code></p><p id="2b86" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><code class="fe lz ma mb mc b">He just arrived one second later</code></p><p id="381c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你对非上下文自然语言处理的有影响力的论文感兴趣，这里有一个简短的列表:</p><ul class=""><li id="03ec" class="md me jg kx b ky kz lb lc le mf li mg lm mh lq mi mj mk ml bi translated"><strong class="kx jh"> Word2Vec </strong> ( <a class="ae jd" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank">米科洛夫等人，2013</a>)；</li><li id="69c1" class="md me jg kx b ky mm lb mn le mo li mp lm mq lq mi mj mk ml bi translated"><strong class="kx jh">用于单词表示的全局向量</strong>(手套)(<a class="ae jd" href="https://www-nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank"> Pennington et al .，2014</a>)；</li><li id="0fe3" class="md me jg kx b ky mm lb mn le mo li mp lm mq lq mi mj mk ml bi translated"><strong class="kx jh"> FastText </strong> ( <a class="ae jd" href="https://arxiv.org/abs/1607.04606" rel="noopener ugc nofollow" target="_blank">米科洛夫等人，2016</a>)；</li></ul><h1 id="a07d" class="mr ms jg bd mt mu mv mw mx my mz na nb km nc kn nd kp ne kq nf ks ng kt nh ni bi translated">Word2Vec</h1><p id="4900" class="pw-post-body-paragraph kv kw jg kx b ky nj kh la lb nk kk ld le nl lg lh li nm lk ll lm nn lo lp lq ij bi translated">2013年，随着<a class="ae jd" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="noopener ugc nofollow" target="_blank"> Word2Vec </a>，谷歌的Mikolov等人彻底改变了嵌入范式:从那时起，嵌入将是一个神经网络的权重，根据任务进行调整，以最小化一些损失。<strong class="kx jh">嵌入已经成为一种神经网络算法。</strong></p><p id="39ff" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们接下来要做的是训练一个简单的神经网络，在某个任务上只有一个隐藏层。但是请注意，我们不会使用网络来解决手头的任务，我们感兴趣的只是代表单词embedding的网络的<strong class="kx jh">权重。</strong></p><blockquote class="no np nq"><p id="6b21" class="kv kw ly kx b ky kz kh la lb lc kk ld nr lf lg lh ns lj lk ll nt ln lo lp lq ij bi translated">我们将训练神经网络做以下事情。给定一个句子中间的特定单词(输入单词)，查看附近的单词(窗口大小)并随机选择一个。一旦经过训练，神经网络将告诉我们，我们词汇表中的每个单词成为我们选择的邻近单词的概率。</p></blockquote><p id="abf5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将训练一个具有单个隐藏层的简单神经网络来执行某项任务，但之后我们实际上不会将该神经网络用于我们训练它的任务！相反，目标只是学习隐藏层的权重(不知何故，这类似于自动编码器的目标)。最后，我们会看到这些权重就是我们试图学习的<em class="ly">单词向量</em>。</p><p id="9486" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">例如，如果我们给训练好的网络输入单词<em class="ly"> New </em>，那么像<em class="ly"> York </em>和<em class="ly"> City </em>这样的单词的输出概率会比<em class="ly"> book </em>和<em class="ly">light sabre</em>更高。</p><p id="c6eb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">可以使用两种不同的方法来构造Word2Vec:</p><ul class=""><li id="5bd3" class="md me jg kx b ky kz lb lc le mf li mg lm mh lq mi mj mk ml bi translated"><strong class="kx jh"> CBOW </strong>:神经网络查看周围的单词，并预测中间出现的单词。</li><li id="a834" class="md me jg kx b ky mm lb mn le mo li mp lm mq lq mi mj mk ml bi translated"><strong class="kx jh"> SkipGram </strong>:神经网络接受一个单词，然后尝试预测周围的单词。</li></ul><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nu"><img src="../Images/20fadacb8223edc912d8fc1e60c02d3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l2egwV-EKim-HA48pRIpsA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><h1 id="4da0" class="mr ms jg bd mt mu mv mw mx my mz na nb km nc kn nd kp ne kq nf ks ng kt nh ni bi translated">从头开始编写程序</h1><p id="f654" class="pw-post-body-paragraph kv kw jg kx b ky nj kh la lb nk kk ld le nl lg lh li nm lk ll lm nn lo lp lq ij bi translated">首先，为了简单起见，让我们从三个单词句子的语料库中创建一个词汇表。</p><figure class="ls lt lu lv gt is"><div class="bz fp l di"><div class="lw lx l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">从自定义语料库创建词汇库</p></figure><p id="147a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">例如，如果我们输入单词<em class="ly">如</em>，上下文单词将是<em class="ly"> I </em>和<em class="ly"> dog。</em></p><figure class="ls lt lu lv gt is"><div class="bz fp l di"><div class="lw lx l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">打印输入和目标单词</p></figure><p id="0e19" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们准备准备我们的数据。我们将使用一个<em class="ly"> window_size = 1，</em>因此给定一个字<em class="ly"> j </em>目标将仅仅是<em class="ly"> j-1 </em>和<em class="ly"> j+1 </em>。</p><figure class="ls lt lu lv gt is"><div class="bz fp l di"><div class="lw lx l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">为skip program准备数据</p></figure><p id="f70a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在构建我们的模型之前，让我们分批构建我们的数据，并将单词转换成<a class="ae jd" href="https://en.wikipedia.org/wiki/One-hot" rel="noopener ugc nofollow" target="_blank"> <em class="ly">单键向量</em> </a> <em class="ly">。</em></p><figure class="ls lt lu lv gt is"><div class="bz fp l di"><div class="lw lx l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">生成批次</p></figure><h2 id="c0b0" class="nv ms jg bd mt nw nx dn mx ny nz dp nb le oa ob nd li oc od nf lm oe of nh og bi translated">建立模型</h2><p id="c2e0" class="pw-post-body-paragraph kv kw jg kx b ky nj kh la lb nk kk ld le nl lg lh li nm lk ll lm nn lo lp lq ij bi translated">这是我们模型的样本。隐层神经元上没有激活函数，但输出神经元使用softmax。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/040cad5110f53f9d8dd85abd013b157f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XBaqjqpnBIXtXLzWjkuLQQ.png"/></div></div></figure><p id="70e6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">和PyTorch一样，让我们定义一个类来构建我们的神经网络。为了简单起见，我们的嵌入大小是<em class="ly"> 2，</em>这样我们可以在二维向量空间上绘制嵌入。我们的神经网络的输入的维数等于<em class="ly"> vocab_size </em>(因为我们使用的是独热向量)。此外，输出具有dimension = <em class="ly"> vocab_size，</em>每个输出神经元的权重告诉我们代表该特定神经元的单词在输入中给定的单词附近的概率。</p><figure class="ls lt lu lv gt is"><div class="bz fp l di"><div class="lw lx l"/></div></figure><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oi"><img src="../Images/dc574763b5da9b5c778dcc9fccefcb14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*luRT1-asdgxKi-qMYAnbaQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><p id="4ee8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">是时候训练我们的模特了！</p><figure class="ls lt lu lv gt is"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="1778" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一旦训练结束，单词嵌入被存储在模型参数中。</p><figure class="ls lt lu lv gt is"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="201a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">输出应该是这样的。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/6eaa73cbb4c96142b4323d35693caeb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*jAzDgfi8dYKxueQSY4-J3A.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><h2 id="9411" class="nv ms jg bd mt nw nx dn mx ny nz dp nb le oa ob nd li oc od nf lm oe of nh og bi translated">绘制嵌入图</h2><figure class="ls lt lu lv gt is"><div class="bz fp l di"><div class="lw lx l"/></div></figure><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/3f5389da167262651b14625eca52b640.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*Um33myWUmZ-vo4dfjC2mrg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><p id="9658" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">好吧，这个情节和嵌入没有意义。这只是一个虚构的例子。我们希望有相近的单词。你能做的就是使用预训练的嵌入，比如<a class="ae jd" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> <em class="ly"> GloVe </em> </a> <em class="ly"> </em>并在你的项目中使用它们！</p><h2 id="5c79" class="nv ms jg bd mt nw nx dn mx ny nz dp nb le oa ob nd li oc od nf lm oe of nh og bi translated">结束了</h2><p id="2c7c" class="pw-post-body-paragraph kv kw jg kx b ky nj kh la lb nk kk ld le nl lg lh li nm lk ll lm nn lo lp lq ij bi translated"><em class="ly">马赛洛·波利蒂</em></p><p id="9916" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" href="https://www.linkedin.com/in/marcello-politi/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>，<a class="ae jd" href="https://twitter.com/_March08_" rel="noopener ugc nofollow" target="_blank"> Twitter </a>，<a class="ae jd" href="https://march-08.github.io/digital-cv/" rel="noopener ugc nofollow" target="_blank"> CV </a></p></div></div>    
</body>
</html>