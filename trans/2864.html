<html>
<head>
<title>Linear Regression, Logistic Regression, and SVM in 10 Minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">10分钟内完成线性回归、逻辑回归和SVM</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-3-classical-machine-learning-models-once-and-for-all-part-1-32a1ac52c0fd#2022-06-22">https://towardsdatascience.com/understanding-3-classical-machine-learning-models-once-and-for-all-part-1-32a1ac52c0fd#2022-06-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="725c" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习</h2><div class=""/><div class=""><h2 id="3a69" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">线性回归与logistic回归和支持向量机有什么关系？</h2></div><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="c525" class="la lb it kw b gy lc ld l le lf"><strong class="kw jd">Table of contents</strong></span><span id="8d82" class="la lb it kw b gy lg ld l le lf"><strong class="kw jd">· </strong><a class="ae lh" href="#a1a6" rel="noopener ugc nofollow"><strong class="kw jd">So, how does machine learning work?</strong></a><br/><strong class="kw jd">· </strong><a class="ae lh" href="#ae6d" rel="noopener ugc nofollow"><strong class="kw jd">Linear Models</strong></a><br/>  ∘ <a class="ae lh" href="#1e7e" rel="noopener ugc nofollow">Linear regression</a><br/>  ∘ <a class="ae lh" href="#a70b" rel="noopener ugc nofollow">Ridge</a><br/>  ∘ <a class="ae lh" href="#5e73" rel="noopener ugc nofollow">Lasso</a><br/>  ∘ <a class="ae lh" href="#cd6f" rel="noopener ugc nofollow">Elastic-Net</a><br/><strong class="kw jd">· </strong><a class="ae lh" href="#551b" rel="noopener ugc nofollow"><strong class="kw jd">Logistic Regression</strong></a><br/><strong class="kw jd">· </strong><a class="ae lh" href="#3bbd" rel="noopener ugc nofollow"><strong class="kw jd">Support Vector Machine (SVM)</strong></a><br/>  ∘ <a class="ae lh" href="#31cc" rel="noopener ugc nofollow">Classification</a><br/>  ∘ <a class="ae lh" href="#0286" rel="noopener ugc nofollow">Regression</a><br/>  ∘ <a class="ae lh" href="#daf9" rel="noopener ugc nofollow">Kernel functions</a><br/><strong class="kw jd">· </strong><a class="ae lh" href="#0f82" rel="noopener ugc nofollow"><strong class="kw jd">A note on preprocessing</strong></a><br/><strong class="kw jd">· </strong><a class="ae lh" href="#2996" rel="noopener ugc nofollow"><strong class="kw jd">Conclusion</strong></a></span></pre><p id="f32c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated">机器学习建模是数据科学家的问题解决者。尽管它不会占用我们大部分的时间，但就个人而言，它比数据清理有趣得多。</p><p id="54ee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">坦率地说，一些模型在数学上是复杂的。好消息是，作为一名数据科学家，你不一定有能力从零开始构建机器学习模型。已经有太多的库可供选择，没有必要重新发明轮子。然而，从鸟瞰的角度了解模型是如何工作的总是好的。</p><h1 id="a1a6" class="mn lb it bd mo mp mq mr ms mt mu mv mw ki mx kj my kl mz km na ko nb kp nc nd bi translated">那么，机器学习是如何工作的呢？</h1><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi ne"><img src="../Images/a5412dba1fa12b5c0aa5c7b6ccd83c4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CWNoicci28F2TUQc-vKijw.png"/></div></div><p class="nm nn gj gh gi no np bd b be z dk translated">机器学习模型的内部运作|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="d4e6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一般来说，所有有监督的机器学习模型的工作方式都是一样的:你有<strong class="lk jd"> <em class="nq">输入数据</em> </strong> <em class="nq"> X </em>和<strong class="lk jd"> <em class="nq">输出数据</em> </strong> <em class="nq"> y </em>，然后模型找到一个<strong class="lk jd"> <em class="nq">映射</em> </strong>从<em class="nq"> X </em>到<em class="nq"> y </em>。这就是为什么机器学习被称为这样:不是我们故意编码将<em class="nq"> X </em>映射到<em class="nq"> y </em>的逻辑，而是模型通过更新其<strong class="lk jd"> <em class="nq">参数</em> </strong>来自主学习。大多数车型还有<strong class="lk jd"> <em class="nq">超参数</em> </strong>，即无法学习，需要用户整定的参数。</p><p id="d6a3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了清楚起见，假设您有<em class="nq"> m </em> <strong class="lk jd"> <em class="nq">观察值</em> </strong>和<em class="nq"> n </em> <strong class="lk jd"> <em class="nq">特征</em> </strong>，并且您正在处理一个单输出任务。然后，<em class="nq"> X </em>是一个<em class="nq"> m × n </em>矩阵，而<em class="nq"> y </em>是一个大小为<em class="nq"> m </em>的向量。为了学习从<em class="nq"> X </em>到<em class="nq"> y </em>的映射，模型必须使用<strong class="lk jd"> <em class="nq">优化器</em> </strong>找到最佳参数，例如梯度下降、BFGS等等。但是什么是最优呢？具体来说什么是最佳的？</p><p id="5ea2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你可以把“寻找最佳参数”的问题看作一个优化问题(是的，机器学习确实只是一个优化问题)。所以，你需要一个<strong class="lk jd"> <em class="nq">目标函数</em> </strong>，或者在其他文献中也叫做<strong class="lk jd"> <em class="nq">代价函数</em> </strong>。</p><p id="f98d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">成本函数根据模型和任务而不同。例如，对于预测参数为<em class="nq"> w </em>的线性回归，常见的成本函数是<em class="nq">误差平方和</em>，表示为</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/102d2b142f5bd0c657c3b3dbaa60975c.png" data-original-src="https://miro.medium.com/v2/resize:fit:198/format:webp/1*YEc7TIFQxnXZpWVoB5gMLQ.png"/></div></figure><p id="7e01" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当<em class="nq"> w </em>最小化该成本函数时，找到最优<em class="nq"> w </em>。</p><h1 id="ae6d" class="mn lb it bd mo mp mq mr ms mt mu mv mw ki mx kj my kl mz km na ko nb kp nc nd bi translated">线性模型</h1><p id="b7a7" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">在下面的线性模型和逻辑回归中，为了方便起见，我们省略了偏差系数<em class="nq"> b </em>。偏差系数使我们的模型更通用，有两种方法可以添加它:</p><ol class=""><li id="8a01" class="nx ny it lk b ll lm lo lp lr nz lv oa lz ob md oc od oe of bi translated">通过在<em class="nq"> X </em>中创建一列1，因此<em class="nq"> X </em>现在是一个<em class="nq"> m × (n+1) </em>矩阵，并且<em class="nq"> w </em>是一个大小为<em class="nq"> n+1 </em>的向量，符号<em class="nq"> Xw </em>保持不变。</li><li id="937d" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">通过将<em class="nq"> Xw </em>显式更改为<em class="nq"> Xw + b </em>。</li></ol><h2 id="1e7e" class="la lb it bd mo ol om dn ms on oo dp mw lr op oq my lv or os na lz ot ou nc iz bi translated">线性回归</h2><p id="d163" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">在线性回归中，目标值<em class="nq"> ŷ </em>预计是特征的线性组合。换句话说，</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/e9e3558b3307dbaf9048eb7f7b23f909.png" data-original-src="https://miro.medium.com/v2/resize:fit:146/format:webp/1*ViIkIw9iRJd7RqwkWNu8Ag.png"/></div></figure><p id="01e6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如前所述，使用这个模型，你可以最小化<em class="nq">相对于<em class="nq"> w </em>的误差平方和</em>，也就是说，</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/e6de40610afe6449e49aa348196549f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/format:webp/1*A_yNnVkX2uUs6FAu9gEDjg.png"/></div></figure><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi ox"><img src="../Images/d416539a8578f3cb4344ddd2ebf2e2cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q0sfcyGbGG0Dms3CnMNtGg.png"/></div></div><p class="nm nn gj gh gi no np bd b be z dk translated">(<strong class="bd oy">左</strong>)线性回归模型与数据拟合得很好| ( <strong class="bd oy">右</strong>)线性回归模型与数据拟合得不太好|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="a2f7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">尽管形式简单，线性回归有一个问题。如果<em class="nq"> X </em>中的一些特征是相关的，那么(<em class="nq"> XᵀX </em> ) <em class="nq"> ⁻ </em>将接近奇异，并且<em class="nq"> w </em>变得对误差高度敏感，导致过拟合。幸运的是，有一个变通办法:如果太大，给<em class="nq"> w </em>一个惩罚。</p><p id="c7f8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这就产生了脊、套索和弹性网回归。</p><h2 id="a70b" class="la lb it bd mo ol om dn ms on oo dp mw lr op oq my lv or os na lz ot ou nc iz bi translated">山脉</h2><p id="fca5" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">为了给<em class="nq"> w </em>一个罚分，我们需要根据<em class="nq"> w </em>向成本函数添加一个量，使得最小化成本函数也最小化这个量。所讨论的量是<em class="nq"> w </em>的欧几里德范数(也称为L2范数)的平方乘以超参数<em class="nq"> α ≥ 0 </em>。目标变成了</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/ff10cf528f7606e7effb48bd3dca653b.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*0ze0c0ZPU0y5tld7opz2lA.png"/></div></figure><p id="5187" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">基于这一目标，不鼓励模型过于复杂，并且倾向于将<em class="nq"> w </em>向零收缩。<em class="nq"> α </em>的值越大，收缩量越大，因此<em class="nq"> w </em>对共线性变得更加稳健。</p><h2 id="5e73" class="la lb it bd mo ol om dn ms on oo dp mw lr op oq my lv or os na lz ot ou nc iz bi translated">套索</h2><p id="2c93" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">有一个替代岭回归的方法。不是将<em class="nq"> w </em>中的<em class="nq">所有</em>系数向零收缩，你的模型也可以学习将<em class="nq">部分</em>系数设置为零。只剩下一些非零系数，有效地减少了相关特性的数量。</p><p id="4793" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">目标变得出奇的容易。我们简单地将岭回归的成本函数中的L2范数的平方与L1范数交换如下</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/979d1bbcc52ae7f5db4caf2d495dd874.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*8xE8mcAT6K5QWl5Iivr4Aw.png"/></div></figure><p id="d8e2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有了这种能力，套索回归也可用于特征选择，并通过选择非零系数来减少<em class="nq"> X </em>的维度。然后，简化的数据可以用于另一个分类器/回归器。很容易看出，<em class="nq"> α </em>的值越高，选择的特征越少。</p><h2 id="cd6f" class="la lb it bd mo ol om dn ms on oo dp mw lr op oq my lv or os na lz ot ou nc iz bi translated">弹性网</h2><p id="e2ee" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">也有可能获得脊和套索回归的优势。弹性网络在成本函数中具有L1和L2范数，允许学习稀疏模型，其中很少有系数像lasso一样非零，同时仍然保持rigde的正则化属性。现在的目标是</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/3b331ae32af394c5f86fcc072acc9c67.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*IdBXYxF9mpF8_AUx4lV-Vw.png"/></div></figure><p id="7bdc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">超参数<em class="nq"> 0 ≤ ρ ≤ 1 </em>称为L1比，它控制着L1和L2范数的组合。请注意，如果<em class="nq"> ρ = 0 </em>，则物镜等效于ridge的物镜。另一方面，如果<em class="nq"> ρ = 1 </em>，那么这个目标就相当于lasso的。</p><p id="f1bf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">以下是对相同数据应用线性模型的示例，其中<em class="nq"> X </em>有4个特征，偏差<em class="nq"> b = 0 </em>。我们可以看到，ridge将系数向零收缩，lasso将第一个系数设置为零，而elastic-net就像是两者的组合。</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="fb72" class="la lb it kw b gy lc ld l le lf">Linear Regression<br/>w = [ 2.7794 -11.3707  46.0162  32.4487]<br/><br/>Ridge<br/>w = [ 2.2607  -9.8212  45.3181  31.9229]<br/><br/>Lasso<br/>w = [-0.      -2.4307  44.7832  31.3252]<br/><br/>Elastic-Net<br/>w = [-0.2630  -2.1721  40.4175  27.8334]</span></pre><p id="4624" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从线性回归到弹性网络，模型具有更好的性能(更低的RMSE)。请注意，这是<em class="nq">而不是</em>的情况。</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/f65055002ba9fa840648c1701b89795b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*IjPxnFfg93sFYS57T-9e2g.png"/></div><p class="nm nn gj gh gi no np bd b be z dk translated">线性模型的性能示例|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><h1 id="551b" class="mn lb it bd mo mp mq mr ms mt mu mv mw ki mx kj my kl mz km na ko nb kp nc nd bi translated">逻辑回归</h1><p id="8034" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">我们已经谈了很多关于回归的问题，所以让我们继续讨论分类。在二进制分类任务中，<em class="nq"> y </em>的每个元素<em class="nq"> yᵢ </em>都是两个类中的一个，可以编码为-1和1。目标是</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/a2440e61ac321c414ec56e2193f35d0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*g5MpjNe-UpmKAoEOxSj9SA.png"/></div></figure><p id="ae98" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<em class="nq"> Xᵢ </em>是垂直矢量形式的<em class="nq"> X </em>的第<em class="nq"> i </em>个观测值(行)。由于我们使用的是<a class="ae lh" href="https://en.wikipedia.org/wiki/Logistic_function" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> <em class="nq">逻辑函数</em> </strong> </a>(在某些文献中也称为sigmoid函数)，因此该任务也称为逻辑回归。</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pd"><img src="../Images/2b94a16306e23d2c0f233a1f82a6906d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EAeSNnNXluL9umJBvgVBag.png"/></div></div><p class="nm nn gj gh gi no np bd b be z dk translated">逻辑回归寻找线性决策边界|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="67e5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将解释为什么这个成本函数有意义。固定一个观察点<em class="nq"> j </em>。逻辑回归的工作方式是首先定义一个<em class="nq">决策边界</em>，在本例中为0。如果<em class="nq"> Xⱼᵀw ≥ 0 </em>，则预测<em class="nq"> ŷⱼ = 1 </em>。否则，预测<em class="nq"> ŷⱼ = -1 </em>。现在…</p><ul class=""><li id="225c" class="nx ny it lk b ll lm lo lp lr nz lv oa lz ob md pe od oe of bi translated">如果<em class="nq"> yⱼ = 1 </em>和<em class="nq"> Xⱼᵀw </em> ≪ <em class="nq"> 0 </em>，那么这次观察的成本很大，因为</li></ul><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pf"><img src="../Images/573e0ae412fda176fefb153c868a49a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*Jeo10AtfgbIf30O4qCFSLw.png"/></div></div></figure><p id="2eaf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，该模型将更倾向于满足<em class="nq"> Xⱼᵀw ≥ 0 </em>，这预测了<em class="nq"> ŷⱼ = 1 </em>，并且符合观测值<em class="nq"> yⱼ = 1 </em>。</p><ul class=""><li id="c3eb" class="nx ny it lk b ll lm lo lp lr nz lv oa lz ob md pe od oe of bi translated">如果<em class="nq"> yⱼ = -1 </em>和<em class="nq"> Xⱼᵀw </em> ≫ <em class="nq"> 0 </em>，那么这个观测的成本就很大，因为</li></ul><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/9141e5a41fadd33008118a8b15390f18.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*XKpqvtb-xktxS4tIti5ZsA.png"/></div></figure><p id="bd68" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，模型将更倾向于满足<em class="nq"> Xⱼᵀw &lt; 0 </em>预测<em class="nq"> ŷⱼ = -1 </em>并符合观察<em class="nq"> yⱼ = -1 </em>。</p><p id="3d3b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">逻辑回归也可以支持L1、L2或两者正则化。</p><h1 id="3bbd" class="mn lb it bd mo mp mq mr ms mt mu mv mw ki mx kj my kl mz km na ko nb kp nc nd bi translated">支持向量机(SVM)</h1><h2 id="31cc" class="la lb it bd mo ol om dn ms on oo dp mw lr op oq my lv or os na lz ot ou nc iz bi translated">分类</h2><p id="02fd" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">当我们看逻辑回归时，它能够为两个类别之间的任意<em class="nq"> x </em>画出一个决策边界<em class="nq"> wᵀx + b = 0 </em>。直观上，如果判定边界与任何类别的最近训练数据点的距离最大，则判定边界实现了良好的分离，因为一般来说，边界越大，分类器的泛化误差越低(尤其是在可分离的类别中)。</p><p id="4a26" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你看一下文献，SVM正试图通过最小化参数w的长度(L2范数)来最大化余量。当我第一次看到这个的时候，我不知道为什么。事实证明，这一事实成立，因为许多数学操作和推理，这将在下文中探讨。</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pd"><img src="../Images/832703b3a6bfe694c46ced3eec5cf9ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZJ4MEqbB5KjJfGmMLrxYAg.png"/></div></div><p class="nm nn gj gh gi no np bd b be z dk translated">线性可分数据的SVM判定边界，边缘边界上的三个观察值称为“支持向量”|图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a>提供</p></figure><p id="2b7a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，支持向量机的工作方式就像逻辑回归一样，只是稍微有点变化:符号(<em class="nq"> wᵀXᵢ + b </em>)对于大多数观察值<em class="nq"> i </em>来说应该是正确的，并且</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/593a1f31617654230a299dbf8d17821f.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*7oFbtyLSjLCYrqoGSgezCA.png"/></div></figure><p id="8ebe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">越大越好。这叫做<strong class="lk jd"> <em class="nq">功能余量</em> </strong>。让</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/06152bac9d6dba8403169424c52d8a46.png" data-original-src="https://miro.medium.com/v2/resize:fit:252/format:webp/1*bpHlY0JQdzRh800BulORIw.png"/></div></figure><p id="a45a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，对于每个<em class="nq"> i = 1，2，…，m </em>，让<em class="nq"> γᵢ </em>是从每个观察值<em class="nq"> Xᵢ </em>到判定边界的距离。从几何学上讲，由<a class="ae lh" href="https://en.wikipedia.org/wiki/Distance_from_a_point_to_a_line" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> <em class="nq">简单代数</em> </strong> </a>，我们有</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pj"><img src="../Images/28312e619705c49d19c8124a9c4e3ac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*kvGRKdPvet8TVxyOmP_7iw.png"/></div></div></figure><p id="38c0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">或者以更紧凑的形式，</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/fe75c05f42ce03313b60394284dd0da6.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*gvM4uaR70v50SmBMGvEs7A.png"/></div></figure><p id="5705" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这叫做<strong class="lk jd"> <em class="nq">几何余量</em> </strong>。设<em class="nq"> γ </em>为所有距离中的最小值，</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/a49c5e597566d63b55058719a1c77270.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/format:webp/1*6cxdsS3DncxqsXQfogieHg.png"/></div></figure><p id="21cd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">那么SVM的目标是在一些约束条件下找到<em class="nq"> γ </em>的最大值，</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/b9e7ce44ecb40aaddaf8854149b5b355.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*X71AAHXvQxssc5zStY0s1A.png"/></div></figure><p id="b2c2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于将<em class="nq"> w </em>和<em class="nq"> b </em>乘以某个因子不会改变几何余量，我们可以找到<em class="nq"> w </em>和<em class="nq"> b </em>使得‖<em class="nq">w</em><em class="nq">= 1</em>。因此，不失一般性，目标变成</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/daf840cd87489c2a3e518a9ea89130ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*oDSJgj_Q7L8Abm4NK60i0w.png"/></div></figure><p id="c089" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">根据功能和几何余量的定义，目标变成</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi po"><img src="../Images/1db12cca63c8388c6891a7b08a547505.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*WaF7EZWKlumcIwbjBqfHlg.png"/></div></figure><p id="c7fb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于将<em class="nq"> w </em>和<em class="nq"> b </em>乘以某个因子不会改变符号(<em class="nq"> wᵀXᵢ + b </em>)但会改变功能裕度，因此我们可以找到<em class="nq"> w </em>和<em class="nq"> b </em>使得功能裕度等于1。因此，不失一般性，目标变成</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/c680d3b662790156d51b3a368cd4b29e.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*-gRQcdaScHNL84dhs-V_aQ.png"/></div></figure><p id="da37" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因为‖ <em class="nq"> w </em> ‖ <em class="nq"> &gt; 0 </em>，这相当于</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/c36bb9aa79feca09faa8c892dfd9c7fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*zeFBERzc89vmACCMDwwjtA.png"/></div></figure><p id="2c0d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面的约束是理想的，表示一个完美的预测。但是类并不总是可以用判定边界完全分离的，所以我们允许一些观察值与它们正确的判定边界相距<em class="nq"> ξᵢ </em>。超参数<em class="nq"> C </em>控制这种惩罚的强度，并且因此充当逆正则化参数。最终目标是</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/e891e30e658573a5aff135be674c6981.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*gu7VfrWi17IkNZacEMAABg.png"/></div></figure><p id="e528" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">就是这样！SVM试图通过最小化参数<em class="nq"> w </em>的长度来最大化余量。</p><h2 id="0286" class="la lb it bd mo ol om dn ms on oo dp mw lr op oq my lv or os na lz ot ou nc iz bi translated">回归</h2><p id="7f1e" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">用于回归的SVM可以直接从分类中采用。我们不是希望<em class="nq"> yᵢ </em> ( <em class="nq"> wᵀXᵢ + b </em>)尽可能大，而是希望|<em class="nq">yᵢ</em>-(<em class="nq">wᵀxᵢ+b</em>)|尽可能小，即误差<em class="nq"> ε </em>尽可能小。再次利用逆正则化，我们有以下回归目标</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/b51cf7b402d9b45b153d5583bab5cbe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*UTxn0Tdfj5copu-8Om9roA.png"/></div></figure><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi ps"><img src="../Images/3b41a8e9dc452363f01e0119c1b23caf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hu8tFDYF64BwqjdTfRL9dg.png"/></div></div><p class="nm nn gj gh gi no np bd b be z dk translated">回归作者SVM |图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><h2 id="daf9" class="la lb it bd mo ol om dn ms on oo dp mw lr op oq my lv or os na lz ot ou nc iz bi translated">核函数</h2><p id="1e64" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">到目前为止，在我们预测输出数据<em class="nq"> y </em>的方式中，我们只使用了输入数据<em class="nq"> X </em>中特征的线性组合。对于逻辑回归和SVM，这导致了线性决策边界。因此，对于不可线性分离的输入数据，该模型的性能很差。但是有一种方法可以改进。</p><p id="07c7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以通过映射<em class="nq">ϕ</em>:ℝ<em class="nq">ⁿ</em>→ℝ<em class="nq">ᵖ</em>将输入数据映射到<strong class="lk jd">高阶特征空间</strong>。在这个更高的维度中，通过正确的映射，数据有可能是可分离的。然后，我们通过我们的模型使用这些映射的数据。</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pt"><img src="../Images/abcee9c94c466af12c3b1fdd32253a1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mR0DeD5p7bOjNe-FGEhPNg.png"/></div></div><p class="nm nn gj gh gi no np bd b be z dk translated">训练数据被映射到一个3维空间，在那里可以容易地找到一个分离的判定边界。|图片出自<a class="ae lh" href="https://commons.wikimedia.org/w/index.php?curid=60458994" rel="noopener ugc nofollow" target="_blank">纪——自己的作品</a></p></figure><p id="2332" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是我们还有另一个问题。SVM和许多其他机器学习模型可以用点积来表示，在高维映射数据中求解点积是非常昂贵的。幸运的是，我们有锦囊妙计:内核技巧<a class="ae lh" href="https://en.wikipedia.org/wiki/Kernel_method" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd"><em class="nq"/></strong></a>。</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pu"><img src="../Images/3e843760c089ae20d1edadbfc883feb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RFhrLKP61OXjVuVOvM0-hw.png"/></div></div><p class="nm nn gj gh gi no np bd b be z dk translated">50×50网格实数的核值可视化。我会把它留在这里，因为它看起来很美|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="600d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">核<em class="nq"> K </em>是一个相似度函数。它是由我们映射数据的点积定义的。所以，如果我们有两个观察值<em class="nq"> x </em>和<em class="nq"> y </em>在ℝ <em class="nq"> ⁿ </em>，</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pv"><img src="../Images/9108047c9fee0166eafa5b51b5298ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*3UxjSy6JDmW9SfIE7cir5A.png"/></div></div></figure><p id="1159" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通过内核技巧，我们可以在不访问高阶特征空间ℝ <em class="nq"> ᵖ </em>的情况下计算这个方程，见鬼，我们甚至不需要知道映射<em class="nq"> ϕ </em>。因此，在我们的模型中计算点积不再昂贵。</p><p id="5b52" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一些常见的内核是:</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/8a4190e7b4a1ca9764df4cbec828b071.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*xO-oe2YusHgi27uq3JCNXw.png"/></div></figure><p id="73f0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">非线性核允许模型学习更复杂的决策边界。核的所有参数都是模型的超参数。如果您将不同的核应用于我们用于分类的先前数据集，您将获得以下决策边界:</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi ox"><img src="../Images/51592b2a6de2665e34496bf84ea1d563.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cz_TehIkLdINLmmXjSQGBw.png"/></div></div><p class="nm nn gj gh gi no np bd b be z dk translated">不同内核的SVM分类|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><blockquote class="px"><p id="bd1f" class="py pz it bd qa qb qc qd qe qf qg md dk translated">SVM在高维空间和特征数量大于观察数量的情况下是有效的。许多不同的内核可供选择(或自己制作)，这使得SVM功能多样。</p></blockquote><p id="6e97" class="pw-post-body-paragraph li lj it lk b ll qh kd ln lo qi kg lq lr qj lt lu lv qk lx ly lz ql mb mc md im bi translated">然而，如果特征的数量远大于观察的数量，在选择核和正则项时避免过度拟合是至关重要的。</p><h1 id="0f82" class="mn lb it bd mo mp mq mr ms mt mu mv mw ki mx kj my kl mz km na ko nb kp nc nd bi translated">关于预处理的一个注记</h1><p id="d143" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">总是有这样一个问题:“在将输入数据<em class="nq"> X </em>输入到模型之前，我应该标准化/规范化它吗？”。讽刺的是，最满意的答案是“看情况”。</p><p id="e79b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果您的模型使用一元解析解，即<a class="ae lh" href="http://mlwiki.org/index.php/Normal_Equation" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> <em class="nq">正规方程</em> </strong> </a>进行线性回归，那么<strong class="lk jd">不需要</strong>进行标准化/规范化。你知道为什么吗？</p><p id="424e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">假设你有一个比其他特征大几千倍的特征，而你没有标准化/规范化它。假设法线方程将该特定特征的系数设置为<em class="nq"> β </em>。如果先标准化/归一化，正规方程会产生一个系数是<em class="nq"> β </em>的几千倍。因此，特征的缩放和产生的系数相互抵消，模型随后给出相同的预测。</p><p id="b7ff" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">另一方面，如果您的模型使用迭代方法，即<a class="ae lh" rel="noopener" target="_blank" href="/complete-step-by-step-gradient-descent-algorithm-from-scratch-acba013e8420"> <strong class="lk jd"> <em class="nq">梯度下降</em> </strong> </a>而不是标准方程，那么您希望首先标准化/规范化您的输入数据以加快收敛。</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi qm"><img src="../Images/f6ce357288ec92657779472362d6785f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FQTXjJK-pHR8hYYqZ-A4LA.png"/></div></div><p class="nm nn gj gh gi no np bd b be z dk translated">数据标准化对梯度下降算法的影响。标准化数据导致更快的收敛|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="66bf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">另一个问题与<strong class="lk jd">模型扩展</strong>(非线性内核、正则化等)有关。例如，学习算法的成本函数中使用的许多元素(例如SVM 的<strong class="lk jd"> RBF核或者线性模型</strong>的<strong class="lk jd"> L1和L2正则化子)假设所有特征都以零为中心，并且具有相同顺序的方差。如果某个要素的方差比其他要素的方差大几个数量级，则它可能会在成本函数中占主导地位，并使估计者无法像预期的那样从其他要素中正确学习。因此，你<strong class="lk jd">需要</strong>来标准化。</strong></p><h1 id="2996" class="mn lb it bd mo mp mq mr ms mt mu mv mw ki mx kj my kl mz km na ko nb kp nc nd bi translated">结论</h1><p id="6a73" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">你已经非常详细地学习了三个最基本的机器学习模型:<strong class="lk jd">线性回归</strong>、<strong class="lk jd">逻辑回归</strong>和<strong class="lk jd"> SVM </strong>。现在，您不仅可以使用已建立的库来构建它，还可以自信地知道它们是如何从内到外工作的，使用它们的最佳实践，以及如何提高它们的性能。</p><p id="f85c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">恭喜你。</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi qn"><img src="../Images/e3c9dbbfe2d111b1259d9b7eee2835cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KGZW7nIZhDc8eSrK"/></div></div><p class="nm nn gj gh gi no np bd b be z dk translated"><a class="ae lh" href="https://unsplash.com/@mukukostudio?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Mukuko工作室</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="a8f8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">以下是一些关键要点:</p><ol class=""><li id="c29a" class="nx ny it lk b ll lm lo lp lr nz lv oa lz ob md oc od oe of bi translated">线性回归有利于回归任务中的<strong class="lk jd">基线</strong>。</li><li id="f5de" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">正则化用于解决<strong class="lk jd">过拟合</strong>，有三种方法可用:<strong class="lk jd"> L1、L2或两者都用</strong>。</li><li id="a52e" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">尽管有他的名字，逻辑回归是用于<strong class="lk jd">分类</strong>任务的。它在两个类之间找到一个<strong class="lk jd">线性判定边界</strong>。</li><li id="e433" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">直观地，如果决策边界具有到任何类别的最近训练数据点的最大距离，则逻辑回归给出较低的泛化误差。这正是SVM所做的。</li><li id="5529" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">得益于内核函数，SVM还通过允许<strong class="lk jd">非线性决策边界</strong>有效地将逻辑回归推向了一个新的高度。</li><li id="b46f" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">SVM适用于高维空间以及特征数量大于观察数量的情况。</li><li id="b523" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">SVM的RBF核或线性模型的L1和L2正则化子假设<strong class="lk jd">标准化</strong>输入数据。</li></ol><p id="37e7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">希望你学点东西:)</p><figure class="kr ks kt ku gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi qo"><img src="../Images/e1a6e3674ab93bcb99796285f9d0175c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*6HsoGpmIb1oibJc_JWbqJA.gif"/></div></div></figure></div><div class="ab cl qp qq hx qr" role="separator"><span class="qs bw bk qt qu qv"/><span class="qs bw bk qt qu qv"/><span class="qs bw bk qt qu"/></div><div class="im in io ip iq"><p id="4859" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">🔥你好！如果你喜欢这个故事，想支持我这个作家，可以考虑 <a class="ae lh" href="https://dwiuzila.medium.com/membership" rel="noopener"> <strong class="lk jd"> <em class="nq">成为会员</em> </strong> </a> <em class="nq">。每月只需5美元，你就可以无限制地阅读媒体上的所有报道。如果你注册使用我的链接，我会赚一小笔佣金。</em></p><p id="bd58" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">🔖<em class="nq">想了解更多关于经典机器学习模型如何工作以及如何优化其参数的信息？或者MLOps大型项目的例子？有史以来最优秀的文章呢？继续阅读:</em></p><div class="qw qx gp gr qy"><div role="button" tabindex="0" class="ab bv gv cb fp qz ra bn rb nk ex"><div class="rc l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw rd re fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l rd re fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----32a1ac52c0fd--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="rh ri gw l"><h2 class="bd jd wk nx fp wl fr fs wm fu fw jc bi translated">从零开始的机器学习</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wn au wo wp wq tb wr an eh ei ws wt wu el em eo de bk ep" href="https://dwiuzila.medium.com/list/machine-learning-from-scratch-b35db8650093?source=post_page-----32a1ac52c0fd--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wv l fo"><span class="bd b dl z dk">8 stories</span></div></div></div><div class="ru dh rv fp ab rw fo di"><div class="di rm bv rn ro"><div class="dh l"><img alt="" class="dh" src="../Images/4b97f3062e4883b24589972b2dc45d7e.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*CWNoicci28F2TUQc-vKijw.png"/></div></div><div class="di rm bv rp rq rr"><div class="dh l"><img alt="" class="dh" src="../Images/b1f7021514ba57a443fe0db4b7001b26.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*wSRsSHYnIiGJFAqC"/></div></div><div class="di bv rs rt rr"><div class="dh l"><img alt="" class="dh" src="../Images/deb73e42c79667024a46c2c8902b81fa.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HVEz7KwzO0tv1Q4d"/></div></div></div></div></div><div class="qw qx gp gr qy"><div role="button" tabindex="0" class="ab bv gv cb fp qz ra bn rb nk ex"><div class="rc l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw rd re fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l rd re fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----32a1ac52c0fd--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="rh ri gw l"><h2 class="bd jd wk nx fp wl fr fs wm fu fw jc bi translated">高级优化方法</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wn au wo wp wq tb wr an eh ei ws wt wu el em eo de bk ep" href="https://dwiuzila.medium.com/list/advanced-optimization-methods-26e264a361e4?source=post_page-----32a1ac52c0fd--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wv l fo"><span class="bd b dl z dk">7 stories</span></div></div></div><div class="ru dh rv fp ab rw fo di"><div class="di rm bv rn ro"><div class="dh l"><img alt="" class="dh" src="../Images/15b3188b0f29894c2bcf3d0965515f44.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*BVMamoNudzn9UlAE"/></div></div><div class="di rm bv rp rq rr"><div class="dh l"><img alt="" class="dh" src="../Images/3249ba2cf680952e2ccdff36d8ebf4a7.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*C1fv3HJdh1RBspwN"/></div></div><div class="di bv rs rt rr"><div class="dh l"><img alt="" class="dh" src="../Images/a73f0494533d8a08b01c2b899373d2b9.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*QZvzgiM2VnhYyx8M"/></div></div></div></div></div><div class="qw qx gp gr qy"><div role="button" tabindex="0" class="ab bv gv cb fp qz ra bn rb nk ex"><div class="rc l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw rd re fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l rd re fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----32a1ac52c0fd--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="rh ri gw l"><h2 class="bd jd wk nx fp wl fr fs wm fu fw jc bi translated">MLOps大型项目</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wn au wo wp wq tb wr an eh ei ws wt wu el em eo de bk ep" href="https://dwiuzila.medium.com/list/mlops-megaproject-6a3bf86e45e4?source=post_page-----32a1ac52c0fd--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wv l fo"><span class="bd b dl z dk">6 stories</span></div></div></div><div class="ru dh rv fp ab rw fo di"><div class="di rm bv rn ro"><div class="dh l"><img alt="" class="dh" src="../Images/41b5d7dd3997969f3680648ada22fd7f.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*EBS8CP_UnStLesXoAvjeAQ.png"/></div></div><div class="di rm bv rp rq rr"><div class="dh l"><img alt="" class="dh" src="../Images/41befac52d90334c64eef7fc5c4b4bde.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*XLpRKnIMcJzBzCwvXrLvsw.png"/></div></div><div class="di bv rs rt rr"><div class="dh l"><img alt="" class="dh" src="../Images/80908ef475e97fbc42efe3fae0dfcff5.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*K_gzBmjv-ZHlU0Q6HeXclQ.jpeg"/></div></div></div></div></div><div class="qw qx gp gr qy"><div role="button" tabindex="0" class="ab bv gv cb fp qz ra bn rb nk ex"><div class="rc l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw rd re fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l rd re fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----32a1ac52c0fd--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="rh ri gw l"><h2 class="bd jd wk nx fp wl fr fs wm fu fw jc bi translated">我最好的故事</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wn au wo wp wq tb wr an eh ei ws wt wu el em eo de bk ep" href="https://dwiuzila.medium.com/list/my-best-stories-d8243ae80aa0?source=post_page-----32a1ac52c0fd--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wv l fo"><span class="bd b dl z dk">24 stories</span></div></div></div><div class="ru dh rv fp ab rw fo di"><div class="di rm bv rn ro"><div class="dh l"><img alt="" class="dh" src="../Images/0c862c3dee2d867d6996a970dd38360d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*K1SQZ1rzr4cb-lSi"/></div></div><div class="di rm bv rp rq rr"><div class="dh l"><img alt="" class="dh" src="../Images/392d63d181090365a63dc9060573bcff.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*hSKy6kKorAfHjHOK"/></div></div><div class="di bv rs rt rr"><div class="dh l"><img alt="" class="dh" src="../Images/f51725806220b60eccf5d4c385c700e9.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HiyGwoGOMI5Ao_fd"/></div></div></div></div></div><div class="qw qx gp gr qy"><div role="button" tabindex="0" class="ab bv gv cb fp qz ra bn rb nk ex"><div class="rc l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw rd re fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l rd re fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated">艾伯斯·乌兹拉</p></div></div><div class="rh ri gw l"><h2 class="bd jd wk nx fp wl fr fs wm fu fw jc bi translated">R中的数据科学</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wn au wo wp wq tb wr an eh ei ws wt wu el em eo de bk ep" href="https://dwiuzila.medium.com/list/data-science-in-r-0a8179814b50?source=post_page-----32a1ac52c0fd--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wv l fo"><span class="bd b dl z dk">7 stories</span></div></div></div><div class="ru dh rv fp ab rw fo di"><div class="di rm bv rn ro"><div class="dh l"><img alt="" class="dh" src="../Images/e52e43bf7f22bfc0889cc794dcf734dd.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*10B3radiyQGAp-QA"/></div></div><div class="di rm bv rp rq rr"><div class="dh l"><img alt="" class="dh" src="../Images/945fa9100c2a00b46f8aca3d3975f288.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*o6A863Vdwq7ThlmW"/></div></div><div class="di bv rs rt rr"><div class="dh l"><img alt="" class="dh" src="../Images/3ca9e4b148297dbc4e7da0a180cf9c99.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*ekmX89TW6N8Bi8bL"/></div></div></div></div></div></div></div>    
</body>
</html>