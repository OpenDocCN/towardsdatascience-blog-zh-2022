<html>
<head>
<title>Idempotent Writes to Delta Lake Tables</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">等幂写入Delta Lake表</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/idempotent-writes-to-delta-lake-tables-96f49addd4aa#2022-07-08">https://towardsdatascience.com/idempotent-writes-to-delta-lake-tables-96f49addd4aa#2022-07-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a98d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用开源三角洲湖的演练</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1e2d3fd897994c2ee1a60f49bf96318b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NiO1Jw-OIEghoRAz"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/photos/JI0KxozvOtQ" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/JI0KxozvOtQ</a></p></figure><h1 id="5aaa" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="7cec" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">根据维基百科:</p><blockquote class="mk ml mm"><p id="534c" class="lo lp mn lq b lr mo jr lt lu mp ju lw mq mr lz ma ms mt md me mu mv mh mi mj ij bi translated"><strong class="lq ir">幂等性</strong>是数学和计算机科学中某些运算的属性，由此它们可以被多次应用而不改变最初应用之后的结果。</p></blockquote><p id="59cb" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">一些非技术性的例子是电梯呼叫按钮和人行横道按钮。在许多情况下，拥有一个幂等的软件API是一个关键的必备特性。其中一种情况是Spark结构化流。默认情况下，结构化流使用检查点和预写日志来保证一次性容错。当使用<code class="fe mw mx my mz b">foreachBatch</code> API时，这并不适用。</p><p id="820e" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">这个API非常强大，通常需要在流数据上应用Spark SQL批处理API，或者将输出写入多个目的地，或者调用像<code class="fe mw mx my mz b">MERGE INTO</code>这样的API来更新下游增量表。这种方法的主要问题是，如果流在<code class="fe mw mx my mz b">foreachBatch</code>中失败或中断，目标表中就有可能出现重复。在本帖中，我们将会看到这种情况(并不罕见)会产生什么影响，以及如何处理它。</p><p id="e429" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">幸运的是，Delta Lake<a class="ae kv" href="https://databricks.com/blog/2022/06/30/open-sourcing-all-of-delta-lake.html" rel="noopener ugc nofollow" target="_blank">已经完全开源</a>,因此很容易理解像delta lake <a class="ae kv" href="https://docs.databricks.com/delta/delta-streaming.html#idempot-write" rel="noopener ugc nofollow" target="_blank">幂等表写</a>这样的某个特性是如何实现的，以及它的限制是什么。</p><h1 id="5852" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">环境设置</h1><p id="6e18" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这应该很容易，我们需要的只是delta lake的最新预览版和python 3.7+虚拟环境。</p><ul class=""><li id="e437" class="na nb iq lq b lr mo lu mp lx nc mb nd mf ne mj nf ng nh ni bi translated">您可以使用Mac/Linux/Windows，但是在Windows上最好使用WSL2。这里会有Spark，你不想浪费时间在wintuils和其他步骤上让Spark在Windows上运行。</li><li id="a55e" class="na nb iq lq b lr nj lu nk lx nl mb nm mf nn mj nf ng nh ni bi translated">使用您喜欢的环境管理工具创建一个新的Python环境并激活它。</li><li id="2295" class="na nb iq lq b lr nj lu nk lx nl mb nm mf nn mj nf ng nh ni bi translated">运行<code class="fe mw mx my mz b">pip install delta-spark==2.0.0rc1</code>。</li><li id="f753" class="na nb iq lq b lr nj lu nk lx nl mb nm mf nn mj nf ng nh ni bi translated">运行<code class="fe mw mx my mz b">pyspark --version</code>确认您的安装，它应该显示<strong class="lq ir"> 3.2.1 </strong>，因为它与<code class="fe mw mx my mz b">delta-spark</code>捆绑在一起。</li></ul><h1 id="042b" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">快乐的场景</h1><p id="ec66" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们都喜欢快乐的场景！至少看到代码按预期工作并产生一些结果感觉很好。让我们构建一些基本的Spark结构化流设置。源将是一个包含10次提交的增量表，其中每次提交都是一个文件。目的地是另一个增量表，但写入将使用<code class="fe mw mx my mz b">foreachBatch</code> API完成，而不是作为传统的增量流接收器。</p><p id="1655" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">复制下面要点的内容，保存为<code class="fe mw mx my mz b">producer.py</code>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="61eb" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">生产者脚本执行以下操作:</p><ul class=""><li id="9eab" class="na nb iq lq b lr mo lu mp lx nc mb nd mf ne mj nf ng nh ni bi translated">从一些导入开始，准备一个文件夹来保存增量表和流检查点。</li><li id="574d" class="na nb iq lq b lr nj lu nk lx nl mb nm mf nn mj nf ng nh ni bi translated">创建一个火花会议与三角洲湖依赖连线给我们。</li><li id="ae8c" class="na nb iq lq b lr nj lu nk lx nl mb nm mf nn mj nf ng nh ni bi translated">创建一些虚拟数据帧，并将其附加到名为source的增量表位置。这个过程重复10次，每次追加(提交)有10条记录，将保存在一个文件中，因此称为<code class="fe mw mx my mz b">repartition(1)</code>。之所以这样设计，是因为流应用程序将被配置为每个微批处理提取一个数据文件(一对一映射到提交)，只是为了让事情更容易理解。总之，我们的源表有100条记录，分成10个文件。</li><li id="1840" class="na nb iq lq b lr nj lu nk lx nl mb nm mf nn mj nf ng nh ni bi translated">脚本的最后一部分回显了一些关于写入数据的指标和细节，以确认它处于下一步的良好状态。</li></ul><p id="a66c" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">在上一步的同一个Python环境中，运行<code class="fe mw mx my mz b">python ./producer.py</code>，您应该会得到如下结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/e372407c57083af6309978b1a0449766.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Q35xfYdySZx75woE1VHVg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="1c19" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">现在，创建另一个名为<code class="fe mw mx my mz b">consumer.py</code>的文件，并用下面的要点填充它。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="88bd" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">这个文件对制作人来说是硬币的另一面:</p><ul class=""><li id="9158" class="na nb iq lq b lr mo lu mp lx nc mb nd mf ne mj nf ng nh ni bi translated">它从经典导入开始，并创建一个Spark会话。</li><li id="781c" class="na nb iq lq b lr nj lu nk lx nl mb nm mf nn mj nf ng nh ni bi translated">然后，它定义了<code class="fe mw mx my mz b">foreachBatch</code> API回调函数，该函数简单地打印批处理Id，回显微批处理的内容，最后将其附加到目标增量表中。这是可以使用的基本逻辑。打印批次Id和微批次仅用于说明目的。</li></ul><p id="d06d" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated"><strong class="lq ir">提示:</strong> <em class="mn">你在微批上做的任何火花动作都会触发整个血统。因此，如果要在函数中多次使用微批处理，缓存它可能会很有用。</em></p><ul class=""><li id="3804" class="na nb iq lq b lr mo lu mp lx nc mb nd mf ne mj nf ng nh ni bi translated">脚本中的下一部分是流查询:<br/> -从使用producer脚本准备的增量表中读取数据流。<br/> -通过使用<code class="fe mw mx my mz b">maxFilesPerTrigger </code> <br/>从每个微批次的源中提取一个单个拼花数据文件-设置检查点位置并连接<code class="fe mw mx my mz b">foreachBatch</code> <br/>的处理程序-触发持续时间为2秒的流并运行它最多30秒，然后继续下一步。如果我们使用一个与<a class="ae kv" href="https://spark.apache.org/releases/spark-release-3-3-0.html#structured-streaming" rel="noopener ugc nofollow" target="_blank"> Spark 3.3 </a>兼容的delta lake(至少对于OSS delta来说现在不是这种情况)，我们可以使用流式触发<code class="fe mw mx my mz b">availableNow</code>。这将使我们不必指定超时，并且它仍然会考虑其他选项，如<code class="fe mw mx my mz b">maxFilesPerTrigger</code>。这里的假设是，在大多数机器上，所有的源表文件将在不到30秒的时间内得到处理。</li><li id="d561" class="na nb iq lq b lr nj lu nk lx nl mb nm mf nn mj nf ng nh ni bi translated">最后，打印目标表中的记录数，猜猜会发生什么？那应该能印100，除非什么东西真的坏了。</li></ul><p id="73b1" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">现在运行<code class="fe mw mx my mz b">python ./consumer.py</code>，让我们看看这是否真的是一个快乐的场景😅。预期的结果将如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/24340acfa6363acef36d66bfde6da268.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gTMx1r0jzfzGO1bESpXFRg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="7ddf" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">正如您所看到的，每个微批次的Id和内容都被打印出来，目标表的最终记录计数为100，证明一切都按预期运行。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/944290e05ddbebeddcb71bf458bb0094.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/0*Oa-hiQZh_3bN4F7K.jpg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://imgflip.com/i/6lmfss" rel="noopener ugc nofollow" target="_blank">https://imgflip.com/i/6lmfss</a></p></figure></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><h1 id="ec86" class="kw kx iq bd ky kz oa lb lc ld ob lf lg jw oc jx li jz od ka lk kc oe kd lm ln bi translated">“不那么”快乐的场景！</h1><p id="54db" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">好了，是时候让事情崩溃了。要重新开始，您可以删除检查点和目标表的文件夹，但更简单的方法是再次运行生成器，这将清理所有相关的文件夹，包括检查点和目标表。</p><p id="066c" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">这次我们将模拟写完微批号5后直接发生错误的事情。将消费者文件中的<code class="fe mw mx my mz b">batch_function</code>代码修改如下:</p><pre class="kg kh ki kj gt of mz og oh aw oi bi"><span id="d02a" class="oj kx iq mz b gy ok ol l om on">def batch_function(df, batch_id):</span><span id="4c3f" class="oj kx iq mz b gy oo ol l om on">    print(f"Processing micro-batch {batch_id}")</span><span id="8c19" class="oj kx iq mz b gy oo ol l om on">    df.show()</span><span id="aa8c" class="oj kx iq mz b gy oo ol l om on">    df.write.mode("append").format("delta").save(target)</span><span id="eab1" class="oj kx iq mz b gy oo ol l om on">    if batch_id == 5:</span><span id="e511" class="oj kx iq mz b gy oo ol l om on">        raise RuntimeError("Simulated exception!")</span></pre><p id="28d3" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">在写入批次Id 5的数据后，最后两行引发一个伪异常。这将导致流崩溃，无法写入检查点文件。实际上，这模拟了未记录或跟踪的对目标系统的影响。现在让我们试一试。</p><pre class="kg kh ki kj gt of mz og oh aw oi bi"><span id="0217" class="oj kx iq mz b gy ok ol l om on">python ./consumer.py</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/0649f8c9bdc22f00072b5fa77cc224bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eROzrF4WCtN3JkfoWAjziw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="9ea2" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">不出所料，程序崩溃了，但我们想确认数据是否已写入目标。您可以运行一个独立的PySpark shell会话来验证这一点。</p><pre class="kg kh ki kj gt of mz og oh aw oi bi"><span id="708f" class="oj kx iq mz b gy ok ol l om on">pyspark --packages io.delta:delta-core_2.12:2.0.0rc1 --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/2ec04fca8d0f6e685b6ab687cd8b9807.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AMBkrbQ1LWte6mDHVVo8XQ.png"/></div></div></figure><p id="2fd4" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">记录的数量是60，这是正确的，因为我们从批次Id 0开始，批次Id 5的内容在目标表中，所以发生了写操作。</p><p id="4916" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">现在想象一下，如果我们移除模拟异常并重新运行消费者应用程序，会发生什么！</p><p id="46dd" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">流检查点文件夹没有任何关于批次Id 5的持久化数据，因此流将在批次Id 5处<strong class="lq ir">恢复</strong>，这显然会在目标增量表中产生重复数据。</p><p id="d0fb" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">因此，删除模拟的异常行并重新运行消费者。流应该从批处理Id 5开始，目标表中的记录数将是110而不是100，这表明数据损坏(在我们的例子中是重复)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/a3703e011c618920bcfc0f14de019f5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CQoIIbDA1hflU3U6JL9usQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="ea31" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">幂等写拯救！</h1><p id="7032" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">因为这是一个非常常见的用例，因为流重启是不可避免的，所以delta lake团队想出了一个解决方案，在从流写入delta表时，这个解决方案非常有效。请记住这一点，因为它不是一个通用的解决方案，所以例如，当写入普通的拼花文件时，使用它是没有意义的。</p><p id="00f1" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">无论如何，因为delta lake最近的开源，所有这些先进的delta lake功能现在都可以在GitHub上获得；我们可以检查源代码，甚至在尝试之前就理解它是如何工作的。</p><p id="dddb" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">核心解决方案围绕两个选项，这两个选项可用于流式查询。第一个是<code class="fe mw mx my mz b">txnAppId</code>，它保存了应用程序写入目标增量表的Id。这是必要的，因为增量表可以有多个并行写入的流，并且每个流都可以是独立的不同应用程序。另一个选项<code class="fe mw mx my mz b">txnVersion</code>是一个序列号，表示正在写入的数据的应用版本。在<code class="fe mw mx my mz b">foreachBatch</code>的情况下，这通常是批次Id。这两个选项的伪代码如下:</p><pre class="kg kh ki kj gt of mz og oh aw oi bi"><span id="8245" class="oj kx iq mz b gy ok ol l om on">txnVersion = options.get("txnVersion")<br/>txnAppId = options.get("txnAppId")</span></pre><p id="563b" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">接下来，如果接收到的版本以前在增量表中出现过(对于同一个appId)，writer类将简单地跳过写入。增量表事务日志不需要存储所有的历史版本，只需要每个应用Id的最新版本就足够了。我稍后会谈到这一点。<a class="ae kv" href="https://github.com/delta-io/delta/blob/585855c452bd8ef5ff512058c7861dfdd3e9d972/core/src/main/scala/org/apache/spark/sql/delta/commands/WriteIntoDelta.scala#L320" rel="noopener ugc nofollow" target="_blank">那个逻辑</a>的伪代码是这样的。</p><pre class="kg kh ki kj gt of mz og oh aw oi bi"><span id="5a67" class="oj kx iq mz b gy ok ol l om on">def hasTransactionBeenExecutedBefore(table, txnVersion, txnAppId)</span><span id="d909" class="oj kx iq mz b gy oo ol l om on">    latestSeenVersion = table.getLatestSeenVersionForApp(txnAppId)</span><span id="c81c" class="oj kx iq mz b gy oo ol l om on">    if (latestSeenVersion &gt;= txnVersion)</span><span id="9a91" class="oj kx iq mz b gy oo ol l om on">        return true<br/>    else<br/>        return false</span></pre><p id="5c6a" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">现在让我们重新开始:</p><ul class=""><li id="eca9" class="na nb iq lq b lr mo lu mp lx nc mb nd mf ne mj nf ng nh ni bi translated">运行生产者</li><li id="d8ad" class="na nb iq lq b lr nj lu nk lx nl mb nm mf nn mj nf ng nh ni bi translated">更新消费者文件中的<code class="fe mw mx my mz b">df.write</code>功能，使<code class="fe mw mx my mz b">.write</code>后有<code class="fe mw mx my mz b">.option("txnAppId", "idempotent_app").option("txnVersion", batch_id)</code>。</li><li id="0f11" class="na nb iq lq b lr nj lu nk lx nl mb nm mf nn mj nf ng nh ni bi translated">运行带有模拟异常的使用者。</li></ul><p id="c9c4" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">该程序应该如预期的那样失败，并且微批次5将被写入目标增量表。现在可以检查的是目标表的增量日志文件。</p><p id="8077" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">在您喜欢的文本编辑器中打开任何一个目标增量表提交文件，如<code class="fe mw mx my mz b">/tmp/idempotent-writes/target/_delta_log/00000000000000000003.json</code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/7545f8f21f543ae32763b3dada4ae9e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2g8QZNmKKt3tjz5ySiYhyw.png"/></div></div></figure><p id="d924" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">第一行是一个名为<a class="ae kv" href="https://github.com/delta-io/delta/blob/master/PROTOCOL.md#transaction-identifiers" rel="noopener ugc nofollow" target="_blank">事务标识符</a>的delta lake协议动作。正如你所看到的，它包含了我们作为一个选项发送的固定申请Id和一个交易版本，即批次Id。Delta lake在后台收集所有这些json文件(有时还有另一个名为checkpoint的parquet文件，它在每10次提交后被写入)来构成Delta表状态的快照视图。在我们的例子中，状态具有由每个应用程序Id写入的最新版本，在我们的例子中是单个应用程序。因此，在下一次写入时；delta lake可以确定它之前是否看到过相同的写尝试，并基于此可以继续或跳过它。</p><p id="a996" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">理论够了！</p><p id="f0f8" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">移除模拟异常，并再次运行使用者。</p><p id="813e" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">嘣！目标表有100条与源表相同的记录，您甚至可以检查目标表，确保没有重复的记录(id &amp; group列可以充当复合PK)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/4bf4497813c2a6c9df65aba66d3360e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FlAe-HW5f-Eem6gteBNZrg.png"/></div></div></figure><p id="220c" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">这个概念简单又酷，可能在非流式场景中也很有用。</p><p id="2a05" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">交易标识符操作将始终记录在增量日志JSON或parquet文件中。正如我提到的，delta-lake不需要提取完整的版本列表来实现这个逻辑。只有每个应用程序Id的最新条目就足够了。例如，在目标表上执行100个微批处理后，<code class="fe mw mx my mz b">txn</code>操作如何出现在增量日志中。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/e18e3d2889288a26a4a8356f74e7c1ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*se_0l4gnTFg0s08flcdKxg.png"/></div></div></figure><h1 id="468e" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">总结和警告</h1><p id="6a8c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">当使用DataFrameWriter从<code class="fe mw mx my mz b">foreachBatch</code> API写入delta lake表时，这个特性是非常必要的。没有它，重复可能会发生，他们可能很难发现，因为他们只会悄悄地发生。</p><p id="1e47" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">记得清理你的测试数据。</p><pre class="kg kh ki kj gt of mz og oh aw oi bi"><span id="6e04" class="oj kx iq mz b gy ok ol l om on">rm -r /tmp/idempotent-writes</span></pre><p id="cf36" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">警告是:</p><ul class=""><li id="a78b" class="na nb iq lq b lr mo lu mp lx nc mb nd mf ne mj nf ng nh ni bi translated">此功能仅在DataFrameWriter API中可用。它不能用来(a̵s̵ ̵o̵f̵ ̵n̵o̵w̵见下面的更新)运行类似<code class="fe mw mx my mz b">MERGE INTO</code>的命令。类似地，如果写给parquet或JDBC，你需要自己解决幂等问题。</li><li id="3c57" class="na nb iq lq b lr nj lu nk lx nl mb nm mf nn mj nf ng nh ni bi translated">从文档中复制一个警告:</li></ul><blockquote class="mk ml mm"><p id="0650" class="lo lp mn lq b lr mo jr lt lu mp ju lw mq mr lz ma ms mt md me mu mv mh mi mj ij bi translated">如果删除流式检查点并用新的检查点重新开始查询，则必须提供不同的<code class="fe mw mx my mz b">appId</code>；否则，来自重新启动的查询的写入将被忽略，因为它将包含相同的<code class="fe mw mx my mz b">txnAppId</code>，并且批处理ID将从0开始。</p></blockquote><p id="6178" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">当您尝试重新开始时，上述情况适用，但是如果重新创建目标增量表，您选择只删除现有记录。该表在数据方面将是空的，但事务日志仍将具有先前流运行的痕迹，因此将跳过之前看到的所有批处理id。</p><p id="4032" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">在上述情况下，如果您将日志记录级别更改为INFO，您将看到如下消息，指示微批处理被跳过。</p><pre class="kg kh ki kj gt of mz og oh aw oi bi"><span id="5aee" class="oj kx iq mz b gy ok ol l om on">22/07/03 21:36:51 INFO WriteIntoDelta: Transaction write of version 0 for application id idempotent_app has already been committed in Delta table id d5e0a924-683a-45a0-a299-718b89409b6a. Skipping this write.</span></pre><p id="3666" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">这并不是一件坏事，特别是当它发生在恢复失败的流应用的正常情况下，使得最后提交的微批处理不会被重新写入目标增量表时。</p><p id="e4b0" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">希望您现在对如何使用增量表中的幂等写来使您的流工作负载对瞬时故障具有鲁棒性有所了解。</p><p id="6b9f" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">【2023年1月更新:</p><p id="e040" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated"><a class="ae kv" href="https://newreleases.io/project/github/delta-io/delta/release/v2.0.2" rel="noopener ugc nofollow" target="_blank"> Deltalake版本2.0.2 </a>扩展了对所有DML操作的支持。奇怪的是，在2.2.0中没有找到同样的支持。看看<a class="ae kv" href="https://gist.github.com/ylashin/6fbbe5a14106f92fc5fab38f4f6fd63e" rel="noopener ugc nofollow" target="_blank">这个要点</a>就一窥端倪。OSS deltlake 2.0.2提供该功能。我已经在Databricks DBR 12.0上试过了，效果很好，甚至可以在之前DBR的Databricks上使用。</p><p id="ed18" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mr lz ma mb mt md me mf mv mh mi mj ij bi translated">快乐流媒体！</p></div></div>    
</body>
</html>