<html>
<head>
<title>Sparse Autoencoder Neural Networks — How to Utilise Sparsity for Robust Information Encoding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">稀疏自动编码器神经网络——如何利用稀疏性进行鲁棒信息编码</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sparse-autoencoder-neural-networks-how-to-utilise-sparsity-for-robust-information-encoding-6aa9ff542bc9#2022-05-03">https://towardsdatascience.com/sparse-autoencoder-neural-networks-how-to-utilise-sparsity-for-robust-information-encoding-6aa9ff542bc9#2022-05-03</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="8d75" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph">神经网络</h2><div class=""/><div class=""><h2 id="1781" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">用一个详细的Python例子比较欠完整和稀疏AE</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/08a32c0439d28a13075551f3a849e8c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1-gz-jCEEnE4XALIUUKVEQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>创建的稀疏自动编码器(SAE)特色图像。</p></figure><h1 id="dab3" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">介绍</h1><p id="2b54" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">自动编码器使我们能够利用由<strong class="md je">编码器</strong>和<strong class="md je">解码器</strong>组成的神经网络架构来提取信息。自动编码器有多种类型，根据它们的结构或它们要解决的问题而有所不同。四个最常见的是:</p><ul class=""><li id="9ebb" class="mx my iu md b me mz mh na mk nb mo nc ms nd mw ne nf ng nh bi translated"><a class="ae li" rel="noopener" target="_blank" href="/autoencoders-ae-a-smart-way-to-process-your-data-using-unsupervised-neural-networks-9661f93a8509"> <strong class="md je">欠完整自动编码器(AE) </strong> </a> —最基本、最广泛使用的类型，通常被称为自动编码器</li><li id="7f49" class="mx my iu md b me ni mh nj mk nk mo nl ms nm mw ne nf ng nh bi translated"><strong class="md je">稀疏自动编码器(SAE) </strong> —使用稀疏性来创建信息瓶颈</li><li id="b5c8" class="mx my iu md b me ni mh nj mk nk mo nl ms nm mw ne nf ng nh bi translated"><a class="ae li" rel="noopener" target="_blank" href="/denoising-autoencoders-dae-how-to-use-neural-networks-to-clean-up-your-data-cd9c19bc6915"> <strong class="md je">去噪自动编码器(DAE) </strong> </a> —用于去除数据或图像中的噪声</li><li id="51c6" class="mx my iu md b me ni mh nj mk nk mo nl ms nm mw ne nf ng nh bi translated"><a class="ae li" rel="noopener" target="_blank" href="/vae-variational-autoencoders-how-to-employ-neural-networks-to-generate-new-images-bdeb216ed2c0"> <strong class="md je">变分自动编码器(VAE) </strong> </a> —将信息编码到分布中，使我们能够使用它来生成新数据</li></ul><p id="5ba0" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">本文将关注<strong class="md je">稀疏自动编码器(SAE) </strong>，并将它们与<strong class="md je"> </strong>欠完整自动编码器(AE)进行比较。</p><h1 id="d5e4" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">内容</h1><ul class=""><li id="a005" class="mx my iu md b me mf mh mi mk nq mo nr ms ns mw ne nf ng nh bi translated">机器学习算法领域中的SAE</li><li id="d96b" class="mx my iu md b me ni mh nj mk nk mo nl ms nm mw ne nf ng nh bi translated">SAE的结构以及它与不完整AE的不同之处</li><li id="d6f6" class="mx my iu md b me ni mh nj mk nk mo nl ms nm mw ne nf ng nh bi translated">如何使用Keras/Tensorflow在Python中构建SAE</li></ul><h1 id="8ff0" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">机器学习算法领域中的稀疏自动编码器(SAE)</h1><p id="96a5" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">我试图对最常见的机器学习算法进行分类，如下所示。</p><p id="34c1" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">虽然我们经常以<strong class="md je">监督</strong>的方式使用带有标签的训练数据的神经网络，但我们也可以以<strong class="md je">非监督或自我监督的方式</strong>使用它们，例如通过使用<strong class="md je">自动编码器</strong>。因此，我觉得神经网络的普遍性和它们对机器学习的独特方法值得一个单独的类别。</p><p id="df23" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">点击下面<strong class="md je">互动图</strong>中的不同类别，找到SAE和其他类型的自动编码器👇。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="nt nu l"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">机器学习算法分类。由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>创建的互动图表。</p></figure><p id="4f70" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated"><strong class="md je"> <em class="nv">如果你喜欢数据科学和机器学习</em> </strong> <em class="nv">，请</em> <a class="ae li" href="https://bit.ly/3sItbfx" rel="noopener ugc nofollow" target="_blank"> <em class="nv">订阅</em> </a> <em class="nv">获取我的新文章的邮件。如果你不是中等会员，可以在这里</em>  <em class="nv">加入</em> <a class="ae li" href="https://bit.ly/36Mozgu" rel="noopener ugc nofollow" target="_blank"> <em class="nv">。</em></a></p><h1 id="3970" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">SAE的结构以及它与不完整AE的不同之处</h1><p id="0d68" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">自动编码器的目的是有效地编码重要的信息。实现这一点的一个常见方法是创建一个瓶颈，迫使模型保留必要的东西，丢弃不重要的部分。</p><p id="7369" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated"><strong class="md je"> Autoencoder可以通过同时训练编码器和解码器</strong>来区分什么是重要的，解码器的目标是从编码表示中重建原始数据。</p><p id="f2b2" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">下图提供了一个瓶颈位于中间的欠完整自动编码器神经网络的示例。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj nw"><img src="../Images/f749520b83085b670caf4b81d77cc54a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0gDmxAoBF6k3hltZvS76fg.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">欠完整自动编码器架构。图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者、</a>使用<a class="ae li" href="http://alexlenail.me/NN-SVG/index.html" rel="noopener ugc nofollow" target="_blank"> AlexNail的NN-SVG工具</a>创建。</p></figure><p id="3bca" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">同时，SAE的目标与不完全AE的目标相同，但是它实现的方式不同。代替(或除此之外)依赖更少的神经元，SAE使用正则化来加强稀疏性。</p><p id="9020" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">所谓稀疏，我们的意思是同时激活的神经元更少，产生了类似于不完全AE的信息瓶颈。请参见下图。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj nx"><img src="../Images/834b9ed6f82c07f3b65ddcf0a807ba19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cs1t0PLZF4tsnRcY9GTFKA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">稀疏自动编码器(SAE)架构。图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者、</a>使用<a class="ae li" href="http://alexlenail.me/NN-SVG/index.html" rel="noopener ugc nofollow" target="_blank"> AlexNail的NN-SVG工具</a>创建。</p></figure><p id="dcf8" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">请注意，上图仅代表许多潜在设置中的一种。例如，可以有多个隐藏层，或者限制中间(瓶颈)层的神经元数量。</p><p id="824c" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">严重不良事件和不良事件之间的主要区别是规范化。在接下来的Python小节中，我们将看到它在实践中是如何工作的。</p><div class="kt ku kv kw gu ab cb"><figure class="ny kx nz oa ob oc od paragraph-image"><a href="https://solclover.com/membership"><img src="../Images/63320331b74bd98eea6402472b4209ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*qkXay39OnVc2IosW6rkxtw.png"/></a></figure><figure class="ny kx nz oa ob oc od paragraph-image"><a href="https://www.linkedin.com/in/saulius-dobilas/"><img src="../Images/60fb21d1cb2701bfb6b71f61c99403e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*vabxOXtQ4T034N_mscHSmQ.png"/></a></figure></div><h1 id="3084" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated"><strong class="ak">如何使用Keras / Tensorflow在Python中构建稀疏AE</strong></h1><h2 id="6f32" class="oe lk iu bd ll of og dn lp oh oi dp lt mk oj ok lv mo ol om lx ms on oo lz ja bi translated">设置</h2><p id="f70e" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">我们需要以下数据和库:</p><ul class=""><li id="3bb3" class="mx my iu md b me mz mh na mk nb mo nc ms nd mw ne nf ng nh bi translated"><a class="ae li" href="https://www.tensorflow.org/api_docs/python/tf/keras/datasets/mnist/load_data" rel="noopener ugc nofollow" target="_blank"> MNIST手写数字数据</a>(版权由Yann LeCun和Corinna Cortes根据<a class="ae li" href="https://creativecommons.org/licenses/by-sa/3.0/" rel="noopener ugc nofollow" target="_blank">知识共享署名-分享3.0许可证持有</a>；数据来源:<a class="ae li" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">MNIST数据库</a></li><li id="a3b1" class="mx my iu md b me ni mh nj mk nk mo nl ms nm mw ne nf ng nh bi translated"><a class="ae li" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"> Numpy </a>用于数据操作</li><li id="bedc" class="mx my iu md b me ni mh nj mk nk mo nl ms nm mw ne nf ng nh bi translated">用于可视化的<a class="ae li" href="https://matplotlib.org/stable/api/index.html" rel="noopener ugc nofollow" target="_blank"> Matplotlib </a>和<a class="ae li" href="https://seaborn.pydata.org/" rel="noopener ugc nofollow" target="_blank"> Seaborn </a></li><li id="237e" class="mx my iu md b me ni mh nj mk nk mo nl ms nm mw ne nf ng nh bi translated"><a class="ae li" href="https://www.tensorflow.org/api_docs/python/tf" rel="noopener ugc nofollow" target="_blank">神经网络的Tensorflow/Keras </a></li></ul><p id="bd69" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">让我们导入所有的库:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="op nu l"/></div></figure><p id="559f" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">上面的代码打印了本例中使用的包版本:</p><pre class="kt ku kv kw gu oq or os ot aw ou bi"><span id="ea6a" class="oe lk iu or b gz ov ow l ox oy">Tensorflow/Keras: 2.7.0<br/>numpy: 1.21.4<br/>matplotlib: 3.5.1<br/>seaborn: 0.11.2</span></pre><p id="55de" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">接下来，我们加载MNIST手写数字数据并显示前十位数字。我们将在可视化<strong class="md je">中使用数字标签(y_train，y_test ),但不用于模型训练</strong>。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="op nu l"/></div></figure><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj oz"><img src="../Images/42f40b3b126c844198d0dea9e8806428.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5gEa1BA4Ce864MxMOKHtnA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">MNIST数据集的前十位数字。图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="9b3a" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">如您所见，我们在训练集中有60，000张图像，在测试集中有10，000张图像。请注意，它们的尺寸是28 x 28像素。为了使这个例子简单，我们将不使用卷积层，而是将图像展平，这样我们就可以通过密集的全连接层。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="op nu l"/></div></figure><pre class="kt ku kv kw gu oq or os ot aw ou bi"><span id="8da9" class="oe lk iu or b gz ov ow l ox oy">New shape of X_train:  (60000, 784)<br/>New shape of X_test:  (10000, 784)</span></pre><h2 id="1364" class="oe lk iu bd ll of og dn lp oh oi dp lt mk oj ok lv mo ol om lx ms on oo lz ja bi translated"><strong class="ak">构建自动编码器模型</strong></h2><p id="90a3" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">我们将建立两个自动编码器模型，以便更容易地看到欠完整AE和稀疏AE中神经元激活之间的差异。</p><p id="c69f" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">我们从不完全AE开始。以下是需要注意的关键事项:</p><ul class=""><li id="0f74" class="mx my iu md b me mz mh na mk nb mo nc ms nd mw ne nf ng nh bi translated">该模型由5层组成:一个输入层、三个隐层和一个输出层。</li><li id="c6c9" class="mx my iu md b me ni mh nj mk nk mo nl ms nm mw ne nf ng nh bi translated">输入和输出层各包含784个神经元(我们数据的形状)，隐藏层的大小减少到16个神经元。</li><li id="41ab" class="mx my iu md b me ni mh nj mk nk mo nl ms nm mw ne nf ng nh bi translated">我们将在50个时期内训练模型，并绘制损失图(见下文)。</li><li id="b17c" class="mx my iu md b me ni mh nj mk nk mo nl ms nm mw ne nf ng nh bi translated">我们将分离模型的编码器部分，并将其保存到我们的项目目录中。请注意，如果您不打算在以后重用同一个模型，您不需要保留它的副本。</li></ul><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="op nu l"/></div></figure><p id="a68a" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">在组装模型之后，上面的代码输出模型摘要，然后是损失图表。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pa"><img src="../Images/1884f87beed5c68e9f6fa36a36a2c88a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZoC6_q8_25E22L5xQ4ZIPg.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">不完全AE模型总结。图片来自<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>。</p></figure><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pb"><img src="../Images/797e15473af4c92deaae3f45eeb7b9ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1qLGWSwmIOqAJJRRUOW1dw.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">不完全AE模型损失图。图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="8723" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">现在让我们建立一个稀疏的自动编码器模型。为了便于两个模型之间的比较，我们保持参数不变。<strong class="md je">唯一的区别是瓶颈层</strong>增加了L1规则。</p><p id="60a3" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">另外，请注意，我们对<strong class="md je">神经元激活(activity _ regulator)</strong>应用正则化，而不是权重(kernel _ regularizer)。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="op nu l"/></div></figure><p id="9ba1" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">以下是我们SAE模型的总结和模型损失图表:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pa"><img src="../Images/d076ffdddbad5e90b1bdcacd6ff23a40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZaYCahTJ8zLNpftBVesSLw.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">稀疏AE模型总结。图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pc"><img src="../Images/eecd2b28719964d505e838d40cf03f41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iRl-pL0L6FKFNzL9Pa4kLw.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">稀疏声发射模型损失图。图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="f990" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">最后，是时候对比这两款车型了！</p><p id="249f" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated"><em class="nv">旁注，如果你想看看模型如何再现原始图像，你可以参考我的GitHub上的完整Jupyter笔记本代码(文章末尾的链接)。</em></p><p id="0114" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">回想一下之前的内容，不同的输入会导致不同神经元的激活。因此，让我们从MNIST测试集中选择一个数字来代表大范围的输入。请注意，我们需要将数组从784重新调整为(28 x 28)以显示图像。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="op nu l"/></div></figure><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pd"><img src="../Images/e86bb5b2ccc486ba2cad6d1e211d3768.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M64UApv47JmN7mXlsG4lbg.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">所选的十位数用于模型比较。图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="876f" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">现在让我们使用每个型号(AE和SAE)的编码器部分对上述图像进行编码。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="op nu l"/></div></figure><p id="eb65" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">编码后，我们绘制一个热图，显示瓶颈中的哪些神经元被激活(≠ 0)。行是神经元，而列是数字(0-9)。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="op nu l"/></div></figure><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pc"><img src="../Images/9372fb8cb2e7e7fa1682a567ae28aace.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EBv3Ykqwa42RxELQ8DBf4w.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">两种声发射模型中的神经元激活。图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="a0ce" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">我们可以从上面的图表中看到，我们的SAE模型更加稀疏，不活跃神经元的数量是不完全AE的三倍。</p><h1 id="50ce" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated"><strong class="ak">结束语</strong></h1><p id="746d" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">我希望我的文章为您提供了SAE和AE之间的有益比较。但是，当然，最好的学习方法是通过实践，所以请随意参考我的Python代码，可以在我的<a class="ae li" href="https://github.com/SolClover/Art050_NN_SAE" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>上找到。</p><p id="8937" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated"><strong class="md je">如果你想在我发表新文章时得到通知</strong>，你可以<a class="ae li" href="https://bit.ly/3sItbfx" rel="noopener ugc nofollow" target="_blank">订阅接收电子邮件</a>或在<a class="ae li" href="https://www.linkedin.com/in/saulius-dobilas/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与我联系。</p><p id="3e02" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated"><strong class="md je">如果你不是中级会员</strong>并且想继续阅读成千上万伟大作家的文章，你可以使用我下面的个性化链接加入:</p><div class="pe pf gq gs pg ph"><a href="https://bit.ly/3J6StZI" rel="noopener  ugc nofollow" target="_blank"><div class="pi ab fp"><div class="pj ab pk cl cj pl"><h2 class="bd je gz z fq pm fs ft pn fv fx jd bi translated">通过我的推荐链接加入Medium索尔·多比拉斯</h2><div class="po l"><h3 class="bd b gz z fq pm fs ft pn fv fx dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pp l"><p class="bd b dl z fq pm fs ft pn fv fx dk translated">solclover.com/membership</p></div></div><div class="pq l"><div class="pr l ps pt pu pq pv lc ph"/></div></div></a></div><p id="3af9" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">如果您有任何问题或建议，请随时联系我们！</p><p id="7c78" class="pw-post-body-paragraph mb mc iu md b me mz ke mg mh na kh mj mk nn mm mn mo no mq mr ms np mu mv mw in bi translated">干杯！🤓<br/> <strong class="md je">索尔·多比拉斯</strong></p></div></div>    
</body>
</html>