<html>
<head>
<title>K-means Clustering and Principal Component Analysis in 10 Minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-means聚类和主成分分析10分钟</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-clustering-and-principal-component-analysis-in-10-minutes-2c5b69c36b6b#2022-07-26">https://towardsdatascience.com/k-means-clustering-and-principal-component-analysis-in-10-minutes-2c5b69c36b6b#2022-07-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="af5d" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习</h2><div class=""/><div class=""><h2 id="735e" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">通过实践案例研究演练</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/1657ad219faf832a2e16692462663921.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HVEz7KwzO0tv1Q4d"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Julian Hochgesang 在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="2eb5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated">这里有两种主要的机器学习模型:有监督的和无监督的。在监督学习中，你有<strong class="lk jd"> <em class="mn">输入数据</em> </strong> <em class="mn"> X </em>和<strong class="lk jd"> <em class="mn">输出数据</em> </strong> <em class="mn"> y </em>，然后模型从<em class="mn"> X </em>到<em class="mn"> y </em>找到一个<strong class="lk jd"> <em class="mn">映射</em> </strong>。在无监督学习中，你只有<strong class="lk jd"> <em class="mn">输入数据</em> </strong> <em class="mn"> X </em>。无监督学习的目标各不相同:在<em class="mn"> X </em>中对观察值进行聚类，降低<em class="mn"> X </em>的维度，在<em class="mn"> X </em>中进行异常检测等。</p><p id="9fb0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于监督学习已经在本系列的<a class="ae lh" rel="noopener" target="_blank" href="/understanding-3-classical-machine-learning-models-once-and-for-all-part-1-32a1ac52c0fd"> <strong class="lk jd"> <em class="mn">第一部分</em> </strong> </a>和<a class="ae lh" rel="noopener" target="_blank" href="/k-nearest-neighbors-naive-bayes-and-decision-tree-in-10-minutes-f8620b25e89b"> <strong class="lk jd"> <em class="mn">第二部分</em> </strong> </a>中进行了广泛的讨论，所以本故事将重点讨论无监督学习。在许多其他无监督的学习任务中，您将遇到k-means聚类和主成分分析(PCA)的降维。</p><pre class="ks kt ku kv gt mo mp mq mr aw ms bi"><span id="abee" class="mt mu it mp b gy mv mw l mx my"><strong class="mp jd">Table of contents</strong></span><span id="bada" class="mt mu it mp b gy mz mw l mx my"><strong class="mp jd">· </strong><a class="ae lh" href="#ba75" rel="noopener ugc nofollow"><strong class="mp jd">K-means Clustering</strong></a><br/><strong class="mp jd">· </strong><a class="ae lh" href="#74ac" rel="noopener ugc nofollow"><strong class="mp jd">Principal Component Analysis (PCA)</strong></a><br/>  ∘ <a class="ae lh" href="#b83e" rel="noopener ugc nofollow">Noise</a><br/>  ∘ <a class="ae lh" href="#8ec4" rel="noopener ugc nofollow">Redundancy</a><br/>  ∘ <a class="ae lh" href="#9c11" rel="noopener ugc nofollow">Case Study</a><br/><strong class="mp jd">· </strong><a class="ae lh" href="#2d76" rel="noopener ugc nofollow"><strong class="mp jd">Conclusion</strong></a></span></pre><h1 id="ba75" class="na mu it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">k均值聚类</h1><p id="add7" class="pw-post-body-paragraph li lj it lk b ll nr kd ln lo ns kg lq lr nt lt lu lv nu lx ly lz nv mb mc md im bi translated"><a class="ae lh" href="https://en.wikipedia.org/wiki/Cluster_analysis" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> <em class="mn">聚类</em> </strong> </a>的思想是对观察值进行分组，使得同一组内的观察值彼此相似(或相关)，而与其他组内的观察值不同(或无关)。有许多算法可以实现这一点，其中之一是k-means。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/3e55b2c447fc1e864879ce6e5fc40dcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oxGbUN-TRImGWAdM"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">梅尔·普尔在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="31cf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">K-means是一种基于质心的聚类算法，其工作原理如下。</p><ol class=""><li id="452c" class="nx ny it lk b ll lm lo lp lr nz lv oa lz ob md oc od oe of bi translated">随机初始化:随机放置k个形心。</li><li id="eb51" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">分类指定:根据到质心的距离，将每个观测值指定到最近的分类。</li><li id="48ef" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">质心更新:将质心移动到同一个聚类的平均观测值。</li><li id="2f68" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">重复步骤2和3，直到达到收敛。</li></ol><p id="296b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">用户在实现k-means聚类时必须确定两个超参数:质心数量<em class="mn"> k </em>，以及所使用的距离度量(通常是<a class="ae lh" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> <em class="mn">欧几里德距离</em> </strong> </a>或<a class="ae lh" href="https://en.wikipedia.org/wiki/Taxicab_geometry" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> <em class="mn">曼哈顿距离</em> </strong> </a>)。在这个故事中，我们将使用欧几里德距离，它是由公式定义的</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/78c6d71332315b4cc008a6b5032a509e.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/0*a0LVIRZOGvRGzqsu.png"/></div></figure><p id="ab40" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<em class="mn"> x </em> = ( <em class="mn"> x₁ </em>，<em class="mn"> x₂ </em>，…，<em class="mn"> xₙ </em>)和<em class="mn"> y </em> = ( <em class="mn"> y₁ </em>，<em class="mn"> y₂ </em>，…，<em class="mn"> yₙ </em>)为维度为<em class="mn"> n </em>的观测值。因此，现在我们只需要确定<em class="mn"> k </em>。</p><p id="f057" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于我们希望观察值组中的观察值彼此相似，因此好的聚类是平方和(WSS)最小的聚类，也就是说，每个观察值到其所属聚类的质心的二次距离之和最小。</p><p id="ff95" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">此外，由于一个组中的观察值需要不同于其他组中的观察值，所以好的聚类具有更大的平方和(BSS)。这里，BSS只是从每个质心到全局平均值的二次距离之和，由每个聚类中的观察值数量加权。</p><p id="0d90" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">数学上，WSS和盲源分离公式如下</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi om"><img src="../Images/91efadda7efaeb3828d050afa07c971a.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/0*5qmcIJoi8I5rXqfk.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/4eee8be85306fcd44c75718761d6d279.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/0*Va3kqrA_VgdyADx5.png"/></div></figure><p id="0333" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在哪里</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/340f21ab477c51af97d777c0c3e82bc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/0*VE3gt1EY1N7GwhUd.png"/></div></figure><p id="969e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">很容易看出，较高的k值对应于较低的WSS。然而，<em class="mn"> k </em>必须从商业的角度来确定，或者更客观地使用所谓的肘方法。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi op"><img src="../Images/31aa267c79e8a7c221f60c31137a2d49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KWAfI508w-Nqmkk6.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用肘法确定k |图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="29f0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">肘方法说我们应该选择<em class="mn"> k </em>，在那里增加它将不会导致WSS更明显的减少。因此从上面的图中，例如，我们选择<em class="mn"> k </em> = 2或<em class="mn"> k </em> = 3。</p><p id="119e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下面是一个逐步k-means聚类过程的例子，用<em class="mn"> k </em> = 3和“<strong class="lk jd"> × </strong>”表示质心。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oq"><img src="../Images/d8bd6256553efa4eab483e29b781b2ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yNRUpXXFz-p5D1ZCfQhtVQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">k-means聚类的逐步过程。它从质心的<strong class="bd or">随机初始化</strong>开始，并对<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a>的每个迭代1、2和3 |图像进行<strong class="bd or">聚类分配</strong>和<strong class="bd or">质心更新</strong></p></figure><p id="7402" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">给定足够的时间，k-means总会收敛。然而，收敛可能是局部最小值。这高度依赖于质心的初始化。因此，计算通常要进行几次<strong class="lk jd"/>，质心的初始化不同。</p><p id="c745" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">帮助解决这个问题的一个方法是<strong class="lk jd"> k-means++ </strong>初始化方案。这将初始化质心(通常)彼此远离，可能会比随机初始化产生更好的结果。</p><p id="8e63" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">K-means是<em class="mn">而不是</em>总是聚类的好选择。由于距离选择和k-means聚类所做的一些隐含假设，一些数据可能会利用k-means弱点。下图显示了其中的一些场景。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oq"><img src="../Images/f64622455381a16e2c872dfd4afc6753.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8VzVRvWJw89hxnbk7-a5_A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">数据不符合k-means作出的一些隐含假设(<strong class="bd or">选择k </strong>、<strong class="bd or">各向同性聚类</strong>、<strong class="bd or">等方差</strong>)，结果产生了不需要的聚类|图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="0c80" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，首先对数据进行转换总是一个好的做法。还有很多其他的聚类算法来解决这个问题，比如谱聚类、凝聚聚类、ward层次聚类、DBSCAN、BIRCH等等。</p><h1 id="74ac" class="na mu it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">主成分分析</h1><p id="f203" class="pw-post-body-paragraph li lj it lk b ll nr kd ln lo ns kg lq lr nt lt lu lv nu lx ly lz nv mb mc md im bi translated">PCA的目标是识别最有意义的基础来重新表达数据。希望这个新的基础能过滤掉噪音，揭示隐藏的结构。PCA问:有没有另一个基，是原始基的<em class="mn">线性组合</em>，最好的重新表达你的数据？</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/75d349f28c76b85eb19fa4e4172d3a5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*54WmnYHBk0ABEAsX"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@sigmund?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">西格蒙德</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="eb8d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">设<em class="mn"> X </em>是一个有<em class="mn"> m </em>个观测值和<em class="mn"> n </em>个特征的数据集，所以<em class="mn"> X </em>是一个<em class="mn"> m </em> × <em class="mn"> n </em>矩阵。为了简化计算，我们将<em class="mn"> X </em>标准化，使其平均值为零。</p><p id="fc06" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">设<em class="mn"> Y </em>为<em class="mn"> X </em>的新表示，另一个<em class="mn"> m </em> × <em class="mn"> n </em>矩阵通过线性变换<em class="mn"> P </em>与<em class="mn"> X </em>相关(其本身就是一个<em class="mn"> n </em> × <em class="mn"> n </em>矩阵)。PCA会找到将<em class="mn"> X </em>转换成<em class="mn">Y</em>T58】线性的<em class="mn"> P </em>，也就是说，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/b27bf81f4b56a16ea453c3a0e28b3d69.png" data-original-src="https://miro.medium.com/v2/resize:fit:152/format:webp/1*IkmfHHpMToxAgiydyLaHxg.png"/></div></figure><p id="3556" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从这个方程中我们可以看出，<em class="mn"> P </em>的列是一组新的用于表示<em class="mn"> X </em>中观测值(行)的基向量，称为<em class="mn"> X </em>的<strong class="lk jd"> <em class="mn">主成分</em> </strong>。现在，什么是基<em class="mn"> P </em>的好选择？我们希望<em class="mn"> Y </em>展示什么特征？要回答这些问题，考虑两个角度:<em class="mn">噪声</em>和<em class="mn">冗余</em>。</p></div><div class="ab cl ou ov hx ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="im in io ip iq"><h2 id="b83e" class="mt mu it bd nb pb pc dn nf pd pe dp nj lr pf pg nl lv ph pi nn lz pj pk np iz bi translated">噪音</h2><p id="8e9f" class="pw-post-body-paragraph li lj it lk b ll nr kd ln lo ns kg lq lr nt lt lu lv nu lx ly lz nv mb mc md im bi translated">记住，在<em class="mn"> Y </em>中，我们希望过滤掉噪声。衡量噪声的标准是信噪比(SNR)，或方差比</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/b77a624700d2ef4a36e7663eaef32ab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:240/format:webp/1*Ai-gukznKmh6OUgq24zo0A.png"/></div></figure><p id="10b5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">高信噪比(≫1)表示高精度数据，而低信噪比表示高噪声数据。我们假设感兴趣的维度存在于具有最大方差和大概最高SNR的方向上。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oq"><img src="../Images/a13df117604720b10b9d29d1fe8b3a71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JM9NywlOELscODd-eje7qg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">信号和噪声的变化用对着数据云的两条线来表示|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><h2 id="8ec4" class="mt mu it bd nb pb pc dn nf pd pe dp nj lr pf pg nl lv ph pi nn lz pj pk np iz bi translated">裁员</h2><p id="423d" class="pw-post-body-paragraph li lj it lk b ll nr kd ln lo ns kg lq lr nt lt lu lv nu lx ly lz nv mb mc md im bi translated">假设你的数据是二维的，有特征<em class="mn"> x </em> ₁和<em class="mn"> x </em> ₂.考虑下面的三个情节。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pm"><img src="../Images/7c2adf642dd8d8cb5f93a0ea3b288ba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A_B04raOtpFQwr8ouxCYDA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">一系列可能的数据冗余。最右侧图中的特征高度相关，表明冗余度高。|图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="06d6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果可以从<em class="mn"> x </em> ₁计算出<em class="mn"> x </em> ₂，我们就说数据具有高冗余度(反之亦然)。如果是这种情况(如上面最右边的图)，其中一个特征是冗余的，可以在不牺牲太多信息的情况下减少数据维数。事实上，这是降维背后的中心思想。</p></div><div class="ab cl ou ov hx ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="im in io ip iq"><p id="cbb2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从这两个角度来看，PCA都需要特征的<em class="mn">方差</em>(用于最大化信号)和特征对的<em class="mn">协方差</em>(用于最小化冗余)<em class="mn"> X </em>。这正是<em class="mn"> X </em>的协方差矩阵的用途，根据定义可以写成</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/16ce7e5f734f98d64c5bd05f3f04ceb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/1*pJ_6i6rQlgPS0B_1ynW62A.png"/></div></figure><p id="11f5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="mn"> Cₓ </em>是一个<em class="mn"> n </em> × <em class="mn"> n </em>矩阵，对角项代表特征的方差，非对角项代表特征对的协方差，反映了我们数据中的噪声和冗余:</p><ol class=""><li id="b134" class="nx ny it lk b ll lm lo lp lr nz lv oa lz ob md oc od oe of bi translated">根据假设，对角线上的大值对应于<strong class="lk jd">有趣的结构</strong>。</li><li id="29d0" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">在非对角线术语中，大幅度对应于高<strong class="lk jd">冗余</strong>。</li></ol><p id="e489" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">设<em class="mn"> Cᵧ </em>为<em class="mn"> Y </em>的协方差矩阵。那么当使用<em class="mn"> P </em>将<em class="mn"> X </em>转化为<em class="mn"> Y </em>时，优化后的<em class="mn"> Cᵧ </em>会是什么样子呢？</p><ol class=""><li id="3593" class="nx ny it lk b ll lm lo lp lr nz lv oa lz ob md oc od oe of bi translated">在<em class="mn"> Y </em>中的每个连续维度应该根据方差进行排序，以突出<strong class="lk jd">有趣的结构</strong>。</li><li id="bc37" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">在<em class="mn"> Cᵧ </em>中的所有非对角线项应该为零。因此，<em class="mn"> Cᵧ </em>一定是对角矩阵，在<em class="mn"> Y </em>中没有<strong class="lk jd">冗余</strong>。</li></ol><p id="59ea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有很多方法可以对角化<em class="mn"> Cᵧ </em>。对于PCA，它假设<em class="mn"> P </em>是一个<a class="ae lh" href="https://en.wikipedia.org/wiki/Orthogonal_matrix" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> <em class="mn">正交矩阵</em> </strong> </a>。现在，常设仲裁法院的任务已经简化为:</p><blockquote class="po"><p id="0749" class="pp pq it bd pr ps pt pu pv pw px md dk translated">在<em class="py"> XP </em> = <em class="py"> Y </em>中找一些正交矩阵<em class="py"> P </em>使得<em class="py">cᵧ</em>=<em class="py">yᵀy</em>/(m1)是对角矩阵。<em class="py"> P </em>列是<em class="py"> X </em>的<strong class="ak"> <em class="py">主成分</em> </strong>。</p></blockquote><p id="06f9" class="pw-post-body-paragraph li lj it lk b ll pz kd ln lo qa kg lq lr qb lt lu lv qc lx ly lz qd mb mc md im bi translated">由于<em class="mn"> Cₓ </em>是对称的，所以存在矩阵<em class="mn"> D </em>和<em class="mn"> E </em>使得<em class="mn"> Cₓ </em> = <em class="mn"> EDEᵀ </em>，其中<em class="mn"> D </em>是对角矩阵，<em class="mn"> E </em>是按列排列的<em class="mn"> Cₓ </em>的特征向量矩阵。而且，<em class="mn"> Cₓ </em>和<em class="mn"> D </em>有相同的特征值。由于<em class="mn"> D </em>是对角矩阵，其特征值就是其主对角线的元素。</p><p id="237d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">选择<em class="mn">P</em>≦<em class="mn">E</em>，然后</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/7f3884176ce91c9a8ffb7bff2aaafef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*jQFeVCbCyx80nCYYxiSQew.png"/></div></figure><p id="cb30" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">总而言之，到目前为止，我们已经成功地找到了一个矩阵<em class="mn"> P </em>，其中它的列是<em class="mn"> X </em>的协方差矩阵<em class="mn"> Cₓ </em>的<strong class="lk jd">特征向量。矩阵<em class="mn"> P </em>满足<em class="mn"> XP </em> = <em class="mn"> Y </em>使得<em class="mn"> Y </em>的协方差矩阵<em class="mn"> Cᵧ </em>为对角矩阵，其中对角线由<em class="mn"> Cₓ </em>的<strong class="lk jd">特征值</strong>组成。所以，<em class="mn"> Y </em>中没有<strong class="lk jd">冗余</strong>。</strong></p><p id="332c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了确保<em class="mn"> Y </em>中的每个连续维度根据方差进行排序，以便突出显示<strong class="lk jd">有趣的结构</strong>，您可以对<em class="mn"> D </em>中<em class="mn"> Cₓ </em>的<strong class="lk jd">特征值</strong>进行重新排序，以便更大的特征值位于左上角(当然也可以通过重新排列<em class="mn"> P </em>的列和<em class="mn"> Pᵀ </em>的行来满足<em class="mn"/></p><p id="8df7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">PCA可以通过只选择重排后<em class="mn"> P </em>的前几列进行降维。要选择的列数是PCA的超参数。列数越多意味着原始数据中解释的差异越多<em class="mn"> X </em>。这对于可视化、汇总大量数据或查找有区别的特征非常有用。</p><p id="46eb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">经过这么长时间的推导，我们终于能够编写PCA算法了:</p><ol class=""><li id="484e" class="nx ny it lk b ll lm lo lp lr nz lv oa lz ob md oc od oe of bi translated">将原始数据<em class="mn"> X </em>标准化为零均值，其中<em class="mn"> X </em>是一个<em class="mn"> m </em> × <em class="mn"> n </em>矩阵。</li><li id="9704" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">计算<em class="mn"> X </em>的协方差矩阵<em class="mn"> Cₓ </em>。</li><li id="e46e" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">计算<em class="mn"> Cₓ </em>的特征值和对应的特征向量。</li><li id="b039" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">将<em class="mn"> Cₓ </em>的特征值按降序排序，并将<em class="mn"> Cₓ </em>的特征向量按降序排序。</li><li id="83e0" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">选择前<em class="mn"> k </em>个特征向量，排列成<em class="mn"> P </em>列。这样，<em class="mn"> P </em>就是一个<em class="mn"> n </em> × <em class="mn"> k </em>矩阵。</li><li id="9cec" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">通过<em class="mn"> Y </em> = <em class="mn"> XP </em>计算缩减后的数据<em class="mn"> Y </em>。因此，<em class="mn"> Y </em>是一个<em class="mn"> m </em> × <em class="mn"> k </em>矩阵。</li></ol><h2 id="9c11" class="mt mu it bd nb pb pc dn nf pd pe dp nj lr pf pg nl lv ph pi nn lz pj pk np iz bi translated">个案研究</h2><p id="16c4" class="pw-post-body-paragraph li lj it lk b ll nr kd ln lo ns kg lq lr nt lt lu lv nu lx ly lz nv mb mc md im bi translated">在本案例研究中，您将使用<a class="ae lh" href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> <em class="mn">虹膜数据集</em> </strong> </a>。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="qf qg l"/></div></figure><p id="89d0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该数据集包含3类，每类50个观察值，其中每类涉及一种鸢尾植物(鸢尾、杂色鸢尾或海滨鸢尾)。根据植物萼片和花瓣的测量结果，数据集中有四个特征。</p><p id="8f11" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您希望在散点图中显示数据集。然而，这是不可能的，因为不存在所谓的四维散点图。因此，您从数据集中提取尽可能多的信息以获得二维表示，并为此使用PCA。</p><p id="ea42" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="mn"> X </em>是一个150 × 4的矩阵，其特征方式如下。</p><pre class="ks kt ku kv gt mo mp mq mr aw ms bi"><span id="34e0" class="mt mu it mp b gy mv mw l mx my">sepal length (cm)    5.843333<br/>sepal width (cm)     3.057333<br/>petal length (cm)    3.758000<br/>petal width (cm)     1.199333</span></pre><p id="97fb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于每个特征，从每个观察值中减去相应的平均值。现在你应该有了这个以零为中心的数据集。姑且称之为<em class="mn"> X_meaned </em>。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="qf qg l"/></div></figure><p id="4969" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="mn"> X_meaned </em>的协方差矩阵是一个4 × 4矩阵</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/5dc004be184b85f9ea3742c2a0059ebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*R1_YCq43UGfKqwZKI6tR-g.png"/></div></figure><p id="aa98" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，<em class="mn"> Cₓ </em>有四个特征值和相应的特征向量。他们是</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/15387985a7155d7fbb275b72179b87d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*DYZWuFToYjFgE6-vRQNFeA.png"/></div></figure><p id="bf27" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">根据特征值降序排列特征向量。既然你想要一个<em class="mn"> X </em>的二维表示，那就挑两个特征值最大的特征向量，表示可以提取的最大信息。他们是v₄的<em class="mn">和v₃的</em>。在考虑顺序的同时，将它们按列排列。你会得到一个4 × 2的线性变换矩阵</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qj"><img src="../Images/90771bfba1157ea51b3ccc172835d4e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*sNNx-F_EHxWSd14oKEloig.png"/></div></figure><p id="b913" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">简化后的数据<em class="mn"> Y </em>可以通过乘以<em class="mn"> X_meaned </em>和<em class="mn"> P </em>来计算。你得到<em class="mn"> Y </em>如下。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="qf qg l"/></div></figure><p id="f920" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有了这个双特征数据集，您现在可以如下可视化虹膜数据集的本质。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qk"><img src="../Images/58b625c3c627dc661126375e5d1d797b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dYOgLjrrrNgOFAv_23mt6A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">虹膜数据集前两个主要成分的散点图|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><h1 id="2d76" class="na mu it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">结论</h1><p id="02d8" class="pw-post-body-paragraph li lj it lk b ll nr kd ln lo ns kg lq lr nt lt lu lv nu lx ly lz nv mb mc md im bi translated">您已经非常详细地学习了<strong class="lk jd"> K均值聚类</strong>并应用了<strong class="lk jd">主成分分析</strong>进行可视化。现在，您不仅可以使用已建立的库来构建它们，还可以自信地知道它们是如何从内到外工作的，使用它们的最佳实践，以及如何提高它们的性能。</p><p id="dbfd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">恭喜你。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ql"><img src="../Images/0eaead0192ced3ac34bc59165e48e869.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rgWJKdbV_nrsaYg1"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">马库斯·斯皮斯克在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div><div class="ab cl ou ov hx ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="im in io ip iq"><p id="788d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">🔥<em class="mn">谢谢！如果你喜欢这个故事，想支持我这个作家，可以考虑</em> <a class="ae lh" href="https://dwiuzila.medium.com/membership" rel="noopener"> <strong class="lk jd"> <em class="mn">成为会员</em> </strong> </a> <em class="mn">。每月只需5美元，你就可以无限制地阅读媒体上的所有报道。如果你注册使用我的链接，我会赚一小笔佣金。</em></p><p id="f7bc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">🔖想知道更多关于梯度下降和许多其他优化器的工作原理吗？继续阅读:</p><div class="qm qn gp gr qo"><div role="button" tabindex="0" class="ab bv gv cb fp qp qq bn qr lb ex"><div class="qs l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qt qu fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qt qu fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----2c5b69c36b6b--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="qx qy gw l"><h2 class="bd jd wb nx fp wc fr fs wd fu fw jc bi translated">从零开始的机器学习</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi we au wf wg wh sr wi an eh ei wj wk wl el em eo de bk ep" href="https://dwiuzila.medium.com/list/machine-learning-from-scratch-b35db8650093?source=post_page-----2c5b69c36b6b--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wm l fo"><span class="bd b dl z dk">8 stories</span></div></div></div><div class="rk dh rl fp ab rm fo di"><div class="di rc bv rd re"><div class="dh l"><img alt="" class="dh" src="../Images/4b97f3062e4883b24589972b2dc45d7e.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*CWNoicci28F2TUQc-vKijw.png"/></div></div><div class="di rc bv rf rg rh"><div class="dh l"><img alt="" class="dh" src="../Images/b1f7021514ba57a443fe0db4b7001b26.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*wSRsSHYnIiGJFAqC"/></div></div><div class="di bv ri rj rh"><div class="dh l"><img alt="" class="dh" src="../Images/deb73e42c79667024a46c2c8902b81fa.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HVEz7KwzO0tv1Q4d"/></div></div></div></div></div><div class="qm qn gp gr qo"><div role="button" tabindex="0" class="ab bv gv cb fp qp qq bn qr lb ex"><div class="qs l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qt qu fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qt qu fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----2c5b69c36b6b--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="qx qy gw l"><h2 class="bd jd wb nx fp wc fr fs wd fu fw jc bi translated">我最好的故事</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi we au wf wg wh sr wi an eh ei wj wk wl el em eo de bk ep" href="https://dwiuzila.medium.com/list/my-best-stories-d8243ae80aa0?source=post_page-----2c5b69c36b6b--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wm l fo"><span class="bd b dl z dk">24 stories</span></div></div></div><div class="rk dh rl fp ab rm fo di"><div class="di rc bv rd re"><div class="dh l"><img alt="" class="dh" src="../Images/0c862c3dee2d867d6996a970dd38360d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*K1SQZ1rzr4cb-lSi"/></div></div><div class="di rc bv rf rg rh"><div class="dh l"><img alt="" class="dh" src="../Images/392d63d181090365a63dc9060573bcff.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*hSKy6kKorAfHjHOK"/></div></div><div class="di bv ri rj rh"><div class="dh l"><img alt="" class="dh" src="../Images/f51725806220b60eccf5d4c385c700e9.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HiyGwoGOMI5Ao_fd"/></div></div></div></div></div><p id="12cc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[1]主成分分析教程(<a class="ae lh" href="https://arxiv.org/abs/1404.1100" rel="noopener ugc nofollow" target="_blank">arxiv.org/abs/1404.1100</a>)</p><p id="5f86" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[2]<a class="ae lh" href="https://scikit-learn.org/stable/modules/clustering.html" rel="noopener ugc nofollow" target="_blank">sci kit-学习用户指南</a></p></div></div>    
</body>
</html>