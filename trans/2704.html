<html>
<head>
<title>go_backwards() — Unravelling its ‘hidden’ secrets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">go _ backward()—揭开它“隐藏”的秘密</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lstm-go-backwards-unravelling-its-hidden-secrets-ed094952b5cc#2022-06-10">https://towardsdatascience.com/lstm-go-backwards-unravelling-its-hidden-secrets-ed094952b5cc#2022-06-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="def4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解其隐藏的细微差别&amp;探索其泄漏的性质！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/73b6462219bbd1c85aa726bc2fc18cf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DbzpEg77lJr5r_ZKgXfALw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">LSTM细胞的表现|图片作者:Christopher Olah </p></figure><h1 id="0dc2" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">简介</strong></h1><p id="5f5f" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">长短期记忆(LSTM)是递归神经网络(RNN)的高级版本，顾名思义，能够在相对较长的序列上存储“上下文”。这使得它们成为NLP任务的完美工具，如文档分类、语音识别、命名实体识别(NER)等。</p><p id="23a9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在许多应用中，如机器翻译、语音识别等。，<strong class="lq ir">来自两边的上下文</strong>提高了基于语言的模型的性能。为了在实践中实现这一点，我们使用双向LSTMs。然而，如果您需要分别提取向前和向后运行的嵌入，您将需要实现两个独立的lstm，其中第一个接收向前的输入，另一个以向后的方式处理输入。</p><p id="690a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了节省您的大量精力，Tensorflow的LSTM让您可以灵活地正常使用give输入，同时在内部以相反的方式处理它，从右到左学习上下文！这是通过将<strong class="lq ir"><em class="mp">go _ backward()= True</em></strong><em class="mp"/>放入LSTM层来实现的。简而言之，如果您的输入是['I '，' am '，' the '，' author']，那么LSTM将从右向左读取，这要感谢go_backwards()，并将输入视为['author '，' the '，' am '，' I']。这允许模型学习双向上下文！</p><p id="cdc3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">到目前为止一切顺利。然而，这里有一个警告！当您在管道中添加正向和反向上下文时，您往往会在这个过程中泄漏大量信息，这可能会导致模型在评估期间给出大约100%的准确性！！这是因为我们没有理解go _ backwards()的真正、微妙的实现。</p><p id="f4f1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在这篇博客中，我使用一个真实的NLP示例来揭示go _ backward()的真正实现，并探索go _ backward()的预期行为的微小变化如何会导致模型评估期间的大量泄漏！</p><h1 id="e857" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">任务:下一个单词预测</h1><p id="2f5b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">考虑下一个单词预测任务，其中基于当前输入，模型需要预测下一个单词。向后的方向接收，比方说，原始句子索引2处的单词，所以它需要预测索引1处的单词！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/423376c01b9acf2ac1678dbc8fb6dacd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K0RbmpxduZ_8R5-HvCkqHg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者对LSTM |图像反向的输入输出表示</p></figure><h1 id="bf31" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">模型架构</h1><p id="13ee" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">让我们考虑下面的架构。我们有两个独立的输入，一个用于LSTMs的正向，另一个用于反向。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/003b4ea65b43151c9838c54d873ea6a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yrKQT9Ur-Hanvw-kkSy7jg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">手头任务的双向LSTMs表示:下一个单词预测|作者图片</p></figure><p id="255a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们创建或利用预先训练的嵌入，如Word2Vec、Glove等。对于每个单词，通过LSTMs传递它们，通过密集层传递嵌入并将其与LSTM1输出相加来使用跳过连接，通过另一组LSTM层传递总和，最后通过一个公共的Softmax层来获得整个词汇中最可能的预测！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/be46a6ed4417598c92cf7fdb0a372414.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5AVF6wtc8ywuxApU9EOJmg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者为任务选择的模型架构|图片</p></figure><h1 id="3a25" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">观察</h1><p id="c5c3" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">上述模型在IMDB训练数据集上训练超过75个时期，具有适当的批量大小、学习速率和实现的早期停止。由于后者，模型训练停止了大约35个时期。你应该注意到从向后的LSTM层输出的惊人结果！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/ff0f0a2a0c0172277e452d0d60aa72eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*taa-tXmluWTnWGHqWUfpiQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">前向和后向LSTM图层的验证精度|图片由作者提供</p></figure><p id="d91c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">你注意到倒数第二排的100%准确率了吗！！！很疯狂，对吧？这意味着模型已经“完美地学习”了正确的模式，根据输入给出输出。但是下一个单词预测并不是一个可以完美学习的任务。这就像说对未来的预测有可能100%准确。😮</p><p id="ec9e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">但是我们知道，在模型中的某个地方，一定会发生一些泄漏，而之前已经暗示了输出。这是我在AI大学AI-3课程(NLP)中做项目时意识到的。这个发现是这篇文章的主要目的——找出这种奇怪行为背后的原因！</p><h1 id="7a9e" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">关于go _ backwards()的一些隐秘的事情？：</h1><p id="1f0c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">LSTM的前进方向通过文件大部分是清楚的。但是，go _ backwards()函数似乎有点棘手。如果你看一下它的文档，你会注意到它正向获取输入序列，在内部反转，然后处理它，最后给出<strong class="lq ir">反转</strong>序列。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/2074979ec5f2fa242321b48d68ca5ab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eyQmZ0eig3S0znXvX614eg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">根据Tensorflow 中的<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM" rel="noopener ugc nofollow" target="_blank"> LSTM文档使用go _ backward()|来源:</a><a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM" rel="noopener ugc nofollow" target="_blank"> Tensorflow LSTM </a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mx"><img src="../Images/9c63a1171aa3a227bba31a069be707a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F0ScQWu9T-xWYq74lP8gNw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者对后退图片的直观应用</p></figure><p id="8dba" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然而，与我们的预期不同，在LSTMs中颠倒顺序并不那么简单。根据go _ backwards()的<a class="ae kv" href="https://github.com/keras-team/keras/blob/v2.9.0/keras/layers/rnn/lstm.py#L346-L813" rel="noopener ugc nofollow" target="_blank">官方文档</a>，<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/reverse_sequence" rel="noopener ugc nofollow" target="_blank"> tf.reverse_sequence( </a>)是用于反转输入的函数，这不是简单的反转。该函数不是从右到左排列，而是只反转由“序列长度”决定的记号的数量，并保持句子的其余部分不变，因为其他记号是填充记号，不需要反转！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mx"><img src="../Images/9c63a1171aa3a227bba31a069be707a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F0ScQWu9T-xWYq74lP8gNw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自LSTM的输出，带有go_backward()=True |作者图片</p></figure><p id="a2aa" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">参数go _ backward()= True意味着LSTM层将接受输入，使用tf.reverse_sequence策略对其进行反转，通过LSTM层找到输出，最后将填充的标记放在前面！！这个最终的反转是使用<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/reverse" rel="noopener ugc nofollow" target="_blank"> tf.reverse() </a>完成的。</p><h1 id="2773" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">引擎盖下:第一LSTM</h1><p id="492b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">所以这就是当你在第一个LSTM层<strong class="lq ir">中把go _ backwards()参数设为真时的情况。请注意，我们假设LSTM是一个身份映射器，即输出作为输入。这样做是为了便于表示和理解。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/3846f4ccdedc58f5431444a4285732fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gIcryxo-Doqjem8nlngGrg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">第一个反向LSTM层的工作|由作者制作的图像</p></figure><p id="6602" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果您仔细观察，就会发现go _ backward()= True的LSTM图层的输出正在执行文档中提到的任务。最终的输出实际上是我们期望的(假设为真)输出的真实反转(字面意义)</p><p id="d196" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">但是，我们没有反转这个输出，而是直接将其用于跳过连接，并在管道中使用is！这就是事情变糟的地方！</p><h1 id="4fdf" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">集中精力解决问题！</h1><p id="0db5" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在我们的模型中，我们将输出(实际上是xD以上的反向输出)添加到密集输入，然后将其传递到下一个反向LSTM。让我们用上面的例子做一次预演，看看第二个LSTM层<strong class="lq ir">和你从LSTM 1:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/a80d2477354452d46ec2dc644135ff20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GF1U3rwFaIdeYPxIe1eNQg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">添加了带有密集图层输出的输入|作者提供的图像</p></figure><p id="ca56" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">您是否注意到输出是如何对称的！！即使我们对加法进行加权，去掉对称性，你也逃不出LSTM2！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/e6917a935a7605479799c89d729d29df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FzdyvLA8HpvdW4qC1auCvQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">第二层反向LSTM层|图片由作者制作</p></figure><p id="2e48" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在假设PAD=0，<s> =1，A=2，B=3，C=4，D=5，</s> =6(为简单起见，用数字表示)，则输入为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/80d363c834f72e37d193f074e871fab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0m67Wr0B4QcsukGOWUuYbA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">按作者呈现输出|图像</p></figure><p id="3fa6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">给定第0行的输入，对于LSTM这样的神经网络来说，找到输出中的模式有多难？😌这就是第二个LSTM层非常快赶上并达到接近100%验证准确性的原因！！</p><p id="2d91" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">问题的根源是go _ backward()= True的LSTM的输出实际上是<strong class="lq ir">反转</strong>，如文档中所述。在通过之前你需要翻转它！</p><h1 id="5342" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">实验</h1><p id="8023" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我手动翻转了两个LSTM图层的输出，在轴=1上使用go _ backward()= True，并在75个时期内获得了两个图层约44%的准确性，并提前停止。后来，我在用于情感分类的IMDB数据集上观察了三种不同的迁移学习实现的以下结果:</p><p id="9836" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">1.基线:85.75% <br/> 2。Word2Vec: 84.38% <br/> 3。ELMo:80.17%[使用来自嵌入层的嵌入、第一LSTM正向和反向运行的级联输出以及第二LSTM的级联输出。</p><p id="aa50" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">(大约50个时期，提前停止)</p><div class="kg kh ki kj gt ab cb"><figure class="nc kk nd ne nf ng nh paragraph-image"><img src="../Images/528b10fada244100369ababd59b310e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*lMLAEHu8Szw2Y3tazNy1Gg.png"/></figure><figure class="nc kk nd ne nf ng nh paragraph-image"><img src="../Images/01c3c03c1914ef27aabb0a7a1432830f.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*Uf1_deHN1m1j8IKUrGysSw.png"/><p class="kr ks gj gh gi kt ku bd b be z dk ni di nj nk translated">训练时模型的丢失和验证准确性|图片由作者提供</p></figure></div><p id="93a9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果你有兴趣详细了解LSTM的内幕，请点击以下链接:</p><div class="nl nm gp gr nn no"><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener  ugc nofollow" target="_blank"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd ir gy z fp nt fr fs nu fu fw ip bi translated">了解LSTM网络</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">2015年8月27日发布人类不是每秒钟都从零开始思考。当你读这篇文章时，你…</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">colah.github.io</p></div></div></div></a></div><h1 id="b02e" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">这个故事的寓意！</h1><blockquote class="nx ny nz"><p id="f2f1" class="lo lp mp lq b lr mk jr lt lu ml ju lw oa mm lz ma ob mn md me oc mo mh mi mj ij bi translated">在实施之前，请务必仔细阅读文档！🤓<br/>在使用工具之前，先了解它们！:)</p></blockquote></div></div>    
</body>
</html>