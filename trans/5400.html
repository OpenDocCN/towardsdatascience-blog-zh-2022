<html>
<head>
<title>Understanding Diffusion Probabilistic Models (DPMs)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解扩散概率模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-diffusion-probabilistic-models-dpms-1940329d6048#2022-12-05">https://towardsdatascience.com/understanding-diffusion-probabilistic-models-dpms-1940329d6048#2022-12-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e508" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">逐步建立导致 DPMs 的推理。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ca9aa929d7a5da8941868e29a9ee1124.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZVkvVjHsetaoXYX9oFdTLQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">致谢:<a class="ae kv" href="https://www.instagram.com/eyeluminations/" rel="noopener ugc nofollow" target="_blank">劳拉·安哥特</a>和菲利普·罗卡(经许可使用)</p></figure><p id="c5ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章是与<a class="ls lt ep" href="https://medium.com/u/20ad1309823a?source=post_page-----1940329d6048--------------------------------" rel="noopener" target="_blank">巴蒂斯特·罗卡</a>共同撰写的。</p><p id="6569" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lu">所表达的观点和意见仅属于作者个人，不代表其雇主的观点或意见。除非另有说明，所有图片均由作者提供。</em></p><h1 id="3a8d" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">介绍</h1><p id="c4fe" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf mp lh li lj mq ll lm ln mr lp lq lr ij bi translated">对复杂概率分布建模是机器学习中的一个中心问题。如果这个问题可以出现在不同的形状下，一个最常见的设定是如下:给定一个只能由一些可用样本描述的复杂概率分布，如何生成一个新的样本？</p><p id="20b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有的生成模型都依赖于相同的基本思想，即把简单分布转换成我们感兴趣的复杂分布。换句话说，假设一个随机变量 X 遵循一个简单的分布(例如高斯分布)，我们在寻找一个函数 G，使得 G(X)遵循我们的目标分布。当然，函数 G 很复杂，无法显式给出。一种可能的方法是用将从可用数据中学习的神经网络来模拟 G。然后，我们讨论深度学习生成模型，如 VAEs、GANs 或 DPMs，它们都依赖于不同的机制来定义和训练这个模拟 g 的生成网络。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/cc87b4908eed1a4fbb4a0c481402d790.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sh6AfbN9m7fmrJNAh5lsTA@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">生成模型旨在学习一个从简单分布中获取数据并将其转换为复杂分布中的数据的函数。</p></figure><p id="4032" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">早在 2013 年，金玛和韦林就推出了可变自动编码器(VAEs)。简而言之，VAEs 的思想是用正则化的潜在空间训练自动编码器。然后，正则化编码器被迫向接近高斯的分布编码数据，而解码器从潜在空间重构数据。这样，我们最终得到一个解码器，它可以作为我们的函数 G，从高斯分布中获取数据，并从原始分布中生成一个新数据。更多细节可以在<a class="ae kv" rel="noopener" target="_blank" href="/understanding-variational-autoencoders-vaes-f70510919f73">我们之前关于 VAEs </a>的文章中找到。</p><p id="c4b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一年后的 2014 年，伊恩·古德菲勒(Ian Goodfellow)引入了生成对抗网络(GANs)。简而言之，其思想是假设如果生成网络 G 从目标分布中产生样本，那么这些样本应该与真正可用的样本不可区分。因此可以推导出 GANs 所依赖的对抗性训练的概念。生成网络 G 被训练成接受随机输入(例如，来自高斯分布)并输出来自目标分布的数据。一个鉴别网络 D 被训练来区分真实数据和生成的数据。两个模型同时训练，在竞争中变得更好(G 试图愚弄 D，D 试图不被 G 愚弄)。在这个训练过程中，G 必须总是产生更有说服力的数据来愚弄 D，换句话说，按照预期，这些数据遵循目标分布。更多细节可以在<a class="ae kv" rel="noopener" target="_blank" href="/understanding-generative-adversarial-networks-gans-cd6e4651a29">我们之前关于 GANs </a>的文章中找到。</p><p id="4401" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果说深度学习生成模型已经受到很多关注很多年了，那么随着一些能够产生出色结果的图像或视频生成模型的出现，它们在最近几年(甚至几个月)受到了更多的关注。在解决视频生成问题之前，大型技术公司一个接一个地首先发布了图像生成模型(来自开放人工智能的<a class="ae kv" href="https://arxiv.org/pdf/2204.06125.pdf" rel="noopener ugc nofollow" target="_blank"> DALL-E 2 </a>，来自谷歌的<a class="ae kv" href="https://imagen.research.google/" rel="noopener ugc nofollow" target="_blank"> Imagen </a>，来自 Meta 的<a class="ae kv" href="https://ai.facebook.com/blog/greater-creative-control-for-ai-image-generation/" rel="noopener ugc nofollow" target="_blank"> Make-A-Scene </a>)。尽管存在差异，这些宣布该领域光明前景的伟大模型都依赖于相同的基本概念:扩散概率模型(DPMs)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/cb847ffed153d76ffd78cce27a084055.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QRCziwTgvzdvn3idjjqehg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Make-A-Scene、Imagen、DALL-E 2 是能够从文本提示生成高质量图像的扩散概率模型。上面的例子是由 Meta Make-A-Scene model 生成的，它从文本提示和基本草图中生成图像，以实现更高水平的创造性控制。</p></figure><p id="39fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">扩散概率模型最早出现在 2015 年，Sohl-Dickstein 的一篇论文中，并由于使用它们取得的巨大成果而获得了越来越多的关注。基本上，DPMs 的思想是学习一个明确定义的随机过程的逆过程，该过程逐渐破坏信息，从我们复杂的目标分布中提取数据，并将它们转化为简单的高斯分布。然后，这种反向过程被期望采取相反方向的路径，将高斯噪声作为输入，并从感兴趣的分布生成数据。</p><p id="ab3d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">事不宜迟，让我们一起(重新)发现 DPMs 吧！</p><h2 id="7ebc" class="mu lw iq bd lx mv mw dn mb mx my dp mf lf mz na mh lj nb nc mj ln nd ne ml nf bi translated">概述</h2><p id="1790" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf mp lh li lj mq ll lm ln mr lp lq lr ij bi translated">在本文中，我们将集中讨论深入理解 DPM 是由什么组成的所需的主要思想。在第一部分，我们将回顾一些与随机过程相关的概念，并介绍什么是扩散过程。然后，是第二部分，我们将给出理解扩散概率模型的主要直觉。特别是，我们将看到扩散过程如何被用来逐步破坏信息，以及我们如何能够学会逆转这一过程，从噪音中逐步产生信息。在第三部分，我们将把我们的直觉形式化，并给出扩散模型的数学描述。最后，在最后一节中，我们将在这个理论框架的基础上，看看扩散模型在实践中是如何训练的。特别是，我们将结束循环，看看训练和采样的实际实现如何很好地表达我们给出的第一个直觉。请注意，尽管我们将讨论他们依赖的许多基本概念，但我们不会进入前面提到的特定模型的细节(Make-A-Scene、Imagen、DALL-E 2 等。)这将是未来帖子的主题。</p><blockquote class="ng nh ni"><p id="283d" class="kw kx lu ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated"><strong class="ky ir">注意:</strong>这篇文章的某些部分比其他部分更注重数学。如果所有的部分不是完全相互独立的，我们尽可能让读者跳过一些部分。介绍扩散过程的部分和给出扩散模型的数学定义的部分可以跳过，因为其他两个部分包含与扩散模型相关的大多数直觉和实际考虑。</p></blockquote></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><h1 id="64c2" class="lv lw iq bd lx ly nt ma mb mc nu me mf jw nv jx mh jz nw ka mj kc nx kd ml mm bi translated">什么是扩散过程？</h1><p id="f666" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf mp lh li lj mq ll lm ln mr lp lq lr ij bi translated">顾名思义，扩散概率模型在某种程度上与扩散过程相关。让我们从头开始，看看什么是扩散过程，它有什么性质。当然，由于这不是这篇文章的核心主题，我们将不得不做一些简化，以保持这篇文章足够短，同时仍然给出基本的想法。</p><p id="624a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">扩散过程是具有连续样本路径的连续时间马尔可夫随机过程。让我们打破这个相当尴尬的定义，以更好地了解我们在谈论什么。随机过程是随机变量的集合，通常用正整数来表示</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/c6a03ad7997f80ed3dcfa775636a7589.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*PdlfJ8c2VgQIcGzksGdKHA@2x.png"/></div></figure><p id="58e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们接着谈离散时间随机过程，或由正实</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/6bb0a789423db6e52e618c5aa7f1e977.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*XFoJuBumSujfaP1-RtxBjg@2x.png"/></div></figure><p id="27db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们讨论连续时间随机过程。类似于一个简单的随机变量被称为样本的实现，我们称随机过程的实现为样本路径或轨迹。对于一个连续时间随机过程，我们说它有连续的样本路径，如果所有可以随机观察到的可能轨迹都是连续的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/8bb6ebfdedc735d361babb59e34a15b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tylt97QP9a--7DuorwcDeQ@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">不同类型的随机过程，在时间和空间上离散或连续。</p></figure><p id="d5ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，马尔可夫过程是一个没有记忆的随机过程。这意味着根据现在和过去的过程的未来行为只取决于现在。过去的信息可以被丢弃，因为它没有提供任何额外的信息。换句话说，过程的未来状态不取决于我们如何到达现在的状态，而仅仅取决于现在的状态。数学上，它可以写成</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/2d32e5d6306be9dcb50b16f2b1089e8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YF5-3TggDOQP90Rs8L-dpg@2x.png"/></div></div></figure><p id="7c2b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">任何扩散过程都可以用随机微分方程(SDE)来描述，它可以写成以下形式</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/c015643c0544229fce6961832999cb98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*Bli4A3gZ2WslTk3DUHMs8w@2x.png"/></div></figure><p id="5d6b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中 a 称为漂移系数，σ称为扩散系数，W 为维纳过程。维纳过程更广为人知的名称是布朗运动。粗略地说，总和的右边部分，与维纳过程相关，是使这个微分方程“随机”的原因。如果没有这第二项，我们将处理一个简单的微分方程。维纳过程通过独立的高斯增量带来一些(连续的)随机性</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/4a0b70b516bef2caae85e6fb180cee2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*7DL2vWFf5_ZWd7BRFd893w@2x.png"/></div></figure><p id="19d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们把微分方程离散化一点，让事情更容易理解，我们得到</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/770149476f71fd397a8b69b986bea4b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7WSOW8IlhQS7kqFJYAhMlQ@2x.png"/></div></div></figure><p id="66d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那也可以写成</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/31c459a3d703235cc1ba4b5b2b06f939.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YRxrqsfDKm5Xyhh4wmsQyQ@2x.png"/></div></div></figure><p id="5a9e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们可以看到，X_{t+dt}的值是 X_t 加上确定性漂移项和由方差与扩散系数平方成比例的正态随机变量定义的随机扩散项的值。因此，扩散系数表示要应用的随机性的强度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/bbc67d42679ccebc60d188e042d6f5cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cM5iFbe1xw_Ll4jwUzU5rQ@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">不同随机微分方程和相关(非随机)微分方程的扩散过程样本。我们可以看到，漂移项给出了一个趋势，而扩散项带来了一些随机性。扩散系数越高，随机性越大。</p></figure><p id="f75c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，让我们以扩散过程的一个有趣的性质作为结论，这个性质在本文后面会很有用。如果 X_t 是一个扩散过程</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/59c6d76187696e78865917e24596a6ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*XZzuMkRvGh98pDv8DB_Wig@2x.png"/></div></figure><p id="e09a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那么逆时过程 X̄_t = X_{T-t}也是具有相同函数形式的扩散过程，由下面的随机微分方程描述</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/c0d93ae40a8c3503b73294e8920b215e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BLRZFvgL9MyOQqDHYQAnDg@2x.png"/></div></div></figure><p id="f2e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中 p(X_t)定义了 X_t 的边际概率</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/cd27c8dc7e5a62295aea908c31a04fbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*JNNDIwW8bKXOnnKb8KRa8Q@2x.png"/></div></figure><p id="4154" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">被称为得分函数。</p></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><h1 id="95fc" class="lv lw iq bd lx ly nt ma mb mc nu me mf jw nv jx mh jz nw ka mj kc nx kd ml mm bi translated">扩散模型的直觉</h1><p id="f5fd" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf mp lh li lj mq ll lm ln mr lp lq lr ij bi translated">通过一些精心选择的系数，扩散过程具有逐渐破坏任何相关信息的效果。事实上，具有“收缩”漂移系数(|a|&lt;1)和非零扩散系数的扩散会逐渐将来自任何复杂分布的数据转变为来自简单各向同性高斯噪声(协方差矩阵可对角化的高斯，意味着所有维度都是独立的高斯)的数据。让我们首先用一个简单的 1D 例子来说明这一点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/228691f16604795f1e41042fef4bcc37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OZdIjKDwd44J1XcYP-KW5Q@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">扩散过程具有逐步破坏信息的效果。在这个 1D 的例子中，任何初始分布都将被转换成高斯分布(渐近地)。</p></figure><p id="9b4c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在 1D 的例子中，我们将“离散扩散过程”定义为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/b3486a2e29cfacad12675edd79e5b062.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SbavC1zaz_l7lABObvD1kQ@2x.png"/></div></div></figure><p id="c8d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们说，对于任何 X_0，这个过程最终将趋向于高斯分布。让我们给出一个非正式的例子来证明这个事实，它也将证明我们系数中的平方根是正确的。经过一定数量的步骤 T 后，我们可以写</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/a1051341a9214a3621133e6beed7a8ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fhjpJb4bHtWF7BOuuAXdyA@2x.png"/></div></div></figure><p id="8666" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中所有高斯分布都是独立的，这意味着第二项可以表示为单个高斯分布，其方差是所有方差的和。对于足够大的步数，我们有</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/4568c41ff44e97370836f2be9d348df9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aF-pdIRVwSj34w9Yz9gRMQ@2x.png"/></div></div></figure><p id="521e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们看到，第一项变得可以忽略不计，方差之和趋向于 1，这意味着在我们的 1D 例子中，对于任何起始点，我们都趋向于标准高斯分布。</p><p id="1a12" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当然，同样的想法可以应用于更高维度。例如，大小为 100*100 的图像的分布是非常复杂的高维分布，其可以逐渐转变为相同维度的非常简单的分布:100*100 各向同性噪声。在这种情况下，扩散过程将以类似于我们在 1D 例子中描述的方式进行，但是它将针对每个像素的每个 RGB 通道进行。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/88b0bd8214c0e683bb8be548b4b56fd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3O-VN8DhsgeceT5cku_klw@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">扩散过程可以被定义为将像图像这样的高维数据变成各向同性的高斯噪声。</p></figure><p id="549e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">到目前为止，我们知道我们可以使用扩散从复杂的分布到简单的各向同性高斯噪声。从我们在本文开头讨论的内容中，可以看出我们对相反的过程更感兴趣:从简单的发行版到复杂的发行版。最重要的是，从复杂分布到简单分布并不困难，因为我们可以简单地随机将复杂分布中的任何一点投影到简单分布中，因为我们知道如何从最后一个分布中进行采样。</p><p id="505f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那么这里使用扩散的真正目的是什么呢？简单的答案是，它为我们提供了一种渐进和结构化的方法，从复杂的分布到各向同性的高斯噪声，这将简化反向过程的学习。让我们对最后一点有个直觉。</p><p id="cb34" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们提到的，将任何给定的图像随机投影到各向同性的高斯噪声中是非常容易的。相反，逆转这个过程是非常困难的:看着一些给定的噪声，我们不能从先前的图像中说太多，因为没有结构可以依赖。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/ba6ac8e2e4796d8942ca01395644ad78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uPHSz-w6FPBOdy-127oyiQ@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">一次从纯噪声到高质量样本可能是一项相当复杂的任务。</p></figure><p id="4893" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们假设这些图像中的每一个都不是直接的，而是通过扩散过程渐进地投影到各向同性高斯噪声中。看最后一步，基本和之前得到的一样是噪音，还是不能多说。然而，看看中间步骤，我们显然有一些线索来指导我们扭转这一进程。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/61421f01e9f8c0c52d15b632c4f86588.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2P96DRATykIZKII2hfX07Q@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">从纯噪声到高质量样本的渐进过程使得我们可以依靠中间步骤来减轻生成任务。</p></figure><p id="6ff2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里重要的一点是:中间步骤是力量扩散模型所依赖的。在这一点上，人们确实可以观察到，尽管扩散过程是渐进进行的，但它最终会从初始复杂分布中提取任何数据点，并将其从简单分布中转化为随机点。所以，最后，既然我们可以一步完成从复杂到简单的发布，为什么还要麻烦中间的步骤呢？与其学习一个多步逆向过程，为什么不学习一个大的逆向过程，它是多步过程的展开版本？难道我们不能认为这是完全一样的吗？</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/1966fd57db26f8dde87f68e45fdc9182.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jj322DaWKZCiKoZEzCnojg@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">乍一看，人们可能会认为学习 T 个小模型或者一个由这些小模型组成的大模型是非常相似的。</p></figure><p id="6942" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果有人想坚持这种推理，事实是将过程分解成中间步骤会带来两件事。首先，所有的逆向步骤彼此之间并不完全不同，这意味着在实践中，我们不必为每个逆向步骤学习完全不同的模型，而是学习一个通用模型，该模型只是将该步骤作为调整其行为的参数。它极大地减少了要学习的模型的大小。其次，除了有更多的参数要学习之外，单次展开的版本更难训练，因为它需要从纯噪声到最终的高质量数据，这使得任务和梯度下降学习更加复杂。相反，渐进模型不需要一次处理全部的复杂性，并且可以在添加越来越精细的信息之前，在早期阶段首先引入粗略的信息。中间步骤在某种意义上也可以被看作是一种引导训练过程使之更容易的方法。</p><p id="5db8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以把扩散模型的主要思想总结如下。我们定义了一个渐进地破坏信息的扩散过程，并学习了期望恢复信息的相反过程。这个反向过程也是渐进的，我们可以依靠这个事实来提高学习效率。在实践中，与不能依赖于这些有价值的中间步骤的一次性模型相比，建模和训练确实可以利用要学习的过程的迭代性质。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/282ab7346dce68e86d7426368bb80a63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TctuhV2q14tY8t9SpT_Hng@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">渐进逆向过程的学习可以利用问题的迭代结构，而直接模型则不能。明显的缺点是采样需要多个步骤，因此时间更长。</p></figure><p id="b4a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">退一步说，我们可以观察到，整体思想与 VAEs 所做的并没有那么远:编码器以结构化的方式将初始复杂分布转换为简单分布，以便能够学习采用相反方向的解码器。DPMs 和 VAEs 之间的主要区别如下。首先，编码器是一个多步骤过程，而不是单步编码器。第二，编码器是完全固定的，并且不需要学习，因为要学习的解码器(逆过程)将直接依赖于扩散过程带来的结构，而不是依赖于在训练期间学习的结构。第三，潜在空间具有与输入完全相同的维数，与 VAE 的潜在空间相反，其维数比编码输入低几个数量级。最后，学习解码器是扩散过程的逆过程，因此是多步骤过程，而不是单次传递函数。</p><p id="f9e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">DPMs 的明显缺点是采样需要多个步骤，这意味着生成过程将比 gan 或 VAEs 更长，因为这类模型需要一次通过。这一根本差异引发了许多问题。假设参数数量相等，当第一个输出通过多次(因此花费更多时间)时，比较 DPMs 输出与 GANs 和 VAEs 输出是否完全公平？如果 DMPs 的力量来自于其生产过程的进步性，那么尝试提取 DMPs 又有多大意义呢？我们不会在这里回答这些问题，但它们肯定是值得思考的有趣问题。</p></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><h1 id="c471" class="lv lw iq bd lx ly nt ma mb mc nu me mf jw nv jx mh jz nw ka mj kc nx kd ml mm bi translated">扩散模型的数学</h1><p id="5745" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf mp lh li lj mq ll lm ln mr lp lq lr ij bi translated">现在我们已经建立了直觉，让我们给出一个更正式的 DPMs 数学公式。尽管扩散模型的连续和离散时间公式都存在，我们将在本文中集中讨论后者。这意味着我们将假设正向扩散过程和反向扩散过程已经被离散成有限数量 T 个步骤。注意，对于正向和反向过程，用高斯转移概率核而不是扩散过程来讨论马尔可夫链更准确，但思想是相同的。</p><p id="bd61" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们用 x_0 来表示我们要从中取样的分布的数据，并用 q(x_0)来表示分布本身。我们将具有高斯转移概率(扩散核)的正向过程定义如下</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/fcb520ad3ddde1395ab8daf44a32671c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*6iQl5zaAqZTUXY_FHLgfZA@2x.png"/></div></figure><p id="e4e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中，β_t 表示每一步要保留的上一步信息与要添加的新噪声之间的权衡。我们也可以写</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/ac359c75910c5aa5010e1fc3d010872b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AfHVBiZ2NY9rcWaEO1701Q@2x.png"/></div></div></figure><p id="ad73" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这里我们可以清楚地看到离散的扩散过程。我们可以很容易地用一个简单的递归论点表明，链中的任何一步都可以根据下式从 x_0 直接生成</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/27ee17f60969c788bbfa3b4cadfae327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*QBYL-EXegfNcZbJlk745jg@2x.png"/></div></div></figure><p id="e1b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在哪里</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/8c9b538ad68cbfa1d7b74339eb481ee9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9GacBu0Sc1BVajCaZbv50A@2x.png"/></div></div></figure><p id="115f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据马尔可夫性质，我们可以将一个给定的前向轨迹的概率写成</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/6a50bb69c4d7b6707f46d1d1ed25cc54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*9ptBlOflUJjMw8Lz7e_nWA@2x.png"/></div></figure><p id="7998" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">a 我们在前一小节中描述过，我们的目标是学习这个正向扩散过程的逆向过程，或者更确切地说，这个正向马尔可夫链的逆向链。我们之前提到，在漂移和扩散系数的一些假设下(这里满足)，扩散过程的逆过程也是相同函数形式的扩散过程(具有相同的扩散系数，但我们稍后将回到这一点)。通过足够小的时间步长(大 T，意味着小β_t ),我们可以对我们正在寻找的反向链进行近似，该反向链也应该是具有高斯转移概率的马尔可夫链。相反的过程</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/ff9d3ef6564398a13f7f50e378ba0a2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*TOZ9nlJEue_Q2-MzzdLV9w@2x.png"/></div></figure><p id="ccae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可以近似计算为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/16275bba5614b9ee1c492e02f5bb56a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8NfjaASf5hLyPtyiLqO3lg@2x.png"/></div></div></figure><p id="c847" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中 _θ和σ_θ是以θ为参数的两个待学习函数。使用马尔可夫特性，给定后向轨迹的概率可以近似为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/6f2d4f3b90383716f8a8b4d77e4e6aa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*RduWp7fu5Gan8pOp6EkZBQ@2x.png"/></div></figure><p id="fc57" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中 p(x_T)是不依赖于θ的各向同性高斯分布</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/4ba81e8fc90844c60b06d8b25ab40204.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*HkOO6gxS2ONlAqQCchZw_w@2x.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/3d55331cf329b19d9d566aed28d30b9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pD8YAacERsM1i8NWBit7Bw@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数学设置的插图。</p></figure><p id="9f67" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">既然我们已经定义了我们的正向过程并模拟了它的反向过程，那么大的问题就来了。我们如何学习 _θ和σ_θ参数？要优化的损耗是多少？我们从逆过程中主要期望的是 p_θ(x_0)接近 q(x_0)。换句话说，在反向过程的最终采样步骤之后，生成数据的分布应该与目标分布相同。因此，逆过程的学习在于找到最小化 q(x_0)和 p_θ(x_0)之间的 KL 散度的 _θ和σ_θ，或者等价地最小化 p_θ(x_0)在 q(x_0)下的负对数似然。从数学上讲，我们希望找到最小化的 _θ和σ_θ</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/37faf02a4f5538cb1b875141622da4c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6pbDJAAjlflmBCXFP09T4A@2x.png"/></div></div></figure><p id="b36e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个阶段，还不清楚中间步骤在哪里起作用，但很快就会起作用。让我们首先稍微修改一下括号中的表达式。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oy"><img src="../Images/49dac6c16bf8ea14fac7289613c2091f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Gbg5cn90x8GChLaQpGF1A@2x.png"/></div></div></figure><p id="2643" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，对数函数是凹的，我们从詹森不等式</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oz"><img src="../Images/d157a7f2e832af3ac3df3f3e26427212.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YHhytTwRVsMK4b_WhtQMTw@2x.png"/></div></div></figure><p id="04a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以，我们可以定义一个 L 的上限</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oy"><img src="../Images/c668fe4c9abbecfd084bd99745ed9d14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3rpX_tEJVSsFbREmBJGUJA@2x.png"/></div></div></figure><p id="5a3a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">代替最小化 L，我们可以最小化 L 的上界，这更容易处理。在推导的这一点上，至少从理论的角度来看，我们已经清楚如何通过从正向过程迭代采样并调整参数以最小化上限中的负对数比来学习反向过程的参数。然而，让我们做最后的努力，把推导推得更远一点，以获得这个上界的更方便的形式。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/446f2b9e15ec43fb468f913af4916063.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GR4HN4T_VusQnMcqkieVng@2x.png"/></div></div></figure><p id="caa0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">利用我们有的贝叶斯定理</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/bcf842e55598e21be45b0cbbb3b4db34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sX1Pc8c_YrAVbe20ewHWJQ@2x.png"/></div></div></figure><p id="84a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pc"><img src="../Images/586b2123bb9f91acfac64ef6529ff7b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YfzEgzLzHnvqZYz5fBjryg@2x.png"/></div></div></figure><p id="f44c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一项不依赖于 _θ和σ_θ，因此在优化过程中不需要考虑。最后一项非常简单明了，也不难优化。最后，剩下的项是高斯分布之间的 KL 散度。事实上，我们已经看到了这一点</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/bf73d029e1629ec5e720fceb2b4881aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OQT704gHO6rmq97Ho6e7-g@2x.png"/></div></div></figure><p id="9a58" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以证明这一点</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pd"><img src="../Images/27abc8426f9047aa10319874b87a2449.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*A8-ApfKN_lhLATG5MIfF3g@2x.png"/></div></div></figure><p id="4429" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在哪里</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pe"><img src="../Images/8bc02629abb661a9ac6cca45f86147b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qDlNCAjFW__Loy9BXwGIJQ@2x.png"/></div></div></figure><p id="8bb0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">具有封闭形式的 2 个高斯分布之间的 KL 散度，我们将在下一节中看到，该上限定义了对于训练过程要最小化的非常易处理的损失函数。</p></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><h1 id="070f" class="lv lw iq bd lx ly nt ma mb mc nu me mf jw nv jx mh jz nw ka mj kc nx kd ml mm bi translated">实践中的扩散模型</h1><p id="6a1e" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf mp lh li lj mq ll lm ln mr lp lq lr ij bi translated">到目前为止，我们已经定义了一个正向过程 q，作为我们的(离散的)扩散过程，逐步破坏信息</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pf"><img src="../Images/43e4667d169e21974c0398dd7afd3cde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nTdWLcqJRgEDMcscgkz7Ng@2x.png"/></div></div></figure><p id="6a50" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以及反向过程 p_θ，模拟要学习的反向过程，并假定逐步恢复信息</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/f0bb93f126ccfaa8a5acbb29d7d167ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wubn5Om_OnwqMuZ5iBuzbg@2x.png"/></div></div></figure><p id="799d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们已经看到，训练我们的反向过程在于找到最小化上限的 _θ和σ_θ</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pg"><img src="../Images/5c667df74672f4e187285dab8091a05c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D1Uydjp6zdphkv4rsj01Gg@2x.png"/></div></div></figure><p id="8b06" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这一点上，值得注意的是，如果我们忽略要学习的反向模型的参数在步骤之间共享，则上限表达式中的不同项是彼此独立的。因此，假设我们的模型有足够的容量，最小化整个上限类似于最小化它的每一项。</p><p id="0482" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们做一些假设，看看如何在实践中进行培训。首先，为了使事情变得简单，我们可以决定将反向过程的方差设置为一个未经训练的时间相关常数，这样</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ph"><img src="../Images/ba666309d1668d90a6782faefa854056.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q3hfg7MdwiZCtg_WcGKBbA@2x.png"/></div></div></figure><p id="d6c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一个选项与 q(x_{t-1} | x_t)的方差相匹配，假设时间步长足够小(大 T，意味着小β_t ),因此，逆向过程具有与正向过程相同的“扩散系数”。第二个选项匹配 q 的方差(x_{t-1} | x_t，x_0)。实际上，研究似乎表明这两种选择给出了相似的结果。我们现在假设第二种选择。</p><p id="496a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它只让我们用一个单一的模型来学习反向过程的意义。提醒一下</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pi"><img src="../Images/49391f1db1927df42a60d1082010cc58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*XTfbRMIxbQy37DlZqHCPpQ@2x.png"/></div></div></figure><p id="9c86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用两个高斯光束的 KL 散度的封闭形式，可以证明</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pj"><img src="../Images/1d3bb369d93a74659b097318276c36d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q7EPMVp4-U0nZ6JOc5br4w@2x.png"/></div></div></figure><p id="519a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们可以看到，上限的每个 KL 散度项对应于一个给定的时间步长，其优化简单地包括最小化模型和以 x_0 为条件的反向过程的均值之间的 L2 距离，两者都在所考虑的时间步长进行评估。</p><p id="8de5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果这个公式已经很方便了，那么通常更倾向于根据噪声重新确定模型的参数。首先，我们注意到</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pk"><img src="../Images/c5099686fa4f77108b6febdbaa1fdc84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t6aBB4XYNJdxdjr0dyRh8w@2x.png"/></div></div></figure><p id="02b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这意味着</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pl"><img src="../Images/2c613bb9689643ff14f5ff1e7718d8b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jzd2QwrOowZAvAsqI3HnSQ@2x.png"/></div></div></figure><p id="a8ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我们可以通过定义来重新确定模型的参数</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pm"><img src="../Images/665338ca3e63cf94aadd7aa1f1976f74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ygsqFMEF7pSEQ2NFgcr0A@2x.png"/></div></div></figure><p id="9368" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以便</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pn"><img src="../Images/28c648f5058fd1cbf02de8593c93500f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hu2pukwof79kIPRqETeE8w@2x.png"/></div></div></figure><p id="0fce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有了这个新的参数化，我们不再学习代表逆过程的均值的模型，而是学习代表已经被添加到 x_0 中以获得 x_t 的噪声的模型。现在，上界的每个 KL 散度项的最小化包括最小化模型和噪声之间的 L2 距离。</p><p id="58ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">到目前为止，我们已经忽略了我们想要最小化的上界中的最后一项</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi po"><img src="../Images/80e5c55133f58ad851fc02ddd8f66ad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*ySLKZ-SV8WUsY8Oo9yFlUg@2x.png"/></div></figure><p id="f0f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管需要注意以适当的方式处理，但根据所选的参数化，该项也可以很好地用 _θ(x_1，1)或ε_θ(x_1，1)来表示。最后，整个表达式相对于模型参数θ是可微的，并且可以通过最小化该上限来完成训练。</p><p id="8503" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，如果精确的上限可用于训练，则建议使用以下更简单的变体<a class="ae kv" href="https://arxiv.org/pdf/2006.11239.pdf" rel="noopener ugc nofollow" target="_blank"/>，并在实践中经常使用</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pp"><img src="../Images/c1020692c3baef1c99604a5aaaacd748.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PvgsUAXTd140mwcdpw6bxg@2x.png"/></div></div></figure><p id="985a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该变型定义了要实现的更简单的损失函数，并且已经被证明有利于样本质量。</p><p id="850d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为与初始上限的主要区别，可以观察到期望不再是在整个随机前向轨迹上，而是在定义随机前向轨迹的单个步骤的随机三元组(初始数据、步骤、噪声)上。这种变化可以用上限中各项的独立性来解释(忽略待优化的参数是共享的这一事实)。正如我们前面已经提到的，这种独立性意味着最小化整个上界相当于最小化它的每一项。在迭代训练过程中，与采样完整的前向轨迹并优化所有项相比，采样单个步骤并优化相应项在理论上是相似的，并且在实践中容易得多。这是这种变体如此吸引人的原因之一。</p><p id="3fd9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最终，我们只剩下这个非常简洁的损失函数需要优化。根据数据的性质，模拟噪声的神经网络几乎可以采用任何形状。在图像的情况下，许多论文使用以 U-Net 形状组织的卷积和注意块，但是其他架构也可以适用。然后，训练简单地包括对一些三元组(初始数据、步长、噪声)进行迭代采样，并对损失函数应用梯度下降步长。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pq"><img src="../Images/73d4cb2d3cffee71543ec811aaf19ba5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WYUzy_OqmwBdKEPIzalSew.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/a7f7d0dfa24a3e5b6e99a5f72f13131e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xZSKEAa0dMa7He48R7WJTA@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">去噪扩散概率模型训练的训练过程的图示。</p></figure><p id="0f56" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在采样时，我们将从从各向同性高斯分布中采样一些噪声开始，然后，我们将使用我们学习的反向过程从目标分布中生成数据。在生成过程的每一步，噪声方面的参数化意味着模型采用噪声数据和当前步骤，并估计噪声分量。然后，可以计算 x_0 的近似值，并用于定义以这个接近的 x_0 为条件的反向过程的平均值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pq"><img src="../Images/2f32535a309ec609c4578e60f6885b72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IJi7TRIRXiAzVsP4OXP37g.png"/></div></div></figure><p id="6631" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">乍一看，人们可能已经发现了我们的采样过程中的一些令人困惑的行为:在第一步(事实上，以及所有后续步骤)，我们估计噪声数据中的噪声，使得计算 x_0 的估计成为可能。那么，为什么要为接下来的所有步骤费心呢？为什么不仅仅保留第一次估计的 x_0 作为我们最终要返回的数据呢？答案是，这个模型并没有被假定为强大到足以一次性消除所有噪声。因此，x_0 的第一次估计并不完美，但它为下一个采样步骤指明了一个粗略的方向，因为我们将根据这一估计从相反的过程中进行采样。</p><p id="f9df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以 x_0 为条件的反向过程的均值公式(_t(x_t，x_0))是 x_t 和 x_0 之间的线性组合(见上面的等式)，它清楚地表明这样一个事实，即我们的下一个采样步骤 x_{t-1}将以某种方式是当前步骤和估计的 x_0 之间的混合。可以非常粗略地说，我们将在“当前步骤和 x_0 之间的某个位置”对下一步进行采样。考虑到这一点，我们可以设想我们的生成模型如下:首先，我们从随机噪声中采样最终步骤 x_T，然后，我们从当前状态逐步估计 x_0，并基于当前状态生成前一步骤，并将该估计作为某种“目标”。答:我们提到过，在最初的步骤中，x_0 估计值不会很好，因为预计模型不具备这样做的预测能力，但是一次又一次的迭代，这些估计值将指导逆向生成过程，并将变得越来越好，直到我们获得高质量的最终样本。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/cdc331c351127946941abe4223105158.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kJmD3XZzN1PHzZcyyiyv0g@2x.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">去噪扩散概率模型的采样过程的图示。</p></figure><p id="7f7e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们可以在我们建立的第一个直觉和我们刚刚给出的更正式的 DPMs 描述之间架起一座桥梁。我们可以“在数学上”看到像 DPMs 这样的渐进生成过程相对于单程生成过程的优势。经过训练的模型不需要一次性处理所有生成。相反，该模型可以首先在早期阶段引入粗略信息，然后在保持去噪的同时逐渐添加越来越多的细节。最重要的是，当对这种反向去噪过程进行建模时，我们利用了这样一个事实，即对具有不同噪声水平的图像进行去噪并不是完全不同的任务，而是依赖于可以通过单个神经网络相互利用和学习的类似机制。</p></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><h1 id="bcee" class="lv lw iq bd lx ly nt ma mb mc nu me mf jw nv jx mh jz nw ka mj kc nx kd ml mm bi translated">外卖食品</h1><p id="0700" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf mp lh li lj mq ll lm ln mr lp lq lr ij bi translated">这篇文章的主要观点是:</p><ul class=""><li id="0e04" class="pr ps iq ky b kz la lc ld lf pt lj pu ln pv lr pw px py pz bi translated">生成过程都旨在实现相同的目标，即定义一个从简单分布中获取数据并将其转换为复杂分布中的数据的函数</li><li id="3128" class="pr ps iq ky b kz qa lc qb lf qc lj qd ln qe lr pw px py pz bi translated">学习一个函数(由一个神经网络表示)是一项相当困难的任务，这个函数从目标复杂分布中获取一些高斯噪声和输出数据</li><li id="b4a4" class="pr ps iq ky b kz qa lc qb lf qc lj qd ln qe lr pw px py pz bi translated">扩散过程或其离散化版本——具有高斯核的马尔可夫链——可用于逐步破坏信息，这意味着它们可用于从复杂的分布(例如有意义的图像)中获取数据，并通过添加噪声将它们从非常简单的分布(例如高斯噪声)逐步转化为数据</li><li id="99d8" class="pr ps iq ky b kz qa lc qb lf qc lj qd ln qe lr pw px py pz bi translated">扩散概率模型的思想是学习扩散的逆过程，期望通过去除噪声，从一些高斯噪声渐进到复杂分布</li><li id="4274" class="pr ps iq ky b kz qa lc qb lf qc lj qd ln qe lr pw px py pz bi translated">任务的迭代性质使其更简单，因为训练的模型不需要一次处理整个生成，并且可以首先引入粗略信息，然后在保持去噪的同时逐渐添加越来越多的细节</li><li id="a90a" class="pr ps iq ky b kz qa lc qb lf qc lj qd ln qe lr pw px py pz bi translated">要学习的模型依赖于以下事实:对具有不同噪声水平的图像进行去噪是依赖于公共机制的相似任务，该公共机制可以通过单个神经网络的训练来共享</li></ul><p id="1bf3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们描述了主要概念，以理解扩散概率模型的基础。当然，仍有许多知识有待探索，从对我们在本帖中描述的基本设置的<a class="ae kv" href="https://arxiv.org/pdf/2102.09672.pdf" rel="noopener ugc nofollow" target="_blank">可能的改进</a>，到<a class="ae kv" href="https://arxiv.org/pdf/2010.02502.pdf" rel="noopener ugc nofollow" target="_blank">去噪扩散隐式模型(DDIM) </a>的想法，经过了条件和指导的概念(【基于 T4】分类器的和<a class="ae kv" href="https://arxiv.org/pdf/2207.12598.pdf" rel="noopener ugc nofollow" target="_blank">分类器自由的</a>)，这些概念是这些天我们在互联网上随处可见的令人惊叹的生成模型的核心。</p><p id="b558" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个帖子已经很长很密了。因此，在这里讨论与 DPMs 相关的更高级机制的细节将会涉及太多的信息。然而，我们强烈建议感兴趣的读者阅读上面链接的论文，以加深对该领域丰富内容的了解。也许，我们会写另一篇文章来解决我们暂时搁置的这些概念…一步一步来，就像我们的 DPMs 一样。</p><p id="02d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢阅读！</p></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><p id="fb85" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用<a class="ls lt ep" href="https://medium.com/u/20ad1309823a?source=post_page-----1940329d6048--------------------------------" rel="noopener" target="_blank">巴蒂斯特·罗卡</a>写的其他文章:</p><div class="qf qg gp gr qh qi"><a rel="noopener follow" target="_blank" href="/understanding-variational-autoencoders-vaes-f70510919f73"><div class="qj ab fo"><div class="qk ab ql cl cj qm"><h2 class="bd ir gy z fp qn fr fs qo fu fw ip bi translated">了解变分自动编码器(VAEs)</h2><div class="qp l"><h3 class="bd b gy z fp qn fr fs qo fu fw dk translated">逐步建立导致 VAEs 的推理。</h3></div><div class="qq l"><p class="bd b dl z fp qn fr fs qo fu fw dk translated">towardsdatascience.com</p></div></div><div class="qr l"><div class="qs l qt qu qv qr qw kp qi"/></div></div></a></div><div class="qf qg gp gr qh qi"><a rel="noopener follow" target="_blank" href="/understanding-generative-adversarial-networks-gans-cd6e4651a29"><div class="qj ab fo"><div class="qk ab ql cl cj qm"><h2 class="bd ir gy z fp qn fr fs qo fu fw ip bi translated">理解生成敌对网络(GANs)</h2><div class="qp l"><h3 class="bd b gy z fp qn fr fs qo fu fw dk translated">一步一步地建立导致 GANs 的推理。</h3></div><div class="qq l"><p class="bd b dl z fp qn fr fs qo fu fw dk translated">towardsdatascience.com</p></div></div><div class="qr l"><div class="qx l qt qu qv qr qw kp qi"/></div></div></a></div></div></div>    
</body>
</html>