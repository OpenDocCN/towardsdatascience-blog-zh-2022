<html>
<head>
<title>Stop using the LIMIT clause wrong with Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">停止使用火花错误的限制条款</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stop-using-the-limit-clause-wrong-with-spark-646e328774f5#2022-05-22">https://towardsdatascience.com/stop-using-the-limit-clause-wrong-with-spark-646e328774f5#2022-05-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4db1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解火花极限及其在大型数据集上的性能</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f179aca3ed114ea03db02d39e5663ea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3MfAIwtkRUgTb_4KYg7RzQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@possessedphotography?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">拥有的摄影</a>在<a class="ae kv" href="https://unsplash.com/s/photos/limit?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="bce3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您来自SQL世界，您必须熟悉LIMIT子句。它非常常用于查看一小块数据。但是有没有想过它是如何工作的？</p><p id="d3cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Spark还提供了通过Dataframe或Spark SQL有限地选择数据块的功能。本文旨在理解spark极限，以及为什么您应该小心地将它用于大型数据集。</p><p id="5025" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那么，我们开始吧。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="f0db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，limit用于获取整个数据集的记录子集，从完整的数据集中获取n条记录。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lz"><img src="../Images/d37a0e4e490a4b8dc3233dddfae69a30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*stXHuAun-2z9Silwbh-1pg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">分布式数据限制的简化可视化(图片由作者提供)</p></figure><h1 id="428f" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">火花极限的内部</h1><p id="3d8f" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">Spark分两步执行limit，先做<code class="fe mx my mz na b">LocalLimit</code>，再做<code class="fe mx my mz na b">GlobalLimit</code>。</p><p id="34b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是它到底是什么意思呢？</p><p id="29e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们都知道在分布式数据中，数据分布在多个分区中，现在当您对此类数据运行限制时，会发生的情况是首先为每个名为<code class="fe mx my mz na b">LocalLimit</code>的分区单独计算限制。所以对于极限n，对于每个分区，计算n个记录。</p><p id="ccdd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个<code class="fe mx my mz na b">LocalLimit</code>之后是准备最终结果的<code class="fe mx my mz na b">GlobalLimit</code>。<em class="nb">(听起来很混乱？不要担心，下一节将有助于您更加清晰地理解)</em></p><p id="3b79" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们首先来看看LIMIT的查询执行计划:</p><pre class="kg kh ki kj gt nc na nd ne aw nf bi"><span id="c1f0" class="ng mb iq na b gy nh ni l nj nk">== Physical Plan ==<br/>Execute InsertIntoHadoopFsRelationCommand (5)<br/>+- * GlobalLimit (4)<br/>   +- Exchange (3)<br/>      +- * LocalLimit (2)<br/>         +- Scan csv  (1)</span></pre><p id="d0b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个<code class="fe mx my mz na b">Global limit</code>导致洗牌，这实际上是一个非常昂贵的操作。(如果你不知道为什么洗牌不好，就做<a class="ae kv" href="https://umbertogriffo.gitbook.io/apache-spark-best-practices-and-tuning/rdd/avoiding_shuffle_less_stage-_more_fast" rel="noopener ugc nofollow" target="_blank">读</a>)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/55be01f345bc901242ec07a337f62c00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kW-oqsD4Kd_Z9GrUAjPPWw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="c30c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是Spark UI的<code class="fe mx my mz na b">Stages</code>视图，在测试数据集上运行<code class="fe mx my mz na b">LIMIT</code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/45836c40f4a5fa5ec2aedf94386878b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cPFn5ntZrCU0ccXvZJcsmw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="eb13" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如上所示，<code class="fe mx my mz na b">LIMIT</code>分两个阶段执行，在执行<code class="fe mx my mz na b">GlobalLimit</code>的第二阶段，只有一个任务在运行。这一个任务使得LIMIT成为昂贵且耗时的操作，因为洗牌正在发生并且它读取洗牌数据。</p><p id="8cb3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于小数据集，这可能不明显，但对于大数据集，考虑随机读取，只运行一个任务来准备结果，即使调整应用程序也不会导致时间的显著减少。</p><p id="2e9a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">综上所述，数据集越大，<code class="fe mx my mz na b">LIMIT</code>越昂贵。另外，<code class="fe mx my mz na b">LIMIT n</code>的<code class="fe mx my mz na b">n</code>值越大，洗牌越大，所以<code class="fe mx my mz na b">LIMIT</code>越贵。我见过<code class="fe mx my mz na b">LIMIT</code>在给定大量内存的情况下，对一个大型数据集花费一个多小时。</p><p id="d56b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">嗯……那还有什么选择呢？</p><p id="5a48" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有趣的是，我读到过关于<code class="fe mx my mz na b">Limit</code>的内容，你可以通过调整<code class="fe mx my mz na b">spark.sql.limit.scaleUpFactor</code>参数让它变得更快，它只是告诉spark实际扫描多少分区来找到记录。(<a class="ae kv" href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-properties.html" rel="noopener ugc nofollow" target="_blank">参考</a>)。尽管如此，我并没有发现这个参数在时间调优上有显著的改进。</p><p id="ef66" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为LIMIT的替代，您可以使用<code class="fe mx my mz na b">TABLESAMPLE</code> ( <a class="ae kv" href="https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-sampling.html" rel="noopener ugc nofollow" target="_blank">参考</a>)，它作为一个单独的阶段运行，显示并行性，并且比LIMIT更快。</p><p id="3078" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe mx my mz na b">TABLESAMPLE</code>的查询执行计划</p><pre class="kg kh ki kj gt nc na nd ne aw nf bi"><span id="1931" class="ng mb iq na b gy nh ni l nj nk">== Physical Plan ==<br/>Execute InsertIntoHadoopFsRelationCommand (3)<br/>+- * Sample (2)<br/>   +- Scan csv  (1)</span></pre><p id="373d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe mx my mz na b">TABLESAMPLE</code>的阶段</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/da7a2a7561d63824d8aa85c9cce3a30d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vFLucnzOvrF53urRVnCoRg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="79bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">嗯，就这些。</p><p id="0541" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总而言之，<code class="fe mx my mz na b">LIMIT</code>性能并不可怕，甚至不明显，除非你开始在大型数据集上使用它，现在我希望你知道为什么！我经历过运行缓慢，无法自己调整应用程序，所以开始深入研究它，并找到它运行缓慢的原因，所以想分享它。</p><p id="caff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">希望这有所帮助！</p><p id="e2d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">快乐编码，<br/> JD</p><p id="34f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">参考:</p><div class="no np gp gr nq nr"><a href="https://stackoverflow.com/questions/54615195/spark-dataframe-limit-function-takes-too-much-time-to-show" rel="noopener  ugc nofollow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd ir gy z fp nw fr fs nx fu fw ip bi translated">火花数据帧限制功能显示时间过长</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">从pyspark.sql导入pyspark从pyspark.conf导入SparkConf导入findspark从…</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">stackoverflow.com</p></div></div><div class="oa l"><div class="ob l oc od oe oa of kp nr"/></div></div></a></div><div class="no np gp gr nq nr"><a href="https://stackoverflow.com/questions/56301329/understanding-spark-explain-collect-vs-global-vs-local-limit" rel="noopener  ugc nofollow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd ir gy z fp nw fr fs nx fu fw ip bi translated">了解Spark解释:收集与全局和局部限制</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">我正在尝试了解Spark/AWS Glue中执行限制的区别，我尝试使用Spark SQL spark.sql("SELECT *…</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">stackoverflow.com</p></div></div><div class="oa l"><div class="og l oc od oe oa of kp nr"/></div></div></a></div><div class="no np gp gr nq nr"><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-properties.html" rel="noopener  ugc nofollow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd ir gy z fp nw fr fs nx fu fw ip bi translated">配置属性</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">import org . Apache . spark . SQL . spark session val spark:spark session = spark session . builder。主(" local[*]")…</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">jaceklaskowski.gitbooks.io</p></div></div></div></a></div></div></div>    
</body>
</html>