<html>
<head>
<title>Probabilistic vs. Deterministic Regression with Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量流的概率回归与确定性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef#2022-12-15">https://towardsdatascience.com/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef#2022-12-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8806" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">概率深度学习</h2></div><h1 id="2cba" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">介绍</h1><p id="ee6a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">本文属于“概率深度学习”系列。这个每周系列涵盖了深度学习的概率方法。主要目标是扩展深度学习模型，以量化不确定性，即知道他们不知道的东西。</p><p id="0600" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">本文将探讨确定性回归和概率性回归之间的主要区别。一般来说，当自变量和因变量之间的关系很好理解并且相对稳定时，确定性回归是实用的。另一方面，当数据存在不确定性或可变性时，概率回归更合适。作为支持我们主张的练习，我们将使用张量流概率来拟合非线性数据的概率模型。</p><p id="d692" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">迄今发表的文章:</p><ol class=""><li id="c91b" class="mb mc it lc b ld lw lg lx lj md ln me lr mf lv mg mh mi mj bi translated"><a class="ae mk" href="https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1" rel="noopener">张量流概率简介:分布对象</a></li><li id="550e" class="mb mc it lc b ld ml lg mm lj mn ln mo lr mp lv mg mh mi mj bi translated"><a class="ae mk" href="https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15" rel="noopener">张量流概率简介:可训练参数</a></li><li id="6e85" class="mb mc it lc b ld ml lg mm lj mn ln mo lr mp lv mg mh mi mj bi translated"><a class="ae mk" rel="noopener" target="_blank" href="/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2">张量流概率中从零开始的最大似然估计</a></li><li id="6252" class="mb mc it lc b ld ml lg mm lj mn ln mo lr mp lv mg mh mi mj bi translated"><a class="ae mk" rel="noopener" target="_blank" href="/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00">tensor flow 中从头开始的概率线性回归</a></li><li id="2733" class="mb mc it lc b ld ml lg mm lj mn ln mo lr mp lv mg mh mi mj bi translated"><a class="ae mk" href="https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef" rel="noopener">使用 Tensorflow 进行概率回归与确定性回归</a></li><li id="5062" class="mb mc it lc b ld ml lg mm lj mn ln mo lr mp lv mg mh mi mj bi translated"><a class="ae mk" href="https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5" rel="noopener"> Frequentist 与 Tensorflow 的贝叶斯统计</a></li></ol><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/b5cbff5de2deacb891010e401b0ff252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xLg8pF_as3JXV55i"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图 1:我们今天的咒语:不是所有的线都是直的(<a class="ae mk" href="https://unsplash.com/photos/j0Da0mEplnY" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="bd6a" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们使用张量流和张量流概率开发我们的模型。TensorFlow Probability 是一个构建在 TensorFlow 之上的 Python 库。我们将从能在张量流概率中找到的基本对象开始，并理解我们如何操纵它们。我们将在接下来的几周内逐步增加复杂性，并将我们的概率模型与现代硬件(如 GPU)上的深度学习相结合。</p><p id="192c" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">像往常一样，代码可以在我的<a class="ae mk" href="https://github.com/luisroque/probabilistic_deep_learning_with_TFP" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p><h1 id="a57c" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">概率回归与确定性回归</h1><h2 id="b821" class="ng kj it bd kk nh ni dn ko nj nk dp ks lj nl nm ku ln nn no kw lr np nq ky nr bi translated">定义</h2><p id="f83b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">确定性回归是一种回归分析，其中自变量和因变量之间的关系是已知和固定的。因此，在给定一组已知自变量的情况下，它是预测因变量的值的有用工具。换句话说，如果向确定性回归模型提供相同的输入，它将总是产生相同的输出。</p><p id="e87f" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">如果我们考虑线性回归模型，Gauss-Markov 定理立即浮现在脑海中，因为它在某些假设下建立了普通最小二乘(OLS)估计的最优性。特别是，高斯-马尔可夫定理表明 OLS 估计量是最好的线性无偏估计量(蓝色)，这意味着它在所有线性无偏估计量中具有最小的方差。然而，高斯-马尔柯夫定理没有解决不确定性或估计中的信念的问题，这是概率方法的一个关键方面。</p><p id="2a06" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">另一方面，概率回归将自变量和因变量的交互方式视为未知，并假设它们会因数据集而异。概率回归模型不是预测因变量的单个值，而是预测因变量的可能值的概率分布。它允许模型考虑数据中的不确定性和可变性，并且在某些情况下可以提供更准确的预测。</p><p id="5f88" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们举一个简单的例子来简化理解。一名研究人员研究学生为考试而学习的时间和他们的分数之间的关系。在这种情况下，研究人员可以使用 OLS 方法来估计回归线的斜率和截距，并使用高斯-马尔可夫定理来证明该估计量的选择。然而，正如我们之前所述，高斯-马尔可夫定理并没有解决估计中的不确定性或置信问题。在概率世界中，重点是使用概率来描述模型或参数的不确定性或可信度，而不仅仅是估计量的最优性。因此，我们可以使用不同的方法来估计回归线的斜率和截距。因此，根据数据和对斜率和截距值的先验信念，我们可能会对学习时间和考试分数之间的关系得出不同的结论。</p><h2 id="f98d" class="ng kj it bd kk nh ni dn ko nj nk dp ks lj nl nm ku ln nn no kw lr np nq ky nr bi translated">贝叶斯统计和偏差-方差权衡</h2><p id="c6b9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">概率回归可以被视为贝叶斯统计的一种形式。它包括将自变量和因变量之间的未知关系视为随机变量，并根据可用数据估计其概率分布。这样，我们可以把它看作是把不确定性和可变性纳入回归分析的一种方法。回想一下，贝叶斯统计是一种统计分析框架，其中所有未知量都被视为随机变量，它们的概率分布随着新数据的观察而更新。这与经典统计学相反，经典统计学通常假设未知量是固定的，但参数是未知的。</p><p id="2102" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">考虑这两种方法之间差异的另一种方法是考虑统计估计中的偏差和方差之间的权衡。偏差是指参数的估计值和实际值之间的差异，而方差是指估计值的分布或可变性。通过提供模型参数的分布而不是单点估计，概率回归有助于减少估计中的偏差，从而提高模型的整体准确性。此外，概率回归可以提供估计值的不确定性或置信度的度量，这有助于根据模型做出决策或预测。当处理有噪声或不完整的数据时，估计值的不确定性较高，这可能是有益的。</p><h1 id="d2e8" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">非线性概率回归</h1><p id="301a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们跳到一个例子，让这些概念更容易理解。我们不会涵盖完全贝叶斯方法，这种方法需要估计认知的不确定性——模型的不确定性。我们将在以后的文章中研究这种不确定性。然而，我们将估计一种不同的不确定性——随机不确定性。它可以定义为数据生成过程中的不确定性。</p><p id="8aaa" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这一次，我们将讨论更复杂的回归分析——非线性回归。与使用直线对变量之间的关系进行建模的线性回归相比，非线性回归允许对变量之间更复杂的关系进行建模。它使非线性回归成为许多机器学习应用程序的有价值的工具，在这些应用程序中，变量之间的关系可能过于复杂，无法使用线性方程精确建模。</p><p id="4760" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们首先创建一些遵循非线性模式的数据:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/3a86ecc111be1e28b42145ba2c64c04f.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*mMFDKJeZHiAvuSj03Qlp_w.png"/></div></figure><p id="0a74" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">请注意，<em class="nt">噪声𝜖𝑖</em>∽<em class="nt">n</em>(0，1)是独立的，并且同分布。</p><pre class="mr ms mt mu gt nu nv nw bn nx ny bi"><span id="39b6" class="nz kj it nv b be oa ob l oc od">x = np.linspace(-1, 1, 1000)[:, np.newaxis]<br/>y = np.power(x, 3) + (3/15)*(1+x)*np.random.randn(1000)[:, np.newaxis]<br/><br/>plt.scatter(x, y, alpha=0.1)<br/>plt.show()</span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/eda5a0ec4ec1deb88282a53cc5639d0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*4oobBmCz0hliQFhmFqUnig.png"/></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图 2:遵循带有高斯噪声的非线性方程人工生成的数据。</p></figure><p id="660c" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">像往常一样，我们将对数似然定义为损失函数。</p><pre class="mr ms mt mu gt nu nv nw bn nx ny bi"><span id="b6f9" class="nz kj it nv b be oa ob l oc od">def negative_log_like(y_true, y_pred):<br/>    return -y_pred.log_prob(y_true)</span></pre><p id="56d8" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">正如我们在以前的文章中看到的，将我们的确定性深度学习方法扩展为概率性的方法是通过使用概率层，例如<code class="fe of og oh nv b">DistributionLambda</code>。回想一下，<code class="fe of og oh nv b">DistributionLambda</code>层返回一个分布对象。它也是 TensorFlow Probability 中实现的其他几个概率层的基类，我们将在以后的文章中使用它们。</p><p id="1877" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">为了建立我们的模型，我们从添加两个致密层开始。第一个有 8 个单元和一个 s 形激活函数。第二个有两个单元，没有激活功能。我们没有添加一个，因为我们想要参数化我们的高斯分布，该分布跟随具有任何真实值的第二密集层。高斯分布由<code class="fe of og oh nv b">DistributionLambda</code>层定义。请记住，分布的规模是标准差，这应该是一个正值。如前所述，我们通过 softplus 函数传递张量分量来考虑这种约束。</p><p id="5288" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">请注意，线性和非线性模型之间的真正区别是添加了密集层作为模型的第一层。</p><pre class="mr ms mt mu gt nu nv nw bn nx ny bi"><span id="3bda" class="nz kj it nv b be oa ob l oc od">model = Sequential([<br/>    Dense(input_shape=(1,), units=8, activation='sigmoid'),<br/>    Dense(2),<br/>    tfpl.DistributionLambda(lambda p:tfd.Normal(loc=p[...,:1], scale=tf.math.softplus(p[...,1:])))<br/>])<br/><br/>model.compile(loss=negative_log_like, optimizer=RMSprop(learning_rate=0.01))<br/>model.summary()</span></pre><p id="5e6b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们可以检查模型的输出形状，以便更好地理解正在发生的事情。我们得到一个空的事件形状和一个批处理形状(1000，1)。1000 指的是批量大小，而额外的维度在我们的问题陈述中没有意义。我们想要表示正态分布的单个随机变量。</p><pre class="mr ms mt mu gt nu nv nw bn nx ny bi"><span id="c9dd" class="nz kj it nv b be oa ob l oc od">y_model = model(x)<br/>y_sample = y_model.sample()<br/>y_model<br/><br/>&lt;tfp.distributions._TensorCoercible 'tensor_coercible' batch_shape=[1000, 1] event_shape=[] dtype=float32&gt;</span></pre><p id="687e" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们可以使用 TensorFlow Probability 提供的包装器来简化最后一层的实现，并使它更符合我们期望得到的输出形状。通过使用<code class="fe of og oh nv b">IndependentNormal</code>层，我们可以构建一个类似于用<code class="fe of og oh nv b">DistributionLambda</code>构建的发行版。同时，我们可以用一个静态的方法，输出概率层需要的参数个数来定义前面<code class="fe of og oh nv b">Dense</code>层的单元个数:<code class="fe of og oh nv b">tfpl.IndependentNormal.params_size</code>。</p><pre class="mr ms mt mu gt nu nv nw bn nx ny bi"><span id="5911" class="nz kj it nv b be oa ob l oc od">model = Sequential([<br/>    Dense(input_shape=(1,), units=8, activation='sigmoid'),<br/>    Dense(tfpl.IndependentNormal.params_size(event_shape=1)),<br/>    tfpl.IndependentNormal(event_shape=1)<br/>])<br/><br/>model.compile(loss=negative_log_like, optimizer=RMSprop(learning_rate=0.01))<br/>model.summary()</span></pre><p id="8635" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">如我们所见，该形状现在已被正确指定，因为批处理形状中的额外维度已被移动到事件形状中。</p><pre class="mr ms mt mu gt nu nv nw bn nx ny bi"><span id="5ed7" class="nz kj it nv b be oa ob l oc od">y_model = model(x)<br/>y_sample = y_model.sample()<br/>y_model<br/><br/>&lt;tfp.distributions._TensorCoercible 'tensor_coercible' batch_shape=[1000, 1] event_shape=[] dtype=float32&gt;</span></pre><p id="667d" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">是时候让模型符合我们合成生成的数据了。</p><pre class="mr ms mt mu gt nu nv nw bn nx ny bi"><span id="76a5" class="nz kj it nv b be oa ob l oc od">model.fit(x=x, y=y, epochs=500, verbose=False)<br/>model.evaluate(x, y)</span></pre><p id="036a" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">正如预期的那样，我们能够捕捉到数据生成过程中的任意不确定性。从下面我们可以得出的置信区间可以看出。概率模型的一个更有趣的特征是我们可以生成的样本，正如我们在下面看到的，遵循数据的原始生成过程。</p><pre class="mr ms mt mu gt nu nv nw bn nx ny bi"><span id="3bdc" class="nz kj it nv b be oa ob l oc od">y_hat = y_model.mean()<br/>y_sd = y_model.stddev()<br/>y_hat_u = y_hat -2 * y_sd<br/>y_hat_d = y_hat + 2*y_sd<br/><br/>fig, (ax_0, ax_1) =plt.subplots(1, 2, figsize=(15, 5), sharey=True)<br/>ax_0.scatter(x, y, alpha=0.4, label='data')<br/>ax_0.scatter(x, y_sample, alpha=0.4, color='red', label='model sample')<br/>ax_0.legend()<br/>ax_1.scatter(x, y, alpha=0.4, label='data')<br/>ax_1.plot(x, y_hat, color='red', alpha=0.8, label='model $\mu$')<br/>ax_1.plot(x, y_hat_u, color='green', alpha=0.8, label='model $\mu \pm 2 \sigma$')<br/>ax1.plot(x, y_hat_d, color='green', alpha=0.8)<br/>ax1.legend()<br/>plt.show()</span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi oi"><img src="../Images/bdddeb98df6aa707ff195754e568da4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d9PXJfWc5jnn8Qv3xBqNqA.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图 3:从概率非线性回归模型生成的样本(左边)及其与数据的拟合(右边)。</p></figure><h1 id="31d8" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">结论</h1><p id="b1bc" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">本文探讨了确定性回归和概率性回归之间的主要区别。我们看到，当自变量和因变量之间的关系很好理解且相对稳定时，确定性回归是实用的。另一方面，当数据存在不确定性或可变性时，概率回归更合适。作为一个练习，我们然后将一个概率模型拟合到非线性数据中。通过在我们的模型的开始添加一个具有激活函数的额外密集层，我们可以学习数据中的非线性模式。我们的最后一层是概率层，它输出一个分布对象。为了与我们的问题陈述更加一致，我们扩展了我们的方法，使用了我们在几篇文章之前探索过的<code class="fe of og oh nv b">IndependentNormal</code>层。它允许我们将批次维度移动到事件形状。接下来，我们成功地拟合了数据，同时提供了对任意不确定性的度量。最后，我们生成了新的样本，这些样本严格遵循数据的原始生成过程。</p><p id="54e8" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">下周，我们将探讨频繁投资者和贝叶斯方法之间的区别。到时候见！</p><p id="d073" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">保持联系:<a class="ae mk" href="https://www.linkedin.com/in/luisbrasroque/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a></p><h1 id="3cf0" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">参考资料和材料</h1><p id="085a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">[1] — <a class="ae mk" href="https://www.coursera.org/specializations/deep-learning" rel="noopener ugc nofollow" target="_blank"> Coursera:深度学习专业化</a></p><p id="c0c5" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">[2] — <a class="ae mk" href="https://www.coursera.org/specializations/tensorflow2-deeplearning" rel="noopener ugc nofollow" target="_blank"> Coursera:深度学习的 tensor flow 2</a>专业化</p><p id="c52d" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">[3] — <a class="ae mk" href="https://www.tensorflow.org/probability/overview" rel="noopener ugc nofollow" target="_blank">张量流概率指南和教程</a></p><p id="bd93" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">[4] — <a class="ae mk" href="https://blog.tensorflow.org/search?label=TensorFlow+Probability&amp;max-results=20" rel="noopener ugc nofollow" target="_blank"> TensorFlow 博客中的 TensorFlow 概率帖子</a></p></div></div>    
</body>
</html>