<html>
<head>
<title>The 5 most promising AI models for Image translation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">5个最有前途的图像翻译人工智能模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-5-most-promising-ai-models-for-image-translation-de2677e526e6#2022-09-28">https://towardsdatascience.com/the-5-most-promising-ai-models-for-image-translation-de2677e526e6#2022-09-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fd72" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从其他图像生成图像的最新模型的技术水平</h2></div><h1 id="1c86" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">图像到图像的翻译</h1><p id="cbb7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi lt translated"><span class="l lu lv lw bm lx ly lz ma mb di"> A </span>根据Solanki，Nayyar和Naved在[1]中的定义，<em class="mc">图像到图像的翻译是将图像从一个域转换到另一个域的过程，其目标是学习输入图像和输出图像之间的映射。</em></p><p id="f6e1" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">换句话说，我们希望模型能够通过学习映射函数f将图像“a”转换为另一个图像“b”。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mi"><img src="../Images/b5c1f09d70a662495c2576eacb004433.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u9Bwb1BO5TnGDHlA4AJirA.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">映射函数f获取图像“a ”,并将其转换为图像“b”。所有图像A的集合称为A的定义域，所有图像b的集合称为b的定义域。照片由<a class="ae my" href="https://unsplash.com/@sswhelan?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">肖恩·惠兰</a>(左)和<a class="ae my" href="https://unsplash.com/@21w8y?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">马特乌斯·德勒加茨</a>(右)在<a class="ae my" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄。作者编辑</p></figure><p id="c429" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">人们可能会想这些模型有什么用，它们在人工智能世界中有什么相关性。然而，它的应用非常广泛，不仅仅局限于艺术或平面设计。例如，能够拍摄一幅图像并将其转换成另一幅图像对于<strong class="kz ir">创建合成数据</strong>非常有用，比如分割图像来训练自动驾驶汽车模型。另一个经过测试的应用是<strong class="kz ir">地图设计</strong>，其中模型能够执行两种转换(卫星视图到地图，反之亦然)。图像转换模型也可以应用于<strong class="kz ir">架构</strong>，其中模型可以就如何完成未完成的项目提出建议。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mz"><img src="../Images/7714998fa1f5f4043f0f237b08778de6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ShLcze1vv_LjRm_G6fT9DA.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">图像翻译最引人注目的应用之一是将简单的图画转换成美丽的风景或绘画。图片由作者编辑</p></figure><h1 id="9cfe" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">5个最有前途的图像翻译人工智能模型</h1><p id="8389" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在过去的几年中，已经开发了几种方法来利用生成模型解决图像到图像的翻译问题。最常用的方法基于以下架构:</p><ul class=""><li id="e9ce" class="na nb iq kz b la md ld me lg nc lk nd lo ne ls nf ng nh ni bi translated">生成对抗网络</li><li id="90f8" class="na nb iq kz b la nj ld nk lg nl lk nm lo nn ls nf ng nh ni bi translated">可变自动编码器(VAE)</li><li id="eaae" class="na nb iq kz b la nj ld nk lg nl lk nm lo nn ls nf ng nh ni bi translated">扩散模型</li><li id="0bcc" class="na nb iq kz b la nj ld nk lg nl lk nm lo nn ls nf ng nh ni bi translated">变形金刚(电影名)</li></ul><h2 id="46d2" class="no kg iq bd kh np nq dn kl nr ns dp kp lg nt nu kr lk nv nw kt lo nx ny kv nz bi translated">Pix2Pix</h2><p id="7218" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">Pix2Pix [2]是基于<strong class="kz ir">条件GAN的模型</strong>。这意味着它的结构由一个发生器网络(G)和一个鉴别器(D)构成。两个网络都在一个对抗性的游戏中接受训练，其中G的目标是生成看起来与数据集相似的新图像，而D必须决定图像是生成的(假的)还是来自数据集(真的)</p><p id="6af4" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">Pix2Pix和其他GAN模型之间的主要区别是:( 1)第一个的生成器将图像作为输入来开始生成过程，而普通GAN使用随机噪声，以及(2) Pix2Pix是一个完全监督的模型，这意味着数据集由来自两个域的成对图像组成(输入和地面真实图像)</p><p id="fd3f" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">文中描述的架构由一个用于发生器的U-Net和一个用于鉴别器的马尔可夫鉴别器或补丁鉴别器定义:<br/> <strong class="kz ir"> * U-Net </strong>:由两个模块组成:<em class="mc">下采样</em>和<em class="mc">上采样</em>。使用卷积层将输入图像缩减为一组更小的图像(称为特征图),然后通过转置卷积进行上采样，直到达到原始输入维数。在<em class="mc">下采样</em>和上采样之间存在跳跃连接。<br/> <strong class="kz ir"> *补丁鉴别器</strong>:卷积网络，其输出是一个矩阵，其中每个元素都是对图像的一个部分(补丁)进行评估的结果。它包括生成的图像和地面真实图像之间的L1距离，以确保生成器学习映射给定输入图像的正确函数。也称为马尔可夫，因为它依赖于来自不同片的像素是独立的假设。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi oa"><img src="../Images/59951a2a5ae8982753c96f45e39ea597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*loC7cZjhByYOPKobFXUPQw.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">Pix2Pix结果。图片来自报纸[2]</p></figure><h2 id="1217" class="no kg iq bd kh np nq dn kl nr ns dp kp lg nt nu kr lk nv nw kt lo nx ny kv nz bi translated">无监督的图像到图像转换(单位)</h2><p id="0da7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在Pix2Pix中，训练过程是完全监督的(即我们需要成对的图像输入-背景事实)。单元方法[3]旨在学习将图像A映射到B的函数，而不需要两个成对的图像用于训练。</p><p id="9669" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">该模型从两个域(A和B)共享一个<strong class="kz ir">公共潜在空间</strong> ( <strong class="kz ir"> <em class="mc"> Z </em> </strong>)的假设出发。直观地说，我们可以将这个潜在空间视为图像域A和b之间的中间阶段。因此，使用绘画到风景的例子，我们可以使用相同的潜在空间后退并生成绘画图像，或者前进以看到令人惊叹的风景(见图X)。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi ob"><img src="../Images/97d9b86b6267de89355d3adfece67901.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LMlXy3YP8oI6YfX1npM-Fw.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">图一。(一)共享潜在空间。(b)单元建筑:X1是一幅图画，X2是一幅美丽的风景；E1，E2是从两个领域(绘画和风景)获取图像并将它们映射到共享潜在空间Z的编码器；G1，G2发电机，和D1，D2鉴别器。虚线表示网络之间的共享层。图片来自报纸[3]</p></figure><p id="7d20" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">单元模型是在一个<strong class="kz ir">耦合VAE-甘架构</strong>(见图1)下开发的，其中编码器的最后一层(e 1，E2)和生成器的第一层(G1，G2)是共享的。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi oa"><img src="../Images/d939a9ebeee2b5d41d284200ca039fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s7HvxP_J34S4rskn_NJ83A.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">单位结果。图片来自报纸[3]</p></figure><h2 id="c39f" class="no kg iq bd kh np nq dn kl nr ns dp kp lg nt nu kr lk nv nw kt lo nx ny kv nz bi translated">调色板</h2><p id="e054" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">Palette [4]是加拿大Google研究团队开发的一个<strong class="kz ir">条件扩散模型</strong>。该模型被训练来执行与图像转换相关的4个不同的任务，实现高质量的结果:<br/> <strong class="kz ir"> (i)着色</strong>:向灰度图像添加颜色<br/> <strong class="kz ir"> (ii)修补</strong>:用逼真的内容填充图像的用户指定区域<br/> <strong class="kz ir"> (iii)去剪裁</strong>:放大图像帧<br/> <strong class="kz ir"> (iv) JPEG恢复</strong>:恢复受损的JPEG图像</p><p id="b1a9" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">在这篇文章中，作者探讨了多任务通用模型和多个专门化模型之间的差异，两者都是针对1M迭代训练的。该模型的架构基于<a class="ae my" href="https://arxiv.org/pdf/2105.05233.pdf" rel="noopener ugc nofollow" target="_blank"> Dhariwal和Nichol 2021 </a>的类调节U-Net模型，并使用1024个图像的批量大小进行1M训练步骤。噪声时间表作为超参数被预先设置和调整，使用不同的时间表进行训练和预测。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi oa"><img src="../Images/f3ad7cee1fca42fd756b920095dbff63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iYOqTNin0HziZof7X4iYOA.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">调色板结果。图片来自报纸[4]</p></figure><h2 id="8a49" class="no kg iq bd kh np nq dn kl nr ns dp kp lg nt nu kr lk nv nw kt lo nx ny kv nz bi translated">视觉变形金刚(ViT)</h2><blockquote class="oc od oe"><p id="7fe8" class="kx ky mc kz b la md jr lc ld me ju lf of mf li lj og mg lm ln oh mh lq lr ls ij bi translated"><strong class="kz ir">注</strong>。虽然下面的两个模型不是明确为图像翻译而设计的，但它们是将变形金刚等强大模型引入计算机视觉领域的一个明显进步。</p></blockquote><p id="73ae" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">视觉变压器(ViT) [5]是对变压器架构(<a class="ae my" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank"> Vaswani et al. 2017 </a> ) <strong class="kz ir">的修改，为图像分类</strong>而开发。该模型将图像作为输入，并输出属于每个定义的类别的概率。</p><p id="ad0e" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">主要问题依赖于这样一个事实，即变换器被设计成接受一维序列作为输入，而不是2D矩阵。为了解决这一障碍，作者建议将图像分割成小块，<strong class="kz ir">将图像视为序列</strong>(或NLP中的句子)<strong class="kz ir">，而小块则视为记号</strong>(或单词)</p><p id="10f5" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">作为一个快速总结，我们可以将整个过程分为3个阶段:<br/> 1)嵌入:分割小块并展平它们→应用线性变换→添加类别令牌(该令牌将充当分类所考虑的图像摘要)→位置嵌入</p><p id="2bb8" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">2)变换器-编码器块:将嵌入的补丁馈入一系列变换器编码器块。注意力机制知道应该关注图像的哪些部分</p><p id="4904" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">3)分类MLP头:将类别令牌通过输出属于每个类别的图像的最终概率的MLP头。</p><p id="baea" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated"><strong class="kz ir">使用ViT的好处</strong>:排列不变。与CNN相比，Transformer不受图像平移(元素位置变化)的影响</p><p id="7d0e" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated"><strong class="kz ir">缺点</strong>:训练需要大量的标记数据(至少14M的图像)</p><h2 id="675b" class="no kg iq bd kh np nq dn kl nr ns dp kp lg nt nu kr lk nv nw kt lo nx ny kv nz bi translated">TransGAN</h2><p id="f96f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">TransGAN [6]是一个基于<strong class="kz ir">变换的GAN模型</strong>，专为不使用任何卷积层的图像生成而设计。相反，发生器和鉴别器由一系列通过上采样和下采样模块连接的变压器构成。</p><p id="86ac" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">发生器的正向处理采用1D阵列的随机噪声样本，并将其通过MLP。直观地说，<strong class="kz ir">我们可以把数组想象成一个句子，把像素值想象成单词</strong>(注意，一个64个元素的数组可以被重新整形为一个1通道的图像8✕8)接下来，作者应用一系列变换块，每个变换块后面都有一个上采样层，使数组(图像)的大小加倍。</p><p id="9f54" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">TransGAN的一个关键特性是<strong class="kz ir"> <em class="mc">网格——自我关注</em> </strong>。当到达高维图像(即，非常长的阵列32✕32 = 1024)时，应用变形器会导致自关注机制的爆炸性成本，因为您需要将1024阵列的每个像素与可以占据该位置的所有255个可能的像素(RGB比例)进行比较。由于这个原因，<em class="mc">网格自关注将全尺寸特征图划分成几个不重叠的网格，并且在每个局部网格内计算标记交互，而不是计算给定标记和所有其他标记之间的对应关系。</em>【参见图2】<br/>鉴别器架构与之前引用的ViT非常相似</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/52cbc6f0b14b2d35302268d74492e58b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*LFmUhXMRTt8Y3BWpRPOpFA.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">图二。网格自我关注来自原论文[5]。现在，我们输入多个句子，一次一个，而不是用一个长句来填充变压器。图片来自报纸[6]</p></figure><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi oa"><img src="../Images/0669959a6e1491c61d13055613998b32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VJ_x2zJAFCniXY9J-mF9Fw.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">不同数据集上的TransGAN结果，图像来自论文[6]</p></figure><h1 id="c09f" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">参考</h1><p id="3660" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">【1】</strong><a class="ae my" href="https://www.sciencedirect.com/book/9780128235195/generative-adversarial-networks-for-image-to-image-translation" rel="noopener ugc nofollow" target="_blank"><strong class="kz ir"><em class="mc">【生成对抗网络】进行意象到意象的翻译</em> </strong> </a>。Arun Solanki、Anand Nayyar和Mohd Naved。爱思唯尔(2021)</p><p id="c777" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated"><a class="ae my" href="https://arxiv.org/pdf/1611.07004.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kz ir">【2】有条件对抗网络的意象到意象翻译</strong> </a>。菲利普伊索拉朱俊彦廷辉周阿列克谢。2018</p><p id="7057" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated"><strong class="kz ir">【3】</strong><a class="ae my" href="https://arxiv.org/pdf/1703.00848.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kz ir">无监督的图像到图像翻译网络</strong> </a>。刘明宇，托马斯·布雷尔，扬·考茨。2018</p><p id="191c" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated"><strong class="kz ir">【4】</strong><a class="ae my" href="https://arxiv.org/pdf/2111.05826.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kz ir">调色板:图像间扩散模型</strong> </a>。奇万·萨哈利亚、陈伟霆、张惠文、克里斯·李、乔纳森·何、蒂姆·萨利曼斯、戴维·J·弗利特、穆罕默德·诺鲁齐。2022</p><p id="0b8f" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated"><strong class="kz ir">【5】</strong><a class="ae my" href="https://arxiv.org/pdf/2010.11929.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kz ir">一幅图像抵得上16x16的文字:变形金刚在尺度上的图像识别</strong> </a>。Alexey Dosovitskiy、Lucas Beyer、Alexander、Dirk Weissenborn、翟晓华、Thomas Unterthiner、Mostafa Dehghani、Matthias Minderer、Georg Heigold、Sylvain Gelly、Jakob Uszkoreit、Neil Houlsby。2021</p><p id="b3c1" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated"><strong class="kz ir">【6】</strong><a class="ae my" href="https://arxiv.org/pdf/2102.07074.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kz ir">trans GAN:两个纯变形金刚可以做一个强GAN，而且那个可以放大</strong> </a>。、张、王1。2021</p></div></div>    
</body>
</html>