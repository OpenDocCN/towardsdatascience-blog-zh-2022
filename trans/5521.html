<html>
<head>
<title>3 Ways To Aggregate Data In PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark 中聚合数据的 3 种方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/3-ways-to-aggregate-data-in-pyspark-72209197c90#2022-12-13">https://towardsdatascience.com/3-ways-to-aggregate-data-in-pyspark-72209197c90#2022-12-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="41cd" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/programming" rel="noopener" target="_blank">编程</a> | <a class="ae ep" href="https://towardsdatascience.com/tagged/big-data" rel="noopener" target="_blank">大数据</a> | <a class="ae ep" href="https://towardsdatascience.com/tagged/pyspark" rel="noopener" target="_blank"> PySpark </a></h2><div class=""/><div class=""><h2 id="aa63" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">PySpark 用编码示例解释基本聚合。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><a href="https://imp.i115008.net/zaX10r"><div class="gh gi kr"><img src="../Images/15c7bb4966fa85c4abce8a20724175c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*but5Gh1BP4lzOnsboTAdgA.jpeg"/></div></a><p class="kz la gj gh gi lb lc bd b be z dk translated">照片由<a class="ae ld" href="https://www.pexels.com/photo/close-up-photo-of-fire-266526/" rel="noopener ugc nofollow" target="_blank">像素</a>上的<a class="ae ld" href="https://www.pexels.com/photo/close-up-photo-of-fire-266526/" rel="noopener ugc nofollow" target="_blank">像素</a>拍摄</p></figure><h2 id="fff8" class="le lf it bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly iz bi translated">建议的点播课程</h2><p id="770b" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh ln mi mj mk lr ml mm mn lv mo mp mq mr im bi translated"><em class="ms">我的一些读者联系我，要求提供点播课程，以了解关于使用 Python 的</em><strong class="mb jd"><em class="ms">Apache Spark</em></strong><em class="ms">的更多信息。这是我推荐的 3 个很好的资源:</em></p><ul class=""><li id="d4f4" class="mt mu it mb b mc mv mf mw ln mx lr my lv mz mr na nb nc nd bi translated"><strong class="mb jd">阿帕奇卡夫卡数据流&amp;阿帕奇火花纳米级(UDACITY) → </strong>非常高质量的课程！</li><li id="3819" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr na nb nc nd bi translated"><a class="ae ld" href="https://imp.i115008.net/zaX10r" rel="noopener ugc nofollow" target="_blank"> <strong class="mb jd">数据工程纳米学位(UDACITY) </strong> </a></li><li id="914a" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr na nb nc nd bi translated"><a class="ae ld" href="https://click.linksynergy.com/deeplink?id=533LxfDBSaM&amp;mid=39197&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fspark-and-python-for-big-data-with-pyspark%2F" rel="noopener ugc nofollow" target="_blank"> <strong class="mb jd"> Spark 与 Python 对于大数据的配合</strong> </a></li></ul><p id="54a5" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated"><em class="ms">还不是中等会员？考虑与我的</em> <a class="ae ld" href="https://anbento4.medium.com/membership" rel="noopener"> <strong class="mb jd"> <em class="ms">推荐链接</em> </strong> </a> <em class="ms">签约，以获得 Medium 提供的一切服务，价格低至</em><strong class="mb jd"><em class="ms">【5 美元一个月</em> </strong> <em class="ms">！</em></p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h1 id="5f45" class="nt lf it bd lg nu nv nw lj nx ny nz lm ki oa kj lq kl ob km lu ko oc kp ly od bi translated">介绍</h1><p id="50e6" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh ln mi mj mk lr ml mm mn lv mo mp mq mr im bi translated"><a class="ae ld" href="https://www.databricks.com/spark/about" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>是一个数据处理引擎，在大型数据集上执行聚合的速度非常快。根据所执行的聚合类型，输出数据集通常会显示:</p><ul class=""><li id="0b0e" class="mt mu it mb b mc mv mf mw ln mx lr my lv mz mr na nb nc nd bi translated"><strong class="mb jd">较低的</strong> <a class="ae ld" href="https://dzone.com/articles/what-is-high-cardinality" rel="noopener ugc nofollow" target="_blank"> <strong class="mb jd">基数</strong> </a> <strong class="mb jd">与原始数据集</strong> →当对一组维度应用聚合时会出现这种情况。</li><li id="4194" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr na nb nc nd bi translated"><strong class="mb jd">相同的</strong> <a class="ae ld" href="https://dzone.com/articles/what-is-high-cardinality" rel="noopener ugc nofollow" target="_blank"> <strong class="mb jd">基数</strong> </a> <strong class="mb jd">与原始数据集</strong> →当对记录窗口应用聚合时会发生这种情况。</li></ul><p id="b84e" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">在本教程中，我将分享使用 Python 在 PySpark 数据帧上执行聚合的三种方法，并解释何时根据目的使用每种方法是有意义的。</p><p id="8d56" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">对于编码示例，我将使用虚构的数据集，包括按世界区域划分的<strong class="mb jd">500 万笔销售交易</strong>。如果你想继续下去，你可以从这个网站下载 CSV 文件。</p><p id="4fcd" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">原始数据集<a class="ae ld" href="https://gist.github.com/anbento0490/49313e47d4e56c7aad35f2baaefb1a8e" rel="noopener ugc nofollow" target="_blank">已被简化</a>，如下表所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="of og di oh bf oi"><div class="gh gi oe"><img src="../Images/cfc4a54ccccb232f8f0e3d6f0a85f2ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zvcJC_Tqxq6MF1SybNkjMA.png"/></div></div></figure><div class="oj ok gp gr ol om"><a rel="noopener follow" target="_blank" href="/3-ways-to-create-tables-with-apache-spark-32aed0f355ab"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd jd gy z fp or fr fs os fu fw jc bi translated">用 Apache Spark 创建表格的 3 种方法</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">学习如何用 PySpark 构建托管和非托管表，以及如何在您的项目中有效地使用它们</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">towardsdatascience.com</p></div></div><div class="ov l"><div class="ow l ox oy oz ov pa kx om"/></div></div></a></div><h1 id="bbf4" class="nt lf it bd lg nu pb nw lj nx pc nz lm ki pd kj lq kl pe km lu ko pf kp ly od bi translated">在 PySpark 中聚合数据</h1><p id="f6e3" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh ln mi mj mk lr ml mm mn lv mo mp mq mr im bi translated">在本节中，我将介绍在 PySpark 数据帧上工作时聚合数据的三种方法。</p><p id="31ca" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">在下面的代码片段中，我将只使用<code class="fe pg ph pi pj b">SUM()</code>函数，但是同样的推理和语法也适用于<code class="fe pg ph pi pj b">MEAN()</code>、<code class="fe pg ph pi pj b">AVG()</code>、<code class="fe pg ph pi pj b">MAX()</code>、<code class="fe pg ph pi pj b">MIN()</code>、<code class="fe pg ph pi pj b">COUNT()</code>和<code class="fe pg ph pi pj b">PIVOT()</code>函数。</p><h2 id="9528" class="le lf it bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly iz bi translated">方法 1:使用 GroupBy( ) +函数</h2><p id="10f8" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh ln mi mj mk lr ml mm mn lv mo mp mq mr im bi translated">在 PySpark 数据帧上运行聚合的最简单方法是将<code class="fe pg ph pi pj b">groupBy()</code>与聚合函数结合使用。</p><p id="4bd2" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">这种方法非常类似于使用 SQL <code class="fe pg ph pi pj b">GROUP BY</code>子句，因为它通过一组维度有效地折叠输入数据集，从而产生粒度更低<a class="ae ld" href="https://www.techopedia.com/definition/31722/granular-data" rel="noopener ugc nofollow" target="_blank">的输出数据集</a> ( <em class="ms">意味着记录更少</em>)。</p><p id="31c9" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">例如，如果选择的函数是<code class="fe pg ph pi pj b">sum()</code>，语法应该是:</p><pre class="ks kt ku kv gt pk pj pl bn pm pn bi"><span id="8010" class="po lf it pj b be pp pq l pr ps">dataframe.groupBy(‘dimension_1’, 'dimension_2', ...).sum(‘metric_1’)</span></pre><p id="d602" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">回到销售数据集，假设任务是计算:</p><ul class=""><li id="1786" class="mt mu it mb b mc mv mf mw ln mx lr my lv mz mr na nb nc nd bi translated"><code class="fe pg ph pi pj b">Total Revenue (£M)</code>由<code class="fe pg ph pi pj b">Region</code>组成</li><li id="1ffb" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr na nb nc nd bi translated">由<code class="fe pg ph pi pj b">Region</code>和<code class="fe pg ph pi pj b">Item Type</code>组成的<code class="fe pg ph pi pj b">Total Revenue (£M)</code></li></ul><p id="5480" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">在这种情况下，你可以写:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pt pu l"/></div></figure><p id="58ae" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">与 SQL 一样，<code class="fe pg ph pi pj b">groupBy()</code>可用于在多个列上运行聚合。下面，找到由<code class="fe pg ph pi pj b">Region</code>聚合的输出数据集:</p><p id="14fd" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated"><strong class="mb jd">输出 1 </strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="of og di oh bf oi"><div class="gh gi pv"><img src="../Images/daf0165d37db438e556b5d15c8b6bd47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8_k6cJ1yCwxZ0qBeBoMBMg.png"/></div></div></figure><p id="1b15" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">以下是由<code class="fe pg ph pi pj b">Region</code>和<code class="fe pg ph pi pj b">Item Type</code>聚合的输出数据集:</p><p id="a564" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated"><strong class="mb jd">输出 2 </strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="of og di oh bf oi"><div class="gh gi oe"><img src="../Images/cfbdfccbfed60fb89566092cc8f50a10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V3-j029_64vkr7JC4iHa3g.png"/></div></div></figure><p id="dd7b" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated"><strong class="mb jd">何时使用/避免→ </strong>当不需要格式化并且您只想在浏览数据集时运行<strong class="mb jd">快速聚合</strong>时，应该使用<strong class="mb jd">该方法。</strong></p><p id="62a9" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">相反，当您希望<strong class="mb jd">执行多重聚合或对聚合输出应用多个转换</strong>时，应该避免使用它。</p><p id="f671" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">例如，对聚合输出进行舍入并将列重命名为更整洁的名称非常麻烦，因为它需要两次单独的转换。</p><h2 id="26b4" class="le lf it bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly iz bi translated">方法 2:使用 GroupBy( ) + AGG()</h2><p id="699f" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh ln mi mj mk lr ml mm mn lv mo mp mq mr im bi translated">使用<code class="fe pg ph pi pj b">groupBy()</code>执行聚合的另一种方式是将所需的函数包装在<code class="fe pg ph pi pj b">AGG()</code>方法中。</p><p id="53f7" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">至于<code class="fe pg ph pi pj b">METHOD 1</code>，这个方法的行为也类似于 SQL <code class="fe pg ph pi pj b">GROUP BY</code>子句，因为它生成一个数据集，其基数<strong class="mb jd"><em class="ms"/></strong>比它的源低。然而，正如您将验证的那样，它比<code class="fe pg ph pi pj b">METHOD 1</code>要方便得多，因为它需要对聚合输出执行多次转换。</p><p id="1e97" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">同样，假设选择的函数是<code class="fe pg ph pi pj b">SUM()</code>，语法应该是:</p><pre class="ks kt ku kv gt pk pj pl bn pm pn bi"><span id="0d35" class="po lf it pj b be pp pq l pr ps">dataframe.groupBy('dimension_1', 'dimension_2', ...).agg(sum(“column_name”))</span></pre><p id="2d9f" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">例如，如果您希望在销售数据集上复制使用<code class="fe pg ph pi pj b">METHOD 1 </code>所取得的成果，您可以编写以下代码:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pt pu l"/></div></figure><p id="aaf4" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">这再次导致两个输出，一个按世界<code class="fe pg ph pi pj b">Region</code>分组，另一个按<code class="fe pg ph pi pj b">Region</code>和<code class="fe pg ph pi pj b">Item Type</code>分组:</p><p id="497d" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated"><strong class="mb jd">输出 1 </strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="of og di oh bf oi"><div class="gh gi pv"><img src="../Images/daf0165d37db438e556b5d15c8b6bd47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8_k6cJ1yCwxZ0qBeBoMBMg.png"/></div></div></figure><p id="77bc" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated"><strong class="mb jd">输出 2 </strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="of og di oh bf oi"><div class="gh gi oe"><img src="../Images/cfbdfccbfed60fb89566092cc8f50a10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V3-j029_64vkr7JC4iHa3g.png"/></div></div></figure><p id="5915" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated"><strong class="mb jd">何时使用/避免→ </strong>在生产中对 PySpark 数据帧执行聚合时，这种方法应该是您的首选解决方案。事实上，使用<code class="fe pg ph pi pj b">AGG()</code>允许您在第 行中应用<strong class="mb jd"> <em class="ms">多重连接转换，使您的代码更加易读和简洁。</em></strong></p><p id="951f" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">另一方面，当您的目标是运行聚合时，您应该避免使用这种方法，保持源数据集的粒度不变(<em class="ms">这意味着不按维度组</em>折叠记录)。</p><p id="443e" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">的确，这个要求，导致了<code class="fe pg ph pi pj b">METHOD 3</code>。</p><h2 id="5acb" class="le lf it bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly iz bi translated">方法 3:使用窗口函数</h2><p id="963e" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh ln mi mj mk lr ml mm mn lv mo mp mq mr im bi translated">在 PySpark DataFrame 中聚合数据的最后一种方法是对行窗口应用函数。</p><p id="c88c" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">这实际上相当于一个 SQL 窗口函数，因此以<code class="fe pg ph pi pj b">SUM()</code>为例，语法将是:</p><pre class="ks kt ku kv gt pk pj pl bn pm pn bi"><span id="a8b1" class="po lf it pj b be pp pq l pr ps"># WINDOW DEFINITION<br/>Window.partitionBy(‘dimension_1’, 'dimension_2', ...)<br/><br/># DF AGGREGATION USING WINDOW FUNCTION<br/>dataframe.withColumn(‘new_column_name’, functions.sum(‘metric_1’)\<br/>         .over(Window.partitionBy(‘dimension_1’)))</span></pre><p id="23ee" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">和在 SQL 中一样，<code class="fe pg ph pi pj b">partitionBy</code>子句用来代替<code class="fe pg ph pi pj b">groupBy()</code>来对特定的行窗口应用<code class="fe pg ph pi pj b">SUM()</code>函数。对于销售数据集，您可以编写:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pt pu l"/></div></figure><p id="c1ee" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">正如您在下面的输出中看到的，这一次列<code class="fe pg ph pi pj b">Total Revenue (£M)</code>保持不变，取而代之的是计算一个新列<code class="fe pg ph pi pj b">Total Revenue (£M) wndw</code> — <em class="ms">，显示窗口</em>中的总收入。</p><p id="14c7" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated"><strong class="mb jd">输出 1 </strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="of og di oh bf oi"><div class="gh gi pw"><img src="../Images/0fb34c38e5d9d335f85263b0cb148b15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ax0ESt5Fm_BdWgbOJZIKZw.png"/></div></div></figure><p id="0020" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated"><strong class="mb jd">输出 2 </strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="of og di oh bf oi"><div class="gh gi px"><img src="../Images/f1aa487f7541f2accf3a2812d2365771.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lACDyOjc9vg1OzlydhlZMA.png"/></div></div></figure><p id="2040" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated"><strong class="mb jd">何时使用/避免→ </strong>当您希望保留数据集的粒度时，应该首选此方法。实际上，窗口函数的一个主要优点是聚合应用于记录窗口，然后显示每一行，而不会折叠源数据集中的记录。</p><p id="bae3" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">另一方面，当您处理非常大的数据集并希望执行聚合以获得更小、更易于管理的输出时，您应该避免使用这种方法。</p><div class="oj ok gp gr ol om"><a rel="noopener follow" target="_blank" href="/3-nanodegrees-you-should-consider-to-advance-your-data-engineering-career-in-2021-baf597debc72"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd jd gy z fp or fr fs os fu fw jc bi translated">3 门数据工程课程，在 2022 年推进您的职业发展</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">加入数据行业，改变角色或通过注册数据工程简单地学习前沿技术…</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">towardsdatascience.com</p></div></div><div class="ov l"><div class="py l ox oy oz ov pa kx om"/></div></div></a></div><h1 id="3730" class="nt lf it bd lg nu pb nw lj nx pc nz lm ki pd kj lq kl pe km lu ko pf kp ly od bi translated">结论</h1><p id="a333" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh ln mi mj mk lr ml mm mn lv mo mp mq mr im bi translated">在本教程中，我讨论了在 Spark 数据帧上工作时，如何使用 Python 执行聚合，至少有三种基本方法。</p><p id="124e" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">从概念上讲，在 PySpark 中聚合数据与在 SQL 中使用<code class="fe pg ph pi pj b">GROUP BY</code>子句或利用窗口函数聚合数据非常相似。</p><p id="eec2" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">具有强大 SQL 背景的数据专业人员通常会发现向 PySpark 的过渡非常简单，尤其是在使用<a class="ae ld" href="https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark-sql-module" rel="noopener ugc nofollow" target="_blank"> pyspark.sql </a>模块时。</p><p id="48a6" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">但是你呢？您是否尝试过使用<a class="ae ld" href="https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.DataFrame.rollup.html" rel="noopener ugc nofollow" target="_blank"> rollup </a>和<a class="ae ld" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.cube.html" rel="noopener ugc nofollow" target="_blank"> cube </a>运行更高级的聚合？</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="04e4" class="le lf it bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly iz bi translated">给我的读者一个提示</h2><blockquote class="pz qa qb"><p id="081b" class="lz ma ms mb b mc mv kd me mf mw kg mh qc nj mj mk qd nk mm mn qe nl mp mq mr im bi translated">这篇文章包括附属链接，如果你购买的话，我可以免费(但实际上是打折)给你一点佣金。</p></blockquote><h1 id="eb51" class="nt lf it bd lg nu pb nw lj nx pc nz lm ki pd kj lq kl pe km lu ko pf kp ly od bi translated"><strong class="ak">消息来源</strong></h1><ul class=""><li id="69f4" class="mt mu it mb b mc md mf mg ln qf lr qg lv qh mr na nb nc nd bi translated"><a class="ae ld" href="https://excelbianalytics.com/wp/downloads-18-sample-csv-files-data-sets-for-testing-sales/" rel="noopener ugc nofollow" target="_blank">数据集最初从 Excel BI Analytics 网站下载，版权免费。</a></li><li id="6645" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr na nb nc nd bi translated"><a class="ae ld" href="https://orangematter.solarwinds.com/2020/01/05/what-is-cardinality-in-a-database/" rel="noopener ugc nofollow" target="_blank">什么是数据库中的基数？</a></li><li id="b95b" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr na nb nc nd bi translated"><a class="ae ld" href="https://sparkbyexamples.com/pyspark/pyspark-groupby-explained-with-example/" rel="noopener ugc nofollow" target="_blank"> PySpark GroupBy 举例说明</a></li><li id="abe5" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr na nb nc nd bi translated"><a class="ae ld" href="https://sparkbyexamples.com/pyspark/pyspark-groupby-agg-aggregate-explained/" rel="noopener ugc nofollow" target="_blank"> PySpark GroupBy Aggregate 解释</a></li></ul></div></div>    
</body>
</html>