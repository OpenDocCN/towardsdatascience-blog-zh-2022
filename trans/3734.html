<html>
<head>
<title>How Autoencoders Outperform PCA in Dimensionality Reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自动编码器如何在降维方面优于PCA</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-autoencoders-outperform-pca-in-dimensionality-reduction-1ae44c68b42f#2022-08-19">https://towardsdatascience.com/how-autoencoders-outperform-pca-in-dimensionality-reduction-1ae44c68b42f#2022-08-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="205c" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">自动编码器的应用</h2><div class=""/><div class=""><h2 id="2f04" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用非线性数据的自动编码器的维数减少</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/edcb6bf76e8e2e0e4e68551f879bdc0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4CAEr1mRC1Jv88vZaj_iHw.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae lh" href="https://unsplash.com/@theshubhamdhage?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Shubham Dhage </a>拍摄的照片</p></figure><p id="0956" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">自动编码器有很多<a class="ae lh" href="https://rukshanpramoditha.medium.com/an-introduction-to-autoencoders-in-deep-learning-ab5a5861f81e#991a" rel="noopener">的实际应用。降维就是其中之一。</a></p><p id="937e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有这么多的<a class="ae lh" rel="noopener" target="_blank" href="/11-dimensionality-reduction-techniques-you-should-know-in-2021-dcb9500d388b">降维技术</a>。自动编码器(AEs)和主成分分析(PCA)在其中很受欢迎。</p><p id="35d4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">PCA不适合非线性数据的降维。相比之下，自动编码器在非线性数据的降维方面做得非常好。</p><h1 id="79dd" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">目标</h1><p id="5bca" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">在本文结束时，您将能够</p><ul class=""><li id="1396" class="nb nc it lk b ll lm lo lp lr nd lv ne lz nf md ng nh ni nj bi translated">使用自动编码器降低输入数据的维数</li><li id="0cf0" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated">使用主成分分析降低输入数据的维数</li><li id="c756" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated">比较PCA和自动编码器的降维性能</li><li id="300b" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated">了解自动编码器如何在降维方面优于PCA</li><li id="bdaf" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated">了解PCA和自动编码器之间的主要区别</li><li id="3b02" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated">了解何时使用哪种降维方法</li></ul><h1 id="6301" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">先决条件</h1><p id="4af5" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">我推荐你阅读下面的文章作为这篇文章的先决条件。</p><ul class=""><li id="eb89" class="nb nc it lk b ll lm lo lp lr nd lv ne lz nf md ng nh ni nj bi translated"><a class="ae lh" href="https://rukshanpramoditha.medium.com/an-introduction-to-autoencoders-in-deep-learning-ab5a5861f81e" rel="noopener"> <strong class="lk jd">深度学习中的自动编码器介绍</strong> </a>(推荐使用，因为你需要在实现之前了解自动编码器的原理)</li><li id="137c" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/generate-mnist-digits-using-shallow-and-deep-autoencoders-in-keras-fb011dd3fec3"> <strong class="lk jd">使用Keras </strong> </a>中的浅层和深层自动编码器生成MNIST数字(推荐使用，因为您需要知道如何使用Keras functional API构建自动编码器)</li><li id="ed7c" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated"><a class="ae lh" href="https://rukshanpramoditha.medium.com/list/pca-and-dimensionality-reduction-special-collection-146045a5acb5" rel="noopener"> <strong class="lk jd"> PCA与降维文章合集</strong> </a>(推荐是因为你需要了解PCA的工作原理，如何应用以及降维背后的大致思路)</li><li id="79de" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated"><a class="ae lh" href="https://rukshanpramoditha.medium.com/two-different-ways-to-build-keras-models-sequential-api-and-functional-api-868e64594820" rel="noopener"> <strong class="lk jd">构建Keras模型的两种不同方式:顺序API和函数API </strong> </a>(推荐，因为您将在这里使用Keras函数API来构建autoencoder模型)</li><li id="bcf4" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated"><a class="ae lh" href="https://rukshanpramoditha.medium.com/acquire-understand-and-prepare-the-mnist-dataset-3d71a84e07e7" rel="noopener"> <strong class="lk jd">获取、理解并准备MNIST数据集</strong> </a>(推荐使用，因为您将在此使用MNIST数据集构建自动编码器和PCA模型)</li></ul><h1 id="15c9" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">用主成分分析法进行降维</h1><p id="2e31" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">首先，我们将使用主成分分析对MNIST数据(见最后的数据集<a class="ae lh" href="#c5f5" rel="noopener ugc nofollow">引文</a>)进行降维，并将输出与原始MNIST数据进行比较。</p><ul class=""><li id="cf54" class="nb nc it lk b ll lm lo lp lr nd lv ne lz nf md ng nh ni nj bi translated"><strong class="lk jd">步骤1: </strong>采集并准备MNIST数据集。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="np nq l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(作者代码)</p></figure><ul class=""><li id="155a" class="nb nc it lk b ll lm lo lp lr nd lv ne lz nf md ng nh ni nj bi translated"><strong class="lk jd">第2步:</strong>仅使用两种成分进行PCA。输入数据的原始维数是784。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="np nq l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(作者代码)</p></figure><ul class=""><li id="1fc8" class="nb nc it lk b ll lm lo lp lr nd lv ne lz nf md ng nh ni nj bi translated"><strong class="lk jd">步骤3: </strong>将PCA后的压缩MNIST数字可视化。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="np nq l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(作者代码)</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/94a4b0733c878b56aa9fcf00500d6ef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*Q3IbJXT59LIZuuuEaBz-8A.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ns"> PCA输出</strong>(作者图片)</p></figure><ul class=""><li id="ecea" class="nb nc it lk b ll lm lo lp lr nd lv ne lz nf md ng nh ni nj bi translated"><strong class="lk jd">步骤4: </strong>将PCA输出与原始MNIST数字进行比较。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/c494505afa49bbb3ce6511f8e285c145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*uDyYO2jHM_YpoFh9Oq4qrg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ns">原MNIST数字</strong>(图片由作者提供)</p></figure><p id="2b1e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">正如您在两个输出中看到的，应用PCA后，MNIST数字不清晰可见。这是因为我们只保留了两个组件，这两个组件没有捕获输入数据中的很多模式。</p><ul class=""><li id="f4ed" class="nb nc it lk b ll lm lo lp lr nd lv ne lz nf md ng nh ni nj bi translated"><strong class="lk jd">步骤5: </strong>使用两个主成分可视化测试数据。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="np nq l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(作者代码)</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/44be72d87f9fb2a538551e24d88fa580.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*zVxmCs7aiHmWMNstK1Wddw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ns">使用两个主成分可视化测试数据</strong>(图片由作者提供)</p></figure><p id="fd2c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所有的数字都没有清晰的独立簇。意味着只有两个分量的PCA模型无法清晰地区分测试数据中的九位数字。</p><h1 id="02be" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">使用自动编码器执行降维</h1><p id="3274" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">现在，我们将构建一个<a class="ae lh" rel="noopener" target="_blank" href="/generate-mnist-digits-using-shallow-and-deep-autoencoders-in-keras-fb011dd3fec3#63cc">深度自动编码器</a>来对相同的MNIST数据进行降维。我们还将潜在向量的维数保持为二维，以便于将输出与PCA返回的先前输出进行比较。</p><ul class=""><li id="dadb" class="nb nc it lk b ll lm lo lp lr nd lv ne lz nf md ng nh ni nj bi translated"><strong class="lk jd">步骤1: </strong>如前所述采集并准备MNIST数据集。</li><li id="7f18" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated"><strong class="lk jd">步骤2: </strong>定义自动编码器架构。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/3aefa1f588d3fe15d58c5a1bcc128a42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*fQ5xtvRUrCCt4AMwvyeZow.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ns">深度自动编码器架构</strong>(图片由作者提供，用draw.io制作)</p></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="np nq l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(作者代码)</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/333980558401bf35f451b88ccf61d8a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*ps3V9swQHasigJAJhLl5Ng.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ns">深度自动编码器架构</strong>(图片由作者提供)</p></figure><p id="0dfe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">网络中有超过一百万个参数。因此，这将需要大量的计算能力来训练这个模型。如果您为此使用传统的CPU，则需要花费大量时间来完成训练过程。不如用一个强大的<a class="ae lh" href="https://rukshanpramoditha.medium.com/setting-up-a-deep-learning-workplace-with-an-nvidia-graphics-card-gpu-for-windows-os-b6bff06eeec7" rel="noopener"> NVIDIA GPU </a>或者<a class="ae lh" href="https://rukshanpramoditha.medium.com/how-to-use-google-colab-free-gpu-to-run-deep-learning-code-incredibly-faster-760604d26c7e" rel="noopener"> Colab free GPU </a>来加速10倍或者20倍的训练过程。</p><ul class=""><li id="d2c1" class="nb nc it lk b ll lm lo lp lr nd lv ne lz nf md ng nh ni nj bi translated"><strong class="lk jd">第三步:</strong>编译、训练、监控损失函数。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="np nq l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(作者代码)</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/758c6b3392ec1dcf07f88c231e629566.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*COYtPXKGhrPaVdfThTJO7g.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(图片由作者提供)</p></figure><ul class=""><li id="f03e" class="nb nc it lk b ll lm lo lp lr nd lv ne lz nf md ng nh ni nj bi translated"><strong class="lk jd">步骤4: </strong>在自动编码之后，可视化压缩的MNIST数字。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="np nq l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(作者代码)</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/f0d2df6d7af7f3fffe19196dfaf60094.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*bDRk_wc_Tc5nF9MhY8k_gw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ns">自动编码器输出</strong>(图片由作者提供)</p></figure><p id="18eb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">自动编码器输出比PCA输出好得多。这是因为自动编码器可以学习数据中复杂的非线性模式。相比之下，PCA只能学习数据中的线性模式。</p><ul class=""><li id="73b3" class="nb nc it lk b ll lm lo lp lr nd lv ne lz nf md ng nh ni nj bi translated"><strong class="lk jd">步骤5: </strong>可视化潜在空间中的测试数据，以查看自动编码器模型如何能够区分测试数据中的九个数字。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="np nq l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(作者代码)</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/af46373dbf3c2dc42087dc43fc22bcff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*MMpuuLOstptRxLjrw9R0nA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ns">可视化潜在空间中的测试数据</strong>(图片由作者提供)</p></figure><p id="8bca" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">大多数数字都有清晰的独立簇。这意味着自动编码器模型可以清楚地区分测试数据中的大多数数字。有些数字重叠，例如，数字4和9之间有重叠。这意味着自动编码器模型不能清楚地区分数字4和9。</p><ul class=""><li id="69b7" class="nb nc it lk b ll lm lo lp lr nd lv ne lz nf md ng nh ni nj bi translated"><strong class="lk jd">步骤6(可选):</strong>使用相同的自动编码器模型，但在隐藏层中不使用任何激活功能，<code class="fe nz oa ob oc b">activation=None</code>在每个隐藏层中。您将获得以下输出。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi od"><img src="../Images/8747f7b14bd10a6e41ac5974399d0342.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*bwd29O6-XTUvtxCvMf_VGA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ns">隐藏层中没有任何激活功能的自动编码器输出</strong>(图片由作者提供)</p></figure><p id="5bb0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该输出甚至比PCA输出更差。我用这个实验向你展示了神经网络中激活函数的<a class="ae lh" href="https://rukshanpramoditha.medium.com/3-amazing-benefits-of-activation-functions-in-neural-networks-22b17b91a46e" rel="noopener">重要性。在神经网络的隐藏层中需要激活函数来学习数据中的非线性关系。</a><a class="ae lh" href="https://rukshanpramoditha.medium.com/what-happens-if-you-do-not-use-any-activation-function-in-a-neural-networks-hidden-layer-s-f3ce089e4508" rel="noopener">如果隐藏层中没有激活函数，神经网络就是巨大的线性模型</a>，其表现甚至比一般的机器学习算法还要糟糕！所以，你需要<a class="ae lh" rel="noopener" target="_blank" href="/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c">为神经网络</a>使用正确的激活函数。</p><h1 id="fbc4" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">PCA和自动编码器的主要区别</h1><ul class=""><li id="c5af" class="nb nc it lk b ll mw lo mx lr oe lv of lz og md ng nh ni nj bi translated"><strong class="lk jd">架构:</strong> PCA是一种通用的机器学习算法。它使用输入数据协方差矩阵的<a class="ae lh" href="https://rukshanpramoditha.medium.com/eigendecomposition-of-a-covariance-matrix-with-numpy-c953334c965d" rel="noopener">特征分解</a>或奇异值分解(SVD)进行降维。相比之下，Autoencoder是一种基于神经网络的架构，比PCA更复杂。</li><li id="d91f" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated"><strong class="lk jd">输入数据集的大小:</strong> PCA通常适用于小数据集。因为Autoencoder是一个神经网络，相比PCA，它需要大量的数据。因此，最好对非常大的数据集使用自动编码器。</li><li id="9f9e" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated"><strong class="lk jd">线性与非线性:</strong> PCA是一种线性降维技术。它只能学习数据中的线性关系。相反，Autoencoder是一种非线性降维技术，它也可以学习数据中复杂的非线性关系。这就是为什么在我们的例子中，自动编码器输出比PCA输出好得多，因为MNSIT数据中有复杂的非线性模式。</li><li id="dbbd" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated"><strong class="lk jd">用途:</strong> PCA只用于降维。但是，自动编码器并不局限于降维。它们还有其他的实际应用如<em class="oh">图像去噪</em>、<em class="oh">图像殖民</em>、<em class="oh">超分辨率</em>、<em class="oh">图像压缩</em>、<em class="oh">特征提取</em>、<em class="oh">图像生成</em>、<em class="oh">水印去除</em>等。</li><li id="bce6" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated"><strong class="lk jd">计算资源:</strong>自动编码器比PCA需要更多的计算资源。</li><li id="b1d6" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated"><strong class="lk jd">训练时间:</strong>与自动编码器相比，PCA运行时间更短。</li><li id="1551" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated"><strong class="lk jd">可解释性:</strong>自动编码器比PCA的可解释性差。</li></ul><h1 id="7432" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">实验(可选)</h1><p id="9e9b" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">您可以通过为以下超参数尝试不同的值来构建不同的自动编码器架构。做完之后，在评论区告诉我结果。</p><ul class=""><li id="c17c" class="nb nc it lk b ll lm lo lp lr nd lv ne lz nf md ng nh ni nj bi translated">层数</li><li id="7373" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated">每层中的节点数</li><li id="a00a" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated">潜在向量的维数</li><li id="46ea" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated">隐藏层中激活函数的类型</li><li id="701d" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated">优化器的类型</li><li id="0bc0" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated">学习率</li><li id="5e24" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated">时代数</li><li id="38c6" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated">批量</li></ul><h1 id="cf21" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">摘要</h1><p id="f067" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">考虑下面的图表，它包含了我们之前获得的所有输出。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/fe346490be41de24e47f399626614c0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*UXnIXoYcAn4f4RkF_vohEg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ns">输出比较</strong>(图片由作者提供)</p></figure><ul class=""><li id="77f0" class="nb nc it lk b ll lm lo lp lr nd lv ne lz nf md ng nh ni nj bi translated">第一行代表原始的MNIST数字。</li><li id="f369" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated">第二行表示由PCA返回的只有两个分量的输出。数字不清晰可见。</li><li id="39c2" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated">第三行表示由自动编码器返回的输出，具有比PCA输出好得多的二维潜在向量。数字现在清晰可见。原因是自动编码器已经学习了MNIST数据中复杂的非线性模式。</li><li id="fdfd" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated">最后一行表示自动编码器返回的输出，在隐藏层中没有任何激活功能。该输出甚至比PCA输出更差。我把这个包括进来是为了向你们展示激活函数在神经网络中的重要性。在神经网络的隐藏层中需要激活函数来学习数据中的非线性关系。如果隐藏层中没有激活函数，神经网络就是巨大的线性模型，甚至比一般的机器学习算法还要糟糕！</li></ul></div><div class="ab cl oj ok hx ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="im in io ip iq"><p id="e05d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">今天的帖子到此结束。</p><p id="cd1c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">如果您有任何问题或反馈，请告诉我。</strong></p><h2 id="b6de" class="oq mf it bd mg or os dn mk ot ou dp mo lr ov ow mq lv ox oy ms lz oz pa mu iz bi translated">阅读下一篇(推荐)</h2><ul class=""><li id="9387" class="nb nc it lk b ll mw lo mx lr oe lv of lz og md ng nh ni nj bi translated">阅读我的“<a class="ae lh" href="https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75" rel="noopener">神经网络与深度学习课程</a>”的全部剧集。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><a href="https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75"><div class="gh gi pb"><img src="../Images/8689557b9a87357f9710c3114a354f86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*8La-hD-bD-4hA9kul-zlag.png"/></div></a><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ns">点击图片进入我的神经网络和深度学习课程</strong>(作者截图)</p></figure></div><div class="ab cl oj ok hx ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="im in io ip iq"><h2 id="db03" class="oq mf it bd mg or os dn mk ot ou dp mo lr ov ow mq lv ox oy ms lz oz pa mu iz bi translated">支持我当作家</h2><p id="8605" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">我希望你喜欢阅读这篇文章。如果你愿意支持我成为一名作家，请考虑 <a class="ae lh" href="https://rukshanpramoditha.medium.com/membership" rel="noopener"> <strong class="lk jd"> <em class="oh">注册会员</em> </strong> </a> <em class="oh">以获得无限制的媒体访问权限。它只需要每月5美元，我会收到你的会员费的一部分。</em></p><div class="pc pd gp gr pe pf"><a href="https://rukshanpramoditha.medium.com/membership" rel="noopener follow" target="_blank"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd jd gy z fp pk fr fs pl fu fw jc bi translated">通过我的推荐链接加入Medium</h2><div class="pm l"><h3 class="bd b gy z fp pk fr fs pl fu fw dk translated">阅读Rukshan Pramoditha(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接…</h3></div><div class="pn l"><p class="bd b dl z fp pk fr fs pl fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="po l"><div class="pp l pq pr ps po pt lb pf"/></div></div></a></div><p id="26cd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="oh">或者，您也可以考虑通过点击本文底部的</em> <strong class="lk jd"> <em class="oh">给予提示</em> </strong> <em class="oh">按钮进行小额捐赠。</em></p><p id="55ce" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">非常感谢你一直以来的支持！下一篇文章再见。祝大家学习愉快！</p></div><div class="ab cl oj ok hx ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="im in io ip iq"><h2 id="c5f5" class="oq mf it bd mg or os dn mk ot ou dp mo lr ov ow mq lv ox oy ms lz oz pa mu iz bi translated">MNIST数据集信息</h2><ul class=""><li id="1fec" class="nb nc it lk b ll mw lo mx lr oe lv of lz og md ng nh ni nj bi translated"><strong class="lk jd">引用:</strong>邓，l，2012。用于机器学习研究的手写数字图像mnist数据库。<strong class="lk jd"> IEEE信号处理杂志</strong>，29(6)，第141–142页。</li><li id="276b" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated">【http://yann.lecun.com/exdb/mnist/】来源:<a class="ae lh" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"/></li><li id="0169" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated"><strong class="lk jd">许可:</strong><em class="oh">Yann le Cun</em>(NYU库朗研究所)和<em class="oh"> Corinna Cortes </em>(纽约谷歌实验室)持有MNIST数据集的版权，该数据集在<em class="oh">知识共享署名-共享4.0国际许可</em>(<a class="ae lh" href="https://creativecommons.org/licenses/by-sa/4.0/" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd">CC BY-SA</strong></a>)下可用。你可以在这里了解更多关于不同数据集许可类型的信息。</li></ul></div><div class="ab cl oj ok hx ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="im in io ip iq"><p id="de7c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="pu pv ep" href="https://medium.com/u/f90a3bb1d400?source=post_page-----1ae44c68b42f--------------------------------" rel="noopener" target="_blank">鲁克山普拉莫迪塔</a><br/><strong class="lk jd">2022–08–19</strong></p></div></div>    
</body>
</html>