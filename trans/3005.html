<html>
<head>
<title>7 Pitfalls to avoid while using Model-Agnostic Interpretation Techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用模型不可知解释技术时要避免的7个陷阱</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/8-pitfalls-to-avoid-while-using-model-agnostic-interpretation-techniques-d0fcca544711#2022-06-30">https://towardsdatascience.com/8-pitfalls-to-avoid-while-using-model-agnostic-interpretation-techniques-d0fcca544711#2022-06-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7043" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">可解释机器学习的一般陷阱</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d34e00adb667d7afc4171f5e9ed740fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qah1hzY2Ru4PWtq0CwVkpQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://pixabay.com/users/mohamed_hassan-5229782/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=4082314" rel="noopener ugc nofollow" target="_blank">穆罕默德·哈桑</a>来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=4082314" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="a6a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着越来越多不容易解释的复杂机器学习算法被采用，可解释的机器学习技术在数据科学社区中越来越受欢迎。</p><p id="b3e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型不可知的解释技术不关心底层的模型，但是它们有能力解释模型并提供有洞察力的模型解释。用于机器学习模型的一些流行的模型不可知的解释技术是<strong class="lb iu"> <em class="lv">【部分依赖图】</em></strong><strong class="lb iu"><em class="lv">置换特征重要性(PFI)、LIME、</em> </strong>和<strong class="lb iu"><em class="lv">【SHAP】</em></strong>。如果应用不当，这些模型不可知的解释技术会导致错误的见解或结论。在本文中，我们将讨论在使用解释技术时要避免的一些常见的8个陷阱。</p><blockquote class="lw lx ly"><p id="360c" class="kz la lv lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">这篇文章的灵感来自Christoph Molnar和他的团队于2021年8月发表的一篇论文。我用通俗易懂的文字概括了这篇文章。</p></blockquote><h1 id="16c6" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">开始使用:</h1><p id="e0cc" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">下面提到的8个陷阱指的是如果数据科学家使用解释技术，他们会在哪里出错。</p><h2 id="4ebc" class="mz md it bd me na nb dn mi nc nd dp mm li ne nf mo lm ng nh mq lq ni nj ms nk bi translated">1)假设一个适合所有人的可解释性:</h2><p id="3656" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">有不同种类的解释技术服务于不同的目的。数据科学家首先需要考虑基于业务约束需要什么样的可解释性。</p><p id="45be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">解决方案:</strong>任何单一的模型解释技术都不适合所有的用例或模型。SHAP是计算特征对模型预测的重要性的首选方法，因为它可以计算每个实例中每个特征的shapely值。然而，置换特征重要性(PFI)优选地用于计算特征重要性wrt以进行模型概括。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/39d035351cb6e7038784fb3246423059.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*WuvqT4ecICo3A8YQapAlpQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://arxiv.org/pdf/2007.04131.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)，选择流行的模型不可知解释技术</p></figure><h2 id="50aa" class="mz md it bd me na nb dn mi nc nd dp mm li ne nf mo lm ng nh mq lq ni nj ms nk bi translated">2)模型泛化能力差:</h2><p id="3b62" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">模型解释技术基于模型是最佳拟合的假设来提供见解。模型解释和底层模型一样好。拟合不足或拟合过度的模型会导致错误的解释见解。</p><p id="32c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">解决方案:</strong>数据科学家应该监控、跟踪和调试底层模型，并对其进行调整，以获得一个健壮的通用模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/54bc71961094b069954cef6c205e9e25.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*yI_3sLiQir-Y2K6aBF-V8Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://www.kaggle.com/general/198890" rel="noopener ugc nofollow" target="_blank">来源</a>)，偏差方差权衡</p></figure><h2 id="7304" class="mz md it bd me na nb dn mi nc nd dp mm li ne nf mo lm ng nh mq lq ni nj ms nk bi translated">3)复杂模型的不必要使用:</h2><p id="55a0" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">有时简单和复杂的模型在期望的度量上有相似的性能。对于模型解释而言，简单模型应始终优先于具有可比性能的复杂模型。</p><p id="8e3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">解决方案:</strong>与具有内核的SVM或复杂的神经网络模型相比，线性回归和决策树等模型更易于解释。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/d3d2948851021058e138953df401a7d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aOrBUks2d9EVNvXQxM67hA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://arxiv.org/pdf/2007.04131.pdf" rel="noopener ugc nofollow" target="_blank"> Source </a>)，<strong class="bd no"> Top: </strong>对线性回归模型(欠拟合)、随机森林(过拟合)和具有径向基核的支持向量机(良好拟合)的训练和测试数据的性能估计，<strong class="bd no"> Bottom: </strong>对数据生成过程(DGP)——这是基本事实——和三个模型的性能估计</p></figure><h2 id="6061" class="mz md it bd me na nb dn mi nc nd dp mm li ne nf mo lm ng nh mq lq ni nj ms nk bi translated">4)忽略特征相关性:</h2><p id="1f0e" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">PFI、PDP、LIME或SHAP等不可知模型解释技术可能会误导在多重共线性数据集上训练的机器学习模型。</p><p id="fdc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">解决方案:</strong>数据科学家应该使用各种统计测试和可视化来检查数据集中相关性的存在，并且应该在建模和解释之前对其进行处理。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/eb66d7d02b9955cb43c8dc45ea7ed263.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RjyN_k6hcgPgxu7pmGTFGw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://arxiv.org/pdf/2007.04131.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)，外推解释</p></figure><h2 id="3dbf" class="mz md it bd me na nb dn mi nc nd dp mm li ne nf mo lm ng nh mq lq ni nj ms nk bi translated">5)忽略模型和近似不确定性:</h2><p id="0227" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">许多解释技术只提供了一个平均估计值，并没有量化不确定性。忽略不确定性的来源会导致对数据中噪声的解释。</p><p id="2e3e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">解决方案:</strong>数据科学家必须反复尝试计算指标，但使用不同的引导样本来量化不确定性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/5d1d2b08f84ba029aaf1bf5a1d575f05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QbT6Y4RgNACKEHJdP6U0CA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(来源)，特征“x1”的部分相关性图</p></figure><h2 id="90ee" class="mz md it bd me na nb dn mi nc nd dp mm li ne nf mo lm ng nh mq lq ni nj ms nk bi translated">6)无法扩展到高维度设置:</h2><p id="ab88" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">对于人类思维来说，解释高维数据的结果是相当困难的。对高维数据集应用模型解释可能会导致大量的高维输出。此外，计算高维数据的模型解释在计算上是昂贵的。</p><p id="679c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">解决方案:</strong>建议在特征工程管道中使用降维技术或将相似特征分组。</p><blockquote class="lw lx ly"><p id="b815" class="kz la lv lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">请参考我之前的文章了解<a class="ae ky" rel="noopener" target="_blank" href="/8-dimensionality-reduction-techniques-every-data-scientists-should-know-1c594f0aa7f2"> 8降维技术</a>。</p></blockquote><div class="nr ns gp gr nt nu"><a rel="noopener follow" target="_blank" href="/8-dimensionality-reduction-techniques-every-data-scientists-should-know-1c594f0aa7f2"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd iu gy z fp nz fr fs oa fu fw is bi translated">每个数据科学家都应该知道的8种降维技术</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">Python中各种降维技术的基本指南</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">towardsdatascience.com</p></div></div><div class="od l"><div class="oe l of og oh od oi ks nu"/></div></div></a></div><h2 id="cea3" class="mz md it bd me na nb dn mi nc nd dp mm li ne nf mo lm ng nh mq lq ni nj ms nk bi translated">7)不合理的随意解释:</h2><p id="fdb8" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">大多数从业者对数据生成过程的偶然洞察感兴趣，这是模型不可知的解释技术所不能提供的。标准的监督ML模型被设计成仅仅利用关联，而不是模拟偶然的关系。</p><p id="76b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">解决方案:</strong>数据科学从业者必须仔细评估是否可以对底层数据生成、学习模型和解释技术做出充分的假设。如果假设得到满足，那么随意的解释是可能的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/7691c6cb693cc25b486e53fda1c8cf62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VqERB-P3TKwp4G2xpwJxIQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://arxiv.org/pdf/2007.04131.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)，重要特征数量与特征数量的关系图</p></figure><h1 id="5439" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">结论:</h1><p id="e54f" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在本文中，我们已经讨论了模型不可知解释技术的7个陷阱，例如，在不良的模型概括、依赖特征、特征之间的交互或者因果解释的情况下，在生成模型解释之前需要记住这些陷阱</p><h1 id="ae20" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">参考资料:</h1><p id="4ea4" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">[1]机器学习模型的模型不可知解释方法的一般陷阱(2021年8月17日):【https://arxiv.org/pdf/2007.04131.pdf<a class="ae ky" href="https://arxiv.org/pdf/2007.04131.pdf" rel="noopener ugc nofollow" target="_blank"/></p><blockquote class="ok"><p id="939c" class="ol om it bd on oo op oq or os ot lu dk translated">感谢您的阅读</p></blockquote></div></div>    
</body>
</html>