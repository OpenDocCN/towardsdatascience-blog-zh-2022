<html>
<head>
<title>Leveraging linear regression for feature selection of categorical and continuous variables</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用线性回归进行分类变量和连续变量的特征选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beyond-linear-regression-467a7fc3bafb#2022-07-29">https://towardsdatascience.com/beyond-linear-regression-467a7fc3bafb#2022-07-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ef34" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何使用群组套索选择前K个最相关的特征</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5840ec86a618ce115316c6eb105533ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t9Dt46Yifi6s6hi8lEBJRg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">[感谢<a class="ae kv" href="https://www.craiyon.com/" rel="noopener ugc nofollow" target="_blank">https://www.craiyon.com/</a>生成]</p></figure><p id="df50" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">线性回归是初级/入门级机器学习(ML)模型之一。这归功于它的…</p><ul class=""><li id="17c5" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">简单性</strong>:它将给定的响应<em class="mb"> y </em>建模为一些变量<em class="mb"> x_1，…，x_p </em>的线性组合</li><li id="caa7" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated"><strong class="ky ir">可解释性</strong>:与变量<em class="mb"> x_j </em>相关的系数暗示了它与响应<em class="mb"> y </em>的关系</li><li id="28ef" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated"><strong class="ky ir">可训练性</strong>:在训练过程中不需要大量的超参数调整。</li></ul><p id="ea8c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">甚至可以说它是数据科学家的<a class="ae kv" href="https://en.wikipedia.org/wiki/%22Hello,_World!%22_program" rel="noopener ugc nofollow" target="_blank">《Hello world》节目</a>的同义词。尽管是基本的，线性回归仍然有一些其他非常有趣的特性要展现…</p><h1 id="e700" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">特征选择的惩罚线性回归</h1><p id="ec10" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">寻找线性回归系数<em class="mb"> β_1，…，β_p </em>包括寻找接近响应的变量的“最佳”线性组合。换句话说，找到使<a class="ae kv" href="https://en.wikipedia.org/wiki/Mean_squared_error#:~:text=If%20a%20vector,is%20computed%20as" rel="noopener ugc nofollow" target="_blank">均方误差(MSE) </a>最小的系数。</p><p id="324b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过考虑MSE加上额外的惩罚项，可以赋予回归系数一些额外的性质。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/2a6584ca08a2f4b7099601aed3f7b32d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*leuRNHGbdAOkHVSNSwkKwA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">要最小化的惩罚目标函数。[作者插图]</p></figure><p id="c281" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这类似于隐式地对模型说:<strong class="ky ir">在考虑强度为<em class="mb"> α </em> </strong> <em class="mb">的惩罚项的同时，尽可能多地保留数据保真度。碰巧的是，通过对惩罚项的特定选择，我们可以为线性回归提供特征选择属性。</em></p><h2 id="34bf" class="nf mi iq bd mj ng nh dn mn ni nj dp mr lf nk nl mt lj nm nn mv ln no np mx nq bi translated">套索特征选择工作良好，直到你考虑分类变量</h2><p id="ec5e" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">当我们将罚项设置为<a class="ae kv" href="https://en.wikipedia.org/wiki/Norm_(mathematics)#:~:text=The%20name%20relates,of%20the%20columns" rel="noopener ugc nofollow" target="_blank"> L1范数</a> —回归系数的绝对值之和时，我们得到了一个具有很少非零系数的解。这在文献中被称为<a class="ae kv" href="https://en.wikipedia.org/wiki/Lasso_(statistics)" rel="noopener ugc nofollow" target="_blank">套索模型</a>，其产生的解决方案享有特征选择属性，因为<strong class="ky ir">(少数)非零系数指示与模型最相关的特征，并因此指示要选择的特征。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/15a71ee9c0d9d8657075b0f74be0970c.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/format:webp/1*KnHFKwghiG3jiO7mXDRWuQ.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/b5eebcc33f8c626216eb4192e3ea0cde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z7jmcBt6rusIf7jpRh-6eg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nt"> Top) </strong>套索的目标函数被最小化。<strong class="bd nt">(Bottom)</strong>拟合套索模型后得到的解决方案。[作者插图]</p></figure><p id="1287" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">经常会遇到分类变量，例如，一个颜色变量的值是“红”、“白”和“蓝”。这些必须事先以数字形式编码，以便在回归模型中使用。为了做到这一点，我们经常求助于使用<a class="ae kv" href="https://en.wikipedia.org/wiki/One-hot#Comparison_with_other_encoding_methods:~:text=important%20than%20%27laugh%27.-,Machine%20learning%20and%20statistics,-%5Bedit%5D" rel="noopener ugc nofollow" target="_blank">一键编码</a>——将一个变量分解成它的虚拟值——来避免隐式地强制执行值之间的顺序。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/a646178144daf15b3940833e85348a8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*YEw0VspWxOpJDReH7saivQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">对以“红色”、“白色”和“蓝色”为值的可变颜色应用一键编码后的结果。[作者插图]</p></figure><p id="8d7f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种特定情况下应用Lasso作为特征选择技术是不合适的，因为它独立地处理变量，因此忽略了它们之间的任何关系，在我们的情况下，它忽略了虚拟变量作为一个整体代表相同的分类变量的事实。</p><p id="b8b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，如果Lasso在<em class="mb"> color_white </em>和<em class="mb"> color_blue </em>中选择了<em class="mb"> color_red </em>，那么它并没有说明“颜色”变量的相关性。</p><h2 id="f2c3" class="nf mi iq bd mj ng nh dn mn ni nj dp mr lf nk nl mt lj nm nn mv ln no np mx nq bi translated">用于连续/分类特征选择的组合套索</h2><p id="9fcf" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">理想情况下，我们需要一个将分类变量的虚拟变量作为一个整体来考虑的惩罚，换句话说，一个在处理分类变量时包含虚拟变量之间潜在的“组结构”的惩罚。</p><p id="7af9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这可以通过稍微改变Lasso的惩罚来处理变量组来实现。为此，我们重新排列了向量中属于同一组的回归系数。然后，我们考虑它们的和<a class="ae kv" href="https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm:~:text=of%20its%20norm.-,Euclidean%20norm,-%5Bedit%5D" rel="noopener ugc nofollow" target="_blank"> L2范数</a>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/4aaad3848234794ab25a1fe20ba2b1c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*47cdwy6A7Be5d58GCVW6-A.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/eb15f48c0bde041702e331422ffc21a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*UlApEKtbmKNNtRmstfq05A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nt"> Top) </strong>组套索目标函数被最小化。<strong class="bd nt">下图)</strong>拟合一组套索模型后得到的解。[作者插图]</p></figure><p id="f58c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由此产生的模型在文献中被称为<a class="ae kv" href="https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning#Group_lasso:~:text=few%20such%20methods.-,Group%20lasso,-%5Bedit%5D" rel="noopener ugc nofollow" target="_blank">组套索</a>。值得一提的是，通过考虑上述惩罚，我们获得了一个模型:</p><ul class=""><li id="3437" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">具有分组特征选择属性。</strong>一个组的变量要么全部被选择(分配非零系数)，要么被排除(分配零系数)。</li><li id="02bb" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated">平等对待一个群体的变量。我们使用的是L2范数，众所周知它是各向同性的，因此不会优先考虑一个组中的任何变量——在我们的例子中，是分类变量的虚拟变量。</li><li id="ef58" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated"><strong class="ky ir">处理单个变量和组变量的混合。</strong>一维空间中的L2范数是绝对值。因此，如果组由一个变量组成，组套索就简化为套索。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/538146d844a56973bb1215b64bd714ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*wrgg1Ho-E0HNbasOAMmYdA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">一维空间中的L2范数是绝对值。[作者插图]</p></figure><p id="1a9a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基于此，我们可以使用组套索来执行连续/分类变量混合的特征选择。<strong class="ky ir">连续变量将被视为单变量组，分类变量将被视为虚拟变量组。</strong></p><h1 id="3b67" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">使用celer拟合群组套索模型</h1><p id="f4ff" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">scikit-learn 是当今ML流行背后的著名python包之一。其用户友好的API和全面的文档降低了进入ML领域的门槛，并使非从业者能够毫无困难地从中受益。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/750e86837816266c79173226e0e0ca07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*7N8w06MjrHVXBrNCEyg4dQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">[作者的迷因]</p></figure><p id="466e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不幸的是，scikit-learn实现中缺少组Lasso。然而，这一点也不奇怪，因为无数的是今天的ML模型，更不用说当你考虑它们的变种！</p><p id="cf24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">很可能，有几个倡议旨在通过标准化ML模型的实现并使它们与scikit-learn API保持一致来保持ML对普通公众的可访问性。</p><h2 id="1c58" class="nf mi iq bd mj ng nh dn mn ni nj dp mr lf nk nl mt lj nm nn mv ln no np mx nq bi translated">celer:一个scikit-learn API conform包，用于套索类模型</h2><p id="5d44" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated"><a class="ae kv" href="https://mathurinm.github.io/celer/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">celer</strong></a><strong class="ky ir"/>是一个python包，它包含了<a class="ae kv" href="https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html#:~:text=This%20package%20focuses%20on%20bringing%20machine%20learning%20to%20non%2Dspecialists%20using%20a%20general%2Dpurpose%20high%2Dlevel%20language.%20Emphasis%20is%20put%20on%20ease%20of%20use%2C%20performance%2C%20documentation%2C%20and%20API%20consistency." rel="noopener ugc nofollow" target="_blank"> scikit-learn愿景</a>，并提供了完全在scikit-learn API下设计的模型，从而与它很好地集成在一起——可以与管道和GridSearchCV等一起使用。因此，拟合<strong class="ky ir"> celer </strong>的模型就像为scikit-learn做同样的事情一样简单。</p><p id="3398" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，<strong class="ky ir"> celer </strong>专门设计用于处理套索类模型，如套索和组合套索。因此，它具有定制的实现，使其能够快速适应这些类型的模型，比scikit-learn 快100倍，并有效地处理大型数据集。</p><h2 id="b7cb" class="nf mi iq bd mj ng nh dn mn ni nj dp mr lf nk nl mt lj nm nn mv ln no np mx nq bi translated">celer Group Lasso入门</h2><p id="a4e9" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">在通过pip安装了<strong class="ky ir"> celer </strong>之后，您可以轻松地安装一个群组套索模型，如下面的代码片段所示。这里，考虑一个具有一个连续变量和两个分类变量的玩具数据集。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="ak"> celer </strong>组套索的启动器示例</p></figure><p id="9c93" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们已经成功地拟合了一组套索，今后剩下的工作就是检查结果解(非零系数)来决定要选择的变量。</p><p id="609a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">理想情况下，在特征选择方面，我们对回答这个问题很感兴趣:<strong class="ky ir">最相关的特征是什么？</strong>因此，我们不能声称我们100%回答了这个问题，因为非零系数——要选择的变量——取决于惩罚的强度<em class="mb"> α </em>，因此我们不能完全控制它们。</p><h1 id="fda2" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">通过惩罚强度控制所选特征的数量</h1><p id="1aeb" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">我们从0开始增加<em class="mb"> α </em>越多——纯线性回归——过滤掉的变量越多——分配零系数。最终，对于“足够大”<em class="mb"> α </em>，我们得到一个零解——所有系数都为零。</p><p id="cae5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这意味着一个变量被高<em class="mb">，</em>α驱逐后“幸存”得越多，它就越显示出它对模型的重要性。因此，我们可以依靠惩罚强度对变量进行评分。</p><p id="0443" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有趣的是，给定一个响应和一组变量，我们可以证明<a class="ae kv" href="https://arxiv.org/pdf/1602.06225.pdf" rel="noopener ugc nofollow" target="_blank">存在一个<em class="mb"> α_max </em> </a> <em class="mb"> </em>，在这个值之上，零解是唯一一个折衷数据保真度和损失的解。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/d5cd67a62b6186aa41a1a50a6567b3a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X0BP5yUTayxEMEPsPLrD-A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在一个网格(0，<em class="oc"> α_max)上对玩具数据集上的<em class="oc">进行套索分组。注意，在α_max以上，所有系数都为零。</em>【作者插图】</em></p></figure><p id="c0b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这意味着每个变量都有一个介于0和<em class="mb"> α_max之间的有限分数。</em>此外，由于我们有一个明确的公式<em class="mb"> α_max，</em>我们可以通过<em class="mb"> α_max </em>进一步归一化分数，以获得介于0和1之间的分数。</p><p id="78dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总结一下方法，我们先在0 <em class="mb"> </em>和<em class="mb"> α_max之间生成一个<em class="mb"> α </em>的网格。</em>然后，我们在每个<em class="mb"> α </em>上拟合群组套索，并跟踪变量何时被分配零系数——这将是它们相应的分数<em class="mb">。</em>最后，我们将这些分数标准化，然后根据这些分数对变量进行排名。</p><p id="f02a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">除了最后一步，以上所有步骤都由<a class="ae kv" href="https://mathurinm.github.io/celer/generated/celer.celer_path.html" rel="noopener ugc nofollow" target="_blank"> celer_path </a>精心处理——celer模型的构建模块。</p><p id="e4e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">回到原来的问题:<strong class="ky ir">最热门的<em class="mb"> K的</em>最相关的特征是什么？</strong>剩下的就是<strong class="ky ir">根据分数对特征进行降序排序，选择前<em class="mb"> K </em>个。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/6341ed3df8a0abafd1e39a3a4165fb4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pJr_H9UuOsETYDbvRzZ-4A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用所考虑的方法得到的玩具数据集的特征分数。[作者插图]</p></figure><h1 id="df07" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">总结和结论</h1><p id="eb37" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">线性回归无疑仍然是最简单和易于理解的模型之一，它与精心选择的惩罚相结合，产生了更多可解释的结果。</p><p id="1cf5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们扩展了线性回归，使用套索组模型对连续/分类变量的混合物进行特征选择。</p><p id="de23" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们使用<strong class="ky ir"> celer </strong>来拟合组Lasso，并依靠其核心解算器— <em class="mb"> celer_path — </em>来控制所选特征的数量。</p><p id="3b50" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我制作了一个详细的GitHub存储库，其中我将介绍的特征选择技术应用于一个真实的数据集。您可以在<a class="ae kv" href="https://github.com/Badr-MOUFAD/beyond-OLS-med-supp-material" rel="noopener ugc nofollow" target="_blank">链接</a>中查看源代码，并利用开发的python实用程序将其应用到您自己的用例中。</p><p id="d7f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">有用链接:</strong></p><ul class=""><li id="45db" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">真实数据集上的例子:</strong><a class="ae kv" href="https://github.com/Badr-MOUFAD/beyond-OLS-med-supp-material" rel="noopener ugc nofollow" target="_blank">https://github.com/Badr-MOUFAD/beyond-OLS-med-supp-material</a></li><li id="68ef" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated"><strong class="ky ir">策勒文档:【https://mathurinm.github.io/celer/】T22</strong></li><li id="75ae" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated"><strong class="ky ir"> celer GitHub库:</strong>T26】https://github.com/mathurinm/celer</li></ul></div></div>    
</body>
</html>