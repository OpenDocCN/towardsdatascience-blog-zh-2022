<html>
<head>
<title>PySpark Recipes: Map And Unpivot</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark配方:映射和取消透视</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pyspark-recipes-map-and-unpivot-d724cca7d0d4#2022-06-11">https://towardsdatascience.com/pyspark-recipes-map-and-unpivot-d724cca7d0d4#2022-06-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="01d6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">PySpark API真的缺少关键功能吗？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ea0afe5c3784cea077230647b78fe92c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*j6BPp64J3xQccVbW"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@williambout?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">威廉·布特</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="7f39" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PySpark提供了一个流畅的API，可以满足大多数需求。尽管如此，有经验的pandas用户可能会发现一些数据转换并不那么简单。本文旨在提供少量的配方来涵盖一些用户可能认为PySpark API本身不支持的用例。实际上它们是被支持的，但是它们确实需要更多的努力(和想象力)。</p><p id="7130" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们从必要的导入和spark会话的创建开始</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="a888" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，出于本文的目的，我们使用一个本地PySpark环境，它有4个内核和8gb内存。我们将为每个配方创建简单的数据框。</p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><p id="2aae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">地图值</strong></p><p id="da3d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一个方法处理映射值，并基于创建一个映射列</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="eb71" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">映射键值对存储在字典中。构造<code class="fe mb mc md me b">chain(*mapping.items())</code>返回一个键值对的链对象as (key1，value1，key2，value2，...)，这些键值对用于创建映射<a class="ae kv" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.create_map.html" rel="noopener ugc nofollow" target="_blank">列</a>。这个映射列本质上是一个常量，因此，我们在数据框的每一行中都有相同的映射。映射是通过检索原始列中每个键的映射值来实现的。原始列可能包含在映射中作为键存在的值，这些值因此被映射为null。原始列中的空值不会导致错误，并且在映射后保持为空。可以使用默认值代替空值，例如</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="bade" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了完整起见，也可以在spark上使用pandas API来实现相同的结果</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="f9d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是本文并没有深入研究这个新的API，它是一个很大的发展，尤其是对于原型开发而言。我在结论中包含了更多关于这个问题的想法。</p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><p id="336d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">融化(又称逆透视)</strong></p><p id="501b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用过熊猫的用户可能想知道我们如何模仿熊猫API中的<a class="ae kv" href="https://pandas.pydata.org/docs/reference/api/pandas.melt.html" rel="noopener ugc nofollow" target="_blank">融化</a>功能。Melt将数据帧转换为一种格式，其中一列或多列为标识符变量(id_vars)，而所有其他列(被视为测量变量(value_vars ))被“解投影”到行轴。换句话说，数据帧从宽格式转换为长格式。</p><p id="3ef2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">演示该配方的起始数据框可以用以下内容构建</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="3066" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在PySpark中可能有几种方法来实现熔化函数。在本教程中，我们演示了两个，试图介绍一种通用的思维方式，这种方式在其他情况下也是有帮助的。第一个<a class="ae kv" href="https://stackoverflow.com/questions/41670103/how-to-melt-spark-dataframe" rel="noopener ugc nofollow" target="_blank">解决方案</a>构造了一个随后被分解的结构数组</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="9611" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了增加行数和减少列数，我们需要以某种方式将值变量的内容打包到一个容器中，然后将容器分解成多行。使用一个理解，我们创建一个结构体数组，该数组被展开并存储在一个新创建的名为<code class="fe mb mc md me b">_vars_and_vals</code>的列中。展开后，我们在每一行中都有一个结构，可以从中检索所有值变量的列名和列值。</p><p id="9f2f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">举例来说，我们可以在起始数据框中取消透视一些或所有列</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="01ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二个<a class="ae kv" href="https://stackoverflow.com/questions/70710359/unpivot-dataframe-in-pyspark-with-new-column" rel="noopener ugc nofollow" target="_blank">解决方案</a>依赖于从数组中创建一个地图，然后对其进行分解。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="834d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">地图的创建也可以通过其他方式来实现，例如<code class="fe mb mc md me b">F.create_map(*chain(*((F.lit(x), F.col(x)) for x in value_vars))). </code>使用地图作为容器可能不太明显，因为分解地图可能不会立即出现在脑海中，尽管<a class="ae kv" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.explode.html" rel="noopener ugc nofollow" target="_blank">文档</a>对此有明确说明。这两个解决方案提供了相同的结果，但是我不确定哪个解决方案的性能更好(欢迎评论)。就可读性而言，我个人认为第一个解决方案更好。</p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><p id="9e56" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">结论</strong></p><p id="f0f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">希望这篇文章提供了关于如何使用生成和处理容器(如映射、数组和结构)的<code class="fe mb mc md me b">pyspark.sql.functions</code>来模拟众所周知的pandas函数的见解。另一个选择是使用最近推出的<a class="ae kv" href="https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html" rel="noopener ugc nofollow" target="_blank"> PySpark pandas API </a>，在Spark v3.2之前它曾被称为<a class="ae kv" href="https://koalas.readthedocs.io/en/latest/whatsnew/v1.8.2.html" rel="noopener ugc nofollow" target="_blank">考拉</a>。在<a class="ae kv" href="https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/faq.html#should-i-use-pyspark-s-dataframe-api-or-pandas-api-on-spark" rel="noopener ugc nofollow" target="_blank">文档</a>中的官方建议是，如果你已经熟悉pandas，就使用PySpark pandas API，如果你不熟悉，就直接使用PySpark API。我认为自己是一个有经验的pandas用户，但是除了快速原型之外，我仍然更喜欢依赖PySpark API。主要原因是，与考虑直接使用PySpark API的解决方案相比，我不需要担心可能需要更多时间才能发现的不兼容性。例如，Spark提供了null(在SQL意义上，作为缺失值)和nan(数字而不是数字)，而pandas没有可用于表示缺失值的原生值。在使用PySpark pandas API时用pandas思考，在某种程度上就像试图通过翻译你的母语来说一种语言。从长远来看，直接用新语言思考可能更好。这只是个人观点，并不是对PySpark熊猫API质量的批评。我确信有不同的观点，意见的多样性对取得进展至关重要。</p></div></div>    
</body>
</html>