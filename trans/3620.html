<html>
<head>
<title>Quick-fire Guide to Multi-Modal ML With OpenAI’s CLIP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多模态ML快速入门指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/quick-fire-guide-to-multi-modal-ml-with-openais-clip-2dad7e398ac0#2022-08-11">https://towardsdatascience.com/quick-fire-guide-to-multi-modal-ml-with-openais-clip-2dad7e398ac0#2022-08-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4097" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解如何使用剪辑和矢量嵌入在文本和图像之间进行转换</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6ee01431494a2c61fc6d2655e1b901db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kgq9SyyeDazYahgfBLJyxQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者修改，背景照片由<a class="ae ky" href="https://unsplash.com/@maximalfocus?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Maximalfocus </a>在<a class="ae ky" href="https://unsplash.com/s/photos/future?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="90ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">经过短短几年的生活，儿童可以理解简单单词背后的概念，并将它们与相关图像联系起来。他们能够识别物理世界的形状和纹理与书面语言的抽象符号之间的联系。</p><p id="bd97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是我们认为理所当然的事情。世界上很少有人(如果有的话)会记得这些“基本”技能超出他们能力的时候。</p><p id="e33d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">电脑不一样。他们可以计算出火箭穿越太阳系所需的参数。但是如果你让一台电脑找到一张“公园里的一只狗”的图像，你最好向美国国家航空航天局索要一张去空间站的免费机票。</p><p id="4b44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">至少，直到最近还是如此。</p><p id="3bfb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们将看看OpenAI的剪辑。能够理解文本和图像之间的关系和概念的“多模态”模型。正如我们将会看到的，剪辑不仅仅是一个花哨的客厅把戏。它能力惊人。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lv lw l"/></div></figure></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="da7e" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">对比学习？</h1><p id="5a95" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated"><strong class="lb iu">C</strong>ontrastive<strong class="lb iu">L</strong>language-<strong class="lb iu">I</strong>mage<strong class="lb iu">P</strong>再训练(CLIP)由两个模型并行训练而成。用于图像嵌入的视觉变换器(ViT) <em class="nb">或</em> ResNet模型和用于语言嵌入的变换器模型。</p><p id="97c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在训练期间，<em class="nb">(图像，文本)</em>对被馈送到相应的模型中，并且都输出表示矢量空间中相应图像/文本的512维矢量嵌入。</p><p id="0324" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nb">对比</em>组件采用这两个向量嵌入，并计算模型损失作为两个向量之间的差异(例如，对比度)。然后优化两个模型以最小化这种差异，并因此学习如何将相似的<em class="nb">(图像，文本)</em>对嵌入到相似的向量空间中。</p><p id="dd1f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个对比预训练过程之后，我们剩下CLIP，一个能够通过共享向量空间理解语言和图像的多模态模型。</p><h1 id="57b7" class="me mf it bd mg mh nc mj mk ml nd mn mo jz ne ka mq kc nf kd ms kf ng kg mu mv bi translated">使用夹子</h1><p id="fa04" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">OpenAI开发并发布了<code class="fe nh ni nj nk b">clip</code>库，可以在GitHub这里找到。然而，Hugging Face的<code class="fe nh ni nj nk b">transformers</code>库托管了CLIP的另一个实现(也是由OpenAI构建的)，这个实现更常用。</p><p id="230b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">拥抱脸实现<em class="nb">不</em>使用ResNet进行图像编码。它使用与文本转换器配对的ViT模型的替代设置。我们将通过把一个简单的文本-图像搜索脚本串在一起来学习如何使用这个实现，这个脚本可以适用于图像-图像、文本-文本和图像-文本模态。</p><h2 id="28dd" class="nl mf it bd mg nm nn dn mk no np dp mo li nq nr mq lm ns nt ms lq nu nv mu nw bi translated">加载数据和剪辑</h2><p id="3d4f" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">首先，我们将安装演示所需的库，下载数据集，并初始化CLIP。</p><pre class="kj kk kl km gt nx nk ny nz aw oa bi"><span id="b569" class="nl mf it nk b gy ob oc l od oe">pip install -U torch datasets transformers</span></pre><p id="ad57" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用<em class="nb">“imagenette”</em>数据集，这是由拥抱脸托管的～10K图像的集合。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of lw l"/></div></figure><p id="e476" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这给了我们从收音机到狗的各种图像。所有这些图像都作为PIL图像对象存储在<code class="fe nh ni nj nk b">'image'</code>特征中。</p><p id="27e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们通过<code class="fe nh ni nj nk b">transformers</code>库初始化CLIP，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of lw l"/></div></figure><p id="ce9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里发生了一些事情:</p><ul class=""><li id="dfd6" class="og oh it lb b lc ld lf lg li oi lm oj lq ok lu ol om on oo bi translated">整个<code class="fe nh ni nj nk b">device</code>部分是设置我们的实例，以使用我们可用的最快的硬件(M1芯片上的MPS，否则CUDA)。</li><li id="cf56" class="og oh it lb b lc op lf oq li or lm os lq ot lu ol om on oo bi translated">我们设定了<code class="fe nh ni nj nk b">model_id</code>。这是在这里找到的剪辑型号<a class="ae ky" href="https://huggingface.co/openai/clip-vit-base-patch32" rel="noopener ugc nofollow" target="_blank">的名称</a>。</li><li id="e56d" class="og oh it lb b lc op lf oq li or lm os lq ot lu ol om on oo bi translated">然后我们初始化一个用于预处理文本的<code class="fe nh ni nj nk b">tokenizer</code>，一个用于预处理图像的<code class="fe nh ni nj nk b">processor</code>，以及一个用于生成矢量嵌入的剪辑<code class="fe nh ni nj nk b">model</code>。</li></ul><p id="54fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们准备开始创建文本和图像嵌入。</p><h2 id="f1e7" class="nl mf it bd mg nm nn dn mk no np dp mo li nq nr mq lm ns nt ms lq nu nv mu nw bi translated">创建文本嵌入</h2><p id="acac" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">文本转换器模型将我们的文本编码成有意义的矢量嵌入。为此，我们首先对文本进行标记化，将其从人类可读的文本转换为转换器可读的标记。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of lw l"/></div></figure><p id="cbda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后使用<code class="fe nh ni nj nk b">get_text_features</code>方法将这些令牌输入模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of lw l"/></div></figure><p id="f2f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们有一个512维的向量来表示短语“雪中的狗”的语义含义。这是我们文本图像搜索的一半。</p><h2 id="c15b" class="nl mf it bd mg nm nn dn mk no np dp mo li nq nr mq lm ns nt ms lq nu nv mu nw bi translated">创建图像嵌入</h2><p id="358b" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">下一步是创建图像嵌入。同样，这非常简单。我们将<code class="fe nh ni nj nk b">tokenizer</code>换成<code class="fe nh ni nj nk b">processor</code>，这将给我们一个叫做<code class="fe nh ni nj nk b">pixel_values</code>的调整后的图像张量。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of lw l"/></div></figure><p id="3540" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们仍然可以看到处理过的图像。它已被调整大小，像素“激活”值不再在Matplotlib可以读取的0–255的典型RGB范围内，因此颜色无法正确显示。尽管如此，我们可以看到这是我们之前看到的同一台索尼收音机。</p><p id="5130" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们用CLIP处理这些输入，这次使用的是<code class="fe nh ni nj nk b">get_image_features</code>方法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of lw l"/></div></figure><p id="b6da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在此基础上，我们用剪辑为文本和图像构建了矢量嵌入。有了这些嵌入，我们可以使用像欧几里德距离、余弦相似性或点积相似性这样的度量来比较它们的相似性。</p><p id="30cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，我们不能只拿一个例子来比较，所以让我们继续，在更大的图像样本上测试。</p><p id="533a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将从<code class="fe nh ni nj nk b">imagenette</code>数据中随机抽取<em class="nb"> 100张</em>图像。为此，我们首先随机选择100个索引位置，并使用它们来构建图像列表。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of lw l"/></div></figure><p id="07b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们遍历这100张图片，用CLIP创建图片嵌入。我们将它们全部添加到一个名为<code class="fe nh ni nj nk b">image_arr</code>的Numpy数组中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of lw l"/></div></figure><p id="d041" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在底部的单元格中，我们可以看到我们的图像嵌入中的最小值和最大值分别是<code class="fe nh ni nj nk b">-7.99</code>和<code class="fe nh ni nj nk b">+3.15</code>。我们将使用<em class="nb">点积相似度</em>来比较我们的向量。如果我们想准确地将它们与点积进行比较，我们需要将它们归一化。我们这样做:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of lw l"/></div></figure><p id="8f27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们准备比较和搜索我们的向量。</p><h2 id="4712" class="nl mf it bd mg nm nn dn mk no np dp mo li nq nr mq lm ns nt ms lq nu nv mu nw bi translated">文本图像搜索</h2><p id="e3c0" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">如前所述，我们将使用点积来比较矢量。文本嵌入将充当“查询”,我们将使用它来搜索最相似的图像嵌入。</p><p id="d5d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先计算查询和图像之间的点积相似度:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of lw l"/></div></figure><p id="a2da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这给了我们100分，例如，每个文本嵌入到图像嵌入对的一对多得分。我们所知道的是按照降序对这些分数进行排序，并返回各自得分最高的图像。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of lw l"/></div></figure><p id="369b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在1号位置，我们有一只狗在雪地里，这是一个很好的结果！这是<em class="nb">很可能是</em>在我们的100张图片样本中<em class="nb">唯一的</em>一张狗在雪中的图片。这就是我们如何使用CLIP执行文本图像搜索。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="a5da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CLIP是一个惊人的模型，可以以任何顺序或组合应用于语言图像领域。我们可以使用相同的方法执行文本-文本、图像-图像和图像-文本搜索。</p><p id="cc4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实上，我们可以通过简单地将图像和文本向量添加到一个存储中，然后使用图像或文本进行查询，来同时完成所有这些工作。</p><p id="fe73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你坚持使用较少的搜索项目，我们的方法是很好的。然而，当我们开始搜索更多的记录时，这是缓慢的，甚至是不可能的。为此，我们需要一个<a class="ae ky" href="https://www.pinecone.io/learn/vector-database/" rel="noopener ugc nofollow" target="_blank">向量数据库</a>。允许我们将它扩展到数百万甚至数十亿条记录。</p><p id="c642" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你有兴趣了解更多关于多模态模型、自然语言处理或矢量搜索的知识，可以查看我的<a class="ae ky" href="https://www.youtube.com/c/jamesbriggs" rel="noopener ugc nofollow" target="_blank"> YouTube频道</a>，联系<a class="ae ky" href="https://discord.gg/c5QtDB9RAP" rel="noopener ugc nofollow" target="_blank"> Discord </a>，或者跟随我的一门免费课程(下面的链接)。</p><p id="1591" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><div class="kj kk kl km gt ou"><a href="https://www.pinecone.io/learn/nlp/" rel="noopener  ugc nofollow" target="_blank"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">用于语义搜索的自然语言处理(NLP)</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">语义搜索长期以来一直是谷歌、亚马逊和谷歌等巨头的技术栈中的关键组件。</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">www.pinecone.io</p></div></div><div class="pd l"><div class="pe l pf pg ph pd pi ks ou"/></div></div></a></div><div class="pj pk gp gr pl ou"><a href="https://www.pinecone.io/learn/image-search/" rel="noopener  ugc nofollow" target="_blank"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">图像搜索的嵌入方法|松果</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">了解如何让机器像人一样理解图像。这个免费课程涵盖了你需要的一切…</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">www.pinecone.io</p></div></div><div class="pd l"><div class="pm l pf pg ph pd pi ks ou"/></div></div></a></div></div></div>    
</body>
</html>