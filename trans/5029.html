<html>
<head>
<title>Machine Learning Algorithms Cheat Sheet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习算法备忘单</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-algorithms-cheat-sheet-2f01d1d3aa37#2022-11-09">https://towardsdatascience.com/machine-learning-algorithms-cheat-sheet-2f01d1d3aa37#2022-11-09</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="380a" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">5种常见算法的快速参考指南</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/5732c39b6468f95db2a8d3486037db72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0qgxAws1SVugxsgIJvQ7Hw.jpeg"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">凯勒·琼斯在<a class="ae kz" href="https://unsplash.com/s/photos/choice?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="b299" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">机器学习的“<strong class="lc iv"> <em class="lw">没有免费的午餐”</em> </strong>定理指出，没有单一的机器学习算法可以解决所有类型的机器学习问题。</p><p id="083e" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">机器学习任务可能会有很大差异，算法的选择将取决于数据的大小、维度和稀疏性等因素。目标变量、数据质量以及特征内部和特征与目标变量之间存在的相互作用和统计关系。</p><p id="8296" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">因此，不可能简单地选择一种算法用于“一刀切”的方法。不同的算法将更适合特定的任务，这取决于它们的具体工作方式。数据科学家通常会选择最终使用的算法，首先确定适合特定问题的算法子集，然后对这些算法进行实验以找到最佳选择。</p><p id="c1f4" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在本文中，我将提供一个快速参考指南，介绍机器学习中最常用的五种算法。这将介绍算法的内部工作原理，以及使每个算法更适合某些任务的考虑因素。</p><p id="a98a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这将包括对线性回归、逻辑回归、随机森林、XGBoost和K-means的简要介绍。对于每个算法，我将涵盖以下内容:</p><ol class=""><li id="a71e" class="lx ly iu lc b ld le lg lh lj lz ln ma lr mb lv mc md me mf bi translated"><strong class="lc iv">算法如何工作。</strong></li><li id="73ff" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated"><strong class="lc iv">一个示例代码实现。</strong></li><li id="fc1b" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated"><strong class="lc iv">在适当情况下使用算法的指南。</strong></li><li id="e612" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated"><strong class="lc iv">优缺点。</strong></li></ol></div><div class="ab cl ml mm hy mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="in io ip iq ir"><h1 id="ae02" class="ms mt iu bd mu mv mw mx my mz na nb nc ka nd kb ne kd nf ke ng kg nh kh ni nj bi translated">1.线性回归</h1><p id="58fc" class="pw-post-body-paragraph la lb iu lc b ld nk jv lf lg nl jy li lj nm ll lm ln nn lp lq lr no lt lu lv in bi translated">线性回归是一种受监督的机器学习算法，用于预测连续的目标变量。对于简单的线性回归，其中有一个<strong class="lc iv">自变量</strong>(特征)和一个<strong class="lc iv">因变量</strong>(目标)，该算法可由以下等式表示。</p><blockquote class="np"><p id="1b8b" class="nq nr iu bd ns nt nu nv nw nx ny lv dk translated">y = a + bX</p></blockquote><p id="8133" class="pw-post-body-paragraph la lb iu lc b ld nz jv lf lg oa jy li lj ob ll lm ln oc lp lq lr od lt lu lv in bi translated">其中<strong class="lc iv"> y </strong>为因变量，<strong class="lc iv"> X </strong>为解释变量，<strong class="lc iv">T25】b</strong>为直线的斜率，<strong class="lc iv"> <em class="lw"> a </em> </strong>为截距。</p><p id="a52f" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">简单的线性回归可视为散点图，其中x轴包含因变量，y轴包含自变量。线性回归算法通过数据点绘制一条最佳拟合线，使预测输出和实际输出之间的差异最小化。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oe"><img src="../Images/4dbfaee38921b0920e4b30419e598f63.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*HoCns-WUPzjGfw4XHAteTg.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">线性回归。作者图片</p></figure><h2 id="3115" class="of mt iu bd mu og oh dn my oi oj dp nc lj ok ol ne ln om on ng lr oo op ni oq bi translated">代码示例</h2><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="or os l"/></div></figure><h2 id="67a9" class="of mt iu bd mu og oh dn my oi oj dp nc lj ok ol ne ln om on ng lr oo op ni oq bi translated">应该在什么时候使用？</h2><ul class=""><li id="07fc" class="lx ly iu lc b ld nk lg nl lj ot ln ou lr ov lv ow md me mf bi translated">线性回归只能用于解决基于回归的问题。</li><li id="d988" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">因变量和自变量之间必须有线性关系。</li><li id="af16" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">残差必须形成正态分布。</li><li id="c305" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">特征之间必须没有相关性。</li><li id="ca84" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">该算法假设训练数据是随机采样的。</li><li id="eaf7" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">最适合基于回归的问题，其中数据中的关系既线性又简单。</li></ul><h2 id="bd3f" class="of mt iu bd mu og oh dn my oi oj dp nc lj ok ol ne ln om on ng lr oo op ni oq bi translated">优势</h2><ul class=""><li id="7d56" class="lx ly iu lc b ld nk lg nl lj ot ln ou lr ov lv ow md me mf bi translated">高度可解释性和快速训练。</li><li id="6511" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">在线性可分数据上表现非常好。</li></ul><h2 id="ab71" class="of mt iu bd mu og oh dn my oi oj dp nc lj ok ol ne ln om on ng lr oo op ni oq bi translated">不足之处</h2><ul class=""><li id="1aa3" class="lx ly iu lc b ld nk lg nl lj ot ln ou lr ov lv ow md me mf bi translated">对异常值不稳健。</li><li id="f656" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">非常简单，因此它不能很好地模拟现实世界数据中的复杂性。</li><li id="20b3" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">这种算法也容易过拟合。</li></ul></div><div class="ab cl ml mm hy mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="in io ip iq ir"><h1 id="49e5" class="ms mt iu bd mu mv mw mx my mz na nb nc ka nd kb ne kd nf ke ng kg nh kh ni nj bi translated">2.逻辑回归</h1><p id="f5ba" class="pw-post-body-paragraph la lb iu lc b ld nk jv lf lg nl jy li lj nm ll lm ln nn lp lq lr no lt lu lv in bi translated">逻辑回归本质上是为适应分类问题而建模的线性回归。逻辑回归不是拟合直线，而是应用<strong class="lc iv"> </strong> <a class="ae kz" href="https://en.wikipedia.org/wiki/Logistic_function" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iv">逻辑函数</strong> </a>来压缩0和1之间的线性方程的输出。结果是穿过数据点的S形曲线而不是直线，如下图所示。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj ox"><img src="../Images/2be40ccca83db33fbbcb7f79785a5f4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kght8nhdjhKXfCHwJZ-7wg.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">逻辑回归曲线。作者图片</p></figure><p id="25af" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">选择介于0和1之间的阈值来分隔类别，通常为0.5。本质上，我们在0.5处画一条横过S曲线的水平线。这条线以上的任何数据点都属于1类，线以下的任何数据点都属于0类。</p><h2 id="c1f8" class="of mt iu bd mu og oh dn my oi oj dp nc lj ok ol ne ln om on ng lr oo op ni oq bi translated">代码示例</h2><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="or os l"/></div></figure><h2 id="e755" class="of mt iu bd mu og oh dn my oi oj dp nc lj ok ol ne ln om on ng lr oo op ni oq bi translated">应该在什么时候使用？</h2><ul class=""><li id="a412" class="lx ly iu lc b ld nk lg nl lj ot ln ou lr ov lv ow md me mf bi translated">这种算法只能用于解决分类问题。</li><li id="e5c6" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">特征和目标变量之间必须有线性关系。</li><li id="e194" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">观察值的数量必须大于特征的数量。</li><li id="07cc" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">最适合于数据关系既线性又简单的分类问题。</li></ul><h2 id="b623" class="of mt iu bd mu og oh dn my oi oj dp nc lj ok ol ne ln om on ng lr oo op ni oq bi translated">优势</h2><ul class=""><li id="e386" class="lx ly iu lc b ld nk lg nl lj ot ln ou lr ov lv ow md me mf bi translated">与线性回归一样，该算法可解释性强，训练速度快。</li><li id="12ad" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">它在线性可分数据上表现非常好。</li></ul><h2 id="7ba2" class="of mt iu bd mu og oh dn my oi oj dp nc lj ok ol ne ln om on ng lr oo op ni oq bi translated">不足之处</h2><ul class=""><li id="7734" class="lx ly iu lc b ld nk lg nl lj ot ln ou lr ov lv ow md me mf bi translated">容易过度拟合。</li><li id="0f99" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">与线性回归一样，它不能很好地模拟复杂的关系。</li></ul></div><div class="ab cl ml mm hy mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="in io ip iq ir"><h1 id="9841" class="ms mt iu bd mu mv mw mx my mz na nb nc ka nd kb ne kd nf ke ng kg nh kh ni nj bi translated">3.随机森林</h1><p id="3048" class="pw-post-body-paragraph la lb iu lc b ld nk jv lf lg nl jy li lj nm ll lm ln nn lp lq lr no lt lu lv in bi translated">随机森林算法构建了一个<strong class="lc iv">决策树</strong>的“森林”。森林中的每棵树都根据一组给定的特征生成一个预测。一旦生成了所有预测，就进行多数投票，并且最常预测的类别形成最终预测。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj oy"><img src="../Images/29134f88b53f28172e573e8c78baa2ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8xz7AN5qk8aDwzuiUqHpFQ.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">随机森林生成的单一决策树。作者图片</p></figure><p id="8519" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">决策树是一种非常直观的算法。它有一个类似流程图的结构，包含一系列代表测试的节点。每个测试的结果导致一个分裂，并且一个或多个<strong class="lc iv">叶节点</strong>被创建，直到实现最终预测。超参数决定了树增长的深度和使用的节点分裂函数。</p><p id="c83c" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">随机森林算法遵循以下步骤序列:</p><ol class=""><li id="98fd" class="lx ly iu lc b ld le lg lh lj lz ln ma lr mb lv mc md me mf bi translated">根据森林中树木的数量，将训练数据集随机分成多个样本。树的数量通过超参数设置。</li><li id="e2f4" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated">使用其中一个数据子集并行训练决策树。</li><li id="778d" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated">评估所有树的输出，并将最常发生的预测作为最终结果。</li></ol><h2 id="1d2a" class="of mt iu bd mu og oh dn my oi oj dp nc lj ok ol ne ln om on ng lr oo op ni oq bi translated">代码示例</h2><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="or os l"/></div></figure><h2 id="d555" class="of mt iu bd mu og oh dn my oi oj dp nc lj ok ol ne ln om on ng lr oo op ni oq bi translated">应该在什么时候使用？</h2><ul class=""><li id="f2a5" class="lx ly iu lc b ld nk lg nl lj ot ln ou lr ov lv ow md me mf bi translated">该算法可用于解决基于分类和回归的问题。</li><li id="24fc" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">由于该算法固有地执行特征选择，因此它特别适合于具有高维数的大型数据集。</li></ul><h2 id="2ff5" class="of mt iu bd mu og oh dn my oi oj dp nc lj ok ol ne ln om on ng lr oo op ni oq bi translated">优势</h2><ul class=""><li id="5f1b" class="lx ly iu lc b ld nk lg nl lj ot ln ou lr ov lv ow md me mf bi translated">它可以模拟线性和非线性关系。</li><li id="7964" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">它对异常值不敏感。</li><li id="1a13" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">随机森林能够在包含缺失数据的数据集上运行良好。</li></ul><h2 id="4e11" class="of mt iu bd mu og oh dn my oi oj dp nc lj ok ol ne ln om on ng lr oo op ni oq bi translated">不足之处</h2><ul class=""><li id="f6b5" class="lx ly iu lc b ld nk lg nl lj ot ln ou lr ov lv ow md me mf bi translated">随机森林很容易过度适应，虽然这可以通过<strong class="lc iv">修剪</strong>得到一定程度的缓解。</li><li id="7778" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">它不像线性回归和逻辑回归那样具有可解释性，尽管可以提取特征重要性来提供某种程度的可解释性。</li></ul></div><div class="ab cl ml mm hy mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="in io ip iq ir"><h1 id="cb72" class="ms mt iu bd mu mv mw mx my mz na nb nc ka nd kb ne kd nf ke ng kg nh kh ni nj bi translated">4.XGBoost</h1><p id="f54b" class="pw-post-body-paragraph la lb iu lc b ld nk jv lf lg nl jy li lj nm ll lm ln nn lp lq lr no lt lu lv in bi translated">XGBoost是一种基于<strong class="lc iv">梯度推进</strong>决策树的算法。它与Random Forest相似，都是构建一个决策树集合，但XGBoost不是并行训练模型，而是顺序训练模型。每个决策树从先前模型产生的错误中学习。这种顺序训练模型的技术被称为<strong class="lc iv">增强</strong>。</p><p id="66bc" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">XGBoost中的梯度指的是使用<strong class="lc iv">弱学习者</strong>的特定类型的增强。弱学习者是非常简单的模型，仅仅比随机机会表现得更好。该算法从一个初始的弱学习者开始。每个随后的模型都以先前决策树产生的错误为目标。这种情况会持续下去，直到无法取得进一步的改进，并最终形成一个强学习者模型。</p><h2 id="53f4" class="of mt iu bd mu og oh dn my oi oj dp nc lj ok ol ne ln om on ng lr oo op ni oq bi translated">代码示例</h2><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="or os l"/></div></figure><h2 id="04b1" class="of mt iu bd mu og oh dn my oi oj dp nc lj ok ol ne ln om on ng lr oo op ni oq bi translated">应该在什么时候使用？</h2><ul class=""><li id="a414" class="lx ly iu lc b ld nk lg nl lj ot ln ou lr ov lv ow md me mf bi translated">它可用于解决基于分类和回归的问题。</li><li id="926a" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">XGBoost通常被认为是对结构化数据进行监督学习的最佳和最灵活的算法之一，因此适用于广泛的数据集和问题类型。</li></ul><h2 id="c50d" class="of mt iu bd mu og oh dn my oi oj dp nc lj ok ol ne ln om on ng lr oo op ni oq bi translated">优势</h2><ul class=""><li id="ebd4" class="lx ly iu lc b ld nk lg nl lj ot ln ou lr ov lv ow md me mf bi translated">XGboost非常灵活，在小型和大型数据集上都能很好地工作。</li><li id="ecf6" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">与其他复杂算法相比，该算法计算效率高，因此训练模型更快。</li></ul><h2 id="1133" class="of mt iu bd mu og oh dn my oi oj dp nc lj ok ol ne ln om on ng lr oo op ni oq bi translated">不足之处</h2><ul class=""><li id="83d7" class="lx ly iu lc b ld nk lg nl lj ot ln ou lr ov lv ow md me mf bi translated">它不能很好地处理非常稀疏或者非结构化的数据。</li><li id="9ae9" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">它被认为是一个黑盒模型，比其他一些算法更难解释。</li><li id="f215" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">由于模型从其前辈的错误中学习的机制，XGBoost可能对异常值很敏感。</li></ul></div><div class="ab cl ml mm hy mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="in io ip iq ir"><h1 id="cf37" class="ms mt iu bd mu mv mw mx my mz na nb nc ka nd kb ne kd nf ke ng kg nh kh ni nj bi translated">5.k表示</h1><p id="30de" class="pw-post-body-paragraph la lb iu lc b ld nk jv lf lg nl jy li lj nm ll lm ln nn lp lq lr no lt lu lv in bi translated">K-means是最流行的聚类算法之一，这是一种无监督的机器学习形式，旨在找到训练数据集中的相似示例组。</p><p id="f3cb" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">该算法首先初始化随机聚类质心。然后，对于每个数据点，通常使用距离度量<a class="ae kz" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iv">欧几里德</strong> </a>距离或<a class="ae kz" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iv">余弦</strong> </a>相似度来将其分配给最近的质心。一旦分配了所有数据点，质心就移动到所分配数据点的平均值。重复这些步骤，直到质心分配停止变化。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oz"><img src="../Images/fabd49389d403bccff40bfa081d28bc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*Ff0DUyFhHdkRpWRQyJYVdg.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">k-意味着具有可视化质心的簇。作者图片</p></figure><h2 id="9591" class="of mt iu bd mu og oh dn my oi oj dp nc lj ok ol ne ln om on ng lr oo op ni oq bi translated">代码示例</h2><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="or os l"/></div></figure><h2 id="4041" class="of mt iu bd mu og oh dn my oi oj dp nc lj ok ol ne ln om on ng lr oo op ni oq bi translated">应该在什么时候使用？</h2><ul class=""><li id="b1f9" class="lx ly iu lc b ld nk lg nl lj ot ln ou lr ov lv ow md me mf bi translated">K-means只适合于非监督聚类。</li><li id="0636" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">对于这类问题，它通常被认为是一个很好的全能算法。</li></ul><h2 id="3f2e" class="of mt iu bd mu og oh dn my oi oj dp nc lj ok ol ne ln om on ng lr oo op ni oq bi translated">优势</h2><ul class=""><li id="2fd0" class="lx ly iu lc b ld nk lg nl lj ot ln ou lr ov lv ow md me mf bi translated">这是一个实现起来相对简单的算法。</li><li id="a023" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">它可以用于大型数据集。</li><li id="041c" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">由此产生的聚类很容易解释。</li></ul><h2 id="ed25" class="of mt iu bd mu og oh dn my oi oj dp nc lj ok ol ne ln om on ng lr oo op ni oq bi translated">不足之处</h2><ul class=""><li id="23fb" class="lx ly iu lc b ld nk lg nl lj ot ln ou lr ov lv ow md me mf bi translated">k均值对异常值很敏感。</li><li id="e882" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">该算法没有找到最佳的聚类数。这必须在实现之前通过其他技术来确定。</li><li id="4a38" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated">聚类的结果不一致。如果K-means在数据集上运行多次，每次都会产生不同的结果。</li></ul></div><div class="ab cl ml mm hy mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="in io ip iq ir"><p id="4afc" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">本文涵盖的算法是机器学习中最广泛使用的一些算法。还有许多算法可供使用，每一种算法都有自己适合特定问题的地方。正如本文开头提到的，目前没有一种算法可以解决所有问题，尽管XGBoost是解决结构化数据问题的最接近的算法。</p><p id="7c13" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这一介绍将提供一些关于数据科学家如何以及为什么选择一种算法而不是另一种算法的见解。下面是每种算法对特定类型问题的适用性的简要总结:</p><ul class=""><li id="b1e2" class="lx ly iu lc b ld le lg lh lj lz ln ma lr mb lv ow md me mf bi translated"><strong class="lc iv">线性回归:</strong>最适合解决基于回归的数据集问题，其中存在线性关系，且关系相对简单。</li><li id="bbfb" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated"><strong class="lc iv">逻辑回归:</strong>最适合解决数据线性可分、数据集维度低的分类问题。</li><li id="0a10" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated"><strong class="lc iv">随机森林:</strong>最适合具有复杂关系的大型高维数据集。</li><li id="9cee" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated"><strong class="lc iv"> XGBoost: </strong>适用于广泛的结构化数据集和问题。与随机森林相比，计算效率更高。</li><li id="8917" class="lx ly iu lc b ld mg lg mh lj mi ln mj lr mk lv ow md me mf bi translated"><strong class="lc iv"> K-means: </strong>最适合解决无监督聚类问题。</li></ul><p id="b64d" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">感谢阅读！</p><h1 id="1e13" class="ms mt iu bd mu mv pa mx my mz pb nb nc ka pc kb ne kd pd ke ng kg pe kh ni nj bi translated">承认</h1><p id="06c1" class="pw-post-body-paragraph la lb iu lc b ld nk jv lf lg nl jy li lj nm ll lm ln nn lp lq lr no lt lu lv in bi translated"><strong class="lc iv"><em class="lw">iris数据集用于生成本文中的决策树图:</em> </strong> <em class="lw">该数据集用于r . a . Fisher 1936年的经典论文，</em> <a class="ae kz" href="http://rcs.chemometrics.ru/Tutorials/classification/Fisher.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lw">分类问题中多重测量的使用</em> </a> <em class="lw">。它是在CCO 1.0通用版(CCO 1)许可下使用的。</em></p></div></div>    
</body>
</html>