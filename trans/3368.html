<html>
<head>
<title>The Word2vec Hyperparameters</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Word2vec超参数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-word2vec-hyperparameters-e7b3be0d0c74#2022-07-26">https://towardsdatascience.com/the-word2vec-hyperparameters-e7b3be0d0c74#2022-07-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="d55b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/word-embeddings-primer" rel="noopener" target="_blank">单词嵌入入门</a></h2><div class=""/><div class=""><h2 id="601a" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">一组创造性的重新加权</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/333a0d0545d660a93a6389e555f74e4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hCfEUZALyk5AXIArLUfyvQ.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@adigold1" rel="noopener ugc nofollow" target="_blank">阿迪·戈尔茨坦</a>在<a class="ae lh" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="adcd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">本文是5ᵗʰ系列文章<strong class="lk jd">中关于单词嵌入的入门:<br/> </strong> 1。<a class="ae lh" href="https://medium.com/@jongim/a-primer-on-word-embeddings-95e3326a833a" rel="noopener">word 2 vec后面有什么</a> | 2。<a class="ae lh" href="https://medium.com/@jongim/words-into-vectors-a7ba23acaf3d" rel="noopener">单词成向量</a> | <br/> 3。<a class="ae lh" href="https://medium.com/@jongim/statistical-learning-theory-26753bdee66e" rel="noopener">统计学习理论</a> | 4。<a class="ae lh" href="https://medium.com/@jongim/the-word2vec-classifier-5656b04143da" rel="noopener">word 2 vec分类器</a> | <br/> 5。<strong class="lk jd">word 2 vec超参数</strong> | 6。<a class="ae lh" href="https://medium.com/@jongim/characteristics-of-word-embeddings-59d8978b5c02" rel="noopener">单词嵌入的特征</a></p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="bc12" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在上一篇文章<a class="ae lh" href="https://medium.com/@jongim/the-word2vec-classifier-5656b04143da" rel="noopener">Word2vec分类器</a>中，我们学习了word 2 vec算法的概念和数学，重点是负采样的跳格模型(SGNS)。</p><p id="97fd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，并不仅仅是SGNS的算法使它有效。2015年，Levy等人将来自传统的基于计数的方法(如PPMI矩阵和SVD降维向量)的最新单词向量与当时可用的基于预测的方法(包括Word2vec SGNS和GloVe)进行了比较。他们发现，Word2vec的大部分性能提升来自其系统设计选择和“超参数”，而不仅仅来自神经网络启发的训练。</p><p id="2577" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在本文中，我们将回顾Word2vec的性能改进预处理建议和算法优化参数。</p><h1 id="a117" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">短语识别</h1><p id="b379" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">Mikolov等人强调了预处理原始文本的重要性，提出了一种识别语料库中出现频率足够高的短语的方法，为短语创建嵌入比单独为单词创建嵌入效果更好(Mikolov等人，2013b)。在NLP中识别短语通常与<em class="ni">命名实体识别</em>相关联，这允许像‘Bush’(植物)和‘Bush’(总统)这样的词可以单独嵌入。Mikolov等人定义的短语识别公式是:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/06b5e2cc58a8afa2b1b4eb1422768dc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*Mf39iGdkh2AbtptBcphvrg@2x.png"/></div></figure><p id="4cfe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<em class="ni">分数</em>是选定的值，高于该值的短语被识别。这里，<em class="ni"> δ </em>是<em class="ni">折现系数</em>。Mikolov等人解释说，他们更喜欢对数据运行得分方程2到4次，每次减少<em class="ni"> δ </em>的值。</p><h1 id="2343" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">词频阈值</h1><p id="b51e" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">Word2vec不是简单地根据单词在原始文本中的外观来训练单词。首先，根据训练前设定的阈值从语料库中去除生僻字。罕见的单词被删除，因为语言模型不太可能需要它们，并且正确训练它们的上下文更少。</p><p id="2975" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Mikolov等人定义了以下公式，其中可以根据参数𝜏(通常在10⁻⁵附近)设置阈值(Mikolov等人，2013b):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/45a19f8e2d3abb43271040c19945590b.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*MryzwAnr_UJ-rlbNAOcP0g@2x.png"/></div></figure><p id="0d68" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<em class="ni"> f </em> ( <em class="ni"> wᵢ </em>)是由单词<em class="ni"> wᵢ </em>表示的语料库的分数。但是，请注意，根据Levy等人(2015年)的说法，Mikolov等人的代码的C编程语言实现使用了以下稍加修改的等式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nl"><img src="../Images/93bc3446eb096aceb0f602712fde09a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*47Oy49Hwb7_oI_FrnX-G-Q@2x.png"/></div></div></figure><h1 id="cbe6" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">子采样率</h1><p id="ad24" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">此外，为了减少训练时间，根据Mikolov等人的代码(McCormick，2019)的C编程语言实现中的以下子采样函数，Word2vec训练更频繁的单词的频率低于它们在语料库中的出现频率:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/bc5393eeaec80bf43ef95a5ca88b6dfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*QSlfNTi3iznDD9miBR_uHQ@2x.png"/></div></figure><p id="f99b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里再次说明，<em class="ni"> f </em> ( <em class="ni"> wᵢ </em>)是由单词<em class="ni"> wᵢ </em>表示的语料库的分数，参数𝜅控制进行多少次子采样。当𝜅设置为默认值0.001时，最常见的英语单词“the”的80%的实例可能会被跳过，但当单词的频率<em class="ni"> f </em> ( <em class="ni"> wᵢ </em>)小于0.0026时，不会发生跳过。</p><h1 id="e543" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">上下文位置加权</h1><p id="ffee" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">当对目标单词周围的窗口<em class="ni"> l </em>中的上下文单词进行训练时，Word2vec不会像最初出现在文本中一样使用这些单词。首先，根据前两节中解释的公式，在训练开始之前删除不常用的单词，跳过常用单词的实例。移除这些单词后，上下文窗口实际上比保留这些单词时要宽。</p><p id="9a5d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，Word2vec对距离目标单词<em class="ni"> t </em>较近的上下文单词的线性加权比距离较远的上下文单词大。例如，如果窗口大小为<em class="ni"> l </em> =5，则位于最远位置的单词有/₅机会被包括在训练中，而恰好在目标单词之前或之后的单词总是被包括在内，如下图所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nn"><img src="../Images/96b8c2c53fbf8fdf45b51f6b382af9df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SVU9w0QPgp1g-uYFXogTMw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd no">上下文位置加权示例</strong>(麦考密克，2019，经许可转载)</p></figure><p id="a1cf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">根据Levy和Goldberg在2014年发表的一篇论文(Levy和Goldberg，2014年)，对于广泛的主题嵌入，窗口大小通常为5。</p><h1 id="2383" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">负样本的加权</h1><p id="b2da" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">在SGNS中，第<em class="ni"> k </em>个否定样本词不是根据它们在文本中的出现频率来选择的(unigram)；也不是同等(统一)选择的。相反，Word2vec使用加权单字频率<em class="ni"> P </em> 𝛼( <em class="ni"> w </em>，其中<em class="ni"> wᵢ </em>是单词，<em class="ni"> f </em> ( <em class="ni"> wᵢ </em>)是该单词在语料库中的总单词数，<em class="ni"> v </em>是词汇表中的单词数，𝛼 <em class="ni"> </em>是米科洛夫等人定义的权重(米科洛夫等人，2013b麦考密克，2019):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/96d996531bd4455d6ca47ce04179ffd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*0cGqYbDm4h89uLI_S7aQJA@2x.png"/></div></div></figure><p id="1b4d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当𝛼=1时，根据单词在语料库中的出现频率来选择单词。对于SGNS，Mikolov等人发现𝛼=0.75效果最好:</p><blockquote class="nq nr ns"><p id="6604" class="li lj ni lk b ll lm kd ln lo lp kg lq nt ls lt lu nu lw lx ly nv ma mb mc md im bi translated">“我们调查了许多关于<em class="it">p</em>【𝛼](<em class="it">w</em>】的选择，发现在我们尝试的每个任务中，一元分布<em class="it"> U </em> ( <em class="it"> w </em>)的3/4rd次方(即<em class="it">u</em>(<em class="it">w</em>)/⁴/<em class="it">z</em>)明显优于一元分布和均匀分布……<em class="it">(米科洛夫等人，2013年b) </em></p></blockquote><p id="53f8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当𝛼=0 <em class="ni">。</em> 75，训练生僻字的概率增加，训练频繁字的概率减少(McCormick，2019)，反映了本系列第二篇文章<a class="ae lh" href="https://medium.com/@jongim/words-into-vectors-a7ba23acaf3d" rel="noopener"> <strong class="lk jd">字成向量</strong> </a>中讨论的重新加权方法的一个客观。</p><p id="aad5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">根据McCormick (2019)的说法，“这种单词选择在[Mikolov等人的] C代码中实现的方式很有趣。麦考密克解释说，米科洛夫等人创建了一个包含1亿个元素的大数组，并根据等式<em class="ni"> P </em> 𝛼( <em class="ni"> w </em>用词汇表中每个单词的索引填充它。在训练过程中，他们通过选择0到1亿之间的随机数来随机选择表中的一个词索引值，从而选择负样本。</p><h1 id="735a" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">摘要</h1><p id="190e" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">在本文中，我们学习了一些建议用于创建单词向量的文本预处理技术，以及一些基于训练的重新加权“技巧”，这些技巧用于在使用Word2vec创建单词嵌入时优化结果并减少训练时间。</p><p id="ee38" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在下一篇文章<a class="ae lh" href="https://medium.com/@jongim/characteristics-of-word-embeddings-59d8978b5c02" rel="noopener"> <strong class="lk jd">单词嵌入的特点</strong> </a>中，我们将看看Word2vec单词嵌入的优缺点。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="d56f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这篇文章是5ᵗʰ系列文章<strong class="lk jd">中关于单词嵌入的初级读本:<br/> </strong> 1。<a class="ae lh" href="https://medium.com/@jongim/a-primer-on-word-embeddings-95e3326a833a" rel="noopener">word 2 vec背后有什么</a> | 2。<a class="ae lh" href="https://medium.com/@jongim/words-into-vectors-a7ba23acaf3d" rel="noopener">单词成向量</a> | <br/> 3。<a class="ae lh" href="https://medium.com/@jongim/statistical-learning-theory-26753bdee66e" rel="noopener">统计学习理论</a> | 4。<a class="ae lh" href="https://medium.com/@jongim/the-word2vec-classifier-5656b04143da" rel="noopener">word 2 vec分类器</a> | <br/> 5。Word2vec超参数| 6。<a class="ae lh" href="https://medium.com/@jongim/characteristics-of-word-embeddings-59d8978b5c02" rel="noopener">单词嵌入的特征</a></p><p id="11c8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">关于这个话题的更多内容:</strong>我推荐的一个了解Word2vec算法更多内容的资源是:McCormick，C. (2019)。<a class="ae lh" href="https://www.chrismccormick.ai/word2vec-ebook" rel="noopener ugc nofollow" target="_blank"><em class="ni">word 2 vec</em></a>的内部运作方式(电子书)。</p><h1 id="8c4d" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">参考</h1><p id="540e" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">Levy和y . Goldberg(2014年)。基于依存关系的单词嵌入。在<em class="ni">计算语言学协会第52届年会会议录(第2卷:短文)</em>(第302–308页)。可在doi<a class="ae lh" href="http://dx.doi.org/10.3115/v1/P14-2050" rel="noopener ugc nofollow" target="_blank">10.3115/v1/P14–2050</a>处获得。</p><p id="5711" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Levy、y . Goldberg和I . Dagan(2015年)。利用从单词嵌入中获得的经验改进分布相似性。计算语言学协会汇刊，3:211–225。可在doi<a class="ae lh" href="https://www.aclweb.org/anthology/W14-1618/" rel="noopener ugc nofollow" target="_blank">10.1162/tacl _ a _ 00134</a>处获得。</p><p id="e324" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">麦考密克，C. (2019)。<em class="ni">word 2 vec的内部工作原理</em> (Pro版，第一版)。电子书。<a class="ae lh" href="https://mccormickml.com" rel="noopener ugc nofollow" target="_blank"><em class="ni">McCormickml.com</em></a>。</p><p id="40b3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Mikolov、Corrado、G . Chen、k . Sutskever和j . Dean(2013年b)。词和短语的分布式表示及其组合性。可从<a class="ae lh" href="https://arxiv.org/abs/1310.4546" rel="noopener ugc nofollow" target="_blank"> arXiv:1310.4546v1 </a>获得。</p><p id="3f42" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">*除非另有说明，数字和图像均由作者提供。</p></div></div>    
</body>
</html>