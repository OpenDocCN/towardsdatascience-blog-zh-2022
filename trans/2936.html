<html>
<head>
<title>Unsupervised Learning: K-Means Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无监督学习:K-均值聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/unsupervised-learning-k-means-clustering-27416b95af27#2022-06-27">https://towardsdatascience.com/unsupervised-learning-k-means-clustering-27416b95af27#2022-06-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="558e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">最快最直观的无监督聚类算法。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/71a55dc9d30ebac92fc7f82b8c16c9ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fEyBjH2Kx6ztYKraGN-bGg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">群集图像-按作者</p></figure><p id="d33e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本文中，我们将介绍k-means聚类算法。我们将首先开始研究算法是如何工作的，然后我们将使用NumPy实现它，最后，我们将研究使用Scikit-learn实现它。</p><h1 id="fe52" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated"><strong class="ak">简介</strong></h1><p id="5013" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">K-means聚类是一种无监督算法，它将未标记的数据分组到不同的聚类中。标题中的K代表将要创建的集群数量。这是在模型训练之前应该知道的事情。例如，如果K=4，那么将创建4个集群，如果K=7，那么将创建7个集群。k-means算法用于欺诈检测、错误检测和确认现实世界中的现有聚类。</p><p id="3414" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该算法是基于质心的，这意味着每个数据点都被分配给质心最近的聚类。当我们使用欧几里德距离计算到质心的距离时，该算法可以用于任何数量的维度。下一节将详细介绍这一点。</p><p id="e8a3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">k-means算法的优点是易于实现，可以扩展到大型数据集，总是能够收敛，并且适合不同形状和大小的聚类。该模型的一些缺点是聚类的数目是手动选择的，聚类依赖于初始值，并且对异常值敏感。</p><h2 id="baa2" class="mo ls iq bd lt mp mq dn lx mr ms dp mb le mt mu md li mv mw mf lm mx my mh mz bi translated">k-均值步长</h2><p id="af69" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">训练K均值模型的步骤可以分为6个步骤。</p><p id="7c44" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">第一步:确定聚类数(K=？)</strong></p><p id="383d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果在模型训练之前就知道K是最好的，但是如果不知道，也有找到K的策略。最常见的是肘方法，它随着K的增加绘制平方距离的总和。通过观察该图，应该有一个点，在该点处，增加聚类的大小为误差函数提供了最小的增益。这就是所谓的弯头，应该选择它作为k的值。如下图所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/14cd50327e0b1152fd017cbb8e271e6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xUzCkZ-8wd3vxl-d3tClKQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">弯头方法图形-作者</p></figure><p id="8493" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">第二步:初始化集群质心</strong></p><p id="e113" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下一步是初始化K个质心作为每个簇的中心。最常见的初始化策略称为Forgy初始化。这是每个聚类的质心作为数据集中的随机数据点初始化的时候。这比随机初始化收敛得更快，因为聚类更可能出现在数据点附近。</p><p id="14c2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">像k-means++初始化这样更复杂的策略旨在选择彼此尽可能远的初始点。这种策略已被证明是k-means算法的最佳初始化方法。你可以在这里看到k-means++背后的数学。</p><p id="d84d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">步骤3:将数据点分配给聚类</strong></p><p id="3429" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在初始化K个聚类之后，每个数据点被分配给一个聚类。这是通过迭代数据中的所有点并计算到每个质心的欧几里德距离来完成的。欧几里德距离公式适用于n维欧几里德空间中的任意两点。这意味着具有任意维数的数据点被分配给最近的聚类。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/bdbb8f4e853163ebad5cd02d6977d712.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zph2bcIC21GLkdBsN7mxqA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">欧几里德距离公式—作者</p></figure><p id="d5d1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">步骤4:更新群集质心</strong></p><p id="4870" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一旦每个数据点被分配到一个聚类，我们就可以更新每个聚类的质心。这是通过取聚类中每个数据点的平均值并将结果指定为聚类的新中心来实现的。</p><p id="ffed" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">第五步:迭代更新</strong></p><p id="3fef" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后，使用新计算的质心，我们遍历所有的数据点，并将它们重新分配给聚类，并重新计算质心。重复进行此操作，直到质心值不再变化。</p><p id="7936" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">k-means算法的美妙之处在于它保证收敛。这是一个祝福也是一个诅咒，因为模型可能会收敛到局部最小值而不是全局最小值。这个想法将在下一节中说明，在下一节中，我们使用Numpy实现该算法，然后在Scikit-learn中实现。</p><h1 id="a2ff" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated"><strong class="ak"> K-Means: Numpy </strong></h1><p id="6342" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">首先，我们将导入必要的python包，并使用Scikit-learn的make_blob函数创建一个二维数据集。对于本文，我们将生成分布在4个集群中的300个数据点。生成的数据如下所示。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/95cb175dbf41dacee49ad66f1d72959c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S8x8R2KQt5s8G0TF3OYFRw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据示例—按作者</p></figure><p id="6986" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以看到样本中有4个不同的聚类，每个聚类中的外围数据点之间有轻微的重叠。接下来，我们将启动两个哈希映射。一个将跟踪聚类质心，另一个将跟踪每个聚类中的数据点。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="0cec" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">既然质心已经初始化，我们将定义另外三个函数。第一种方法接收两个数据点，并计算欧几里德距离。第二个函数更新每个聚类的质心，最后一个函数为每个聚类分配数据点。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="62ca" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后三个函数通过提供一种方法来检查模型是否收敛，拥有一个主训练函数，并可视化模型结果，从而将所有这些放在一起。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="ee5c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，将上面的所有函数放在一起，我们可以训练模型，并在两行代码中可视化我们的结果。通过使用函数式编程实现算法，我们能够使代码可读，限制重复，并促进代码的改进。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/04dedd4b720a9aff1629dee8471d7e33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5KqMIti0gpAVMm3G4qYKBA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">k-均值结果—按作者</p></figure><p id="3397" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在上面的模型训练中，我们看到模型能够在4个时期后收敛，并正确识别我们生成的聚类。这很好，但是生成的簇的结果高度依赖于初始的质心。如果我们用不同的初始质心训练模型多次，我们可以看到模型如何在不同的训练周期收敛到不同的最小值。这就是K-means++初始化有用的地方，因为它增加了收敛到全局最小值的可能性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/be5495523fba2ef6d6a37d3ea1c2c44f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7tkCz2MvyCYP1QETU77Pzg.png"/></div></div></figure><p id="e2cb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">太好了！现在我们能够从头开始实现k-means算法，但是这在现实世界中可行吗？不完全是。在现实世界中，您更可能使用现有的模型库，如Scikit——学习训练、测试和部署模型。</p><h1 id="ef8b" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">K-Means: Scikit-Learn</h1><p id="832b" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">使用现有库的好处是它们被优化以减少训练时间，它们通常带有许多参数，并且它们需要更少的代码来实现。Scikit-learn还包含许多其他的机器学习模型，并且使用一致的语法来访问不同的模型。</p><p id="afb5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在下面的单元格中，我们实现了与上面相同的k-means聚类算法，除了默认情况下我们使用k-means++初始化质心。所有这些都是在不到20行代码中完成的！</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/765f4ae1a3bb4dc6aec8fac477e6d0d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qgDMx1gUk7KSOVM0dSuCRg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">sci kit-学习结果—按作者</p></figure><p id="736f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如预期的那样，我们能够正确地识别4个集群。当Scikit-learn实现使用kmeans++初始化起始质心时，该算法几乎在每次重新运行训练周期时都收敛到全局最小值。</p><h2 id="f068" class="mo ls iq bd lt mp mq dn lx mr ms dp mb le mt mu md li mv mw mf lm mx my mh mz bi translated">最后的想法</h2><p id="23f2" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">K-means是一种易于实现的无监督聚类算法，几乎不需要任何时间就可以进行训练。由于该模型通过最小化数据点与其相应聚类之间的距离之和来训练，因此它可与其他机器学习模型相关。</p><p id="e06a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这篇文章的代码可以在这里找到。</p><p id="922c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">资源</strong></p><ol class=""><li id="9049" class="nf ng iq kx b ky kz lb lc le nh li ni lm nj lq nk nl nm nn bi translated"><a class="ae na" href="https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages" rel="noopener ugc nofollow" target="_blank">k means的利与弊——谷歌</a></li><li id="d17c" class="nf ng iq kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated"><a class="ae na" href="https://medium.com/analytics-vidhya/comparison-of-initialization-strategies-for-k-means-d5ddd8b0350e" rel="noopener">k意味着初始化方法— Nitish Kumar Thakur </a></li><li id="0739" class="nf ng iq kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated"><a class="ae na" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn Kmeans文档</a></li><li id="48d3" class="nf ng iq kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated"><a class="ae na" href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)" rel="noopener ugc nofollow" target="_blank">肘法—百科</a></li><li id="0efb" class="nf ng iq kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated"><a class="ae na" href="https://www.javatpoint.com/k-means-clustering-algorithm-in-machine-learning" rel="noopener ugc nofollow" target="_blank">k均值聚类—Java point</a></li><li id="3244" class="nf ng iq kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated"><a class="ae na" href="https://stackedit.io/app#" rel="noopener ugc nofollow" target="_blank"> StackEdit降价编辑</a></li></ol></div></div>    
</body>
</html>