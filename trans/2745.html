<html>
<head>
<title>Predictive Parameters in a Logistic Regression: Making Sense of it All</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归中的预测参数:理解一切</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predictive-parameters-in-a-logistic-regression-making-sense-of-it-all-476bde9825f3#2022-06-14">https://towardsdatascience.com/predictive-parameters-in-a-logistic-regression-making-sense-of-it-all-476bde9825f3#2022-06-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/0e668aa13f60f4f88710f951e5f56dec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UOJmvHismB0QG-2pjNOl2w.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">所有图片由作者提供</p></figure><div class=""/><div class=""><h2 id="fdf5" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">获得对超出典型优势比解释的logit模型参数的深入理解</h2></div><p id="c49f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi lq translated"><span class="l lr ls lt bm lu lv lw lx ly di"> L </span>逻辑回归，也称为logit模型，是线性回归的强大替代品，它允许人们对二分、二进制结果(即0或1)进行建模，并在给定观察值的情况下对所述结果发生的概率提供非常准确的预测。logit模型中的参数估计可以提供关于不同解释变量或特征如何影响模型预测的见解。许多读者可能都熟悉用<strong class="kw jg">优势比</strong>来解释logit模型参数(如果不熟悉，不用担心，我会在下面简要介绍)。然而，根据<strong class="kw jg">概率</strong>对这些参数的解释并不简单，但是对如何解释这些参数的充分理解可以提供<em class="lz">巨大的</em>直觉来解释模型的潜在预测行为。</p><blockquote class="ma"><p id="702a" class="mb mc jf bd md me mf mg mh mi mj lp dk translated">做出预测是非常强大的，但是直观地解释模型的预测组件可以将您的项目分析带到下一个级别。</p></blockquote><p id="a50c" class="pw-post-body-paragraph ku kv jf kw b kx mk kg kz la ml kj lc ld mm lf lg lh mn lj lk ll mo ln lo lp ij bi translated">在这篇文章的结尾，你会以一种新的视角来看待逻辑回归，并且理解如何用惊人的直觉来解释模型参数。本文假设对logit模型有一个简单的基础知识，从而以一种可理解的方式更专注于解释模型参数。然而，我们将首先简要讨论logit模型背后的理论。然后，我们将深入讨论如何将模型参数解释为边际效应。最后，我们将介绍一个利用以下<a class="ae mp" href="https://www.kaggle.com/datasets/dhanushnarayananr/credit-card-fraud?resource=download" rel="noopener ugc nofollow" target="_blank"> Kaggle数据集预测欺诈性信用卡交易的实例。</a></p><h1 id="c963" class="mq mr jf bd ms mt mu mv mw mx my mz na kl nb km nc ko nd kp ne kr nf ks ng nh bi translated">逻辑回归速成班</h1><p id="c738" class="pw-post-body-paragraph ku kv jf kw b kx ni kg kz la nj kj lc ld nk lf lg lh nl lj lk ll nm ln lo lp ij bi translated"><a class="ae mp" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank"> Logit模型</a>属于更广泛的<a class="ae mp" href="https://en.wikipedia.org/wiki/Generalized_linear_model" rel="noopener ugc nofollow" target="_blank">广义线性模型</a> (GLMs)家族，简而言之，当感兴趣的结果遵循不同于高斯的基本分布时，允许灵活拟合线性模型，并通过链接函数将线性模型与感兴趣的结果相关联。标准线性回归是一个特例，其中连接函数是单位函数。在二元结果的情况下，线性回归(称为线性概率模型)可以提供小于0或大于1的预测值(见图1)。这显然提出了问题，因为概率自然地限制在0和1之间。然而，GLM提供了一个方便的框架来解决这个问题！</p><p id="5ea7" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">logit模型是一个特例，它允许对遵循<a class="ae mp" href="https://en.wikipedia.org/wiki/Bernoulli_distribution" rel="noopener ugc nofollow" target="_blank">伯努利分布</a>的二元结果进行建模。logit模型特别有吸引力，因为使用的链接函数(logit函数)在0和1之间。因此，如概率空间中所预期的，所有模型预测都被限制在0和1之间。下面的图1提供了一个两变量情况下线性概率模型和逻辑回归模型拟合之间的直观比较。</p><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nn"><img src="../Images/b788bb7651c16c802baea540f0a63a43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9c9tXB9EwtosAevKiXfg0w.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><strong class="bd ns">图1 </strong></p></figure><p id="b259" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在数学上，logit模型的特征是:</p><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/19f209a73288ef41f08ffaac15f7bea5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l_IIGWSbPx4DWLRbe_VJlw.png"/></div></div></figure><p id="bbf1" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">其中<strong class="kw jg"> <em class="lz"> X </em> </strong>表示模型中观察到的解释变量或特征的矩阵，而<em class="lz"> p(X) </em>表示<em class="lz"> y </em>取值为1的概率。给定具有<em class="lz"> y </em>分布伯努利的模型设置，logit模型估计的目标是最大化以下似然函数，这是我们的联合分布:</p><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nu"><img src="../Images/3f16fd42afc2dfee4e672be3a2a1e01a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GikTEohUeVvt719HiswFOw.png"/></div></div></figure><p id="7515" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">简而言之，我们的优化问题寻求选择将最大化(2)的(1)中的参数(即<strong class="kw jg"> <em class="lz"> β </em> </strong>)。注意，当估计的概率对于具有<em class="lz"> y </em> = 1的个体接近1并且对于具有<em class="lz"> y </em> = 0的个体接近0时，将(2)最大化。为此，可以采用似然函数的对数来获得对数似然，并使用梯度下降或相关算法来解决这个问题。关于逻辑回归的<a class="ae mp" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank"> wiki页面</a>提供了对logit模型估计的深入探讨。</p><h1 id="ff9c" class="mq mr jf bd ms mt mu mv mw mx my mz na kl nb km nc ko nd kp ne kr nf ks ng nh bi translated">将Logit参数解释为边际效应</h1><p id="91a1" class="pw-post-body-paragraph ku kv jf kw b kx ni kg kz la nj kj lc ld nk lf lg lh nl lj lk ll nm ln lo lp ij bi translated">边际效应可以被认为是由感兴趣的解释变量(或特征)的变化对结果(或目标)变量产生的平均(或<em class="lz">边际</em>)效应。这可以类似地概念化如下:在平均(或<strong class="kw jg"> <em class="lz">【边际】</em> </strong>观察/个体，改变一个解释变量对结果的<strong class="kw jg">效果</strong>是什么。在我们的二元变量的例子中，这类似于估计改变一个解释变量对观察结果的概率的平均影响。</p><blockquote class="nv nw nx"><p id="6d15" class="ku kv lz kw b kx ky kg kz la lb kj lc ny le lf lg nz li lj lk oa lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="jf">注意</em> : </strong>边际效应必须仅解释为<em class="jf">关联</em>和<strong class="kw jg">而非</strong>因果关系。因果关系需要额外的识别假设，我在最近的一篇文章中提到了这一点。</p></blockquote><p id="4f0c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">通过认识到边际效应仅仅是一个变化率，人们可以立即注意到这归结为对解释变量求导。为了简单起见，我们首先从简单的线性回归开始。假设我们有以下线性回归:</p><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ob"><img src="../Images/0514d04b6388fd8d3a3e777034b1fbe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KURM9_UvwKOLSCX-iNh4kw.png"/></div></div></figure><p id="e334" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">为了找到任何变量的边际效应，我们可以对(3)中感兴趣的<em class="lz"> x </em>求导。对任何<em class="lz"> x* </em>的导数简单来说就是:</p><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oc"><img src="../Images/6298cd3492a51eab9613ba9dc5869360.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Rg1eK695wo8WG1SlnD4EA.png"/></div></div></figure><p id="4a56" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">注意，在这种情况下，我们有一个恒定的边际效应，这是有意义的，因为线性回归是y到x的线性投影。</p><blockquote class="nv nw nx"><p id="e155" class="ku kv lz kw b kx ky kg kz la lb kj lc ny le lf lg nz li lj lk oa lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="jf">释义:</em> </strong>平均起来，x* <strong class="kw jg"> </strong>增加一个单位就是<strong class="kw jg"> <em class="jf">关联</em> </strong> <em class="jf"> </em>与<strong class="kw jg"> </strong> y增加一个β*变化。</p></blockquote><p id="1d6e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">现在，细心的读者可能会注意到，这个导数对于logit模型来说并不是微不足道的(见下文对对数优势和优势比的讨论)。考虑等式2中描述的逻辑模型。(1).关于<strong class="kw jg">任意<em class="lz"> x* </em> </strong> <em class="lz"> </em>的导数可以利用链和商法则求解。因此，我们可以发现<em class="lz"> x* </em>对<em class="lz"> y </em>发生概率的边际效应如下:</p><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi od"><img src="../Images/a05c96fbea2528a050b2a8f77e5cc54a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TvW-uukzPAK9dn4NKiw3vA.png"/></div></div></figure><p id="1842" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这里我们可以看到边际效应现在是<em class="lz"> x的</em>本身的值的函数。这也是有意义的，因为logit函数是非线性的(见图1)。这给了我们评估任何<em class="lz">x</em>组合的边际效应的能力。然而，如果我们想总结总体边际效应，我们有两个选择:</p><ol class=""><li id="dc5e" class="oe of jf kw b kx ky la lb ld og lh oh ll oi lp oj ok ol om bi translated"><strong class="kw jg">计算<em class="lz">平均值</em>边际效应</strong> —这需要对每个观察值使用(5)计算边际效应，然后计算平均值</li><li id="691b" class="oe of jf kw b kx on la oo ld op lh oq ll or lp oj ok ol om bi translated"><strong class="kw jg">计算平均边际效应<em class="lz"/></strong><em class="lz"/>——这需要将所有解释变量的平均值代入(5)并计算边际效应</li></ol><p id="e6a2" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">一个比另一个没有立竿见影的好处，两者在不同的背景下提供不同的解释。然而，平均边际效应提供了最清晰的解释，因此将是我们在这篇文章的剩余部分所要处理的。</p><blockquote class="nv nw nx"><p id="e6fc" class="ku kv lz kw b kx ky kg kz la lb kj lc ny le lf lg nz li lj lk oa lm ln lo lp ij bi translated">请注意，所有的计算都可以很容易地扩展到计算边际效应，不仅在解释变量的平均值，但在任何组合的价值。我将把这个留给感兴趣的读者，下一节中提供的代码可以很容易地扩充到这样做(即，将您感兴趣的每个变量的值插入到(5)中，以获得该观察的边际效应)。这可以提供<strong class="kw jg">非常强大的</strong>洞察力，了解预测参数边际效应如何因特定类型的个人/观察而变化！</p></blockquote><p id="2f1c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">然而，对logit模型中平均边际效应的解释如下:</p><p id="7d86" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg">平均边际效应:</strong></p><blockquote class="nv nw nx"><p id="ee51" class="ku kv lz kw b kx ky kg kz la lb kj lc ny le lf lg nz li lj lk oa lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="jf">释义:</em> </strong>平均而言，x*每增加一个单位,<strong class="kw jg"> <em class="jf">关联</em> </strong> <em class="jf"> </em>与y发生概率的{计算值}个百分点变化。</p></blockquote><h2 id="9273" class="os mr jf bd ms ot ou dn mw ov ow dp na ld ox oy nc lh oz pa ne ll pb pc ng pd bi translated">对数赔率、赔率和赔率比</h2><p id="a41c" class="pw-post-body-paragraph ku kv jf kw b kx ni kg kz la nj kj lc ld nk lf lg lh nl lj lk ll nm ln lo lp ij bi translated">在我们提供一个实际应用的数字示例之前，讨论logit模型、对数优势、优势和优势比之间的关系是很重要的。逻辑回归结果用概率来解释是很常见的，这是因为～经过一些代数～我们可以将(1)改写为:</p><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi od"><img src="../Images/a733079693e65ebb865580fbaea88ab9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kxqwiV56Q38Sjoh1SZQarg.png"/></div></div></figure><p id="c2fb" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">左手边是对数优势。因此，逻辑回归在对数优势方面具有恒定的边际效应，其中:</p><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pe"><img src="../Images/cff2229bf9c39df4f663e1014148bd3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cAqZoiHOWJaVLk1YfKnrNg.png"/></div></div></figure><p id="1f33" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">然而，就对数优势而言，边际效应与任何直觉都相去甚远。因此，我们可以通过取(6)的指数来根据几率求解模型:</p><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pf"><img src="../Images/ba9c1db3769fcfadd3e6d2b7af71eced.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QZeQcD7ywmVuTkwEHhSsPw.png"/></div></div></figure><p id="d10f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">然后，通常逻辑回归参数通过计算优势比以优势来解释，其中，使用(8)，我们获得:</p><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pg"><img src="../Images/dea54e8c8d15533d886bd285dc2e3652.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JhAGxjE5d1vYICH12veCXg.png"/></div></div></figure><p id="7675" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">请注意，在二元变量的情况下，<em class="lz"> x* </em>的分母值为0，因此我们比较等于1和等于0的指标比率(即男性与女性的比率)。在二元或连续情况下，解释如下:</p><blockquote class="nv nw nx"><p id="f283" class="ku kv lz kw b kx ky kg kz la lb kj lc ny le lf lg nz li lj lk oa lm ln lo lp ij bi translated"><strong class="kw jg">解释:</strong>平均而言，x*每增加一个单位,<strong class="kw jg">与将y发生的几率乘以{计算值}相关联</strong>。</p></blockquote><p id="6ed5" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在我看来，对这些的解释并不总是像概率解释那样清晰，除非一个人经常接触和使用对数优势、优势和优势比。然而，(7-9)可以提供对<em class="lz"> x* </em>分别对对数优势、优势和优势比的边际效应的洞察。</p><h2 id="a884" class="os mr jf bd ms ot ou dn mw ov ow dp na ld ox oy nc lh oz pa ne ll pb pc ng pd bi translated">可选:非线性和相互作用</h2><p id="e986" class="pw-post-body-paragraph ku kv jf kw b kx ni kg kz la nj kj lc ld nk lf lg lh nl lj lk ll nm ln lo lp ij bi translated">假设我们有以下两个信念:<em class="lz"> x* </em>很可能与<em class="lz"> y </em>有二次关系，我们相信效果会因<em class="lz">性别</em>而不同。我们可以扩充我们的logit模型，以包括两个额外的工程特性，如下所示:</p><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pg"><img src="../Images/5afcb1134c272a5dfd993a18a36152c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mqVpZC8VIiQ5q9pHIaMkBw.png"/></div></div></figure><p id="5654" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">其中，我们包含了<em class="lz"> x* </em>的平方项，并使用虚拟变量<em class="lz"> x* </em>对该个体是否为男性进行交互。因此，我们对边际效应的解释现在会稍微少一些细微差别。</p><blockquote class="nv nw nx"><p id="b895" class="ku kv lz kw b kx ky kg kz la lb kj lc ny le lf lg nz li lj lk oa lm ln lo lp ij bi translated">注意，无论何时我们包含一个交互项，我们都必须注意首先在模型中包含每个未交互的变量<em class="jf">和</em>(也就是说，也单独包含虚拟的<em class="jf">和</em>)。否则，相互作用项将吃掉性别对y的原始影响，而实际上相互作用项可能是多余的。</p></blockquote><p id="258f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">现在相对于<em class="lz"> x* </em>对(10)进行微分，我们得到:</p><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ph"><img src="../Images/369ed095837573102a98714305132bc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YSaqO33b_6sJwVOdnR-V-A.png"/></div></div></figure><p id="fed7" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们现在可以看到，<em class="lz">，</em>由于非线性，边际效应将根据<em class="lz"> x* </em>的值以及该个体是男性还是女性而进一步变化。这可以让我们通过计算每个男性和女性的平均边际效应来计算男性和女性的平均边际效应。我们可以在根据概率求解(10)之后，类似地计算(9)中的概率比。这些例子将留给感兴趣的读者，我们到目前为止所涵盖的内容应该足以计算这些。</p><h1 id="8244" class="mq mr jf bd ms mt mu mv mw mx my mz na kl nb km nc ko nd kp ne kr nf ks ng nh bi translated">预测信用卡欺诈的边际效应</h1><p id="759f" class="pw-post-body-paragraph ku kv jf kw b kx ni kg kz la nj kj lc ld nk lf lg lh nl lj lk ll nm ln lo lp ij bi translated">为了演示我们上面讨论的具体示例，我们将利用以下关于信用卡交易的<a class="ae mp" href="https://www.kaggle.com/datasets/dhanushnarayananr/credit-card-fraud?resource=download" rel="noopener ugc nofollow" target="_blank"> Kaggle数据集</a>，目的是建立一个模型来预测交易是否是欺诈性的。数据集在每个事务上都有以下变量:</p><ul class=""><li id="b00b" class="oe of jf kw b kx ky la lb ld og lh oh ll oi lp pi ok ol om bi translated"><strong class="kw jg"> <em class="lz">距离_从_家</em> </strong>(连续)</li><li id="a5c9" class="oe of jf kw b kx on la oo ld op lh oq ll or lp pi ok ol om bi translated"><strong class="kw jg"> <em class="lz">距离_从_最后_事务</em> </strong>(连续)</li><li id="fc45" class="oe of jf kw b kx on la oo ld op lh oq ll or lp pi ok ol om bi translated"><strong class="kw jg"> <em class="lz">比率_对_中值_购买_价格</em> </strong>(连续)</li><li id="d364" class="oe of jf kw b kx on la oo ld op lh oq ll or lp pi ok ol om bi translated"><strong class="kw jg"> <em class="lz">【重复_零售商(1/0) </em> </strong>(二进制)</li><li id="dd8b" class="oe of jf kw b kx on la oo ld op lh oq ll or lp pi ok ol om bi translated"><strong class="kw jg"> <em class="lz">【已用_芯片(1/0)】</em></strong>(二进制)</li><li id="2b81" class="oe of jf kw b kx on la oo ld op lh oq ll or lp pi ok ol om bi translated"><strong class="kw jg"><em class="lz">【used _ pin _ number】</em></strong><strong class="kw jg"><em class="lz">(1/0)</em></strong>(二进制)</li><li id="1dc4" class="oe of jf kw b kx on la oo ld op lh oq ll or lp pi ok ol om bi translated"><strong class="kw jg"> <em class="lz">【在线_订单】</em> </strong> <strong class="kw jg"> <em class="lz"> (1/0) </em> </strong>(二进制)</li><li id="8a4a" class="oe of jf kw b kx on la oo ld op lh oq ll or lp pi ok ol om bi translated"><strong class="kw jg">目标:</strong> <strong class="kw jg"> <em class="lz">欺诈</em></strong><strong class="kw jg"><em class="lz">(1/0)</em></strong>(二进制)</li></ul><p id="3f0e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">首先，让我们导入数据并生成汇总统计数据:</p><pre class="no np nq nr gt pj pk pl bn pm pn bi"><span id="0067" class="po mr jf pk b be pp pq l pr ps">import pandas as pd<br/><br/># Import Data<br/>fraud = pd.read_csv("card_transdata.csv")<br/><br/># Summary Statistics<br/>fraud.describe().round(3)</span></pre><p id="c7fb" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">从中我们可以获得以下汇总统计数据:</p><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pt"><img src="../Images/3611c466afabe3673793626155c23df9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O1q1nGJJogeRXxQMIuRYgw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><strong class="bd ns">图2 </strong></p></figure><p id="1f7d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们现在将使用scikit-learn构建一个逻辑回归模型。假设我们已经完成了训练和验证模型的适当步骤，并且已经确定了适当的模型。我们的最终模型如下:</p><pre class="no np nq nr gt pj pk pl bn pm pn bi"><span id="8db5" class="po mr jf pk b be pp pq l pr ps">from sklearn.compose import ColumnTransformer <br/>from sklearn.pipeline import Pipeline<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.linear_model import LogisticRegression<br/><br/># Build Model Pipeline<br/>features = list(fraud.iloc[:,0:7].columns)<br/><br/>cont_feat = features[:3] # Continuous Features<br/>bin_feat = features[3:] # Binary Features<br/><br/>standardize = ColumnTransformer([<br/>    ('cont', StandardScaler(), cont_feat), # Standardize continuous features<br/>    ('binary','passthrough',bin_feat)<br/>    ])<br/><br/>pipeline = Pipeline([<br/>    ('standardize',standardize),<br/>    ('logit',LogisticRegression())<br/>    ])<br/><br/># Fit Pipeline<br/>model = pipeline.fit(fraud[features],fraud['fraud'])<br/><br/># Pull Final Model<br/>final_mod = model._final_estimator</span></pre><p id="e10d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们建立了logit模型来预测信用卡交易是否是欺诈性的。现在，让我们转到解释模型参数，以了解模型的内部工作方式以及每个功能在推动预测方面的后续作用。我们将定义一个函数来计算逻辑回归在概率和赔率方面的边际效应:</p><pre class="no np nq nr gt pj pk pl bn pm pn bi"><span id="1c87" class="po mr jf pk b be pp pq l pr ps">import numpy as np<br/>import pandas as pd<br/><br/>def logit_margeff(model, X, features, kind='probability'):<br/>    <br/>    coef = model.coef_<br/>    intercept = model.intercept_<br/>    <br/>    if kind == 'probability':<br/>        <br/>        logodds = intercept+np.dot(X,coef.T)<br/>    <br/>        marg_effects=[]<br/>        for i in range(coef.size):<br/>            marg_eff = np.mean(coef[0,i]*np.exp(-logodds)/(1+np.exp(-logodds))**2).round(3)<br/>            marg_effects.append(marg_eff)<br/>    <br/>    elif kind == "odds":<br/>        <br/>        marg_effects=[]<br/>        for i in range(coef.size):<br/>            marg_eff = (np.exp(coef[0,i])).round(3)<br/>            marg_effects.append(marg_eff)<br/>    <br/>    marginal_effects = {}<br/>    marginal_effects['features'] = features<br/>    marginal_effects[f'marginal_effects_{kind}'] = marg_effects<br/><br/>    df = pd.DataFrame(marginal_effects)<br/>    <br/>    return df</span></pre><blockquote class="nv nw nx"><p id="ca8e" class="ku kv lz kw b kx ky kg kz la lb kj lc ny le lf lg nz li lj lk oa lm ln lo lp ij bi translated">注意，第14行是使用(5)计算的平均边际效应，第21行是使用(9)计算的优势比。</p></blockquote><p id="7e74" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在我们定义了这个函数之后，我们要做的就是输入我们建立的logit模型和特征矩阵。让我们首先用概率来解释输出:</p><pre class="no np nq nr gt pj pk pu pv aw pw bi"><span id="cb78" class="os mr jf pk b gy px py l pz ps">logit_margeff(final_mod,fraud[features],features,kind='probability')</span></pre><p id="4f10" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">其中我们有每个特征的边际效应的相应输出:</p><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi qa"><img src="../Images/f41283fb26df43715f15821bd341c5d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pAUfCNYr5xSP2DZqUaOKTQ.png"/></div></div></figure><p id="d0d0" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">回想一下，我们已经标准化了所有连续特征，因此一个单位的增加对应于一个标准差的增加。我们将解释一个连续特征、<em class="lz">离家距离、</em>和一个二元特征、<em class="lz">已用个人识别码的估计平均边际效应。</em></p><blockquote class="nv nw nx"><p id="e1f6" class="ku kv lz kw b kx ky kg kz la lb kj lc ny le lf lg nz li lj lk oa lm ln lo lp ij bi translated"><strong class="kw jg">解释(distance_from_home): </strong>平均而言，交易发生地距离持卡人家庭住址的距离每增加一个标准差(65.391)，交易欺诈的概率就会增加2.4个百分点<strong class="kw jg"/>。</p><p id="0cfd" class="ku kv lz kw b kx ky kg kz la lb kj lc ny le lf lg nz li lj lk oa lm ln lo lp ij bi translated"><strong class="kw jg">解释(used_pin_number): </strong>平均而言，包括使用pin码的信用卡交易与交易欺诈的概率降低32.3个百分点<strong class="kw jg"/>。</p></blockquote><p id="ce4a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">现在，就几率而言:</p><pre class="no np nq nr gt pj pk pu pv aw pw bi"><span id="ecb1" class="os mr jf pk b gy px py l pz ps">logit_margeff(final_mod,fraud[features],features,kind='odds')</span></pre><p id="6b66" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">每个特征具有相应的边际效应输出:</p><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi qb"><img src="../Images/f9c9e5c4ba4bc840784d67996da0c2ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I37aV0dRMVkyAYrw_8n05w.png"/></div></div></figure><p id="a5e9" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">以赔率表示的相同特征的解释如下:</p><blockquote class="nv nw nx"><p id="6648" class="ku kv lz kw b kx ky kg kz la lb kj lc ny le lf lg nz li lj lk oa lm ln lo lp ij bi translated"><strong class="kw jg">解释(distance_from_home): </strong>平均而言，交易发生地与持卡人家庭地址之间的距离每增加一个标准差(65.391)，交易被欺诈的几率就会增加2.7倍。</p><p id="5e5c" class="ku kv lz kw b kx ky kg kz la lb kj lc ny le lf lg nz li lj lk oa lm ln lo lp ij bi translated"><strong class="kw jg">解释(used_pin_number): </strong>平均而言，包括使用pin号的信用卡交易与将交易欺诈的几率乘以0相关联。换句话说，平均来说，使用pin码几乎可以完美地预测交易可能是<strong class="kw jg">而不是</strong>欺诈。</p></blockquote><h1 id="e96a" class="mq mr jf bd ms mt mu mv mw mx my mz na kl nb km nc ko nd kp ne kr nf ks ng nh bi translated">讨论</h1><p id="2451" class="pw-post-body-paragraph ku kv jf kw b kx ni kg kz la nj kj lc ld nk lf lg lh nl lj lk ll nm ln lo lp ij bi translated">我希望这篇文章有助于明确如何从logit模型参数中提取出<strong class="kw jg"> <em class="lz">非常有意义的见解</em> </strong>。很明显，根据概率的边际效应解释提供了大量的直觉和在logit模型框架下预测机制的可解释性。一般来说，这些参数解释了模型如何进行预测，以及解释了目标和特征之间的关联。然而，在额外的识别假设下，我们可以做出更有力的声明，将模型参数解释为某些特征和目标之间的因果关系(我在之前的<a class="ae mp" href="https://medium.com/r?url=https%3A%2F%2Ftowardsdatascience.com%2Fcontrolling-for-x-9cb51652f7ad" rel="noopener">帖子</a>中简要讨论了这一点)。我希望这篇文章增加了你对逻辑回归的了解和欣赏！</p><h2 id="ed2a" class="os mr jf bd ms ot ou dn mw ov ow dp na ld ox oy nc lh oz pa ne ll pb pc ng pd bi translated">资料组</h2><p id="276c" class="pw-post-body-paragraph ku kv jf kw b kx ni kg kz la nj kj lc ld nk lf lg lh nl lj lk ll nm ln lo lp ij bi translated">可在Kaggle上获得:<a class="ae mp" href="https://www.kaggle.com/datasets/dhanushnarayananr/credit-card-fraud?resource=download" rel="noopener ugc nofollow" target="_blank">信用卡欺诈</a> ( <strong class="kw jg">许可证:</strong> CC0:公共领域)</p><h2 id="d1e5" class="os mr jf bd ms ot ou dn mw ov ow dp na ld ox oy nc lh oz pa ne ll pb pc ng pd bi translated">相关职位</h2><ul class=""><li id="ca62" class="oe of jf kw b kx ni la nj ld qc lh qd ll qe lp pi ok ol om bi translated"><a class="ae mp" href="https://medium.com/r?url=https%3A%2F%2Ftowardsdatascience.com%2Fcontrolling-for-x-9cb51652f7ad" rel="noopener">控制“X”</a></li></ul></div><div class="ab cl qf qg hu qh" role="separator"><span class="qi bw bk qj qk ql"/><span class="qi bw bk qj qk ql"/><span class="qi bw bk qj qk"/></div><div class="ij ik il im in"><p id="41b7" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">感谢你阅读我的帖子！我在Medium上的帖子试图利用 <strong class="kw jg"> <em class="lz">计量经济学</em> </strong> <em class="lz">和</em> <strong class="kw jg"> <em class="lz">统计/机器学习</em> </strong> <em class="lz">技术来探索现实世界和理论应用。此外，我试图通过理论和模拟提供某些方法论的理论基础。最重要的是，我写作是为了学习！我希望把复杂的话题变得更容易理解。如果你喜欢这个帖子，请考虑</em> <a class="ae mp" href="https://medium.com/@jakepenzak" rel="noopener"> <strong class="kw jg"> <em class="lz">跟我上媒</em> </strong> </a> <em class="lz">！</em></p></div></div>    
</body>
</html>