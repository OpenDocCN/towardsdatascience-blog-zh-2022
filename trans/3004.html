<html>
<head>
<title>Flux.jl on MNIST — A performance analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MNIST上的Flux.jl性能分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/flux-jl-on-mnist-a-performance-analysis-c660c2ffd330#2022-06-30">https://towardsdatascience.com/flux-jl-on-mnist-a-performance-analysis-c660c2ffd330#2022-06-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/db26956529fd614aa3f30843a3a3adf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EKAZEdTXZzjWVx3IiuRizA.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">布拉登·科拉姆在<a class="ae jd" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><div class=""/><div class=""><h2 id="60cd" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">在上一篇文章中定义了各种模型和训练算法后，我们现在来看看它们在应用于MNIST数据集时的表现。</h2></div><h1 id="b426" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">介绍</h1><p id="1039" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在之前的一篇文章(<a class="ae jd" rel="noopener" target="_blank" href="/flux-jl-on-mnist-variations-of-a-theme-c3cd7a949f8c"><em class="mj">flux . JL on MNIST——主题变奏曲</em> </a>)中，我们定义了三种不同的神经网络(NN)，目的是使用MNIST数据集识别手写数字。此外，我们实现了梯度下降算法(GD)的三个变体来训练这些模型，它们可以与不同的成本函数和不同的优化器结合使用。</p><p id="14d2" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">因此，我们有相当多的积木，它们可以以各种方式组合，以实现我们识别手写数字的目标。具体来说，我们有以下组件(要了解更多信息，请查看上面引用的前一篇文章):</p><ul class=""><li id="01a9" class="mp mq jg lp b lq mk lt ml lw mr ma ms me mt mi mu mv mw mx bi translated">型号:<em class="mj"> 4LS，3LS，2LR </em></li><li id="1e4d" class="mp mq jg lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">GD变体:<em class="mj">批量GD、随机GD </em>和<em class="mj">小批量GD </em></li><li id="ab18" class="mp mq jg lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">成本函数:均方误差(<em class="mj"> mse </em>)和交叉熵(<em class="mj"> ce </em>)</li><li id="0a00" class="mp mq jg lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">优化者:<em class="mj">下降</em>和<em class="mj">亚当</em></li></ul><p id="af7f" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">在本文中，我们将分析这些组件的不同配置对于图像识别的表现如何，以及需要多少努力(以GD的迭代次数和用于该目的的时间表示)来训练它们。</p><h1 id="816b" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">学习曲线</h1><p id="4c5c" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">我们不仅对模型在用GD训练后的性能感兴趣，而且我们还想通过在每次迭代后查看成本函数(或<em class="mj">损失函数</em>的同义词)的值来了解GD在执行过程中的表现。</p><p id="8e8f" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">绘制这些值与迭代次数的关系，我们得到所谓的<em class="mj">学习曲线</em>。这条曲线很好地描述了GD收敛的速度，以及在训练过程中是否存在问题(例如，如果GD陷入平稳状态)。</p><p id="d1d7" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">为了在培训期间收集这些值，我们必须在GD实现中添加几行代码:</p><figure class="nd ne nf ng gt is"><div class="bz fp l di"><div class="nh ni l"/></div></figure><ul class=""><li id="3e28" class="mp mq jg lp b lq mk lt ml lw mr ma ms me mt mi mu mv mw mx bi translated">由于计算损失在计算上可能是昂贵的，我们不希望总是在每次<em class="mj">迭代时都这样做，而是只在每次<em class="mj">到第n次</em>迭代时才这样做。这由新的(可选的)关键字参数<code class="fe nj nk nl nm b">logstep</code>(第1行)控制。1的<code class="fe nj nk nl nm b">logstep</code>意味着我们在每次</em>迭代后<em class="mj">计算并存储损失，例如100的值意味着我们仅在每次第100次迭代后才这样做。<br/>仅举例说明差异有多大:使用随机GD对4LS模型进行4000次迭代，不记录损失需要3.8秒，每次迭代计算损失需要262.9秒。</em></li><li id="ee56" class="mp mq jg lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">在第2行中，用于存储损失历史的数组<code class="fe nj nk nl nm b">hist</code>被初始化为零。</li><li id="4ae4" class="mp mq jg lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">在第9行中，我们检查是否已经到达应该记录损失的迭代，并将相应的损失值存储在<code class="fe nj nk nl nm b">hist</code>数组中(第10行)。</li><li id="34e2" class="mp mq jg lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">在函数的末尾(第13行)，我们返回完整的损失历史。</li></ul><p id="7878" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated"><em class="mj">注意</em> : Flux提供了一种回调机制，可以根据时间间隔来实现这种日志记录。即，使用该机制，损失值不会在每<em class="mj"> n </em>次迭代之后被记录，而是在每<em class="mj"> n </em>秒之后被记录。</p><h1 id="96ae" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">准确(性)</h1><p id="9788" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">训练完模型的参数后，我们想知道这组参数在用于识别手写数字时的表现如何。因此，我们将测试数据集上的模型预测与测试数据集的实际标签(<code class="fe nj nk nl nm b">test_y_raw</code>)进行比较，并计算正确识别数字的比例。这个分数叫做模型的<em class="mj">精度</em>。</p><p id="f75a" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">例如，我们通过调用<code class="fe nj nk nl nm b">model4LS(test_x)</code>获得4LS模型的预测。结果是一个由10，000个热点向量组成的数组。为了将它们与<code class="fe nj nk nl nm b">test_y_raw</code>中的标签进行比较，我们必须将独热向量转换成数字。这可以使用函数<code class="fe nj nk nl nm b">Flux.onecold()</code>来完成，它简单地返回一个热向量中1位的索引。也就是说，如果第一位是1，函数返回1，如果第二位是1，我们得到2，依此类推。由于我们的模型产生一个第一位设置为1的独热向量，如果“数字0”被识别，我们必须将<code class="fe nj nk nl nm b">onecold()</code>的结果减1以获得正确的标签。</p><p id="5da4" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">因此，精度计算如下:</p><pre class="nd ne nf ng gt nn nm no np aw nq bi"><span id="16d0" class="nr kw jg nm b gy ns nt l nu nv">function accuracy(y_hat, y_raw)<br/>    y_hat_raw = Flux.onecold(y_hat) .- 1<br/>    count(y_hat_raw .== y_raw) / length(y_raw)<br/>end</span></pre><p id="3938" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">例如，调用<code class="fe nj nk nl nm b">accuracy(model4LS(test_x), test_y_raw)</code>将获得测试数据集上4LS模型的准确性。</p><p id="e48f" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated"><code class="fe nj nk nl nm b">accuracy</code>功能使用btw。<em class="mj">用<code class="fe nj nk nl nm b">.</code>-算子广播朱莉娅的</em>机制。第一个公式中的运算<code class="fe nj nk nl nm b">.-</code>从表达式左侧数组的每个元素的<em class="mj">中减去1。第二个公式中的<code class="fe nj nk nl nm b">.==</code>将<code class="fe nj nk nl nm b">y_hat_raw</code>的<em class="mj">每个</em>元素与<code class="fe nj nk nl nm b">y_raw</code>中的<em class="mj">每个</em>对应元素进行比较(产生一个布尔数组)。</em></p><h1 id="42e7" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">培训和分析</h1><p id="c855" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">以下所有的训练都是在一台8 GB内存的苹果M1上使用Julia 1.7.1和Flux 0.13.3完成的。</p><p id="6d05" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">列出的所有运行时间都是使用等于批量的<code class="fe nj nk nl nm b">logstep</code>完成的。即损失函数值仅在运行结束时取一次，因此对运行时间没有显著影响。</p><p id="b0c9" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">给那些想要自己重复分析的人一个提示:如果你想要用相同的配置分析不同数量的迭代，例如，比较1000次迭代和2000次迭代的结果，你不必运行1000次迭代和2000次迭代。相反，这可以以相加的方式完成(即，首先运行1000次迭代，然后再运行1000次)，因为每次迭代都应用于同一组模型参数。</p><h1 id="81d3" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">分析</h1><h2 id="35b9" class="nr kw jg bd kx nw nx dn lb ny nz dp lf lw oa ob lh ma oc od lj me oe of ll og bi translated">批量梯度下降</h2><p id="c767" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">我们从一个“经典”组合开始分析:批处理GD变体和<code class="fe nj nk nl nm b">Desc</code>-优化器。对于4LS-和3LS-模型，我们将使用<em class="mj"> mse </em>损失函数和2LR交叉熵。正如前一篇文章中所讨论的，这些模型和成本函数的配对应该工作得最好。</p><p id="550d" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">一些实验表明，0.2的学习率在这种情况下效果很好。</p><p id="febc" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated"><strong class="lp jh">4LS和3LS的结果<br/> </strong>使用500次迭代，我们可以看到4LS模型在大约前200步快速收敛，然后明显变慢。在3LS模型上，这种减速在不到100次迭代时就已经开始了:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/89d0d3820baa6c0ab8d4370a8fb32d31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lxIFQd29dSgw7gfJVnprGQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代1–500[图片由作者提供]</p></figure><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/fd6d62233337d54217517c42bd675239.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yeheml0z00Rs8lSLYJ5z6w.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代1–500[图片由作者提供]</p></figure><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oi"><img src="../Images/94657c1255cd69172f1ba8debffb9ff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4qe-zrAQsKFQefYzNOUr5Q.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">批次GD — 4LS、3LS[图片由作者提供]</p></figure><p id="7ed4" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">训练3LS模型需要更长的时间，因为它有更多的参数。但是我们可以看到:并不是随着参数数量的线性增加。更高的努力也导致了更好的结果，在大约2000次迭代后显示:4LS保持在大约11%的精度，而3LS增加到几乎20%。</p><p id="10bf" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">但是在这两种情况下，结果都不是压倒性的(至少在评估的迭代范围内)。</p><p id="83c6" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated"><strong class="lp jh">2LR的结果</strong><br/>2LR-型号的结果完全不同。通过500次迭代，我们在大约200秒内获得了几乎93%的准确率。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/2b42495a8ec1f121312a14c804e9d802.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y-slbkY70QG-EvgnSbAbTg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代1–500[图片由作者提供]</p></figure><p id="2b5b" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">因此，在将迭代次数增加到4000次后，这些数字进一步提高，达到96%以上的准确度。</p><p id="bd18" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">放大迭代8000次到16000次之间的学习曲线，我们可以看到在损失方面仍有良好的进展。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/6e30cc95174f8e1a0c9b08e65d49d96a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZOQCjo79DlQ-ZQd2Wk4ybw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代1–16，000[图片由作者提供]</p></figure><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/a3d1a8044b6533d6a87288d92271dfc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ay5p6Xd1VaJ4BtVxbpa31A.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代8000–16000次[图片由作者提供]</p></figure><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oi"><img src="../Images/ac38ea14f4879a6a7e5dcff69658dbc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*udE09QV4gx1IOIVvHXuaYA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">批量GD—2LR[图片由作者提供]</p></figure><p id="d20a" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">进行更多的迭代仍然会减少损失:从16，000次迭代到32，000次迭代会将其值从0.033减少到0.013。但是准确性没有明显的变化(它保持在96%和97%之间，甚至随着迭代次数的增加而有所下降)。所以这可能是我们从这个配置中能得到的最好的了(真的不差！).</p><p id="c08f" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">在这一点上请注意:损失是根据训练数据计算的，而准确性是基于测试数据的。因此，这里描述的情况可能是对训练数据的过度拟合(因此损失进一步减少)，这不会导致对测试数据的更好预测(即准确性没有提高)。</p><h2 id="a264" class="nr kw jg bd kx nw nx dn lb ny nz dp lf lw oa ob lh ma oc od lj me oe of ll og bi translated">随机梯度下降</h2><p id="12e4" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">现在我们来看看GD的下一个变种。使用随机GD，我们应该期望大大降低运行时间，但可能也会降低质量。</p><p id="b17f" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated"><strong class="lp jh">4LS和3LS </strong> <br/>的结果，实际上，4LS-model在不到一秒的时间内完成500次迭代，达到0.090的损耗和10.28%的精度。对于3LS模型来说，同样数量的迭代需要一秒多一点的时间，而我们得到的准确率接近14%。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/427a958c051145330bc6be5f84187b7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WfOOn_ejtiTFOtxt4kGLMQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代1–500[图片由作者提供]</p></figure><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/5f35183d28fabaf567fb389293d904cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SXLztqGReQFMcvi3uGdk0w.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代1–500[图片由作者提供]</p></figure><p id="27e1" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">仔细观察学习曲线，我们还可以看到它们不像批次GD中的曲线那样平滑。曲线上的小突起是由于随机GD并不总是以最佳方式达到最小值。</p><p id="14fc" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">这里出现了与上面相同的问题:我们能通过更多的迭代获得(显著)更好的结果吗？用4LS做1000次迭代，精度(至少有一点)提高到11.35%。但是在2，000次迭代时，我们再次变差(10.28%)，在8，000次迭代时，我们回到11.35%。发生了什么事？</p><p id="e59d" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">放大学习曲线(迭代2，000到4，000次之间)很有启发性。在这里，我们可以在一个具体的例子上看到我们之前在理论上描述的:随机GD不以最直接的方式移动到最小值，而是以之字形路径移动。而在这里，它似乎根本没有朝着正确的方向前进。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/54332ac336d7c804cd1a089ba438ef5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BQMl8GKFjnkMYEc904lvCQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代2000–4000次[图片由作者提供]</p></figure><p id="a59d" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">对于3LS模型，同样的情况似乎会发生，因为我们在500次迭代时达到13.94%的精度，在1000次迭代时回落到13.12%。但是学习曲线表明这里的情况不同:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/b310b9ff3fc80d456fd342083d1bca2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jpr5zJZ4KgKf787ggW99Ww.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代500–2000次[图片由作者提供]</p></figure><p id="6ecd" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">曲线也是振荡的，但是有一个明显的下降趋势。因此，经过16，000次迭代后，我们的准确率接近80%，经过512，000次迭代后，我们的准确率达到96.64%。</p><p id="7945" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">除此之外，学习曲线以一种有趣的方式发展:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/e984fb55f1a410cd33fc0197441ecd79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TLJ5fgSVUHsbqX8a5X-HqA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代1–16，000[图片由作者提供]</p></figure><p id="9f81" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">在最初几次迭代中迅速降低之后，损耗在大约4，000次迭代之前没有太大变化。有一个平稳期。但是我们又一次看到了显著的进步。</p><p id="125f" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">所以也许4LS显示了同样的现象，我们不应该这么早放弃？事实上，在15，000次和20，000次迭代之间的范围内，损耗开始下降得更快，在大约60，000次迭代以上的范围内，学习曲线再次变得更陡:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oj"><img src="../Images/3a43aba019e31a4bc6d9e048b06c0b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D8BYD0mnp9dFp9GEtUwNDw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代4001–54，000和4001–154，000[图片由作者提供]</p></figure><p id="d1bb" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">通过超过1 M的迭代，我们在4LS的情况下实现了94.78%的准确度。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ok"><img src="../Images/86d5b1ae6fec50202239d6a891f67c48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*STYVf-OI8Oj91LLiBa-oCw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">随机GD — 4LS，3LS[图片由作者提供]</p></figure><p id="c623" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated"><strong class="lp jh">2LR的结果<br/> </strong>与4LS和3LS相比，2LR在500次迭代时的性能明显更差:我们仅获得9.75%的精度(但与使用批处理GD的大约200秒相比，只需要2.7秒)。不幸的是，如果我们做更多的迭代，结果不会变得更好。即使迭代超过200万次，我们也只能得到10.1%的准确率。学习曲线显示了与这一发现一致的清晰画面:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/e4792dafe3bcb88d6fb700df6f3e276a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VBJ-5nA9Twe_NeMPZQBgdA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代8，001–2，048，000[图片由作者提供]</p></figure><p id="8a59" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">为什么我们用这种配置会得到这么差的结果？答案大概是:太无知了。随机GD忽略60，000个实例中的59，999个(这是一个很大的数目！).然后我们将剩下的少量信息输入损失函数。<em class="mj"> mse </em>以同样的方式使用该信息的每一位来计算损失，但是<em class="mj"> ce </em>本身有一些内置的“无知”:它只考虑产生的唯一热向量的一个值(最大值)。也就是说，它非常重视那个值，而忽略了其他值。从许多实例来看，这可能是一个好策略，但是在我们的情况下(尤其是从一个未经训练的模型开始)，这可能只会导致随机的无意义。</p><h2 id="1250" class="nr kw jg bd kx nw nx dn lb ny nz dp lf lw oa ob lh ma oc od lj me oe of ll og bi translated">小批量梯度下降</h2><p id="4216" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">由于小批量GD是其他两种变体之间的折衷，它应该在我们到目前为止考虑的所有配置上都工作良好，但是比随机GD运行需要更长的训练时间。让我们看看，如果训练运行符合我们的期望。</p><p id="898d" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated"><strong class="lp jh">4LS和3LS的结果<br/> </strong>实际上，我们对4LS和3LS模型都获得了极好的结果:在大约50万次迭代时，可分别实现94.56%的准确度，在200万次迭代时，可分别实现97.68%的准确度。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ok"><img src="../Images/0718bf3dd7b99f462f1df29024e8bb25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aGWbRdI2GkW5U0aGW54vLg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">小批量GD — 4LS，3LS[图片由作者提供]</p></figure><p id="5182" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated"><strong class="lp jh">2LR的结果</strong> <br/>同样适用于2LR模型:这里我们在64，000次迭代中有96.98%的准确度。例如，学习曲线比带有<em class="mj"> mse </em>的4LS模型的学习曲线更加振荡，但它明显收敛:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ol"><img src="../Images/73df1054980bbd6fc9336a7e764ad10c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*usBYrvh25I-G_d9_D6ZVaA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代4，001–512，000[图片由作者提供]</p></figure><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ok"><img src="../Images/8229b3e9446c32bbed44152af92400d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S-XCVAgtLbXrgqsrq_yvRw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">小批量GD—2LR[图片由作者提供]</p></figure><h1 id="f980" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">结论</h1><p id="5b78" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">当我们查看每种测试配置所能达到的最佳精度时，我们会看到下图:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/deca67838b60c9615d9127958b356524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9pUEWEcqdNLTmDgxlIPkzw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">训练时间的最高准确度[图片由作者提供]</p></figure><p id="2051" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">每个模型<em class="mj">都有一个</em> GD变体，这并没有带来好的结果(至少在测试的迭代范围内)。剩下的两个GD变量导致所有模型的精度值远远超过90%，这是非常好的。但是在训练时间上的差异是惊人的:3LS和mini-batch用了将近一个小时，而2LR和mini-batch用了不到50秒。</p><p id="225a" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">在下表中，我们计算了一个名为“努力”的绩效指标，它是训练时间和准确性之间的比率。所以你可以看到这方面的佼佼者:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi on"><img src="../Images/80d47bdb50bee2efb33c01c66c417ace.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NNXy9M35BJ7sFUgHjuhzXw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">按“努力”排名[图片由作者提供]</p></figure><p id="c270" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">另一个有趣的观察是，更多的模型参数不一定导致更好的准确性。</p><p id="fd1a" class="pw-post-body-paragraph ln lo jg lp b lq mk kh ls lt ml kk lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">因此，我们可以看到我们的“构建模块”的不同配置如何导致行为和KPI的有趣变化。…但是我们是不是忘了什么？亚当怎么办？嗯，那是另一篇文章的素材:-)。</p></div></div>    
</body>
</html>