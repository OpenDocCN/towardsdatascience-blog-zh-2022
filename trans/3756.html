<html>
<head>
<title>Privacy preserving Recommenders based on Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于强化学习的隐私保护推荐器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/privacy-preserving-recommenders-based-on-reinforcement-learning-13fd2bc391d3#2022-08-21">https://towardsdatascience.com/privacy-preserving-recommenders-based-on-reinforcement-learning-13fd2bc391d3#2022-08-21</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="1665" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">将隐私与强化学习相结合</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/390949e4a6247804d3132ac6c619b737.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bTm4RrKu8Rvaaiwt.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">Pic鸣谢:探索未知，作者Soma Biswas (Flickr: <a class="ae kz" href="https://flic.kr/p/2neEnwZ" rel="noopener ugc nofollow" target="_blank">链接</a>，经允许转载)</p></figure><h1 id="fd87" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">介绍</h1><p id="18df" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">在本文中，我们主要关注在对话环境中实现推荐。更具体地说，我们考虑一个能够与用户对话的应用程序，向用户提供有趣和相关的信息；(按需)响应特定的用户查询，或者以及时和相关的警报形式主动提供。</p><p id="4d01" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><em class="mt">互动个性化</em>:虽然今天的聊天机器人能够根据用户情绪来个性化对话，但这种情绪通常来自用户的查询/响应——通常由用户明确提供的用户反馈。这导致了对提供这种反馈的用户的依赖，并意味着代表用户的额外努力。此外，反馈是非常个人化的，由用户特别提供，而不考虑周围环境、位置(家、办公室、市场……)、附近的其他家庭成员等。</p><blockquote class="mu mv mw"><p id="5af1" class="ls lt mt lu b lv mo jv lx ly mp jy ma mx mq md me my mr mh mi mz ms ml mm mn in bi translated">这篇文章旨在通过提供基于<em class="iu">隐含</em>用户反馈、<em class="iu">适应历史和环境背景</em>的高度个性化和交互式体验来改善这一方面。</p></blockquote><ul class=""><li id="40e5" class="na nb iu lu b lv mo ly mp mb nc mf nd mj ne mn nf ng nh ni bi translated"><em class="mt">隐式反馈</em>:基于用户提供的显式反馈(例如，用户响应、NPS分数/评级)和隐式反馈(例如，在接收到聊天机器人响应/通知时面部表情的变化)来计算用户情绪</li><li id="5878" class="na nb iu lu b lv nj ly nk mb nl mf nm mj nn mn nf ng nh ni bi translated"><em class="mt">环境背景</em>:第二个区别点基于我们考虑用户环境(周围)因素的事实，例如位置(家庭、办公室、市场……)、温度、照明、一天中的时间、天气、附近的其他家庭成员等等；根据用户/环境情绪调整对话的参数。</li><li id="d6f4" class="na nb iu lu b lv nj ly nk mb nl mf nm mj nn mn nf ng nh ni bi translated"><em class="mt">对话历史</em>:任何用户反馈都需要与迄今为止用户对对话的反应相结合。这使得我们可以忽略用户反馈/情绪的任何突然变化，这可能是由于外部因素造成的，与应用程序/聊天机器人的交互无关。</li></ul><p id="9546" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv">隐私保护交互</strong>:捕捉用户情感，尤其是。环境方面，需要高度的知识规。用户上下文。因此，我们需要以保护隐私的方式执行此操作，以充分解决用户隐私问题，同时提供高度个性化的体验。</p><blockquote class="mu mv mw"><p id="2227" class="ls lt mt lu b lv mo jv lx ly mp jy ma mx mq md me my mr mh mi mz ms ml mm mn in bi translated">本文通过向核心聊天机器人和强化学习(RL)框架添加隐私层来实现这一点。</p></blockquote><p id="4e06" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">RL是我们用来使对话实时适应用户/环境上下文的核心技术。</p><p id="32d2" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">相对于当前技术水平，此处的主要优势是:</p><blockquote class="no"><p id="0b06" class="np nq iu bd nr ns nt nu nv nw nx mn dk translated">从聊天机器人的角度来看，我们展示了用户反馈(包括环境参数)和对话历史如何以隐私保护的方式作为参数来适应/个性化用户对话。</p><p id="3aa6" class="np nq iu bd nr ns nt nu nv nw nx mn dk translated">从<strong class="ak"> RL框架</strong>的角度，我们展示了RL奖励计算是如何分布的，从而在提供奖励函数的准确表示的同时保护用户隐私。</p></blockquote><h1 id="edda" class="la lb iu bd lc ld le lf lg lh li lj lk ka ny kb lm kd nz ke lo kg oa kh lq lr bi translated">背景</h1><p id="5911" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated"><em class="mt">建议</em>:拥有一个由有趣的文章、事实、数据和视频组成的知识库，其中的文章根据他们最可能感兴趣的用户资料进行分类。然后向用户提供最相关文章的基于简档的提醒/通知。</p><p id="e68f" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><em class="mt">聊天机器人</em>:以上是一次性互动的经典例子，虽然个性化；错过互动部分。聊天机器人可以克服这个限制。今天的企业聊天机器人是基于FAQ的[1，8]。这种聊天机器人的核心是一个意图识别自然语言理解(NLU)引擎，它通过问题变体的硬编码示例进行训练。当没有符合30%以上置信度的意图时，聊天机器人会返回一个后备答案。对于所有其他情况，NLU引擎会随响应一起返回相应的可信度。高级聊天机器人还会考虑嵌入在查询/响应中的用户情绪，以相应地调整响应。</p><p id="084e" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><em class="mt">强化学习(RL) </em> : RL指的是AI/ML的一个分支，针对目标导向的问题。RL算法能够通过在许多步骤上最大化奖励函数来实现复杂的目标，例如在许多步骤上在游戏中赢得的分数。奖励功能的工作原理类似于用糖果和打屁股来激励孩子，这样算法在做出错误决定时会受到惩罚，在做出正确决定时会受到奖励——这就是强化。RL算法如下图所示——图1。有关RL框架的详细介绍，感兴趣的读者可以参考[2]。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj ob"><img src="../Images/a4813d013938f9649fd1c5509c1d401e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WUf0t7NxDn5SAy6v.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图1:强化学习(作者图片)</p></figure><h1 id="806c" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">基于RL的推荐解决方案架构</h1><p id="9df7" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">在这一节中，我们概述了基于隐式用户反馈实现个性化交互的解决方案架构，接下来是下一节中的隐私保护扩展。</p><p id="d1cc" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">图2中的架构示出了包括隐式用户反馈的用户和环境条件如何</p><p id="989d" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">1.使用可用的传感器收集以计算“当前”反馈，包括环境背景(例如，用户的网络摄像头照片可用于推断用户对聊天机器人响应/通知的情绪、房间照明条件和附近的其他用户)</p><p id="0252" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">2.其然后与用户对话历史相结合，以量化用户“情感曲线”,并忽略由于不相关因素引起的情感的任何突然变化</p><p id="90bd" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">3.导致与提供给用户的最后聊天机器人响应/应用通知相对应的合计奖励值。</p><blockquote class="mu mv mw"><p id="96ba" class="ls lt mt lu b lv mo jv lx ly mp jy ma mx mq md me my mr mh mi mz ms ml mm mn in bi translated">然后，该奖励值被反馈给RL代理，以便RL策略从知识库中选择下一个最佳聊天机器人响应/应用程序通知。</p></blockquote><p id="80c6" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">这导致了分布式RL奖励计算，据我们所知，这在以前的文献中还没有考虑过。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj oc"><img src="../Images/8ac60c6124abac933bd86c6760393b51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dO8JALv5jrByU7hMUW7Hrg.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图2:个性化架构——分布式RL奖励计算(图片由作者提供)</p></figure><h1 id="4ffa" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">隐私保护交互</h1><p id="a80f" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">隐私保护保证由多种技术提供:</p><ul class=""><li id="5aa6" class="na nb iu lu b lv mo ly mp mb nc mf nd mj ne mn nf ng nh ni bi translated">在“边缘”设备/传感器上分布情感计算，例如可以在摄像机本身上计算“当前”情感。它可以被计算和匿名化，使得只有归一化的情感分数被传播到移动设备或云以供进一步计算。不需要在任何点传播捕获的(网络摄像头)用户图像，该图像可以在“当前”情感计算之后被销毁。</li><li id="927e" class="na nb iu lu b lv nj ly nk mb nl mf nm mj nn mn nf ng nh ni bi translated">隐私保护方案，例如秘密共享和同态加密[3]，可以用于将正/负“当前”情感值与移动设备上的历史情感曲线相加和，而移动设备上的客户端应用不知道实际的“当前”情感值。</li></ul><blockquote class="mu mv mw"><p id="573b" class="ls lt mt lu b lv mo jv lx ly mp jy ma mx mq md me my mr mh mi mz ms ml mm mn in bi translated">这允许RL代理受益于非常详细和准确的奖励值，尽管是以隐私保护的方式。</p></blockquote><p id="b6cd" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">隐私保护RL奖励计算流水线如图3所示。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj od"><img src="../Images/c725d9280f69036e952ffc25319c9931.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_hoIevsaTV56KzCZ3vfIFw.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图3:隐私保护对话架构(图片由作者提供)</p></figure><h1 id="a89c" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">隐私保护强化学习</h1><h2 id="7f46" class="oe lb iu bd lc of og dn lg oh oi dp lk mb oj ok lm mf ol om lo mj on oo lq op bi translated">传感器/ <strong class="ak">边缘设备</strong>的隐私保护</h2><p id="b855" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">在这种情况下，边缘设备可以是(独立的)摄像头、麦克风、恒温器、智能手表，或者是嵌入在承载应用的移动设备中的摄像头、麦克风、加速度计。</p><p id="cba4" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">例如，我们认为用户情绪是基于边缘设备/传感器捕获的音频、视觉、文本反馈来计算的。我们进一步假设，对于每个感觉参数，用户可以提供对应于不同方面(例如位置、人、健康/活动)的隐私敏感度等级(低、中、高)。</p><p id="c245" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">让<em class="mt">在</em>、<em class="mt"> Vt </em>、<em class="mt"> Tt </em>参考在<em class="mt"> t </em>期间捕获的音频、视觉和文本反馈。</p><ul class=""><li id="c534" class="na nb iu lu b lv mo ly mp mb nc mf nd mj ne mn nf ng nh ni bi translated">例如，让<em class="mt"> Vt </em>对应于背景中有家庭成员的用户的视频帧。在这种情况下，关于“人”的“低”、“中”和“高”隐私设置将对应于保持画面原样、模糊人的面部、裁剪图像以将人从画面中完全移除。</li><li id="55f2" class="na nb iu lu b lv nj ly nk mb nl mf nm mj nn mn nf ng nh ni bi translated">类似地，让<em class="mt"> Tt </em>对应于用户提供的文本响应，例如“我明天要去克拉科夫出差”在这种情况下，关于“位置”的“低”、“中”和“高”隐私设置将对应于保持响应原样，在响应中将“克拉科夫”抽象为“欧洲的某个地方”，从响应中完全移除目的地:“我明天将出差”</li></ul><h2 id="6f40" class="oe lb iu bd lc of og dn lg oh oi dp lk mb oj ok lm mf ol om lo mj on oo lq op bi translated">通过移动应用程序保护隐私</h2><p id="6df4" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">让<em class="mt"> P(At) </em>、<em class="mt"> P(Vt) </em>和<em class="mt"> P(Tt) </em>表示各自捕获的感官反馈，由边缘设备传感器应用隐私保护——根据用户指定的设置。边缘设备然后与移动应用共享<em class="mt"> P(At) </em>、<em class="mt"> P(Vt) </em>和<em class="mt"> P(Tt) </em>。</p><p id="144d" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">让</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oq"><img src="../Images/d76a258a1d7582b7ad5fbe57a35184b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*YTWdAyioUvvqcS10Zrxvbw.png"/></div></figure><p id="de30" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">表示基于各自的感觉反馈独立计算的用户情绪。用户情感计算可以被认为是输出在<em class="mt">1–10</em>之间的值的分类器。</p><p id="ed18" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">下一步，也是移动应用的主要功能，是聚合上述(独立计算的)情感值，并计算合并的用户情感<em class="mt"> St </em>。聚集函数例如可以是加权平均值:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj or"><img src="../Images/53ea267f6614a31b281c99e207e1e3dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yv2H5mb7-6SMXDvXw1rmtA.png"/></div></div></figure><p id="4db4" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">只要它满足下面的一致性检查:如果在3个用户情感值之间存在显著的差异；然后可以应用不同的策略来巩固它们:</p><p id="a1be" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">a.忽略这个反馈周期，因为不同的感官反馈之间有太多的差异。这通过将值<em class="mt"> 0 </em>分配给综合用户情绪<em class="mt"> St </em>来表示。</p><p id="bddd" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">b.向显式反馈提供比隐式反馈更高的权重，例如，如果文本情感得分是基于用户键入的响应计算的，而视觉得分是基于背景帧计算的；然后给予基于文本的情感分数更高的权重。例如，用户可能在快照中带着他的孩子微笑，但从他的文本/语音响应来看，他似乎“有压力”——在这种情况下，我们优先考虑“有压力”的用户情绪。</p><blockquote class="mu mv mw"><p id="e192" class="ls lt mt lu b lv mo jv lx ly mp jy ma mx mq md me my mr mh mi mz ms ml mm mn in bi translated">隐私保护应用的输出是提供(合并的)用户情感<em class="iu"> St </em>，以及由用户提供的任何明确的用户响应<em class="iu"> {P(At)，P(Vt)，P(Tt)} </em>(根据边缘设备/传感器处的用户设置匿名化)。</p></blockquote><p id="1ce0" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">例如，在聊天对话的情况下，在“位置”隐私设置为“中等”的情况下，它会将<em class="mt"> St </em>连同用户响应“我明天将去欧洲出差”一起发送到后端RL/NLP引擎。</p><h2 id="7d96" class="oe lb iu bd lc of og dn lg oh oi dp lk mb oj ok lm mf ol om lo mj on oo lq op bi translated">后端隐私保护— RL引擎</h2><p id="2843" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">我们关注强化学习(RL)引擎的两个关键功能:奖励功能和(RL代理)策略——它们共同调节RL引擎内容个性化。我们考虑一个分类内容目录，其中应用推荐/(聊天)响应被分组为与用户兴趣相关的类别，例如旅游、娱乐、健康、位置等。</p><p id="9f88" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我们首先概述奖励函数<em class="mt"> fr </em>，它基于确定的用户情绪<em class="mt"> St </em>计算对应于最后一个RL引擎(在的动作<em class="mt">)推荐/响应的奖励。</em></p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj os"><img src="../Images/969fa06f13f322da744d73d4b66dc92e.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/format:webp/1*TPNElNJMQoFWvd5OZ663Tw.png"/></div></figure><p id="81c9" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">其中函数<em class="mt"> fr </em>逻辑如下所示:</p><ul class=""><li id="9f85" class="na nb iu lu b lv mo ly mp mb nc mf nd mj ne mn nf ng nh ni bi translated">如果<em class="mt"> (St = 0) </em>，由于反馈收集过程中的不一致性，我们简单地忽略这个学习循环。</li><li id="f593" class="na nb iu lu b lv nj ly nk mb nl mf nm mj nn mn nf ng nh ni bi translated"><strong class="lu iv">历史归一化</strong>:对于<em class="mt">St =[1–10]</em>，在将<em class="mt"> St </em>值赋给奖励值<em class="mt"> rt </em>之前，会根据历史上下文进一步归一化。历史背景确定如下:<br/> (a)在正在进行的(连续的)对话的情况下，将当前情绪与到目前为止的对话的情绪曲线进行比较，以标准化其影响。</li></ul><blockquote class="no"><p id="fb13" class="np nq iu bd nr ns ot ou ov ow ox mn dk translated">例如，如果对话情绪已经变得更糟，低的当前情绪可能不仅仅是上一个动作的错误；因此，奖励需要相应地调整。</p><p id="3b0c" class="np nq iu bd nr ns nt nu nv nw nx mn dk translated"><em class="oy">另一方面，给定恶化的情感曲线，高情感值将暗示最后的动作对用户具有非常积极的影响；因此相应的回报需要进一步提高。</em></p></blockquote><ul class=""><li id="b3e2" class="na nb iu lu b lv oz ly pa mb pb mf pc mj pd mn nf ng nh ni bi translated">在<strong class="lu iv">特别推荐</strong>的情况下，可以应用类似的归一化逻辑，其中根据先前接收到的反馈(情感值)对当前情感<em class="mt"> St </em>进行校准，该反馈用于与上一个动作相同类别的推荐。</li><li id="f424" class="na nb iu lu b lv nj ly nk mb nl mf nm mj nn mn nf ng nh ni bi translated"><strong class="lu iv">延迟奖励</strong>:对于连续对话和特别推荐，也可以应用“延迟奖励”的替代策略，其中最后<em class="mt"> m </em>行动的奖励被组合并(追溯性地)应用于【在的<em class="mt">、<em class="mt">在-1 </em>、…、<em class="mt">在-m </em>。例如，如果对于类别“旅行”的推荐，当前情绪<em class="mt"> St </em>较低，已知用户过去对该推荐(对其他“旅行”推荐)反应非常积极；延迟奖励策略将简单地缓冲当前值(在</em>、<em class="mt">和</em>)，并向RL代理(策略)提供指示以尝试相同类别的另一个推荐来“验证”用户情绪——在更新在的<em class="mt">和在+1 </em>的<em class="mt">的奖励之前。有关RL延迟奖励政策的更正式描述，请参考[4]。</em></li></ul><h1 id="617b" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">参考</h1><ol class=""><li id="fb1c" class="na nb iu lu b lv lw ly lz mb pe mf pf mj pg mn ph ng nh ni bi translated">Biswas，D.: <em class="mt">聊天机器人&amp;自然语言搜索。</em>走向数据科学，2019 ( <a class="ae kz" rel="noopener" target="_blank" href="/chatbots-natural-language-search-cc097f671b2b">链接</a>)</li><li id="7518" class="na nb iu lu b lv nj ly nk mb nl mf nm mj nn mn ph ng nh ni bi translated">巴尔托，a .，萨顿，R.S.: <em class="mt">强化学习:导论</em>。麻省理工学院出版社，2018年，<a class="ae kz" href="http://incompleteideas.net/book/RLbook2018.pdf" rel="noopener ugc nofollow" target="_blank">http://incompleteideas.net/book/RLbook2018.pdf</a></li><li id="e692" class="na nb iu lu b lv nj ly nk mb nl mf nm mj nn mn ph ng nh ni bi translated">D.比斯瓦斯。保护隐私的聊天机器人对话。进行中。第34届NeurIPS隐私保护机器学习研讨会(PPML)，2020年(<a class="ae kz" href="https://ppml-workshop.github.io/ppml20/pdfs/Biswas.pdf" rel="noopener ugc nofollow" target="_blank"> pdf </a>)。也发表在《走向数据科学》(<a class="ae kz" rel="noopener" target="_blank" href="/hidden-privacy-risks-of-chatbot-conversations-881dbeeb98a">链接</a>)</li><li id="d018" class="na nb iu lu b lv nj ly nk mb nl mf nm mj nn mn ph ng nh ni bi translated">基于强化学习的推荐系统。走向数据科学(<a class="ae kz" rel="noopener" target="_blank" href="/reinforcement-learning-based-recommender-systems-492a2674fcc">链接</a>)，也发表在2020年9月第24届欧洲人工智能大会(ECAI)的“医疗保健人工智能进展”专题中。</li><li id="60f2" class="na nb iu lu b lv nj ly nk mb nl mf nm mj nn mn ph ng nh ni bi translated">崔顺实，夏，黄，吴..使用双聚类技术的基于强化学习的推荐系统。arXiv:1801.05532，2018。</li><li id="91cf" class="na nb iu lu b lv nj ly nk mb nl mf nm mj nn mn ph ng nh ni bi translated">刘，冯，唐，r，李，x，叶，y，陈，h，郭，h，张，y:<em class="mt">基于深度强化学习的推荐与显式用户-项目交互建模</em>。arXiv:1810.12027，2018</li><li id="da88" class="na nb iu lu b lv nj ly nk mb nl mf nm mj nn mn ph ng nh ni bi translated">Taghipour，Kardan，a .，Ghidary，S.S.: <em class="mt">基于使用的网络推荐:强化学习方法</em>。摘自:美国计算机学会推荐系统会议，第113-120页(2007)。</li><li id="8846" class="na nb iu lu b lv nj ly nk mb nl mf nm mj nn mn ph ng nh ni bi translated">Ricciardelli，e .，Biswas，D.: <em class="mt">基于强化学习的自我改进聊天机器人</em>。in:2019年第四届强化学习与决策多学科会议。也发表在《走向数据科学》(<a class="ae kz" rel="noopener" target="_blank" href="/self-improving-chatbots-based-on-reinforcement-learning-75cca62debce">链接</a></li></ol></div></div>    
</body>
</html>