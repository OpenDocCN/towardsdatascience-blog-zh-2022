<html>
<head>
<title>How to Speed Up Training for Large Language Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何加快大型语言模型的训练</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-speed-up-training-for-large-language-models-81ffb30c36b2#2022-08-12">https://towardsdatascience.com/how-to-speed-up-training-for-large-language-models-81ffb30c36b2#2022-08-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="72fd" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">探索序列并行性和选择性激活重新计算的概念</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d3a0d553eda348da69287aa270b2fa06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4QpdBM516FvHCha1B67-Yw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">艾莉娜·格鲁布尼亚克在<a class="ae ky" href="https://unsplash.com/s/photos/neural-network?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="925a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着人工智能的进步，现在可以训练大型语言模型来完成自然语言处理任务。通常，大型语言模型包含超过1000亿个参数，并使用高级算法在大型语料库上进行训练。</p><p id="7b75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大型语言模型(LLM)非常有效，能够很好地概括大多数下游任务，如文本生成、翻译、摘要、语义搜索等。因此，它为自然语言处理领域的开发人员和研究人员提供了新的能力。</p><p id="dd4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在撰写本文时，世界各地的研究人员在训练大型语言模型方面取得了重大突破。下面的列表重点介绍了一些最著名的具有一流性能的大型语言模型:</p><ul class=""><li id="976d" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><a class="ae ky" href="https://github.com/facebookresearch/metaseq" rel="noopener ugc nofollow" target="_blank"> OPT-175B </a> —一个基于公开可用数据集的拥有1750亿个参数的语言模型。这是Meta AI研究计划的一部分。</li><li id="b824" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://huggingface.co/bigscience/bloom" rel="noopener ugc nofollow" target="_blank"> BLOOM </a> —在46种语言和13种编程语言上训练的1760亿个参数模型。这个模型是由大科学促成的。</li><li id="5434" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://github.com/NVIDIA/Megatron-LM" rel="noopener ugc nofollow" target="_blank">威震天-图灵NLG模型</a>—105层变压器模型，拥有5300亿个参数。这是微软和英伟达的共同努力。</li></ul><p id="2b61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练一个大型的语言模型并不容易，而且常常伴随着许多挑战。例如:</p><ul class=""><li id="9e1c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">不可能在GPU的内存中容纳大型语言模型的所有参数。它需要分布式软件和硬件轨道来训练大型语言模型。</li><li id="a605" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">训练时间长得不切实际，这使得训练大型语言模型的成本非常高。整个训练过程需要跨越数千个GPU的并行性。此外，算法需要进行优化，以便在内存和计算上高效且可扩展。</li></ul><p id="d290" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有两种新方法可用于改进大型语言模型的训练:</p><ul class=""><li id="ec67" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><a class="ae ky" href="https://arxiv.org/abs/2205.05198" rel="noopener ugc nofollow" target="_blank">序列并行性</a></li><li id="0b9e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://arxiv.org/abs/2205.05198" rel="noopener ugc nofollow" target="_blank">选择性激活重新计算(SAR) </a></li></ul><p id="3af3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">NVIDIA AI平台在其对NeMo威震天大型语言模型的最新更新中演示了这些技术。根据所提供的基准，当训练GPT-3模型的不同变体时，这两种技术都将训练时间减少了约30%(最小的模型约为220亿个参数，而最大的模型高达1万亿个参数)。</p><blockquote class="mj mk ml"><p id="a962" class="kz la mm lb b lc ld ju le lf lg jx lh mn lj lk ll mo ln lo lp mp lr ls lt lu im bi translated">本教程涵盖了这两种技术背后的基本概念。对于实际的实现，请前往<a class="ae ky" href="https://github.com/NVIDIA/Megatron-LM" rel="noopener ugc nofollow" target="_blank">官方库</a>。</p></blockquote></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="2166" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">序列并行性</h1><p id="ea82" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">序列并行与张量级模型并行协同工作。首先，它注意到了以前没有并行化的transformer层区域。这些层在序列维度上是独立的。</p><p id="d53f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请看下图中变形层的并行模式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/b8cfae3034c200c9f0d67ae2163aec9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ySti3xMiWXb9FQFS.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由英伟达公司提供</p></figure><p id="fad2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随后，它将通过沿着序列维度分割变换器层，在张量并行设备上分布这些区域的计算和激活存储器。结果，不需要重新计算，因为分布式激活可以用于反向传递。</p><p id="d888" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据<a class="ae ky" href="https://arxiv.org/abs/2205.05198" rel="noopener ugc nofollow" target="_blank">发表的论文，</a>嵌入层的输出是一个大小为s × b × h的三维张量。该输出也代表了变换器模块的输入。</p><blockquote class="mj mk ml"><p id="ef16" class="kz la mm lb b lc ld ju le lf lg jx lh mn lj lk ll mo ln lo lp mp lr ls lt lu im bi translated"><code class="fe nv nw nx ny b"><em class="it">a</em></code> —注意头的数量</p><p id="cc4a" class="kz la mm lb b lc ld ju le lf lg jx lh mn lj lk ll mo ln lo lp mp lr ls lt lu im bi translated"><code class="fe nv nw nx ny b"><em class="it">s</em></code> —序列长度</p><p id="d289" class="kz la mm lb b lc ld ju le lf lg jx lh mn lj lk ll mo ln lo lp mp lr ls lt lu im bi translated"><code class="fe nv nw nx ny b"><em class="it">b</em></code> —微量批量</p><p id="f11c" class="kz la mm lb b lc ld ju le lf lg jx lh mn lj lk ll mo ln lo lp mp lr ls lt lu im bi translated"><code class="fe nv nw nx ny b"><em class="it">h</em></code> —隐藏尺寸大小</p><p id="ed25" class="kz la mm lb b lc ld ju le lf lg jx lh mn lj lk ll mo ln lo lp mp lr ls lt lu im bi translated"><code class="fe nv nw nx ny b"><em class="it">t</em></code> —张量平行尺寸</p></blockquote><p id="79c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">多层感知器(MLP)块所遵循的层范数的公式如下:</p><blockquote class="mj mk ml"><p id="edd1" class="kz la mm lb b lc ld ju le lf lg jx lh mn lj lk ll mo ln lo lp mp lr ls lt lu im bi translated">Y = LayerNorm(X)，</p><p id="6755" class="kz la mm lb b lc ld ju le lf lg jx lh mn lj lk ll mo ln lo lp mp lr ls lt lu im bi translated">Z = GeLU(Y A)，</p><p id="6021" class="kz la mm lb b lc ld ju le lf lg jx lh mn lj lk ll mo ln lo lp mp lr ls lt lu im bi translated">W = ZB，</p><p id="8c5e" class="kz la mm lb b lc ld ju le lf lg jx lh mn lj lk ll mo ln lo lp mp lr ls lt lu im bi translated">V =压降(W)</p></blockquote><p id="1165" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">x是尺寸为s × b × h的层范数的输入。另一方面，A和B表示线性层的权重矩阵。作者将张量并行和序列并行操作组合成单个减少-分散操作。</p><p id="d4ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将所有这些部分结合在一起，最终的等式如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/21e1dbabe190340f65cb492611b87f3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ivBtU_WEjmaIvu7QbEogdg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者提供。方程式是使用Markdown中的LaTeX创建的。</p></figure><p id="910c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个注意块大约需要</p><pre class="kj kk kl km gt oa ny ob oc aw od bi"><span id="b19c" class="oe my it ny b gy of og l oh oi">11sbh + 5as^2b</span></pre><p id="576a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">字节的存储。细分如下:</p><h2 id="f1a4" class="oe my it bd mz oj ok dn nd ol om dp nh li on oo nj lm op oq nl lq or os nn ot bi translated">MLP</h2><ul class=""><li id="aea6" class="lv lw it lb b lc np lf nq li ou lm ov lq ow lu ma mb mc md bi translated">两个线性层以2sbh和8sbh的大小存储它们的输入。</li><li id="036c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">GeLU非线性以8sbh大小存储其输入用于反向传播。</li><li id="58f1" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">dropout存储其大小为sbh的掩码。</li></ul><p id="cf60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MLP块需要大约19sbh字节的存储。</p><h2 id="5e64" class="oe my it bd mz oj ok dn nd ol om dp nh li on oo nj lm op oq nl lq or os nn ot bi translated">层范数</h2><ul class=""><li id="fd9f" class="lv lw it lb b lc np lf nq li ou lm ov lq ow lu ma mb mc md bi translated">每个层norm存储其大小为2sbh的输入，使其总存储量为4sbh。</li></ul><p id="5947" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">存储变压器网络单层的激活所需的总存储器如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/da6afe12250df0803873243a0c04b3aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sFCswNCH4rOu-_hh46fqdw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者提供。方程式是使用Markdown中的LaTeX创建的。</p></figure><p id="41bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过应用t向张量并行，存储每层激活所需的内存现在是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/ce1609824b1b8ac845521b82a746997e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oaJ5Eg-zV0JPB8mter8pBg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者提供。方程式是使用Markdown中的LaTeX创建的。</p></figure><p id="8aa3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过结合t-way张量并行性和序列并行性，可以进一步减少所需的存储器:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/d136979d5c454da3c3129f0244042d62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*5J4ywhiJ0hCwmFSR-KuSKQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者提供。方程式是使用Markdown中的LaTeX创建的。</p></figure></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="fc13" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">选择性激活重新计算(SAR)</h1><p id="a765" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">顾名思义，选择性激活重新计算通过仅重新计算每个转换器层的部分而不是整个转换器层来优化性能。这种方法对于由于内存限制而需要重新计算力的情况非常有用。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/770b55317c2f83d2e2635ff06ae70162.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QvoqpQNEFbrjNpyd.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由英伟达公司提供</p></figure><p id="6761" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">红色虚线示出了应用选择性激活重新计算的区域。请注意，不同的激活需要不同数量的重新计算操作。</p><p id="801c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过组合t-way张量并行性、序列并行性和选择性激活重新计算，所需的存储器减少到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/2dea52f2800d971066c05fecae141ee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*E9QlVS4GwdRyHD9IyTcZLQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者提供。方程式是使用Markdown中的LaTeX创建的。</p></figure><p id="c840" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的等式清楚地概述了这两种技术允许所需的激活记忆与序列长度成线性比例。此外，它们现在独立于注意力头的数量。</p><blockquote class="mj mk ml"><p id="be28" class="kz la mm lb b lc ld ju le lf lg jx lh mn lj lk ll mo ln lo lp mp lr ls lt lu im bi translated">有关序列并行性和选择性激活重新计算的更多信息，可以参考下面的研究论文中的<a class="ae ky" href="https://arxiv.org/abs/2205.05198" rel="noopener ugc nofollow" target="_blank">。</a></p></blockquote></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="b208" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">结论</h1><p id="6d8f" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">序列并行和选择性激活重新计算是加速大型变压器模型训练的两种新颖而简单的方法。这两种技术具有相似的内存节省，并且有望将训练所需的内存减少大约5倍。因此，训练大型语言模型的成本和时间显著减少。</p><p id="5322" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">话虽如此，它要被大众广泛采用还有很长的路要走。希望人工智能社区将继续在现有研究的基础上创新，使每个人都有可能访问和部署大型语言模型。</p><p id="f619" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢你阅读这篇文章。祝你有美好的一天！</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="bc3c" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">参考</h1><ol class=""><li id="44c5" class="lv lw it lb b lc np lf nq li ou lm ov lq ow lu pb mb mc md bi translated"><a class="ae ky" href="https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/" rel="noopener ugc nofollow" target="_blank">使用OPT-175B民主化访问大规模语言模型</a></li><li id="0581" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu pb mb mc md bi translated"><a class="ae ky" href="https://bigscience.huggingface.co/" rel="noopener ugc nofollow" target="_blank">推出全球最大的开放多语言模型——BLOOM</a></li><li id="6bcd" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu pb mb mc md bi translated"><a class="ae ky" href="https://developer.nvidia.com/blog/nvidia-ai-platform-delivers-big-gains-for-large-language-models/" rel="noopener ugc nofollow" target="_blank"> NVIDIA AI平台为大型语言模型带来巨大收益</a></li></ol></div></div>    
</body>
</html>