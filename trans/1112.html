<html>
<head>
<title>Constrained Beam Search with 🤗 Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">约束波束搜索🤗变形金刚(电影名)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/new-hugging-face-feature-constrained-beam-search-with-transformers-7ebcfc2d70e9#2022-03-22">https://towardsdatascience.com/new-hugging-face-feature-constrained-beam-search-with-transformers-7ebcfc2d70e9#2022-03-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f408" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个新的拥抱脸特性允许你定制和引导你的语言模型输出(比如在输出中强制一个特定的序列)。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/45d5aa1915e919027904ce69e936c6d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1F4C5ZJMaxrHskH3ra4vdw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由 Priscilla Du Preez 在<a class="ae ky" href="https://unsplash.com/s/photos/hugging?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄|一个新的“拥抱”面部特征！</p></figure><p id="1b08" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">抱脸变形金刚</a>有了新功能！它被称为<strong class="lb iu"> <em class="lv">约束光束搜索</em> </strong>，它允许我们引导文本生成过程，这在以前是完全独立于模型的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><h1 id="4396" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">介绍</h1><p id="6abc" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">有时我们确切地知道在文本生成输出中我们想要什么。</p><p id="371e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，在神经机器翻译任务中，我们可能通过字典查找知道哪些单词必须包含在最终翻译中。有时，由于特定的上下文，对于语言模型来说几乎同样可能的生成输出对于最终用户来说可能并不同样理想。</p><p id="1aec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两种情况都可以通过允许用户告诉模型哪些单词必须包含在最终输出中来解决。新的约束波束搜索特性和新的<code class="fe mv mw mx my b">model.generate()</code>函数的<code class="fe mv mw mx my b">force_words_ids</code>参数允许我们这样做！</p><h1 id="c428" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">示例 1:强制单词</h1><p id="a9d5" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">假设我们试图将<code class="fe mv mw mx my b">"How old are you?"</code>翻译成德语。</p><p id="8e4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mv mw mx my b">"Wie alt bist du?"</code>是你在非正式场合说的话，<code class="fe mv mw mx my b">"Wie alt sind Sie?"</code>是你在正式场合说的话。</p><p id="2981" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据上下文的不同，我们可能希望一种形式比另一种形式更正式，但是我们如何告诉模型呢？</p><h1 id="bd44" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">传统波束搜索</h1><p id="26f4" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">下面是我们如何在<em class="lv">传统光束搜索设置中进行文本翻译。</em></p><p id="f4e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们首先安装变形金刚库:</p><pre class="kj kk kl km gt mz my na nb aw nc bi"><span id="1df9" class="nd lz it my b gy ne nf l ng nh">!pip install -q git+https://github.com/huggingface/transformers.git</span></pre><p id="1c6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是教科书 huggingface 代码，用于使用文本生成来完成像 NMT 这样的任务，它是通过传统的波束搜索实现的:</p><pre class="kj kk kl km gt mz my na nb aw nc bi"><span id="0b1a" class="nd lz it my b gy ne nf l ng nh">from transformers import AutoTokenizer, AutoModelForSeq2SeqLM</span><span id="d85d" class="nd lz it my b gy ni nf l ng nh">tokenizer = AutoTokenizer.from_pretrained("t5-base")<br/>model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")</span><span id="a666" class="nd lz it my b gy ni nf l ng nh">encoder_input_str = "translate English to German: How old are you?"</span><span id="f13c" class="nd lz it my b gy ni nf l ng nh">input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids</span><span id="80a0" class="nd lz it my b gy ni nf l ng nh">outputs = model.generate(<br/>    input_ids,<br/>    num_beams=10,<br/>    num_return_sequences=1,<br/>    no_repeat_ngram_size=1,<br/>    remove_invalid_values=True,<br/>)</span><span id="453b" class="nd lz it my b gy ni nf l ng nh">print("Output:\n" + 100 * '-')<br/>print(tokenizer.decode(outputs[0], skip_special_tokens=True))</span><span id="ce27" class="nd lz it my b gy ni nf l ng nh">Output:<br/>--------------------------------------------------------------------<br/>Wie alt bist du?</span></pre><h1 id="2df1" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">使用约束波束搜索</h1><p id="b1e4" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">但是如果我们知道我们想要一个正式的输出，而不是非正式的输出呢？如果我们从先前的知识中知道这一代人必须包括什么，并且我们可以<em class="lv">将它</em>注入到这一代人中，会怎么样？</p><p id="7548" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是使用<code class="fe mv mw mx my b">force_words_ids</code>关键字参数到<code class="fe mv mw mx my b">model.generate()</code>的可能结果:</p><pre class="kj kk kl km gt mz my na nb aw nc bi"><span id="525f" class="nd lz it my b gy ne nf l ng nh">tokenizer = AutoTokenizer.from_pretrained("t5-base")<br/>model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")</span><span id="e3a4" class="nd lz it my b gy ni nf l ng nh">encoder_input_str = "translate English to German: How old are you?"</span><span id="3b80" class="nd lz it my b gy ni nf l ng nh">force_words = ["Sie"]</span><span id="a0f4" class="nd lz it my b gy ni nf l ng nh">input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids<br/>force_words_ids = tokenizer(force_words, add_special_tokens=False).input_ids</span><span id="d3f5" class="nd lz it my b gy ni nf l ng nh">outputs = model.generate(<br/>    input_ids,<br/>    force_words_ids=force_words_ids,<br/>    num_beams=5,<br/>    num_return_sequences=1,<br/>    no_repeat_ngram_size=1,<br/>    remove_invalid_values=True,<br/>)</span><span id="23f0" class="nd lz it my b gy ni nf l ng nh">print("Output:\n" + 100 * '-')<br/>print(tokenizer.decode(outputs[0], skip_special_tokens=True))</span><span id="69e8" class="nd lz it my b gy ni nf l ng nh">Output:<br/>--------------------------------------------------------------------<br/>Wie alt sind Sie?</span></pre><p id="7022" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，我们能够利用关于我们期望输出的先验知识来引导这一代人。以前，我们必须生成一堆可能的输出，然后过滤出符合我们要求的输出。现在我们可以在生成阶段做到这一点。</p><h1 id="dab2" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">示例 2:析取约束</h1><p id="3e56" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">我们上面提到了一个用例，其中我们知道我们想要在最终输出中包含哪些单词。这方面的一个例子可能是在神经机器翻译期间使用字典查找。</p><p id="2347" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，如果我们不知道使用哪种<em class="lv">单词形式</em>，而我们希望像<code class="fe mv mw mx my b">["raining", "rained", "rains", ...]</code>这样的输出同样可能，那该怎么办呢？从更一般的意义上来说，总会有这样的情况，我们不希望<em class="lv">一字不差地</em>一个字母一个字母地精确表达，也可以考虑其他相关的可能性。</p><p id="c678" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">允许这种行为的约束是<em class="lv">析取约束</em>，它允许用户输入一个单词列表，其目的是指导生成，使得最终输出必须只包含<em class="lv">单词列表中的至少一个</em>。</p><p id="aa93" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是一个混合使用上述两种约束的示例:</p><pre class="kj kk kl km gt mz my na nb aw nc bi"><span id="3d99" class="nd lz it my b gy ne nf l ng nh">from transformers import GPT2LMHeadModel, GPT2Tokenizer</span><span id="b03d" class="nd lz it my b gy ni nf l ng nh">model = GPT2LMHeadModel.from_pretrained("gpt2")<br/>tokenizer = GPT2Tokenizer.from_pretrained("gpt2")</span><span id="b71c" class="nd lz it my b gy ni nf l ng nh">force_word = "scared"<br/>force_flexible = ["scream", "screams", "screaming", "screamed"]</span><span id="d8d9" class="nd lz it my b gy ni nf l ng nh">force_words_ids = [<br/>    tokenizer([force_word], add_prefix_space=True, add_special_tokens=False).input_ids,<br/>    tokenizer(force_flexible, add_prefix_space=True, add_special_tokens=False).input_ids,<br/>]</span><span id="bdb2" class="nd lz it my b gy ni nf l ng nh">starting_text = ["The soldiers", "The child"]</span><span id="5a86" class="nd lz it my b gy ni nf l ng nh">input_ids = tokenizer(starting_text, return_tensors="pt").input_ids</span><span id="b9fc" class="nd lz it my b gy ni nf l ng nh">outputs = model.generate(<br/>    input_ids,<br/>    force_words_ids=force_words_ids,<br/>    num_beams=10,<br/>    num_return_sequences=1,<br/>    no_repeat_ngram_size=1,<br/>    remove_invalid_values=True,<br/>)</span><span id="a0a1" class="nd lz it my b gy ni nf l ng nh">print("Output:\n" + 100 * '-')<br/>print(tokenizer.decode(outputs[0], skip_special_tokens=True))<br/>print(tokenizer.decode(outputs[1], skip_special_tokens=True))</span><span id="65a5" class="nd lz it my b gy ni nf l ng nh">Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.</span><span id="1409" class="nd lz it my b gy ni nf l ng nh">Output:<br/>--------------------------------------------------------------------<br/>The soldiers, who were all scared and screaming at each other as they tried to get out of the</span><span id="6969" class="nd lz it my b gy ni nf l ng nh">The child was taken to a local hospital where she screamed and scared for her life, police said.</span></pre><p id="9731" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你所看到的，第一个输出使用了<code class="fe mv mw mx my b">"screaming"</code>，第二个输出使用了<code class="fe mv mw mx my b">"screamed"</code>，两个输出都一字不差地使用了<code class="fe mv mw mx my b">"scared"</code>。从<code class="fe mv mw mx my b">["screaming", "screamed", ...]</code>中选择的列表不一定是单词形式；这可以满足我们只需要一个单词列表的任何用例。</p><h1 id="53b4" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">为什么很难</h1><p id="e1e9" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">但是，这其实是一个非常不小的问题。这是因为任务要求我们在最终输出中的某个地方强制生成某些子序列<em class="lv">，在生成过程中的某个点<em class="lv"/>。</em></p><p id="5510" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">问题是波束搜索逐令牌生成序列<em class="lv"/>。这就产生了以下问题:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/7db64a76bf0978dfc65707f82684a1d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vJof-Nq_eFX39dPjlBdNjQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="87bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您有不同需求的多个约束，该怎么办呢？如果你想强制短语<code class="fe mv mw mx my b">P1</code> <em class="lv">和</em>也是短语<code class="fe mv mw mx my b">P2</code>呢？如果您希望模型在这两个短语之间进行选择，该怎么办？如果我们想强制使用短语<code class="fe mv mw mx my b">P1</code>，并且只强制使用短语列表<code class="fe mv mw mx my b">[P21, P22, P23]</code>中的一个短语，该怎么办？</p><p id="fd65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是我们已经看到，上面的代码示例演示了上述所有情况的可能性，不管它们看起来有多么做作。接下来的部分将解释这一切是如何工作的。</p><h1 id="0bac" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">传统波束搜索</h1><p id="464e" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">以下是传统波束搜索的一个例子，摘自之前的<a class="ae ky" href="https://huggingface.co/blog/how-to-generate" rel="noopener ugc nofollow" target="_blank">博文</a>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/d65b60fd97a744d1f0324afa08a134a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*NENsiNkKBCh_dT_Y.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自传统波束搜索上的<a class="ae ky" href="https://huggingface.co/blog/how-to-generate" rel="noopener ugc nofollow" target="_blank">拥抱脸贴子</a></p></figure><p id="f67b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与贪婪搜索不同，波束搜索通过保持一个更长的假设列表来工作。在上图中，我们在生成的每个可能步骤中显示了三个下一个可能的令牌。</p><p id="d232" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<code class="fe mv mw mx my b">num_beams=3</code>的情况下，这里有另一种方式来看上面例子的波束搜索的第一步:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/7ee570c38a0fc72781c747e821b4f0a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*h8Wn2aNdmDt7pllO.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="a704" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">波束搜索将允许<em class="lv">进一步考虑<code class="fe mv mw mx my b">"The nice"</code>和<code class="fe mv mw mx my b">"The car"</code>中的</em>，而不是像贪婪搜索那样只选择<code class="fe mv mw mx my b">"The dog"</code>。</p><p id="63dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下一步中，我们考虑我们在上一步中创建的三个分支的下一个可能的令牌。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/ec25e7cb37a727c7e7e9c57d0aac8a31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jxJbnpGpn89LkCzS.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="6585" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管我们最终<em class="lv">认为</em>比<code class="fe mv mw mx my b">num_beams</code>输出多得多，但在这一步结束时，我们将它们减少到<code class="fe mv mw mx my b">num_beams</code>。我们不能一直分支下去，那么我们必须跟踪的<code class="fe mv mw mx my b">beams</code>的数量将会变得难以控制的长(10 步之后的 10 个波束将会是 100 亿个波束！).</p><p id="bd24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于生成的其余部分，我们重复上述步骤，直到满足结束标准，例如生成<code class="fe mv mw mx my b">&lt;eos&gt;</code>令牌或到达<code class="fe mv mw mx my b">max_length</code>。分支、排序、减少和重复。</p><h1 id="c333" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">约束波束搜索</h1><p id="32b9" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">约束波束搜索试图通过<em class="lv">在生成的每一步注入</em>期望的令牌来满足约束。</p><p id="cb74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们试图在生成输出中强制使用短语<code class="fe mv mw mx my b">"is fast"</code>。</p><p id="30f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在传统的射束搜索设置中，我们在每个分支找到顶部<code class="fe mv mw mx my b">k</code>最有可能的下一个标记，并附加它们以供考虑。在受约束的设置中，我们做了同样的事情，但是也添加了标记，这些标记将带我们<em class="lv">更接近满足我们的约束</em>。这里有一个演示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/82ef7f1cdd717bb33dee602a1090d85b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dQ4i1VkU5iHPAHnp.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="4de7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了常见的高概率 next 令牌(如<code class="fe mv mw mx my b">"dog"</code>和<code class="fe mv mw mx my b">"nice"</code>)之外，我们还强制使用令牌<code class="fe mv mw mx my b">"is"</code>，以便更接近满足约束<code class="fe mv mw mx my b">"is fast"</code>。</p><p id="3f50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于下一步，下面的分支候选项与传统的波束搜索基本相同。但是和上面的例子一样，受约束的波束搜索通过在每个新的分支施加约束来增加现有的候选:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/ff9447033486f2fbf869331e7e40150e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GKUVznfIOOkrPnLx.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="0a58" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">银行</h1><p id="4862" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">在我们谈论下一步之前，我们需要考虑在上面的步骤中我们可以看到的不良行为。</p><p id="d5c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">天真地在输出中强制输入想要的短语<code class="fe mv mw mx my b">"is fast"</code>的问题是，大多数时候，你会得到像上面的<code class="fe mv mw mx my b">"The is fast"</code>这样无意义的输出。这实际上使得这个问题很难解决。在<code class="fe mv mw mx my b">huggingface/transformers</code>提出的<a class="ae ky" href="https://github.com/huggingface/transformers/issues/14081#issuecomment-1004479944" rel="noopener ugc nofollow" target="_blank">原始特性请求问题</a>中可以找到关于解决这个问题的复杂性的更深入的讨论。</p><p id="9461" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">银行通过在满足约束和创造合理产出之间建立平衡来解决这个问题。</p><p id="57c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第 n 组<em class="lv"> n </em>指的是在满足约束方面取得了 n <em class="lv"> n 步进展的<em class="lv">梁列表。在将所有可能的波束分类到它们各自的组中之后，我们进行循环选择。在上面的例子中，我们将从库 2 中选择最可能的输出，然后从库 1 中选择最可能的输出，从库 0 中选择一个输出，从库 2 中选择第二个最可能的输出，从库 1 中选择第二个最可能的输出，依此类推。由于我们使用的是<code class="fe mv mw mx my b">num_beams=3</code>，我们只需重复上述过程三次，就可以得到<code class="fe mv mw mx my b">["The is fast", "The dog is", "The dog and"]</code>。</em></em></p><p id="ae8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这样，即使我们<em class="lv">强迫</em>模型考虑我们已经手动附加了所需标记的分支，我们仍然跟踪其他可能更有意义的高概率序列。尽管<code class="fe mv mw mx my b">"The is fast"</code>完全满足了我们的约束，但它并不是一个非常明智的短语。幸运的是，我们在未来的步骤中有<code class="fe mv mw mx my b">"The dog is"</code>和<code class="fe mv mw mx my b">"The dog and"</code>可以使用，这有望在以后产生更合理的输出。</p><p id="501c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上述示例的第三步演示了这种行为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/f5c0a8a152564390691a33c5db7e76ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iZoAFukoCVZyIFjy.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="d73b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意<code class="fe mv mw mx my b">"The is fast"</code>不需要手动添加任何约束标记，因为它已经完成了(即，已经包含了短语<code class="fe mv mw mx my b">"is fast"</code>)。此外，请注意像<code class="fe mv mw mx my b">"The dog is slow"</code>或<code class="fe mv mw mx my b">"The dog is mad"</code>这样的光束实际上是在存储体 0 中，因为，尽管它包括令牌<code class="fe mv mw mx my b">"is"</code>，它必须从头开始重新生成<code class="fe mv mw mx my b">"is fast"</code>。通过在<code class="fe mv mw mx my b">"is"</code>后添加类似于<code class="fe mv mw mx my b">"slow"</code>的东西，它有效地<em class="lv">重置了它的进程</em>。</p><p id="aa40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，请注意我们是如何得到包含约束短语的合理输出的:<code class="fe mv mw mx my b">"The dog is fast"</code>！</p><p id="02c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们最初很担心，因为盲目地添加想要的标记会导致像<code class="fe mv mw mx my b">"The is fast"</code>这样无意义的短语。然而，使用来自银行的循环选择，我们最终隐含地放弃了无意义的输出，而选择了更有意义的输出。</p><h1 id="2a58" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">关于<code class="fe mv mw mx my b">Constraint</code>类和自定义约束的更多信息</h1><p id="85b7" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">解释的主要内容可以总结如下。在每一步，我们都不断地纠缠模型来考虑满足我们的约束的记号，同时跟踪不满足我们的约束的波束，直到我们以合理的高概率结束包含我们想要的短语的序列。</p><p id="15d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，设计这个实现的一个原则方法是将每个约束表示为一个<code class="fe mv mw mx my b">Constraint</code>对象，其目的是跟踪其进度，并告诉波束搜索接下来要生成哪些令牌。尽管我们已经为<code class="fe mv mw mx my b">model.generate()</code>提供了关键字参数<code class="fe mv mw mx my b">force_words_ids</code>，但以下是后端实际发生的情况:</p><pre class="kj kk kl km gt mz my na nb aw nc bi"><span id="ca89" class="nd lz it my b gy ne nf l ng nh">from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, PhrasalConstraint</span><span id="d107" class="nd lz it my b gy ni nf l ng nh">tokenizer = AutoTokenizer.from_pretrained("t5-base")<br/>model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")</span><span id="ccb8" class="nd lz it my b gy ni nf l ng nh">encoder_input_str = "translate English to German: How old are you?"</span><span id="0ba1" class="nd lz it my b gy ni nf l ng nh">constraints = [<br/>    PhrasalConstraint(<br/>        tokenizer("Sie", add_special_tokens=False).input_ids<br/>    )<br/>]</span><span id="0743" class="nd lz it my b gy ni nf l ng nh">input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids<br/></span><span id="015f" class="nd lz it my b gy ni nf l ng nh">outputs = model.generate(<br/>    input_ids,<br/>    constraints=constraints,<br/>    num_beams=10,<br/>    num_return_sequences=1,<br/>    no_repeat_ngram_size=1,<br/>    remove_invalid_values=True,<br/>)<br/></span><span id="ed05" class="nd lz it my b gy ni nf l ng nh">print("Output:\n" + 100 * '-')<br/>print(tokenizer.decode(outputs[0], skip_special_tokens=True))</span><span id="bdc8" class="nd lz it my b gy ni nf l ng nh">Output:<br/>--------------------------------------------------------------------<br/>Wie alt sind Sie?</span></pre><p id="069e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以自己定义一个，并将其输入到<code class="fe mv mw mx my b">constraints</code>关键字参数中，以设计您的独特约束。你只需要创建一个<code class="fe mv mw mx my b">Constraint</code>抽象接口类的子类，并遵循它的要求。你可以在<code class="fe mv mw mx my b">Constraint</code>的定义中找到更多信息，找到<a class="ae ky" href="https://github.com/huggingface/transformers/blob/master/src/transformers/generation_beam_constraints.py" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="de7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一些独特的想法(尚未实施；也许你可以试一试！)包括类似于<code class="fe mv mw mx my b">OrderedConstraints</code>、<code class="fe mv mw mx my b">TemplateConstraints</code>的约束，这些约束可以进一步添加。目前，生成是通过在输出中的任何地方包含序列来完成的。例如，在前面的例子中，一个序列包含惊吓- &gt;尖叫，另一个包含尖叫- &gt;惊吓。<code class="fe mv mw mx my b">OrderedConstraints</code>允许用户指定满足这些约束的顺序。</p><p id="2656" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mv mw mx my b">TemplateConstraints</code>可以考虑更适合的功能使用，目标可以是:</p><pre class="kj kk kl km gt mz my na nb aw nc bi"><span id="952e" class="nd lz it my b gy ne nf l ng nh">starting_text = "The woman"<br/>template = ["the", "", "School of", "", "in"]</span><span id="3656" class="nd lz it my b gy ni nf l ng nh">possible_outputs == [<br/>   "The woman attended the Ross School of Business in Michigan.",<br/>   "The woman was the administrator for the Harvard school of Business in MA."<br/>]</span></pre><p id="751a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者:</p><pre class="kj kk kl km gt mz my na nb aw nc bi"><span id="aa05" class="nd lz it my b gy ne nf l ng nh">starting_text = "The woman"<br/>template = ["the", "", "", "University", "", "in"]</span><span id="a9dd" class="nd lz it my b gy ni nf l ng nh">possible_outputs == [<br/>   "The woman attended the Carnegie Mellon University in Pittsburgh.",<br/>]<br/>impossible_outputs == [<br/>  "The woman attended the Harvard University in MA."<br/>]</span></pre><p id="47b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者，如果用户不关心两个单词之间的标记数量，那么可以只使用<code class="fe mv mw mx my b">OrderedConstraint</code>。</p><h1 id="ca3c" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">结论</h1><p id="0756" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">你可以在这里找到更详细的官方博客文章<a class="ae ky" href="https://huggingface.co/blog/constrained-beam-search" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="8b6c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">约束波束搜索为我们提供了一种灵活的方法，将外部知识和需求注入到文本生成中。以前，没有简单的方法来将模型识别为 1。包括序列列表，其中 2。其中一些是可选的，一些不是，例如 3。它们在序列中的某处<em class="lv">生成</em>在各自合理的位置。现在，我们可以通过混合<code class="fe mv mw mx my b">Constraint</code>对象的不同子类来完全控制我们这一代！</p><p id="5f4f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一新功能主要基于以下论文:</p><ul class=""><li id="4ef1" class="nq nr it lb b lc ld lf lg li ns lm nt lq nu lu nv nw nx ny bi translated"><a class="ae ky" href="https://arxiv.org/abs/1612.00576" rel="noopener ugc nofollow" target="_blank">带约束波束搜索的引导式开放词汇图像字幕</a></li><li id="050e" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated"><a class="ae ky" href="https://arxiv.org/abs/1804.06609" rel="noopener ugc nofollow" target="_blank">用于神经机器翻译的具有动态波束分配的快速词汇约束解码</a></li><li id="21ba" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated"><a class="ae ky" href="https://aclanthology.org/N19-1090/" rel="noopener ugc nofollow" target="_blank">改进翻译和单语重写的词汇约束解码</a></li><li id="401c" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated"><a class="ae ky" href="https://arxiv.org/pdf/2107.09846.pdf" rel="noopener ugc nofollow" target="_blank">因果引导生成</a></li></ul><p id="8383" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">和上面的那些一样，许多新的研究论文正在探索使用外部知识(例如，KGs，KBs)来指导大型深度学习模型的输出的方法。希望这种约束波束搜索特性成为实现这一目的的另一种有效方式。</p></div></div>    
</body>
</html>