<html>
<head>
<title>3 (Bad) Reasons ML Teams avoid Building Fair Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML团队避免构建公平模型的3个(坏)原因</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/3-bad-reasons-ml-teams-avoid-building-fair-models-d8f85a1edc23#2022-02-01">https://towardsdatascience.com/3-bad-reasons-ml-teams-avoid-building-fair-models-d8f85a1edc23#2022-02-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2c91" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">浅谈机器学习模型中的社会公平</h2></div><p id="ee35" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ML有公关问题。</p><p id="8f7c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管我们对更智能、更细致、更强大的移动营销的能力突飞猛进，产生了巨大的积极社会价值，但消费者对移动营销的担忧并没有消退。</p><p id="6d20" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">事实上，它似乎在增长。</strong></p><p id="60e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种趋势对于任何公司、团队或专业人士(像我一样)来说都是令人担忧的，他们的主要功能是为消费者应用构建ML。</p><p id="1c15" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我认为这种趋势正在增长的原因是因为<strong class="kk iu"> ML团队在构建模型的时候主动避免了“公平”</strong>。我将讲述我见过的团队避免这种情况的3个主要原因，以及当我们进一步进入2020年代时，为什么这些原因不足以忽视公平。</p><h1 id="1e8c" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">现状</h1><p id="c805" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">解决ML应用中的“公平”是例外而不是规则。几乎所有的ML应用程序都是在没有充分考虑“公平”概念的情况下推出的。我认为这有三个原因:</p><h2 id="d791" class="mb lf it bd lg mc md dn lk me mf dp lo kr mg mh lq kv mi mj ls kz mk ml lu mm bi translated">(1)定义不清:公平对于机器学习意味着什么？</h2><p id="90b7" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">在机器学习之外，判断一件事是否“公平”是一项困难的工作。添加数学、统计和B]你的团队通常只理解的缺乏盒子模型—</p><p id="da38" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mn">什么是地面真理？我们测试哪些群体/身份？模型性能的差异有多大值得偏见？我们能确定这不会与其他真正提升性能的特性混淆吗？</em></p><p id="d72f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">—使得这项任务几乎不可能完成。一个团队如何测试社会偏见？</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mo"><img src="../Images/b90da5841d32f3bf648a140e2014f879.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*heL9ByagT1MYE6JP"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">硅谷的幕后人员发现讨论公平是徒劳的。<a class="ae ne" href="https://unsplash.com/@cherrydeck?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">cherydeck</a>在<a class="ae ne" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="fbbe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="mn">左右目前ML队伍争… </em> </strong></p><p id="c16c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于定义和量化最大似然公平是一项棘手的任务，ata的科学家们通常会放弃试图在他们的模型中明确解决社会偏见的想法。</p><p id="7150" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于数据科学中难以定义/量化的指标，这是一种常见的反应，通常也是一种好的反应。另一种选择是尝试将一个模糊的术语翻译成一个可量化的度量标准，<a class="ae ne" rel="noopener" target="_blank" href="/ml-product-management-has-a-translation-problem-65e87df655b1">这对于ML项目来说常常以灾难告终</a>。</p><p id="3ecd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然公平肯定是一个模糊的概念，但有一些清晰的、几乎普遍接受的定义:</p><blockquote class="nf ng nh"><p id="2fb9" class="ki kj mn kk b kl km ju kn ko kp jx kq ni ks kt ku nj kw kx ky nk la lb lc ld im bi translated"><strong class="kk iu"> ML公平性:</strong>如果结果独立于给定变量，特别是那些被认为敏感的变量，例如不应与结果相关的个体特征(即<a class="ae ne" href="https://en.wikipedia.org/wiki/Gender" rel="noopener ugc nofollow" target="_blank">性别</a>、<a class="ae ne" href="https://en.wikipedia.org/wiki/Ethnicity" rel="noopener ugc nofollow" target="_blank">种族</a>、<a class="ae ne" href="https://en.wikipedia.org/wiki/Sexual_orientation" rel="noopener ugc nofollow" target="_blank">性取向</a>、<a class="ae ne" href="https://en.wikipedia.org/wiki/Disability" rel="noopener ugc nofollow" target="_blank">残疾</a>等)，则给定的ML模型是公平的。).[ <a class="ae ne" href="https://en.wikipedia.org/wiki/Fairness_(machine_learning)" rel="noopener ugc nofollow" target="_blank">链接</a></p></blockquote><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nl"><img src="../Images/3c6d985fb1b06b6a71d3c8a4dfd0944a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cE9LUPzqab-_OjF_"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">定义大联盟公平并不像大联盟团队想象的那么难</p></figure><p id="0c97" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在模型开发阶段，上述定义可以很容易地转化为几个ML测试用例。</p><p id="7481" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，一个团队可以通过简单的皮尔逊测试来检查模型的预测与性别的相关程度:</p><pre class="mp mq mr ms gt nm nn no np aw nq bi"><span id="238a" class="mb lf it nn b gy nr ns l nt nu">import numpy as np<br/>from scipy.stats import pearsonr</span><span id="5dbd" class="mb lf it nn b gy nv ns l nt nu">p = pearsonr(gender_values, labels)</span><span id="951f" class="mb lf it nn b gy nv ns l nt nu"># If there is modest correlation between label and gender,</span><span id="047f" class="mb lf it nn b gy nv ns l nt nu"># we should attempt to mitigate this</span><span id="b447" class="mb lf it nn b gy nv ns l nt nu">if np.abs(p) &gt;= 0.5:<br/>   print("Potential Gender Bias")</span></pre><p id="32bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">或者使用模型的特征重要性分数+与年龄相关的特征，以确保模型不会隐含地发现有偏差的信号:</p><pre class="mp mq mr ms gt nm nn no np aw nq bi"><span id="1838" class="mb lf it nn b gy nr ns l nt nu"># Using SKLearn Tree model + panda dataset<br/>import pandas as pd<br/>import numpy as np<br/>from <!-- -->sklearn.ensemble import <!-- -->GradientBoostingRegressor</span><span id="45db" class="mb lf it nn b gy nv ns l nt nu">tree_model = GradientBoostingRegressor()<br/>tree_model.fit(X_train, y_train)</span><span id="726a" class="mb lf it nn b gy nv ns l nt nu">top_ten_imp_thresh = sorted(tree_model.<!-- -->feature_importances_)[-10]</span><span id="2045" class="mb lf it nn b gy nv ns l nt nu">for i in range(len(X.columns)):</span><span id="c05f" class="mb lf it nn b gy nv ns l nt nu">    col = X.columns[i]</span><span id="32b2" class="mb lf it nn b gy nv ns l nt nu">    imp_score = tree_model.<!-- -->feature_importances_[i]<br/>    fair_score = <!-- -->pearsonr(X[col].values, bias_labels)</span><span id="640e" class="mb lf it nn b gy nv ns l nt nu">    # Check each column for importance + bias<br/>    print(col)</span><span id="77a1" class="mb lf it nn b gy nv ns l nt nu">    if np.abs(<!-- -->fair_score<!-- -->) &gt;= 0.5:<br/>       print("Potential Bias")<br/>    if imp_score &gt;= top_ten_imp_thresh:<br/>       print("Important Feature")<br/>    print()</span></pre><p id="e35f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些当然是不可思议的简单例子，但即使是简单的方法也能给人启发。</p><p id="6e5f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您永远无法真正知道您是否已经从您的模型中消除了所有偏见，但是简单地尝试解决它将确保社区发布的模型对我们的最终用户更加公平。</p><h2 id="71ee" class="mb lf it bd lg mc md dn lk me mf dp lo kr mg mh lq kv mi mj ls kz mk ml lu mm bi translated">(2)默许:垃圾进，垃圾出</h2><p id="7772" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">解决公平性的另一个反对意见是将对公平性缓解的控制从建模团队移交给底层数据。短语:</p><blockquote class="nf ng nh"><p id="f3c5" class="ki kj mn kk b kl km ju kn ko kp jx kq ni ks kt ku nj kw kx ky nk la lb lc ld im bi translated">垃圾进，垃圾出</p></blockquote><p id="8736" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">是DS社区的主要内容。而在工业界，这是绝对正确的。我们把大部分时间花在清理数据上，而不是创造任何东西。数据科学家可以构建模型来发现数据中的复杂模式…但是如果数据不够干净，无法发现这些模式，我们就无能为力了！</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nw"><img src="../Images/ce9383b132e385d522651232718108b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LObsWvrsRwOwU-rE"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">有时候，数据科学感觉就像在垃圾堆里找到金子。埃文·德米科利在<a class="ae ne" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="4eef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将这与ML公平性联系起来，如果数据本身包含模型拾取的有偏见的模式，我们就什么也做不了。或者我们甚至没有数据来检验我们的模型是否有偏差。</p><p id="0a7a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">简而言之，有两个与数据相关的异议:</p><ol class=""><li id="f7a6" class="nx ny it kk b kl km ko kp kr nz kv oa kz ob ld oc od oe of bi translated"><strong class="kk iu">我们没有足够或正确的数据来验证我们的模型是否公平</strong></li><li id="ac29" class="nx ny it kk b kl og ko oh kr oi kv oj kz ok ld oc od oe of bi translated"><strong class="kk iu">算法偏差是由于我们的数据和社会固有的偏差</strong></li></ol><p id="289a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然两个不同的论点，都含蓄地得出结论ML公平是“不可能的”给定的情况。</p><p id="aa1c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> To (#1) </strong>，如果你有足够的数据来验证你的模型是准确的，你就有足够的数据来验证你的模型是公平的。如果您的应用程序是面向客户的，您绝对有足够的客户信息来围绕算法偏差构建测试。</p><p id="ac56" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">即使没有明确的人口统计/身份信息，散列的电子邮件或邮政编码也可以为您的团队提供足够的果汁来解决ML公平性问题。你甚至可以使用像IBM的fairness 360 这样的开源工具来完成这个任务。</p><div class="ol om gp gr on oo"><a href="http://aif360.mybluemix.net/" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">AI公平360</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">这个可扩展的开源工具包可以帮助你检查，报告，并减轻歧视和偏见的机器…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">aif360.mybluemix.net</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc my oo"/></div></div></a></div><p id="b221" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> To (#2) </strong>，<strong class="kk iu"> <em class="mn">仅仅因为你的数据/社会中存在偏见，并不意味着你注定要延续这种偏见。</em> </strong></p><p id="b0c9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对算法偏见的测试肯定会揭示内在的数据/社会偏见，但一旦你意识到这些问题，你就可以解决它们，就像你会解决任何其他非公平相关的偏见一样，如不平衡的班级。</p><h2 id="9f4d" class="mb lf it bd lg mc md dn lk me mf dp lo kr mg mh lq kv mi mj ls kz mk ml lu mm bi translated">(3)金钱万能:为了商业价值而忽视公平</h2><p id="494a" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">在我看来，这是算法社会偏见在模型开发中被忽略的最常见原因，即使大多数数据科学家不会明确地说出来:</p><blockquote class="nf ng nh"><p id="7f54" class="ki kj mn kk b kl km ju kn ko kp jx kq ni ks kt ku nj kw kx ky nk la lb lc ld im bi translated"><strong class="kk iu"> ML团队需要不惜一切代价交付商业价值。如今，这种成本包括忽略社会偏见缓解，以加快模型部署。</strong></p></blockquote><p id="2d5c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ML产品开发仍处于初级阶段。85%的人工智能项目失败。</p><p id="4b19" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">即使有正确的数据、人员和策略，ML项目也只能在20%的时间里实现ROI。</p><p id="b1b7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从公司的角度来看(今天的<em class="mn"/>)，一个ML团队的唯一目标是尽快交付商业价值。为了完成这项任务，我们的工作岌岌可危。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi pd"><img src="../Images/1439ab8b6f6145736d129dad904adafa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WtF-QDuMeAmLd1EA"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">正如气候变化已经进入主流辩论，ML Fairness也是如此，由<a class="ae ne" href="https://unsplash.com/@clemono?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Clem Onojeghuo </a>在<a class="ae ne" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="3824" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就像几代人都忽略了环境后果，因为金钱激励压倒了任何反对意见，有偏见的模型在规模决策时的广泛社会反响被忽略，以确保ML团队的项目属于推动ROI价值的15%的计划。</p><p id="a87c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">而且要快。</p><p id="ecba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">“<em class="mn">快速移动和打破东西”</em>后脸书工程开发的咒语很难转化为ML任务，但特别是使解决像算法社会偏见这样的外部性成为一个额外的不必要的步骤。</p><p id="bce0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如一位DS的知己告诉我的:</p><blockquote class="nf ng nh"><p id="17cf" class="ki kj mn kk b kl km ju kn ko kp jx kq ni ks kt ku nj kw kx ky nk la lb lc ld im bi translated">我们已经有了构建和发布模型的紧迫期限。【他老板】不问公平，我就不考了。很糟糕，但事实就是如此。</p></blockquote><p id="52e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但就像工业公司进入公共话语的环境后果一样，科技公司的社会后果也会如此。它将开始影响底线。</p><h1 id="84d8" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">结论</h1><p id="7703" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">公众对公平ML模型的期望<a class="ae ne" href="https://www.fintechnews.org/report-shows-consumers-dont-trust-artificial-intelligence/" rel="noopener ugc nofollow" target="_blank">已经改变</a>。围绕ML公平发展的现状将需要向紧张的消费者保证，他们可以信任公司如何管理他们的个人数据，并且他们的模型的预测是同样准确和公平的。</p><p id="a8e2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">“人工智能冬天”一词通常用来描述对人工智能研究的兴趣急剧下降的时期。我假设</p><blockquote class="nf ng nh"><p id="b838" class="ki kj mn kk b kl km ju kn ko kp jx kq ni ks kt ku nj kw kx ky nk la lb lc ld im bi translated"><strong class="kk iu">除非ML团队承认并解决消费者对人工智能日益增长的不满，否则我们可能会进入一个奇怪的“消费者人工智能冬天”:</strong></p><p id="96d6" class="ki kj mn kk b kl km ju kn ko kp jx kq ni ks kt ku nj kw kx ky nk la lb lc ld im bi translated"><strong class="kk iu">大多数消费者非常不信任和不喜欢使用ML的产品，因此消费者对ML的应用不再流行。</strong></p></blockquote><p id="4398" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">避免这种命运将涉及添加护栏和功能，以确保我们的模型满足消费级技术所需的期望。</p><p id="2411" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如所讨论的，ML社区能够并且应该解决这些模型的缺点。如果做不到这一点，将会(1)直接导致野生环境中更糟糕的结果，以及(2)从根本上削弱公众对人工智能产品的信任和渴望。</p></div></div>    
</body>
</html>