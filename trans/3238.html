<html>
<head>
<title>Iterated Reweighted Least Squares and GLMs Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">迭代加权最小二乘法和GLMs解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/iterated-reweighted-least-squares-and-glms-explained-9c0cc0063526#2022-07-19">https://towardsdatascience.com/iterated-reweighted-least-squares-and-glms-explained-9c0cc0063526#2022-07-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f136" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用Python实现了详细的实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3b2aca5701db84d39b093ebfc3eda82f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0O7VNtg__LIHgU8QjcR-Rg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://unsplash.com/@tine999" rel="noopener ugc nofollow" target="_blank">蒂内·伊万尼奇</a>在Unsplash上拍摄</p></figure><p id="c472" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">广义线性模型</em> (GLM)是一种回归模型，我们对通常的线性回归模型的线性假设进行广义化。由于这种非线性，估计回归参数不会像估计线性回归参数那样简单。</p><p id="fd77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">迭代加权最小二乘</em> (IRLS)算法，有时也称为<em class="lv">迭代加权最小二乘</em> (IWLS)，是一种找到广义线性模型的最大似然估计的方法。它是加权最小二乘法的扩展。先来一个简短的背景介绍。</p><h1 id="c409" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">背景</h1><p id="292a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在线性模型中，我们可以使用正规方程来估计回归的参数，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/8d6d9aa25c232f5d2e1cbfec174a8aa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SYTKD-0h6bvin0eSAJuo2Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">普通最小二乘估计</p></figure><p id="f062" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法在均值为零和方差不变的情况下会有误差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/128ce6c3964beb608bc5d0306a3fe9d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DLnuWdyA-gps6MKoRUzQVA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线性误差分布</p></figure><p id="7df5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果预测值和被预测值之间的关系不是线性的，那么如果我们坚持使用上面的正规方程，我们将会得到具有非恒定方差的误差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/f88b723e71d55efe2f134b6e25282366.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZBpxd1FQd4lwQsLAN9fQUA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">非线性误差分布</p></figure><p id="6bde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种结果并不理想。为了更好的可预测性，我们希望有恒定的误差方差。通过一些我不会展示的数学操作，我们可以使用加权最小二乘法将误差分布转化为标准的多元正态分布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/e29c6dd9ad8defafee80b9266deaf255.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ezgjC6mpSh15d3XBhpsDqA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">加权最小二乘估计</p></figure><p id="0ebb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于特定的原因，我们仍然不能将它用于GLMs。GLM的y变量不同于预测变量。考虑下面的数据(我们也将用于代码)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="da70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们看到y变量是一个计数变量，表示成功的次数。对于我们的情况，我们将最大成功数保持为1，这是一个逻辑回归问题。二项式(和逻辑)回归不预测实例的成功次数，而是预测成功的概率。y变量和所需预测变量之间的差异是有问题的，因为我们无法将y变量代入加权最小二乘方程。为了解决这个问题，我们引入了IRLS算法。但是在我们跳到算法之前，我需要解释一下GLM的基础知识。</p><h1 id="a053" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">广义线性模型(GLM)</h1><p id="551f" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">顾名思义，GLMs是线性回归的概括，其中预测变量通过字母<code class="fe mz na nb nc b">g</code>表示的链接函数与线性模型相关。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/d75bb3e44660280844ce9ed367da02b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*il08LdPTjUyLR_zkbqsy7A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GLMs</p></figure><p id="d38c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你预测的变量是有限域的，那么链接函数就很方便。例如，如果您想将概率建模为您的响应变量。如你所知，概率的值只能在<em class="lv"> 0 </em>和<em class="lv"> 1 </em>之间。普通的线性回归不能满足这种定义域限制。因此我们引入了链接函数。下面的形式通常更容易理解。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/031bf923a72edb4d626285c494ff3203.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*CI1JFLTGcXs-whNQ9IO7Mg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GLMs</p></figure><p id="6337" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里的<code class="fe mz na nb nc b">μ</code>是要预测的概率。但是，通过适当选择链接函数，您可以使用它来模拟具有任何其他域限制的任何其他变量。</p><p id="1311" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注</strong>。链接函数必须是可微的。</p><p id="d9df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">先说链接功能。对于我们的例子，我们需要找到一个将无限域转换成<code class="fe mz na nb nc b">[0, 1]</code>域的函数。很多函数都有这个属性。我将向您展示这种转换最流行的链接函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/db0441e0ae8f50d4b2c417b7e192545d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CRm2JFq2qA86YR85-x1uXA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Logit函数</p></figure><p id="a12e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的函数就是著名的logit函数。logit link函数适用于预测概率的二元<em class="lv"> y </em>变量。</p><p id="dc44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注</strong>。仅当<em class="lv"> y </em>变量来自指数族分布时，glm才有效。</p><h1 id="c625" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">指数族</h1><p id="1a8b" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">逻辑回归有一个值为1或0的二元<em class="lv"> y </em>变量。我们可以说这个数据来自一个具有非常数概率参数的伯努利分布。这个概率就是我们试图模拟的。因为伯努利或二项式分布来自指数族，所以我们可以通过GLM对此建模。但是这个指数族是什么呢？</p><p id="3b61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果一个分布的密度或质量函数可以用下面的形式来表示，那么这个分布就属于指数族。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/048260f55c01c866c34f366f64b4a9f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0BzcQYctxWLv1MvRoJfbpQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">指数族</p></figure><p id="e1c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将展示如何以这种形式表示二项分布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/63b6270c55517f1ad558e5618fdc4e79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dH7WB3vf2dtRNd4MUA9pkA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">证明二项分布是指数族的一部分</p></figure><p id="af14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有其他方法来表示指数族的参数，因为它们不是唯一的。我花了这么长时间来解释指数族，因为我们需要一个参数的方差函数的概念。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/d0343093bca7ee03a608031bcbce5cfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n6uhRi6R2Z8G9Iw_FKHLZA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">方差函数</p></figure><p id="e9fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我知道这很难找到，所以我将提供一个更简单的方法来推导方差函数。本质上，方差函数是一种用均值表示分布方差的方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/28fd0eb1b6f842b24d97642c2efe8605.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yul32ZJUdKs4MMcqKq9LJw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">方差定义</p></figure><p id="8efe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">二项式分布的均值是<code class="fe mz na nb nc b">np</code>，而方差是<code class="fe mz na nb nc b">np(1-p)</code>。从上面的推导中我们知道<code class="fe mz na nb nc b">a(ϕ) = 1</code>，我可以把我的方差函数表示如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/ac8055f53e524db769fbeca092c8a67c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xCdEPCUTzLty0EDXdV5uYQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">二项分布的方差函数</p></figure><p id="8a6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们用<code class="fe mz na nb nc b">np</code>代替<code class="fe mz na nb nc b">μ</code>，我们将得到方差<code class="fe mz na nb nc b">np(1-p)</code>。假设在正态分布的例子中，均值和方差之间没有直接的联系。在这种情况下，您必须使用公式手动找到方差函数。</p><h1 id="81a8" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">迭代加权最小二乘法</h1><p id="37bb" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们从定义一个“新”回归问题开始，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/ce71650fbb5d96919aaf6c196aab7351.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hOfLjGGzsZju6OvoF4XZPw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式1。新回归</p></figure><p id="a18c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们知道误差的方差，就可以估计参数。误差的方差可通过以下公式获得。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/207f96a5928abc3eca2451114ddcd383.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GC7pg2b-1wdcirpRZi2h0w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式2。误差方差</p></figure><p id="8a79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，对于GLMs，<code class="fe mz na nb nc b">a(ϕ)</code>项可以完全忽略，因为它将在下面的计算中抵消。</p><p id="9678" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从这里，我们可以使用加权最小二乘法来估计参数。这就是<code class="fe mz na nb nc b">a(ϕ)</code>抵消的地方。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/817a78410f4b8973a0d619137e9ff048.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kMaOsiphrp2Lp-FaukpEHg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式3。加权最小二乘法</p></figure><p id="17b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用当前的参数估计，我们找到了<code class="fe mz na nb nc b">μ</code>的新值，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/41d662454bd60235051f548241050a59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*BIlnK4Z0cV2ZNKx87ZrjKw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式4。查找μ</p></figure><p id="9361" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回想一下，我们定义的<code class="fe mz na nb nc b">z</code>和<code class="fe mz na nb nc b">Σ</code>是<code class="fe mz na nb nc b">μ</code>的函数，<code class="fe mz na nb nc b">μ</code>是<code class="fe mz na nb nc b">β</code>的函数，<code class="fe mz na nb nc b">β</code>是<code class="fe mz na nb nc b">z</code>和<code class="fe mz na nb nc b">Σ</code>的函数。我们通过迭代这个递归关系来估计参数。</p><p id="9d22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果对数似然的增加不再显著，则算法停止。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/d9cc180dd34cf3779c50267a3e59c3e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C5U6g3-j5uTgXMmv0c-d0A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式5。逻辑回归的对数似然。</p></figure><p id="e15d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，这个对数似然方程只对逻辑回归有效。对于其他分布，公式将取决于所选的链接函数。</p><p id="8421" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们来编译一下。迭代加权最小二乘算法；</p><ol class=""><li id="3fc6" class="np nq it lb b lc ld lf lg li nr lm ns lq nt lu nu nv nw nx bi translated">在定义的域内初始化<code class="fe mz na nb nc b">μ</code>。我将用一个<code class="fe mz na nb nc b">0.5</code>概率数组进行初始化。</li><li id="f6ac" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated">给定<code class="fe mz na nb nc b">μ</code>的当前值，使用<em class="lv">等式1 </em>和e <em class="lv">等式2 </em>计算<code class="fe mz na nb nc b">z</code>和<code class="fe mz na nb nc b">Σ</code>。</li><li id="b3ff" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated">给定<code class="fe mz na nb nc b">z</code>和<code class="fe mz na nb nc b">Σ</code>的当前值，使用加权最小二乘法公式计算<code class="fe mz na nb nc b">β</code>；e <em class="lv">第三季</em>。</li><li id="4d10" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated">给定<code class="fe mz na nb nc b">β</code>的当前值，使用<em class="lv">等式4计算<code class="fe mz na nb nc b">μ</code>。</em></li><li id="e71f" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated">如果对数似然的增加(<em class="lv">等式5 </em>)小于预先指定的ε，则停止。否则，返回步骤<em class="lv"> 2 </em>。</li></ol><h1 id="631e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">Python代码</h1><p id="86e4" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">让我们把所有的拼图拼在一起。首先是链接功能。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="8548" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们的算法部分，</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="74d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是我的算法代码。我将这些部分与上面概述的步骤相匹配。请尝试匹配上面的每个等式。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="c59b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，参数中的<code class="fe mz na nb nc b">X</code>需要一个截距列。运行下面一行代码，</p><pre class="kj kk kl km gt od nc oe of aw og bi"><span id="70f8" class="oh lx it nc b gy oi oj l ok ol">b = IWLS(X, y)</span></pre><p id="cbc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">会将参数估计值返回给<code class="fe mz na nb nc b">b</code>变量。</p><pre class="kj kk kl km gt od nc oe of aw og bi"><span id="1e57" class="oh lx it nc b gy oi oj l ok ol">IWLS completed on 8 iterations<br/>array([1.10999575, 9.12479957, 2.18745957])</span></pre><p id="6fcf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以利用这些信息来预测成功的可能性。</p><pre class="kj kk kl km gt od nc oe of aw og bi"><span id="d44e" class="oh lx it nc b gy oi oj l ok ol">phat = ig(X @ b)<br/>yhat = np.where(phat &gt;= 0.5, 1, 0)</span></pre><p id="efae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里<code class="fe mz na nb nc b">phat</code>存储概率预测，而<code class="fe mz na nb nc b">yhat</code>存储二项式预测。</p><h1 id="39e4" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">比较</h1><p id="9027" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">据我所知，Python <code class="fe mz na nb nc b">scikit-learn</code>库没有IRLS解算器，所以我们无法将我们的结果与他们的进行比较。但是幸运的是，R <code class="fe mz na nb nc b">faraway</code>软件包使用IRLS作为他们的GLM解算器。我们来对比一下。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="b3e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运转</p><pre class="kj kk kl km gt od nc oe of aw og bi"><span id="0551" class="oh lx it nc b gy oi oj l ok ol">summary(model)</span></pre><p id="9b48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">会回来，</p><pre class="kj kk kl km gt od nc oe of aw og bi"><span id="83a0" class="oh lx it nc b gy oi oj l ok ol">Call:<br/>glm(formula = Success ~ ., family = binomial, data = df)</span><span id="4362" class="oh lx it nc b gy om oj l ok ol">Deviance Residuals: <br/>     Min        1Q    Median        3Q       Max  <br/>-1.72517  -0.12458  -0.00007   0.48620   1.07683</span><span id="f9b6" class="oh lx it nc b gy om oj l ok ol">Coefficients:<br/>            Estimate Std. Error z value Pr(&gt;|z|)<br/>(Intercept)    1.110      1.283   0.865    0.387<br/>Height         9.125      8.489   1.075    0.282<br/>Weight         2.187      2.099   1.042    0.297</span><span id="b35c" class="oh lx it nc b gy om oj l ok ol">(Dispersion parameter for binomial family taken to be 1)</span><span id="b590" class="oh lx it nc b gy om oj l ok ol">Null deviance: 13.8629  on 9  degrees of freedom<br/>Residual deviance:  5.6807  on 7  degrees of freedom<br/>AIC: 11.681</span><span id="f80c" class="oh lx it nc b gy om oj l ok ol">Number of Fisher Scoring iterations: 8</span></pre><p id="ad05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回想一下我们之前的输出，</p><pre class="kj kk kl km gt od nc oe of aw og bi"><span id="1066" class="oh lx it nc b gy oi oj l ok ol">IWLS completed on 8 iterations<br/>array([1.10999575, 9.12479957, 2.18745957])</span></pre><p id="7969" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是精确的参数估计和迭代次数。我不知道他们使用的实际停止条件，但是看起来我们的方法非常符合。</p><h1 id="73d6" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">恭喜</h1><p id="2e3a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">您已成功完成这篇文章。查看我在GitHub 上的完整代码，了解如何正确地预处理数据并在实际应用中使用我们的函数。请在评论中分享你的反馈和想法。感谢您的阅读！</p></div></div>    
</body>
</html>