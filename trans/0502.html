<html>
<head>
<title>Why Reinforcement Learning Doesn’t Need Bellman’s Equation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么强化学习不需要贝尔曼方程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-reinforcement-learning-doesnt-need-bellman-s-equation-c9c2e51a0b7#2022-02-20">https://towardsdatascience.com/why-reinforcement-learning-doesnt-need-bellman-s-equation-c9c2e51a0b7#2022-02-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f4d8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">强化学习中著名的贝尔曼方程和MDP公式的再评价</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cca171261a59299985b961586dcfee86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5RKoB_U5Lr1u6gqI"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">理查德·贝尔曼在圣莫尼卡的兰德公司工作时，创立了动态编程和他著名的递归方程。照片由<a class="ae ky" href="https://unsplash.com/@ironstagram?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Sung Shin </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="6b0f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在学术界，将强化学习(RL)算法与马尔可夫决策过程(MDP)公式和著名的<strong class="li iu">贝尔曼方程</strong>联系起来经常是样板文件。乍一看，这是有意义的，因为我们通常旨在为MDP找到近似最优的策略。然而，在许多方面，RL已经偏离了动态编程的起源如此之远，以至于人们可能会想，在我们的问题定义中，我们是否真的需要贝尔曼方程。</p><p id="de51" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这篇文章并不是对贝尔曼方程本身的批评。正如我们将很快看到的，这是一个非常优雅的公式，在一行数学中捕捉到甚至最混乱的问题。考虑到许多现实世界的组合问题的问题状态比宇宙中的原子还多，这是一个相当大的壮举。</p><p id="f7af" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">当你理解了顺序决策问题的复杂性和重要性，你可能会像我一样欣赏贝尔曼的贡献。然而，这并不意味着我们需要在RL中使用它们。</p><h1 id="3d16" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">什么是动态编程？</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/5c17c2104ca0a766f75b2de504d4613c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JlVR51osqshxOaUy"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@julianmyleshoward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">朱利安·迈尔斯</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="7575" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">动态编程起源于20世纪50年代。当时，理查德·贝尔曼在圣莫尼卡的兰德公司工作。就在那里，坐在一棵棕榈树下——或者我是这么想的——理查德·贝尔曼发明了他著名的方程式。</p><p id="65cc" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">虽然我不会在这里深入解释动态编程，但关键的概念是，跨越一个时间范围的大问题可以分解成一系列嵌套的小问题。这些较小的问题可以递归地解决，产生一个最优的<strong class="li iu">决策策略</strong>，为每个问题状态提供最佳的行动。</p><p id="7818" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">要部署动态规划，我们必须能够将问题定义为MDP，包括(I)状态空间、(ii)动作空间、(iii)转移函数和(iv)奖励函数(可能还有(v)贴现率)的形式化。决策必须只依赖于当前状态——所谓的<strong class="li iu">马尔可夫(或无记忆)性质</strong>。如果条件满足，我们可以(理论上)使用动态规划来解决问题。</p><p id="1540" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">贝尔曼认识到，对于每个状态，可以定义一个<strong class="li iu">价值函数</strong>，捕捉一个行为的直接回报/成本和(贴现的)下游价值。例如，我们可能决定每天储备多少产品，预测概率销售情况。</p><p id="3297" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">考虑一个三态问题。在每个状态下，我们可以采取一个动作<code class="fe mw mx my mz b">a∈A</code>。然后，以概率<code class="fe mw mx my mz b">p_s,s’,a</code>，我们移动到另一个具有相关价值函数<code class="fe mw mx my mz b">V(s’)</code>的状态。我们可以将每个状态的值函数定义为一个方程组:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/d0341ba3aa5236218fc0930403d3e727.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8_kwwKeW7JmZ-ckd9yJ2fw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">三态问题的价值函数系统[图片由作者提供]</p></figure><p id="71cd" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">最大化每个价值函数优化了策略。简而言之，如果我们知道最优价值函数，我们总是可以选择最大化预期累积回报的行动。简单地说，这就是贝尔曼的解决方案。</p><p id="cba3" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">价值函数方法的美妙之处在于，我们不需要知道任何关于实际问题结构或决策规则的东西。<strong class="li iu">知道最优值函数就等于知道最优策略。</strong></p><p id="01c0" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">寻找最优值函数的经典动态规划求解方法有<a class="ae ky" rel="noopener" target="_blank" href="/implement-value-iteration-in-python-a-minimal-working-example-f638907f3437">值迭代</a>和<a class="ae ky" rel="noopener" target="_blank" href="/implement-policy-iteration-in-python-a-minimal-working-example-6bf6cc156ca9">策略迭代</a>。您可以在以下文章中详细了解它们:</p><div class="nb nc gp gr nd ne"><a rel="noopener follow" target="_blank" href="/implement-value-iteration-in-python-a-minimal-working-example-f638907f3437"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">用Python实现值迭代——一个最小的工作示例</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">掌握简单和经典的动态规划算法，寻找马尔可夫决策过程的最优解…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns ks ne"/></div></div></a></div><div class="nb nc gp gr nd ne"><a rel="noopener follow" target="_blank" href="/implement-policy-iteration-in-python-a-minimal-working-example-6bf6cc156ca9"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">用Python实现策略迭代——一个最小的工作示例</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">了解这个经典的动态规划算法，以优化解决马尔可夫决策过程模型</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="nt l np nq nr nn ns ks ne"/></div></div></a></div><h1 id="38e0" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">贝尔曼方程和维数灾难</h1><p id="b1cf" class="pw-post-body-paragraph lg lh it li b lj nu ju ll lm nv jx lo lp nw lr ls lt nx lv lw lx ny lz ma mb im bi translated">刚才讨论的三态问题非常容易处理，但是如果有一百万个状态，方程组就会变得一团糟。幸运的是，我们不需要手写出所有的方程。事实上，整个系统可以被压缩成一行至高无上的数学优雅:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/f35fd69e984b78ec7d6b743fc9c50573.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jPTbCtwrhK5uzdixJtkbIw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">贝尔曼方程。由于价值函数之间的相互引用，系统是递归的。可以使用诸如值迭代之类的动态编程技术来找到最佳值函数。</p></figure><p id="a447" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">想想就觉得很刺眼。一个状态<code class="fe mw mx my mz b">s</code>可以是一个包含元素的巨大向量，甚至可能包含随机知识。动作<code class="fe mw mx my mz b">a</code>可能受到许多约束，并且只能通过求解复杂的数学程序来识别。看似简单的概率函数<code class="fe mw mx my mz b">P</code>可能隐藏着一个怪诞的随机系统，导致许多可能的结果状态<code class="fe mw mx my mz b">s’</code>，它们都需要明确的评估。随机环境中的顺序优化通常是非常非常混乱的。然而，贝尔曼设法将所有这些动态捕捉到一个单一的，易于理解的线。这是一件艺术品，真的。</p><p id="7cc9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">不幸的是，模拟一个问题的能力并不意味着我们能够真正解决这个问题。还记得那个三态方程组吗？事实上，这个问题不会只包含三种状态，而是更多。假设我们有一百万个状态，每个状态有一百万个可能的动作，以及一百万个状态(结果)我们可以在之后到达。这需要考虑10种⁸可能性，大致相当于地球上沙粒的数量。那些熟悉组合优化的人知道解决如此大规模的问题有多容易，甚至更难。</p><p id="b1ea" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">理查德·贝尔曼本人非常清楚他的解决方法在计算上的局限性，他创造了术语<em class="mc">‘维数灾难’</em>来强调他的方法缺乏可扩展性。其实可能是<strong class="li iu">三咒</strong> : <em class="mc">状态空间</em>，<em class="mc">动作空间</em>，<em class="mc">结果空间</em>。</p><p id="ed0a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对于许多问题，无论我们有多少计算能力，我们都无法将MDP分解为最优。这就是强化学习的用武之地。</p><h1 id="3fdc" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">价值函数逼近</h1><p id="e929" class="pw-post-body-paragraph lg lh it li b lj nu ju ll lm nv jx lo lp nw lr ls lt nx lv lw lx ny lz ma mb im bi translated">如果我们希望解决贝尔曼方程，我们必须找到每个状态的价值函数<code class="fe mw mx my mz b">s∈S</code>。对于每个单独的价值函数，我们应该评估与每个潜在结果<code class="fe mw mx my mz b">s’∈S’</code>相关联的价值函数(结果空间<code class="fe mw mx my mz b">S’</code>很可能等同于完整的状态空间<code class="fe mw mx my mz b">S</code>)。对于大的状态空间，这根本行不通。</p><p id="e501" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">典型的强化学习方法是(I)重复采样随机状态转换(而不是穷尽地评估每个可能的结果),以及(ii)用一组特征代替完整的状态描述(避免需要为每个单独的状态定义值函数)。因此，我们的目标是找到一个有希望接近最优的政策。但是，有多种方法可以实现这一点。</p><p id="ab8d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为了接近动态规划方法，我们可以求助于<strong class="li iu">价值函数逼近</strong>。通过这种方法，我们明确地试图通过一个近似函数对<code class="fe mw mx my mz b">V(s)</code>建模。例子有Q-tables ( <a class="ae ky" rel="noopener" target="_blank" href="/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff"> SARSA，Q-learning </a>)，<a class="ae ky" rel="noopener" target="_blank" href="/using-linear-programming-to-boost-your-reinforcement-learning-algorithms-994977665902">基于特征的线性函数逼近</a>，或者<a class="ae ky" rel="noopener" target="_blank" href="/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e"> critic networks </a>。请注意，贝尔曼方程——当然是以近似形式——用于表达<strong class="li iu">决策政策</strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/5a58e8788a6b4539ea88af6dd3c3a646.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*w5a6feaXt_MYAkG7Ok4g6A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">价值函数近似的策略定义。对于这个解决方案类，政策制定和贝尔曼方程之间有一个可见的联系，尽管递归方面已经消失了。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/18447166425ae5d1f43a2dce0a32c46e.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*ir-sKc9H0SxLy03DP7w7wA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线性值函数近似值的示例。这里，我们用特征θ_f(s，a)和相应的权重ϕ_f.的线性组合来代替“真实”值函数</p></figure><p id="9551" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">你可能会说我们近似贝尔曼方程，这种类比并不太牵强。</p><p id="bcc2" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">尽管如此，这个类比并不适用于强化学习的整个领域，也不适用于人类的整体决策。如果你接近一个十字路口，你可能会想到<em class="mc">‘左’</em><em class="mc">‘右’</em><em class="mc">‘直’</em>而不是去思考更远的路的下游值。许多经理完全能够在日常基础上做出连续决策，甚至没有听说过贝尔曼方程。无数的RL算法没有尝试明确地对价值函数建模。</p><p id="1330" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">简而言之，价值函数不是制定决策政策的必需品。</p><h1 id="09c2" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">目标函数</h1><p id="6756" class="pw-post-body-paragraph lg lh it li b lj nu ju ll lm nv jx lo lp nw lr ls lt nx lv lw lx ny lz ma mb im bi translated">我们正在慢慢了解这个故事的关键(是的，终于)。如果有这么多不需要定义价值函数的解决方案，为什么我们一直把它们包含在我们的问题公式中？</p><p id="5bd5" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">似乎一个普遍的误解是这个惯例的根源。简而言之:贝尔曼方程是<em class="mc">而不是</em>一个目标函数。这是一个<strong class="li iu">最优条件</strong>。如果我们找到了最优值函数，我们就找到了最优策略。然而，<strong class="li iu">和MDP </strong> <strong class="li iu">的定义无论如何都不需要值函数</strong>。</p><p id="b55c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">混淆的根源很容易看出:暗示目标的最大化函数，动态规划基础，最优值函数的最优政策保证。但是，我再重复一遍:<em class="mc">贝尔曼方程只是一个最优性条件</em>。可以使用动态规划来求解该问题，以找到MDP的最优策略。不多不少。</p><p id="a3c7" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果目标不是贝尔曼方程，那是什么？</p><p id="266b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">答案在<strong class="li iu">奖励函数</strong>(这是任何MDP的强制组件)。鉴于<em class="mc">价值</em>功能是一个有点抽象和人为的概念，<em class="mc">回报</em>功能为我们采取的每一个行动定义了一个非常实际的回报(或成本)。当我们在一个(可能是无限的)时间范围内进行优化时，将我们的目标建立在累积回报的基础上是有意义的。</p><p id="0fae" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对于<em class="mc">有限的</em>决策范围，我们可以简单地将奖励的总和作为我们的<strong class="li iu">目标函数</strong>(例如，最大化下周的销售额)。对于无限的地平线问题，我们可以采用贴现报酬序列或者简单的平均报酬(理查德·萨顿建议后者)。请注意，这些是非常自然的目标，符合人类通常的决策方式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/c73890a5dd7a2083eca1bc4fbed29be4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*ga6LVib8njKPCmphl6272w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">有限视野目标函数的例子。在这种情况下，我们只是随着时间的推移最大化总报酬。注意我们不需要价值函数。</p></figure><h1 id="fbeb" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">其他强化学习课程</h1><p id="0986" class="pw-post-body-paragraph lg lh it li b lj nu ju ll lm nv jx lo lp nw lr ls lt nx lv lw lx ny lz ma mb im bi translated">随着贝尔曼方程的消失，常见的RL方法如<a class="ae ky" rel="noopener" target="_blank" href="/a-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b">策略梯度算法</a>更有意义。事实上，在沃伦·鲍威尔提出的四类政策中，只有一类(价值函数逼近)与贝尔曼方程密切相关。在如下概述的备选策略类中，我们看不到动态编程的痕迹:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/6a0e27f1a176fcdd69c6f08fe0c69a0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*iG5nbx_z16W0Cibo-sSuLw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">策略函数近似(PFA)提供了基于状态返回决策的直接表达式。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/7d8b5569fdaf30e024b0b98778a3abc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*wb4oSQpvSFPhpehXg1-XJA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">成本函数逼近(CFA)增加了奖励函数，以纳入下游效应，而没有为此目的公开定义价值函数。</p></figure><p id="add3" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">你可以看到强化学习当然不需要求解贝尔曼方程，甚至不需要近似求解。</p><div class="nb nc gp gr nd ne"><a rel="noopener follow" target="_blank" href="/why-discount-future-rewards-in-reinforcement-learning-a833d0ae1942"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">为什么强化学习要对未来的回报打折扣？</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">从数学，金融，生活，当然还有强化的角度讨论贴现率…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="of l np nq nr nn ns ks ne"/></div></div></a></div><div class="nb nc gp gr nd ne"><a rel="noopener follow" target="_blank" href="/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">强化学习的四个策略类别</h2><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="og l np nq nr nn ns ks ne"/></div></div></a></div><h1 id="5a57" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">结束语</h1><p id="c9dd" class="pw-post-body-paragraph lg lh it li b lj nu ju ll lm nv jx lo lp nw lr ls lt nx lv lw lx ny lz ma mb im bi translated">是时候完成这个循环了。强化学习是解决具有计算挑战性的顺序决策问题的框架。由于它们可能变得相当复杂，我们需要精确和明确的问题定义来与利益相关者交流并找到解决方案。出于这个目的，使用MDP符号惯例是非常合理的。</p><p id="6cdd" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">尽管相当一部分RL算法试图逼近贝尔曼方程中假设的值函数，但许多解决方案根本不遵循动态编程范式。由于贝尔曼方程是一个<strong class="li iu">最优条件，而不是一个目标函数</strong>，包含它在最好的情况下是多余的，在最坏的情况下是误导的。如果有的话，假设我们遵循基于价值的方法，RL决策<em class="mc">政策</em>可能是贝尔曼方程的近似。</p><p id="9426" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">一个MDP只需要四个成分:(I)状态空间，(ii)动作空间，(iii)转移函数和(iv)奖励函数。贝尔曼方程或价值函数无处可寻。RL需要一个<strong class="li iu">目标函数</strong>来处理，但这只是一个简单的奖励的总和或平均序列。如果我们不逼近价值函数，贝尔曼方程就没有作用。</p><p id="24a6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">动态规划和相应的递归值函数系统是一块数学上的辉煌。贝尔曼的工作是顺序决策的一个突破，即使在今天，它仍然保留了很大的理论和实践影响。</p><p id="3488" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">和任何东西一样，只是有一个使用它的时间和地点。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="8b8f" class="md me it bd mf mg oh mi mj mk oi mm mn jz oj ka mp kc ok kd mr kf ol kg mt mu bi translated">外卖食品</h1><ul class=""><li id="6760" class="om on it li b lj nu lm nv lp oo lt op lx oq mb or os ot ou bi translated">贝尔曼方程是求解MDPs的一个<strong class="li iu">最优性条件</strong>，而不是目标函数。潜在的概念是，拥有最优价值函数等同于拥有最优策略。价值函数系统可以使用动态规划来求解。</li><li id="4224" class="om on it li b lj ov lm ow lp ox lt oy lx oz mb or os ot ou bi translated">典型的<strong class="li iu">目标函数</strong>只是将累积奖励最大化，例如，采用贴现奖励流或一段时间内的平均值。这些只需要<em class="mc">奖励</em>函数，不需要<em class="mc">值</em>函数。</li><li id="a712" class="om on it li b lj ov lm ow lp ox lt oy lx oz mb or os ot ou bi translated">许多强化学习解决方案与动态编程没有直接联系。贝尔曼方程仅明确用于四个策略类别中的一个<strong class="li iu">，即价值函数逼近。</strong></li><li id="2679" class="om on it li b lj ov lm ow lp ox lt oy lx oz mb or os ot ou bi translated">贝尔曼所宣扬的价值函数并不一定反映人类如何做出决策。最大化(折扣)<strong class="li iu">累积或平均奖励</strong>是一种更自然的沟通决策的方式。</li></ul><h1 id="8e29" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">进一步阅读</h1><p id="d6be" class="pw-post-body-paragraph lg lh it li b lj nu ju ll lm nv jx lo lp nw lr ls lt nx lv lw lx ny lz ma mb im bi translated">贝尔曼河(1961年)。<em class="mc">自适应控制过程:指导游览</em>。普林斯顿大学出版社。</p><p id="d2cc" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">鲍威尔，W.B. (2019)。随机优化的统一框架。<em class="mc">欧洲运筹学杂志</em>275.3(2019):795–821。</p><p id="c3a3" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">萨顿和巴尔托公司(2018年)。<em class="mc">强化学习:简介</em>。麻省理工出版社。</p><p id="c447" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">鲍威尔和梅塞尔(2015年)。能源中的随机优化教程——第二部分:能量存储图解。<em class="mc"> IEEE电力系统汇刊</em>31.2(2015):1468–1475。</p><p id="b820" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">纳伊克，a .，沙里夫，r .，Yasui，n .，姚，h .和萨顿，R. (2019)。折扣强化学习不是一个优化问题。【https://arxiv.org/pdf/1910.02140.pdf T4】</p><p id="dc80" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">维基百科(2022)。<em class="mc">贝尔曼方程。</em><a class="ae ky" href="https://en.wikipedia.org/wiki/Bellman_equation" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Bellman_equation</a></p><p id="3876" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">维基百科(2022)。<em class="mc">动态编程。</em><a class="ae ky" href="https://en.wikipedia.org/wiki/Dynamic_programming" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Dynamic_programming</a></p><p id="0b34" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">维基百科(2022)。<em class="mc">马尔可夫决策过程。</em><a class="ae ky" href="https://en.wikipedia.org/wiki/Markov_decision_process" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Markov_decision_process</a></p><p id="d8ff" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">维基百科(2022)。<em class="mc">理查德·贝尔曼。</em><a class="ae ky" href="https://en.wikipedia.org/wiki/Richard_E._Bellman#cite_note-15" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Richard_E._Bellman</a></p></div></div>    
</body>
</html>