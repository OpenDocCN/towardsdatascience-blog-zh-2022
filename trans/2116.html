<html>
<head>
<title>How to make a Transformer for time series forecasting with PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用PyTorch制作用于时间序列预测的转换器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e#2022-05-12">https://towardsdatascience.com/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e#2022-05-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ab35" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">这篇文章将向你展示如何一步一步地将时序转换器架构图转换成PyTorch代码</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/05045ba0ffa4a053fa750343850e9201.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DX3oCuJhoH5H21nb1wgXew.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">变电站。图片由<a class="ae kv" href="https://pixabay.com/photos/hockenheim-switchyard-transformer-844452/" rel="noopener ugc nofollow" target="_blank"> WikimediaImages </a>提供。</p></figure><p id="db24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">变压器模型在许多时间序列预测问题中表现出了最先进的性能[1][2][3]。</p><p id="4b13" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，您将学习如何在PyTorch中为时间序列预测编写一个transformer架构。具体来说，我们将对论文<em class="ls">“时间序列预测的深度转换模型:流感流行案例”</em> [2]中使用的架构进行编码，我们将使用他们的架构图作为出发点。</p><p id="8ed3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我将一步一步地展示如何对图中的每个组件进行编码。通过这种方式，您将学习解释transformer架构图并将其转换为代码的通用技巧。</p><p id="61a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将解释这个过程，就好像您以前从未实现过transformer模型一样。然而，我确实假设你对PyTorch和机器学习有基本的了解。最终的结果将是一个我们称之为<code class="fe lt lu lv lw b">TimeSeriesTransformer</code>的类，在这个类中所有的东西都聚集在一起。</p><p id="4f3f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我还将解释模型的<code class="fe lt lu lv lw b">forward()</code>方法的输入必须是什么，以及如何创建它们。</p><p id="03cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">值得注意的是，没有一个转换器模型架构。存在几种不同的变压器架构。由此自然得出结论，例如，当我说编码器由x、y、z组成时，我指的是我们在本文中实现的transformer架构的编码器，而不是某种通用的transformer编码器。</p><p id="7cef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我们将在本文中实现的架构图:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lx"><img src="../Images/725dc570cb3d00d1fd3c19bd10370ccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fKbqqiSAVg3a7PV2DSUn2Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图一。图片作者:吴，格林，本&amp;奥巴尼恩，2020 [2]</p></figure><p id="8462" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，虽然该图仅描述了两个编码器层和两个解码器层，但作者实际上在每个层中都使用了四个层[2]。</p><p id="db13" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的表1给出了构建图1中的时序转换器架构所需的所有组件的概述，以及使用什么类来制作每个组件。如您所见，我们只需要实现一个定制类。PyTorch中提供了其他所有内容。耶！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ly"><img src="../Images/299fe66417a7f0b4921a339637899180.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kJQScy37g5kNOf6rzPfxPA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表1。时序转换器组件概述。图片由Kasper Groes阿尔宾Ludvigsen。</p></figure><p id="417e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">起初让我困惑的是，在图1中，输入层和位置编码层被描述为编码器的一部分，而在解码器端，输入层和线性映射层被描述为解码器的一部分。</p><p id="df44" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在最初的transformer论文[4]中，情况并非如此，其中输入层、位置编码层和线性层被描述为与编码器和解码器分离(参见下面的图2)。</p><p id="cb5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了与最初的transformer文件保持一致，我将在这篇文章中说，编码器和解码器仅由<em class="ls"> n </em>个堆叠的编码器或解码器层组成，我将把其他层视为编码器和解码器之外的独立层。此外，根据最初的transformer论文[4]，我不会将“添加和规范化”操作称为层。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/f26b5f22aca49c8091fa70fce2f30e4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*2KrICIr3FUjUj1ukBvbNKw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图二。在最初的transformer论文中，输入层和位置编码层被描述为与编码器和解码器分离，这与[2]相反。瓦斯瓦尼等人的图片2017 [4]</p></figure><p id="3d1c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该员额的其余部分结构如下:</p><ol class=""><li id="23fa" class="ma mb iq ky b kz la lc ld lf mc lj md ln me lr mf mg mh mi bi translated">首先，我们将在名为<code class="fe lt lu lv lw b">TimeSeriesTransformer</code>的课程中了解如何制作变压器的每个组件，以及如何将它们组装在一起</li><li id="e185" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">然后，我将展示如何为模型创建输入</li></ol><p id="bd73" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将不提供组件内部工作的详细描述，因为这些在别处已经解释得很清楚了(例如[5][6])。</p><p id="3391" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当你读完这篇文章后，你可能想学习如何在推理过程中使用时间序列转换器:</p><div class="mo mp gp gr mq mr"><a rel="noopener follow" target="_blank" href="/how-to-run-inference-with-a-pytorch-time-series-transformer-394fd6cbe16c"><div class="ms ab fo"><div class="mt ab mu cl cj mv"><h2 class="bd ir gy z fp mw fr fs mx fu fw ip bi translated">如何使用PyTorch时序转换器运行推理</h2><div class="my l"><h3 class="bd b gy z fp mw fr fs mx fu fw dk translated">在您不知道解码器输入的情况下，在推断时间使用PyTorch转换器进行时间序列预测</h3></div><div class="mz l"><p class="bd b dl z fp mw fr fs mx fu fw dk translated">towardsdatascience.com</p></div></div><div class="na l"><div class="nb l nc nd ne na nf kp mr"/></div></div></a></div><h1 id="5b4e" class="ng nh iq bd ni nj nk nl nm nn no np nq jw nr jx ns jz nt ka nu kc nv kd nw nx bi translated">1.分解变压器架构</h1><p id="29b8" class="pw-post-body-paragraph kw kx iq ky b kz ny jr lb lc nz ju le lf oa lh li lj ob ll lm ln oc lp lq lr ij bi translated">让我们将图中所示的转换器架构分解成它的组成部分。</p><h2 id="fe89" class="od nh iq bd ni oe of dn nm og oh dp nq lf oi oj ns lj ok ol nu ln om on nw oo bi translated"><strong class="ak"> 1.1。编码器输入层</strong></h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/0d64326a2d86d7765592ec8314c4f684.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m7q7JLq5Bq0Z6AZm4qewkg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者吴，格林，本&amp;奥巴尼恩，2020 [2](本人重点)</p></figure><p id="76ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">编码器输入层简单地实现为神经网络。线性()层。<code class="fe lt lu lv lw b">in_features</code>参数必须等于作为模型输入的变量数量。在一个单变量时间序列预测问题中，<code class="fe lt lu lv lw b">in_features = 1</code>。<code class="fe lt lu lv lw b">out_features</code>参数必须是<code class="fe lt lu lv lw b">d_model</code>，它是一个在【4】中具有值<code class="fe lt lu lv lw b">512</code>的超参数。</p><p id="ddb1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是<code class="fe lt lu lv lw b">TimeSeriesTransformer</code>类中的代码:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><h2 id="0a68" class="od nh iq bd ni oe of dn nm og oh dp nq lf oi oj ns lj ok ol nu ln om on nw oo bi translated"><strong class="ak"> 1.2。位置编码层</strong></h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/de8496294f2fea662ebbafff363be2c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*msSSWD3grWzbs9J1ochYmw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者吴，格林，本&amp;奥巴尼恩，2020 [2](本人重点)</p></figure><p id="da5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最初的transformer论文的作者非常简洁地描述了位置编码层的作用以及为什么需要它:</p><blockquote class="ot ou ov"><p id="22c0" class="kw kx ls ky b kz la jr lb lc ld ju le ow lg lh li ox lk ll lm oy lo lp lq lr ij bi translated">由于我们的模型不包含递归和卷积，为了让模型利用序列的顺序，我们必须注入一些关于序列中记号的相对或绝对位置的信息</p></blockquote><p id="0dd1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是将位置编码器实现为一个类的一种方法。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="704a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是如何在<code class="fe lt lu lv lw b">TimeSeriesTransformer</code>类中使用<code class="fe lt lu lv lw b">PositionalEncoder</code>类:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="6c0c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如您所见，<code class="fe lt lu lv lw b">dim_val</code>是作为<code class="fe lt lu lv lw b">d_model</code>参数提供的。这很重要，因为编码器输入层产生大小为<code class="fe lt lu lv lw b">dim_val</code>的输出。</p><h2 id="45e7" class="od nh iq bd ni oe of dn nm og oh dp nq lf oi oj ns lj ok ol nu ln om on nw oo bi translated"><strong class="ak"> 1.3。编码器层</strong></h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oz"><img src="../Images/b346d05d6cbe7021144905015ed2521a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YOUHG_E5JUxwnHhNXGxJsA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者吴，格林，本&amp;奥巴尼恩，2020 [2](本人重点)</p></figure><p id="857b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，虽然该图仅描述了两个编码器层，但作者实际上使用了四个编码器层[2]。</p><p id="8563" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]使用的编码器层与PyTorch Transformer库所基于的[4]使用的编码器层相同，因此我们可以简单地使用PyTorch来创建编码器层。</p><p id="50b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这样做的方法是首先制作一个对象，我们可以称之为<code class="fe lt lu lv lw b">encoder_layer</code>，用<code class="fe lt lu lv lw b">torch.nn.TransformerEncoderLayer</code>这样:</p><p id="0c2e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe lt lu lv lw b">encoder_layer = torch.nn.TransformerEncoderLayer(d_model, nhead, batch_first=True)</code></p><p id="f98c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过使用<code class="fe lt lu lv lw b">torch.nn.TransformerEncoderLayer</code>，该层将自动具有如上所述的自我关注层和前馈层，以及中间的“添加&amp;正常化”。注意，没有必要使<code class="fe lt lu lv lw b">encoder_layer</code>成为<code class="fe lt lu lv lw b">TimeSeriesTransformer</code>类的实例属性，因为它只是作为参数传递给<code class="fe lt lu lv lw b">nn.TransformerEncoder</code>。</p><p id="5b1e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后像这样将<code class="fe lt lu lv lw b">encoder_layer</code>对象作为参数传递给<code class="fe lt lu lv lw b">torch.nn.TransformerEncoder</code>，以便堆叠4个相同的编码器层:</p><p id="f38a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe lt lu lv lw b">self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=4, norm=None)</code></p><p id="94f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意<code class="fe lt lu lv lw b">norm</code>是<code class="fe lt lu lv lw b">nn.TransformerEncoder</code>中的一个可选参数，当使用标准<code class="fe lt lu lv lw b">nn.TransformerEncoderLayer</code>类时传递一个标准化对象是多余的，因为<code class="fe lt lu lv lw b">nn.TransformerEncoderLayer</code>已经在每一层后标准化了。可选参数用于不包括标准化的自定义编码器层[7]。</p><p id="a858" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是编码器代码在<code class="fe lt lu lv lw b">TimeSeriesTransformer</code>类中的一个片段。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><h2 id="6ce2" class="od nh iq bd ni oe of dn nm og oh dp nq lf oi oj ns lj ok ol nu ln om on nw oo bi translated">1.4.解码器输入层</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/64eb16baa63d05724f58e20e0128f884.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V7Srw1U98JMAus8eU5S26Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者吴，格林，本&amp;奥巴尼恩，2020 [2](本人重点)</p></figure><p id="e656" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">解码器输入层只是一个线性层，就像编码器输入层一样。<code class="fe lt lu lv lw b">in_features</code>参数必须等于作为模型输入的变量的数量。在一个单变量时间序列预测问题中，<code class="fe lt lu lv lw b">in_features = 1</code>。<code class="fe lt lu lv lw b">out_features</code>参数必须是<code class="fe lt lu lv lw b">d_model</code>，它是一个在【4】中具有值<code class="fe lt lu lv lw b">512</code>的超参数。我们将使用这个值，因为[2]没有指定它。</p><p id="4102" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是<code class="fe lt lu lv lw b">TimeSeriesTransformer</code>类中的代码:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><h2 id="65b8" class="od nh iq bd ni oe of dn nm og oh dp nq lf oi oj ns lj ok ol nu ln om on nw oo bi translated">1.5.解码器层</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/24b9ee8be25512b7d78d2a3f6dafd660.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W8Fa55YBqIdaPtOTv8yoYw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者吴，格林，本&amp;奥巴尼恩，2020 [2](本人重点)</p></figure><p id="7c45" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，尽管该图仅描绘了两个解码器层。作者实际上使用了四个解码器层[2]。</p><p id="59c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">解码器的制作方式与编码器完全相同，根据[2]，其中<code class="fe lt lu lv lw b">num_layers</code>为4。</p><p id="c0cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]没有指定他们使用多少头，所以我们将按照[4]使用<code class="fe lt lu lv lw b">nheads=8</code>。</p><p id="531c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里有一个片段展示了解码器代码放在<code class="fe lt lu lv lw b">TimeSeriesTransformer</code>类中时的样子。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><h2 id="f673" class="od nh iq bd ni oe of dn nm og oh dp nq lf oi oj ns lj ok ol nu ln om on nw oo bi translated">1.6.线性映射层</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/61d1a8db409ae79ce1b8a6e9a63fba6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HuFUNQQXU3VO754bqTBdAg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者吴，格林，本&amp;奥巴尼恩，2020 [2](本人重点)</p></figure><p id="eb86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管这一层被称为“线性映射层”，但除了参数值之外，它实际上与编码器和解码器输入层相同:</p><p id="8195" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe lt lu lv lw b">in_features</code>必须等于输出序列长度乘以d_model，以适应解码器的输出。</p><p id="6382" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe lt lu lv lw b">out_features</code>必须等于目标序列长度，因为线性映射层是变压器模型的最后一层。因此，如果您的时间序列数据集由每小时的数据点组成，并且您想要预测未来24小时，<code class="fe lt lu lv lw b">out_features</code>必须是24。</p><p id="dd9b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是<code class="fe lt lu lv lw b">TimeSeriesTransformer</code>类中的代码:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><h2 id="2f1a" class="od nh iq bd ni oe of dn nm og oh dp nq lf oi oj ns lj ok ol nu ln om on nw oo bi translated">组装变压器模型</h2><p id="a916" class="pw-post-body-paragraph kw kx iq ky b kz ny jr lb lc nz ju le lf oa lh li lj ob ll lm ln oc lp lq lr ij bi translated">既然我们已经看到了如何对图中所示的构成transformer模型的每个组件进行编码，我们将把它们放在一个类中。参数的默认值是[2]中使用的值。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><h2 id="bdb2" class="od nh iq bd ni oe of dn nm og oh dp nq lf oi oj ns lj ok ol nu ln om on nw oo bi translated">初始化转换器模型</h2><p id="c28e" class="pw-post-body-paragraph kw kx iq ky b kz ny jr lb lc nz ju le lf oa lh li lj ob ll lm ln oc lp lq lr ij bi translated">既然我们已经看到了如何编写<code class="fe lt lu lv lw b">TimeSeriesTransformer</code>类，我还想快速展示如何初始化模型以及什么值作为参数传递。模型的实现方式，只有<code class="fe lt lu lv lw b">input_size</code>、<code class="fe lt lu lv lw b">dec_seq_len</code>和<code class="fe lt lu lv lw b">max_seq_len</code>是必需的，其余都有默认值。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><h1 id="bad1" class="ng nh iq bd ni nj nk nl nm nn no np nq jw nr jx ns jz nt ka nu kc nv kd nw nx bi translated">2.如何创建变压器模型的输入</h1><p id="7e77" class="pw-post-body-paragraph kw kx iq ky b kz ny jr lb lc nz ju le lf oa lh li lj ob ll lm ln oc lp lq lr ij bi translated">正如在<code class="fe lt lu lv lw b">TimeSeriesTransformer</code>类中看到的，我们模型的<code class="fe lt lu lv lw b">forward()</code>方法接受4个参数作为输入。在这一节中，我将解释如何创建这四个对象。</p><p id="c564" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些输入是:</p><ol class=""><li id="4ed5" class="ma mb iq ky b kz la lc ld lf mc lj md ln me lr mf mg mh mi bi translated"><code class="fe lt lu lv lw b">src</code></li><li id="9b25" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated"><code class="fe lt lu lv lw b">trg</code></li><li id="0824" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated"><code class="fe lt lu lv lw b">src_mask</code></li><li id="1666" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated"><code class="fe lt lu lv lw b">trg_mask</code></li></ol><h2 id="0791" class="od nh iq bd ni oe of dn nm og oh dp nq lf oi oj ns lj ok ol nu ln om on nw oo bi translated">2.1.如何为时序转换器模型创建src和trg</h2><p id="c51c" class="pw-post-body-paragraph kw kx iq ky b kz ny jr lb lc nz ju le lf oa lh li lj ob ll lm ln oc lp lq lr ij bi translated">让我们先仔细看看<code class="fe lt lu lv lw b">src</code>和<code class="fe lt lu lv lw b">trg</code>是如何为时序变压器模型制作的。</p><p id="3ace" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe lt lu lv lw b">src</code>是编码器输入，是“源”的简称。<code class="fe lt lu lv lw b">src</code>只是整个序列中连续数据点的子集。<code class="fe lt lu lv lw b">src</code>的长度决定了您的模型在进行预测时会考虑多少过去的数据点。如果数据集的分辨率为每小时，则一天有24个数据点，如果您希望模型基于过去两天的数据进行预测，则<code class="fe lt lu lv lw b">src</code>的长度应为48。</p><p id="4375" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe lt lu lv lw b">trg</code>是解码器输入。Trg是“target”的缩写，但这有点误导，因为它不是实际的目标序列，而是由<code class="fe lt lu lv lw b">src</code>的最后一个数据点和实际目标序列中除最后一个数据点以外的所有数据点组成的序列。这就是为什么人们有时称trg序列为“右移”。<code class="fe lt lu lv lw b">trg</code>的长度必须等于实际目标序列的长度【2】。你有时会看到术语<code class="fe lt lu lv lw b">tgt</code>被用作同义词。</p><p id="173a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是对数据点<code class="fe lt lu lv lw b">src</code>和<code class="fe lt lu lv lw b">trg</code>必须包含的内容的简明解释:</p><blockquote class="ot ou ov"><p id="801b" class="kw kx ls ky b kz la jr lb lc ld ju le ow lg lh li ox lk ll lm oy lo lp lq lr ij bi translated">在典型的训练设置中，我们训练模型从10个跟踪的每周数据点预测4个未来的每周ILI比率。也就是说，给定编码器输入(x1，x2，…，x10)和解码器输入(x10，…，x13)，解码器的目标是输出(x11，…，x14)。<em class="iq"> ([2]第5页)</em></p></blockquote><p id="b878" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">给定一个序列，这里有一个函数产生<code class="fe lt lu lv lw b">src</code>和<code class="fe lt lu lv lw b">trg</code>以及实际的目标序列<code class="fe lt lu lv lw b">trg_y</code>。<code class="fe lt lu lv lw b">src</code>和<code class="fe lt lu lv lw b">trg</code>对象被输入到模型中，而<code class="fe lt lu lv lw b">trg_y</code>是目标序列，当计算损失时，模型的输出与该目标序列进行比较。给<code class="fe lt lu lv lw b">get_src_trg()</code>函数的<code class="fe lt lu lv lw b">sequence</code>必须是整个数据集的子序列，长度为input _ sequence _ length+target _ sequence _ length。</p><p id="372a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是创建<code class="fe lt lu lv lw b">src</code>、<code class="fe lt lu lv lw b">trg</code>和<code class="fe lt lu lv lw b">trg_y</code>的函数:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><h2 id="ceec" class="od nh iq bd ni oe of dn nm og oh dp nq lf oi oj ns lj ok ol nu ln om on nw oo bi translated">2.2.屏蔽变压器中的解码器输入</h2><p id="233a" class="pw-post-body-paragraph kw kx iq ky b kz ny jr lb lc nz ju le lf oa lh li lj ob ll lm ln oc lp lq lr ij bi translated">我们现在已经看到了如何生成我们的模型的<code class="fe lt lu lv lw b">forward()</code>方法所需要的前两个输入。现在让我们考虑我们的模型的<code class="fe lt lu lv lw b">forward()</code>方法需要的最后两个输入:<code class="fe lt lu lv lw b">src_mask</code>和<code class="fe lt lu lv lw b">trg_mask</code></p><p id="68f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但首先你应该知道在变形金刚的上下文中有两种类型的屏蔽:</p><ol class=""><li id="15a5" class="ma mb iq ky b kz la lc ld lf mc lj md ln me lr mf mg mh mi bi translated"><em class="ls">填充蒙版</em>。当使用不同长度的序列(句子通常具有不同的长度)时，短于所选最大序列长度(这是一个可以具有任何值的超参数，例如50)的序列将用填充标记填充。填充标记必须被屏蔽，以防止模型关注这些标记。</li><li id="5f6e" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated"><em class="ls">解码器输入屏蔽</em>(又名“前瞻屏蔽”)。这种类型的屏蔽防止解码器在“考虑”令牌<em class="ls"> t </em>具有什么“含义”时关注未来的令牌。</li></ol><p id="4e4e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我们不会填充我们的序列，因为我们将以这样一种方式实现我们的自定义数据集类，即所有序列都具有相同的长度。因此，在我们的例子[8]中不需要<em class="ls">填充屏蔽</em>，也没有必要屏蔽编码器输入[9]</p><p id="8c0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，我们需要使用<em class="ls">解码器输入屏蔽</em>，因为这种类型的屏蔽总是必要的。</p><p id="8c61" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">回想一下，解码器接收两个输入:</p><ol class=""><li id="a78b" class="ma mb iq ky b kz la lc ld lf mc lj md ln me lr mf mg mh mi bi translated">编码器输出</li><li id="d8f3" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">解码器输入</li></ol><p id="0a9c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这两个都需要屏蔽。</p><p id="7648" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了屏蔽这些输入，我们将为模型的<code class="fe lt lu lv lw b">forward()</code>方法提供两个屏蔽张量:</p><ol class=""><li id="52db" class="ma mb iq ky b kz la lc ld lf mc lj md ln me lr mf mg mh mi bi translated"><code class="fe lt lu lv lw b">src_mask</code>这会屏蔽编码器输出</li><li id="88f1" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated"><code class="fe lt lu lv lw b">trg_mask</code>这会屏蔽解码器输入</li></ol><p id="cb24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们的例子中，<code class="fe lt lu lv lw b">src_mask</code>将需要具有以下尺寸:</p><p id="a690" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[目标序列长度，编码器序列长度]</p><p id="76ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">并且<code class="fe lt lu lv lw b">trg_mask</code>将需要具有大小:</p><p id="dde9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[目标序列长度，目标序列长度]</p><p id="ac94" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是生成遮罩的方法:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="8095" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是如何使用遮罩作为模型的输入:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><h1 id="268f" class="ng nh iq bd ni nj nk nl nm nn no np nq jw nr jx ns jz nt ka nu kc nv kd nw nx bi translated">时间序列转换器的完整示例</h1><p id="6c1f" class="pw-post-body-paragraph kw kx iq ky b kz ny jr lb lc nz ju le lf oa lh li lj ob ll lm ln oc lp lq lr ij bi translated">我已经创建了<a class="ae kv" href="https://github.com/KasperGroesLudvigsen/influenza_transformer" rel="noopener ugc nofollow" target="_blank">这个回购</a>，它包含了一个完整的例子和一些时间序列数据。repo还包含用时序转换器模型运行推理的代码，我的文章“<a class="ae kv" href="https://medium.com/towards-data-science/how-to-run-inference-with-a-pytorch-time-series-transformer-394fd6cbe16c" rel="noopener">如何用PyTorch时序转换器</a>运行推理”中描述了这些代码</p><div class="mo mp gp gr mq mr"><a rel="noopener follow" target="_blank" href="/multi-step-time-series-forecasting-with-xgboost-65d6820bec39"><div class="ms ab fo"><div class="mt ab mu cl cj mv"><h2 class="bd ir gy z fp mw fr fs mx fu fw ip bi translated">XGBoost多步时间序列预测</h2><div class="my l"><h3 class="bd b gy z fp mw fr fs mx fu fw dk translated">本文展示了如何使用XGBoost生成多步时间序列预测和24小时电价预测…</h3></div><div class="mz l"><p class="bd b dl z fp mw fr fs mx fu fw dk translated">towardsdatascience.com</p></div></div><div class="na l"><div class="pc l nc nd ne na nf kp mr"/></div></div></a></div></div><div class="ab cl pd pe hu pf" role="separator"><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi"/></div><div class="ij ik il im in"><p id="bb7f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样！我希望你喜欢这篇文章🤞</p><p id="69e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请留下评论让我知道你的想法。如果您有任何问题或建议，我将非常高兴收到您的来信🙌</p><p id="3dc7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关注更多与时间序列预测相关的帖子。我也写绿色软件工程和数据科学对环境的影响，比如这里的<a class="ae kv" href="https://kaspergroesludvigsen.medium.com/the-10-most-energy-efficient-programming-languages-6a4165126670" rel="noopener"/>和这里的<a class="ae kv" rel="noopener" target="_blank" href="/8-podcast-episodes-on-the-climate-impact-of-machine-learning-54f1c19f52d"/>🍀</p><p id="2617" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请随时在LinkedIn上与我联系。</p><h1 id="38a8" class="ng nh iq bd ni nj nk nl nm nn no np nq jw nr jx ns jz nt ka nu kc nv kd nw nx bi translated">参考</h1><p id="91e3" class="pw-post-body-paragraph kw kx iq ky b kz ny jr lb lc nz ju le lf oa lh li lj ob ll lm ln oc lp lq lr ij bi translated">[1]<a class="ae kv" href="https://arxiv.org/abs/2109.12218" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2109.12218</a></p><p id="cf3f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]<a class="ae kv" href="https://arxiv.org/abs/2001.08317" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2001.08317</a></p><p id="3114" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://arxiv.org/abs/2012.07436" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2012.07436</a></p><p id="6c09" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1706.03762.pdf</a></p><p id="7177" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[5]<a class="ae kv" rel="noopener" target="_blank" href="/how-to-code-the-transformer-in-pytorch-24db27c8f9ec#1b3f">https://towards data science . com/how-to-code-the-transformer-in-py torch-24 db 27 c 8 F9 EC # 1b3f</a></p><p id="0058" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[6]http://jalammar.github.io/illustrated-transformer/<a class="ae kv" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"/></p><p id="c669" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://github.com/pytorch/pytorch/issues/24930" rel="noopener ugc nofollow" target="_blank">https://github.com/pytorch/pytorch/issues/24930</a></p><p id="4dc6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[8]<a class="ae kv" href="https://github.com/huggingface/transformers/issues/4083" rel="noopener ugc nofollow" target="_blank">https://github.com/huggingface/transformers/issues/4083</a></p><p id="ed9d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[9]<a class="ae kv" href="https://medium.com/analytics-vidhya/masking-in-transformers-self-attention-mechanism-bad3c9ec235c" rel="noopener">https://medium . com/analytics-vid hya/masking-in-transformers-self-attention-mechanism-bad 3c 9 EC 235 c</a></p></div></div>    
</body>
</html>