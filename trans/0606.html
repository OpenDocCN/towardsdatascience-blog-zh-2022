<html>
<head>
<title>Visually understand XGBoost, LightGBM and CatBoost Regularization Parameters</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">直观理解XGBoost、LightGBM和CatBoost正则化参数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/visually-understand-xgboost-lightgbm-and-catboost-regularization-parameters-aa12abcd4c17#2022-02-24">https://towardsdatascience.com/visually-understand-xgboost-lightgbm-and-catboost-regularization-parameters-aa12abcd4c17#2022-02-24</a></blockquote><div><div class="fc ij ik il im in"/><div class="io ip iq ir is"><div class=""/><figure class="gl gn jt ju jv jw gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi js"><img src="../Images/55acfde391c92ba091f2dbcf5f85418a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*La2-aWqcxwwXkU1q"/></div></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">里卡多·戈麦斯·安吉尔在<a class="ae kh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><blockquote class="ki kj kk"><p id="61bc" class="kl km kn ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj io bi translated"><strong class="ko iw">更新</strong>:发现我关于渐变提升的新书<a class="ae kh" href="https://www.amazon.com/dp/B0BJ82S916" rel="noopener ugc nofollow" target="_blank">实用渐变提升</a>。这是用python中的许多例子对渐变增强的深入探究。</p></blockquote><div class="lk ll gp gr lm ln"><a href="https://www.amazon.com/dp/B0BJ82S916" rel="noopener  ugc nofollow" target="_blank"><div class="lo ab fo"><div class="lp ab lq cl cj lr"><h2 class="bd iw gy z fp ls fr fs lt fu fw iu bi translated">实用的渐变增强:深入探究Python中的渐变增强</h2><div class="lu l"><h3 class="bd b gy z fp ls fr fs lt fu fw dk translated">这本书的梯度推进方法是为学生，学者，工程师和数据科学家谁希望…</h3></div></div><div class="lv l"><div class="lw l lx ly lz lv ma kb ln"/></div></div></a></div><h1 id="e181" class="mb mc iv bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">构建正确的模型</h1><p id="e172" class="pw-post-body-paragraph kl km iv ko b kp mz kr ks kt na kv kw nb nc kz la nd ne ld le nf ng lh li lj io bi translated">当考虑一个数据科学问题时，一个优秀的数据科学家应该对驱动其模型性能的主要特性有所了解。</p><p id="6af8" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">尽管从理论上讲，相信自动ml、自动特征化、自动超参数调整会更好，但依靠人类的专业知识来驱动模型调整通常更省时省力。</p><p id="0325" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">结合人类的直觉和计算机科学马力是非常强大的。</p><p id="0f08" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">将人类的直觉转化为模型选择和训练需要对模型如何工作有深刻的理解。</p><h1 id="4474" class="mb mc iv bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">一幅画胜过千言万语</h1><p id="8f58" class="pw-post-body-paragraph kl km iv ko b kp mz kr ks kt na kv kw nb nc kz la nd ne ld le nf ng lh li lj io bi translated">在本文中，我们将直观地展示当使用梯度推进方法时，超参数如何控制系综树的形状。</p><p id="3883" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">有许多这样的参数，但是我们将集中于驱动模型复杂性的参数:正则化参数。</p><p id="ce3f" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">幸运的是，(逻辑上)决策树的梯度提升的三个主要实现XGBoost、LightGBM和CatBoost主要共享相同的正则化超参数。</p><h1 id="9a4c" class="mb mc iv bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">超参数及其对决策树构建的影响</h1><p id="44b2" class="pw-post-body-paragraph kl km iv ko b kp mz kr ks kt na kv kw nb nc kz la nd ne ld le nf ng lh li lj io bi translated">在详细讨论最重要的超参数之前，让我们先介绍一些结构。</p><p id="9308" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">我们可以将超参数分为三类:</p><ul class=""><li id="e659" class="nh ni iv ko b kp kq kt ku nb nj nd nk nf nl lj nm nn no np bi translated">驱动树结构的超参数</li><li id="2e05" class="nh ni iv ko b kp nq kt nr nb ns nd nt nf nu lj nm nn no np bi translated">驱动权重值的超参数</li><li id="a939" class="nh ni iv ko b kp nq kt nr nb ns nd nt nf nu lj nm nn no np bi translated">推动训练的超参数</li></ul><p id="8cc1" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">正则化参数通常影响树结构以及权重值。</p><h2 id="7ec2" class="nv mc iv bd md nw nx dn mh ny nz dp ml nb oa ob mp nd oc od mt nf oe of mx og bi translated">结构超参数</h2><p id="f701" class="pw-post-body-paragraph kl km iv ko b kp mz kr ks kt na kv kw nb nc kz la nd ne ld le nf ng lh li lj io bi translated">第一类对树的结构有影响:本质上是树的深度和叶子的数量。</p><h2 id="9829" class="nv mc iv bd md nw nx dn mh ny nz dp ml nb oa ob mp nd oc od mt nf oe of mx og bi translated">内容超参数</h2><p id="b6d0" class="pw-post-body-paragraph kl km iv ko b kp mz kr ks kt na kv kw nb nc kz la nd ne ld le nf ng lh li lj io bi translated">第二组控制附加到叶子上的值，即预测值。</p><h2 id="4376" class="nv mc iv bd md nw nx dn mh ny nz dp ml nb oa ob mp nd oc od mt nf oe of mx og bi translated">学习超参数</h2><p id="6f34" class="pw-post-body-paragraph kl km iv ko b kp mz kr ks kt na kv kw nb nc kz la nd ne ld le nf ng lh li lj io bi translated">最后，第三种驱动训练:算法是否使用所有样本，我们是否应用所有计算的增益，…</p></div><div class="ab cl oh oi hz oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="io ip iq ir is"><h1 id="ad83" class="mb mc iv bd md me oo mg mh mi op mk ml mm oq mo mp mq or ms mt mu os mw mx my bi translated">可视化正则化超参数</h1><h2 id="8042" class="nv mc iv bd md nw nx dn mh ny nz dp ml nb oa ob mp nd oc od mt nf oe of mx og bi translated">Reg alpha</h2><p id="8f53" class="pw-post-body-paragraph kl km iv ko b kp mz kr ks kt na kv kw nb nc kz la nd ne ld le nf ng lh li lj io bi translated">先说一个重要但不容易理解的参数:<code class="fe ot ou ov ow b">reg_alpha</code>。顾名思义，该参数对模型的正则化有影响。</p><p id="5926" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">正则化是一种数学方法，其目标是使函数光滑。它经常被用在物理学中，把一个不规则的函数转化成一个规则的，光滑的函数。</p><p id="e8a7" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">使用正则化函数非常方便，因为这种函数在微分时有很好的性质。当应用梯度推进时，我们需要一个至少可以微分两次的函数:一次是梯度，另一次是Hessian。</p><p id="810a" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">对于决策树的梯度推进，正则化函数如下所示:</p><figure class="oy oz pa pb gt jw gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/3fbacae2216db2ab5b17a75559b6de8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*gh-Jgf3m1JdOf0pF69v2rw.png"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">大多数GBT方法中使用的正则化公式。作者的公式。</p></figure><p id="9e86" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">在训练期间，该算法试图最小化该函数以及目标函数。</p><p id="e3b2" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">在这个公式中可以看到，<code class="fe ot ou ov ow b">alpha</code>是正则化函数<code class="fe ot ou ov ow b">Omega</code>的三个参数之一。</p><p id="6786" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated"><code class="fe ot ou ov ow b">gamma</code>用于控制如何惩罚模型中的树的数量。</p><p id="58bd" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated"><code class="fe ot ou ov ow b">lambda</code>控制我们想要惩罚多少平方树叶权重的总和。当应用平方函数时，我们通常将该参数称为L2正则化项。</p><p id="0e29" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated"><code class="fe ot ou ov ow b">alpha</code>控制我们想要惩罚多少叶子权重的绝对值之和。我们通常将该参数称为L1正则化项，因为仅应用了绝对误差。</p><p id="ab7a" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">已知使用高<code class="fe ot ou ov ow b">alpha</code>生成所谓的<em class="kn">稀疏模型，即</em>模型，其中权重趋于零。最大化空值至少有两个好处:</p><ul class=""><li id="c3e6" class="nh ni iv ko b kp kq kt ku nb nj nd nk nf nl lj nm nn no np bi translated">它简化了模型</li><li id="103a" class="nh ni iv ko b kp nq kt nr nb ns nd nt nf nu lj nm nn no np bi translated">它减小了模型的大小，因为不需要存储空值。</li></ul><p id="4ec5" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">这类似于<em class="kn">稀疏矩阵</em>。让我们看看下面的代码，在著名的虹膜分类数据集上，低的<code class="fe ot ou ov ow b">alpha</code>与高的<code class="fe ot ou ov ow b">lambda</code>相比如何:</p><figure class="oy oz pa pb gt jw"><div class="bz fp l di"><div class="pc pd l"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">比较高L2正则化和高L1正则化。作者代码。</p></figure><p id="1121" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">两个模型的表现都很好，但是正如你所看到的，在第一个模型中，使用L2正则化，没有权重是空的，而在第二个例子中，使用L1正则化，我们得到65个空权重。</p><p id="e9fb" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">因此使用非空的<code class="fe ot ou ov ow b">reg_alpha</code>降低了模型的复杂性。</p><p id="78d0" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">正如这篇文章所承诺的，我们可以看到不同之处。让我们看看使用L1正则化时的其中一个树:</p><figure class="oy oz pa pb gt jw gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi pe"><img src="../Images/f3357c31a221b797ab074ccff6e716c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GsIzFeTKgGZU4MdfAFMprw.png"/></div></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">当使用高α时，第五个估计量。作者的情节。</p></figure><p id="8882" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">使用高L2正则化的相同估计量:</p><figure class="oy oz pa pb gt jw gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi pf"><img src="../Images/d3c48a57c1410b0c19ab9d27d74e6862.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u38mYWfMC5IfKi0b2BvMAQ.png"/></div></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">当使用高λ时，第五个估计器。作者的情节。</p></figure><p id="044d" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">正如你所看到的，在L1的例子中，生成的树要简单得多，并且包含零作为权重。</p><h2 id="1a42" class="nv mc iv bd md nw nx dn mh ny nz dp ml nb oa ob mp nd oc od mt nf oe of mx og bi translated">Reg lambda</h2><p id="31ff" class="pw-post-body-paragraph kl km iv ko b kp mz kr ks kt na kv kw nb nc kz la nd ne ld le nf ng lh li lj io bi translated">现在让我们更深入地了解一下<code class="fe ot ou ov ow b">lambda</code>。你可能已经注意到，或者已经知道，如果你已经读过我以前的文章:</p><div class="lk ll gp gr lm ln"><a rel="noopener follow" target="_blank" href="/diy-xgboost-library-in-less-than-200-lines-of-python-69b6bf25e7d9"><div class="lo ab fo"><div class="lp ab lq cl cj lr"><h2 class="bd iw gy z fp ls fr fs lt fu fw iu bi translated">用不到200行python代码DIY XGBoost库</h2><div class="lu l"><h3 class="bd b gy z fp ls fr fs lt fu fw dk translated">XGBoost解释了梯度推进方法和惠普调整，通过建立自己的梯度推进库…</h3></div><div class="pg l"><p class="bd b dl z fp ls fr fs lt fu fw dk translated">towardsdatascience.com</p></div></div><div class="lv l"><div class="ph l lx ly lz lv ma kb ln"/></div></div></a></div><p id="020f" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated"><code class="fe ot ou ov ow b">lambda</code>缩放平方权重的和<code class="fe ot ou ov ow b">w_j</code>，因此在相对于<code class="fe ot ou ov ow b">w_j</code>求导时仍然出现。</p><p id="1856" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">这意味着与<code class="fe ot ou ov ow b">alpha</code>相反，<code class="fe ot ou ov ow b">lambda</code>对梯度值有影响，并隐含在最优解中:</p><figure class="oy oz pa pb gt jw gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/a72779bed5705fe63488bed14c01910c.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*XaAmpPfvhJ0c4CVb1oE6Hw.png"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">最佳重量公式。由作者创作。</p></figure><p id="3497" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">正如我在以前的文章中所述，当目标函数是平方误差时，hessian的值是2，因此<code class="fe ot ou ov ow b">H_j</code>是附加到当前节点的样本数量的两倍。</p><p id="116c" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">这意味着<code class="fe ot ou ov ow b">lambda</code>的值越大，越会降低最佳值，因为只有几个样本附加到一个节点上。</p><p id="f320" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">但这并不是<code class="fe ot ou ov ow b">lambda</code> as在训练上的唯一效果。它还具有结构效应，因为<code class="fe ot ou ov ow b">lambda</code>也隐含在最佳增益中:</p><figure class="oy oz pa pb gt jw gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/71740a95d59e44b2eff99bc25ead8dc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*K3J0pbO3ImKsBg9pH9yUzg.png"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">最佳增益公式。作者的公式。</p></figure><p id="ca45" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">提醒一下，最佳增益用于选择节点的最佳分割。具有最佳增益的分割将被保留为最佳分割。</p><p id="a93a" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">相对于样本数量而言具有大的<code class="fe ot ou ov ow b">lambda</code>也会降低增益以及给定分割被认为是最佳分割的机会。</p><p id="85e7" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">让我们用一个简单的例子来说明这一切:我们想根据整数的符号对它们进行分类。负整数应该标记为1，而负整数应该标记为-1:</p><figure class="oy oz pa pb gt jw"><div class="bz fp l di"><div class="pc pd l"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">过度拟合模型。作者代码。</p></figure><p id="c156" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">正如我们可以立即看到的，这个模型是过度拟合的，因为每个预测都有一个唯一的标签，而只需要两个标签。</p><p id="2595" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">让我们为<code class="fe ot ou ov ow b">lambda</code>使用一个正值:</p><figure class="oy oz pa pb gt jw"><div class="bz fp l di"><div class="pc pd l"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">我们现在只预测两个值。作者代码。</p></figure><p id="66ee" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">正如预期的那样，使用更大的<code class="fe ot ou ov ow b">lambda</code>值，得到的模型得到了简化。它现在只预测两个值。这是<code class="fe ot ou ov ow b">lambda</code>对生成的树的结构影响。</p><p id="8e14" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">然而，正如我们在上面看到的，<code class="fe ot ou ov ow b">lambda</code>倾向于减少权重值，因此我们不预测1和-1，而是预测更小的值。使用更大的数据集在理论上应该可以减少这种影响，has <code class="fe ot ou ov ow b">lambda</code>在样本数量方面将变得可以忽略不计。</p><figure class="oy oz pa pb gt jw"><div class="bz fp l di"><div class="pc pd l"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">作者代码。</p></figure><p id="b124" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">正如所料，更大的数据集减少了<code class="fe ot ou ov ow b">lambda</code>对权重的影响。该模型现在预测的值非常接近1和-1。</p><p id="e773" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">生成的树现在是:</p><figure class="oy oz pa pb gt jw gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi pe"><img src="../Images/04605ede3264dac33259db4c95787029.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7TMCln_J1Uj_nVkL0yeBpQ.png"/></div></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">一个简单的模型，使用正确的λ。作者的情节。</p></figure><h2 id="84dd" class="nv mc iv bd md nw nx dn mh ny nz dp ml nb oa ob mp nd oc od mt nf oe of mx og bi translated">微克</h2><p id="a0c7" class="pw-post-body-paragraph kl km iv ko b kp mz kr ks kt na kv kw nb nc kz la nd ne ld le nf ng lh li lj io bi translated">正则化函数中隐含的另一个参数是<code class="fe ot ou ov ow b">gamma</code>，正如你在上面<code class="fe ot ou ov ow b">Omega</code>的定义中看到的。</p><p id="8c3d" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">看看这个公式，当有太多的树时，<code class="fe ot ou ov ow b">gamma</code>通过惩罚目标来增加模型的复杂性。<code class="fe ot ou ov ow b">T</code>树木的总数。</p><p id="d35d" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">查看针对给定拆分添加两个新节点所带来的增益公式也很有趣:</p><figure class="oy oz pa pb gt jw gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi pk"><img src="../Images/8d899a6236fb463c6935b403ce30eadf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*skq2J-IlpWUVB6R_GaTo3A.png"/></div></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">非零伽玛会产生负增益。作者的公式。</p></figure><p id="0f18" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">该公式表明，只有当gamma为空时，两个新节点的增益大于原始节点的增益，拆分才会产生正增益。</p><p id="5772" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">使用非空伽马增加了另一个约束:增益必须大于<code class="fe ot ou ov ow b">gamma</code>，否则增益将为负。</p><p id="38ee" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">让我们看看上一篇关于对负整数和正整数进行分类的文章，如何使用<code class="fe ot ou ov ow b">gamma</code>来获得一个合适的模型:</p><figure class="oy oz pa pb gt jw"><div class="bz fp l di"><div class="pc pd l"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">不使用和使用gamma训练模型。作者代码。</p></figure><p id="ce74" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">第一个模型是用<code class="fe ot ou ov ow b">gamma = 0</code>训练的，我们可以看到模型在预测中过度拟合。我们不是只有两个值，而是每个输入都有不同的值。</p><p id="84f5" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">生成的树太复杂了:</p><figure class="oy oz pa pb gt jw gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi pe"><img src="../Images/ade1fbdebfa0551a1396558f73d2d7aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V5B1-jF5_hoGERSVxYbGFg.png"/></div></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">当gamma为空时，模型复杂性不受控制。作者的情节。</p></figure><p id="ec7b" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">相反，第二个模型使用<code class="fe ot ou ov ow b">gamma = 1</code>，这意味着当且仅当增益大于1时，新的级别被添加到树中。</p><p id="f8e3" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">预测表明，这一次我们有了一个合理的模型，因为我们只预测了两个不同的值:一个正值和一个负值。</p><p id="547a" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">绘制结果树证实了:</p><figure class="oy oz pa pb gt jw gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/94388e496db67699e1201822e0e56aac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*-Z7T1kuCJ7OJvkMKtf5rpg.png"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">当gamma为非空时，得到的树被简化。作者的情节。</p></figure></div><div class="ab cl oh oi hz oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="io ip iq ir is"><h1 id="27b0" class="mb mc iv bd md me oo mg mh mi op mk ml mm oq mo mp mq or ms mt mu os mw mx my bi translated">结论</h1><p id="cdd6" class="pw-post-body-paragraph kl km iv ko b kp mz kr ks kt na kv kw nb nc kz la nd ne ld le nf ng lh li lj io bi translated">Alpha、Lambda和gamma是驱动决策树构建的三个重要参数。</p><p id="502f" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">总的来说，使用<code class="fe ot ou ov ow b">alpha</code>生成稀疏树，这将强调主要特征并丢弃其他特征。这导致了简单的树</p><p id="17c2" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated"><code class="fe ot ou ov ow b">lambda</code>用于剔除数量不足或至少重量不足的样品。</p><p id="7ebf" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw nb ky kz la nd lc ld le nf lg lh li lj io bi translated">最后，<code class="fe ot ou ov ow b">gamma</code>作为对树中节点数量的直接影响。</p></div></div>    
</body>
</html>