<html>
<head>
<title>How to write a Custom Keras model so that it can be deployed for Serving</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何编写定制的Keras模型，以便可以部署它来提供服务</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-write-a-custom-keras-model-so-that-it-can-be-deployed-for-serving-7d81ace4a1f8#2022-04-15">https://towardsdatascience.com/how-to-write-a-custom-keras-model-so-that-it-can-be-deployed-for-serving-7d81ace4a1f8#2022-04-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c161" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何将定制层、建模、丢失、预处理、后处理改编成可服务的API</h2></div><p id="9d89" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您编写的唯一Keras模型是带有预建层(如Dense和Conv2D)的顺序模型或功能模型，则可以忽略本文。但是在你ML生涯的某个时候，你会发现你是在子类化一个层或者一个模型。或者自己编写损失函数，或者在服务过程中需要自定义预处理或后处理。在这一点上，你会发现你不能很容易地使用</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="dbe9" class="lk ll iq lg b gy lm ln l lo lp">model.save(EXPORT_PATH)</span></pre><p id="35b6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">即使您能够成功保存模型，您可能会发现模型无法成功部署到TensorFlow服务或包装TF服务的Vertex AI或Sagemaker等受管服务中。即使您成功地部署了它，您可能会发现结果不直观或者完全错误。</p><p id="7ae6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不幸的是，解释您必须做什么的文档分散在多个页面中。一些推荐的方法将会工作，如果你做的一切都是正确的，但不会报告错误，一些方法会导致训练中的戏剧性减速，另一些方法在服务中不灵活，一些方法会导致模型保存花费几个小时，并且通常错误消息可能很难理解。</p><figure class="lb lc ld le gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lq"><img src="../Images/0b961ac3515bf8558160cdb8f000fd6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cBelOXs2nVjO6_-mwCNRhA.jpeg"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图片由<a class="ae mc" href="https://pixabay.com/users/congerdesign-509903/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1976725" rel="noopener ugc nofollow" target="_blank">康格设计</a>来自<a class="ae mc" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1976725" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="41c5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章是关于如果你在Keras中有一个自定义的东西(层，模型，Lambda，Loss，预处理器，后处理器)你必须做什么。</p><h2 id="6ab5" class="lk ll iq bd md me mf dn mg mh mi dp mj ko mk ml mm ks mn mo mp kw mq mr ms mt bi translated">示例模型</h2><p id="bb26" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">为了说明，我将使用Keras示例中的命名实体识别<a class="ae mc" href="https://keras.io/examples/nlp/ner_transformers/" rel="noopener ugc nofollow" target="_blank"> (NER)模型。基本上，一个被训练来识别名字和位置的NER会使用以下形式的句子:</a></p><p id="e28d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">约翰去了巴黎</p><p id="1027" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并返回:</p><p id="45af" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mz">说出位置</em></p><p id="c001" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个模型本身如何工作并不重要。只是它涉及自定义Keras层和自定义Keras模型(即，它们涉及子类层。层和keras。型号):</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="7d5c" class="lk ll iq lg b gy lm ln l lo lp">class TransformerBlock(layers.Layer):<br/>...</span><span id="0b65" class="lk ll iq lg b gy na ln l lo lp">class TokenAndPositionEmbedding(layers.Layer):<br/>...</span><span id="5ce8" class="lk ll iq lg b gy na ln l lo lp">class NERModel(keras.Model):<br/>...</span></pre><p id="2be7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文附带的<a class="ae mc" href="https://github.com/lakshmanok/lakblogs/blob/main/deployable_ner.ipynb" rel="noopener ugc nofollow" target="_blank">完整代码在GitHub </a>上。</p><p id="85fe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种NLP模型必须对输入文本进行一些定制的预处理。基本上，输入的句子被拆分成单词，小写，然后转换成基于词汇表的索引。该模型在词汇id上被训练:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="f5f9" class="lk ll iq lg b gy lm ln l lo lp">def map_record_to_training_data(record):<br/>    record = tf.strings.split(record, sep="\t")</span><span id="5fb7" class="lk ll iq lg b gy na ln l lo lp">    tokens = tf.strings.split(record[0])<br/>    tokens = tf.strings.lower(tokens)<br/>    tokens = vocab_lookup_layer(tokens)</span><span id="82ca" class="lk ll iq lg b gy na ln l lo lp">    tags = tf.strings.split(record[1])<br/>    tags = label_lookup_layer(tags)<br/>    return tokens, tags</span></pre><p id="3493" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输入tf.data管道是:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="b498" class="lk ll iq lg b gy lm ln l lo lp">train_dataset = (<br/>    tf.data.TextLineDataset('train.csv')<br/>    .map(map_record_to_training_data)<br/>    .padded_batch(batch_size)<br/>)</span></pre><p id="ab0a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并且用定制损失来训练模型:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="40bf" class="lk ll iq lg b gy lm ln l lo lp">loss = CustomNonPaddingTokenLoss()<br/>ner_model.compile(optimizer="adam", loss=loss)<br/>ner_model.fit(train_dataset, epochs=10)</span></pre><p id="64db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以明白我为什么选择这个例子——它有自定义*一切*。</p><h2 id="b0e4" class="lk ll iq bd md me mf dn mg mh mi dp mj ko mk ml mm ks mn mo mp kw mq mr ms mt bi translated">出口</h2><p id="01eb" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">当我们试图导出这个模型来部署它时会发生什么？通常，这是它将涉及的内容:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="05a8" class="lk ll iq lg b gy lm ln l lo lp">model.save(EXPORT_PATH)</span></pre><p id="cd50" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我们将保存的模型交给Sagemaker或Vertex AI等服务，它会:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="3bda" class="lk ll iq lg b gy lm ln l lo lp">model = saved_model.load_model(EXPORT_PATH)<br/>model.predict(...)</span></pre><p id="aa4b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不幸的是，由于上面所有的自定义层和代码，这种stratightforward方法行不通。</p><p id="7e5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们这样做时:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="9a1e" class="lk ll iq lg b gy lm ln l lo lp">ner_model.save(EXPORT_PATH)</span></pre><p id="f757" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们得到一个错误:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="6bfe" class="lk ll iq lg b gy lm ln l lo lp">Unknown loss function: CustomNonPaddingTokenLoss. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.</span></pre><h2 id="d41c" class="lk ll iq bd md me mf dn mg mh mi dp mj ko mk ml mm ks mn mo mp kw mq mr ms mt bi translated">问题1:未知损失函数</h2><p id="8f93" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">我们如何解决这个问题？我将在本笔记本的后面向您展示如何注册自定义对象。但是首先要意识到的是<strong class="kh ir">我们不需要导出损失</strong>。损失只是训练需要，部署不需要。</p><p id="c3dc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，我们可以做一件更简单的事情。只是消除损失:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="c4eb" class="lk ll iq lg b gy lm ln l lo lp"># remove the custom loss before saving.<br/>ner_model.compile('adam', loss=None)<br/>ner_model.save(EXPORT_PATH)</span></pre><p id="b568" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">成功！一句话:<strong class="kh ir">在导出Keras模型进行部署之前，消除定制损失。</strong>这是一行程序。</p><h2 id="b898" class="lk ll iq bd md me mf dn mg mh mi dp mj ko mk ml mm ks mn mo mp kw mq mr ms mt bi translated">问题2:错误的输入形状</h2><p id="3f12" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">现在，让我们做TensorFlow Serving(或包装TensorFlow Serving的托管服务，如Sagemaker或Keras)所做的事情:调用我们刚刚加载的模型的predict()方法:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="9758" class="lk ll iq lg b gy lm ln l lo lp">sample_input = [<br/>     "Justin Trudeau went to New Delhi India",<br/>     "Vladimir Putin was chased out of Kyiv Ukraine"<br/>]<br/>model.predict(sample_input)</span></pre><p id="133d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不幸的是，我们得到一个错误:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="2d0b" class="lk ll iq lg b gy lm ln l lo lp">Could not find matching concrete function to call loaded from the SavedModel. Got:<br/>        * Tensor("inputs:0", shape=(None,), dtype=<strong class="lg ir">string</strong>)</span><span id="4a27" class="lk ll iq lg b gy na ln l lo lp">Expected:<br/>        * TensorSpec(shape=(None, None), dtype=tf.<strong class="lg ir">int64</strong>, name='inputs')</span></pre><p id="e952" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">捕捉预处理</strong></p><p id="2a85" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是因为我们试图发送一个完整的句子(一个字符串)，但我们的模型是在一组词汇id上训练的。这就是为什么期望的输入是一组int64。</p><p id="5e2f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们调用tf.strings.split()、tf.strings.lower和vocab_lookup_layer()时，我们在tf.data()管道中做到了这一点:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="4269" class="lk ll iq lg b gy lm ln l lo lp">def map_record_to_training_data(record):<br/>    record = tf.strings.split(record, sep="\t")</span><span id="ca29" class="lk ll iq lg b gy na ln l lo lp">    tokens = tf.strings.split(record[0])<br/>    tokens = tf.strings.lower(tokens)<br/>    tokens = vocab_lookup_layer(tokens)</span><span id="323d" class="lk ll iq lg b gy na ln l lo lp">...</span></pre><p id="3ae6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在预测过程中，我们也必须重复这种预处理。</p><p id="9c5d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">怎么会？</p><p id="73bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">嗯，我们可以在顶点AI上使用一个<a class="ae mc" href="https://cloud.google.com/blog/topics/developers-practitioners/add-preprocessing-functions-tensorflow-models-and-deploy-vertex-ai" rel="noopener ugc nofollow" target="_blank">预处理容器</a>或者一些类似的功能。但是这有点违背了我们拥有一个简单的、全方位部署的Keras模型的目的。</p><p id="c72b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">相反，我们应该重组我们的tf.data输入管道。我们想要的是有一个函数(在这里，我称之为process_descr ),我们可以从tf.data管道和我们导出的模型中调用它:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="f923" class="lk ll iq lg b gy lm ln l lo lp">def process_descr(descr):<br/>  # split the string on spaces, and make it a rectangular tensor<br/>  tokens = tf.strings.split(tf.strings.lower(descr))<br/>  tokens = vocab_lookup_layer(tokens)<br/>  max_len = MAX_LEN # max([x.shape[0] for x in tokens])<br/>  input_words = tokens.to_tensor(default_value=0, shape=[tf.rank(tokens), max_len])<br/>  return input_words</span></pre><p id="f7bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这使得我们的训练代码可以像以前一样工作。当我们准备好保存它时，我们需要用模型中包含的预处理代码创建一个层。</p><p id="4353" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一种方法是定义一个调用预处理函数的预测签名。然而，这是有问题的，因为如果您的定制模型中有错误，您将不会知道这些错误(问我我是如何知道的)。</p><p id="747b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">带预处理层的新模型</strong></p><p id="65f0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个更简单的方法是，告诉你错误并给你一个机会去修正它们，就像我们对损失函数所做的那样。定义一个新的标准模型，它有一个lambda层，在将它提供给定制模型之前进行预处理，并将其写出。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="9685" class="lk ll iq lg b gy lm ln l lo lp">temp_model = tf.keras.Sequential([<br/>  tf.keras.Input(shape=[], dtype=tf.string, name='description'),<br/>  tf.keras.layers.Lambda(process_descr),<br/>  ner_model                            <br/>])<br/>temp_model.compile('adam', loss=None)<br/>temp_model.save(EXPORT_PATH)<br/>!ls -l {EXPORT_PATH}</span></pre><p id="9af1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不幸的是，前面提到的未报告的错误现在出现了。什么错误？</p><h2 id="f7d9" class="lk ll iq bd md me mf dn mg mh mi dp mj ko mk ml mm ks mn mo mp kw mq mr ms mt bi translated">问题3:未追踪张量</h2><p id="8dcd" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">我们得到的错误消息是:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="75d5" class="lk ll iq lg b gy lm ln l lo lp">Tried to export a function which references 'untracked' resource Tensor</span></pre><p id="2cad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是怎么回事？这里的问题是:当你写一个自定义的Keras层或Keras损失或Keras模型，你是在定义代码。但是，当您导出模型时，您必须用它创建一个平面文件。代码会发生什么变化？丢了！那么预测是如何进行的呢？</p><p id="fd46" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你需要告诉Keras如何传入所有的构造函数参数等等。然后，Keras将处理代码，恢复对象，并做正确的事情。</p><p id="175c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">方法是定义一个getConfig()方法，该方法包含所有的构造函数参数。基本上，自定义图层如下所示:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="03be" class="lk ll iq lg b gy lm ln l lo lp">class TokenAndPositionEmbedding(layers.Layer):<br/>    def __init__(self, maxlen, vocab_size, embed_dim):<br/>        super(TokenAndPositionEmbedding, self).__init__()<br/>        self.token_emb = keras.layers.Embedding(<br/>            input_dim=vocab_size, output_dim=embed_dim<br/>        )<br/>        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)<br/><br/>    def call(self, inputs):<br/>        maxlen = tf.shape(inputs)[-1]<br/>        positions = tf.range(start=0, limit=maxlen, delta=1)<br/>        position_embeddings = self.pos_emb(positions)<br/>        token_embeddings = self.token_emb(inputs)<br/>        return token_embeddings + position_embeddings</span></pre><p id="399b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">必须是这样的:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="be93" class="lk ll iq lg b gy lm ln l lo lp"><strong class="lg ir">@tf.keras.utils.register_keras_serializable() # 1</strong><br/>class TokenAndPositionEmbedding(layers.Layer):<br/>    def __init__(self, maxlen, vocab_size, embed_dim, <strong class="lg ir">**kwargs): # 2</strong><br/>        super(TokenAndPositionEmbedding, self).__init__(<strong class="lg ir">**kwargs) # 3</strong><br/>        self.token_emb = keras.layers.Embedding(<br/>            input_dim=vocab_size, output_dim=embed_dim<br/>        )<br/><br/>        <strong class="lg ir">#4 save the constructor parameters for get_config()<br/>        self.maxlen = maxlen<br/>        self.vocab_size = vocab_size<br/>        self.embed_dim = embed_dim</strong><br/><br/>        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)<br/><br/>    def call(self, inputs):<br/>        maxlen = tf.shape(inputs)[-1]<br/>        positions = tf.range(start=0, limit=maxlen, delta=1)<br/>        position_embeddings = self.pos_emb(positions)<br/>        token_embeddings = self.token_emb(inputs)<br/>        return token_embeddings + position_embeddings<br/><br/>   <strong class="lg ir"> def get_config(self): # 5<br/>        config = super().get_config()<br/>        # save constructor args<br/>        config['maxlen'] = self.maxlen<br/>        config['vocab_size'] = self.vocab_size<br/>        config['embed_dim'] = self.embed_dim<br/>        return config</strong></span></pre><p id="7d44" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有5个变化:</p><ol class=""><li id="70e8" class="nb nc iq kh b ki kj kl km ko nd ks ne kw nf la ng nh ni nj bi translated">添加注记以向Keras注册自定义图层</li><li id="503e" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">将**kwargs添加到构造函数参数中</li><li id="7af4" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">向超级构造函数添加一个**kwargs</li><li id="fb63" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">将构造函数参数保存为实例字段</li><li id="80da" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">定义一个保存构造函数参数的get_config方法</li></ol><p id="eb59" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我们对我们的自定义层和模型类这样做了(查看GitHub中的笔记本以获得完整代码)，我们仍然无法保存。错误消息与之前完全相同。我们有一个未追踪的张量。但是我们只是完成了所有的定制课程，做了正确的事情。发生什么事了？</p><h2 id="a4dc" class="lk ll iq bd md me mf dn mg mh mi dp mj ko mk ml mm ks mn mo mp kw mq mr ms mt bi translated">问题Lambda层中未跟踪的资源</h2><p id="7e04" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">我们还有一个问题。即使我们检查并修复了所有的定制层和模型，仍然有一段用户定义的代码。</p><p id="2e9d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们用于预处理的Lamda层！它使用词汇表，并且vocab_lookup_layer是一个未被跟踪的资源:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="21fb" class="lk ll iq lg b gy lm ln l lo lp">def process_descr(descr):<br/>  # split the string on spaces, and make it a rectangular tensor<br/>  tokens = tf.strings.split(tf.strings.lower(descr))<br/>  tokens = <strong class="lg ir">vocab_lookup_layer</strong>(tokens)<br/>  max_len = MAX_LEN # max([x.shape[0] for x in tokens])<br/>  input_words = tokens.to_tensor(default_value=0, shape=[tf.rank(tokens), max_len])<br/>  return input_words</span></pre><p id="ac3c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">仅仅修改函数并不能解决这个问题。</p><p id="7833" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一句话:Lambda层是危险的，很难意识到我们忘记了什么资源。</p><p id="74a4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我建议你去掉所有的Lambda图层，用自定义图层来代替。</p><p id="178e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们这样做吧，确保记住每个自定义层都需要的5个步骤:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="76c5" class="lk ll iq lg b gy lm ln l lo lp"><a class="ae mc" href="http://twitter.com/tf" rel="noopener ugc nofollow" target="_blank">@tf</a>.keras.utils.register_keras_serializable(name='descr')<br/>class PreprocLayer(layers.Layer):<br/>    def __init__(self, vocab_lookup_layer, **kwargs):<br/>        super(PreprocLayer, self).__init__(**kwargs)</span><span id="5c28" class="lk ll iq lg b gy na ln l lo lp">        # save the constructor parameters for get_config() to work properly<br/>       <strong class="lg ir"> self.vocab_lookup_layer = vocab_lookup_layer</strong></span><span id="b817" class="lk ll iq lg b gy na ln l lo lp">   def call(self, descr, training=False):<br/>        # split the string on spaces, and make it a rectangular tensor<br/>        tokens = tf.strings.split(tf.strings.lower(descr))<br/>        tokens = <strong class="lg ir">self.vocab_lookup_layer(tokens)</strong><br/>        max_len = MAX_LEN # max([x.shape[0] for x in tokens])<br/>        input_words = tokens.to_tensor(default_value=0, shape=[tf.rank(tokens), max_len])<br/>        return input_words</span><span id="d8c8" class="lk ll iq lg b gy na ln l lo lp">    def get_config(self):<br/>        config = super().get_config()<br/>        # save constructor args<br/>        <strong class="lg ir">config['vocab_lookup_layer'] = self.vocab_lookup_layer</strong><br/>        return config</span></pre><p id="d6c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们的临时储蓄模型变成了:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="ce28" class="lk ll iq lg b gy lm ln l lo lp">temp_model = tf.keras.Sequential([<br/>  tf.keras.Input(shape=[], dtype=tf.string, name='description'),<br/>  PreprocLayer(vocab_lookup_layer),<br/>  ner_model                            <br/>])<br/>temp_model.compile('adam', loss=None)<br/>temp_model.save(EXPORT_PATH)<br/>!ls -l {EXPORT_PATH}</span></pre><p id="e861" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当然，您应该返回并修复您的tf.data输入管道，以使用层而不是process_descr函数。幸运的是，这很容易。只需将process_descr()调用替换为对PreprocLayer()的调用:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="1de3" class="lk ll iq lg b gy lm ln l lo lp">PreprocLayer(vocab_lookup_layer)(['Joe Biden visited Paris'])</span></pre><p id="91ac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">做的事情和:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="ace1" class="lk ll iq lg b gy lm ln l lo lp">process_descr(['Joe Biden visited Paris'])</span></pre><h2 id="fbe2" class="lk ll iq bd md me mf dn mg mh mi dp mj ko mk ml mm ks mn mo mp kw mq mr ms mt bi translated">后加工</h2><p id="d22d" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">现在，当我们加载模型并调用predict时，我们得到了正确的行为:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="a37d" class="lk ll iq lg b gy lm ln l lo lp">model = tf.keras.models.load_model(EXPORT_PATH)<br/>sample_input = [<br/>     "Justin Trudeau went to New Delhi India",<br/>     "Vladimir Putin was chased out of Kyiv Ukraine"<br/>]<br/>model.predict(sample_input)</span></pre><p id="284d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不过，这会返回一组概率:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="f62a" class="lk ll iq lg b gy lm ln l lo lp">array([[[7.6006036e-03, 4.3546227e-03, 9.7820580e-01, 1.3501652e-03,<br/>         5.0268644e-03, 3.4619651e-03],<br/>        [6.8284925e-03, 1.7240658e-02, 9.1373536e-04, 9.6674633e-01,<br/>         5.9596724e-03, 2.3111277e-03],</span></pre><p id="6d62" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">真烦人。我们能对输出进行后处理吗？当然可以。我们必须找到这个数组的argmax，然后查找对应于这个索引的标签。例如，如果第二项是最大概率，我们将得到B-NAME映射[1]。</p><p id="f901" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">既然我们在Keras down中有了自定义代码，那么应用自定义层方法就很简单了:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="acaa" class="lk ll iq lg b gy lm ln l lo lp"><a class="ae mc" href="http://twitter.com/tf" rel="noopener ugc nofollow" target="_blank">@tf</a>.keras.utils.register_keras_serializable(name='tagname')<br/>class OutputTagLayer(layers.Layer):<br/>    def __init__(self, mapping, **kwargs):<br/>        super(OutputTagLayer, self).__init__(**kwargs)</span><span id="6130" class="lk ll iq lg b gy na ln l lo lp">        # save the constructor parameters for get_config() to work properly<br/>        self.mapping = mapping</span><span id="7dcd" class="lk ll iq lg b gy na ln l lo lp">        # construct<br/>        self.mapping_lookup = tf.lookup.StaticHashTable(<br/>            tf.lookup.KeyValueTensorInitializer( <br/>                tf.range(start=0, limit=len(mapping.values()), delta=1, dtype=tf.int64),<br/>                tf.constant(list(mapping.values()))),<br/>                default_value='[PAD]')</span><span id="f761" class="lk ll iq lg b gy na ln l lo lp">    def call(self, descr_tags, training=False):<br/>        prediction = tf.argmax(descr_tags, axis=-1)<br/>        prediction = self.mapping_lookup.lookup(prediction)<br/>        return prediction</span><span id="a4a1" class="lk ll iq lg b gy na ln l lo lp">    def get_config(self):<br/>        config = super().get_config()<br/>        # save constructor args<br/>        config['mapping'] = self.mapping<br/>        return config</span></pre><p id="8a15" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是那个代码呢？那是什么@#$@R$@#啊？</p><p id="e17c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在Python中，我们可以简单地做:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="d965" class="lk ll iq lg b gy lm ln l lo lp">mapping[ np.argmax(prob) ]</span></pre><p id="ce70" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们必须进行映射。查找:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="7168" class="lk ll iq lg b gy lm ln l lo lp">prediction = tf.argmax(descr_tags, axis=-1)<br/>prediction = self.mapping_lookup.lookup(prediction)</span></pre><p id="594a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">mapping_lookup本身就是一种资源，它是一个与dict:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="4c7a" class="lk ll iq lg b gy lm ln l lo lp">self.mapping_lookup = tf.lookup.StaticHashTable(<br/>            tf.lookup.KeyValueTensorInitializer( <br/>                tf.range(start=0, limit=len(mapping.values()), delta=1, dtype=tf.int64),<br/>                tf.constant(list(mapping.values()))),<br/>                default_value='[PAD]')</span></pre><p id="a3b1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在撰写本文时，TensorFlow在存储关键字为整数的dicts时有一个bug，所以我正在入侵对tf.range()的调用。抱歉。</p><p id="d9f6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是这样做的好处是，您可以简单地采用这个模型并部署它。没有额外的预处理容器、后处理容器等。代码全部运行在GPU上(加速了！)而且速度超快。</p><p id="6c1f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">也很直观。我们请求一个字符串并返回识别出的单词。发送内容:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="2c99" class="lk ll iq lg b gy lm ln l lo lp">model = tf.keras.models.load_model(EXPORT_PATH)<br/>sample_input = [<br/>     "Justin Trudeau went to New Delhi India",<br/>     "Vladimir Putin was chased out of Kyiv Ukraine"<br/>]<br/>model.predict(sample_input)</span></pre><p id="d721" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">回馈:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="278b" class="lk ll iq lg b gy lm ln l lo lp">array([[b'B-NAME', b'I-NAME', b'OUT', b'OUT', b'B-LOCATION',<br/>        b'I-LOCATION', b'I-LOCATION', b'[PAD]', b'[PAD]', b'[PAD]',<br/>        b'[PAD]', b'[PAD]', b'[PAD]', b'[PAD]', b'[PAD]', b'[PAD]'],<br/>       [b'B-NAME', b'I-NAME', b'OUT', b'OUT', b'OUT', b'OUT',<br/>        b'B-LOCATION', b'I-LOCATION', b'[PAD]', b'[PAD]', b'[PAD]',<br/>        b'[PAD]', b'[PAD]', b'[PAD]', b'[PAD]', b'[PAD]']], dtype=object)</span></pre><p id="d70e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">即所提供的句子中每个单词的标签。</p><p id="2669" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽情享受吧！</p><h2 id="8a71" class="lk ll iq bd md me mf dn mg mh mi dp mj ko mk ml mm ks mn mo mp kw mq mr ms mt bi translated">推荐阅读:</h2><ol class=""><li id="a56b" class="nb nc iq kh b ki mu kl mv ko np ks nq kw nr la ng nh ni nj bi translated"><a class="ae mc" href="https://github.com/lakshmanok/lakblogs/blob/main/deployable_ner.ipynb" rel="noopener ugc nofollow" target="_blank">我的完整代码在GitHub </a>上。为了可读性，我省略了本文中的一些细节。请务必查阅笔记本。您可以在Colab或任何Jupyter环境中运行它。</li><li id="3c48" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">Keras示例中的命名实体识别<a class="ae mc" href="https://keras.io/examples/nlp/ner_transformers/" rel="noopener ugc nofollow" target="_blank"> (NER)模型</a>。很好的说明模型。但是您将无法部署它。</li><li id="8c7c" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">点击此处阅读关于<a class="ae mc" href="https://keras.io/guides/making_new_layers_and_models_via_subclassing/" rel="noopener ugc nofollow" target="_blank"> Keras定制图层</a>的所有信息。不要错过关于序列化的“可选”部分。如果您想要部署自定义层，这是强制性的。</li><li id="c263" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">一旦你写了你的自定义层，你必须做<a class="ae mc" href="https://keras.io/guides/serialization_and_saving/" rel="noopener ugc nofollow" target="_blank">自定义对象注册</a>。但是注册定制对象很容易出错。您可能会忘记一些，错误消息不会告诉您错过了哪一个。最好使用<a class="ae mc" href="https://keras.io/api/utils/serialization_utils/#registerkerasserializable-function" rel="noopener ugc nofollow" target="_blank">标注快捷方式</a>。希望你看到了——这是那一页上讨论的第三个选项。</li><li id="7907" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">在这里阅读关于λ层的内容。没有任何关于导出带有Lambda层的模型的陷阱，这些Lambda层中有全局对象。希望您阅读了前面关于序列化模型的章节，并意识到它们同样适用于Lambda层包装的函数！</li></ol><p id="948e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">抱歉，打扰了。</p></div></div>    
</body>
</html>