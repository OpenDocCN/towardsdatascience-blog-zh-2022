<html>
<head>
<title>Primer on Cleaning Text Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">清理文本数据入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/primer-to-cleaning-text-data-7e856d6e5791#2022-09-02">https://towardsdatascience.com/primer-to-cleaning-text-data-7e856d6e5791#2022-09-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8b61" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">清洗文本是自然语言处理预处理的一个重要部分</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/79002ef8101df35bc463f491fe2531c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AZqJ7x_BbT2md62d"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自<a class="ae ky" href="https://www.pexels.com/ko-kr/photo/macbook-pro-1181373/" rel="noopener ugc nofollow" target="_blank">像素</a>的免费使用照片</p></figure><h1 id="4e62" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="c6d4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在自然语言处理(NLP)领域，预处理是一个重要的阶段，在这里进行文本清理、词干提取、词汇化和词性标注等工作。在NLP预处理的这些不同方面中，我将涵盖我们可以应用的文本清理方法的综合列表。这里的文本清理指的是移除或转换文本的某些部分，以便文本变得更容易被正在学习文本的NLP模型理解的过程。这通常通过减少文本数据中的噪声使NLP模型表现得更好。</p><h1 id="3796" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">将所有字符转换成小写</h1><p id="419c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">string包(Python中的默认包)包含各种有用的字符串函数。lower函数就是其中之一，把所有字符都变成小写。</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="6109" class="ms la it mo b gy mt mu l mv mw"><strong class="mo iu">def </strong>make_lowercase(token_list):<br/>    # Assuming word <a class="ae ky" href="https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html" rel="noopener ugc nofollow" target="_blank">tokenization</a> already happened</span><span id="8f10" class="ms la it mo b gy mx mu l mv mw">    # Using list comprehension --&gt; loop through every word/token, make it into lower case and add it to a new list<br/>    words = [word.lower() for word in token_list]    </span><span id="eb5e" class="ms la it mo b gy mx mu l mv mw">    # join lowercase tokens into one string<br/>    cleaned_string = " ".join(words) <br/>    return cleaned_string</span></pre><h1 id="5ddd" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">删除标点符号</h1><p id="9243" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Python中的string.punctuation(就是前面提到的包)包含以下几项标点符号。</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="ea72" class="ms la it mo b gy mt mu l mv mw">#$%&amp;\’()*+,-./:;?@[\\]^_{|}~`</span><span id="1de6" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">import </strong>string</span><span id="5b79" class="ms la it mo b gy mx mu l mv mw">text = "It was a great night! Shout out to @Amy Lee for organizing wonderful event (a.k.a. on fire)."</span><span id="3ff3" class="ms la it mo b gy mx mu l mv mw">PUNCT_TO_REMOVE = string.punctuation</span><span id="a440" class="ms la it mo b gy mx mu l mv mw">ans = text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))</span><span id="76a1" class="ms la it mo b gy mx mu l mv mw">ans<br/>&gt;&gt; "It was a great night Shout out to Amy Lee for organizing wonderful event aka on fire"</span></pre><p id="72c1" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">字符串包中的另一个方法translate函数使用输入字典来执行映射。maketrans函数是translate函数的兄弟方法，它创建用作translate方法输入的字典。请注意，maketrans函数接受3个参数，如果总共传递了3个参数，则第三个参数中的每个字符都被映射为None。这个特性可以用来删除字符串中的字符。</p><p id="1e11" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">根据上面的代码片段，我们将maketrans函数的第一个和第二个参数指定为空字符串(因为我们不需要这些参数)，并将第三个参数指定为上面string.punctuation中定义的标点项。然后，存储在变量<em class="nd">文本</em>中的字符串中的标点符号将被删除。</p><h1 id="6f16" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">删除号码</h1><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="7cdb" class="ms la it mo b gy mt mu l mv mw">text = "My cell phone number is 123456. Please take note."</span><span id="a780" class="ms la it mo b gy mx mu l mv mw">text_cleaned = ''.join([i for i in text if not i.isdigit()])</span><span id="f3ec" class="ms la it mo b gy mx mu l mv mw">text_cleaned<br/>&gt;&gt; "My cell phone number is. Please take note."</span></pre><p id="f1f2" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">你也可以使用正则表达式做同样的事情，它是字符串操作最好的朋友之一。</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="ab46" class="ms la it mo b gy mt mu l mv mw">text_cleaned = [re.sub(r’\w*\d\w*’, ‘’, w) for w in text]</span><span id="5de7" class="ms la it mo b gy mx mu l mv mw">text_cleaned<br/>&gt;&gt; "My cell phone number is. Please take note."</span></pre><h1 id="20aa" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">移除表情符号</strong></h1><p id="9f5f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">随着各种社交媒体平台生成的非结构化文本数据的数量不断增加，更多的文本数据包含非典型字符，如表情符号。表情符号可能很难被机器理解，并且可能会给你的NLP模型添加不必要的噪声。从文本数据中删除表情符号就是这种情况。然而，如果你试图进行情感分析，尝试将表情符号转换成某种文本格式而不是彻底删除它们可能是有益的，因为表情符号可以包含与手头文本相关的情感的有用信息。一种方法是创建您自己的自定义词典，将不同的表情符号映射到一些表示与表情符号相同情感的文本(例如{🔥:火})。</p><p id="985a" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">看看这篇<a class="ae ky" href="https://www.analyticsvidhya.com/blog/2022/01/text-cleaning-methods-in-nlp/" rel="noopener ugc nofollow" target="_blank">帖子</a>，它展示了如何从你的文本中删除表情符号。</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="1a66" class="ms la it mo b gy mt mu l mv mw"><strong class="mo iu">import </strong>re</span><span id="7e56" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">def </strong>remove_emoji(string):</span><span id="c6a8" class="ms la it mo b gy mx mu l mv mw">     emoji_pattern = re.compile(“[“</span><span id="e534" class="ms la it mo b gy mx mu l mv mw">     u”U0001F600-U0001F64F” # emoticons</span><span id="1bf8" class="ms la it mo b gy mx mu l mv mw">     u”U0001F300-U0001F5FF” # symbols &amp; pictographs</span><span id="3c9c" class="ms la it mo b gy mx mu l mv mw">     u”U0001F680-U0001F6FF” # transport &amp; map symbols</span><span id="1d9a" class="ms la it mo b gy mx mu l mv mw">     u”U0001F1E0-U0001F1FF” # flags (iOS)</span><span id="6d22" class="ms la it mo b gy mx mu l mv mw">     u”U00002702-U000027B0"</span><span id="c2b4" class="ms la it mo b gy mx mu l mv mw">     u”U000024C2-U0001F251"</span><span id="7b72" class="ms la it mo b gy mx mu l mv mw">     “]+”, flags=re.UNICODE)</span><span id="cf17" class="ms la it mo b gy mx mu l mv mw">     return emoji_pattern.sub(r’’, string)</span><span id="4e6a" class="ms la it mo b gy mx mu l mv mw">remove_emoji(“game is on 🔥🔥”)</span><span id="11b2" class="ms la it mo b gy mx mu l mv mw">&gt;&gt; 'game is on '</span></pre><h1 id="8ad4" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">拼出宫缩</strong></h1><p id="b3e7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">python中的收缩包(需要使用！pip安装收缩)允许我们拼出收缩。通过在执行标记化时创建更多的标记，拼出缩写可以为文本数据添加更多的信息。例如，在下面的代码片段中，当执行基于空白的单词标记化时，标记“would”不会被视为单独的标记。相反，它是象征“她愿意”的一部分。但是，一旦我们修复了缩写，我们就会看到，在执行单词标记化时，单词“would”作为一个独立的标记存在。这为NLP模型添加了更多的令牌以供使用。这可以帮助模型更好地理解文本的含义，从而提高各种NLP任务的准确性。</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="d3c9" class="ms la it mo b gy mt mu l mv mw"><strong class="mo iu">import </strong>contractions</span><span id="453f" class="ms la it mo b gy mx mu l mv mw">text = “She<strong class="mo iu">'d</strong> like to hang out with you sometime!”</span><span id="169d" class="ms la it mo b gy mx mu l mv mw">contractions.fix(text)</span><span id="a95a" class="ms la it mo b gy mx mu l mv mw">&gt;&gt; “She <strong class="mo iu">would </strong>like to hang out with you sometime!”</span></pre><p id="a0c6" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">但是由于这个包可能不是100%全面的(即没有覆盖存在的每一个缩写)，您也可以创建自己的自定义字典，将包没有覆盖的某些缩写映射到这些缩写的拼写版本。这篇<a class="ae ky" href="https://studymachinelearning.com/text-data-cleaning-preprocessing/" rel="noopener ugc nofollow" target="_blank">帖子</a>向你展示了如何做到这一点的例子！</p><h1 id="026d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">剥离HTML标签</h1><p id="9112" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们使用Python的BeautifulSoup包来剥离HTML标签。这个包是用于网页抓取的，但是它的html解析器工具可以用来剥离HTML标签，如下所示！</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="6be3" class="ms la it mo b gy mt mu l mv mw"><strong class="mo iu">def</strong> strip_html_tags(text):<br/>    soup = BeautifulSoup(text, "html.parser")<br/>    stripped_text = soup.get_text()<br/>    <strong class="mo iu">return</strong> stripped_text</span><span id="4113" class="ms la it mo b gy mx mu l mv mw"><br/># Below is another variation for doing the same thing</span><span id="5f8d" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">def</strong> clean_html(html):     <br/>     # parse html content<br/>     soup = BeautifulSoup(html, "html.parser")</span><span id="6cfc" class="ms la it mo b gy mx mu l mv mw">     for data in soup(['style', 'script', 'code', 'a']):<br/>     # Remove tags<br/>         data.decompose( )</span><span id="91b1" class="ms la it mo b gy mx mu l mv mw">     # return data by retrieving the tag content<br/>     return ' '.join(soup.stripped_strings)</span></pre><h1 id="4b4b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">删除重音字符</h1><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="2d1a" class="ms la it mo b gy mt mu l mv mw"><strong class="mo iu">import</strong> unicodedata</span><span id="fa7a" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">def</strong> remove_accent_chars(text):<br/>    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')<br/>    <strong class="mo iu">return</strong> text</span></pre><h1 id="d624" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">删除URL、提及(@)、hastags (#)和特殊字符</strong></h1><p id="3ec2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们可以利用正则表达式来删除URL、提及、标签和特殊字符，因为它们保持一定的结构和模式。下面只是一个例子，说明我们如何匹配字符串中的URL、提及和标签模式，并删除它们。请记住，应该有多种方法，因为有多种方法可以形成正则表达式来获得相同的输出。</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="2453" class="ms la it mo b gy mt mu l mv mw"><strong class="mo iu">## Remove URLs</strong></span><span id="46c4" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">import </strong>re</span><span id="b415" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">def </strong>remove_url(text):<br/>     return re.sub(r’https?:\S*’, ‘’, text)</span><span id="c0c7" class="ms la it mo b gy mx mu l mv mw">print(remove_url('The website <a class="ae ky" href="https://www.google.com/" rel="noopener ugc nofollow" target="_blank">https://www.spotify.com/</a> crashed last night due to high traffic.'))<br/>&gt;&gt; 'The website crashed last night due to high traffic.'</span><span id="f0cf" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">## Remove Mentions (@) and hastags (#)</strong></span><span id="8c46" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">import </strong>re</span><span id="5035" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">def </strong>remove_mentions_and_tags(text):<br/>     text = re.sub(r'@\S*', '', text)<br/>     <strong class="mo iu">return </strong>re.sub(r'#\S*', '', text)</span><span id="64ac" class="ms la it mo b gy mx mu l mv mw">print(remove_mentions_and_tags('Thank you @Jay for your contribution to this project! #projectover'))<br/>&gt;&gt; 'Thank you Jay for your contribution to this project! projectover'</span><span id="11d9" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">## Remove Special Characters</strong></span><span id="151b" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">def</strong> remove_spec_chars(text):<br/>     text = re.sub('[^a-zA-z0-9\s]', '' , text)<br/>     <strong class="mo iu">return </strong>text</span><span id="b159" class="ms la it mo b gy mx mu l mv mw"><a class="ae ky" href="https://medium.com/mlearning-ai/nlp-a-comprehensive-guide-to-text-cleaning-and-preprocessing-63f364febfc5" rel="noopener">https://medium.com/mlearning-ai/nlp-a-comprehensive-guide-to-text-cleaning-and-preprocessing-63f364febfc5</a></span></pre><h1 id="dffb" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">删除停止字</strong></h1><p id="4921" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html" rel="noopener ugc nofollow" target="_blank">停用词是一些非常常见的词，在帮助选择文档或为自然语言处理</a>建模时可能没有什么价值。通常，当我们对自然语言处理进行预处理时，这些单词可能会从文本数据中被丢弃或删除。这是因为停用词由于出现频率过高，可能不会增加提高NLP模型准确性的价值。就像典型的机器学习模型一样，低方差的特征价值较低，因为它们无助于模型基于这些特征区分不同的数据点。这同样适用于NLP，其中停用词可以被认为是低方差特征。同样，停用词会导致模型过度拟合，这意味着我们开发的模型对于看不见的数据表现不佳，并且缺乏推广到新数据点的能力。</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="ef3f" class="ms la it mo b gy mt mu l mv mw"># Retrieve stop word list from NLTK<br/>stopword_list = nltk.corpus.stopwords.words(‘english’)</span><span id="b4db" class="ms la it mo b gy mx mu l mv mw">stopword_list.remove(‘no’)</span><span id="0d57" class="ms la it mo b gy mx mu l mv mw">stopword_list.remove(‘not’)</span><span id="fede" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">from</strong> <strong class="mo iu">nltk.tokenize.toktok</strong> <strong class="mo iu">import</strong> ToktokTokenizer</span><span id="bb61" class="ms la it mo b gy mx mu l mv mw">tokenizer = ToktokTokenizer( )</span><span id="25fa" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">def</strong> remove_stopwords(text, is_lower_case=<strong class="mo iu">False</strong>):</span><span id="1e59" class="ms la it mo b gy mx mu l mv mw">     tokens = tokenizer.tokenize(text)</span><span id="9a15" class="ms la it mo b gy mx mu l mv mw">     tokens = [token.strip( ) <strong class="mo iu">for</strong> token <strong class="mo iu">in</strong> tokens] # List comprehension: loop through every token and strip white space</span><span id="669f" class="ms la it mo b gy mx mu l mv mw">     filtered_tokens = [token <strong class="mo iu">for</strong> token <strong class="mo iu">in</strong> tokens <strong class="mo iu">if</strong> token <strong class="mo iu">not</strong> <strong class="mo iu">in</strong> stopword_list] # Keep only the non stop word tokens in the list</span><span id="028c" class="ms la it mo b gy mx mu l mv mw">     filtered_text = ' '.join(filtered_tokens) # join all those tokens using a space as a delimiter</span><span id="b97f" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">    return</strong> filtered_text</span></pre><p id="4bb0" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">请注意，有另一种方法可以从一个名为SpaCy的不同包中检索停用词，这是另一个常用于NLP任务的有用包。我们可以这样做:</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="ecfa" class="ms la it mo b gy mt mu l mv mw"><strong class="mo iu">import</strong> <strong class="mo iu">spacy</strong></span><span id="42c2" class="ms la it mo b gy mx mu l mv mw">en = spacy.load('en_core_web_sm') # load the english language small model of spacy</span><span id="c40b" class="ms la it mo b gy mx mu l mv mw">stopword_list = en.Defaults.stop_words</span></pre><h1 id="9ff6" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">警告和一些结束语</h1><p id="452e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">就像任何其他数据科学任务一样，不应该盲目地进行NLP的预处理。考虑你的目标是什么。例如，从你搜集的社交媒体文本数据中移除标签和提及符号，你想从中得到什么？是因为这些符号没有给你正在构建的预测某个语料库的情感的NLP模型增加多少价值吗？除非您提出这些问题并且能够清楚地回答，否则您不应该临时清理文本。请记住，询问“为什么”在数据科学领域非常重要。</p><p id="abc5" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">在本文中，在进入NLP循环的下一阶段之前，我们查看了清理文本的各种方法的综合列表，如词汇化和如何实现它们的代码片段。</p><p id="c794" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">如果你觉得这篇文章有帮助，请考虑通过以下链接注册medium来支持我: )</p><p id="c9ea" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">joshnjuny.medium.com<a class="ae ky" href="https://joshnjuny.medium.com/membership" rel="noopener"/></p><p id="c778" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">你不仅可以看到我，还可以看到其他作者写的这么多有用和有趣的文章和帖子！</p><h1 id="2ffd" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">关于作者</h1><p id="85a0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><em class="nd">数据科学家。加州大学欧文分校信息学专业一年级博士生。</em></p><p id="4509" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated"><em class="nd">密歇根大学刑事司法行政记录系统(CJARS)经济学实验室的前研究领域专家，致力于统计报告生成、自动化数据质量审查、构建数据管道和数据标准化&amp;协调。Spotify前数据科学实习生。Inc .(纽约市)。</em></p><p id="15fb" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">他喜欢运动、健身、烹饪美味的亚洲食物、看kdramas和制作/表演音乐，最重要的是崇拜我们的主耶稣基督。结账他的 <a class="ae ky" href="http://seungjun-data-science.github.io" rel="noopener ugc nofollow" target="_blank"> <em class="nd">网站</em> </a> <em class="nd">！</em></p></div></div>    
</body>
</html>