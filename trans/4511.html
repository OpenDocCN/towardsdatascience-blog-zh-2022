<html>
<head>
<title>A.I. Mathematician? A Simplified Look at DeepMind’s AlphaTensor</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能数学家？DeepMind的AlphaTensor的简化视图</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-i-mathematician-a-simplified-look-at-deepminds-alphatensor-feeeac90c9f3#2022-10-06">https://towardsdatascience.com/a-i-mathematician-a-simplified-look-at-deepminds-alphatensor-feeeac90c9f3#2022-10-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8667" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">谷歌DeepMind的AlphaTensor是AI驱动的数学创新的开始。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5395b3fa709d6a8512343b8109d3d130.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cDGlIxk0N1IweUA-5PluBg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://www.freepik.com/free-vector/scientific-formulas-chalkboard_7458753.htm#query=math&amp;position=13&amp;from_view=search&amp;track=sph" rel="noopener ugc nofollow" target="_blank">图像来源</a></p></figure><p id="ed87" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">谷歌的DeepMind研究小组最近开发了“AlphaTensor”。第一个人工智能系统，用于发现执行矩阵乘法等基本任务的新算法。这一应用为50年来寻找两个矩阵相乘的最快方法的数学挑战带来了创新。我们来分解一下这个发现的意义。</p><h1 id="a916" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">什么是矩阵乘法？</h1><p id="30b9" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们大多数人第一次接触矩阵乘法是在初中或高中。它由一系列按预定义顺序的乘法和加法组成，用于计算两个n次张量的乘积。两个矩阵相乘最著名的机制是使用行和列的点积(图1)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/de9288b93a07bdcf3bf38164d761fe9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iwSTRp1wki91PuBfikxKbQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图一。矩阵点积—(图片由作者提供)</p></figure><p id="9145" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在K-12教育之外，这种数学运算影响了现代计算，并且是数字系统的基石。几乎任何可以标记或编码成数字的多维表示的数据类型都需要一些矩阵乘法。图像处理和计算机视觉应用极大地受益于高效的矩阵运算。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/d03fa8751020ee5d29c81914d1de9a8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WYUtmuluFOcUWh0U.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图二。卷积神经网络运算— <a class="ae kv" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="bb07" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在卷积神经网络中(图2)，离散卷积相当于在滤波器权重和滤波器下的值之间取点积，简而言之，这是另一种矩阵乘法应用。</p><p id="8957" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数百年来，数学家们认为标准点积是矩阵相乘的最佳和最有效的方法。直到Colker Strassen介绍了Strassen的算法(图3)，并通过证明存在更有效的算法而震惊了世界。Strassen的算法比传统方法少用了一个标量乘法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/95ab04e07c12b3efaa8f329ccb19f3fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EHebknGt1uef_ewmgxOLFg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3。与Strassen算法相比的标准算法，Strassen算法使用少一个标量乘法(7而不是8)来乘以2x2矩阵。对于整体效率来说，乘法比加法重要得多。(图片由作者提供)</p></figure><p id="f472" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管Strassen的算法包含更少的算术运算，但必须考虑到，对于当前一代的CPU，乘法器可以在每个机器周期产生一个结果，但加法器也可以。乘法器需要更多的门和更长的等待时间来获得第一个结果，但它们在吞吐量方面的速度相同。乘法比加法具有更高的“大O”复杂度(图4)。总之，Strassen效率更高，因为他少执行了一个乘法步骤。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/7126fddd010711413a50394e91528e72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d6XM4ijzUeAyOTcWnJwVCQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4。数学运算的计算复杂度— <a class="ae kv" href="https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Arithmetic_functions" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="599b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">AlphaTensor进一步利用人工智能技术，通过自我游戏和强化学习进行优化，发现新的矩阵乘法算法。</p><h1 id="fb1b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">什么是深度强化学习？</h1><p id="739b" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">深度强化学习(Deep RL)利用深度神经网络的能力来优化特定问题的解决方案。我们采用代理人、环境和报酬的概念作为现实世界系统的模型，其目标是代理人在特定环境中最大化其报酬。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/934bc7e8b7182ab2cb4c1243d6bc8631.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JGO5OXTSWqHgFCD3lNMLlg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5。棋盘— <a class="ae kv" href="https://www.freepik.com/free-photo/chess-game-business-strategy-concept_2753717.htm#query=chess&amp;position=0&amp;from_view=search&amp;track=sph" rel="noopener ugc nofollow" target="_blank">图片来源</a></p></figure><p id="082b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设您考虑国际象棋游戏(图5)，其中的目标是捕获对手的棋子并将死他们的国王。在这种情况下，棋盘上每一个可能的场景都有一套最佳的走法，可以最大化成功的几率。对于这种情况，RL模式可以定义如下:</p><ul class=""><li id="bdfb" class="mu mv iq ky b kz la lc ld lf mw lj mx ln my lr mz na nb nc bi translated">代理人就是棋手。</li><li id="cbe5" class="mu mv iq ky b kz nd lc ne lf nf lj ng ln nh lr mz na nb nc bi translated">环境就是棋盘和所有的棋子。</li><li id="d8fd" class="mu mv iq ky b kz nd lc ne lf nf lj ng ln nh lr mz na nb nc bi translated">奖励框架:<br/> —靠近对方棋盘一侧加2分<br/> —俘获一个棋子加5分<br/> —俘获一个骑士或主教加10分<br/> —俘获一个女王加50分<br/> —失去任何棋子减10分<br/> —失去你的女王减50分</li></ul><p id="fbf2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们会对每一步给予适当的奖励，直到任何一个玩家被对弈或者出现和棋(图6)。这个过程将反复执行，直到我们的模型分数始终高到足以满足我们的需求。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/98edbe745739c04789000b291296b446.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IgKOeiTXXrtQ5tttKyQKMA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图6。强化学习培训周期—来自Microsoft Powerpoint 2022的个别图片(图片由作者提供)</p></figure><p id="243a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">AlphaTensor做了一些非常类似的事情，以发现数学运算的最佳组合，从而以一种新颖有效的方式执行矩阵乘法。</p><h1 id="1523" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">AlphaTensor如何实现算法发现的自动化？</h1><p id="2559" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">利用自我游戏的概念，DeepMind的团队建立了他们的强化学习模式，用于优化矩阵乘法问题。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/7c5829327ad874f557f1fa4e56299549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Beq5NND-IqqaWPtM0342Cw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://www.freepik.com/free-vector/mathematics-concept-illustration_10733824.htm#query=mathematics&amp;position=24&amp;from_view=search&amp;track=sph" rel="noopener ugc nofollow" target="_blank">图像来源</a></p></figure><p id="b5b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如他们在<a class="ae kv" href="https://www.nature.com/articles/s41586-022-05172-4" rel="noopener ugc nofollow" target="_blank">的论文</a>中所描述的，他们的模式看起来像这样:</p><ul class=""><li id="1912" class="mu mv iq ky b kz la lc ld lf mw lj mx ln my lr mz na nb nc bi translated">AlphaTensor代理在不了解现有矩阵乘法算法的情况下启动，并负责编译一组操作，以最小化矩阵乘法的正确值与其结果之间的误差。</li><li id="686b" class="mu mv iq ky b kz nd lc ne lf nf lj ng ln nh lr mz na nb nc bi translated">环境是被操作的矩阵的实际定义。在这种情况下，DeepMind使用3D张量。</li><li id="9f4e" class="mu mv iq ky b kz nd lc ne lf nf lj ng ln nh lr mz na nb nc bi translated">奖励被定义为新算法结果中的误差测量值和使张量归零所采取的步骤数的组合(处理所涉及的矩阵的所有分量)。</li></ul><p id="e47a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种模式产生了一个搜索空间，其中可能的算法大大超过了宇宙中的原子。作为参考，DeepMind多年前解决的围棋(图7)游戏中每一步可能的走法比AlphaTensor遇到的复杂度少30个数量级。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/2f6cae1c9fbffb4945c57720aafe62c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4YdE1rMDRdExci0d.JPG"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图7。围棋棋盘游戏— <a class="ae kv" href="https://en.wikipedia.org/wiki/Go_(game)" rel="noopener ugc nofollow" target="_blank">图片来源</a></p></figure><p id="0d55" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了增加这一努力的可行性，AlphaTensor采用了“一种新的神经网络架构，它结合了特定问题的归纳偏差、生成有用合成数据的程序以及利用问题对称性的方法。”</p><p id="c579" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随着时间的推移，DeepMind团队观察到AlphaTensor重新发现Strassen等可行算法的能力有所提高，并最终超过人类定义的方法，以比任何探索都更快的速度支持算法。</p><p id="456c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在使用Strassen算法和AlphaTensor对5x5和5x5(图8)矩阵进行乘法运算的测试中，我们发现AlphaTensor可以在76次乘法中完成运算，而Strassen的乘法是80次！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/20a244ac38627d1b82f0b7835b1d31fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0MZl7K2LbBCnv2p0fX7FWw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图8。5x5矩阵乘法示例(图片由作者提供)</p></figure><h1 id="c183" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">为什么这是一件大事？</h1><p id="5312" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">AlphaTensor帮助揭示了矩阵乘法算法的丰富性。这些知识无疑将塑造多维数据处理速度和效率的未来。</p><p id="f255" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">AlphaTensor还证明了机器学习技术可以用来发现超越人类创造力的数学创新。这种深度强化学习的味道还处于早期，AlphaTensor的工作更多地是作为可行性的证明，而不是立即的解决方案。这是计算优化和自优化AGI向前迈出的激动人心的一步。</p><p id="18b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">DeepMind的GitHub包括AlphaTensor发现的一些算法和一个jupyter笔记本，上面有加载和测试这些算法的说明。我在下面的源代码中链接了它。</p><p id="d0be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="nm">别忘了关注</em> </strong> <a class="ae kv" href="https://eduand-alvarez.medium.com/" rel="noopener"> <strong class="ky ir"> <em class="nm">我的简介更多文章</em> </strong> </a> <strong class="ky ir"> <em class="nm">这样！</em>T13】</strong></p><p id="2eba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">来源:</strong></p><ul class=""><li id="63b2" class="mu mv iq ky b kz la lc ld lf mw lj mx ln my lr mz na nb nc bi translated"><em class="nm">用AlphaTensor发现新颖算法|</em><a class="ae kv" href="https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor" rel="noopener ugc nofollow" target="_blank"><em class="nm">https://www . deep mind . com/blog/discovery-novel-algorithms-with-alpha tensor</em></a></li><li id="f539" class="mu mv iq ky b kz nd lc ne lf nf lj ng ln nh lr mz na nb nc bi translated"><em class="nm">数学运算的计算复杂度|</em><a class="ae kv" href="https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Arithmetic_functions" rel="noopener ugc nofollow" target="_blank"><em class="nm">https://en . Wikipedia . org/wiki/Computational _ complexity _ of _ mathematic _ operations #算术_functions </em> </a></li><li id="4d03" class="mu mv iq ky b kz nd lc ne lf nf lj ng ln nh lr mz na nb nc bi translated"><em class="nm">alpha tensor GitHub |</em><a class="ae kv" href="https://github.com/deepmind/alphatensor" rel="noopener ugc nofollow" target="_blank"><em class="nm">https://github.com/deepmind/alphatensor</em></a></li><li id="e6c3" class="mu mv iq ky b kz nd lc ne lf nf lj ng ln nh lr mz na nb nc bi translated"><em class="nm">利用强化学习发现更快的矩阵乘法算法| Fawzi，a .等.|</em><a class="ae kv" href="https://www.nature.com/articles/s41586-022-05172-4" rel="noopener ugc nofollow" target="_blank"><em class="nm">https://www.nature.com/articles/s41586-022-05172-4</em></a></li></ul></div></div>    
</body>
</html>