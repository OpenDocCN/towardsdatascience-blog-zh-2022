<html>
<head>
<title>Modeling Marketing Mix using PyMC3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 PyMC3 建模营销组合</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/modeling-marketing-mix-using-pymc3-ba18dd9e6e68#2022-02-23">https://towardsdatascience.com/modeling-marketing-mix-using-pymc3-ba18dd9e6e68#2022-02-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="83f1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">实验先验、数据标准化，并将贝叶斯建模与 Robyn(脸书的开源 MMM 包)进行比较</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c24250ca4250af79f658c5ca2e60edad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gOA3Ip6qZxMIdw2i"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">杰里米·贝赞格在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="5104" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di">在这篇文章中，我将贝叶斯方法应用于评估不同媒体渠道的广告支出对收入的影响的营销问题。我涵盖了贝叶斯建模的几个方面，这些方面对 MMM 从业者应该很重要:</span></p><ul class=""><li id="d422" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">因变量和自变量的标准化以及先验的选择</li><li id="1007" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">响应变量的标准化对估计效果的影响</li></ul><p id="abef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，我将结果与 Robyn 框架进行了比较，遵循其方法论，使用岭回归的 MMM 建模开源包，以及用于超参数优化的无梯度优化框架。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><p id="188f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">之前关于营销组合建模和贝叶斯推理的出版物</strong></p><p id="ab78" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有不少关于使用贝叶斯编程对营销组合建模的文章，涵盖了建模的不同方面，例如:</p><ul class=""><li id="d884" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">贝叶斯框架及其揭示数据真实参数的稳健性:<a class="ae kv" href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf" rel="noopener ugc nofollow" target="_blank">具有遗留和形状效应的媒体混合建模的贝叶斯方法</a>，<a class="ae kv" rel="noopener" target="_blank" href="/carryover-and-shape-effects-in-media-mix-modeling-paper-review-fd699b509e2d">媒体混合建模中的遗留和形状效应:论文综述</a></li><li id="c85d" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">倍增 MMM: <a class="ae kv" rel="noopener" target="_blank" href="/python-stan-implementation-of-multiplicative-marketing-mix-model-with-deep-dive-into-adstock-a7320865b334">倍增营销组合模型的 Python/STAN 实现</a></li><li id="4d4f" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">股票和饱和效应建模:<a class="ae kv" rel="noopener" target="_blank" href="/bayesian-marketing-mix-modeling-in-python-via-pymc3-7b2071f6001a">通过 PyMC3 在 Python 中进行贝叶斯营销组合建模</a></li><li id="4dc0" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">贝叶斯模型的实际用法:<a class="ae kv" href="https://discourse.pymc.io/t/a-bayesian-approach-to-media-mix-modeling-by-michael-johns-zhenyu-wang/6024" rel="noopener ugc nofollow" target="_blank"> HelloFresh </a></li></ul><h1 id="29d2" class="mw mx iq bd my mz na nb nc nd ne nf ng jw nh jx ni jz nj ka nk kc nl kd nm nn bi translated"><strong class="ak">文章的结构如下</strong></h1><ul class=""><li id="eab4" class="mb mc iq ky b kz no lc np lf nq lj nr ln ns lr mg mh mi mj bi translated">营销组合建模 —我简单介绍一下 MMM 背后的理论。</li><li id="8c18" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir">Adstock/carry Effect</strong>—我介绍了几何 ad stock 函数的差异，提供了 Robyn 框架中使用的几何函数的 ano 实现</li><li id="652c" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir">收益递减/饱和效应</strong> —我涵盖了可用于收益递减建模的各种函数</li><li id="49ab" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir">建模</strong> —这是本文的主要部分，我在其中探索了数据规范化对结果的影响。我使用 Robyn 团队提供的演示数据，并遵循 Robyn 的数据处理方法。最后，我将结果与 Robyn 的建模进行比较。</li></ul></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="5d6b" class="mw mx iq bd my mz nt nb nc nd nu nf ng jw nv jx ni jz nw ka nk kc nx kd nm nn bi translated"><strong class="ak">营销组合建模</strong></h1><blockquote class="ny nz oa"><p id="321e" class="kw kx ob ky b kz la jr lb lc ld ju le oc lg lh li od lk ll lm oe lo lp lq lr ij bi translated">我花在广告上的钱有一半都浪费了；问题是我不知道是哪一半(约翰·沃纳梅克)</p></blockquote><p id="a273" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">MMM 的目标是了解销售的驱动因素，衡量所有可能影响销售的因素的影响。这些因素可分为两大类:一类是对销售只有间接影响的因素(也称为基准)，如经济形势、节假日、天气、竞争；另一类是对销售有直接影响的因素(也称为营销贡献)，如在不同媒体渠道(如电视、广播、在线平台或价格)上的广告支出(ad spend)以及促销。</p><p id="1c8d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">建模器的目标是定义可能影响销售的相关组件，准备营销活动和其他(控制)变量的历史时间序列数据，并建立一个统计模型来估计每个组件对销售的影响。这通常是使用多元线性回归来完成的。</p><p id="afe4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">整个过程因两种营销效应而变得复杂:结转效应和收益递减效应。广告对销售的影响可能会有延续效应，也就是说，在广告之后的几天或几周内，销售会受到广告的影响。此外，由于消费者反应迟缓，广告效果可能不会立竿见影，在这种情况下，我们谈论的是<strong class="ky ir">滞后效应</strong>。</p><p id="d641" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">收益递减</strong>表明，从特定的媒体支出开始，支出和销售之间的关系不是线性的，而是达到了一个饱和点<strong class="ky ir"/>，在这个点上，额外的广告投入不会导致销售的增加。</p><p id="b53a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">MMM 面临的挑战是对每个媒体渠道的遗留和饱和效应以及基线和媒体成分进行建模，将一些约束应用于从营销角度来看有意义的建模。其中一个限制是，媒体支出对销售有积极影响，这要求线性模型估计媒体渠道的正系数。</p><p id="e10a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">解决这一挑战需要一个建模框架，能够根据不同的约束和先验知识优化各种参数。这可以通过使用一些通用的超参数优化框架或使用贝叶斯编程来实现。在本文中，我使用 PyMC3，一个贝叶斯框架，来建模营销组合。</p><h1 id="79ca" class="mw mx iq bd my mz na nb nc nd ne nf ng jw nh jx ni jz nj ka nk kc nl kd nm nn bi translated"><strong class="ak">库存/结转影响</strong></h1><p id="b370" class="pw-post-body-paragraph kw kx iq ky b kz no jr lb lc np ju le lf of lh li lj og ll lm ln oh lp lq lr ij bi translated">我们可以使用几个 adstock 变换函数来模拟遗留效应。常用的变换是所谓的几何衰减的 adstock。然而，它有两种口味。第一个在这篇<a class="ae kv" href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中描述，有三个参数需要估计:</p><ul class=""><li id="063c" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">广告效应的<strong class="ky ir">衰减率</strong>:0&lt;α&lt;1，简单来说就是如果我们今天投资 100 欧元，α为 0.5，那么明天的预期效应将是 50 欧元</li><li id="8b3a" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">假设媒体频道的最大<strong class="ky ir">效果持续时间</strong>(以天、周为单位)</li><li id="c7dd" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir">峰值效应θ (0 ≤ θ ≤ L-1)的延迟</strong>，对不会立即开始的广告支出可能产生的滞后效应进行建模。</li></ul><p id="ae64" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个版本转换的实现可以在<a class="ae kv" rel="noopener" target="_blank" href="/python-stan-implementation-of-multiplicative-marketing-mix-model-with-deep-dive-into-adstock-a7320865b334">这里</a>找到。基于该版本的其他实现，但是具有稍微不同的延迟权重计算，可以在这里的<a class="ae kv" rel="noopener" target="_blank" href="/carryover-and-shape-effects-in-media-mix-modeling-paper-review-fd699b509e2d">和这里的</a>和<a class="ae kv" rel="noopener" target="_blank" href="/an-upgraded-marketing-mix-modeling-in-python-5ebb3bddc1b6">中找到。</a></p><p id="6a64" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二个版本更容易实现。它只有一个参数——衰变率α。Robyn 团队也在他们的框架中使用这个版本。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/f7346e751d7f6e60e6a68ecf4f0ade48.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*KosGR6wApZAYhZpMV50CJw.jpeg"/></div></figure><p id="0766" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中，α是衰减率，x(t)是当前广告花费，y(t-1)是时间 t-1 的累积广告效果，y(t)是时间 t 的最终广告。</p><p id="02b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">python 中的实现有几行代码:</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="b4a7" class="oo mx iq ok b gy op oq l or os">def <strong class="ok ir">adstock_geometric</strong>(x: float, alpha: float):<br/>  x_decayed = np.zeros_like(x)<br/>  x_decayed[0] = x[0]</span><span id="d38e" class="oo mx iq ok b gy ot oq l or os">  for xi in range(1, len(x_decayed)):<br/>    x_decayed[xi] = x[xi] + alpha* x_decayed[xi - 1]</span><span id="68cd" class="oo mx iq ok b gy ot oq l or os">  return x_decayed</span></pre><p id="0333" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">示例:</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="3dd6" class="oo mx iq ok b gy op oq l or os">x = np.array([1.0, 2.0, 3.0, 4.0, 5.0 , 6.0])<br/><strong class="ok ir">adstock_geometric</strong>(x, 0.5)</span><span id="56c6" class="oo mx iq ok b gy ot oq l or os">#output: 1.0, 2.5, 4.25, 6.125, 8.062, 10.031</span></pre><ul class=""><li id="7bfe" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">第<strong class="ky ir"> 2 </strong>天的库存量为 2.5: 2.0 + 1.0 * 0.5</li><li id="2657" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">第三天<strong class="ky ir">的库存量为 4.25: 3.0 + 2.5 * 0.5</strong></li><li id="1bab" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">第<strong class="ky ir"> 4 </strong>天的库存为 6.125: 4 + 4.25 * 0.5</li><li id="1756" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi">….</li></ul><p id="ffbe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当应用于时间序列时，adstock 效应的结果可以在下图中看到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/8153df1dd1138b5389eaf7a5d23add70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kt2xMIhRDnU-3KUk82TSDQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="3d27" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为我们要使用 PyMC3，所以我们必须重写 no 中的几何衰减函数。</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="b756" class="oo mx iq ok b gy op oq l or os">def <strong class="ok ir">adstock_geometric_theano_pymc3</strong>(x, theta):<br/>    x = tt.as_tensor_variable(x)<br/>    </span><span id="8c33" class="oo mx iq ok b gy ot oq l or os">    def adstock_geometric_recurrence_theano(index, <br/>                                            input_x, <br/>                                            decay_x,   <br/>                                            theta):<br/>        return tt.set_subtensor(decay_x[index], <br/>               tt.sum(input_x + theta * decay_x[index - 1]))</span><span id="1dca" class="oo mx iq ok b gy ot oq l or os">    len_observed = x.shape[0]</span><span id="a94b" class="oo mx iq ok b gy ot oq l or os">    x_decayed = tt.zeros_like(x)<br/>    x_decayed = tt.set_subtensor(x_decayed[0], x[0])</span><span id="ca71" class="oo mx iq ok b gy ot oq l or os">    output, _ = theano.scan(<br/>        fn = adstock_geometric_recurrence_theano, <br/>        sequences = [tt.arange(1, len_observed), x[1:len_observed]], <br/>        outputs_info = x_decayed,<br/>        non_sequences = theta, <br/>        n_steps = len_observed - 1<br/>    )<br/>    <br/>    return output[-1]</span></pre></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="3c47" class="mw mx iq bd my mz nt nb nc nd nu nf ng jw nv jx ni jz nw ka nk kc nx kd nm nn bi translated"><strong class="ak">收益递减/饱和效应</strong></h1><p id="8bf5" class="pw-post-body-paragraph kw kx iq ky b kz no jr lb lc np ju le lf of lh li lj og ll lm ln oh lp lq lr ij bi translated">有各种函数可以用来模拟广告支出和销售之间的非线性关系，例如<a class="ae kv" href="https://analyticsartist.wordpress.com/2015/03/08/advertising-diminishing-returns-saturation/" rel="noopener ugc nofollow" target="_blank">幂函数</a>、<a class="ae kv" href="https://analyticsartist.wordpress.com/2015/03/08/advertising-diminishing-returns-saturation/" rel="noopener ugc nofollow" target="_blank">负指数</a>或<a class="ae kv" href="https://engineering.hellofresh.com/bayesian-media-mix-modeling-using-pymc3-for-fun-and-profit-2bd4667504e6" rel="noopener ugc nofollow" target="_blank">逻辑</a>。Robyn 团队使用本文<a class="ae kv" href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf" rel="noopener ugc nofollow" target="_blank">中描述的 Hill 函数</a>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/0922f4f25eb17f2d9c18dcad50d0f4e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*KkUbF37kXmQKD4E_m9qGnQ.jpeg"/></div></figure><p id="7fdb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<strong class="ky ir"> α </strong>控制曲线形状，<strong class="ky ir"> γ </strong>控制拐点。<strong class="ky ir"> α </strong>越大，收益递减越有 S 形。<strong class="ky ir"> α </strong>越小，C 形越多。下面的图展示了不同的饱和度曲线作为α和γ的函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/ab56e7c09293dc9eca5457cfa6616540.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hCbjMb1MHSDRTD7BCTM_Lw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/867e50396954a071d3dcb65bef658b22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-9buMO8mreOpnL2x5vRUgA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="5a0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于饱和希尔函数是在 adstock 变换后应用的，因此在 ano 中不需要特殊处理。输入<strong class="ky ir"> x </strong>已经是一个张量。</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="edb7" class="oo mx iq ok b gy op oq l or os">def <strong class="ok ir">saturation_hill_pymc3</strong>(x, alpha, gamma): <br/>    x_s_hill = x ** alpha / (x ** alpha + gamma ** alpha)<br/>    return x_s_hill</span></pre></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="2f6e" class="mw mx iq bd my mz nt nb nc nd nu nf ng jw nv jx ni jz nw ka nk kc nx kd nm nn bi translated"><strong class="ak">建模</strong></h1><h2 id="7618" class="oo mx iq bd my oy oz dn nc pa pb dp ng lf pc pd ni lj pe pf nk ln pg ph nm pi bi translated"><strong class="ak">数据</strong></h2><p id="fa92" class="pw-post-body-paragraph kw kx iq ky b kz no jr lb lc np ju le lf of lh li lj og ll lm ln oh lp lq lr ij bi translated">我使用由<a class="ae kv" href="https://github.com/facebookexperimental/Robyn" rel="noopener ugc nofollow" target="_blank"> Robyn </a>提供的演示数据集，并遵循相同的数据准备方法步骤，以获得相同的比较基准。为了与 Robyn 解决方案进行比较，我运行了<a class="ae kv" href="https://github.com/facebookexperimental/Robyn/tree/main/demo" rel="noopener ugc nofollow" target="_blank">演示。Robyn 包自带的 R </a>文件，用 R 写的，设置没有任何改动。</p><p id="278b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该数据集包含 208 周的收入，包括:</p><ul class=""><li id="dfb2" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">5 个媒体消费渠道:电视、网络、印刷品、facebook、搜索</li><li id="cdb6" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">2 个也有曝光信息(印象，点击)的媒体渠道:facebook_I，search_clicks_P</li><li id="b363" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">无支出有机媒体:时事通讯</li><li id="bf24" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">控制变量:事件、节假日、竞争对手销售额(competitor_sales_B <strong class="ky ir"> ) </strong></li></ul><p id="8bb3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">建模窗口为 2016 年 11 月 21 日至 2018 年 8 月 20 日的 92 周。为了使其通用，我定义了两个索引变量来引用这个时间窗口:</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="830b" class="oo mx iq ok b gy op oq l or os">START_ANALYSIS_INDEX = 52<br/>END_ANALYSIS_INDEX = 144</span></pre><p id="6325" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">数据准备</strong></p><p id="461c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们必须应用 Robyn 中描述的两个准备步骤:</p><blockquote class="ny nz oa"><p id="4f63" class="kw kx ob ky b kz la jr lb lc ld ju le oc lg lh li od lk ll lm oe lo lp lq lr ij bi translated">Robyn 利用<a class="ae kv" href="https://facebook.github.io/prophet/" rel="noopener ugc nofollow" target="_blank"> Prophet </a>，脸书的开源 ML 库进行时间序列预测。我们使用 Prophet 直接从响应中自动分解趋势、季节性和节假日影响，作为进一步建模的输入变量。这种能力通常会提高模型拟合度，减少残差中的自回归模式。</p></blockquote><p id="5dce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，我们可以使用 Prophet 分解将分类变量如<strong class="ky ir">事件</strong>转换成数值。</p><p id="0e26" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二步准备:</p><blockquote class="ny nz oa"><p id="5729" class="kw kx ob ky b kz la jr lb lc ld ju le oc lg lh li od lk ll lm oe lo lp lq lr ij bi translated">当使用曝光变量(印象、点击、GRPs 等)而不是花费时，Robyn 在曝光和花费之间拟合了一个带有米氏门登函数的非线性模型，以建立花费-曝光关系</p></blockquote><p id="8e7c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二步将允许我们将曝光度转换为最终<strong class="ky ir">支出份额与效果份额</strong>比较的支出。</p><p id="b8c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们先加载数据:</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="3b43" class="oo mx iq ok b gy op oq l or os">data = pd.read_csv("./data/data_raw_2015-11-23__2019-11-11.csv", parse_dates = ["DATE"])<br/>data.columns = [c.lower() if c in ["DATE"] else c for c in data.columns]<br/>data</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pj"><img src="../Images/d5c065e251fd1ad882b63c6b3c4ef6fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i1Or765oZjJ-Jst46qXubw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="c7ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假日数据是 Prophet 最初附带的一个单独的文件。原始假日数据具有每日粒度，因此应该首先在每周级别上进行聚合。Robyn 在他们的演示中使用了德国假日。</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="e93d" class="oo mx iq ok b gy op oq l or os">holidays = pd.read_csv("./data/prophet_holidays_daily.csv", parse_dates = ["ds"])<br/>holidays["begin_week"] = holidays["ds"].dt.to_period('W-SUN').dt.start_time<br/>#combine same week holidays into one holiday<br/>holidays_weekly = holidays.groupby(["begin_week", "country", "year"], as_index = False).agg({'holiday':'#'.join, 'country': 'first', 'year': 'first'}).rename(columns = {'begin_week': 'ds'})<br/>holidays_weekly_de = holidays_weekly.query("(country == 'DE')").copy()<br/>holidays_weekly_de</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/2bf6f29a7b079e0a2947059fa3665c81.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*lLAVMLnNjYvgToSYjczm2A.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="8253" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">先知分解</strong></p><p id="759f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们准备将 Prophet 拟合到我们的数据中，包括假期，一个分类变量，并使用每年的季节性。重要的是要注意，我们将分解应用于所有可用的数据，而不是建模窗口。</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="efe6" class="oo mx iq ok b gy op oq l or os">prophet_data = data.rename(columns = {'revenue': 'y', 'date': 'ds'})<br/>#add categorical into prophet<br/>prophet_data = pd.concat([prophet_data, pd.get_dummies(prophet_data["events"], drop_first = True, prefix = "events")], axis = 1)</span><span id="92f6" class="oo mx iq ok b gy ot oq l or os">prophet = <strong class="ok ir">Prophet</strong>(<strong class="ok ir">yearly_seasonality</strong>=True, holidays=holidays_weekly_de)<br/>prophet.<strong class="ok ir">add_regressor</strong>(name = "events_event2")<br/>prophet.<strong class="ok ir">add_regressor</strong>(name = "events_na")</span><span id="95c3" class="oo mx iq ok b gy ot oq l or os">prophet.fit(prophet_data[["ds", "y", "events_event2", "events_na"]])<br/>prophet_predict = prophet.<strong class="ok ir">predict</strong>(prophet_data[["ds", "y", "events_event2", "events_na"]])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pl"><img src="../Images/84de4b0705a916ab2e8027e486dd942d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PdI5svLelLhMu54fjtwrFA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="0589" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们提取季节性、趋势、假日和事件组件:</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="bdf2" class="oo mx iq ok b gy op oq l or os">prophet_columns = [col for col in prophet_predict.columns if (col.endswith("upper") == False) &amp; (col.endswith("lower") == False)]<br/><strong class="ok ir">events_numeric </strong>= prophet_predict[prophet_columns].filter(like = "events_").sum(axis = 1)</span><span id="2bfd" class="oo mx iq ok b gy ot oq l or os">final_data = data.copy()<br/>final_data["<strong class="ok ir">trend</strong>"] = prophet_predict["trend"]<br/>final_data["<strong class="ok ir">season</strong>"] = prophet_predict["yearly"]<br/>final_data["<strong class="ok ir">holiday</strong>"] = prophet_predict["holidays"]<br/>final_data["<strong class="ok ir">events</strong>"] = (events_numeric - np.min(events_numeric)).values</span></pre><p id="a8b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">支出风险评估</strong></p><p id="11c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这一步中，我们使用米氏门登函数来估计支出-敞口的非线性关系</p><p id="4cac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">支出-风险函数定义如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/03e7ddc0f7f4b6fe237e6ec4418b4499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*ApyqLNoej6qPNvUZIhrbyg.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="4c0f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中 Vmax 和 Km 是我们需要估计的两个参数。这两个参数稍后将被用于找到暴露-花费的反向关系。为建模窗口估计参数。</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="e61b" class="oo mx iq ok b gy op oq l or os">#define the function<br/><strong class="ok ir">spend_to_exposure_menten_func = </strong>lambda spend, V_max, K_m<strong class="ok ir">: V_max * spend / (K_m + spend)</strong></span><span id="b24f" class="oo mx iq ok b gy ot oq l or os">media_exposures = ["facebook_I", "search_clicks_P"]<br/>media_spends = ["facebook_S", "search_S"]</span><span id="af6d" class="oo mx iq ok b gy ot oq l or os">media_spend_exposure_df = pd.DataFrame()<br/>for (media_exposure, media_spend) in zip(media_exposures, media_spends):<br/>    V_max = final_data[media_exposure].values[START_ANALYSIS_INDEX : END_ANALYSIS_INDEX].max()<br/>    K_m   = V_max / 2<br/>    spend = final_data[media_spend].values[START_ANALYSIS_INDEX : END_ANALYSIS_INDEX]<br/>    exposure = final_data[media_exposure].values[START_ANALYSIS_INDEX : END_ANALYSIS_INDEX]<br/>    best_values, _ = <strong class="ok ir">optimize.curve_fit</strong>(f = spend_to_exposure_menten_func, xdata = spend, ydata = exposure, p0 = [V_max, K_m])<br/>    media_spend_exposure_df = pd.concat([media_spend_exposure_df, pd.DataFrame({'spend': [media_spend], 'exposure': [media_exposure], 'V_max': [best_values[0]], 'K_m': [best_values[1]]})]).reset_index(drop = True)<br/>    <br/>media_spend_exposure_df</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/7bd092683e52d2e406af66fc37cd108b.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*FU7eX3xyWFrzkw9CVLH9FA.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="edc6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">贝叶斯建模</strong></p><p id="6285" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们已经准备好建模了。我们将我们的响应变量(收入)建模为加性线性回归，可以用以下等式描述:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi po"><img src="../Images/308c1ec68a39626e692b8da308f5ea44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jFS6s4DOzdcIv5yUqbYAwg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="fbaf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<em class="ob"> b_0 </em>对应于基线收入，<em class="ob"> b_m </em>系数对应于通过 adstock 和 saturation 函数转换的媒体变量，<em class="ob"> b_c </em>系数对应于控制变量，ϵ是一些噪声。所有提到的系数、噪声以及吸附和饱和函数的参数都应该由模型来估计。</p><p id="e7d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在建模之前，我们必须做出的第一个决定是，我们将如何标准化我们的因变量和自变量。通过标准化独立变量，我们可以将我们的模型推广到其他数据源，因为我们可以对大多数独立变量使用相同的先验。此外，很难对非标准化数据设定先验。因此，我对独立变量应用 0–1 归一化，并对响应变量进行两种不同的归一化实验:</p><ul class=""><li id="3587" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">扩展 100K</li><li id="59a7" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">0–1 标准化</li></ul><p id="2c83" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">缩放 100K </strong></p><p id="927f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最初的收入范围是 672250–3827520，因此通过将收入扩大 100K，我得到了以下范围:6.72–38.27，这使得试验先验更容易。</p><p id="144c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我定义我的输入变量:</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="0aa9" class="oo mx iq ok b gy op oq l or os">data = final_data<br/><strong class="ok ir">transform_variables </strong>= ["trend", "season", "holiday", "competitor_sales_B", "events", "tv_S", "ooh_S", "print_S", "facebook_I", "search_clicks_P", "newsletter"]</span><span id="229b" class="oo mx iq ok b gy ot oq l or os"><strong class="ok ir">delay_channels </strong>= ["tv_S", "ooh_S", "print_S", "facebook_I", "search_clicks_P", "newsletter"]</span><span id="c052" class="oo mx iq ok b gy ot oq l or os"><strong class="ok ir">media_channels </strong>= ["tv_S", "ooh_S", "print_S", "facebook_I", "search_clicks_P"]</span><span id="a8e9" class="oo mx iq ok b gy ot oq l or os"><strong class="ok ir">control_variables </strong>= ["trend", "season", "holiday", "competitor_sales_B", "events"]</span><span id="d8c3" class="oo mx iq ok b gy ot oq l or os"><strong class="ok ir">target </strong>= "revenue"</span></pre><p id="447e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我使用<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html" rel="noopener ugc nofollow" target="_blank">最小最大缩放器</a>将自变量标准化，并将因变量缩放 100K</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="77db" class="oo mx iq ok b gy op oq l or os">data_transformed = data.copy()</span><span id="d38a" class="oo mx iq ok b gy ot oq l or os">numerical_encoder_dict = {}</span><span id="2428" class="oo mx iq ok b gy ot oq l or os">for feature in transform_variables:<br/>    scaler = <strong class="ok ir">MinMaxScaler</strong>()<br/>    original = data[feature].values.reshape(-1, 1)<br/>    transformed = scaler.fit_transform(original)<br/>    data_transformed[feature] = transformed<br/>    numerical_encoder_dict[feature] = scaler</span><span id="16d5" class="oo mx iq ok b gy ot oq l or os">dependent_transformation = None<br/>original = data[target].values<br/>data_transformed[target] = <strong class="ok ir">original / 100_000</strong></span></pre><p id="d79f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">建模部分包括几个步骤:</p><ul class=""><li id="171d" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">我首先通过媒体通道(延迟通道)进行迭代，并定义 adstock 和饱和度变换的先验。我试验了不同的先验，但最终我使用了那些在<a class="ae kv" href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf" rel="noopener ugc nofollow" target="_blank">贝叶斯方法论文</a>中描述的先验</li><li id="7354" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">我对所有可用数据应用 adstock 变换，以允许使用历史数据建立结转效应</li><li id="7c6d" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">我在由 START_ANALYSIS_INDEX 和 END_ANALYSIS_INDEX 定义的建模窗口上应用饱和度变换</li><li id="c374" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">我使用半正态分布强制延迟通道的回归系数为正</li><li id="b715" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">接下来，我遍历其余的变量，对系数的符号没有任何限制</li><li id="8b65" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">我将截距的先验定义为从收入平均值开始的正态分布。我遵循罗宾的方法，将截距限制为正值。</li></ul><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="d1bb" class="oo mx iq ok b gy op oq l or os">response_mean = []<br/>with pm.Model() as model_2:<br/>    for channel_name in <strong class="ok ir">delay_channels</strong>:<br/>        print(f"Delay Channels: Adding {channel_name}")<br/>        <br/>        x = data_transformed[channel_name].values<br/>        <br/>        <strong class="ok ir">adstock_param</strong> = pm.Beta(f"{channel_name}_adstock", 3, 3)<br/>        <strong class="ok ir">saturation_gamma</strong> = pm.Beta(f"{channel_name}_gamma", 2, 2)<br/>        <strong class="ok ir">saturation_alpha</strong> = pm.Gamma(f"{channel_name}_alpha", 3, 1)<br/>        <br/>        x_new = <strong class="ok ir">adstock_geometric_theano_pymc3</strong>(x, adstock_param)<br/>        x_new_sliced = x_new[START_ANALYSIS_INDEX:END_ANALYSIS_INDEX]<br/>        saturation_tensor = <strong class="ok ir">saturation_hill_pymc3</strong>(x_new_sliced, saturation_alpha, saturation_gamma)<br/>        <br/>        <strong class="ok ir">channel_b</strong> = pm.HalfNormal(f"{channel_name}_media_coef", sd = 3)<br/>        response_mean.append(<strong class="ok ir">saturation_tensor * channel_b</strong>)<br/>        <br/>    for control_var in <strong class="ok ir">control_variables</strong>:<br/>        print(f"Control Variables: Adding {control_var}")<br/>        <br/>        x = data_transformed[control_var].values[START_ANALYSIS_INDEX:END_ANALYSIS_INDEX]<br/>        <br/>        <strong class="ok ir">control_beta</strong> = pm.Normal(f"{control_var}_control_coef", sd = 3)<br/>        control_x = <strong class="ok ir">control_beta * x</strong><br/>        response_mean.append(control_x)<br/>        <br/>    <strong class="ok ir">intercept </strong>= pm.Normal("intercept", np.mean(data_transformed[target].values), sd = 3)<br/>    #intercept = pm.HalfNormal("intercept", 0, sd = 3)<br/>        <br/>    <strong class="ok ir">sigma </strong>= pm.HalfNormal("sigma", 4)<br/>    <br/>    <strong class="ok ir">likelihood </strong>= pm.Normal("outcome", mu = <strong class="ok ir">intercept + sum(response_mean)</strong>, sd = sigma, observed = data_transformed[target].values[START_ANALYSIS_INDEX:END_ANALYSIS_INDEX])</span></pre><p id="fdda" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我从以前的分布中生成样本，以检查我对以前的选择是否合理:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pp"><img src="../Images/5bdf0887e8bacfe4ac917c349a152a52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xYTAjyfCGhzi2pPdynWiTQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="b53f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">并绘制先验分布:</p><div class="kg kh ki kj gt ab cb"><figure class="pq kk pr ps pt pu pv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/05d8ab678dd8c97821ea39f42136d869.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*lxqJSNX-nGwCxEBylpjaXg.jpeg"/></div></figure><figure class="pq kk pw ps pt pu pv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/8f8b09aab824292db30d026d95359781.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*hmGYbeei11GkipkIveTfkA.jpeg"/></div></figure><figure class="pq kk px ps pt pu pv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/a2e8ef66c051397270376685da1d04cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*rVxWIpPgwygELQ-Keea46w.jpeg"/></div></figure></div><div class="ab cb"><figure class="pq kk py ps pt pu pv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/6c882700c3a4a63dd29970149b19389c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*mRwLY4WN_ZxTaKDYTNr07A.jpeg"/></div></figure><figure class="pq kk py ps pt pu pv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/ec6102b45b0ad008eeb3653037333504.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*EoodJUSDsIqH9DJ59ciYwQ.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk pz di qa qb translated">作者图片</p></figure></div><p id="35c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们可以拟合模型了:</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="f834" class="oo mx iq ok b gy op oq l or os">with model_2:<br/>    trace = pm.sample(1000, tune=1000, step=None, <strong class="ok ir">target_accept = 0.95</strong>, return_inferencedata=True)<br/>    trace_summary = az.summary(trace)</span></pre><p id="6c90" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将<em class="ob"> target_accept </em>参数增加到 0.95，因为我得到了一些带有默认值的收敛警告。</p><p id="8878" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当拟合完成时，我从后面采样数据</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="2443" class="oo mx iq ok b gy op oq l or os">with model_2:<br/>    ppc_all = <strong class="ok ir">pm.sample_posterior_predictive</strong>(<br/>        trace, var_names=["outcome"] + list(trace_summary.index), random_seed=42<br/>    )<br/>az.plot_ppc(az.from_pymc3(posterior_predictive=ppc_all, model=model_2), var_names = ["outcome"])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/5ed63d3992fdfd67fdb8189ed143b113.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*8N906yQOVLx0BsolcDhfVQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="7b07" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">观察到的数据的分布不是正态的，而是严格为正的，而我定义的可能性是正态分布的，这就是为什么我们在最低收入水平上看到不匹配。这也可能是一些收敛问题的原因，但使用<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank">标准缩放器</a>或<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html" rel="noopener ugc nofollow" target="_blank"> PowerTransformer </a>并没有带来任何改善，所以我决定坚持使用更直观的标准化。</p><p id="0201" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有了后验样本，我们现在可以测量拟合优度，绘制各种辅助图，如残差图，并执行收入分解和测量渠道贡献。</p><p id="7d9b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">拟合优度</strong></p><p id="8bab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Robyn 用来测量预测误差的指标之一是归一化均方根误差(<a class="ae kv" href="https://www.marinedatascience.co/blog/2019/01/07/normalizing-the-rmse/" rel="noopener ugc nofollow" target="_blank"> NRMSE </a>)。</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="fded" class="oo mx iq ok b gy op oq l or os">def <strong class="ok ir">nrmse</strong>(y_true, y_pred):<br/>   return np.sqrt(np.mean((y_true - y_pred) ** 2)) / (np.max(y_true) - np.min(y_true))</span></pre><p id="206b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预测收入是后验样本的平均值乘以 100K:</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="038e" class="oo mx iq ok b gy op oq l or os">y_true = data[target].values[START_ANALYSIS_INDEX:END_ANALYSIS_INDEX]</span><span id="23db" class="oo mx iq ok b gy ot oq l or os">#restore the original revenue by multiplying back 100K<br/>y_pred = ppc_all["outcome"].mean(axis = 0) * 100_000</span><span id="3996" class="oo mx iq ok b gy ot oq l or os">print(f"RMSE: {np.sqrt(np.mean((y_true - y_pred)**2))}")<br/>print(f"MAPE: {np.mean(np.abs((y_true - y_pred) / y_true))}")<br/>print(f"NRMSE: {nrmse(y_true, y_pred)}")</span></pre><p id="6d5a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">分解</strong></p><p id="54c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了按渠道分解收入，我们必须使用模型估计的参数对媒体渠道应用库存和饱和度，然后乘以相应的估计系数。</p><p id="06fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">模型估计的参数和系数汇总:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/2ee8abd9a9a7d1bd2c75168b1e783b6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*8MF8RCeuOxOvmjoN6I1YBA.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="ff36" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我绘制了实际收入、预测的后验收入以及通过合计每个组成部分的收入贡献计算的收入，以比较分解的收入是否与预测的后验收入匹配。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qe"><img src="../Images/55740fca7e5b88028171ffb24e42f691.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nn0vw8AQbz03-EQR0UafjQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者的形象</p></figure><p id="0970" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">分解收入的 NRMSE:0.058，MAPE: 0.067</p><p id="704d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后一步是计算媒体渠道支出份额，并将其与收入份额(效果份额)进行比较</p><p id="cf09" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我使用原始数据计算支出份额。我找到了使用其曝光信息(facebook_I，search_clicks_P)建模的变量的花费，使用曝光与花费的关系:</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="7cd8" class="oo mx iq ok b gy op oq l or os"><strong class="ok ir">exposure_to_spend_menten_func </strong>= lambda exposure, V_max, K_m: <strong class="ok ir">exposure * K_m / (V_max - exposure)</strong></span><span id="a7e8" class="oo mx iq ok b gy ot oq l or os">spend_df = pd.DataFrame()<br/>for media_channel in <strong class="ok ir">media_channels</strong>:<br/>    temp_series = data[media_channel].iloc[START_ANALYSIS_INDEX:END_ANALYSIS_INDEX].values<br/>    #exposure to spend should<br/>    if len(media_spend_exposure_df[media_spend_exposure_df.exposure == media_channel]) &gt; 0:<br/>        <br/>        <strong class="ok ir">vmax </strong>= media_spend_exposure_df[media_spend_exposure_df.exposure == media_channel]["V_max"].iloc[0]<br/>        <strong class="ok ir">km </strong>= media_spend_exposure_df[media_spend_exposure_df.exposure == media_channel]["K_m"].iloc[0]<br/>        <strong class="ok ir">spends </strong>= <strong class="ok ir">exposure_to_spend_menten_func</strong>(temp_series, V_max = vmax, K_m = km)<br/>        spends_total = spends.sum()<br/>    else:<br/>        spends_total = temp_series.sum()<br/>        <br/>    spend_df = pd.concat([spend_df, pd.DataFrame({'media': [media_channel], 'total_spend': [spends_total]})]).reset_index(drop=True)</span><span id="5610" class="oo mx iq ok b gy ot oq l or os">spend_df["<strong class="ok ir">spend_share</strong>"] = spend_df["total_spend"] / spend_df["total_spend"].sum()<br/>spend_df</span></pre><p id="7de3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我用分解的信息找出这些变量的影响份额。</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="0a5a" class="oo mx iq ok b gy op oq l or os">response_df = pd.DataFrame()<br/>for media_channel in <strong class="ok ir">media_channels</strong>:<br/>    response = data_transformed_decomposed[media_channel].iloc[START_ANALYSIS_INDEX:END_ANALYSIS_INDEX].values<br/>    response_total = response.sum()<br/>    <br/>    response_df = pd.concat([response_df, pd.DataFrame({'media': [media_channel], 'total_effect': [response_total]})]).reset_index(drop=True)<br/>response_df["<strong class="ok ir">effect_share</strong>"] = response_df["total_effect"] / response_df["total_effect"].sum()</span><span id="0c2f" class="oo mx iq ok b gy ot oq l or os">response_df</span></pre><p id="d0d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，绘制支出份额与效果份额的对比图:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qf"><img src="../Images/7894c1cde9fe1a349c50f68a1b9fdeae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*76s2I5HXiP2cXiKZGCwh0Q.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="d00e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">0–1 归一化</strong></p><p id="f50c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们进行同样的实验，但是这次将响应变量归一化到 0 和 1 之间</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="f171" class="oo mx iq ok b gy op oq l or os">dependent_transformation = <strong class="ok ir">MinMaxScaler</strong>()<br/>original = data[target].values.reshape(-1, 1)<br/>transformed = dependent_transformation.fit_transform(original)<br/>data_transformed[target] = transformed</span></pre><p id="3b6c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">截距和系数的先验现在应根据响应范围进行调整:</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="c8a7" class="oo mx iq ok b gy op oq l or os">response_mean = []<br/>with pm.Model() as model_3:<br/>    for channel_name in <strong class="ok ir">delay_channels</strong>:<br/>        print(f"Delay Channels: Adding {channel_name}")<br/>        <br/>        x = data_transformed[channel_name].values<br/>        <br/>        adstock_param = pm.Beta(f"{channel_name}_adstock", 3, 3)<br/>        saturation_gamma = pm.Beta(f"{channel_name}_gamma", 2, 2)<br/>        saturation_alpha = pm.Gamma(f"{channel_name}_alpha", 3, 1)<br/>        <br/>        x_new = adstock_geometric_theano_pymc3(x, adstock_param)<br/>        x_new_sliced = x_new[START_ANALYSIS_INDEX:END_ANALYSIS_INDEX]<br/>        saturation_tensor = saturation_hill_pymc3(x_new_sliced, saturation_alpha, saturation_gamma)<br/>        <br/>        <strong class="ok ir">channel_b </strong>= pm.HalfNormal(f"{channel_name}_media_coef", sd = 0.1)<br/>        response_mean.append(saturation_tensor * channel_b)<br/>        <br/>    for control_var in <strong class="ok ir">control_variables</strong>:<br/>        print(f"Control Variables: Adding {control_var}")<br/>        <br/>        x = data_transformed[control_var].values[START_ANALYSIS_INDEX:END_ANALYSIS_INDEX]<br/>        <br/>        <strong class="ok ir">control_beta </strong>= pm.Normal(f"{control_var}_control_coef", 0.1, sd = 0.1)<br/>        control_x = control_beta * x<br/>        response_mean.append(control_x)<br/>        <br/>    <br/>    <strong class="ok ir">intercept </strong>= pm.HalfNormal("intercept", 0.1)<br/>        <br/>    <strong class="ok ir">sigma </strong>= pm.HalfNormal("sigma", 0.15)<br/>    <br/>    <strong class="ok ir">likelihood </strong>= pm.Normal("outcome", mu = <strong class="ok ir">intercept + sum(response_mean)</strong>, sd = <strong class="ok ir">sigma</strong>, observed = data_transformed[target].values[START_ANALYSIS_INDEX:END_ANALYSIS_INDEX])</span></pre><p id="e971" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">事先分配似乎是合理的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qg"><img src="../Images/5142211180f53d5f2841be901ac71eeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0tZhDxSOeKiFeCvwzcpbeQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="4c96" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">建模和后验预测检查与之前相同，因此让我们检查拟合优度，并绘制花费与效果份额的对比图:</p><p id="19f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">NRMSE: 0.058，MAPE: 0.065</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qh"><img src="../Images/0c86dcdd8889beb165be5fd3c19a31c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dSMjj_G9Qq38o8aKaBZZjw.jpeg"/></div></div></figure><p id="7434" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在比较这两种模型:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qi"><img src="../Images/99e0f2b343f995950247827bd50fa0d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sRAA_gBbRW-aKhhMwO_Y5A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="4b6b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在估计的效果份额上有微小的差异，但是我可以得出结论，在估计效果份额与花费份额的相对大小上，两个模型是一致的</p><p id="9f5d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">与罗宾的比较</strong></p><p id="8168" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Robyn 生成一组模型，并允许建模者选择与业务最相关的模型。对于几个最佳解决方案，它会生成支出份额与效果份额的对比图，并将它们保存在文件中。</p><p id="42f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在最好的模型中，我主观地挑选了一个或多或少与 PyMC3 生成的模型相当的模型:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qj"><img src="../Images/157b287866a4b98a7462f49fd077be95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PZU7IwWXP2Zmclq4UMVQvg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Robyn 生成的图像</p></figure><p id="0864" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将 Robyn 与两款 PyMC3 车型进行了比较:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qk"><img src="../Images/94e4e480febd46de2b3d897f49eefda6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b8gBShYElXkBpyG7AOETbw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="aa52" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">比较表明，在 PyMC3 中生成的两个模型的结果彼此更相似，而不是与 Robyn 更相似。一个可能的原因是，与 PyMC3 生成的解决方案类似的解决方案是首选，但不是首选。另一个原因可能与 Robyn 选择顶级候选人的方式有关:Robyn 使用两种方法进行多目标优化:旨在减少模型预测误差的 NRMSE 和 RSSD(分解均方根距离):</p><blockquote class="ny nz oa"><p id="4f2e" class="kw kx ob ky b kz la jr lb lc ld ju le oc lg lh li od lk ll lm oe lo lp lq lr ij bi translated">该距离说明了花费份额和渠道的系数分解份额之间的关系。如果距离太远，其结果可能太不现实——例如，花费最小的媒体活动获得最大的效果</p></blockquote></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="bd00" class="mw mx iq bd my mz nt nb nc nd nu nf ng jw nv jx ni jz nw ka nk kc nx kd nm nn bi translated"><strong class="ak">结论</strong></h1><p id="7754" class="pw-post-body-paragraph kw kx iq ky b kz no jr lb lc np ju le lf of lh li lj og ll lm ln oh lp lq lr ij bi translated">在本文中，我试验了自变量和因变量、先验和后验分布的标准化。我使用了 Robyn 提出的数据准备方法，该方法允许在真实场景中重用本文中的代码。同样，我将贝叶斯建模的结果与 Robyn 进行了比较。由于模型的最终选择与业务相关，如果没有额外的业务背景和校准，在这些实验中很难确定 Robyn 或 PyMC3 生成的模型是否更好。如果只比较 NRMSE，罗宾选择的顶级模特 NRMSE 更低，因此更适合。由于 Robyn 额外优化了与业务相关的指标，因此它很少有机会生成一个在统计上准确但从营销角度看不切实际的模型。我相信贝叶斯 MMM 解决方案还可以有进一步的改进。一些对我来说仍然开放的问题是如何改善先验参数化以解决收敛警告，以及什么样的非正态概率分布可以用于非正态分布的正响应变量。</p><p id="464e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">完整的代码可以从我的<a class="ae kv" href="https://github.com/slavakx/bayesian_mmm" rel="noopener ugc nofollow" target="_blank"> Github repo </a>下载</p><p id="d485" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢阅读！</p></div></div>    
</body>
</html>