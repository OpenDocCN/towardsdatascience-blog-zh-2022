<html>
<head>
<title>Building a Deep Neural Network from Scratch using Numpy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Numpy从头构建深度神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-deep-neural-network-from-scratch-using-numpy-4f28a1df157a#2022-09-21">https://towardsdatascience.com/building-a-deep-neural-network-from-scratch-using-numpy-4f28a1df157a#2022-09-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2fde" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">现代深度学习库是强大的工具，但它们可能会导致从业者想当然地认为神经网络的功能原理是理所当然的</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a72a0b442c2fe95cddf80af0b2d79255.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p4DEycGf09m5cNo_Jwsgsg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:unsplash.com</p></figure><p id="acf6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个项目中，我在不借助任何深度学习库(Tensorflow，Keras，Pytorch)的情况下，构建了一个深度神经网络。我之所以将自己强加于这项任务，是因为如今，使用多个python库提供的高级工具来构建深度和复杂的神经网络是毫不费力的。毫无疑问，这对机器学习专业人员来说是一个巨大的优势:我们只需要几行代码就可以创建强大的模型。然而，这种方法有一个很大的缺点，就是让这些网络的功能不清楚，因为它们是在“引擎盖下”发生的。</p><p id="287a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于任何想要巩固对这些神奇工具的理解的人来说，从头开始构建深度神经网络是一个很好的练习。</p><p id="0cba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章将涵盖理论和实践两部分。理论部分是理解实现的必修课。对于理论，我们需要代数和微积分的基础知识，而对于编码部分，只会用到内置的Python函数和Numpy。</p><p id="34fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种方法在存储缓存值的策略上不同于其他实现。此外，与大多数实现不同，该代码允许我们比较无限可能的网络架构，因为层和激活单元的数量是由用户定义的。</p><h1 id="f295" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">问题定式化</h1><p id="b8e3" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在这个应用程序中，我创建了一个深度神经网络来解决著名的MNIST分类问题。</p><p id="0db6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST数据集</a>是一个手写数字的大型数据库。该数据集包含70，000个小图像(28 x 28像素)，每个图像都被标记。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/533d7cc87bcac188329fdcfbc1e61057.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/0*VchwF76hnn6ev1Dt.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自<a class="ae kv" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST数据集</a>的手写数字</p></figure><h1 id="9312" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">理论</h1><p id="c6b8" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在这一节中，我将概述应用程序的理论部分。我将为正向传播和反向传播的每一步定义所有矩阵，特别注意阐明所有矩阵的维数。</p><h2 id="65e3" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">投入</h2><p id="aa10" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">输入包括形状为28×28像素的m个训练图像。因此，每个图像由大小为784的一维数组表示。为了加快计算速度，我将利用矢量化技术。我将整个训练集存储在一个矩阵X中。X的每一列代表一个训练示例:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/58ef5e41c29afc37aca85655f123d5e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/1*qbV2di-QGg3ybA6mO_yZdQ.gif"/></div></figure><p id="8c99" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尺寸为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/d34f67a7cd3474e9fcaf6c84ad45f254.png" data-original-src="https://miro.medium.com/v2/resize:fit:184/1*9VNaKGWvAIC0P1li6vQfyw.gif"/></div></figure><h2 id="4e1c" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">正向传播</h2><p id="4b5f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">为了澄清解释，让我们假设建立一个神经网络，包括:</p><ul class=""><li id="7235" class="ne nf iq ky b kz la lc ld lf ng lj nh ln ni lr nj nk nl nm bi translated">输入层</li><li id="6147" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">1个大小为10的隐藏层，具有ReLu激活功能</li><li id="31c6" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">1个大小为10的隐藏层，具有Softmax激活功能</li><li id="659d" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">输出层</li></ul><p id="d5e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有矩阵和计算可以很容易地扩展到任何架构的全连接网络。</p><p id="8f3a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于每一层，正向传播由2个步骤组成:</p><ul class=""><li id="cf9f" class="ne nf iq ky b kz la lc ld lf ng lj nh ln ni lr nj nk nl nm bi translated">权重和偏差的应用</li><li id="08cc" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">激活函数的计算</li></ul><p id="34c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于隐藏层1，我们使用矩阵乘法和矩阵加法来应用权重和偏差:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/08f0d99c44676cc656e71b13bddd576c.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/1*mZQWUoZmY0GzwpqOCLCIRA.gif"/></div></figure><p id="5cac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们需要计算选定的激活函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/edb9a9ab055f2eb22e8122ce3f188219.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/1*tQtL0bMzNFDk0DbmSkI2pg.gif"/></div></figure><p id="d928" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">遵循矩阵乘法规则，维数为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/26b091650e40ab04f76318229dd4a6bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:194/1*5I9jX8oKk9-ET40i0ItHhw.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/8787357d3d40fb2509aa2a5e33810189.png" data-original-src="https://miro.medium.com/v2/resize:fit:222/1*OPF4p9AintflNmRew2SI1A.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/d4484e48569fdec6eb3c5f6d1968c0d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/1*qxTCBXCBC-T6fP23csbaew.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/a61799f8dfe087edb34b011ff7c4d287.png" data-original-src="https://miro.medium.com/v2/resize:fit:208/1*TxVh3tC07nrU_NlCqDKcww.gif"/></div></figure><p id="9886" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第2层也是如此:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/8eb924304f0601c08e423b3a836a94dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/1*G_bc62GuNMCa0AZbn8a-7A.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/34816de1cfbe31f4aeec21babbca6915.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/1*aYnwSqjbZGgMWiJHb1ib7Q.gif"/></div></figure><p id="fc82" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">矩阵维数为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/703792ee642c9314ac178843647b59eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:194/1*j6JNQPooLOMdH3tSsOyEfA.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/89db299a5a1819d8ffbd01697c898e9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:208/1*b2Ilcj-_VSGmj_bG6ZMnmQ.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/d4484e48569fdec6eb3c5f6d1968c0d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/1*qxTCBXCBC-T6fP23csbaew.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/39443c43d294297bdac00cdb704deee9.png" data-original-src="https://miro.medium.com/v2/resize:fit:196/1*RFjBg99IfRyPqALBABzgEA.gif"/></div></figure><p id="0ac1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一般来说，对于任何层l，这两个步骤都是通过这些简单的方程进行的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/aae600c99b051a611517846531d0f400.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/1*hb7yP4chQIr1aXUOfScHtg.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/a938adad15bc023088a9b65b3b3715a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:218/1*UkfOLHbZFmRYcuIFU7S6Fw.gif"/></div></figure><p id="b66d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在前向传播结束时，到达层L，我们计算预测:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/d173e1dc167a6e836a4283f8a920364d.png" data-original-src="https://miro.medium.com/v2/resize:fit:136/1*XCUKixo5Aye1eBu3I3TkHg.gif"/></div></figure><h2 id="a96a" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">反向传播</h2><p id="121b" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">反向传播的目的是计算损失函数相对于网络每层权重的偏导数。一旦我们知道了导数，我们就可以应用梯度下降优化来调整它们的值。</p><p id="d8b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">反向传播的第一步是计算预测的误差。将第2层视为最后一层，我们有:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/0330e09a46b2b717ce5a49c21268afd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/1*7fSD8CQxrn2Pqzj7_iWO4g.gif"/></div></figure><p id="b907" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/5b75dc59edbc90df7abefd28a9c6874c.png" data-original-src="https://miro.medium.com/v2/resize:fit:212/1*H2zuYHeCGK9Dz_Q2GnaAVQ.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/333c5caa25866c6fbf39f13524c5e099.png" data-original-src="https://miro.medium.com/v2/resize:fit:196/1*6Z5xaWL4g3GSFZEUCbFUgg.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/9c1639a81aa3ebb8525516c670c7cf95.png" data-original-src="https://miro.medium.com/v2/resize:fit:168/1*5BM-xV3o7tFAQGJ4DB8DzQ.gif"/></div></figure><p id="6bd7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们可以计算损失函数相对于第2层的权重和偏差的导数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/d099618e9a97c3f1705517c5b7086b13.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/1*pdWVZg-3WueyZzdXteTfUA.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/1708e1f84247817503881c74f3bef3d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/1*2SKR4PVcXG5cJquGRZPVIA.gif"/></div></figure><p id="2fd2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尺寸如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/58443b2c2ca4a11b8e04a2918f820a80.png" data-original-src="https://miro.medium.com/v2/resize:fit:226/1*SI2TKH-bI5QG9GAvvRjqaA.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/7508124f4993c1111db69d928add1f94.png" data-original-src="https://miro.medium.com/v2/resize:fit:210/1*m5g54eSwufn8Y0JzYJWovQ.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/eb1a59d09d8ea160a58a9fbb56db62f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:190/1*DWwLXPs27lENkC-myrOqJg.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/f54da1137b81b34533d166fc8afe4210.png" data-original-src="https://miro.medium.com/v2/resize:fit:256/1*-WgtZo4Y8h_hgn9XuEqGnA.gif"/></div></figure><p id="c26f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦我们知道了最终层的所有导数，反向传播过程就包括通过网络的各层反向传播，并如下计算偏导数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/9f86bd0d9f301e784e413b0dd854f0c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/1*wyyidZEBT0cxEXfhsMzH1w.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/9af06bd7a5e33749d15c6024f22d81d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/1*hIkDqXagcBftWdC2FeeZFA.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/525b0696fd4db997d8aa7f6a43a16b13.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/1*RAcFBVJLyrg2c6SQMRe-OQ.gif"/></div></figure><h2 id="b37b" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">参数更新</h2><p id="ec11" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">知道了损失函数的梯度，我们就知道向哪个方向移动才能达到最优。因此，我们更新参数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/87c5479fdb4f98d28425f0405277c88d.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/1*3Wir-w2VAUApwut7p-OxXw.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/7190d4323978ff43178ca8f38498350d.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/1*PKyeO9GlObszLUqzW-7UNw.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/8d8a976872a1b902a5e45a0f5836a0fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/1*E0dPOIlcEpJdYpkTxV8s0g.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/24e7f9534a1ae43eb860a2201b35144a.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/1*U-CbLWz744uTScPg2zrrYw.gif"/></div></figure><h1 id="330e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">编码</h1><p id="47bd" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">本节介绍了用于实现深度神经网络的所有功能。完整的代码可以在我的<a class="ae kv" href="https://github.com/andreoniriccardo/deep-neural-network-from-scratch" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>中找到。</p><p id="1e8b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面链接的GitHub存储库中，您会找到5个文件:</p><ul class=""><li id="cef8" class="ne nf iq ky b kz la lc ld lf ng lj nh ln ni lr nj nk nl nm bi translated">“README.md”:这是一个展示项目的降价文件</li><li id="9cef" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">“train.csv”:这是一个包含MNIST数据集训练集的csv文件</li><li id="57df" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">“test.csv”:这是一个包含MNIST数据集测试集的csv文件</li><li id="e6e3" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">“main.py”:这是一个Python脚本，我们将从这里运行神经网络</li><li id="1714" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">“utils.py”:这是一个Python文件，我们在其中定义了构建神经网络所需的函数</li></ul><p id="6c49" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将主要关注“utils.py”文件，因为它是大部分网络实现所在的位置。</p><p id="9768" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一个功能是<code class="fe oq or os ot b"><strong class="ky ir">init_params</strong></code>。它将层的尺寸作为输入，并返回包含所有随机初始化的权重和偏差的字典:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="1738" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我定义所有的激活函数及其导数。在这个应用程序中，我们将使用ReLu和Softmax激活。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="36f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在好戏开始了。<code class="fe oq or os ot b"><strong class="ky ir">forward_prop</strong></code> <strong class="ky ir"> </strong>函数将输入X和网络参数(权重和偏差)作为输入，并返回包含每层激活的字典。输出字典还包含各图层的Z矩阵，作为一种缓存。这是因为在反向传播阶段需要Z矩阵。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="4df3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe oq or os ot b"><strong class="ky ir">back_prop</strong></code>函数可能是整个实现的核心。它从最后一层到第一层扫描网络，并计算关于每层的每个权重和偏差的损失函数的梯度。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="e7f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在反向传播步骤期间计算的梯度稍后用于更新权重和偏差。<code class="fe oq or os ot b"><strong class="ky ir">update_params</strong></code> <strong class="ky ir"> </strong>函数处理这个任务。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="a1f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来的两个辅助功能<code class="fe oq or os ot b"><strong class="ky ir">get_predictions</strong></code> <strong class="ky ir"> </strong>和<code class="fe oq or os ot b"><strong class="ky ir">get_accuracy</strong></code> <strong class="ky ir"> </strong>分别用于从最终层选择预测(即选择具有最高Softmax分数的类别)和计算预测的准确度。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="9ae7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我将上面的所有函数总结在<code class="fe oq or os ot b"><strong class="ky ir">gradient_descent_optimization</strong></code> <strong class="ky ir"> </strong>函数中:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><h1 id="9ba0" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">模型评估</h1><p id="6200" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我想尝试的第一个网络架构包括:</p><ul class=""><li id="dc65" class="ne nf iq ky b kz la lc ld lf ng lj nh ln ni lr nj nk nl nm bi translated">大小为784的输入层</li><li id="3158" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">大小为10的隐藏层#1和ReLu激活</li><li id="4f22" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">大小为10的隐藏层#2和Softmax激活</li><li id="d06a" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">大小为1的输出层</li></ul><p id="5bbe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该层在我的Python代码中用列表<code class="fe oq or os ot b"><strong class="ky ir">[784, 10, 10]</strong></code>表示。没有必要在列表中包含输出图层，因为它没有关联的权重。从现在开始，所有的网络都将由描述其架构的python列表来表示。</p><p id="7d4b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在训练网络1000次迭代后，精确度收敛到大约88%的值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/0ef938078bb2a301d8b903f0d2b578a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rmU6v6f5FxX5D3ISvxawJg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="3d79" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">考虑到网络的规模，这并不可怕，但是这与我们在这个任务中想要达到的结果相差甚远。一个更深更广的网络<code class="fe oq or os ot b"><strong class="ky ir">[784, 256, 128, 64, 10]</strong></code> <strong class="ky ir"> </strong>，经过500次迭代的训练，达到了97%的准确率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/50618c63002fc6dada0bef4cfdf6e0f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OsAmISev5CxiLbvgFXmbuQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="3b36" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于第二个网络，每次迭代的训练时间显著增加，因为额外的层和神经元使其计算要求更高。</p><p id="af6c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了达到更好的图像分类效果，通常使用不同类型的网络:卷积神经网络。</p><p id="ea4b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请随意修改我的<a class="ae kv" href="https://github.com/andreoniriccardo/deep-neural-network-from-scratch" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>中的代码，并探索当层数和单元数改变时结果如何变化。</p></div></div>    
</body>
</html>