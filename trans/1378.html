<html>
<head>
<title>Principal Component Regression — Clearly Explained and Implemented</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分回归——明确解释并实施</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-regression-clearly-explained-and-implemented-608471530a2f#2022-04-06">https://towardsdatascience.com/principal-component-regression-clearly-explained-and-implemented-608471530a2f#2022-04-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d672" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">基于主成分分析(PCA)的回归技术的概念和Python实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/48add18cfdff43320bbdd89615fd919d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*I9KxIV7_hdVSgl1I"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">约翰·安维克在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="50f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">主成分分析(PCA)是一种众所周知的<strong class="lb iu">降维</strong>技术，但是您知道吗，我们也可以在<strong class="lb iu">回归分析</strong>中应用PCA背后的概念？</p><p id="b6d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文提供了对<strong class="lb iu">主成分回归</strong> (PCR)的清晰解释，包括它的理论概念、好处、注意事项和Python实现。</p><h2 id="1e9b" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">内容</h2><blockquote class="mo mp mq"><p id="9310" class="kz la mr lb b lc ld ju le lf lg jx lh ms lj lk ll mt ln lo lp mu lr ls lt lu im bi translated"><strong class="lb iu"><em class="it">(1)</em></strong><a class="ae ky" href="#c965" rel="noopener ugc nofollow"><em class="it">理论概念</em></a><br/><strong class="lb iu"><em class="it">(2)</em></strong><a class="ae ky" href="#3b4c" rel="noopener ugc nofollow"><em class="it">好处和注意事项</em></a><br/><strong class="lb iu"><em class="it">(3)</em></strong><a class="ae ky" href="#e6d6" rel="noopener ugc nofollow"><em class="it">Python实现</em> </a></p></blockquote></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="c965" class="nc lw it bd lx nd ne nf ma ng nh ni md jz nj ka mg kc nk kd mj kf nl kg mm nm bi translated">(1)理论概念</h1><p id="8f8f" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">主成分回归(PCR)是一种回归技术，其目标与标准线性回归相同，即模拟目标变量和预测变量之间的关系。</p><p id="97f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不同的是，PCR使用<strong class="lb iu">主成分作为回归分析的预测变量</strong>，而不是原始特征。</p><p id="d022" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PCR分三步进行:</p><ol class=""><li id="8bb8" class="ns nt it lb b lc ld lf lg li nu lm nv lq nw lu nx ny nz oa bi translated"><strong class="lb iu">应用PCA从预测变量生成</strong>主成分，主成分的数量与原始特征的数量相匹配<strong class="lb iu"> <em class="mr"> p </em> </strong></li><li id="82a2" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu nx ny nz oa bi translated">保留解释大部分方差的第一个<strong class="lb iu"> <em class="mr"> k </em> </strong> <em class="mr"> </em>主成分(其中<strong class="lb iu"><em class="mr">k</em></strong>&lt;<strong class="lb iu"><em class="mr">p</em></strong>)，其中<strong class="lb iu"> <em class="mr"> k </em> </strong>由交叉验证确定</li><li id="7bf2" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu nx ny nz oa bi translated">在这些<strong class="lb iu"> <em class="mr"> k </em> </strong>主成分上拟合线性回归模型(使用普通最小二乘法)</li></ol><p id="449c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个想法是，较少数量的主成分代表数据中的大部分可变性和(假定的)与目标变量的关系<strong class="lb iu">。因此，我们不是使用所有的原始特征进行回归，而是仅利用主成分的子集</strong>。</p><p id="1017" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管与目标变量<strong class="lb iu"> </strong>的关系的假设并不总是成立的，但它通常是产生良好结果的足够合理的近似。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/e1afd2b7167849f12a11fafd53a676ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*L4P8clPINxuLn1qj"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@alandelacruz4?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">艾伦·德拉克鲁斯</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="3b4c" class="nc lw it bd lx nd ne nf ma ng nh ni md jz nj ka mg kc nk kd mj kf nl kg mm nm bi translated">(2)益处和注意事项</h1><h2 id="c8dd" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">利益</h2><ul class=""><li id="098a" class="ns nt it lb b lc nn lf no li oh lm oi lq oj lu ok ny nz oa bi translated">PCR在<strong class="lb iu"> <em class="mr"> k </em> </strong> <em class="mr"> </em>主成分而不是所有原始特征上拟合线性回归模型，从而帮助<strong class="lb iu">减少过拟合</strong>。理论上，这比基于所有原始特征训练的标准线性回归模型具有更好的性能。</li><li id="ab20" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu ok ny nz oa bi translated">PCR通过移除与小特征值相关的主成分来帮助<strong class="lb iu">消除数据中的多重共线性</strong>。</li><li id="78f0" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu ok ny nz oa bi translated">在具有许多特征(<strong class="lb iu">高基数</strong>)和<strong class="lb iu">显著多重共线性</strong>的数据中，性能提升更为显著。</li></ul><h2 id="e194" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">警告</h2><ul class=""><li id="abc0" class="ns nt it lb b lc nn lf no li oh lm oi lq oj lu ok ny nz oa bi translated">PCR<strong class="lb iu">不</strong>被认为是特征选择方法，因为回归中使用的结果主成分是原始特征的<strong class="lb iu">线性组合。这意味着我们仍然依赖于所有的原始特性，而不是它们的子集。</strong></li><li id="f911" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu ok ny nz oa bi translated">由于在模型中使用了主成分，与原始特征相比，这些预测器失去了它们的“<strong class="lb iu">解释能力</strong>”。</li><li id="abb9" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu ok ny nz oa bi translated">PCA变换是无监督的，这意味着在确定主成分时，它不<strong class="lb iu">而不是</strong>考虑目标变量。因此，<strong class="lb iu">不能保证</strong>方差最大的主成分将是<strong class="lb iu">预测目标</strong>的最佳主成分。具有高预测能力但低方差的方向有可能被忽略，从而导致较差的模型性能。</li></ul></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="e6d6" class="nc lw it bd lx nd ne nf ma ng nh ni md jz nj ka mg kc nk kd mj kf nl kg mm nm bi translated">(3) Python实现</h1><p id="8f13" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">现在让我们一步一步地看一下在Python中应用PCR的指南。在这个项目中，我们将使用传统的葡萄酒质量数据集 ，因为它容易获得，简单，并且是数字格式的。</p><p id="b17e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目标是<strong class="lb iu">预测葡萄酒质量</strong>，作为<strong class="lb iu">回归</strong>任务的一部分，使用诸如酒精含量和酸度等葡萄酒特性。</p><p id="6207" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以参考这个<a class="ae ky" href="https://github.com/kennethleungty/Principal-Component-Regression" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>的<a class="ae ky" href="https://github.com/kennethleungty/Principal-Component-Regression/blob/main/notebooks/Principal_Component_Regression_Wine_Quality.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> Jupyter笔记本</strong> </a>中的代码跟随。</p><div class="ol om gp gr on oo"><a href="https://kennethleungty.medium.com/membership" rel="noopener follow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">通过我的推荐链接加入媒体-梁建华</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">以一杯咖啡的价格访问我所有的内容(和所有的媒体文章)!</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">kennethleungty.medium.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc ks oo"/></div></div></a></div><h2 id="d748" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">步骤1 —初始设置</h2><p id="e1b9" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">我们首先导入所有必要的库和数据，然后执行适当的训练测试分割。原始数据集有十个预测变量和一个目标变量(<strong class="lb iu"><em class="mr"/></strong>)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/6769feeed7c71437e084745390efc30c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4rd7ZJPxdEx8UoYGG7_vjg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">原始葡萄酒质量数据集样本|作者图片</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pe pf l"/></div></figure></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="80a0" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">步骤2 —标准化功能</h2><p id="8ec3" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">在运行PCR之前，至关重要的是<strong class="lb iu">在生成主要成分之前将</strong>每个原始特征标准化到相同的比例。</p><p id="4def" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">原因是主成分计算是基于<strong class="lb iu">方差</strong>，方差受特征的<strong class="lb iu">范围</strong>影响。</p><p id="8b72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果不应用标准化，则范围较大(因此方差较大)的特征将占主导地位，并在生成的主成分中扮演不公平的重要角色。</p><p id="6507" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> sklearn .预处理</strong> </a>中的<code class="fe pg ph pi pj b"><strong class="lb iu">scale()</strong></code>方法对原始特征进行标准化，即中心到均值，刻度到单位方差。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pe pf l"/></div></figure></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="c9e8" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">步骤3 —运行基线回归模型</h2><p id="43bf" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">为了评估PCR模型的性能，我们需要有基准来比较。因此，我们运行<strong class="lb iu">三个基线模型</strong>(标准线性回归、套索回归和岭回归)并保存RMSE分数。</p><p id="0bd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特别是，我们希望保存以下内容:</p><ul class=""><li id="3466" class="ns nt it lb b lc ld lf lg li nu lm nv lq nw lu ok ny nz oa bi translated"><strong class="lb iu">训练集</strong>中的10重交叉验证RMSE</li><li id="6b7f" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu ok ny nz oa bi translated">在<strong class="lb iu">测试集</strong>上预测RMSE</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pe pf l"/></div></figure></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="b661" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">步骤4 —生成主成分</h2><p id="f155" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">原始特征的主要成分可以使用<code class="fe pg ph pi pj b">PCA()</code>从<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">sk learn . decomposition</strong></a>中生成。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pe pf l"/></div></figure><p id="ef51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">得到的11个主成分的预览如下所示，第一个主成分<strong class="lb iu"/>(包含最多信息)位于最左栏。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pk"><img src="../Images/846528cd5242d4c29c4680c12b673e9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V-zIbtNXaI7597oZoOuTHA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">主成分值预览|作者图片</p></figure><p id="b9f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以使用<code class="fe pg ph pi pj b">pca.explained_variance_ratio_</code>查看每个主成分的<strong class="lb iu">解释方差</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/8abde42c4c28097739521d3458c3f19e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DwSskQ5_txd0j-R4O6mcSQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">解释了所有主要成分的差异|图片由作者提供</p></figure></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="1917" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">步骤5-确定主成分的数量</h2><p id="5f39" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">由于我们不想使用所有的主成分，现在的问题是<strong class="lb iu">我们应该使用多少</strong>主成分进行回归分析。</p><p id="7c43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">主成分的数量(<strong class="lb iu"> <em class="mr"> k </em> </strong>)通常由<br/> <strong class="lb iu">交叉验证</strong>和视觉分析确定。</p><p id="9774" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">值<strong class="lb iu"> <em class="mr"> k </em> </strong>本质上是一个我们需要调优的超参数。我们迭代越来越多的主成分，以纳入回归建模和评估结果RMSE评分。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pe pf l"/></div></figure><p id="df6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一个任务是检查<strong class="lb iu">训练集交叉验证RMSE </strong>与所用主成分数量的关系图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/66e676bb9a3b1cc74ae56be5fded0add.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bmAzMGTezneOfokztvE0Gg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">RMSE与主成分数的关系图|图片由作者提供</p></figure><p id="92e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到，PCR的训练集性能随着主成分的增加而提高(即RMSE降低)，这与我们的预期一致。</p><p id="1c54" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">绿线是使用所有原始特征的基线标准线性回归模型的RMSE基准。</p><p id="0e5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该图显示，当存在<strong class="lb iu"> <em class="mr"> M=9 </em> </strong>主成分时，出现<strong class="lb iu">最低交叉验证RMSE(图中的最小点)</strong>。实际上，<strong class="lb iu"> <em class="mr"> M=9 </em> </strong>处的RMSE略低于绿线。</p><p id="1df3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mr">注:如果我们想要更少的元件，同时保持相对相似的性能，这里选择</em><strong class="lb iu"><em class="mr">M = 7</em></strong><em class="mr">也是合理的选择。</em></p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="4b80" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">步骤6 —用最佳数量的主成分运行PCR</h2><p id="eff0" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">在确定要使用的主成分的最佳数量(即<strong class="lb iu"> <em class="mr"> M=9 </em> </strong>)之后，我们继续在我们的测试数据集上运行PCR。</p><p id="d3cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，这是通过在<strong class="lb iu">训练集</strong>的前9个主成分上训练新的线性回归模型来完成的。然后，我们使用这个训练好的模型对<strong class="lb iu">测试集</strong>的前9个主成分进行预测。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pe pf l"/></div></figure><p id="75f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也可以重复上述步骤来评估在<strong class="lb iu"> <em class="mr"> M=7 </em> </strong>主成分上训练的PCR模型的性能。</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="822e" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">第7步—评估和解释结果</h2><p id="db3c" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">下表显示了来自各种模型的RMSE结果，包括使用不同数量的主成分训练的两个PCR模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/b4c01b98e48ad079aff9ff8c279a7a76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*3dvv_ifp4Pl03GddXvrFeA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">回归模型的RMSE结果。RMSE越低，模型越好|作者图片</p></figure><p id="0aa8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到，具有9个主成分的PCR<strong class="lb iu">与基于<strong class="lb iu">所有11个</strong>原始特征训练的三个基线回归模型表现相当。</strong></p><p id="4bd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具有7个成分的PCR的性能稍差是因为需要更多的主成分来充分模拟该数据中的目标。</p><p id="1c60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">值得记住的是，在最初的几个主成分捕获了预测因子的大部分变化以及与目标的关系的情况下，PCR表现得更好。</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="0a86" class="nc lw it bd lx nd ne nf ma ng nh ni md jz nj ka mg kc nk kd mj kf nl kg mm nm bi translated">包装东西</h1><p id="a00d" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">主成分回归被称为回归分析的<strong class="lb iu">降维</strong>方法，因为它减少了回归建模中要估计的系数数量。</p><p id="96dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然与标准线性回归相比，它可能会导致模型性能的提高，但有几个注意事项需要考虑，我们在本文中对此进行了探讨。</p><p id="72c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要查看这个项目的Python代码，欢迎你来探索这个<a class="ae ky" href="https://github.com/kennethleungty/Principal-Component-Regression" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> GitHub repo </strong> </a>。</p><h1 id="5aef" class="nc lw it bd lx nd po nf ma ng pp ni md jz pq ka mg kc pr kd mj kf ps kg mm nm bi translated">在你走之前</h1><p id="6ada" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">欢迎您加入<strong class="lb iu">我的数据科学学习之旅</strong>。跟随此<a class="ae ky" href="https://kennethleungty.medium.com/" rel="noopener">媒体</a>页面并查看我的<a class="ae ky" href="https://github.com/kennethleungty" rel="noopener ugc nofollow" target="_blank"> GitHub </a>以了解实用和教育数据科学内容。同时，享受应用主成分回归的乐趣！</p><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">F1分数的微观、宏观和加权平均值，解释清楚</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">理解微观平均、宏观平均和多级F1分数加权平均背后的概念…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pt l oz pa pb ox pc ks oo"/></div></div></a></div><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/assumptions-of-logistic-regression-clearly-explained-44d85a22b290"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">逻辑回归的假设，解释清楚</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">理解并实现基本数据科学建模技术之一的假设检查(用Python)</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pu l oz pa pb ox pc ks oo"/></div></div></a></div><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/why-bootstrapping-actually-works-1e75640cf172"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">为什么自举真的有效</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">一个简单的门外汉解释为什么这种流行的技术在数据科学中有意义</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pv l oz pa pb ox pc ks oo"/></div></div></a></div><div class="ol om gp gr on oo"><a href="https://kennethleungty.medium.com/membership" rel="noopener follow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">通过我的推荐链接加入媒体-梁建华</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div></div><div class="ox l"><div class="pw l oz pa pb ox pc ks oo"/></div></div></a></div></div></div>    
</body>
</html>