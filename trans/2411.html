<html>
<head>
<title>Random Forest or XGBoost? It is Time to Explore LCE</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机森林还是XGBoost？是时候探索LCE了</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/random-forest-or-xgboost-it-is-time-to-explore-lce-2fed913eafb8#2022-05-26">https://towardsdatascience.com/random-forest-or-xgboost-it-is-time-to-explore-lce-2fed913eafb8#2022-05-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6220" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">LCE:本地级联合奏</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/90e58c846610c7598634f206bae324ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*-7F8CDjjMNdhLX9U9EBblw.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">在<a class="ae kr" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae kr" href="https://unsplash.com/@david_bxl?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> David Bruyndonckx </a>拍摄的照片。</p></figure><p id="ff95" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi lo translated">【O】在过去的几年里，Random Forest [Breiman，2001]和XGBoost [Chen and Guestrin，2016]已经成为在解决分类和回归的许多挑战方面表现最好的机器学习方法。从业者面临着一个反复出现的问题:对于给定的数据集，我们应该选择哪一个？<strong class="ku ir">局部级联集成(LCE)</strong>【Fauvel et al .，2022】是一种新的机器学习方法，它提出来回答这个问题。它结合了两者的优势，采用互补的多样化方法来获得更好的泛化预测器。因此，<strong class="ku ir"> LCE进一步增强了<strong class="ku ir">随机森林</strong>和<strong class="ku ir"> XGBoost的预测性能。</strong></strong></p><p id="94fe" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">本文介绍了LCE和相应的<a class="ae kr" href="https://pypi.org/project/lcensemble/" rel="noopener ugc nofollow" target="_blank"> Python包</a>，并附有一些代码示例。LCE包<strong class="ku ir">与scikit兼容-学习</strong>；它通过<a class="ae kr" href="https://scikit-learn.org/stable/modules/generated/sklearn.utils.estimator_checks.check_estimator.html#sklearn.utils.estimator_checks.check_estimator" rel="noopener ugc nofollow" target="_blank">检查估计器</a>。因此，它可以与scikit-learn管道和模型选择工具进行交互。</p><h1 id="c85a" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">LCE演示</h1><p id="c657" class="pw-post-body-paragraph ks kt iq ku b kv mp jr kx ky mq ju la lb mr ld le lf ms lh li lj mt ll lm ln ij bi translated">集合方法的构建包括将准确和多样的单个预测因子结合起来。有两种互补的方式来生成不同的预测器:<em class="mu"> (i) </em>通过改变训练数据分布，以及<em class="mu"> (ii) </em>通过学习训练数据的不同部分。</p><p id="2e2a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们在发展LCE时采用了这两种多样化的方法。首先，<em class="mu"> (i) </em> LCE结合了两种众所周知的方法，这两种方法通过对偏差-方差权衡的互补效应来修改原始训练数据的分布:bagging [Breiman，1996](方差减少)和boosting [Schapire，1990](偏差减少)。然后，<em class="mu"> (ii) </em> LCE学习训练数据的不同部分，以捕捉基于分治策略(决策树)无法在全局发现的新关系。在详细介绍LCE如何结合这些方法之前，我们先介绍一下在解释LCE时用到的关键概念。</p><p id="43d5" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">偏差-方差权衡定义了学习算法在训练集之外进行概括的能力。<em class="mu">偏差</em>是由学习算法的系统误差产生的预测误差的分量。高偏差意味着学习算法不能捕获训练集的底层结构(欠拟合)。<em class="mu">方差</em>衡量学习算法对训练集变化的敏感度。高方差意味着算法对训练集的学习过于紧密(过度拟合)。目标是最小化偏差和方差。<em class="mu">套袋</em>对方差减少有主效应；这是一种生成预测值的多个版本(引导复制)并使用这些版本获得聚合预测值的方法。目前最先进的采用装袋的方法是随机森林。而<em class="mu">升压</em>对偏置降低有主要影响；<em class="mu"> </em> it <em class="mu"> </em>是一种迭代学习弱预测器并将它们相加以创建最终强预测器的方法。在添加弱学习者之后，数据权重被重新调整，允许未来的弱学习者更多地关注先前弱学习者预测错误的例子。目前最先进的使用boosting的方法是XGBoost。图1说明了装袋和增压方法之间的区别。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mv"><img src="../Images/5a66a7658ff650d60a98491d082a31f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VkDaloNLOE0kDp_Y-D0hUg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图一。植物病害数据集上的套袋与加强。n——估计量的数量。图片由作者提供。</p></figure><p id="3e22" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">因此，我们的新集成方法LCE结合了boosting-bagging方法来处理机器学习模型面临的偏差-方差权衡；此外，它采用分而治之的方法来个性化训练数据不同部分的预测器误差。LCE如图2所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mv"><img src="../Images/f05f7dfecb45ce8cac197ae4c613fadb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*acaFBjS1In9k0e63iOWYqw.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图二。植物病害数据集上的局部级联系综，参考图1，以军校生蓝装袋，以红色增强。n —树的数量，XGB — XGBoost。图片由作者提供。</p></figure><p id="291e" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">具体来说，LCE基于级联概化:它按顺序使用一组预测器，并在每个阶段向输入数据集添加新属性。新属性是从被称为基础学习器的预测器给出的输出(例如，分类器的类概率)中导出的。LCE遵循分而治之的策略(决策树)在局部应用级联泛化，并通过使用基于提升的预测器作为基础学习器来减少决策树中的偏差。采用当前性能最好的最新boosting算法作为基本学习器(XGBoost，例如，图2中的XGB ⁰、XGB)。当生长树时，通过将每个决策节点处的基础学习器的输出作为新属性添加到数据集，增强沿着树向下传播(例如，图2中的XGB ⁰( <em class="mu"> D </em>)。预测输出表明基础学习者正确预测样本的能力。在下一个树级别，基础学习者利用添加到数据集的输出作为加权方案，以更多地关注先前预测错误的样本。然后，通过使用装袋来减轻由增强决策树产生的过拟合。Bagging通过用原始数据集的替换从随机抽样中创建多个预测值来提供方差减少(例如，图2中的<em class="mu"> D </em>、<em class="mu"> D </em>)。最后，通过简单多数投票来聚合树。为了被用作预测器，LCE在每个节点中存储由基础学习器生成的模型。</p><h2 id="49cb" class="na ly iq bd lz nb nc dn md nd ne dp mh lb nf ng mj lf nh ni ml lj nj nk mn nl bi translated">缺失数据</h2><p id="a291" class="pw-post-body-paragraph ks kt iq ku b kv mp jr kx ky mq ju la lb mr ld le lf ms lh li lj mt ll lm ln ij bi translated">我们选择<strong class="ku ir">本地处理丢失的数据。</strong>与XGBoost类似，LCE排除了分割的缺失值，并使用块传播。在节点分割期间，块传播将所有丢失数据的样本发送到错误较少的决策节点端。</p><h2 id="3223" class="na ly iq bd lz nb nc dn md nd ne dp mh lb nf ng mj lf nh ni ml lj nj nk mn nl bi translated">超参数</h2><p id="a07e" class="pw-post-body-paragraph ks kt iq ku b kv mp jr kx ky mq ju la lb mr ld le lf ms lh li lj mt ll lm ln ij bi translated">LCE的超参数是基于树的学习中的经典参数(例如，<code class="fe nm nn no np b">max_depth</code>、<code class="fe nm nn no np b">max_features</code>、<code class="fe nm nn no np b">n_estimators</code>)。此外，LCE在树的每个节点学习特定的XGBoost模型，并且它只需要指定XGBoost超参数的范围。然后，每个XGBoost模型的<strong class="ku ir">超参数由Hyperopt </strong> [Bergstra等人，2011]自动设置，Hyperopt是一种使用Parzen估计器算法树的基于序列模型的优化。Hyperopt从先前的选择和基于树的优化算法中选择下一个超参数。Parzen树估计器满足或超过超参数设置的网格搜索和随机搜索性能。LCE超参数的完整列表可在其<a class="ae kr" href="https://lce.readthedocs.io/en/latest/api.html" rel="noopener ugc nofollow" target="_blank">文档</a>中找到。</p><h2 id="9a5a" class="na ly iq bd lz nb nc dn md nd ne dp mh lb nf ng mj lf nh ni ml lj nj nk mn nl bi translated">公布的结果</h2><p id="79fc" class="pw-post-body-paragraph ks kt iq ku b kv mp jr kx ky mq ju la lb mr ld le lf ms lh li lj mt ll lm ln ij bi translated">我们最初在[Fauvel等人，2019]中为特定应用设计了LCE，然后在[Fauvel等人，2022]中的公共UCI数据集[Dua和Graff，2017]上对其进行了评估。结果表明，LCE平均获得了比包括随机森林和XGBoost在内的最新分类器更好的预测性能。</p><h1 id="4c14" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">Python包和代码示例</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/a9cfb25f6140111623ccdee54b0bd3ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*5sH0YJFO6hCXxJL__-LJEg.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图片由作者提供。</p></figure><h2 id="ae38" class="na ly iq bd lz nb nc dn md nd ne dp mh lb nf ng mj lf nh ni ml lj nj nk mn nl bi translated">装置</h2><p id="e2d5" class="pw-post-body-paragraph ks kt iq ku b kv mp jr kx ky mq ju la lb mr ld le lf ms lh li lj mt ll lm ln ij bi translated">LCE有一个<a class="ae kr" href="https://pypi.org/project/lcensemble/" rel="noopener ugc nofollow" target="_blank"> Python包</a> (Python ≥ 3.7)。可以使用<code class="fe nm nn no np b">pip</code>进行安装:</p><pre class="kg kh ki kj gt nr np ns nt aw nu bi"><span id="732f" class="na ly iq np b gy nv nw l nx ny">pip install lcensemble</span></pre><p id="e6c0" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">或者<code class="fe nm nn no np b">conda</code>:</p><pre class="kg kh ki kj gt nr np ns nt aw nu bi"><span id="c534" class="na ly iq np b gy nv nw l nx ny">conda install -c conda-forge lcensemble</span></pre><h2 id="aae5" class="na ly iq bd lz nb nc dn md nd ne dp mh lb nf ng mj lf nh ni ml lj nj nk mn nl bi translated">代码示例</h2><p id="8a5e" class="pw-post-body-paragraph ks kt iq ku b kv mp jr kx ky mq ju la lb mr ld le lf ms lh li lj mt ll lm ln ij bi translated">LCE包<strong class="ku ir">兼容scikit-learn</strong>；它通过<a class="ae kr" href="https://scikit-learn.org/stable/modules/generated/sklearn.utils.estimator_checks.check_estimator.html#sklearn.utils.estimator_checks.check_estimator" rel="noopener ugc nofollow" target="_blank">检查估计器</a>。因此，它可以与scikit-learn管道和模型选择工具进行交互。以下示例说明了LCE在公共数据集上用于分类和回归任务的情况。还显示了包含缺失值的数据集上的LCE的示例。</p><ul class=""><li id="c75b" class="nz oa iq ku b kv kw ky kz lb ob lf oc lj od ln oe of og oh bi translated">在<a class="ae kr" href="https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noopener ugc nofollow" target="_blank">虹膜数据集</a>上使用LCE进行分类</li></ul><p id="ec65" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">Iris数据集上的这个示例说明了如何训练LCE模型并将其用作预测器。它还通过使用<code class="fe nm nn no np b">cross_val_score</code>演示了LCE与scikit-learn模型选择工具的兼容性。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div></figure><ul class=""><li id="8a37" class="nz oa iq ku b kv kw ky kz lb ob lf oc lj od ln oe of og oh bi translated">缺失值虹膜数据集上的LCE分类</li></ul><p id="7d0d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这个例子说明了LCE对缺失值的稳健性。Iris训练集被修改为每个变量有20%的缺失值。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div></figure><ul class=""><li id="848f" class="nz oa iq ku b kv kw ky kz lb ob lf oc lj od ln oe of og oh bi translated">对<a class="ae kr" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html" rel="noopener ugc nofollow" target="_blank">糖尿病数据集</a>进行LCE回归</li></ul><p id="616b" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">最后，这个例子展示了如何在回归任务中使用LCE。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h1 id="91d8" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">结论</h1><p id="a8f3" class="pw-post-body-paragraph ks kt iq ku b kv mp jr kx ky mq ju la lb mr ld le lf ms lh li lj mt ll lm ln ij bi translated">本文介绍了LCE，一种用于一般分类和回归任务的新集成方法，以及相应的Python包。有关LCE的更多信息，请参考发表在<em class="mu">数据挖掘和知识发现</em>杂志上的相关<a class="ae kr" href="https://hal.inria.fr/hal-03599214/document" rel="noopener ugc nofollow" target="_blank">论文</a>。</p><h1 id="c91f" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">参考</h1><p id="373f" class="pw-post-body-paragraph ks kt iq ku b kv mp jr kx ky mq ju la lb mr ld le lf ms lh li lj mt ll lm ln ij bi translated">J.伯格斯特拉、巴登内、本吉奥和凯格尔。超参数优化算法。2011年第24届国际神经信息处理系统会议论文集。</p><p id="d325" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">长度布莱曼。打包预测值。机器学习，24(2):123–140，1996年。</p><p id="d0eb" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">长度布莱曼。随机森林。机器学习，45(1):5–32，2001。</p><p id="f47c" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">T.陈和C. Guestrin。XGBoost:一个可扩展的树提升系统。2016年第22届ACM SIGKDD知识发现和数据挖掘国际会议论文集。</p><p id="ce32" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">D.杜瓦和c .格拉夫。UCI机器学习知识库，2017年。</p><p id="50dd" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">K.Fauvel，V. Masson，E. Fromont，P. Faverdin和A. Termier。走向可持续的奶牛管理——发情检测的机器学习增强方法。2019年第25届ACM SIGKDD知识发现和数据挖掘国际会议论文集。</p><p id="a0ef" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">K.Fauvel、E. Fromont、V. Masson、P. Faverdin和A. Termier。XEM:一种用于多元时间序列分类的可解释的设计集成方法。数据挖掘和知识发现，36(3):917–957，2022。</p><p id="ed4c" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">R.沙皮雷。弱可学性的力量。机器学习，5(2):197–227，1990年。</p></div></div>    
</body>
</html>