<html>
<head>
<title>Comprehensive Guide to Principal Component Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析综合指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/comprehensive-guide-to-principal-component-analysis-bb4458fff9e2#2022-02-09">https://towardsdatascience.com/comprehensive-guide-to-principal-component-analysis-bb4458fff9e2#2022-02-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a5f5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">主成分分析的理论解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7d16af363c42aea5f1be08fbec48d4cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pqm4utPeIVClOEOvqn8KCQ.png"/></div></div></figure><p id="0c8f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当您想要减少大型<a class="ae lq" href="https://databasecamp.de/en/data" rel="noopener ugc nofollow" target="_blank">数据</a>集中的变量数量时，可以使用主成分分析(简称:PCA)。PCA试图只留下那些在<a class="ae lq" href="https://databasecamp.de/en/data" rel="noopener ugc nofollow" target="_blank">数据</a>集中仍然解释大部分方差的变量。因此，<a class="ae lq" href="https://databasecamp.de/en/statistics/correlation-and-causation" rel="noopener ugc nofollow" target="_blank">与其他特征强烈相关的所有特征都被移除。</a></p><h1 id="a932" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">我们什么时候用主成分分析？</h1><p id="98e2" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">当<a class="ae lq" href="https://databasecamp.de/en/data" rel="noopener ugc nofollow" target="_blank">数据集</a>具有相互<a class="ae lq" href="https://databasecamp.de/en/statistics/correlation-and-causation" rel="noopener ugc nofollow" target="_blank">相关</a>的变量，即相互依赖时，各种算法，如<a class="ae lq" href="https://databasecamp.de/en/ml/linear-regression-basics" rel="noopener ugc nofollow" target="_blank">线性回归</a>，会出现问题。对于这样的模型，重要的是我们在不减少<a class="ae lq" href="https://databasecamp.de/en/data" rel="noopener ugc nofollow" target="_blank">数据</a>的信息内容的情况下减少维度。</p><p id="140c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">PCA的另一个应用是在聚类分析中，比如k-means聚类，我们需要预先定义聚类的个数。减少<a class="ae lq" href="https://databasecamp.de/en/data" rel="noopener ugc nofollow" target="_blank">数据集</a>的维度有助于我们获得信息的第一印象，并能够估计，例如，哪些是最重要的变量，以及<a class="ae lq" href="https://databasecamp.de/en/data" rel="noopener ugc nofollow" target="_blank">数据集</a>可能有多少个聚类。</p><h1 id="a70d" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">PCA是如何工作的？</h1><p id="28db" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">主成分分析的核心思想是，一个数据集中可能有几个变量测量同一事物，即相关的。因此，不同的维度可以组合成更少的所谓主成分，而不会损害数据集的有效性。例如，体型与鞋码有很高的相关性，因为在许多情况下，高个子也有较大的鞋码，反之亦然。因此，如果我们把鞋码作为一个变量从数据集中去掉，信息量并没有真正减少。</p><p id="9356" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在<a class="ae lq" href="https://databasecamp.de/en/statistics" rel="noopener ugc nofollow" target="_blank">统计</a>中，一个数据集的信息量是由方差决定的。这表示数据点离中心有多远。方差越小，数据点越接近平均值，反之亦然。因此，较小的方差表明平均值已经是数据集的良好估计值。</p><p id="1cd3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在第一步中，PCA试图找到最大化数据集的解释方差的变量。然后，逐步添加更多的变量来解释方差的剩余部分，因为方差，即与均值的偏差，包含了最多的信息。如果我们想基于它训练一个模型，就应该保留它。</p><p id="7009" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">第一步，主成分分析试图找到一条尽可能使它和数据点之间的距离最小的直线。该程序与<a class="ae lq" href="https://databasecamp.de/en/ml/linear-regression-basics" rel="noopener ugc nofollow" target="_blank">线性回归</a>中的程序相同。因此，该线是数据集所有单个特征的总和，并形成第一主成分。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/3bc34dfc2a7245802f50bd429e8658d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gQfBPbuOmc-NSr0D.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">第一主成分|图片:作者</p></figure><p id="e658" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后尝试创建与第一主分量正交(即垂直)的第二条线，并再次最小化到数据点的距离。这些线必须彼此正交，因为主成分不应该彼此相关，并且因为垂直线也很可能解释不包含在第一成分中的方差。</p><h1 id="acba" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">目标是多少个主成分？</h1><p id="0b75" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">基本上，在主成分的数量和剩余的信息内容之间有一个<a class="ae lq" href="https://databasecamp.de/en/statistics/correlation-and-causation" rel="noopener ugc nofollow" target="_blank">相关性</a>。这意味着，随着更多的组成部分，你也解释了更多的差异，从而有更多的信息包含在数据集中。而极少的成分，则意味着维度大大降低了，这就是主成分分析的目的。</p><p id="0cd9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然而，根据Kaiser (1960 ),有一个很好的参考点，根据这个参考点可以选择组件。应该只选择方差大于1的主成分。因为只有这些成分比数据集中的单个变量能解释更多的变化，这可能并确实导致维数减少。</p><h1 id="7f06" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">如何解释主成分？</h1><p id="23a8" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">主成分本身很难解释，因为它们是各维度的线性组合。因此，它们代表了几个变量的加权混合。但是，在实际应用中，这些变量的组合也可以具体解释。</p><p id="842b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">例如，考虑一个包含各种个人信息的数据集，比如年龄、体重、身高、信用度、收入、储蓄和债务。例如，在这个数据集中，可能出现两个主要成分。第一个主成分大概由信用度、收入、储蓄和债务等维度组成，这些变量的系数很高。例如，这个主成分可以被解释为这个人的财务稳定性。</p><h1 id="a068" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">主成分分析我们需要什么要求？</h1><p id="d596" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">与类似的统计分析相比，为了获得有意义的结果，主成分分析只有几个必须满足的要求。<a class="ae lq" href="https://databasecamp.de/en/data" rel="noopener ugc nofollow" target="_blank">数据集</a>应具备的基本属性有:</p><ul class=""><li id="3b6e" class="mt mu it kw b kx ky la lb ld mv lh mw ll mx lp my mz na nb bi translated">特征之间的<a class="ae lq" href="https://databasecamp.de/en/statistics/correlation-and-causation" rel="noopener ugc nofollow" target="_blank">相关性</a>应该是线性的。</li><li id="74b5" class="mt mu it kw b kx nc la nd ld ne lh nf ll ng lp my mz na nb bi translated"><a class="ae lq" href="https://databasecamp.de/en/data" rel="noopener ugc nofollow" target="_blank">数据集</a>应该没有异常值，即严重偏离质量的单个<a class="ae lq" href="https://databasecamp.de/en/data" rel="noopener ugc nofollow" target="_blank">数据</a>点。</li><li id="8a76" class="mt mu it kw b kx nc la nd ld ne lh nf ll ng lp my mz na nb bi translated">如果可能，变量应该是连续的。</li><li id="449c" class="mt mu it kw b kx nc la nd ld ne lh nf ll ng lp my mz na nb bi translated"><a class="ae lq" href="https://databasecamp.de/en/statistics/population-and-sample" rel="noopener ugc nofollow" target="_blank">样本</a>越大，PCA的结果越好。</li></ul><p id="6f1a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">并非所有的<a class="ae lq" href="https://databasecamp.de/en/data" rel="noopener ugc nofollow" target="_blank">数据集</a>都可以直接用于主成分分析。必须确保<a class="ae lq" href="https://databasecamp.de/en/data" rel="noopener ugc nofollow" target="_blank">数据</a>近似为<a class="ae lq" href="https://databasecamp.de/en/statistics/normal-distribution" rel="noopener ugc nofollow" target="_blank">正态分布</a>和区间比例，即两个数值之间的区间始终具有相同的间距。例如，日期是按时间间隔计算的，因为从1980年1月1日到1981年1月1日的时间间隔与从2020年1月1日到2021年1月1日的时间间隔相同(闰年除外)。最重要的是，区间缩放必须由用户自己判断，并且不能通过标准化的统计测试来检测。</p><h1 id="3842" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">Python中的主成分分析</h1><p id="c5c9" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">有许多程序可以自动计算主成分分析，并且可以将结果与不同数量的成分进行比较。在<a class="ae lq" href="https://databasecamp.de/en/python-coding" rel="noopener ugc nofollow" target="_blank"> Python </a>中，这在模块“Scikit-Learn”的帮助下工作，在这里我们还将仔细查看其<a class="ae lq" href="https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html#sphx-glr-auto-examples-decomposition-plot-pca-iris-py" rel="noopener ugc nofollow" target="_blank">示例</a>。</p><p id="50ed" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在该应用中，使用了所谓的<a class="ae lq" href="https://archive.ics.uci.edu/ml/datasets/iris" rel="noopener ugc nofollow" target="_blank">虹膜数据集</a>。这是机器学习领域中一个流行的训练数据集。这是来自生物学的数据，更精确的信息来自所谓的鸢尾属植物。对于每一朵花，花瓣的长度和宽度以及所谓的萼片都是可用的。我们将有关植物的信息存储在变量X中，将相应花朵的名称存储在变量y中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="036c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在我们的例子中，我们试图将这四个维度减少到三个主要部分，以便能够在三维图中可视化它们。数据的实际转换发生在三行代码中。首先，我们需要设置一个PCA对象，其中包含所需数量的组件。然后，我们可以将其应用于我们的数据集，并最终将四维值转换为三维值:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj ni l"/></div></figure><p id="ec05" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在Matplotlib的帮助下，我们的结果可以在三维图中可视化，并且可以看到，即使在三维空间中，同一花卉物种的植物仍然彼此接近。因此，数据集的真实信息内容没有丢失。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk ni l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/ad687b4fc5ec57e49bd6fa016b9cf6f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xe9zXXftBUO7x1Kd.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">主成分分析和虹膜数据集|图片:作者</p></figure><h1 id="0d3f" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">这是你应该带走的东西</h1><ul class=""><li id="939f" class="mt mu it kw b kx mj la mk ld nl lh nm ll nn lp my mz na nb bi translated">主成分分析用于大型数据集中的降维。</li><li id="dd5e" class="mt mu it kw b kx nc la nd ld ne lh nf ll ng lp my mz na nb bi translated">它有助于基于它的机器学习模型的数据预处理，如聚类分析或线性回归。</li><li id="9d17" class="mt mu it kw b kx nc la nd ld ne lh nf ll ng lp my mz na nb bi translated">数据集中必须满足某些先决条件，PCA才有可能实现。例如，特征之间的相关性应该是线性的。</li></ul><h1 id="dbbe" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">关于主成分分析主题的其他文章</h1><ul class=""><li id="0723" class="mt mu it kw b kx mj la mk ld nl lh nm ll nn lp my mz na nb bi translated">你可以在我们的同事Studyflix 找到关于主成分分析的详细解释，包括一个说明性的视频。</li><li id="6eb5" class="mt mu it kw b kx nc la nd ld ne lh nf ll ng lp my mz na nb bi translated">对驱动因素分析的已知、当前和新的要求。载于:凯勒等人(编辑。):Marktforschung der Zukunft——门还是机器？，威斯巴登，2016年，第231–243页。</li><li id="3a10" class="mt mu it kw b kx nc la nd ld ne lh nf ll ng lp my mz na nb bi translated">电子计算机在因素分析中的应用。载于:教育和心理测量，1960年第1期，第141-151页。</li></ul></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><p id="fc8c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="nv">如果你喜欢我的作品，请在这里订阅</em><a class="ae lq" href="https://medium.com/subscribe/@niklas_lang" rel="noopener"><em class="nv"/></a><em class="nv">或者查看我的网站</em> <a class="ae lq" href="http://www.databasecamp.de/en/homepage" rel="noopener ugc nofollow" target="_blank"> <em class="nv">数据大本营</em> </a> <em class="nv">！还有，medium允许你每月免费阅读</em> <strong class="kw iu"> <em class="nv"> 3篇</em> </strong> <em class="nv">。如果你希望有</em><strong class="kw iu"><em class="nv">无限制的</em> </strong> <em class="nv">访问我的文章和数以千计的精彩文章，不要犹豫，点击我的推荐链接:</em><a class="ae lq" href="https://medium.com/@niklas_lang/membership" rel="noopener">【https://medium.com/@niklas_lang/membership】</a>每月花$<strong class="kw iu"><em class="nv">5</em></strong><em class="nv">获得会员资格</em></p></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><div class="kj kk kl km gt nw"><a rel="noopener follow" target="_blank" href="/learn-coding-13-free-sites-to-help-you-do-it-9b2c1b92e573"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd iu gy z fp ob fr fs oc fu fw is bi translated">学习编码:13个免费网站帮助你开始</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">一旦你决定要学习编码，你会被众多的在线工具宠坏，这些工具可以帮助你…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="og l oh oi oj of ok ks nw"/></div></div></a></div><div class="ol om gp gr on nw"><a rel="noopener follow" target="_blank" href="/introduction-to-random-forest-algorithm-fed4b8c8e848"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd iu gy z fp ob fr fs oc fu fw is bi translated">随机森林算法简介</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">算法是如何工作的，我们可以用它来做什么</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="oo l oh oi oj of ok ks nw"/></div></div></a></div><div class="ol om gp gr on nw"><a rel="noopener follow" target="_blank" href="/understanding-mapreduce-with-the-help-of-harry-potter-5b0ae89cc88"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd iu gy z fp ob fr fs oc fu fw is bi translated">借助《哈利·波特》理解MapReduce</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">MapReduce是一种允许并行处理大型数据集的算法，例如，在多台计算机上…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="op l oh oi oj of ok ks nw"/></div></div></a></div></div></div>    
</body>
</html>