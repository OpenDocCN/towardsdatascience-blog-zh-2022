<html>
<head>
<title>k-Nearest Neighbors for Lithology Classification from Well Logs Using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Python 对测井记录进行岩性分类的 k-最近邻</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-nearest-neighbors-for-lithology-classification-from-well-logs-using-python-a13ac69367ce#2022-11-29">https://towardsdatascience.com/k-nearest-neighbors-for-lithology-classification-from-well-logs-using-python-a13ac69367ce#2022-11-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8b7a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">基于测井测量细分地下</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8aafa1a78aad36ff35c324a6823af0c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BvTG2XdzK6WExd3S"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@jdubs?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">王占山</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="a8c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">k-最近邻(kNN)是一种流行的非参数监督机器学习算法，可以应用于分类和基于回归的问题。它很容易用 Python 实现，也很容易理解，这使得它成为一个很好的算法，可以在您开始机器学习之旅时开始学习。</p><p id="1dad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将介绍 kNN 算法如何工作，以及如何使用 Python 的 Scikit-Learn 库将其应用于测井数据。</p><h2 id="bc27" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">kNN 算法是如何工作的？</h2><p id="8934" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">对数据进行分类是机器学习的主要应用之一。因此，有许多算法可用。kNN 算法就是其中之一。</p><p id="530f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">kNN 背后的想法非常简单。彼此靠近的点被认为是相似的。</p><p id="beba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当一个新的数据点被引入到一个训练数据集中时，发生以下步骤</p><ol class=""><li id="305d" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">确定 k 值-用于分类新数据点的点数</li><li id="e839" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">计算待分类数据点与 k 个最近点之间的距离(欧几里德或曼哈顿)</li><li id="8a37" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">识别 k 个最近邻</li><li id="047b" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">在这些 k 个最近邻中，我们计算每个类中数据点的数量</li><li id="72a2" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">使用多数表决，将新数据点分配给出现次数最多的类别</li></ol><p id="6328" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面这个简单的例子展示了这个过程，我们假设 k 是 3，最近的点都是一个类。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/9a8f8318d6704e380d075b30d587672f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J2JAR7Gek5DVLAg4q6ZjgA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">k-最近邻应用于新数据点的示例，其中 k=3。图片由作者提供。</p></figure><p id="246b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 k-最近邻是混合类的情况下，我们可以使用多数表决，如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/b4eaccf23a5d459355a1d9ce5b4de135.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WK4iLherWD0685MoDpyRew.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">k-最近邻应用于新数据点的示例，其中 k=5，最近点是类的混合。图片由作者提供。</p></figure><h2 id="1f71" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">k 近邻(kNN)的应用</h2><ul class=""><li id="89b0" class="mt mu it lb b lc mo lf mp li nj lm nk lq nl lu nm mz na nb bi translated">推荐系统</li><li id="1fb6" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated">模式检测—例如欺诈检测</li><li id="b244" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated">文本挖掘</li><li id="544e" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated">气候预测</li><li id="5fea" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated">信用评级分析</li><li id="b785" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated">医学分类</li><li id="2c97" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated">岩性预测</li></ul><h2 id="e629" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">k 近邻(kNN)的优势</h2><ul class=""><li id="e4c8" class="mt mu it lb b lc mo lf mp li nj lm nk lq nl lu nm mz na nb bi translated">简单易懂</li><li id="6fda" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated">使用 Sci-kit Learn 通过 Python 轻松实现</li><li id="c06c" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated">可以快速处理小型数据集</li><li id="ee8d" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated">无需调整多个参数</li><li id="9d97" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated">不需要对数据做出假设</li><li id="6327" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated">可以应用于二元和多类问题</li></ul><h2 id="a326" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">缺点</strong>k-最近邻(kNN)</h2><ul class=""><li id="c212" class="mt mu it lb b lc mo lf mp li nj lm nk lq nl lu nm mz na nb bi translated">使用大型数据集进行分类可能会很慢</li><li id="13e3" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated">受到维数灾难的影响-随着要素数量的增加，算法可能难以做出准确的预测</li><li id="05df" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated">可能对数据比例敏感，即使用不同单位测量的特征</li><li id="993d" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated">受到噪音和异常值的影响</li><li id="ffe9" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated">对不平衡的数据集敏感</li><li id="9a7f" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated">在使用算法之前，需要处理缺失值</li></ul><h1 id="be47" class="nn lw it bd lx no np nq ma nr ns nt md jz nu ka mg kc nv kd mj kf nw kg mm nx bi translated">用 Scikit 实现 KNN-学习相分类</h1><h2 id="b498" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">导入所需的库</h2><p id="61ae" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">对于本教程，我们需要一些 Python 库和模块。</p><p id="5c58" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们将把<code class="fe ny nz oa ob b">pandas</code>作为<code class="fe ny nz oa ob b">pd</code>导入。这个库允许我们从 csv 文件中加载数据，并将数据存储在内存中以备后用。</p><p id="f476" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，我们还有许多来自 sci-kit 学习库中的模块:</p><ul class=""><li id="4959" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu nm mz na nb bi translated"><code class="fe ny nz oa ob b">KNeighborsClassifer</code>进行 kNN 分类</li><li id="cfae" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated"><code class="fe ny nz oa ob b">train_test_split</code>用于将我们的数据分成训练和测试数据集</li><li id="0611" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated"><code class="fe ny nz oa ob b">StandardScaler</code>用于标准化特征的比例</li><li id="1b69" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated"><code class="fe ny nz oa ob b">classification_report</code>、<code class="fe ny nz oa ob b">confusion_matrix</code>和<code class="fe ny nz oa ob b">accuracy_score</code>用于评估模型性能</li></ul><p id="92a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，为了可视化我们的数据，我们将混合使用<code class="fe ny nz oa ob b">matplotlib</code>和<code class="fe ny nz oa ob b">seaborn</code>。</p><pre class="kj kk kl km gt oc ob od bn oe of bi"><span id="54ee" class="og lw it ob b be oh oi l oj ok">import pandas as pd<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.metrics import classification_report, confusion_matrix, accuracy_score<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt</span></pre><h2 id="4a46" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">导入所需的数据</h2><p id="9eee" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">下一步是加载我们的数据。</p><p id="9dd2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在本教程中使用的数据集是作为 Xeek 和 FORCE 2020 <em class="ol"> (Bormann et al .，2020) </em>举办的机器学习竞赛的一部分使用的训练数据集的子集。它是在挪威政府的 NOLD 2.0 许可下发布的，详细信息可以在这里找到:<a class="ae ky" href="https://data.norge.no/nlod/en/2.0/" rel="noopener ugc nofollow" target="_blank">挪威开放政府数据许可(NLOD) 2.0 </a>。</p><p id="6e31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完整的数据集可以通过以下链接获得:<a class="ae ky" href="https://doi.org/10.5281/zenodo.4351155" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.5281/zenodo.4351155</a>。</p><p id="291c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了读取数据，我们可以调用<code class="fe ny nz oa ob b">pd.read_csv()</code>并传入训练文件的相对位置。</p><pre class="kj kk kl km gt oc ob od bn oe of bi"><span id="a95e" class="og lw it ob b be oh oi l oj ok">df = pd.read_csv('Data/Xeek_train_subset_clean.csv')</span></pre><p id="965b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦加载了数据，我们就可以调用<code class="fe ny nz oa ob b">describe()</code>方法来查看数据集中的数字列。这为我们提供了特性的概述。</p><pre class="kj kk kl km gt oc ob od bn oe of bi"><span id="790b" class="og lw it ob b be oh oi l oj ok">df.describe()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/831078866a8c1117289ab16a0c9be7a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ACe7tZ1BTlI6qKjjUpWog.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Xeek Force 2020 测井岩性竞赛数据统计。图片由作者提供。</p></figure><h1 id="eb81" class="nn lw it bd lx no np nq ma nr ns nt md jz nu ka mg kc nv kd mj kf nw kg mm nx bi translated">数据准备</h1><h2 id="3561" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">处理缺失数据</strong></h2><p id="ffdd" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在我们继续 kNN 算法之前，我们首先需要进行一些数据准备。</p><p id="ef6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于 kNN 算法不处理丢失的值，我们需要首先处理这些。最简单的方法是执行列表式删除。如果行中的任何要素有缺失值，这将删除该行。</p><p id="f3af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">强烈建议您对数据集进行全面分析，以了解丢失数据的原因以及是否可以修复。</p><p id="b67d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管这种方法看起来是一种快速的解决方案，但它可以显著减少数据集。</p><pre class="kj kk kl km gt oc ob on oo aw op bi"><span id="573c" class="lv lw it ob b gy oq or l os ok">df = df.dropna()</span></pre><h2 id="a2cc" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">选择培训和测试功能</h2><p id="7078" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">接下来，我们需要选择哪些特性将用于构建 kNN 模型，以及哪些特性将成为我们的目标特性。</p><p id="1930" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这个例子，我使用一系列测井测量来建立模型，并使用岩性描述作为目标特征。</p><pre class="kj kk kl km gt oc ob on oo aw op bi"><span id="fb30" class="lv lw it ob b gy oq or l os ok"># Select inputs and target<br/>X = df[['RDEP', 'RHOB', 'GR', 'NPHI', 'PEF', 'DTC']]<br/>y = df['LITH']</span></pre><p id="a865" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与任何机器学习模型一样，我们需要将数据分成一个训练集和一个测试集，前者用于训练/构建我们的模型，后者用于验证我们的模型在未知数据上的性能。</p><pre class="kj kk kl km gt oc ob od bn oe of bi"><span id="1614" class="og lw it ob b be oh oi l oj ok">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)</span></pre><h2 id="5d5b" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">标准化特征值</h2><p id="f039" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">当使用不同比例和范围的测量时，标准化它们是很重要的。这有助于减少模型训练时间，并减少对依赖基于距离的计算的模型的影响。</p><p id="a778" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">标准化数据本质上包括计算特征的平均值，从每个数据点中减去它，然后除以特征的标准偏差。</p><p id="06d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<a class="ae ky" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">scikit-learn</strong></a><strong class="lb iu"/>中，我们可以使用<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">standard scaler</strong></a>类来转换我们的数据。</p><p id="e947" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们使用训练数据来拟合模型，然后使用<code class="fe ny nz oa ob b">fit_transform</code>函数对其进行转换。</p><p id="5300" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当涉及到测试数据时，我们不想让 StandardScaler 适应这些数据，因为我们已经这样做了。相反，我们只是想应用它。这是使用<code class="fe ny nz oa ob b">transform</code>方法完成的。</p><p id="565f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">值得注意的是，在训练测试分割之后应用标准缩放器，并且它仅适合于训练数据集。一旦定标器模型被拟合，它就被应用于测试数据集。这有助于防止数据从测试数据集泄漏到 kNN 模型中。</p><pre class="kj kk kl km gt oc ob od bn oe of bi"><span id="c0b8" class="og lw it ob b be oh oi l oj ok">scaler = StandardScaler()<br/><br/>#Fit the StandardScaler to the training data<br/>X_train = scaler.fit_transform(X_train)<br/><br/># Apply the StandardScaler, but not fit, to the validation data<br/>X_test = scaler.transform(X_test)</span></pre><h1 id="b281" class="nn lw it bd lx no np nq ma nr ns nt md jz nu ka mg kc nv kd mj kf nw kg mm nx bi translated">构建 kNN 分类器</h1><p id="a1f7" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">创建 KNeighborsClassifier 时，我们可以指定几个参数。这些的全部细节可以在<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">这里</strong> </a>找到。当然，我们不必提供任何东西，将使用默认参数。</p><p id="526e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">默认情况下，用于分类新数据点的点数设置为 5。这意味着这 5 个最近点的类别将用于对该新点进行分类。</p><pre class="kj kk kl km gt oc ob od bn oe of bi"><span id="cda4" class="og lw it ob b be oh oi l oj ok">clf = KNeighborsClassifier()</span></pre><p id="e855" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦分类器被初始化，我们接下来需要使用我们的训练数据来训练模型(<code class="fe ny nz oa ob b">X_train</code> &amp; <code class="fe ny nz oa ob b">y_train</code>)。为此，我们调用<code class="fe ny nz oa ob b">clf</code>，然后调用<code class="fe ny nz oa ob b">fit</code>方法。</p><p id="33b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<code class="fe ny nz oa ob b">fit</code>方法中，我们传递我们的训练数据。</p><pre class="kj kk kl km gt oc ob od bn oe of bi"><span id="f122" class="og lw it ob b be oh oi l oj ok">clf.fit(X_train, y_train)</span></pre><h1 id="063d" class="nn lw it bd lx no np nq ma nr ns nt md jz nu ka mg kc nv kd mj kf nw kg mm nx bi translated">使用 kNN 模型进行预测</h1><p id="4f1c" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">一旦模型被训练，我们现在可以通过从分类器调用<code class="fe ny nz oa ob b">predict</code>方法来对我们的测试数据进行预测。</p><pre class="kj kk kl km gt oc ob on oo aw op bi"><span id="2669" class="lv lw it ob b gy oq or l os ok">y_pred = clf.predict(X_test)</span></pre><h1 id="339f" class="nn lw it bd lx no np nq ma nr ns nt md jz nu ka mg kc nv kd mj kf nw kg mm nx bi translated">评估模型性能</h1><h2 id="f0c9" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">使用模型精度</h2><p id="2b9e" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">为了理解我们的模型在测试数据上的表现，我们可以使用一些度量标准和工具。</p><p id="5ba7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们想要快速评估我们的模型表现如何，我们可以调用准确性评分方法。这为我们提供了相对于预测总数有多少预测是正确的指示。</p><pre class="kj kk kl km gt oc ob od bn oe of bi"><span id="c4e9" class="og lw it ob b be oh oi l oj ok">accuracy_score(y_test, y_pred)</span></pre><p id="7e2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将返回值 0.8918532439941167，并告诉我们我们的模型已经正确预测了 89.2%的标签。</p><p id="ff21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，这个值可能会产生误导，尤其是在我们处理不平衡的数据集时。如果有一个阶级占优势，那么这个阶级比少数阶级有更高的机会被正确预测。占主导地位的类将通过提高准确度分数来影响准确度分数，从而给人一种我们的模型做得很好的假象。</p><h2 id="ef7c" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">使用分类报告</h2><p id="79b6" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们可以进一步评估，看看分类报告。这提供了额外的度量以及每个类的预测程度的指示。</p><p id="32a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其他指标包括:</p><ul class=""><li id="49de" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu nm mz na nb bi translated"><strong class="lb iu"> precision: </strong>指示该类中有多少值被正确预测。值介于 0.0 和 1.0 之间，1 表示最好，0 表示最差。</li><li id="fff1" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated"><strong class="lb iu"> recall: </strong>提供了分类器能够为该类找到所有阳性案例的程度的度量。</li><li id="f3b4" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated"><strong class="lb iu"> f1-score: </strong>精度和召回率的加权调和平均值，生成 1.0(好)和 0.0(差)之间的值。</li><li id="bb46" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nm mz na nb bi translated"><strong class="lb iu">支持:</strong>这是数据集中该类的实例总数。</li></ul><p id="0df1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了查看分类报告，我们可以调用下面的代码，并将<code class="fe ny nz oa ob b">y_test</code>和<code class="fe ny nz oa ob b">y_pred</code>传递给<code class="fe ny nz oa ob b">classification_report</code>函数。</p><pre class="kj kk kl km gt oc ob od bn oe of bi"><span id="c099" class="og lw it ob b be oh oi l oj ok">print(classification_report(y_test, y_pred))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/e3955df657884fe0318c187078b5fd9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*RVb62GXRBJrHklXzyIx7-w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在 Scikit-learn 中使用 k 最近邻进行岩性预测的分类报告。图片由作者提供。</p></figure><p id="cdf4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们仔细观察结果，我们会发现我们正在处理一个不平衡的数据集。我们可以看到页岩、砂岩和石灰岩类占主导地位，因此具有相对较高的精确度和召回分数。而石盐、凝灰岩和白云石的精度和召回率相对较低。</p><p id="b927" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这一点上，我会考虑回到原始数据集，并确定我可以处理这种不平衡的方法。这样做应该会大大提高模型性能。</p><h2 id="ea8d" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">混淆矩阵</h2><p id="6547" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们可以使用另一个工具来查看我们的模型表现如何，这就是混淆矩阵。此工具总结了我们的分类模型在对每个类别进行预测时的表现。</p><p id="73a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">生成的混淆矩阵有两个轴。一个轴包含模型预测的类，另一个轴包含实际的类标签。</p><p id="2374" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以在 Python 中生成两个版本。第一个是一个简单的混乱矩阵的打印读数，可能很难阅读或向他人展示。第二个是使用 seaborn 生成的热图版本</p><pre class="kj kk kl km gt oc ob od bn oe of bi"><span id="94bc" class="og lw it ob b be oh oi l oj ok"># Simple Printed Confusion Matrix<br/>cf_matrix = confusion_matrix(y_test, y_pred)<br/>print(cf_matrix)<br/><br/># Graphical version using seaborn and matplotlib<br/># Prepare the labels for the axes<br/>labels = ['Shale', 'Sandstone', 'Sandstone/Shale', <br/>        'Limestone', 'Tuff', 'Marl', 'Anhydrite', <br/>        'Dolomite', 'Chalk', 'Coal', 'Halite']<br/>labels.sort()<br/><br/># Setup the figure<br/>fig = plt.figure(figsize=(10,10))<br/>ax = sns.heatmap(cf_matrix, annot=True, cmap='Reds', fmt='.0f',<br/>                xticklabels=labels, <br/>                yticklabels = labels)<br/>ax.set_title('Seaborn Confusion Matrix with labels\n\n')<br/>ax.set_xlabel('\nPredicted Values')<br/>ax.set_ylabel('Actual Values ');</span></pre><p id="4ec1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们运行上面的代码时，我们得到了下面的打印表格和图表。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/39215fd8d8fa24ad82afd760926b19a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*1VXVV0Z-GhgeQgYa_Qvodg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">测井测量岩性 kNN 分类预测结果的混淆矩阵。图片由作者提供。</p></figure><p id="af32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由此产生的混淆矩阵为我们提供了模型正确和错误预测的类别的指示。我们可以开始识别模型可能预测错误岩性的任何模式。</p><p id="b458" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，如果我们看看石灰石类。我们可以看到 2613 个点被正确预测，然而，185 个点被预测为白垩，135 个点被预测为泥灰。这两种岩性都具有方解石性质，并具有与石灰石相似的性质。因此，我们可以回过头来看看我们的功能，以确定是否需要其他功能，或者是否需要删除一些功能。</p><h1 id="cd8f" class="nn lw it bd lx no np nq ma nr ns nt md jz nu ka mg kc nv kd mj kf nw kg mm nx bi translated">摘要</h1><p id="e120" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">k-最近邻算法是一种功能强大但易于理解的监督机器学习算法，可应用于基于分类的问题，尤其是在地球科学领域。</p><p id="6450" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本教程展示了我们如何进行一系列预先分类的测井测量，并对新数据进行预测。然而，在预处理数据和处理不平衡数据集时应该小心，这在地下应用中是常见的。</p></div><div class="ab cl ov ow hx ox" role="separator"><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa"/></div><div class="im in io ip iq"><p id="e22a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ol">感谢阅读。在你走之前，你一定要订阅我的内容，把我的文章放到你的收件箱里。</em> <a class="ae ky" href="https://andymcdonaldgeo.medium.com/subscribe" rel="noopener"> <strong class="lb iu"> <em class="ol">你可以在这里做！</em></strong></a><strong class="lb iu"><em class="ol"/></strong><em class="ol">或者，您也可以</em> <a class="ae ky" href="https://fabulous-founder-2965.ck.page/2ca286e572" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="ol">注册我的简讯</em> </strong> </a> <em class="ol">免费将更多内容直接发送到您的收件箱。</em></p><p id="b75c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其次，你可以通过注册成为会员来获得完整的媒介体验，并支持我和成千上万的其他作家。每月只需花费你 5 美元，你就可以接触到所有精彩的媒体文章，也有机会通过写作赚钱。如果你用 <a class="ae ky" href="https://andymcdonaldgeo.medium.com/membership" rel="noopener"> <strong class="lb iu"> <em class="ol">我的链接</em> </strong> </a> <strong class="lb iu"> <em class="ol">，</em> </strong> <em class="ol">报名，你直接用你的一部分费用支持我，不会多花你多少钱。如果你这样做了，非常感谢你的支持！</em></p></div></div>    
</body>
</html>