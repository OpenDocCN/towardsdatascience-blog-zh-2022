<html>
<head>
<title>Accelerating and scaling Temporal Graph Networks on the Graphcore IPU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图核IPU上时态图网络的加速和缩放</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/accelerating-and-scaling-temporal-graph-networks-on-the-graphcore-ipu-c15ac309b765#2022-06-14">https://towardsdatascience.com/accelerating-and-scaling-temporal-graph-networks-on-the-graphcore-ipu-c15ac309b765#2022-06-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="322e" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">缩放GNNs</h2><div class=""/><div class=""><h2 id="1a05" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">图形神经网络(GNNs)标准硬件的适用性是图形ML社区中经常被忽视的问题。在这篇文章中，我们将探讨在Graphcore开发的新硬件架构上实现时态GNNs，该架构是为图形结构的工作负载量身定制的。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/22ef39885a8db41847a6a76c3a29d4a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jd-5ORBRBUH9cldt2Ux_gg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Graphcore弓IPU机。图片:图表核心</p></figure><p id="d5fd" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">这篇文章与</em> <a class="ae mb" href="https://twitter.com/emaros96" rel="noopener ugc nofollow" target="_blank"> <em class="ma">伊曼纽·罗西</em> </a> <em class="ma">和</em> <a class="ae mb" href="https://twitter.com/Daniels_Data" rel="noopener ugc nofollow" target="_blank"> <em class="ma">丹尼尔·贾斯特斯</em> </a> <em class="ma">合著，基于与英国半导体公司</em><a class="ae mb" href="https://www.graphcore.ai/" rel="noopener ugc nofollow" target="_blank"><em class="ma">graph core</em></a><em class="ma">的合作。</em></p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="0e92" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated">图结构的数据出现在许多处理交互实体的复杂系统的问题中。近年来，将机器学习方法应用于图结构数据的方法，特别是图神经网络(GNNs)，已经见证了流行的巨大增长。</p><p id="e2fd" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">大多数GNN架构都假设图形是<em class="ma">固定的</em>。然而，这种假设往往过于简单:因为在许多应用程序中，底层系统是动态的，图表会随着时间而变化。这是例如社交网络或推荐系统中的情况，其中描述用户与内容的交互的图形可以实时改变。最近开发了几个能够处理动态图的GNN架构，包括我们自己的<a class="ae mb" rel="noopener" target="_blank" href="/temporal-graph-networks-ab8f327f2efe?sk=741e5d00c64bff1e98ed0e2ce248013a">时态图网络</a> (TGNs) [1]。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ms"><img src="../Images/d10e387cf801d08bb18ef2627a70f895.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cTrg5CC40a2e_3y3"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">通过在时间戳t  ₁ <em class="mt">和t </em> ₂.获得新的边，人与人之间的交互图是动态变化的</p></figure><p id="fa16" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">在</span>这篇文章中，我们探索了TGNs在不同大小的动态图中的应用，并研究了这类模型的计算复杂性。我们使用Graphcore的<a class="ae mb" href="https://www.graphcore.ai/products" rel="noopener ugc nofollow" target="_blank"> Bow智能处理单元</a> (IPU)来训练TGNs，并展示为什么IPU的架构非常适合解决这些复杂性，在将单个IPU处理器与NVIDIA A100 GPU进行比较时，速度提高了一个数量级。</p><p id="96a2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">TGN架构在<a class="ae mb" rel="noopener" target="_blank" href="/temporal-graph-networks-ab8f327f2efe?sk=741e5d00c64bff1e98ed0e2ce248013a">我们之前的帖子</a>中有详细描述，由两个主要组件组成:首先，节点嵌入是通过经典的图形神经网络架构生成的，这里实现为单层图形注意力网络[2]。此外，TGN还保留了一个总结每个节点过去所有交互的内存。这种存储通过稀疏读/写操作来访问，并使用门控循环网络(GRU) [3]通过新的交互来更新。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mu"><img src="../Images/b62ae76268174b127e7a62a96419de08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qraN7DMSKiQbdQRk"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="mt"> TGN建筑。底部一行代表一个只有一个消息传递步骤的GNN。顶行说明了图形中每个节点的额外内存。</em></p></figure><p id="1b9f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们关注通过获得新边而随时间变化的图。在这种情况下，给定节点的存储器包含关于以该节点为目标的所有边以及它们各自的目的节点的信息。通过间接贡献，给定节点的记忆也可以保存关于更远的节点的信息，从而使得图形注意力网络中的附加层可有可无。</p><h1 id="ffb6" class="mv mw iq bd mx my mz na nb nc nd ne nf kf ng kg nh ki ni kj nj kl nk km nl nm bi translated">对小图的应用</h1><p id="154b" class="pw-post-body-paragraph le lf iq lg b lh nn ka lj lk no kd lm ln np lp lq lr nq lt lu lv nr lx ly lz ij bi translated">我们首先在JODIE Wikipedia数据集[4]上试验TGN，这是一个Wikipedia文章和用户的二分图，其中用户和文章之间的每条边都代表用户对文章的编辑。该图由9，227个节点(8，227个用户和1，000篇文章)和157，474条带时间戳的边组成，这些边用描述编辑的172维LIWC特征向量[5]进行了注释。</p><p id="6df7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在训练期间，边被逐批插入到最初断开的节点集合中，同时使用真实边和随机采样的负边的对比损失来训练模型。验证结果被报告为在随机采样的负边缘上识别真实边缘的概率。</p><p id="8d00" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">直观地说，大批量对训练和推理都有不利的后果:节点内存和图的连通性都只在处理完一整批后才更新。因此，一个批次中的后续事件可能依赖于过时的信息，因为它们不知道该批次中的早期事件。事实上，我们观察到大批量对任务性能的不利影响，如下图所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ms"><img src="../Images/67cce02735c3833cfa3c33f77895efe9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*i4WDpkXcG-IIgeFv"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="mt">当使用不同的批量大小进行训练并使用固定的批量大小10进行验证时(左)，以及当使用固定的批量大小10进行训练并使用不同的批量大小进行验证时(右)，TGN对JODIE/Wikipedia数据的准确性。</em></p></figure><p id="058d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated">然而，小批量的使用强调了快速存储器访问对于在训练和推理期间实现高吞吐量的重要性。因此，与批量较小的GPU相比，具有较大处理器内存的IPU显示出了更大的吞吐量优势，如下图所示。特别地，当使用10 TGN的批量时，在IPU上的训练可以快大约11倍，并且即使使用200的大批量，在IPU上的训练仍然快大约3倍。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ms"><img src="../Images/58590697b07ec66d4b539d8273c48e48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*trQOXhJnqD-3xnc6"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="mt">与NVIDIA A100 GPU相比，使用Bow2000 IPU系统中的单个IPU时，不同批量的吞吐量有所提高。</em></p></figure><p id="4b41" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">为了</span>更好地了解graph core IPU上TGN培训吞吐量的提高，我们调查了不同硬件平台在TGN关键操作上花费的时间。我们发现，花费在GPU上的时间主要由注意力模块和GRU决定，这两种操作在IPU上执行效率更高。此外，在所有操作中，IPU处理小批量产品的效率更高。</p><p id="eaf9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">特别是，我们观察到IPU的优势随着更小和更分散的内存操作而增长。更一般地说，我们的结论是，当计算和内存访问非常异构时，IPU架构比GPU显示出显著的优势。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/4e74da7965a1640cf690d0cd6d3c1061.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/0*gmXGRalaeLMpQKNm"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="mt">TGN在不同批量的IPU和GPU上关键操作的时间对比。</em></p></figure><h1 id="40f4" class="mv mw iq bd mx my mz na nb nc nd ne nf kf ng kg nh ki ni kj nj kl nk km nl nm bi translated">缩放至大图</h1><p id="7a5b" class="pw-post-body-paragraph le lf iq lg b lh nn ka lj lk no kd lm ln np lp lq lr nq lt lu lv nr lx ly lz ij bi translated">虽然默认配置中的TGN模型相对较轻，只有大约260，000个参数，但是当将该模型应用于大型图形时，大多数IPU处理器内存都由节点内存使用。然而，由于它很少被访问，这个张量可以被移动到片外存储器，在这种情况下，处理器内存储器的利用率与图形的大小无关。</p><p id="cfdb" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">为了</span>在大型图上测试TGN架构，我们将它应用于一个匿名图，该图包含1550万Twitter用户之间的2.61亿个关注[6]。这些边被分配728个不同的时间戳，这些时间戳尊重日期排序，但是不提供关于随后发生的实际日期的任何信息。由于该数据集中不存在节点或边要素，因此该模型完全依赖于图表拓扑和时间演变来预测新的链接。</p><p id="9e5f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">由于大量数据使得与单个负样本相比识别正边缘的任务过于简单，我们使用1000个随机采样的负边缘中的真实边缘的平均倒数排名(MRR)作为验证度量。此外，我们发现当增加数据集大小时，模型性能受益于更大的隐藏大小。对于给定的数据，我们将潜在大小256确定为准确性和吞吐量之间的最佳点。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nt"><img src="../Images/004527e7175d001597afaa1fed4e75cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dUDfVdi9TzdTHBtbOTI_vQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="mt">模型不同隐藏大小的1000个负样本的平均倒数排名。</em></p></figure><p id="6504" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di"> U </span>使用片外存储器作为节点存储器会使吞吐量降低大约两倍。然而，使用不同大小的诱导子图以及具有10倍Twitter图节点数和随机连通性的合成数据集，我们证明了吞吐量几乎与图的大小无关(见下表)。在IPU上使用这种技术，TGN可以应用于几乎任意大小的图，仅受可用主机内存量的限制，同时在训练和推理期间保持非常高的吞吐量。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ms"><img src="../Images/825fac41e0227a070244f4e8f7641d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ih9l-7-Gj4ns_WWs"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="mt">在不同图尺寸上训练隐藏尺寸为256的TGN每批尺寸为256的时间。Twitter-tiny与JODIE/Wikipedia数据集的大小相似。</em></p></figure></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="459b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">正如</span>我们之前已经<a class="ae mb" rel="noopener" target="_blank" href="/deep-learning-on-graphs-successes-challenges-and-next-steps-7d9ec220ba8?sk=b260a9446e9a25a0904acdddb693e0a8">反复提到过</a>，选择实现图形ML模型的硬件是一个至关重要的问题，但也是一个经常被忽视的问题。特别是在研究社区中，抽象出底层硬件的云计算服务的可用性导致了这方面的某些“懒惰”。然而，当涉及到实现在具有实时延迟要求的大规模数据集上工作的系统时，就不能再简单地考虑硬件了。我们希望我们的研究能引起更多人对这一重要课题的关注，并为未来图形ML应用的更有效的算法和硬件架构铺平道路。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="da96" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[1] E. Rossi等，<a class="ae mb" href="https://arxiv.org/abs/2006.10637" rel="noopener ugc nofollow" target="_blank">动态图上深度学习的时态图网络</a> (2020) arXiv:2006.10637 .参见随附的<a class="ae mb" rel="noopener" target="_blank" href="/temporal-graph-networks-ab8f327f2efe?sk=741e5d00c64bff1e98ed0e2ce248013a">博文</a>。</p><p id="0dea" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[2]p . veli kovi等人，图形注意网络(2018年)ICLR。</p><p id="a00a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[3] K. Cho等，<a class="ae mb" href="https://arxiv.org/abs/1409.1259" rel="noopener ugc nofollow" target="_blank">论神经机器翻译的性质:编码器-解码器方法</a> (2014)，arXiv:1409.1259。</p><p id="9035" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[4] S. Kumar等，<a class="ae mb" href="https://arxiv.org/abs/1710.10903" rel="noopener ugc nofollow" target="_blank">预测时态交互网络中的动态嵌入轨迹</a> (2019) KDD。</p><p id="beda" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[5] J. W. Pennebaker等人，《语言学调查和字数统计:LIWC 2001年》(2001年)。马威:劳伦斯·厄尔鲍姆联合公司71。</p><p id="665f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[6] A. El-Kishky等，<a class="ae mb" href="https://arxiv.org/pdf/2205.06205.pdf" rel="noopener ugc nofollow" target="_blank"> kNN-Embed:多兴趣候选检索的局部平滑嵌入混合</a> (2022) arXiv:2205.06205。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="2362" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">我们感谢Douglas Orr和Gianandrea Minneci为实施IPU TGNs所做的贡献。关于图的深度学习的附加文章，请参见《走向数据科学》中我的</em> <a class="ae mb" rel="noopener" target="_blank" href="https://towardsdatascience.com/graph-deep-learning/home"> <em class="ma">其他帖子</em> </a> <em class="ma">，</em> <a class="ae mb" href="https://michael-bronstein.medium.com/subscribe" rel="noopener"> <em class="ma">订阅我的帖子</em> </a> <em class="ma">，获取</em> <a class="ae mb" href="https://michael-bronstein.medium.com/membership" rel="noopener"> <em class="ma">中等会员</em> </a> <em class="ma">，或者关注我的</em><a class="ae mb" href="https://twitter.com/mmbronstein" rel="noopener ugc nofollow" target="_blank"><em class="ma">Twitter</em></a><em class="ma">。</em></p></div></div>    
</body>
</html>