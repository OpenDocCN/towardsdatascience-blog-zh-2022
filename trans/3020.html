<html>
<head>
<title>Permutative Language Modeling, Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">置换语言建模，解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/permutative-language-modeling-explained-9a7743d979b4#2022-07-01">https://towardsdatascience.com/permutative-language-modeling-explained-9a7743d979b4#2022-07-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="917f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">流行的变压器XLNet背后的核心功能解释如下</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/80a866cca73fc082ab3d80b8023b6bfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jewbOfkvuY4ub8iw"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">卢克·斯塔克波尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="f346" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将解释您需要了解的关于置换语言建模(PLM)的一切，以及它是如何与整个XLNet架构联系在一起的。</p><p id="4c68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在NLP中，像BERT这样功能强大的通用转换器是最先进的/标准的转换器，许多人将其用于流行的任务，如情感分析、问题回答、语言建模，甚至用于BERT必须微调的特殊任务。虽然BERT对于某些任务/设置非常好，但是一个强大的、不太为人所知的转换器是XLNet。XLNet是最强大的变压器之一，在20个不同的任务中超过BERT。</p><h2 id="6942" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">以前的方法</h2><p id="8398" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在最近几年的研究领域中，预训练语言模型因其在下游NLP任务中表现出色的能力而广受欢迎。基本上，它们真的很擅长针对非常具体的NLP任务进行微调。我说的语言模型是什么意思？在NLP中，语言模型是在给定序列中所有前面/其他单词的情况下，预测序列中的下一个单词的模型。通常，像BERT这样的现代语言模型是预先训练的，因为它们首先在普通英语语言(在维基百科这样的公共网站上)上进行训练，然后在更具体的下游任务上进行微调，如情感分析、问题回答等。这些语言模型通过为序列中的每个单词生成丰富的单词嵌入来工作，然后它们将这些用于其他特定的任务。</p><p id="78d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到XLNet为止，在语言建模中有两种主要的竞争方法。</p><h2 id="5d3b" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">自回归</strong></h2><p id="596d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">自回归方法使用先前时间步长的观测值来预测序列中下一个时间步长的值。本质上，想想LSTM/双LSTM人做语言建模的方式。它使用序列中已经预测的单词来预测序列中的下一个单词。虽然这种方法是第一种最先进的语言建模方法，但是它有一些局限性。</p><p id="250e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，当预测序列中的下一个单词时，自回归方法依赖于左边的单词或右边的单词。例如，当使用自回归语言模型预测句子中的第5个单词时，该模型仅使用信息/依赖于第5个单词左边的单词，或者第5个单词右边的单词(如果以向后的方式预测)。即使是双LSTM也没有真正以上下文相关的方式进行语言建模。双LSTM运行常规的从左到右的LSTM，然后是从右到左的LSTM，并使用一些线性图层将来自两个lstm的信息聚合在一起。它们独立地进行从左到右和从右到左的过程，而不是同时进行。这意味着在双LSTM中组合前向和后向信息真正发生在网络的末端，而不是网络内部。下面是一个双LSTM的架构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/14cdae1f4a2bcc4f141d442edc42b1fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2fKbNimvPd-nk2FW9Hwwhw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="86b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，自回归方法的另一个限制是，由于其固有的结构，序列中正确预测的下一个单词完全依赖于正确预测序列中前面单词的模型。举个例子，</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="621c" class="lv lw it mv b gy mz na l nb nc">P(w1w2w3) = P(w1) * P(w2|w1) * P(w3|w1w2) ... </span></pre><p id="751b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正确预测一行中三个单词(w1、w2和w3)的概率等于上面的公式。正如您所看到的，正确预测一行中的三个单词有许多依赖关系，每次将一个单词添加到序列中，它被正确预测的概率就会越来越低。我们的目标是最大化这个概率P(w1w2w3)，复合效应使得LSTMs的训练速度非常慢，因为每次训练它们时，它们的权重/精度都只有边际上的提高。</p><h2 id="56d4" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">自动编码</h2><p id="51e8" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">自动编码方法旨在解决自回归方法中存在的一些限制。使用自动编码方法的最著名的模型是像BERT这样的变形金刚。这种方法基于原始的transformer，旨在通过捕获双方的依赖关系来捕获双向上下文。</p><p id="f0e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用两种方法的组合对BERT进行预训练以学习语言/文本:掩蔽语言建模和下一句预测。虽然对于BERT来说，这是一种理解文本和接受英语语言预训练的非常有效的方法，但是自动编码方法的主要局限性在于预训练中使用屏蔽语言建模。如果你不知道什么是屏蔽语言建模，它本质上是一个填空任务，BERT随机屏蔽输入中的一些标记，并训练自己使用屏蔽标记周围的上下文信息来预测屏蔽标记实际上是什么。这种方法的优势在于，它允许BERT使用来自输入中任何地方的信息进行组合，以生成更高级/更丰富的单词嵌入。这种方法的一个限制是，当预测一个屏蔽的标记时，输入中有其他屏蔽的标记，如果两个或更多屏蔽的标记相互依赖，会发生什么？BERT将无法捕获/学习这两个标记之间的上下文信息，从而使它为这些标记生成的编码/嵌入不够标准。BERT的另一个限制是它只能接受固定长度的序列:BERT最多只能接受512个令牌作为输入。另一方面，XLNet没有输入约束。</p><h2 id="1042" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">XLNet</h2><p id="e115" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">XLNet是一个自回归模型，旨在解决BERT和LSTMs带来的限制，它在许多任务上优于BERT。</p><p id="a908" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就像BERT一样，XLNet这样的语言模型必须能够生成单词嵌入矩阵。如果一个句子被输入到模型中，模型应该能够为每个单词输出一个固定长度的向量嵌入。为了创建这些丰富的、上下文化的嵌入，XLNet使用置换语言建模。</p><p id="649b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">给定一个序列<strong class="lb iu"><em class="nd">【x】</em></strong>，自回归模型(如LSTM或XLNet)是一种计算概率的模型，该概率为<strong class="lb iu"><em class="nd">P(Xi | x&lt;I)</em></strong>(|代表给定，因此这实质上意味着该模型在给定序列中所有0…i个单词出现的概率的情况下预测序列中第I个单词出现的概率)。这类模型(像LSTM的)是不对称的，并且没有从语料库中的所有标记关系中学习。LSTMs仅仅是序列的一种可能排列上的自回归模型:从左到右的记号顺序。</p><p id="ba6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">诸如ELMo之类的其他类型的自回归模型允许模型学习令牌的上下文表示。在ELMo的具体情况下，它可以允许模型以相反的顺序学习<strong class="lb iu"> <em class="nd"> P(xi | x &gt; i) </em> </strong>或上下文信息。与LSTM的模型相似，ELMo模型在序列的两种可能排列上是自回归的，向前和向后。除了ELMo之外，还有许多其他可能的组合来查看模型可以在哪里学习一些标记之间的有趣关系。例如，<strong class="lb iu"> <em class="nd"> P(xi | xi-1，xi+1) </em> </strong>，可以允许一些有趣的嵌入。本质上，记号的任何组合都可以让自回归模型学习有趣的关系，这是置换语言建模背后的基本基础。</p><p id="eec7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">XLNet本质上做了我上面描述的事情，但是针对一个序列的所有不同排列。考虑一个有N=4个标记的序列[狗，毕竟是哺乳动物]。看看4个都有的那套！排列<em class="nd"> Z = </em> [[1，2，3，4]，[1，3，2，4] … [4，3，2，1]]。XLNet是一个对所有这些排列都是自回归的模型。它预测一个记号<strong class="lb iu"><em class="nd"/></strong>的概率，给定所有先前的记号<strong class="lb iu"> <em class="nd"> x &lt; i </em> </strong>并且它可以计算序列的任何排列的概率。</p><p id="8207" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，给定前面的元素，它可以计算第3个元素的概率，并且它对任何排列都这样做，以便它可以看到前面的元素的哪个组合最有可能(最高概率)产生正确的第3个元素，即单词<em class="nd"> all </em>。考虑到所有4个！置换，XLNet考虑了令牌之间所有可能的依赖关系。</p><p id="aac1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然置换语言建模是XLNet的主要贡献，但它不是XLNet的唯一组件。XLNet还包含像注意力屏蔽这样的东西——这有助于它理解词序——和两个自我注意力流。你可以在这篇由Borealis AI撰写的文章中读到更多关于这些概念的内容。</p><h2 id="2812" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">XLNet的缺点</h2><p id="6376" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">虽然置换语言建模是<a class="ae ky" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank">论文</a>的主要贡献，并且它确实成功地克服了屏蔽语言建模问题，但是它有一些缺点。首先，也是最明显的，与BERT相比，XLNet通常计算量更大，训练时间更长。特别是在当前的研究环境中，许多关于变形金刚/语言模型的论文被训练得更快、资源更少。这方面的一个例子是像DistilBERT这样的模型。XLNet的另一个弱点是它在较小/较短的输入句子上表现不佳。在XLNet的预训练中，它主要是对长输入序列进行预训练，置换语言建模旨在捕捉长期依赖关系，因此XLNet不擅长短输入。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><p id="d98a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望您觉得这些内容很容易理解。如果你认为我需要进一步阐述或澄清什么，请在下面留言。</p><h2 id="d66b" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">参考</h2><p id="4bdd" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">XLNet:用于语言理解的广义自回归预训练:【https://arxiv.org/abs/1906.08237 T2】</p><p id="bd05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">北极光AI:了解XLNet:【https://www.borealisai.com/en/blog/understanding-xlnet/】T4</p></div></div>    
</body>
</html>