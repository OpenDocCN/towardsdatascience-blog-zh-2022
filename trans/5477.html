<html>
<head>
<title>Algorithmic Bias in Healthcare and Some Strategies for Mitigating It</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">医疗保健中的算法偏差及其缓解策略</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/algorithmic-bias-in-healthcare-and-some-strategies-for-mitigating-it-835abe28df7#2022-12-08">https://towardsdatascience.com/algorithmic-bias-in-healthcare-and-some-strategies-for-mitigating-it-835abe28df7#2022-12-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5485" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">当心你的人工智能系统中的偏见:有时这是生死攸关的事情</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/96c8afda90722f829547c577f432ca6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DxWYKdtUrd1_YhHX"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://unsplash.com/@nci" rel="noopener ugc nofollow" target="_blank">国立癌症研究所</a>在<a class="ae ky" href="https://unsplash.com/photos/701-FJcjLAQ" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="169a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="1a1b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">不知道你怎么样，我第一次接触 AI 是在全是智能机器人的电影里。他们的行为让我相信，整个世界都会被那些愤怒的机器征服。</p><p id="8414" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">但是我的信念已经改变了，因为现在，我相信人工智能可以做得比试图毁灭世界更好。</p><blockquote class="ms mt mu"><p id="0df7" class="lr ls mv lt b lu mn ju lw lx mo jx lz mw mp mc md mx mq mg mh my mr mk ml mm im bi translated">人工智能可以改善医疗保健，拯救世界各地数百万人的生命。</p></blockquote><h1 id="9e1b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">但是什么是人工智能🤖？</h1><p id="40ac" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">人工智能有多种定义，大多数都集中在同一个意思上。以下是我对人工智能的三点定义。</p><ul class=""><li id="a679" class="mz na it lt b lu mn lx mo ma nb me nc mi nd mm ne nf ng nh bi translated">这是教育算法使用大型复杂数据集像人类一样行动的过程。</li><li id="593c" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">它在训练过程中从数据中学习，以保持稳定，从而做出更好的决策。</li><li id="a33e" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">这些模型一旦在非常大的患者数据上进行训练，就可以做出比真正的人类专家更准确的预测。</li></ul><p id="e427" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">例如，在医疗保健领域，人工智能正被用于提供正确的诊断和治疗建议，所有这些都需要对结果进行清晰的解释。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/48842fe232b3a166dabbf83f69cf97b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j2uUJQQP9O69PK7BNVTyTg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">人工智能在医疗保健中的图示(图片由作者提供)</p></figure><p id="eb78" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Geoffrey_Hinton" rel="noopener ugc nofollow" target="_blank">被称为深度学习教父的杰弗里·辛顿</a>在 2016 年发表的声明引起了放射科医生的注意:</p><blockquote class="ms mt mu"><p id="5cac" class="lr ls mv lt b lu mn ju lw lx mo jx lz mw mp mc md mx mq mg mh my mr mk ml mm im bi translated">人们现在应该停止培训放射科医生。很明显，在 5 年内，深度学习将会比放射科医生做得更好</p></blockquote><p id="db63" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后，一年后，<a class="ae ky" href="https://arxiv.org/abs/1711.05225" rel="noopener ugc nofollow" target="_blank"> CheXNet </a>和来自斯坦福大学的 6 名放射科医生在接受了数千张图像(肺炎和非肺炎)的训练后，在最近的一次测试中进行了 50 次肺炎胸部 x 光检查。你猜怎么着:<code class="fe no np nq nr b">ChexNet</code>赢了<code class="fe no np nq nr b">81% accuracy</code>对<code class="fe no np nq nr b">79% accuracy of the 6 Radiologists</code>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/8ed7c47c62cab85a6ba9b285e044a856.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x97wXyAF0OTmCI6Mw8s0OA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">ChexNet Vs. 6 放射科医生在胸部 x 线诊断(图片由作者提供)</p></figure><p id="24fb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">出于训练目的，该模型被给予了数万张图像(肺炎和非肺炎)。</p><p id="e585" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是许多突出人工智能如何执行通常由医生执行的任务的项目之一。性能表明这些算法在诊断疾病方面变得更好了多少，甚至有时具有如上所示的更好的性能。</p><blockquote class="ms mt mu"><p id="9433" class="lr ls mv lt b lu mn ju lw lx mo jx lz mw mp mc md mx mq mg mh my mr mk ml mm im bi translated">等等，你是在告诉我们人工智能是医疗保健中所有完美的顶峰吗？</p></blockquote><h1 id="7b7d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">不，人工智能不仅仅是医疗领域的荣耀</h1><p id="4255" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">尽管有这些承诺，人工智能的使用并非没有法律风险。我们可以从这两个案例中说明人工智能是如何导致医疗保健领域对个人的歧视<strong class="lt iu"> <em class="mv">。</em> </strong>你可以从<a class="ae ky" href="https://medcitynews.com/2020/08/how-to-mitigating-algorithmic-bias-in-healthcare/?rf=1" rel="noopener ugc nofollow" target="_blank">如何减轻医疗中的算法偏差</a>的新闻中了解这两个案例的更多细节。</p><p id="7599" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">你可能知道，种族偏见仍然是一个问题，但当它被传播到医疗保健领域时，情况会变得更糟，因为在这里，这是一个生死攸关的问题。</p><h2 id="cdc8" class="nt la it bd lb nu nv dn lf nw nx dp lj ma ny nz ll me oa ob ln mi oc od lp oe bi translated">第一例→联合健康集团</h2><p id="0284" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Optum (UnitedHealth group)开发了一种商业算法，以确定哪些患者需要额外的医疗护理(医疗需求最大的患者)。</p><p id="9f18" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">算法中的偏差使得需要额外护理的黑人患者数量减少了一半以上，并错误地得出结论，黑人患者比同样患病的白人患者更健康。</p><blockquote class="ms mt mu"><p id="ed98" class="lr ls mv lt b lu mn ju lw lx mo jx lz mw mp mc md mx mq mg mh my mr mk ml mm im bi translated">为什么会这样？</p></blockquote><p id="5a56" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">任何明显的种族信息(如肤色、邮政编码等。)实际上是在算法训练过程中使用的。但种族与其他因素相关，如历史医疗支出，以评估未来的医疗需求，这使得它反映了经济不平等，而不是病人的真正医疗需求。</p><p id="e5e9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">根据<a class="ae ky" href="https://www.kff.org/" rel="noopener ugc nofollow" target="_blank">凯泽家族基金会</a> (KFF)…</p><ul class=""><li id="c56c" class="mz na it lt b lu mn lx mo ma nb me nc mi nd mm ne nf ng nh bi translated">家庭更有可能将年度预算的 1%用于医疗保健。</li><li id="6e39" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">低收入住房的平均医疗支出是 235 美元，而私人保险是 2401 美元。</li></ul><p id="eea5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们知道 Optum 正在通过连接和服务 150 个国家的整个卫生系统来推动卫生保健，我们已经可以想象这种偏见对人们生活的影响。</p><h2 id="7ac3" class="nt la it bd lb nu nv dn lf nw nx dp lj ma ny nz ll me oa ob ln mi oc od lp oe bi translated">第二个案例→新冠肺炎的药物发现</h2><p id="cce8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在新冠肺炎期间，开发了一个人工智能系统来对病人进行分类，并加快新疫苗的发现。</p><p id="3ef2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">人工智能系统能够以 70%至 80%的准确率预测哪些患者可能会患严重的肺部疾病。</p><p id="7885" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这个人工智能系统的问题是，分诊过程仅仅基于患者的症状和先前存在的状况，这可能是有偏见的，因为基于种族和社会经济地位的差异。</p><h1 id="43d9" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">减轻算法偏差的 5 种策略</h1><p id="c93c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这些年来，人工智能模型发展迅速。与此同时，我们也看到了许多失误。人工智能系统中的偏见最终导致了一个不公平的社会。</p><p id="c91e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">以下五个策略(不是一个详尽的列表)可以用来减轻这些偏见。</p><ol class=""><li id="6020" class="mz na it lt b lu mn lx mo ma nb me nc mi nd mm of nf ng nh bi translated"><strong class="lt iu"> <em class="mv">收集和使用多样化的训练数据</em> </strong>:算法中偏差的一个主要原因是使用了不代表真实世界人群的训练数据。为了缓解这种情况，收集和使用不同的训练数据是很重要的，这些数据准确地反映了将对其使用该算法的人群的人口统计、背景和特征。</li><li id="0571" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm of nf ng nh bi translated"><strong class="lt iu"> <em class="mv">测试算法的偏差</em> </strong>:在一个算法被训练之后，测试它的偏差是很重要的，以确保它做出公平和无偏见的决策。这可以通过多种方法来实现，包括进行偏见审计和使用公平指标来衡量算法的性能。</li><li id="bc70" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm of nf ng nh bi translated"><strong class="lt iu"> <em class="mv">使用算法公平技术</em> </strong>:有几种算法公平技术可以用来减轻算法中的偏差，包括调整数据以减少偏差的预处理算法，在训练过程中进行调整的处理中算法，以及调整算法输出以使其更加公平的后处理算法。</li><li id="7d41" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm of nf ng nh bi translated"><strong class="lt iu"> <em class="mv">确保透明度和问责制</em> </strong>:减少算法偏差的另一个重要步骤是确保算法透明和问责。这意味着对算法如何工作提供清晰的解释，定期审查和更新算法以消除任何可能引入的偏见，并为个人提供质疑算法所做决定的机制。</li><li id="021e" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm of nf ng nh bi translated"><strong class="lt iu"><em class="mv"/></strong>:最后，与多元化的利益相关者(包括可能受算法影响的个人和社区)进行互动非常重要，以便了解他们的观点，并将他们的反馈纳入算法的设计和实施中。这有助于确保算法是公平和公正的，并且它准确地反映了将要使用它的人群的需求和关注。</li></ol><h1 id="541b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">这些最佳实践的局限性。</h1><p id="170b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">偏见以不同的形式存在，处理起来很有挑战性，并且可能发生在开发生命周期的不同阶段。</p><ul class=""><li id="5a47" class="mz na it lt b lu mn lx mo ma nb me nc mi nd mm ne nf ng nh bi translated">历史偏见:当收集来训练人工智能系统的数据不再反映当前现实时，就会出现这种情况。例如，尽管性别薪酬差距仍然是一个问题，但在过去这一问题更为严重。</li><li id="cf76" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated"><strong class="lt iu">代表性偏差</strong>:这种偏差的结果取决于如何定义训练数据以及如何从人群中取样。这种场景的一个例子是用于训练第一个面部识别系统的数据，这些数据主要依赖于白人面孔，这导致模型很难识别黑人面孔和其他深色皮肤的面孔。</li><li id="ecb6" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated"><strong class="lt iu">测量偏差</strong>:当训练数据特征或测量与真实数据不同时出现。这是图像识别系统的常见问题，其中数据主要从一种类型的摄像机收集，而现实世界的数据来自多种类型的摄像机。</li><li id="41e5" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated"><strong class="lt iu">编码/人类偏见</strong>:这通常发生在科学家带着他们对研究的主观想法深入一个项目的时候。例如:<em class="mv">“非白人患者接受的心血管干预和肾脏移植较少”，以及“黑人妇女在被诊断患有乳腺癌后更有可能死亡”。</em> <a class="ae ky" href="https://www.jointcommission.org/resources/news-and-multimedia/newsletters/newsletters/quick-safety/quick-safety-issue-23-implicit-bias-in-health-care/implicit-bias-in-health-care/#.Y4VE3uzMJTY" rel="noopener ugc nofollow" target="_blank"> <em class="mv">来源</em> </a></li></ul><h2 id="90cf" class="nt la it bd lb nu nv dn lf nw nx dp lj ma ny nz ll me oa ob ln mi oc od lp oe bi translated">结论</h2><p id="ccfd" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">对于任何数据项目，尤其是医疗保健项目，不要忘记人工智能中的潜在偏见，这一点很重要。</p><p id="6559" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">早期采用正确的系统，并在整个项目生命周期中保持它们，可以帮助快速识别主要问题，并以有效的方式对它们做出响应。</p><p id="3ed9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">请记住，让我们非常小心，非常有意识地设计那些人工智能系统。</p><p id="6f32" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你喜欢阅读我的故事，并希望支持我的写作，考虑<a class="ae ky" href="https://zoumanakeita.medium.com/membership" rel="noopener">成为一个媒体成员</a>。每月支付 5 美元，你就可以无限制地阅读媒体上的故事。</p><p id="9fe7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">欢迎在<a class="ae ky" href="https://zoumanakeita.medium.com/" rel="noopener">媒体</a>、<a class="ae ky" href="https://twitter.com/zoumana_keita_" rel="noopener ugc nofollow" target="_blank">推特</a>和<a class="ae ky" href="https://www.youtube.com/channel/UC9xKdy8cz6ZuJU5FTNtM_pQ" rel="noopener ugc nofollow" target="_blank"> YouTube </a>上关注我，或者在<a class="ae ky" href="https://www.linkedin.com/in/zoumana-keita/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上打招呼。讨论人工智能、人工智能、数据科学、自然语言处理和人工智能是一种乐趣！</p><h1 id="d5ea" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">额外资源</h1><p id="379c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">健康算法中的种族偏见</p><p id="aa04" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://jamanetwork.com/journals/jama/fullarticle/2766098" rel="noopener ugc nofollow" target="_blank">新冠肺炎和种族/民族差异</a></p></div></div>    
</body>
</html>