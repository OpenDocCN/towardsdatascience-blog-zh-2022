<html>
<head>
<title>KNN Algorithm from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">KNN 算法从零开始</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/knn-algorithm-from-scratch-37febe0c15b3#2022-11-16">https://towardsdatascience.com/knn-algorithm-from-scratch-37febe0c15b3#2022-11-16</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="5eb1" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">KNN 算法的实现和细节解释</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/cbbaa25984d71b28937bf47113bcb1f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1wAA7n5c61P_Kffn"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">Guillermo Ferla 在<a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h2 id="1fd5" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">KNN 的背景</h2><p id="cf1c" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">KNN 代表 K 近邻。名字本身表明它考虑最近的邻居。它是一种有监督的机器学习算法。有趣的是，我们可以用该算法解决分类和回归问题。它是最简单的机器学习模型之一。虽然这是一个简单的模型，但有时它扮演着重要的角色，基本上是当我们的数据集很小，问题很简单的时候。该算法也称为懒惰算法。这些是 KNN 算法的总结。</p><p id="1271" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">我将从 KNN 最基本的东西开始解释，这样你就能用心理解这篇文章了。文末可以自己实现算法(不需要任何机器学习库)。</p></div><div class="ab cl mu mv hy mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="in io ip iq ir"><h2 id="02b7" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">目录</h2><ol class=""><li id="7f5d" class="nb nc iu ly b lz ma mc md lj nd ln ne lr nf mo ng nh ni nj bi translated"><code class="fe nk nl nm nn b"><a class="ae kz" href="#5bee" rel="noopener ugc nofollow"><strong class="ly iv">Euclidean Distance</strong></a></code></li><li id="ecdd" class="nb nc iu ly b lz no mc np lj nq ln nr lr ns mo ng nh ni nj bi translated"><code class="fe nk nl nm nn b"><a class="ae kz" href="#43fe" rel="noopener ugc nofollow"><strong class="ly iv">Overview of the KNN Algorithm</strong></a></code></li><li id="81e6" class="nb nc iu ly b lz no mc np lj nq ln nr lr ns mo ng nh ni nj bi translated"><code class="fe nk nl nm nn b"><a class="ae kz" href="#143a" rel="noopener ugc nofollow"><strong class="ly iv">Why is KNN a Lazy Algorithm?</strong></a></code></li><li id="3605" class="nb nc iu ly b lz no mc np lj nq ln nr lr ns mo ng nh ni nj bi translated"><code class="fe nk nl nm nn b"><a class="ae kz" href="#143a" rel="noopener ugc nofollow"><strong class="ly iv">Step-by-Step Implementation of KNN</strong></a></code></li></ol></div><div class="ab cl mu mv hy mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="in io ip iq ir"><h2 id="5bee" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">欧几里得距离</h2><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nt"><img src="../Images/dac2ed444da59e6d0911fd34d19f874f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CHb0DWx7F0PUcoNTYdFNmg.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="a534" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">这里，<code class="fe nk nl nm nn b"><strong class="ly iv"><em class="nu">(X1, Y1)</em></strong></code>和<code class="fe nk nl nm nn b"> <strong class="ly iv"><em class="nu">(X2, Y2)</em></strong></code>是图像中显示的两个点。我们可以用下面的公式计算两点之间的距离。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nv"><img src="../Images/589c790f4d2d6e030114e6db92512aa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*JNHZUeXeF-U8sKQKR_pFkw.png"/></div></figure><p id="fee7" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">如果我们有两个以上的特征，我们需要将距离的平方加到上面的公式中来得到距离。</p><h2 id="43fe" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">KNN 算法概述</h2><p id="7d79" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">该名称表明该算法考虑最近的元素来预测新数据的值。流程图显示了 KNN 的步骤。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nw"><img src="../Images/e9db4dfe70e52048268873237d26afa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*So4oKjLc4Y9rkPUAEGanxQ.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">KNN 算法流程图(图片由作者提供)</p></figure><p id="94cc" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated"><em class="nu">我来解释一下。</em></p><p id="6cda" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated"><strong class="ly iv">第一步:计算距离</strong></p><p id="af9b" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">首先，我们需要加载带标签的数据集，因为 KNN 算法是一种监督学习算法。看下图。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nx"><img src="../Images/fda2e68247d028aa74504cb0b02cb4b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*VA0Q_yftXffPVAtvNAsl0g.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">与数据点的距离(图片由作者提供)</p></figure><p id="f824" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">假设我们的数据集只有两个要素，我们绘制的数据如图所示。蓝点和红点表示两个不同的类别。让我们拥有新的未标记的数据，这些数据需要基于给定的数据集进行分类。</p><p id="9310" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">在图像中，需要对中心点进行分类。现在，我们将计算所有数据与未标记数据之间的距离。从中心点开始的箭头代表距离。</p><p id="48b4" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated"><strong class="ly iv">步骤 2:选择 K-最近邻</strong></p><p id="a4a4" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">在上一步中，我们计算了新点与所有其他数据的距离。我们将根据距离对数据点进行升序排序。最后，我们将考虑来自未标记数据的最近点的数量<strong class="ly iv"> <em class="nu"> K </em> </strong>。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj ny"><img src="../Images/f4d55485946e2d0b5e28325a25fd2669.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Uk26akyzZJiB7wxCPAmcA.jpeg"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="df5b" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">在上图中，我已经考虑了<strong class="ly iv"> 3 </strong>最近的数据点<strong class="ly iv"> <em class="nu"> (K=3) </em> </strong>。观察形象；在<strong class="ly iv">的 3 个</strong>最近点<strong class="ly iv">中，2 个</strong>数据属于红色类别，1 个属于蓝色类别。所以，红色是多数阶级。根据<strong class="ly iv"> <em class="nu"> KNN </em> </strong>算法，新的数据点将被归类为红色。</p><p id="286e" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated"><strong class="ly iv"> <em class="nu">遇到回归问题，我们会考虑 K 个最近数据点的平均值。</em> </strong></p><h2 id="b2ff" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">为什么 KNN 是一个懒惰的算法？</h2><p id="9971" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">KNN 没有训练期。对于每个预测，算法需要经历相同的过程。在训练期间没有可以优化的参数。所以，这是一个懒惰的算法。当数据集很大时，预测需要更长的时间。</p><h2 id="143a" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">从零开始实施 KNN</h2><p id="f577" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">让我们写几行代码来实现算法。</p><p id="ae31" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated"><em class="nu">导入模块。</em></p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="nz oa l"/></div></figure><p id="eb76" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">创建计算距离的函数。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="ob oa l"/></div></figure><p id="f702" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated"><code class="fe nk nl nm nn b"><strong class="ly iv">euclidean</strong></code> <strong class="ly iv"> </strong>函数有两个参数，即<code class="fe nk nl nm nn b">p1</code>和<code class="fe nk nl nm nn b">p2</code>。根据<code class="fe nk nl nm nn b"><strong class="ly iv">Euclidean Distance</strong></code> <strong class="ly iv"> </strong>部分解释的公式，该函数将计算从<code class="fe nk nl nm nn b">p1</code>点到<code class="fe nk nl nm nn b">p2</code>点的距离。</p><p id="3f1b" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">下一步，我们将编写一个函数，用于保存数据集的每个点与新数据点的距离，并整理数据。最后，我们将选择具有多数类的新数据点的类。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="oc oa l"/></div></figure><p id="3271" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">我们已经创建了<code class="fe nk nl nm nn b"><strong class="ly iv">‘predict’ </strong></code>函数来查找一组新数据点的预测。让我们使用我们的<code class="fe nk nl nm nn b"> ‘predict’</code>函数来获得<code class="fe nk nl nm nn b"><strong class="ly iv">iris </strong></code>数据集的预测。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="od oa l"/></div></figure><p id="88e1" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">这里，我们已经手动选择了训练和测试数据。我们首先将数据随机化以防止偏差。然后我们选择 80%的数据进行训练，剩下的进行测试。最后，我们针对 7 个最近邻居(k=7)测试了我们的模型。T3】</p><p id="e92c" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">文章[1]帮助我实现了 KNN 算法。</p><p id="8c8e" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated"><strong class="ly iv"> <em class="nu">大功告成。我们从零开始实施 KNN。我们喝杯咖啡，思考一下算法。如果有任何困惑，不要忘记发表评论(或联系我)。</em> </strong></p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj oe"><img src="../Images/71ca9007a35c031f6e6b22673aaf0a23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MPPTeaC82cO5XcWb"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">照片由<a class="ae kz" href="https://unsplash.com/@kylejglenn?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">凯尔·格伦</a>在<a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h2 id="b071" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">结论</h2><p id="71c3" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">KNN 算法似乎非常简单。但有时，它在解决重要的机器学习问题方面发挥着重要作用。当我们的数据有噪音时，我们需要解决简单的问题。总是跑向深度学习模型是不可取的，因为它需要巨大的计算能力和数据。如果我们总是盲目地跳过深度学习模型，我们不会得到一个好的结果。良好的实践是对所有的 ML 模型有深入的直觉，并通过分析问题做出适当的决策。</p></div><div class="ab cl mu mv hy mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="in io ip iq ir"><p id="eb7e" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">此处提供了完整的实现。</p><div class="of og gq gs oh oi"><a href="https://deepnote.com/workspace/zubair063-9767-9eddd116-b682-475a-9479-3807be1e71db/project/KNN-from-Scratch-87efde85-a815-4ac6-9936-9818d283592d/notebook/Notebook%201-58df07ed20a54f0d971497c24bbc9eef" rel="noopener  ugc nofollow" target="_blank"><div class="oj ab fp"><div class="ok ab ol cl cj om"><h2 class="bd iv gz z fq on fs ft oo fv fx it bi translated">深度笔记</h2><div class="op l"><h3 class="bd b gz z fq on fs ft oo fv fx dk translated">面向数据科学家和研究人员的托管笔记本电脑。</h3></div><div class="oq l"><p class="bd b dl z fq on fs ft oo fv fx dk translated">deepnote.com</p></div></div><div class="or l"><div class="os l ot ou ov or ow kt oi"/></div></div></a></div><h2 id="7ccc" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">参考</h2><ol class=""><li id="5f7a" class="nb nc iu ly b lz ma mc md lj nd ln ne lr nf mo ng nh ni nj bi translated"><a class="ae kz" href="https://www.askpython.com/python/examples/k-nearest-neighbors-from-scratch" rel="noopener ugc nofollow" target="_blank">K-从零开始与皮索最近的邻居</a> n</li></ol></div><div class="ab cl mu mv hy mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="in io ip iq ir"><p id="f1e5" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated"><em class="nu">阅读关于数据科学统计的热门文章(完整指南)</em></p><div class="of og gq gs oh oi"><a rel="noopener follow" target="_blank" href="/ultimate-guide-to-statistics-for-data-science-a3d8f1fd69a7"><div class="oj ab fp"><div class="ok ab ol cl cj om"><h2 class="bd iv gz z fq on fs ft oo fv fx it bi translated">数据科学统计终极指南</h2><div class="op l"><h3 class="bd b gz z fq on fs ft oo fv fx dk translated">数据科学统计一瞥:标准指南</h3></div><div class="oq l"><p class="bd b dl z fq on fs ft oo fv fx dk translated">towardsdatascience.com</p></div></div><div class="or l"><div class="ox l ot ou ov or ow kt oi"/></div></div></a></div></div></div>    
</body>
</html>