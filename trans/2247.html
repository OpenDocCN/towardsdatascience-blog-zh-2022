<html>
<head>
<title>SBERT vs. Data2vec on Text Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">SBERT与Data2vec在文本分类上的比较</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sbert-vs-data2vec-on-text-classification-e3c35b19c949#2022-05-18">https://towardsdatascience.com/sbert-vs-data2vec-on-text-classification-e3c35b19c949#2022-05-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5bcc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用两个流行的预训练拥抱脸模型进行文本分类的代码演示</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9a8a0ef4da7db5656b01406a96202359.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NHW99vlTagmKDBfH5Ho8MA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="11d9" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">介绍</h1><p id="6b14" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">我个人确实相信，所有花哨的人工智能研究和先进的人工智能算法工作都只有极小的价值，如果不是零的话，直到它们可以应用于现实生活的项目，而无需向用户要求大量的资源和过多的领域知识。拥抱脸搭建了桥梁。拥抱脸是成千上万预先训练好的模型的家园，这些模型通过开源和开放科学为人工智能的民主化做出了巨大贡献。</p><p id="cab2" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">今天，我想给你一个端到端的代码演示，通过进行多标签文本分类分析来比较两个最受欢迎的预训练模型。</p><p id="6e5a" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">第一款是<a class="ae mo" href="https://www.sbert.net/" rel="noopener ugc nofollow" target="_blank">sentence transformers(SBERT)</a>。这实际上是由达姆施塔特科技大学的<a class="ae mo" href="https://www.informatik.tu-darmstadt.de/ukp/ukp_home/index.en.jsp" rel="noopener ugc nofollow" target="_blank">泛在知识处理实验室的团队为各种任务创建的一系列预训练模型的集合。我在以前的项目中使用过几次SBERT。他们甚至有一个python库，为您提供了不使用拥抱脸API和Pytorch框架的灵活性。点击这里查看</a><a class="ae mo" href="https://www.sbert.net/docs/installation.html" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="651d" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">第二个模型是<a class="ae mo" href="https://ai.facebook.com/blog/the-first-high-performance-self-supervised-algorithm-that-works-for-speech-vision-and-text/" rel="noopener ugc nofollow" target="_blank"> Data2vec </a>，这是一个由Meta(脸书)的AI团队提供的强大的预训练模型。它是一个自我监督的框架(教师模型→学生模型),为文本、音频和图像编码而设计。如果你对它是如何开发的感兴趣，你可以在这里找到原文<a class="ae mo" href="https://arxiv.org/abs/2202.03555" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="7fa4" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">数据</h1><p id="3ee0" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">对于数据，我使用一个著名的开源文本数据集:BBC新闻集团(它的许可证在这里:<a class="ae mo" href="https://opendatacommons.org/licenses/dbcl/1-0/" rel="noopener ugc nofollow" target="_blank">https://opendatacommons.org/licenses/dbcl/1-0/</a>)。您可以通过执行以下操作来加载数据:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者代码</p></figure><p id="a540" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">或者，你可以从我的GitHub repo中找到一个预处理过的CSV版本:<a class="ae mo" href="https://github.com/jinhangjiang/Medium_Demo/blob/main/Data2vec_vs_SBERT/bbc-text.csv" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/Jin hangjiang/Medium _ Demo/blob/main/data 2 vec _ vs _ SBERT/BBC-text . CSV</a></p><h1 id="25ea" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">代码演示</h1><h2 id="953c" class="mr kw iq bd kx ms mt dn lb mu mv dp lf lw mw mx lh ma my mz lj me na nb ll nc bi translated">步骤1:安装并导入我们需要的包</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者代码</p></figure><h2 id="8325" class="mr kw iq bd kx ms mt dn lb mu mv dp lf lw mw mx lh ma my mz lj me na nb ll nc bi translated">步骤2:拆分数据进行验证</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者代码</p></figure><p id="5ae8" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">这里注意一个细节:我使用的是CSV文件，而不是从sklearn导入数据。于是我给了输入数据为<strong class="lp ir">一个list (X.tolist()) </strong>。如果不这样做，模型稍后会抛出错误。</p><h2 id="d72c" class="mr kw iq bd kx ms mt dn lb mu mv dp lf lw mw mx lh ma my mz lj me na nb ll nc bi translated">第三步。对文本进行标记</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者代码</p></figure><p id="68b9" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">以下是一些澄清:</p><p id="c0bc" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir"> <em class="nd"> model_name </em> </strong>:该参数应为您要使用的预训练模型的名称字符串。你可以找到可用的型号:<a class="ae mo" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/models</a></p><p id="009b" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir"><em class="nd">【max _ length】</em></strong>:该参数将直接影响训练时间和训练速度。如果每个文档都很长，您可能希望指定模型为每个文档处理的文本的长度。</p><p id="6af5" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir"> <em class="nd">填充</em> </strong>:如果给定了max_length，则将该参数设置为True。填充到批中最长的序列(如果只提供一个序列，则不填充)</p><h2 id="10e0" class="mr kw iq bd kx ms mt dn lb mu mv dp lf lw mw mx lh ma my mz lj me na nb ll nc bi translated">步骤4:将嵌入转换成torch数据集</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者代码</p></figure><h2 id="e25a" class="mr kw iq bd kx ms mt dn lb mu mv dp lf lw mw mx lh ma my mz lj me na nb ll nc bi translated">第五步:给模特打电话</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者代码</p></figure><p id="768a" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir"><em class="nd">AutoModelForSequenceClassification</em></strong>:“AutoModel…”将帮助您自动识别要使用的正确型号。到目前为止，它对我来说很好。“……用于序列分类”专门用于分类问题。</p><p id="592d" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">to("cuda "):如果你的机器上有GPU，你可以在末尾添加这个函数来利用GPU的能力。如果没有此功能，训练时间通常会显著增加。</p><p id="7671" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">步骤6:定义评估指标</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者代码</p></figure><p id="2b4a" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir"> <em class="nd"> metrics_name </em> </strong>:这应该是一个字符串。对于我们的演示，我选择“f1”作为评估指标。你可以在这里找到可用的选项:<a class="ae mo" href="https://github.com/huggingface/datasets/tree/master/metrics" rel="noopener ugc nofollow" target="_blank">https://github.com/huggingface/datasets/tree/master/metrics</a></p><p id="5983" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir"> <em class="nd">平均</em> </strong>:我传递这个参数是因为我在用f1的分数来评估一个多标签分类问题。它不是一个通用参数。</p><h2 id="eaae" class="mr kw iq bd kx ms mt dn lb mu mv dp lf lw mw mx lh ma my mz lj me na nb ll nc bi translated">步骤7:微调预训练的模型</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者代码</p></figure><p id="3143" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">步骤8:保存您的微调模型，使其可重用</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者代码</p></figure><p id="3bec" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">当您想要使用您的微调模型时，您需要做的就是用您自己的模型的路径替换步骤3和步骤5中的模型名称字符串。</p><p id="8ac1" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">您可以在这里访问完整的代码脚本:<a class="ae mo" href="https://github.com/jinhangjiang/Medium_Demo/blob/main/Data2vec_vs_SBERT/Data2vec_vs_SBERT_512.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/Jin hangjiang/Medium _ Demo/blob/main/data 2 vec _ vs _ SBERT/data 2 vec _ vs _ SBERT _ 512 . ipynb</a></p><h1 id="fb38" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">结果</h1><h2 id="c121" class="mr kw iq bd kx ms mt dn lb mu mv dp lf lw mw mx lh ma my mz lj me na nb ll nc bi translated">对于SBERT(最大长度= 512，纪元= 5):</h2><p id="259b" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">f1最佳成绩:0.985034</p><p id="2618" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">f1平均得分:0.959524</p><p id="aed9" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">挂壁时间:15分36秒</p><p id="25dc" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">内存增加:5.0294 GB</p><h2 id="a960" class="mr kw iq bd kx ms mt dn lb mu mv dp lf lw mw mx lh ma my mz lj me na nb ll nc bi translated">对于Data2vec (max_length = 512，epoch = 5):</h2><p id="93e1" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">f1最佳成绩:0.976871</p><p id="2927" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">f1平均得分:0.957822</p><p id="0a67" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">挂壁时间:15分8秒</p><p id="7c0f" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">内存增加:0.3213 GB</p><p id="cc76" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">每次运行模型时，结果可能会有一些变化。总的来说，经过5次尝试，我可以得出结论，SBERT在最好的f1成绩方面有更好的表现，而Data2vec使用了更少的内存。两款车型的f1平均成绩非常接近。</p><p id="02da" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">看完今天的演示，您应该有以下收获:</p><ol class=""><li id="abe9" class="ne nf iq lp b lq mj lt mk lw ng ma nh me ni mi nj nk nl nm bi translated">如何使用预先训练好的模型来标记文本数据</li><li id="56aa" class="ne nf iq lp b lq nn lt no lw np ma nq me nr mi nj nk nl nm bi translated">如何正确地将您的数据转换为torch数据集</li><li id="ca9d" class="ne nf iq lp b lq nn lt no lw np ma nq me nr mi nj nk nl nm bi translated">如何利用和微调预先训练的模型</li><li id="d598" class="ne nf iq lp b lq nn lt no lw np ma nq me nr mi nj nk nl nm bi translated">如何保存您的模型以备将来使用</li><li id="4cc0" class="ne nf iq lp b lq nn lt no lw np ma nq me nr mi nj nk nl nm bi translated">Data2vec和SBERT的性能比较</li></ol><p id="40e7" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir"> <em class="nd">请随时与我联系</em></strong><a class="ae mo" href="https://www.linkedin.com/in/jinhangjiang/" rel="noopener ugc nofollow" target="_blank"><strong class="lp ir"><em class="nd">LinkedIn</em></strong></a><strong class="lp ir"><em class="nd">。</em>T13】</strong></p><h1 id="5309" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">参考</h1><p id="0cbe" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">Baevski等人(2022年)。data2vec:语音、视觉和语言自我监督学习的一般框架。<a class="ae mo" href="https://arxiv.org/pdf/2202.03555.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2202.03555.pdf</a></p><p id="8629" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">拥抱脸。(2022).文本分类。<a class="ae mo" href="https://huggingface.co/docs/transformers/tasks/sequence_classification" rel="noopener ugc nofollow" target="_blank">https://hugging face . co/docs/transformers/tasks/sequence _ class ification</a></p><p id="c6d7" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">Reimers，n .和Gurevych，I. (2019年)。句子伯特:使用暹罗伯特网络的句子嵌入。<a class="ae mo" href="https://arxiv.org/pdf/1908.10084.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1908.10084.pdf</a></p></div></div>    
</body>
</html>