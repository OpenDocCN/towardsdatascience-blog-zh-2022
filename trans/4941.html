<html>
<head>
<title>R-NL: Robust Nonlinear Shrinkage</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">R-NL:鲁棒非线性收缩</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/high-dimensional-covariance-estimation-when-tails-are-heavy-da61723ce9b3#2022-11-02">https://towardsdatascience.com/high-dimensional-covariance-estimation-when-tails-are-heavy-da61723ce9b3#2022-11-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b24d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">重尾时的高维协方差估计</h2></div><p id="f0ee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">在</span>这篇文章中，我讨论了我们最近在<a class="ae lk" href="https://arxiv.org/abs/2210.14854" rel="noopener ugc nofollow" target="_blank"> arXiv </a>上发表的论文“R-NL:基于非线性收缩的椭圆分布协方差矩阵估计”中的一种新的协方差估计方法。我介绍了我们正在解决的问题，尝试给出一些关于我们如何解决它的直觉，并简要介绍了我们开发的简单代码。在路上，我触及了一些有趣的概念，比如(健壮的)“泰勒估计量”，我觉得这些概念在现代数据科学中没有得到充分利用，可能是因为没有作者在他们的论文中提供代码(或者可能是因为大多数关于这个主题的论文似乎出现在信号处理社区中，这是他们被数据科学家忽视的另一个潜在原因)。</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><p id="7552" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">非线性收缩</strong></p><p id="df9c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/nonlinear-shrinkage-an-introduction-825316dda5b8">非线性收缩</a>是高维协方差估计的有力工具。在这篇新论文中，我和我的合著者介绍了一个经过修改的版本，它倾向于在重尾模型中产生更好的结果，同时在其他情况下保持强大的结果。为了展示这种新方法及其解决的问题，让我们从一个R示例开始。我们首先加载必要的函数，并定义维度<em class="ls"> p </em>和示例数量<em class="ls"> n，</em>以及我们想要研究的真实协方差矩阵。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="c77d" class="mc md iq ly b gy me mf l mg mh"># For simulating the data:<br/>library(mvtnorm)</span><span id="e042" class="mc md iq ly b gy mi mf l mg mh"># The NL/QIS method available on <a class="ae lk" href="https://github.com/MikeWolf007/covShrinkage/blob/main/qis.R" rel="noopener ugc nofollow" target="_blank">https://github.com/MikeWolf007/covShrinkage/blob/main/qis.R</a><br/>source("qis.R")</span><span id="fef6" class="mc md iq ly b gy mi mf l mg mh"># Set the seed, n and p<br/>set.seed(1)</span><span id="4c11" class="mc md iq ly b gy mi mf l mg mh"># p quite high relativ to n<br/>n&lt;-300<br/>p&lt;-200</span><span id="36f4" class="mc md iq ly b gy mi mf l mg mh">#Construct the dispersion matrix<br/>Sig&lt;-sapply(1:p, function(i) {sapply(1:p, function(j) 0.7^{abs(i-j)} )} )</span></pre><p id="71a1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们这里定义的协方差矩阵对应一个AR过程。也就是说，虽然观察值是独立的，但是尺寸<em class="ls"> X_i </em>和<em class="ls"> X_j </em>的相关性越小，则<em class="ls"> i </em>和<em class="ls"> j </em>之间的差值绝对值越大。这意味着相关性指数变小，离对角线越远，实际上有一些结构需要学习。</p><p id="e321" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，让我们用之前定义的相关结构模拟来自高斯分布的随机向量的独立同分布样本:</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="6b9b" class="mc md iq ly b gy me mf l mg mh">### Multivariate Gaussian case<br/>X&lt;-rmvnorm(n = n, sigma = Sig)</span></pre><p id="503e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<a class="ae lk" rel="noopener" target="_blank" href="/nonlinear-shrinkage-an-introduction-825316dda5b8">非线性收缩文章<em class="ls">、</em>、</a>中，我解释了如果只允许修改样本协方差矩阵的<em class="ls">特征值</em>，但必须保持<em class="ls">特征向量</em>不变(这是许多收缩方法所做的):让因此<strong class="kh ir"><em class="ls"/></strong><em class="ls">_ j</em>、<em class="ls"> j=1，..p </em>，是样本协方差矩阵的特征向量，并且</p><figure class="lt lu lv lw gt mk gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/d9f530866282a3be7047ffbf3bb10b54.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*g2g8SuAioFIpIuQjDVKdsw.png"/></div></figure><p id="8245" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">特征向量矩阵。那么最佳值由下式给出</p><figure class="lt lu lv lw gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mn"><img src="../Images/f8b2d6a93705bee5489d69a75436b160.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gm7X0gdnFwwU3UTsUG67uQ.png"/></div></div></figure><p id="c089" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从而产生最佳的估计量</p><figure class="lt lu lv lw gt mk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/73d19edc021c662e2c496c3c3d3a1028.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*Oarbjaw495ygtULYQXN_og.png"/></div></figure><p id="2aea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，根据(固定的)样本特征向量,( 1)中的最优值通常不完全等于真实特征值，而是真实特征值的线性组合。</p><p id="1adc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，让我们计算样本协方差矩阵和非线性收缩矩阵，并检查我们离理想情况有多近:</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="e816" class="mc md iq ly b gy me mf l mg mh">## Sample Covariance Matrix<br/>samplespectral&lt;-eigen(cov(X))</span><span id="2c77" class="mc md iq ly b gy mi mf l mg mh">## Nonlinear Shrinkage<br/>Cov_NL&lt;-qis(X)<br/>NLvals&lt;-sort( diag( t(samplespectral$vectors)%*%Cov_NL%*%samplespectral$vectors  ), decreasing=T)</span><span id="22aa" class="mc md iq ly b gy mi mf l mg mh">## Optimal: u_j'*Sig*u_j for all j=1,...,p<br/>optimalvals&lt;-sort(diag( t(samplespectral$vectors)%*%Sig%*%samplespectral$vectors  ), decreasing=T)</span><span id="37cc" class="mc md iq ly b gy mi mf l mg mh">plot(sort(samplespectral$values, decreasing=T), type="l", cex=1.5, lwd=2, lty=2, ylab="Eigenvalues",)<br/>lines(optimalvals, type="l", col="red", cex=1.5, lwd=2, lty=1)<br/>lines(NLvals, type="l", col="green", cex=1.5, lwd=2, lty=3)<br/>title(main="Multivariate Gaussian")</span><span id="aa19" class="mc md iq ly b gy mi mf l mg mh">legend(200, 8, legend=c("Sample Eigenvalues", "Attainable Truth", "NL"),col=c("black", "red", "green"), lwd=2, lty=c(2,1,3), cex=1.5)</span></pre><p id="4bdf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这给出了情节:</p><figure class="lt lu lv lw gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mt"><img src="../Images/1ced1d2db8050d2808d00d0fd37319da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JDBAChvG0VaWIDgMZ1N-Sg.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">来源:作者</p></figure><p id="ffe3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该图显示了(1)对角线上的最佳值，以及样本和非线性收缩估计值。看起来像人们希望的那样，样本特征值显示出过度分散(对于大值来说太大，对于小值来说太小)，而非线性收缩非常接近理想值。这正是我们所期望的，因为与样本大小<em class="ls"> n=300 </em>相比，维度<em class="ls"> p=200 </em>相当高。相对于<em class="ls"> n </em>选择的<em class="ls"> p </em>越大，样本协方差矩阵看起来就越差。</p><p id="2d8a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们做同样的事情，但是模拟一个具有4个自由度的多元t分布，一个(非常)重尾分布:</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="5fac" class="mc md iq ly b gy me mf l mg mh">### Multivariate t case<br/>X&lt;-rmvt(n=n, sigma=Sig, df=4)</span><span id="60f8" class="mc md iq ly b gy mi mf l mg mh">## Truth<br/>Sig &lt;-4/(4-2)*Sig  ## Need to rescale with a t distribution</span><span id="5fd9" class="mc md iq ly b gy mi mf l mg mh">## Sample Covariance Matrix<br/>samplespectral&lt;-eigen(cov(X))</span><span id="45c0" class="mc md iq ly b gy mi mf l mg mh">## Nonlinear Shrinkage<br/>Cov_NL&lt;-QIS(X)$Sig<br/>NLvals&lt;-sort( diag( t(samplespectral$vectors)%*%Cov_NL%*%samplespectral$vectors  ), decreasing=T)</span><span id="72f9" class="mc md iq ly b gy mi mf l mg mh">## Optimal: u_j'*Sig*u_j for all j=1,...,p<br/>optimalvals&lt;-sort(diag( t(samplespectral$vectors)%*%Sig%*%samplespectral$vectors  ), decreasing=T)</span><span id="b87b" class="mc md iq ly b gy mi mf l mg mh">plot(sort(samplespectral$values, decreasing=T), type="l", cex=15, lwd=2, lty=2, ylab="Eigenvalues",)<br/>lines(optimalvals, type="l", col="red", cex=1.5, lwd=2, lty=1)<br/>lines(NLvals, type="l", col="green", cex=1.5, lwd=2, lty=3)<br/>title(main="Multivariate t")</span><span id="1345" class="mc md iq ly b gy mi mf l mg mh">legend(200, 40, legend=c("Sample Eigenvalues", "Attainable Truth", "NL"),col=c("black", "red", "green"), lty=c(2,1,3), cex=1.5, lwd=2)</span></pre><figure class="lt lu lv lw gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mt"><img src="../Images/53916d40428a4e7ddd2d9d1b336b7a39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7GBDW1lH4VcgoW0qKLMFdQ.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">来源:作者</p></figure><p id="7a7e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，这看起来不太好了！绿色的非线性收缩值也显示了一些过度分散:大的值太大，而小的值有点太小。显然，在有限样本中，重尾会扭曲非线性收缩。如果有一种方法在高斯情况下显示同样惊人的结果，但在重尾模型中也保持良好的结果，那就太好了。这是我们新方法背后的动机。现在，我将介绍一些细节，首先介绍这种方法的关键:椭圆分布。</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><p id="c21a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">椭圆分布</strong></p><p id="ea93" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">椭圆分布类包括合理范围的不同分布，如多元高斯分布、多元t分布、多元广义双曲线分布等。如果随机向量<strong class="kh ir"> X </strong>遵循椭圆分布，则可以写成</p><figure class="lt lu lv lw gt mk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/36e5a9862f035ad11e4694a470d4f010.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*h1Kkc__FPO3F9UJN3nWMuQ.png"/></div></figure><p id="b8bb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<strong class="kh ir"> S </strong>在<em class="ls"> p </em>维度上均匀分布在单位球面上<em class="ls"> R </em>是某个独立于<strong class="kh ir"> S </strong>的非负随机变量。这听起来可能很复杂，但它只是意味着椭圆分布可以简化为圆形(二维)或球形(一般)上的均匀分布。因此，这些类型的分布有一个非常具体的结构。特别需要提到重要的一点:上式中，<strong class="kh ir"> H </strong>称为<em class="ls">色散矩阵</em>，与<em class="ls">协方差矩阵</em>相对。在本文中，我们想要估计协方差矩阵，如果它存在，给出如下</p><figure class="lt lu lv lw gt mk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/16ebc19bc9e7ff695d39a1b0df648708.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*whurbjSlrgh_4f4QopfgzQ.png"/></div></figure><p id="5e17" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这已经很有趣了:在椭圆模型中，<strong class="kh ir"> H </strong>通过假设存在，但是如果<em class="ls"> R </em>的期望值不是有限的，协方差矩阵可能不存在！例如，自由度小于2的多元t分布没有协方差矩阵。尽管如此，我们仍然可以在这种情况下估计出<strong class="kh ir"> H </strong>。所以色散矩阵在某种意义上是一个更一般的概念。然而，在本文中，我们将假设协方差矩阵存在，在这种情况下，我们从上面看到，离差和协方差矩阵在一个常数之前是相同的。</p><p id="bb50" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有趣的是，我们可以看一下<strong class="kh ir"> Z </strong> = <strong class="kh ir"> X/||X||，</strong>这是随机向量<strong class="kh ir"> X </strong>除以它的欧几里德范数<strong class="kh ir"> </strong>，这个东西<em class="ls">将总是有相同的分布</em>！其实这只是在<em class="ls"> p维</em>球面上的均匀分布。我们可以在<em class="ls"> p=2 </em>的例子中看到这一点。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="dfe3" class="mc md iq ly b gy me mf l mg mh">X&lt;-rmvnorm(n = n, sigma = diag(2))<br/>Y&lt;-rmvt(n = n, sigma = diag(2), df=4)</span><span id="84b4" class="mc md iq ly b gy mi mf l mg mh"># standardize by norm<br/>ZGaussian&lt;-t(apply(X,1, function(x) x/sqrt(sum(x^2)) ))<br/>Zt &lt;- t(apply(Y,1, function(x) x/sqrt(sum(x^2)) ))</span><span id="4900" class="mc md iq ly b gy mi mf l mg mh">par(mfrow=c(1,2))<br/>plot(ZGaussian)<br/>plot(Zt)</span></pre><p id="8de9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这给了</p><figure class="lt lu lv lw gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi na"><img src="../Images/d4455ed07d1b7ac01f447557bb6fcb75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u77c_5rCCHEjo9PqjdYImQ.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">来源:作者</p></figure></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h2 id="2f98" class="mc md iq bd nb nc nd dn ne nf ng dp nh ko ni nj nk ks nl nm nn kw no np nq nr bi translated">(线性收缩)泰勒估计量</h2><p id="2f85" class="pw-post-body-paragraph kf kg iq kh b ki ns jr kk kl nt ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">泰勒在[1]中推导出的对离差矩阵<strong class="kh ir"> H </strong>的估计，利用了<strong class="kh ir"> Z </strong> = <strong class="kh ir"> X/||X|| </strong>总是具有相同分布的事实。使用该分布的似然性，可以得到一个最大似然估计量(基本上只需求导并设置为零)，如下所示:</p><figure class="lt lu lv lw gt mk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/d50ead3d5f97315d49980003029d106b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*ExjMfKfKcYxaAlW1VhoiYg.png"/></div></figure><p id="26dd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，这只是隐式地定义了<strong class="kh ir"> H </strong>(它既在左边也在右边)，所以尝试到达<strong class="kh ir"> H </strong>的自然方式是迭代:</p><figure class="lt lu lv lw gt mk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/9c828230cbc9b6eb41bcdb49aef0982f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zt3Uj90djvh755KANBSXDQ.png"/></div></figure><p id="b3d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中第二步只是出于技术原因需要的重正化。可以证明，这个简单的迭代方案将收敛于(2)中的解。<strong class="kh ir"> H </strong>的这个估计量就是所谓的“泰勒估计量”。</p><blockquote class="nz oa ob"><p id="4dd4" class="kf kg ls kh b ki kj jr kk kl km ju kn oc kp kq kr od kt ku kv oe kx ky kz la ij bi translated">泰勒估计量是对来自椭圆分布的独立样本分布矩阵的迭代估计。它是利用椭圆随机向量按其欧几里德范数标准化后总是具有相同的分布这一事实得到的。</p></blockquote><p id="8950" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好，这是一种抵抗重尾的稳健协方差或离差估计的方法。但是上面的泰勒估计器只对<em class="ls"> p &lt; n、</em>有效，并且当<em class="ls"> p </em>接近<em class="ls"> n </em>时会恶化，所以我们仍然需要针对<em class="ls"> p </em>接近甚至大于<em class="ls"> n </em>的情况进行鲁棒估计。信号处理社区中的许多论文只是通过在每次迭代中使用线性收缩(我在这里也解释了<a class="ae lk" rel="noopener" target="_blank" href="/nonlinear-shrinkage-an-introduction-825316dda5b8"/>)来做到这一点。这看起来像这样:</p><figure class="lt lu lv lw gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi of"><img src="../Images/33ad0426d2e4ef5adb5bcf1de9552651.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OXDfRAWjFtuRCp_p8g9DOg.png"/></div></div></figure><p id="33bb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不同的ρ选择激发了不同的论文。例如，开始这一系列研究的论文之一是[2]。我们现在改为使用<em class="ls">非线性</em>收缩连同泰勒的方法。</p><blockquote class="nz oa ob"><p id="4ef2" class="kf kg ls kh b ki kj jr kk kl km ju kn oc kp kq kr od kt ku kv oe kx ky kz la ij bi translated">通过在每次迭代中使用线性收缩，我们可以稳健地估计泰勒的高维估计量。不同的论文已经找到了计算自由参数ρ的智能自适应方法。</p></blockquote></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><p id="e8c5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">鲁棒非线性收缩</strong></p><p id="a602" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在的目标是使用与上面相同的迭代方案，但不是使用线性收缩，而是使用非线性收缩进行迭代。不幸的是，这并不简单。我不会在这篇文章中详细介绍，因为需要一些技巧来使它工作，但相反，我指的是我们在<a class="ae lk" href="https://github.com/hedigers/RNL_Code" rel="noopener ugc nofollow" target="_blank"> Github </a>和论文上的实现。我们提供的实现可能也很方便，因为上面提到的信号处理社区的文章似乎都没有给出代码或在包中实现他们的方法。</p><p id="ac4a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了展示性能和代码，我们从这个新方法开始重复多变量t分析。为了完整起见，我们重申上面的整个过程:</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="dc59" class="mc md iq ly b gy me mf l mg mh">### Multivariate t case<br/>X&lt;-rmvt(n=n, sigma=Sig, df=4)</span><span id="8732" class="mc md iq ly b gy mi mf l mg mh">## Truth<br/>Sig &lt;-4/(4-2)*Sig  ## Need to rescale with a t distribution</span><span id="4c57" class="mc md iq ly b gy mi mf l mg mh">## Sample Covariance Matrix<br/>samplespectral&lt;-eigen(cov(X))</span><span id="ac92" class="mc md iq ly b gy mi mf l mg mh">## Nonlinear Shrinkage<br/>Cov_NL&lt;-QIS(X)$Sig<br/>NLvals&lt;-sort( diag( t(samplespectral$vectors)%*%Cov_NL%*%samplespectral$vectors  ), decreasing=T)</span><span id="ea2c" class="mc md iq ly b gy mi mf l mg mh">## R-NL code from <a class="ae lk" href="https://github.com/hedigers/RNL_Code" rel="noopener ugc nofollow" target="_blank">https://github.com/hedigers/RNL_Code</a><br/>Cov_RNL&lt;-RNL(X)<br/>RNLvals&lt;-sort( diag( t(samplespectral$vectors)%*%Cov_RNL%*%samplespectral$vectors  ), decreasing=T)</span><span id="ccd9" class="mc md iq ly b gy mi mf l mg mh">## Optimal: u_j'*Sig*u_j for all j=1,...,p<br/>optimalvals&lt;-sort(diag( t(samplespectral$vectors)%*%Sig%*%samplespectral$vectors  ), decreasing=T)</span><span id="1521" class="mc md iq ly b gy mi mf l mg mh">plot(sort(samplespectral$values, decreasing=T), type="l", cex=1.5, lwd=2, lty=2, ylab="Eigenvalues",)<br/>lines(optimalvals, type="l", col="red", cex=1.5, lwd=2, lty=1)<br/>lines(NLvals, type="l", col="green", cex=1.5, lwd=2, lty=3)<br/>lines( RNLvals, type="l", col="darkblue", cex=1.5, lwd=2, lty=3)<br/>title(main="Multivariate t")</span><span id="4b59" class="mc md iq ly b gy mi mf l mg mh">legend(200, 40, legend=c("Sample Eigenvalues", "Attainable Truth", "NL", "R-NL"),<br/>       col=c("black", "red", "green", "darkblue"), lty=c(2,1,3,4), cex=1.5, lwd=2)</span></pre><figure class="lt lu lv lw gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi og"><img src="../Images/5490702296f355a42853765ddfa9ef34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SRccWlmnj4pbXn48py8xeA.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">来源:作者</p></figure><p id="1b5d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">R-NL(蓝线)几乎完全在红线上，因此反映了我们在高斯情况下看到的NL的良好性能！这正是我们想要的。此外，尽管在上面的代码中可能很难理解，但使用该函数非常简单，<em class="ls"> RNL(X) </em>给出了协方差矩阵的估计值(如果你可以假设它存在的话)，而<em class="ls"> RNL(X，cov=F) </em>给出了对<strong class="kh ir"> H </strong>的估计值。</p><p id="d207" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，如果我们在高斯例子的开始使用RNL函数，这些值看起来几乎与非线性收缩完全相同。事实上，下图显示了使用我们的两种方法R-NL和稍加修改的R-C-NL以及一系列竞争对手的模拟结果。设置几乎与上面的代码完全一样，具有相同的色散矩阵和<em class="ls"> n=300 </em>，<em class="ls"> p=200。</em>不同之处在于，我们现在改变了多元t分布的尾部决定参数，在网格上从3(极重尾)到无穷大(高斯情况)。我们不会详细讨论y轴上的数字到底是什么意思，只是说越大越好，100是最大值。竞争对手“R-LS”和“R-GMV-LS”是两种线性收缩泰勒估计量，如上所述，而“NL”是非线性收缩。可以看出，对于重尾，我们比后者(好得多)得多，然后一旦我们接近高斯尾行为，我们就收敛到相同的值。</p><figure class="lt lu lv lw gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi oh"><img src="../Images/823c20b1bb71fc99d04d5133ac7a6b00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-up4AM0hLmPdi3lD9-JiKg.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">arXiv论文的模拟结果。</p></figure></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h2 id="3fac" class="mc md iq bd nb nc nd dn ne nf ng dp nh ko ni nj nk ks nl nm nn kw no np nq nr bi translated">结论</h2><p id="05be" class="pw-post-body-paragraph kf kg iq kh b ki ns jr kk kl nt ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">本文讨论了论文“R-NL:高维椭圆分布的快速稳健协方差估计”。关于arXiv的论文包含了广泛的模拟设置，表明R-NL和R-C-NL估计量在广泛的情况下表现非常好。</p><p id="372e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我希望这些估计量也能成功地用于许多实际应用中！</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><p id="ad79" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">参考文献</strong></p><p id="d344" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[1]泰勒博士(1987年a)。多元散度的无分布M估计。统计年鉴，15(1):234–251。</p><p id="8b8f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]陈、威塞尔和希罗(2011年)。高维协方差矩阵的稳健收缩估计。IEEE信号处理汇刊，59(9):4097–4107</p></div></div>    
</body>
</html>