<html>
<head>
<title>Overcoming ML Data Preprocessing Bottlenecks With gRPC</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用gRPC克服ML数据预处理瓶颈</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/overcoming-ml-data-preprocessing-bottlenecks-with-grpc-ca30fdc01bee#2022-05-31">https://towardsdatascience.com/overcoming-ml-data-preprocessing-bottlenecks-with-grpc-ca30fdc01bee#2022-05-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e3c2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">提高资源利用率和加速培训的简单解决方案</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d9defdd83dab980b0f5c00966b6f9e0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gOTEEsL3_z44p1dv"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">安德斯·诺贝克·博恩霍尔姆在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="5e66" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">深度学习项目的健康程度的一个衡量标准是它利用分配给它的训练资源的程度。无论您是在云中还是在您自己的私有基础架构上进行培训，培训资源都是要花钱的，任何一段时间的闲置都代表着增加培训吞吐量和整体生产力的潜在机会。对于培训加速器来说尤其如此——通常是最昂贵的培训资源——无论是GPU、谷歌TPU还是哈瓦那高迪。</p><p id="7519" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇博客是上一篇主题为<a class="ae kv" rel="noopener" target="_blank" href="/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851">克服数据预处理瓶颈</a>的文章的续篇，在这篇文章中，我们解决了一个不希望出现的场景，在这个场景中，您的训练加速器(此后假设为GPU)发现自己处于空闲状态，同时等待来自任务过重的CPU的数据输入。这篇文章介绍了解决这种瓶颈的几种不同方法，并通过一个玩具示例展示了它们，同时强调最佳选择在很大程度上取决于手头的模型和项目的具体情况。讨论的一些解决方案有:</p><ol class=""><li id="5d2b" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">选择CPU与GPU计算比率更适合您的工作负载的训练实例，</li><li id="7dfd" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">通过将一些CPU操作转移到GPU来改善CPU和GPU之间的工作负载平衡，以及</li><li id="37fd" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">将一些CPU计算卸载到辅助CPU工作设备。</li></ol><p id="8e7f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第三个选项是使用<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/service" rel="noopener ugc nofollow" target="_blank"> TensorFlow数据服务API</a>演示的，只需一行代码，您就可以对部分预处理管道进行编程，使其在一个或多个预配置的远程设备上运行。我们展示了如何通过将tf.data服务应用于我们的玩具示例，我们能够完全消除CPU瓶颈并最大化GPU利用率。听起来很棒，对吧？遗憾的是，这一引人注目的解决方案有两个明显的局限性:</p><ol class=""><li id="6c35" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">它仅限于张量流框架，并且</li><li id="74f6" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">即使在TensorFlow框架内，tf.data服务也仅限于仅使用本机TensorFlow操作编程的管道。这可能是一个限制性的约束，因为许多预处理管线需要没有内置张量流等价物的操作。</li></ol><p id="daf3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文的目标是展示一种更通用的方法，使用gRPC将预处理卸载到辅助设备，gRPC是TensorFlow数据服务的基础协议。</p><p id="210c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管本文中的例子将基于PyTorch 1.10和Amazon EC2，但是我们将描述的技术也可以应用于其他培训框架和基础设施。请记住，当你读到这篇文章时，我们使用的一些库可能已经发生了变化，可能需要对我们共享的代码样本进行一些调整。</p><h1 id="1543" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">运行中的CPU瓶颈</h1><p id="63e6" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">作为一个玩具示例，我们将使用下面的PyTorch模型，它大致基于<a class="ae kv" href="https://github.com/pytorch/examples/tree/main/mnist" rel="noopener ugc nofollow" target="_blank">这个MNIST官方示例</a>。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="7b57" class="ni mh iq ne b gy nj nk l nl nm">import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>import torch.optim as optim<br/>from torchvision import datasets, transforms<br/>import time</span><span id="3e9b" class="ni mh iq ne b gy nn nk l nl nm">class Net(nn.Module):<br/>    def __init__(self):<br/>        super(Net, self).__init__()<br/>        self.conv1 = nn.Conv2d(1, 32, 3, 1)<br/>        self.conv2 = nn.Conv2d(32, 64, 3, 1)<br/>        self.dropout1 = nn.Dropout(0.25)<br/>        self.dropout2 = nn.Dropout(0.5)<br/>        self.fc1 = nn.Linear(9216, 128)<br/>        self.fc2 = nn.Linear(128, 10)</span><span id="343b" class="ni mh iq ne b gy nn nk l nl nm">    def forward(self, x):<br/>        x = self.conv1(x)<br/>        x = F.relu(x)<br/>        x = self.conv2(x)<br/>        x = F.relu(x)<br/>        x = F.max_pool2d(x, 2)<br/>        x = self.dropout1(x)<br/>        x = torch.flatten(x, 1)<br/>        x = self.fc1(x)<br/>        x = F.relu(x)<br/>        x = self.fc2(x)<br/>        output = F.log_softmax(x, dim=1)<br/>        return output</span><span id="df7c" class="ni mh iq ne b gy nn nk l nl nm"><strong class="ne ir">class MyMNIST(datasets.MNIST):<br/>    '''<br/>    A personalized extension of the MNIST class in which we<br/>    modify the __len__ operation to return the maximum value<br/>    of int32 so that we do not run out of data. <br/>    '''<br/>    def __len__(self) -&gt; int:<br/>        import numpy as np<br/>        return np.iinfo(np.int32).max</strong></span><span id="4573" class="ni mh iq ne b gy nn nk l nl nm"><strong class="ne ir">    def __getitem__(self, index: int):<br/>        return super(MyMNIST,self).__getitem__(index%len(self.data))</strong></span><span id="1703" class="ni mh iq ne b gy nn nk l nl nm">def main():<br/>    from torch.profiler import profile, schedule, \<br/>        ProfilerActivity, tensorboard_trace_handler<br/>    profiler = profile(activities=[ProfilerActivity.CPU,  <br/>                                   ProfilerActivity.CUDA],<br/>                       schedule=schedule(wait=120, warmup=5,<br/>                                         active=20, repeat=0),<br/>                       on_trace_ready=tensorboard_trace_handler(<br/>                                 dir_name='profile'),<br/>                       profile_memory=True)</span><span id="ea6d" class="ni mh iq ne b gy nn nk l nl nm">    use_cuda = torch.cuda.is_available()<br/>    device = torch.device("cuda" if use_cuda else "cpu")<br/>    train_kwargs = {'batch_size': 8192,<br/>                    'num_workers': 8,<br/>                    'pin_memory': True<br/>                   }<br/>    transform=transforms.Compose([<br/>        transforms.ToTensor(),<br/>        transforms.Normalize((0.1307,), (0.3081,))<br/>        ])<br/>    dataset = <strong class="ne ir">MyMNIST</strong>('/tmp/data', train=True, download=True,<br/>                   transform=transform)<br/>    train_loader = torch.utils.data.DataLoader(dataset,<br/>                                               **train_kwargs)<br/>    model = Net().to(device)<br/>    optimizer = optim.Adadelta(model.parameters())<br/>    model.train()<br/>    t = time.perf_counter()</span><span id="6b0c" class="ni mh iq ne b gy nn nk l nl nm">    for idx, (data, target) in enumerate(train_loader, start=1):<br/>        data, target = data.to(device), target.to(device)<br/>        optimizer.zero_grad()<br/>        output = model(data)<br/>        loss = F.nll_loss(output, target)<br/>        loss.backward()<br/>        optimizer.step()<br/>        profiler.step()<br/>        if idx % 100 == 0:<br/>            print(<br/>             f'{idx}: avg step time: {(time.perf_counter()-t)/idx}')</span><span id="e9f5" class="ni mh iq ne b gy nn nk l nl nm">if __name__ == '__main__':<br/>    main()</span></pre><p id="facc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们在以下部分的目标是比较不同场景下的<em class="no">培训吞吐量</em>。我们将<em class="no">而不是</em>关心模型是否收敛。作为对<em class="no">训练吞吐量</em>的度量，我们将使用每秒输入模型的样本的平均数量。因为我们的目标是最大化吞吐量，所以我们增加批量大小，直到<em class="no">步进时间/批量大小</em>的值达到最小值或者耗尽内存。我们进一步覆盖内置的PyTorch <em class="no"> MNIST </em>数据集，这样我们就不会用完数据样本。</p><p id="499c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当在一个<a class="ae kv" href="https://aws.amazon.com/ec2/instance-types/p3/" rel="noopener ugc nofollow" target="_blank"> Amazon EC2 p3.2xlarge </a>实例上运行上述脚本时(使用<a class="ae kv" href="https://aws.amazon.com/machine-learning/amis/" rel="noopener ugc nofollow" target="_blank">深度学习AMI (Ubuntu 18.04)版本60.1 </a>和内置pytorch_p38 conda环境)，我们记录的平均步进时间为0.33秒，或平均每秒约2.6万个样本的吞吐量。</p><p id="76f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为常见深度学习训练作业的典型，模型训练(包括正向传递和梯度计算)在GPU上计算，而训练数据在传递到GPU之前在CPU核心上加载和预处理。在我们希望研究的用例中，预处理流水线无法跟上GPU的速度。因此，在等待来自CPU的数据输入时，GPU将间歇地空闲。我们通过向预处理流水线添加相对较重的模糊操作来演示这一现象。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="8688" class="ni mh iq ne b gy nj nk l nl nm">transform=transforms.Compose([<br/>    transforms.ToTensor(),<br/>    transforms.Normalize((0.1307,), (0.3081,)),<br/>    <strong class="ne ir">transforms.GaussianBlur(11)</strong><br/>    ])</span></pre><p id="e995" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们使用上面的更改重新运行脚本时，步长时间跳到1.01秒，吞吐量下降到大约每秒8000个样本。我们强调，在GPU上运行的训练图没有任何变化；训练速度的显著降低是CPU上运行的计算繁重的预处理流水线的结果。</p><p id="4d99" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的训练循环中，我们启用了内置的<a class="ae kv" href="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html" rel="noopener ugc nofollow" target="_blank"> PyTorch分析器</a>。在我们的输入瓶颈的情况下，可以在<a class="ae kv" href="https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html" rel="noopener ugc nofollow" target="_blank"> TensorBoard </a>中查看收集的概要。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/deb34f7c6cf51e9e756fbc9389131550.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lCTkjioJi5PcOJgFXqCZHA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">TensorBoard Profiler报告(按作者)</p></figure><p id="af42" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">分析报告显示，GPU利用率低于10%，近90%的训练步骤时间集中在加载和处理训练数据上。</p><p id="d507" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">应该注意的是，进一步的分析表明，即使在基线运行中，如果没有大量的模糊操作，也会出现数据输入瓶颈，尽管不太严重。稍后，当我们能够演示低至0.17秒的步进时间时，我们将看到这方面的证据。</p><p id="a3f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">克服数据输入流水线中瓶颈的方法之一是将一部分数据预处理工作卸载给一个或多个辅助CPU设备。在下一节中，我们将探索如何使用<a class="ae kv" href="https://grpc.io/docs/what-is-grpc/introduction/" rel="noopener ugc nofollow" target="_blank"> gRPC </a>协议来实现这一点。</p><h1 id="ba0f" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">使用gRPC卸载数据处理</h1><p id="0d09" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在<a class="ae kv" href="https://en.wikipedia.org/wiki/Remote_procedure_call" rel="noopener ugc nofollow" target="_blank">远程过程调用</a> (RPC)系统中，客户端-服务器接口由服务器定义，服务器实现客户端可以远程调用的指定方法。在下文中，我们将定义一个专用的gRPC服务，用于使用<a class="ae kv" href="https://developers.google.com/protocol-buffers/docs/overview" rel="noopener ugc nofollow" target="_blank">协议缓冲区</a>提取已处理的训练批次。服务器和客户端将在本入门教程之后用Python实现。</p><h2 id="d28d" class="ni mh iq bd mi nq nr dn mm ns nt dp mq lf nu nv ms lj nw nx mu ln ny nz mw oa bi translated">GRPC接口</h2><p id="e77d" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">我们创建一个名为<em class="no"> data_feed.proto </em>的protobuf文件，它用一个<em class="no"> get_samples </em>方法定义了一个<em class="no"> DataFeed </em>服务:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="223a" class="ni mh iq ne b gy nj nk l nl nm">syntax = "proto3";</span><span id="2042" class="ni mh iq ne b gy nn nk l nl nm">service DataFeed {<br/>    rpc get_samples(Config) returns (stream Sample) {}<br/>}</span><span id="0840" class="ni mh iq ne b gy nn nk l nl nm">message Config {<br/>}</span><span id="f282" class="ni mh iq ne b gy nn nk l nl nm">message Sample {<br/>   bytes image = 1;<br/>   bytes label = 2;<br/>}</span></pre><p id="6386" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该方法被定义为返回类型为<em class="no">的样本</em>的流，其中每个样本代表一个由<em class="no"> batch_size </em>图像及其关联标签组成的训练批次。</p><p id="f5d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如<a class="ae kv" href="https://grpc.io/docs/languages/python/basics/" rel="noopener ugc nofollow" target="_blank">教程</a>中所述，下面的命令将生成相应的gRPC Python文件，<em class="no"> data_feed_pb2.py </em>和<em class="no"> data_feed_pb2_grpc.py </em>，我们将使用它们来实现服务器和客户端。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="243b" class="ni mh iq ne b gy nj nk l nl nm">python -m grpc_tools.protoc -I&lt;protos folder&gt; --python_out=&lt;out_path&gt; --grpc_python_out=&lt;out_path&gt; &lt;path to proto file&gt;</span></pre><h2 id="fe32" class="ni mh iq bd mi nq nr dn mm ns nt dp mq lf nu nv ms lj nw nx mu ln ny nz mw oa bi translated">GRPC服务器</h2><p id="7946" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">下一步是实现我们的数据馈送gRPC服务器。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="722f" class="ni mh iq ne b gy nj nk l nl nm">import multiprocessing as mp<br/>from concurrent import futures<br/>import grpc<br/><strong class="ne ir">import data_feed_pb2<br/>import data_feed_pb2_grpc<br/></strong>import torch<br/>from torchvision import datasets, transforms<br/>import numpy as np</span><span id="a16b" class="ni mh iq ne b gy nn nk l nl nm"><strong class="ne ir"># The following class implements the data feeding servie<br/>class DataFeedService(data_feed_pb2_grpc.DataFeedServicer):<br/>    def __init__(self, q):<br/>        '''<br/>        param q: A shared queue containing data batches<br/>        '''<br/>        self.q = q<br/>    def get_samples(self, request, context):<br/>        while True:<br/>            sample = self.q.get()<br/>            yield data_feed_pb2.Sample(image=sample[0], <br/>                                       label=sample[1])</strong></span><span id="7977" class="ni mh iq ne b gy nn nk l nl nm"><strong class="ne ir"># The data loading and preprocessing logic.<br/># We chose to keep the existing logic unchanged, just instead<br/># of feeding the model, the dataloader feeds a shared queue</strong></span><span id="2dbd" class="ni mh iq ne b gy nn nk l nl nm">class MyMNIST(datasets.MNIST):<br/>    def __len__(self) -&gt; int:<br/>        return np.iinfo(np.int32).max</span><span id="0c92" class="ni mh iq ne b gy nn nk l nl nm">    def __getitem__(self, index: int):<br/>        return super(MyMNIST,self).__getitem__(index%len(self.data))</span><span id="a5ec" class="ni mh iq ne b gy nn nk l nl nm"><strong class="ne ir">def fill_queue(q,kill):<br/></strong>    <strong class="ne ir">train_kwargs = {'batch_size': 8192, 'num_workers': 16}</strong><br/>    transform=transforms.Compose([<br/>            transforms.ToTensor(),<br/>            transforms.Normalize((0.1307,), (0.3081,)),<br/>            transforms.GaussianBlur(11)<br/>            ])<br/>    dataset = MyMNIST('/tmp/data', train=True,<br/>                           transform=transform, download=True)<br/>    loader = torch.utils.data.DataLoader(dataset, **train_kwargs)<br/>    for batch_idx, (data, target) in enumerate(loader):<br/>        added = False<br/>        while not added and not kill.is_set():<br/>            try:<br/><strong class="ne ir">                # convert the data to bytestrings and add to queue</strong>               <br/><strong class="ne ir">                q.put((data.numpy().tobytes(),<br/>                       target.type(torch.int8).numpy().tobytes()),<br/>                       timeout=1)<br/></strong>                added = True<br/>            except:<br/>                continue</span><span id="fed7" class="ni mh iq ne b gy nn nk l nl nm">def serve():<br/><strong class="ne ir">    '''<br/>    Initialize the data batch queue and start up the service.<br/>    '''<br/>    </strong>q = mp.Queue(<strong class="ne ir">maxsize=32</strong>)<br/>    kill = mp.Event() # an mp.Event for graceful shutdown<br/>    p = mp.Process(target=fill_queue, args=(q,kill))<br/>    p.start()<br/><strong class="ne ir">    server = grpc.server(futures.ThreadPoolExecutor(max_workers=8))<br/>    data_feed_pb2_grpc.add_DataFeedServicer_to_server(<br/>        DataFeedService(q), server)<br/>    server.add_insecure_port('[::]:50051')<br/>    server.start()<br/>    server.wait_for_termination()<br/></strong>    kill.set()<br/>    p.join()</span><span id="2d59" class="ni mh iq ne b gy nn nk l nl nm">if __name__ == '__main__':<br/>    serve()</span></pre><p id="31a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如代码文档中所描述的，我们选择了一个保持数据加载逻辑完整的实现，并且数据批次被输入到一个共享队列中。<em class="no"> DataFeedService </em>的<em class="no"> get_samples </em>函数从同一队列中取出批次，并以连续数据流的形式发送给客户端。</p><p id="7f56" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该脚本包括一些常量值，应该根据基于gRPC的解决方案的部署方式进行调整(或自动配置):</p><ul class=""><li id="c14a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr ob ly lz ma bi translated">gRPC服务器的<em class="no"> ThreadPoolExecutor </em>的<em class="no"> max_workers </em>设置应该根据您想要创建的gRPC客户端的数量进行配置。因为我们计划为培训设备上的每个CPU内核创建一个客户端，并且我们的ec2p 3.2 x大型实例有8个vcpu，所以我们将其设置为8。</li><li id="bba7" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr ob ly lz ma bi translated">输入到数据加载器的<em class="no"> train_kwargs </em>结构的<em class="no"> num_workers </em>设置应该配置为我们的服务器实例上的vCPUs数量。我们打算使用的实例是由16个vCPUs组成的<a class="ae kv" href="https://aws.amazon.com/ec2/instance-types/c5/" rel="noopener ugc nofollow" target="_blank"> EC2 c5.4xlarge </a>，我们已经相应地设置了<em class="no"> num_workers </em>。</li><li id="31b7" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr ob ly lz ma bi translated">对于共享队列的<em class="no"> maxsize </em>值，我们简单地选择了两倍于<em class="no"> num_workers </em>的值。</li></ul><p id="1ee3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们应该强调，实现数据馈送服务器有许多不同的方式。我们在这里展示的实现对于像我们的玩具问题这样的用例来说不一定是最佳的。以下是更多相关信息。</p><h2 id="cea8" class="ni mh iq bd mi nq nr dn mm ns nt dp mq lf nu nv ms lj nw nx mu ln ny nz mw oa bi translated">GRPC客户</h2><p id="28b6" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">最后，我们修改我们的训练脚本，以便使用来自gRPC服务器的经过处理的训练数据。下面的块包括修改:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="0205" class="ni mh iq ne b gy nj nk l nl nm"><strong class="ne ir">class RemoteDataset(torch.utils.data.IterableDataset):<br/>    '''<br/>    An iterable PyTorch dataset that opens a connection to the<br/>    gRPC server and reads from a stream of data batches <br/>    '''<br/>    def __iter__(self):<br/>        import grpc<br/>        import data_feed_pb2_grpc<br/>        import data_feed_pb2<br/>        import numpy as np<br/>        host = '&lt;ip of host&gt;'<br/>        channel = grpc.insecure_channel(f'{host}:50051',<br/>                    # overwrite the default max message length<br/>                    options=[('grpc.max_receive_message_length',<br/>                               200 * 1024 * 1024)])<br/>        stub = data_feed_pb2_grpc.DataFeedStub(channel)<br/>        samples = stub.get_samples(data_feed_pb2.Config())<br/>        for s in samples:<br/>            image = torch.tensor(np.frombuffer(s.image, <br/>                              dtype=np.float32)).reshape(<br/>                                       [8192, 1, 28, 28])<br/>            label = torch.tensor(np.frombuffer(s.label, <br/>                              dtype=np.int8)).reshape(<br/>                                       [8192]).type(torch.int64)<br/>            yield image, label</strong></span><span id="88aa" class="ni mh iq ne b gy nn nk l nl nm">def main():<br/>    device = torch.device("cuda")<br/>    train_kwargs = {<strong class="ne ir">'batch_size': None, #the data is already batched</strong><br/>                    <strong class="ne ir">'num_workers': 8,</strong><br/>                    'pin_memory': True<br/>                   }<br/>    <strong class="ne ir">dataset = RemoteDataset()<br/>    </strong>train_loader = torch.utils.data.DataLoader(dataset,<br/>                                               **train_kwargs)<br/>    model = Net().to(device)<br/>    optimizer = optim.Adadelta(model.parameters())<br/>    model.train()<br/>    t = time.perf_counter()</span><span id="3ee6" class="ni mh iq ne b gy nn nk l nl nm">    for idx, (data, target) in enumerate(train_loader, start=1):<br/>        data, target = data.to(device), target.to(device)<br/>        optimizer.zero_grad()<br/>        output = model(data)<br/>        loss = F.nll_loss(output, target)<br/>        loss.backward()<br/>        optimizer.step()<br/>        profiler.step()<br/>        if idx % 100 == 0:<br/>            print(<br/>             f'{idx}: avg step time: {(time.perf_counter()-t)/idx}')<br/>if __name__ == '__main__':<br/>    main()</span></pre><p id="4b44" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们已经定义了一个可迭代的PyTorch数据集，<em class="no"> RemoteDataset </em>，它打开一个到gRPC服务器的连接，并从一个数据批次流中读取数据。<em class="no"> DataLoader </em> workers的数量设置为8，即训练实例上vCPUs的数量。因此，总共将创建8个gRPC客户端。</p><p id="ab1b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们再次强调，选择gRPC客户端实现主要是因为它的简单性，并且很可能存在更优的实现。</p><h2 id="7142" class="ni mh iq bd mi nq nr dn mm ns nt dp mq lf nu nv ms lj nw nx mu ln ny nz mw oa bi translated">结果</h2><p id="a046" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">是时候衡量我们的gRPC解决方案在多大程度上成功缓解了我们的CPU瓶颈了。下表总结了我们在EC2 c5.4xlarge和EC2.9xlarge实例上运行数据服务器时获得的吞吐量结果。对于EC2.9xlarge实验，我们更新了常量参数以反映36个vCPUs的存在。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/8cdc2c1f23ac86979c1d5769d4e14b08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*Bzn-jmj4iHXGxMFSXRSwkQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">实验结果(作者)</p></figure><p id="16b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用16位vCPU工作人员使我们的培训速度提高了约60%,使用36位vCPU工作人员提高了80%以上。考虑到EC2实例的<a class="ae kv" href="https://aws.amazon.com/ec2/pricing/on-demand/" rel="noopener ugc nofollow" target="_blank">小时成本(在撰写本文时，p3.2xlarge的成本为3.06美元，c5.4xlarge的成本为0.68美元，c5.9xlarge的成本为1.53美元)，我们发现将数据处理卸载到16个vCPU工作线程可以节省超过50%的成本，36个vCPU工作线程可以节省大约75%的成本。我们应该注意，我们的成本计算没有考虑到加速开发时间的潜在收益(例如，为其他任务释放人力和计算资源)。</a></p><p id="d5f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们应该注意到，c5.9xlarge实验产生了比基线实验更低的步进时间。这表明，即使是基线实验也没有最大限度地利用GPU，并且它也遇到了数据输入管道上的瓶颈。</p><p id="0f4c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的TensorBoard截图总结了c5.9xlarge案例的培训表现:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/b216f5189762702aad7c93c2dac8e99b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S7Mg0lhILcVHrLWSEEg5uw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">TensorBoard Profiler报告(按作者)</p></figure><p id="18b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然瓶颈的严重程度已经显著降低，但很明显仍有进一步优化的空间，因为GPU仍有大约30%的空闲时间。</p><h2 id="b4f8" class="ni mh iq bd mi nq nr dn mm ns nt dp mq lf nu nv ms lj nw nx mu ln ny nz mw oa bi translated">实施说明</h2><p id="8e8c" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">我们用一些关于基于gRPC的解决方案和我们已经演示的具体实现的一般说明来结束这一部分。</p><p id="0843" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">探索所有选项</strong>:虽然以我们所描述的方式根除CPU瓶颈的想法可能非常有说服力，但我们建议您在采用基于gRPC的解决方案之前考虑其他替代方案。您可能会发现，在没有太大困难的情况下，您可以优化您的数据处理以更有效地利用CPU，或者不同类型的训练实例包含更适合您的工作负载的CPU计算与GPU计算的比率。</p><p id="02d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">优化负载平衡</strong>:在我们展示的解决方案中，我们选择将整个预处理管道卸载到gRPC服务器。但是，这可能会导致训练实例上的CPU核心利用率严重不足。更有效的解决方案是使用保持GPU完全活动所需的最少数量的总CPU，并平衡训练实例上的CPU和辅助CPU之间的负载，使它们都得到充分利用。这可以通过在gRPC服务器上运行预处理流水线的开始部分并将部分处理的数据发送到训练实例以完成流水线来实现。</p><p id="af62" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">小心网络IO瓶颈</strong>:需要注意的一件事是从服务器传递到客户端的数据量。否则，我们最终可能会用网络瓶颈取代CPU瓶颈。网络IO带宽限制由服务器和客户端实例的属性决定。为了减少带宽，可以考虑利用gRPC协议中内置的<a class="ae kv" href="https://github.com/grpc/grpc/blob/master/src/python/grpcio_tests/tests/unit/_compression_test.py" rel="noopener ugc nofollow" target="_blank">压缩支持</a>。</p><p id="277c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">扩展到多GPU </strong>:虽然我们提出的解决方案可以扩展到支持多GPU——数据分布式设置，没有太大的困难，但如何做到这一点的细节可能会有所不同。根据您的项目，您可以选择单个gRPC服务器实例和为所有GPU核心提供服务的单个服务、单个实例上的多个服务、多个服务器实例或其他组合。</p><p id="5584" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">Python GRPC的多重处理问题</strong>:要知道GRPC的Python实现在处理分叉时有局限性。这可能会干扰PyTorch数据加载器，py torch数据加载器依赖多处理来支持多工作。gRPC服务器和客户机的任何实现都需要协商这种潜在的冲突。参见<a class="ae kv" href="https://github.com/grpc/grpc/blob/master/doc/fork_support.md" rel="noopener ugc nofollow" target="_blank">此处</a>了解更多关于此限制的详细信息。</p><p id="d241" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">与</strong><a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/service" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">tf.data service</strong></a>的对比:与我们提出的解决方案类似，TF . data service也依赖于gRPC协议。然而，与我们的解决方案相反，它是基于C++ gRPC的，具有更少的限制，可能更优。此外，tf.data服务对用户隐藏了gRPC实现的许多细节，在某种程度上，将操作卸载到数据服务被简化为仅向输入管道添加一行代码。这使得找到管道中执行卸载的最佳点变得特别容易。相比之下，我们的解决方案需要更多的努力。如果您的工作负载与tf.data服务兼容，很可能会带来更好的性能，并且可能是更好的选择。然而，正如我们已经提到的，在撰写本文时，tf.data服务仅限于只包含本机TensorFlow操作的管道。我们描述的基于gRPC的解决方案要灵活得多——它不仅在构建输入管道时提供了更大的自由度，而且还允许对如何在CPU工作者之间分配任务进行更大的控制。</p><h1 id="1eb4" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">摘要</h1><p id="19c1" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">过去几年人工智能的进步，部分归功于强大的训练加速器的可用性。我们认为，作为开发者，我们有责任关注我们如何使用这种专门的机器，以便最大限度地提高效率，增加成本节约，并减少碳排放。本文重点介绍了我们可以用来提高资源利用率的一个选项——将数据预处理卸载到一个或多个辅助CPU实例。</p><p id="467e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">像往常一样，请随时提出意见、问题和纠正。</p></div></div>    
</body>
</html>