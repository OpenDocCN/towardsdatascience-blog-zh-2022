<html>
<head>
<title>Foundational RL: Markov States, Markov Chain, and Markov Decision Process</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基础 RL:马尔可夫状态、马尔可夫链和马尔可夫决策过程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/foundational-rl-markov-states-markov-chain-and-markov-decision-process-be8ccc341005#2022-12-09">https://towardsdatascience.com/foundational-rl-markov-states-markov-chain-and-markov-decision-process-be8ccc341005#2022-12-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="72bf" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">强化学习之路</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3df8de33fac8dbc355e14dd2403f4439.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QaPrco2iqI9CcEITsbEp1g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者使用人工智能工具 Dreamstudio 生成的封面照片(授权为<a class="ae ky" href="https://creativecommons.org/publicdomain/zero/1.0/" rel="noopener ugc nofollow" target="_blank">https://creativecommons.org/publicdomain/zero/1.0/</a></p></figure><p id="e12d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> R </span>强化学习(RL)是一种机器学习，在这种机器学习中，代理通过试错来学习与其环境进行交互，以使回报最大化。它不同于监督学习，在监督学习中，代理根据标记的示例进行训练，也不同于无监督学习，在无监督学习中，代理学习识别未标记数据中的模式。在强化学习中，代理人学习在一个环境中采取行动以获得最大的回报，如赢得分数或赢得游戏。</p><blockquote class="me"><p id="e591" class="mf mg it bd mh mi mj mk ml mm mn lu dk translated">强化学习对于广泛的应用是有用的，包括机器人、自然语言处理和游戏。</p></blockquote><p id="0774" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在本文中，我构建了一些基本概念来理解强化学习。在<a class="ae ky" rel="noopener" target="_blank" href="/foundational-rl-solving-markov-decision-process-d90b7e134c0b">的下一篇文章</a>中，我会谈到马尔科夫决策过程的解决方案。</p><div class="mt mu gp gr mv mw"><a rel="noopener follow" target="_blank" href="/foundational-rl-solving-markov-decision-process-d90b7e134c0b"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">基础 RL:求解马尔可夫决策过程</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">强化学习之路</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="ng l nh ni nj nf nk ks mw"/></div></div></a></div><h1 id="6ecc" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">代理人、行动、奖励和目标</h1><p id="7202" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">在 RL 中，我们有一个代理，我们使用某种算法训练它采取某些行动，使回报最大化，以达到最终目标。最终目标可能是非常遥远的未来，或者不断变化(就像自主导航一样)。</p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><h1 id="0a63" class="nl nm it bd nn no op nq nr ns oq nu nv jz or ka nx kc os kd nz kf ot kg ob oc bi translated">马尔可夫状态</h1><p id="2930" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">在强化学习中，状态是指主体所处的当前情境或环境。它表示代理在给定时间点拥有的关于其环境的信息。例如，自主车辆的位置和速度可以是 RL 问题中的状态。代理使用状态信息来决定下一个时间步采取什么行动来最大化奖励。</p><p id="f89f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 RL 中，我们关心马尔可夫状态，其中状态具有所有未来状态仅依赖于当前状态的性质。这意味着主体不需要记住它与环境交互的全部历史来做决定。相反，它可以简单地关注当前状态并据此采取行动。这使得学习过程更加有效，因为代理不必存储和处理大量信息。此外，它使代理的行为更可预测，因为它完全由当前状态决定。这在许多应用中都很有用，比如机器人和控制系统。</p><p id="8d67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以将车辆的马尔可夫状态编码如下:</p><pre class="kj kk kl km gt ou ov ow bn ox oy bi"><span id="c15e" class="oz nm it ov b be pa pb l pc pd"># define the states of the vehicle<br/>STOPPED = 0<br/>MOVING_FORWARD = 1<br/>MOVING_BACKWARD = 2<br/><br/># define the actions of the vehicle<br/>STOP = 0<br/>MOVE_FORWARD = 1<br/>MOVE_BACKWARD = 2<br/><br/># define the Markov state of the vehicle<br/>class VehicleMarkovState:<br/>  def __init__(self, state, action):<br/>    self.state = state<br/>    self.action = action<br/><br/># define a function to encode the Markov state of the vehicle<br/>def encode_markov_state(vehicle_state, vehicle_action):<br/>  return VehicleMarkovState(vehicle_state, vehicle_action)<br/><br/># example: encode the Markov state of a vehicle that is moving forward<br/>markov_state = encode_markov_state(MOVING_FORWARD, MOVE_FORWARD)<br/>print(markov_state.state)  # prints 1 (MOVING_FORWARD)<br/>print(markov_state.action)  # prints 1 (MOVE_FORWARD)</span></pre><h1 id="92fe" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">马尔可夫链</h1><p id="1168" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">马尔可夫链是一个有限状态机，其中每个状态都是一个马尔可夫状态。马尔可夫链由许多状态组成，具有从一个状态到另一个状态的转移概率。在马尔可夫链中，转移到特定状态的概率取决于当前状态和经过的时间，而不用担心过去发生了什么。</p><p id="52d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">马尔可夫链不同于随机过程，因为在随机过程中，现在发生的事情取决于过去发生的事情，而不仅仅是最近发生的事情。</p><p id="74aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们考虑一个例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/dc45562ddfd3b97f6bc6efe155c59c40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KtRFJXF34Dq2dxSKzkvlzA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。一个马尔可夫链，图片由作者提供</p></figure><p id="4ce5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有两个马尔可夫状态 A，和 B，从 A 到 B 的转移概率是 0.7，从 B 到 A 的转移概率是 0.9，从 B 到 B 的转移概率是 0.1，从 A 到 A 的转移概率是 0.3。这个想法如图 1 所示。我们可以用 Python 对此进行编码，如下所示:</p><pre class="kj kk kl km gt ou ov ow bn ox oy bi"><span id="123c" class="oz nm it ov b be pa pb l pc pd"># define the states of the Markov chain<br/>A = 0<br/>B = 1<br/><br/># define the transition probabilities<br/>transition_probs = [[0.3, 0.7],  # transition probabilities from A<br/>                    [0.9, 0.1]]  # transition probabilities from B<br/><br/># define a class to represent the Markov chain<br/>class MarkovChain:<br/>  def __init__(self, states, transition_probs):<br/>    self.states = states<br/>    self.transition_probs = transition_probs<br/><br/># define a function to encode the Markov chain<br/>def encode_markov_chain(markov_states, markov_transition_probs):<br/>  return MarkovChain(markov_states, markov_transition_probs)<br/><br/># example: encode the Markov chain<br/>markov_chain = encode_markov_chain([A, B], transition_probs)<br/>print(markov_chain.states)  # prints [0, 1]<br/>print(markov_chain.transition_probs)  # prints [[0.3, 0.7], [0.9, 0.1]]</span></pre><h1 id="9af4" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">马尔可夫决策过程</h1><p id="b2d3" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">马尔可夫决策过程或 MDP 是马尔可夫链的延伸。在 MDP，根据某种动作<strong class="lb iu">和</strong>，状态从一个马尔可夫状态转换到另一个马尔可夫状态。这种转变会带来相应的回报。MDP 是一个 4 元组模型(𝓢、𝓐、𝓟、𝓡)，其中<em class="pf"> s </em> ∈ 𝓢是一个状态，<em class="pf"> a </em> ∈ 𝓐是当代理是一个状态<em class="pf"> s </em>时采取的一个动作，𝓟 <em class="pf"> (s' | s，a) </em>是在动作<em class="pf"> a </em>(或一些动作)的影响下从<em class="pf"> s </em>转移到状态<em class="pf">s’</em>的转移概率矩阵</p><p id="db51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">策略函数:</strong>在 RL 文献中通常用π表示的策略函数规定了从状态空间𝓢到动作空间𝓐.的映射</p><p id="f114" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MDP 可以用来模拟自动驾驶汽车的决策过程。在这种情况下，MDP 的状态可能表示汽车和环境中其他对象(如其他汽车和障碍物)的不同位置和速度。MDP 的动作可能代表自动驾驶汽车可以采取的不同动作，例如加速、刹车或转弯。MDP 的奖励可能代表不同行为的价值或效用，例如避免碰撞或快速到达目的地。使用 MDP，自动驾驶汽车可以学习采取最大化其回报的行动，例如避免碰撞和快速到达目的地。</p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><p id="3073" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文为开始强化学习提供了第一手基础。在下一篇文章中，我将讨论更多的概念，比如价值函数、动态规划、如何求解马尔可夫决策过程以及部分可观测性 MDP。</p><p id="43a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二部分:<a class="ae ky" rel="noopener" target="_blank" href="/foundational-rl-solving-markov-decision-process-d90b7e134c0b">https://towards data science . com/fundamental-rl-solving-Markov-decision-process-d 90 b 7 e 134 c0b</a></p><div class="mt mu gp gr mv mw"><a rel="noopener follow" target="_blank" href="/foundational-rl-solving-markov-decision-process-d90b7e134c0b"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">基础 RL:求解马尔可夫决策过程</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">强化学习之路</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="ng l nh ni nj nf nk ks mw"/></div></div></a></div></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><h1 id="c959" class="nl nm it bd nn no op nq nr ns oq nu nv jz or ka nx kc os kd nz kf ot kg ob oc bi translated">参考</h1><ol class=""><li id="2ca0" class="pj pk it lb b lc od lf oe li pl lm pm lq pn lu po pp pq pr bi translated">罗纳德·霍华德(1960)。<a class="ae ky" href="http://web.mit.edu/dimitrib/www/dpchapter.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="pf">动态规划和马尔可夫过程</em> </a> (PDF)。麻省理工学院出版社。</li><li id="0051" class="pj pk it lb b lc ps lf pt li pu lm pv lq pw lu po pp pq pr bi translated">强化学习和随机优化:连续决策的统一框架。)，威利(2022)。精装本。ISBN 9781119815051。</li></ol></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><blockquote class="me"><p id="b9bf" class="mf mg it bd mh mi mj mk ml mm mn lu dk translated">你喜欢这篇文章吗？给我买杯咖啡。</p><p id="dc67" class="mf mg it bd mh mi mj mk ml mm mn lu dk translated">喜欢我的作品吗？加入我的<a class="ae ky" href="https://rahulbhadani.medium.com/subscribe" rel="noopener">邮件列表</a>。</p><p id="c364" class="mf mg it bd mh mi mj mk ml mm mn lu dk translated">想了解更多 STEM 相关话题？加入<a class="ae ky" href="https://rahulbhadani.medium.com/membership" rel="noopener">介质</a></p></blockquote></div></div>    
</body>
</html>