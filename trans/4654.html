<html>
<head>
<title>How to Accelerate your PyTorch GPU Training with XLA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何通过XLA加速您的PyTorch GPU培训</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-accelerate-your-pytorch-training-with-xla-on-aws-3d599bc8f6a9#2022-10-17">https://towardsdatascience.com/how-to-accelerate-your-pytorch-training-with-xla-on-aws-3d599bc8f6a9#2022-10-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="92f8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">PyTorch/XLA的力量以及亚马逊SageMaker培训编译器如何简化其使用</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ad2a423f8117441b5974b0adb0d228f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*U3cCzkY7bhrJxvv-"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@patrickian4?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">帕特里克·福尔</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kw"><img src="../Images/3031b76d3ab321267edead189bcdeca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*9B576mjB_w4ujsl20xazlw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">按作者</p></figure><p id="7bb8" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在我们过去的许多帖子中(例如<a class="ae kv" rel="noopener" target="_blank" href="/cloud-ml-performance-checklist-caa51e798002">这里</a>)，我们都强调了管理培训成本的重要性。我们一直在寻求通过迭代过程1来提高训练运行时性能的方法。<strong class="kz ir"> <em class="lt">分析</em> </strong>我们的工作负载，以确定性能瓶颈和资源利用不足，以及2 .<strong class="kz ir"> <em class="lt">优化</em> </strong>我们的工作负载以消除瓶颈并提高资源利用率。</p><p id="876d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在这篇文章中，我们探索了通过使用<strong class="kz ir"> XLA编译</strong>来优化我们PyTorch训练步骤的潜力。我们将从XLA的简介开始。接下来我们将演示它在AWS上的使用，并展示<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler.html" rel="noopener ugc nofollow" target="_blank">Amazon sage maker Training Compiler</a>如何简化它的配置。我们将以XLA带来的一些(当前的)限制来结束。</p><h2 id="9f4e" class="lu lv iq bd lw lx ly dn lz ma mb dp mc lg md me mf lk mg mh mi lo mj mk ml mm bi translated">放弃</h2><p id="cfb9" class="pw-post-body-paragraph kx ky iq kz b la mn jr lc ld mo ju lf lg mp li lj lk mq lm ln lo mr lq lr ls ij bi translated">我们在这篇文章中的目的是介绍加快PyTorch/XLA培训的潜在机会。我们的目的不是认可它的使用，也不是认可我们将提到的任何其他培训工具的使用。与往常一样，最佳培训配置高度依赖于您的模型的具体细节。我开篇的那首诗是为了娱乐目的收录的(我忍不住)。尽管有其含义，但不能保证PyTorch/XLA会加快你的训练。虽然我们的重点(和演示)将是Amazon SageMaker培训环境，但我们讨论的大部分内容也适用于其他培训环境。</p><h1 id="1c7a" class="ms lv iq bd lw mt mu mv lz mw mx my mc jw mz jx mf jz na ka mi kc nb kd ml nc bi translated">与XLA的JIT编译</h1><p id="e63c" class="pw-post-body-paragraph kx ky iq kz b la mn jr lc ld mo ju lf lg mp li lj lk mq lm ln lo mr lq lr ls ij bi translated"><a class="ae kv" href="https://developers.googleblog.com/2017/03/xla-tensorflow-compiled.html" rel="noopener ugc nofollow" target="_blank">最初创建</a>是为了加速<a class="ae kv" href="https://www.tensorflow.org/xla" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>工作负载，XLA(代表<strong class="kz ir">T5 AccT7】T8】elerated<strong class="kz ir">T11】L</strong><em class="lt">linear</em><strong class="kz ir"><em class="lt">A</em></strong><em class="lt">代数</em>采用<em class="lt"> Just in Time </em> (JIT)编译技术来优化底层加速器的计算图。更具体地，在运行时，XLA编译器将分析与模型相关联的完整计算图，将连续的张量运算融合在一起，并输出用于底层加速器的最佳机器代码。这与标准方法相反，在标准方法中，每个张量运算都是独立优化的。融合图形组件可以通过多种方式提高性能，包括:</strong></p><ol class=""><li id="90b8" class="nd ne iq kz b la lb ld le lg nf lk ng lo nh ls ni nj nk nl bi translated">减少总体机器级操作(FLOPS)的数量，</li><li id="3e9d" class="nd ne iq kz b la nm ld nn lg no lk np lo nq ls ni nj nk nl bi translated">减少需要由CPU加载到加速器中的计算内核的数量，以及</li><li id="6a6a" class="nd ne iq kz b la nm ld nn lg no lk np lo nq ls ni nj nk nl bi translated">减少计算图的内存占用。释放的空间可用于增加训练批次大小，这可能会进一步提高性能。</li></ol><p id="ba0a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">如<a class="ae kv" href="https://www.tensorflow.org/xla" rel="noopener ugc nofollow" target="_blank"> XLA概览页面</a>所示，优化整个图表而非单个操作有可能减少培训时间。</p><p id="1b0c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">顺便提一句，XLA的另一个优势是它是专门为支持新的后端设备而设计的。更多详情请参见此处的<a class="ae kv" href="https://www.tensorflow.org/xla/architecture" rel="noopener ugc nofollow" target="_blank">和此处</a>的<a class="ae kv" href="https://www.tensorflow.org/xla/developing_new_backend" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="7bca" class="ms lv iq bd lw mt mu mv lz mw mx my mc jw mz jx mf jz na ka mi kc nb kd ml nc bi translated">XLA换火炬</h1><p id="86e2" class="pw-post-body-paragraph kx ky iq kz b la mn jr lc ld mo ju lf lg mp li lj lk mq lm ln lo mr lq lr ls ij bi translated">与TensorFlow相反，为PyTorch实现XLA提出了一个独特的挑战，我们将在下面的小节中解释。</p><h2 id="506b" class="lu lv iq bd lw lx ly dn lz ma mb dp mc lg md me mf lk mg mh mi lo mj mk ml mm bi translated">急切执行与图形执行</h2><p id="0035" class="pw-post-body-paragraph kx ky iq kz b la mn jr lc ld mo ju lf lg mp li lj lk mq lm ln lo mr lq lr ls ij bi translated">深度学习框架可以根据它们表示和执行机器学习模型的模式进行分类。一些框架，最著名的是TensorFlow(在v1中默认，在v2中通过<a class="ae kv" href="https://www.tensorflow.org/guide/intro_to_graphs" rel="noopener ugc nofollow" target="_blank"> tf.function </a>)，支持<strong class="kz ir">图模式</strong>，其中模型首先被表示为计算图(在Python中)，然后由单独的执行引擎处理和执行(例如，在C++中)。其他框架，比如PyTorch，急切地执行它们的模型<strong class="kz ir"/>，也就是说，模型不是存储在数据结构中，而是直接执行每一行代码。许多人认为Eager模式更容易调试，并且支持更强的编程表达能力。这通常被视为PyTorch框架最近越来越受欢迎的原因(截至本文撰写之时)。</p><p id="d22c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">查看<a class="ae kv" rel="noopener" target="_blank" href="/eager-execution-vs-graph-execution-which-is-better-38162ea4dbf6">这篇文章</a>了解更多关于这两种执行模式的区别。</p><p id="aad7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">显然，考虑到图定义和图执行<strong class="kz ir">之间的分离，XLA的JIT编译机制与<strong class="kz ir">图模式</strong>的执行方法非常一致。</strong>支持XLA和<strong class="kz ir">急切执行模式</strong>需要更多创意。有几种方法可以解决这个问题(参见本文<a class="ae kv" href="https://arxiv.org/pdf/2102.13267.pdf" rel="noopener ugc nofollow" target="_blank">的第2节</a>进行简单的调查)。正如我们将看到的，在<a class="ae kv" href="https://github.com/pytorch/xla" rel="noopener ugc nofollow" target="_blank"> PyTorch/XLA </a>库中引入和使用的方法是<a class="ae kv" href="https://github.com/pytorch/xla/blob/master/API_GUIDE.md#xla-tensors-are-lazy" rel="noopener ugc nofollow" target="_blank">懒惰张量系统</a>。</p><h2 id="6538" class="lu lv iq bd lw lx ly dn lz ma mb dp mc lg md me mf lk mg mh mi lo mj mk ml mm bi translated">PyTorch/XLA</h2><p id="efc2" class="pw-post-body-paragraph kx ky iq kz b la mn jr lc ld mo ju lf lg mp li lj lk mq lm ln lo mr lq lr ls ij bi translated"><a class="ae kv" href="https://github.com/pytorch/xla" rel="noopener ugc nofollow" target="_blank"> PyTorch/XLA </a>是一个Python库，其创建的主要目的是使用XLA编译来支持在<a class="ae kv" href="https://cloud.google.com/tpu/" rel="noopener ugc nofollow" target="_blank"> Google Cloud TPUs </a>上进行基于<a class="ae kv" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>的训练(例如，参见这里的<a class="ae kv" href="https://medium.com/pytorch/pytorch-xla-is-now-generally-available-on-google-cloud-tpus-f9267f437832" rel="noopener"/>)。PyTorch/XLA的基本方法是<strong class="kz ir">惰性张量系统</strong>。惰性张量是一种定制的张量类型，在PyTorch/XLA中被称为<em class="lt"> XLA张量</em>。与标准PyTorch张量相反，操作不会立即(或“急切地”)执行，而是收集到形成中间表示(IR)图的操作序列中。当被提示时(例如，在每个训练步骤结束时)，IR图被传递到XLA编译器，在那里它经历对底层加速器的优化。通过这种方式，惰性张量系统支持<strong class="kz ir">领域特定编译</strong> (DSC)，同时保持<em class="lt">热切执行</em>的外观和感觉，这是PyTorch培训框架的标志。虽然使用惰性张量可以在不改变模型定义的情况下进行XLA编译，但了解标准张量和XLA张量之间的细微差别很重要。关于懒惰张量的更多细节以及如何理解他们的行为，请务必查看<a class="ae kv" href="https://pytorch.org/blog/understanding-lazytensor-system-performance-with-pytorch-xla-on-cloud-tpu/" rel="noopener ugc nofollow" target="_blank">这篇内容丰富的帖子</a>。</p><h2 id="9d7f" class="lu lv iq bd lw lx ly dn lz ma mb dp mc lg md me mf lk mg mh mi lo mj mk ml mm bi translated">用于GPU的PyTorch/XLA</h2><p id="64aa" class="pw-post-body-paragraph kx ky iq kz b la mn jr lc ld mo ju lf lg mp li lj lk mq lm ln lo mr lq lr ls ij bi translated">虽然<a class="ae kv" href="https://pytorch.org/xla/release/1.12/index.html" rel="noopener ugc nofollow" target="_blank"> API文档</a>描述PyTorch/XLA支持所有<em class="lt"> XLA设备</em>、<strong class="kz ir">包括GPU</strong>，但是您可能会发现，在撰写本文时，对<a class="ae kv" href="https://cloud.google.com/tpu/" rel="noopener ugc nofollow" target="_blank"> Google Cloud TPUs </a>的支持已经被高度优先考虑。天真地搜索“py torch/GPU上的XLA”会出现几个关于其支持的免责声明，以及一些创建自定义GPU支持构建的非官方说明(例如，参见<a class="ae kv" href="https://github.com/pytorch/xla/issues/1448" rel="noopener ugc nofollow" target="_blank"> this github issue </a>)。</p><p id="fe3a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">令人欣慰的是，一些云服务提供商已经创建了docker映像，专门支持GPU上的PyTorch/XLA。在<a class="ae kv" href="https://cloud.google.com/" rel="noopener ugc nofollow" target="_blank"> GCP </a>可用支持见<a class="ae kv" href="https://github.com/pytorch/xla/#-available-images-and-wheels" rel="noopener ugc nofollow" target="_blank">此处</a>，在<a class="ae kv" href="https://aws.amazon.com/" rel="noopener ugc nofollow" target="_blank"> AWS </a>最新支持图片见<a class="ae kv" href="https://github.com/aws/deep-learning-containers/blob/master/huggingface/pytorch/training/docker/1.11/py3/cu113/Dockerfile.trcomp.gpu" rel="noopener ugc nofollow" target="_blank">此处</a>。在下一节中，我们将使用亚马逊SageMaker培训服务提供一个在GPU上使用PyTorch/XLA的例子。</p><h1 id="f1df" class="ms lv iq bd lw mt mu mv lz mw mx my mc jw mz jx mf jz na ka mi kc nb kd ml nc bi translated">py torch/AWS上的XLA</h1><p id="f383" class="pw-post-body-paragraph kx ky iq kz b la mn jr lc ld mo ju lf lg mp li lj lk mq lm ln lo mr lq lr ls ij bi translated">在下面的代码块中，我们展示了如何使用torch_xla，PyTorch/XLA Python模块来训练一个<a class="ae kv" href="https://huggingface.co/docs/transformers/model_doc/vit" rel="noopener ugc nofollow" target="_blank"> HuggingFace视觉转换器</a>模型。为了突出torch_xla流和标准流之间的区别，我们实现了这两个流，并添加了一个“<em class="lt"> is_xla </em>”标志来在它们之间切换。代码假设我们将在一个有8个GPU的实例上运行。我们强调了一些XLA特有的代码行。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="664d" class="lu lv iq ns b gy nw nx l ny nz">import time<br/>import torch<br/>import os<br/>import json<br/>from torch.utils.data import Dataset</span><span id="54ee" class="lu lv iq ns b gy oa nx l ny nz">num_gpus = 8<br/><strong class="ns ir">is_xla = True</strong></span><span id="cbac" class="lu lv iq ns b gy oa nx l ny nz">if is_xla:<br/>  import torch_xla.core.xla_model as xm<br/>  import torch_xla.distributed.parallel_loader as pl<br/>  if os.environ.get('XRT_WORKERS') is None:<br/><strong class="ns ir">    # Add ENVARS required for XLA<br/></strong>    host=json.loads(os.environ["SM_HOSTS"])[0]<br/>    os.environ["XRT_WORKERS"] = f'localservice:0;{host}:43857'<br/>    os.environ['XRT_SHARD_WORLD_SIZE'] = '1'<br/>    os.environ['XRT_HOST_ORDINAL'] = '0'<br/>    os.environ['FI_EFA_USE_DEVICE_RDMA'] = '1'<br/>    os.environ['NCCL_PROTO'] =  'simple'<br/>    os.environ['XLA_FIX_DIV_FP64'] = '1'<br/>    os.environ['OFI_NCCL_NIC_DUP_CONNS'] = str(num_gpus)<br/>    os.environ["GPU_NUM_DEVICES"] = str(num_gpus)<br/>else:<br/>  # DDP setup<br/>  import torch.distributed as dist<br/>  def setup(rank, world_size):<br/>    os.environ['MASTER_ADDR'] = os.environ.get('MASTER_ADDR',<br/>                                               'localhost')<br/>    os.environ['MASTER_PORT'] = os.environ.get('MASTER_PORT',<br/>                                               str(2222))<br/>    dist.init_process_group('nccl', rank=rank,<br/>                            world_size=world_size)<br/>  # wrap the model with DDP<br/>  def wrap_model(model,local_rank):<br/>    from torch.nn.parallel import DistributedDataParallel as DDP<br/>    model.to(torch.cuda.current_device())<br/>    model = DDP(model,device_ids=[local_rank])<br/>    return model</span><span id="8d30" class="lu lv iq ns b gy oa nx l ny nz"># A fake dataset<br/>class FakeDataset(Dataset):<br/>  def __len__(self):<br/>    return 10000000<br/>  def __getitem__(self, index):<br/>    rand_image = torch.randn([3, 224, 224], dtype=torch.float32)<br/>    label = torch.tensor(data=[index % 1000], dtype=torch.int64)<br/>    return rand_image, label</span><span id="53ea" class="lu lv iq ns b gy oa nx l ny nz">def build_model():<br/>  from transformers import ViTForImageClassification, ViTConfig<br/>  return ViTForImageClassification(ViTConfig(num_labels=1000))</span><span id="a396" class="lu lv iq ns b gy oa nx l ny nz">def main(rank, world_size=num_gpus):<br/>  dataset = FakeDataset()<br/>  model = build_model()<br/><strong class="ns ir">  if is_xla:<br/>    device = xm.xla_device()<br/>    rank = xm.get_local_ordinal()<br/></strong>    model = model.to(device)<br/>  else:<br/>    setup(rank, world_size)<br/>    torch.cuda.set_device(rank)<br/>    model = wrap_model(model,rank)<br/>  batch_size = 128<br/>  optimizer = torch.optim.Adam(model.parameters())<br/>  data_loader = torch.utils.data.DataLoader(<br/>                                  dataset,<br/>                                  batch_size=batch_size,<br/>                                  num_workers=12)<br/><strong class="ns ir">  if is_xla:<br/>    data_loader = pl.MpDeviceLoader(data_loader, device)</strong></span><span id="0478" class="lu lv iq ns b gy oa nx l ny nz">  loss_function = torch.nn.CrossEntropyLoss()<br/>  t0 = time.perf_counter()<br/>  for idx, (inputs, targets) in enumerate(data_loader, start=1):<br/>    if not is_xla:<br/>      inputs = inputs.to(torch.cuda.current_device())<br/>      targets = targets.to(torch.cuda.current_device())<br/>    targets = torch.squeeze(targets,-1)<br/>    optimizer.zero_grad()<br/>    outputs = model(inputs)<br/>    loss = loss_function(outputs['logits'], targets)<br/>    loss.backward()<br/><strong class="ns ir">    if is_xla:<br/>      xm.optimizer_step(optimizer)<br/>    else:<br/>      optimizer.step()<br/></strong>    if rank == 0 and idx%1000 == 0:<br/>      batch_time = time.perf_counter() - t0<br/>      print(f'step: {idx}: mean step time is {batch_time/1000}')<br/>      t0 = time.perf_counter()</span><span id="dbf9" class="lu lv iq ns b gy oa nx l ny nz">  if not is_xla:<br/>    dist.destroy_process_group()</span><span id="d818" class="lu lv iq ns b gy oa nx l ny nz">def _mp_fn(index):<br/>  main(index)</span><span id="99bf" class="lu lv iq ns b gy oa nx l ny nz">if __name__ == "__main__":<br/>  if is_xla:<br/>    import torch_xla.distributed.xla_multiprocessing as mp<br/>  else:<br/>    import torch.multiprocessing as mp<br/>  mp.spawn(_mp_fn, nprocs=num_gpus, join=True)</span></pre><p id="339a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了在<a class="ae kv" href="https://aws.amazon.com/sagemaker/" rel="noopener ugc nofollow" target="_blank"> Amazon SageMaker </a>中运行培训脚本，我们需要对我们的培训作业进行编程，以使用其中一个<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-support.html#training-compiler-supported-frameworks-pytorch" rel="noopener ugc nofollow" target="_blank"> docker映像</a>，该映像包含一个torch_xla版本，该版本经过专门配置和调整，可以在Amazon SageMaker的GPU培训实例上运行。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="eac4" class="lu lv iq ns b gy nw nx l ny nz"><strong class="ns ir">image_uri = "763104351884.dkr.ecr.us-east-1.amazonaws.com/" \<br/>            "huggingface-pytorch-trcomp-training:1.11.0-" \<br/>            "transformers4.21.1-gpu-py38-cu113-ubuntu20.04"</strong></span><span id="f8be" class="lu lv iq ns b gy oa nx l ny nz">from sagemaker.pytorch import PyTorch<br/>estimator = PyTorch(entry_point='train.py',<br/>                    role=&lt;role&gt;,<br/>                    instance_type='ml.p4d.24xlarge',<br/>                    instance_count=1,<br/>                    <strong class="ns ir">image_uri=image_uri</strong>)</span><span id="b193" class="lu lv iq ns b gy oa nx l ny nz">estimator.fit()</span></pre><p id="25f7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">诚然，我们上面分享的培训脚本有点乱。特别是环境变量设置，需要大量的反复试验，(以及一些逆向工程)。在下一小节中，我们将展示如何使用<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler.html" rel="noopener ugc nofollow" target="_blank"> Amazon SageMaker Training编译器</a>获得一个更简洁、更通用的解决方案。</p><h2 id="0a90" class="lu lv iq bd lw lx ly dn lz ma mb dp mc lg md me mf lk mg mh mi lo mj mk ml mm bi translated">亚马逊SageMaker培训编译器</h2><p id="237a" class="pw-post-body-paragraph kx ky iq kz b la mn jr lc ld mo ju lf lg mp li lj lk mq lm ln lo mr lq lr ls ij bi translated"><a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler.html" rel="noopener ugc nofollow" target="_blank">亚马逊SageMaker训练编译器</a>是亚马逊SageMaker的一项功能，旨在加速SageMaker管理的GPU实例上深度学习模型的训练。在引擎盖下，SageMaker Training编译器通过以我们上面描述的方式使用XLA编译来实现训练加速。对于其<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-enable-pytorch.html" rel="noopener ugc nofollow" target="_blank"> PyTorch支持</a>，SageMaker Training编译器加载一个定制构建torch_xla的docker映像，自动配置培训环境供其使用，并启动xla培训作业。特别是，SageMaker Training编译器设置了所有模糊的(未记录的)环境变量，从而将用户所需的操作减少到<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-enable-pytorch.html#training-compiler-enable-pytorch-pysdk" rel="noopener ugc nofollow" target="_blank">几个简单的步骤</a>。</p><p id="e796" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">下面的代码块展示了如何用<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler.html" rel="noopener ugc nofollow" target="_blank"> Amazon SageMaker培训编译器</a>启动我们的PyTorch培训工作。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="fd04" class="lu lv iq ns b gy nw nx l ny nz">from sagemaker.huggingface import HuggingFace, TrainingCompilerConfig</span><span id="5039" class="lu lv iq ns b gy oa nx l ny nz">distribution={'pytorchxla': {'enabled': True}}</span><span id="b29a" class="lu lv iq ns b gy oa nx l ny nz">estimator=HuggingFace(entry_point='train.py',<br/>                      role=&lt;role&gt;,<br/>                      instance_type='ml.p4d.24xlarge',<br/>                      instance_count=1,<br/>                      transformers_version='4.21.1',<br/>                      pytorch_version='1.11.0',<br/>                      compiler_config=TrainingCompilerConfig(),<br/>                      distribution=distribution)</span><span id="c30b" class="lu lv iq ns b gy oa nx l ny nz">estimator.fit()</span></pre><p id="b084" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">请注意，如果您的训练脚本使用了<a class="ae kv" href="https://huggingface.co/docs/transformers/index" rel="noopener ugc nofollow" target="_blank"> HuggingFace transformer模型</a>，您可以通过使用高级<a class="ae kv" href="https://huggingface.co/docs/transformers/main_classes/trainer" rel="noopener ugc nofollow" target="_blank"> Huggingface训练器</a> API来进一步简化SageMaker训练编译器(和torch_xla)的使用，如<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-pytorch-models.html#training-compiler-pytorch-models-transformers-trainer" rel="noopener ugc nofollow" target="_blank">特性文档</a>中所述。</p><p id="f23f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">请务必查看<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-pytorch-models.html" rel="noopener ugc nofollow" target="_blank">功能文档</a>了解更多详情。关于不同使用模式的示例，参见<a class="ae kv" href="https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-training-compiler/index.html" rel="noopener ugc nofollow" target="_blank"> API文档</a>。</p><h2 id="4efc" class="lu lv iq bd lw lx ly dn lz ma mb dp mc lg md me mf lk mg mh mi lo mj mk ml mm bi translated">结果</h2><p id="4fe9" class="pw-post-body-paragraph kx ky iq kz b la mn jr lc ld mo ju lf lg mp li lj lk mq lm ln lo mr lq lr ls ij bi translated">在下表中，我们比较了Vision Transformer在使用和不使用XLA编译时的运行时性能，以每秒<strong class="kz ir">个样本</strong>来衡量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/e39093edfbb11821d5ad136870f7baee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HTAUWEF2_c6IGUBuEY_kSg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">XLA编译对每秒采样数的影响—越高越好(作者)</p></figure><p id="fee6" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">应用XLA编译将我们模型的性能提高了<strong class="kz ir"> ~15% </strong>。我们再次强调，XLA编译的潜在好处非常依赖于模型的细节。在不同的ML模型上运行相同的实验可能会产生非常不同的比较结果。</p><p id="5f95" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">请注意，XLA跑的步长时间稳定下来需要大约10，000步，或者接近一个小时。这是XLA编译的一个已知症状。</p><h1 id="8de4" class="ms lv iq bd lw mt mu mv lz mw mx my mc jw mz jx mf jz na ka mi kc nb kd ml nc bi translated">XLA限制</h1><p id="d830" class="pw-post-body-paragraph kx ky iq kz b la mn jr lc ld mo ju lf lg mp li lj lk mq lm ln lo mr lq lr ls ij bi translated">在使用XLA编译器之前，了解它的一些限制是很重要的。</p><h2 id="233f" class="lu lv iq bd lw lx ly dn lz ma mb dp mc lg md me mf lk mg mh mi lo mj mk ml mm bi translated">模型依赖性</h2><p id="8e3c" class="pw-post-body-paragraph kx ky iq kz b la mn jr lc ld mo ju lf lg mp li lj lk mq lm ln lo mr lq lr ls ij bi translated">在这篇文章中，我们强调了这样一个事实，XLA为<strong class="kz ir">提供了潜在的</strong>性能提升，但不是性能提升的保证。您是否会从使用XLA中受益，如果会，性能会有多显著，都高度依赖于您的模型的具体情况。在某些情况下，例如当模型包含动态形状的张量时，您可能会发现使用XLA会降低性能。虽然通常很难预测你的模式是否“XLA友好”，但有几个资源(如<a class="ae kv" href="https://www.tensorflow.org/xla/known_issues" rel="noopener ugc nofollow" target="_blank">此处</a>)应该可以为你提供一些一般性的指导。</p><p id="6ee5" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">正如我们在上面的例子中看到的，XLA编译是一个发生在多个训练步骤中的过程。根据您的模型，训练步长收敛到其XLA最优值可能需要几分钟或更长时间。特别是，如果你的整体训练持续时间相对较短，你可能看不到使用XLA的好处。</p><h2 id="a9ce" class="lu lv iq bd lw lx ly dn lz ma mb dp mc lg md me mf lk mg mh mi lo mj mk ml mm bi translated">调试和实验的局限性</h2><p id="57cb" class="pw-post-body-paragraph kx ky iq kz b la mn jr lc ld mo ju lf lg mp li lj lk mq lm ln lo mr lq lr ls ij bi translated">PyTorch培训框架及其热切的执行策略流行的原因之一是它简化了调试和实验。在PyTorch中，人们可以计算任何中间张量的值，而不需要跳来跳去这样做(如在图形模式执行中)。通过采用XLA汇编，我们基本上放弃了这一点。可以通过支持XLA和非XLA流来减轻这种限制(如上面的脚本所示)，但是，我们看到的行为总是有可能是XLA流所独有的。</p><p id="a1f4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">使调试和实验变得复杂的另一个因素是我们的训练依赖于定制的torch_xla构建。截至本文写作时，还没有官方的torch_xla <a class="ae kv" href="https://pypi.org/" rel="noopener ugc nofollow" target="_blank"> Python包</a>。在受控环境中评估XLA行为的一个选项是<a class="ae kv" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html" rel="noopener ugc nofollow" target="_blank">从CSP的映像库中提取XLA特定的docker映像</a>(或者使用SageMaker的本地模式来完成)。PyTorch/XLA文档包括配置torch_xla在CPU 上运行的<a class="ae kv" href="https://pytorch.org/xla/release/1.11/index.html#before-you-start" rel="noopener ugc nofollow" target="_blank">指令。然而，您可能会发现在本地GPU上运行docker映像更具挑战性。无论如何，以这种方式进行调试和试验显然是有局限性的，而且远非理想。</a></p><h2 id="41b9" class="lu lv iq bd lw lx ly dn lz ma mb dp mc lg md me mf lk mg mh mi lo mj mk ml mm bi translated">代码调整</h2><p id="eaef" class="pw-post-body-paragraph kx ky iq kz b la mn jr lc ld mo ju lf lg mp li lj lk mq lm ln lo mr lq lr ls ij bi translated">使用torch_xla需要大量的代码修改。这些可以分为两种类型，使用torch_xla所需的对培训流程的调整，以及使用torch_xla时作为最佳实践推荐的调整。在上面的代码示例中，我们演示了一些最基本的<em class="lt">所需的修改</em>。在实践中，经常需要额外的修改来保存和加载模型，集成自动混合精度等等。这里记录了所需的调整<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-pytorch-models.html#training-compiler-pytorch-models-non-trainer" rel="noopener ugc nofollow" target="_blank">。这里有几个资源，比如这里的</a><a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-tips-pitfalls.html" rel="noopener ugc nofollow" target="_blank">这里的</a>和<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-pytorch-models.html#training-compiler-pytorch-models-best-practices" rel="noopener ugc nofollow" target="_blank">这里的</a>，描述了<em class="lt">推荐的改编</em>以最大限度地利用torch_xla。这些通常旨在最大限度地减少XLA汇编的次数。只有当模型可以被XLA编译一次并在所有后续步骤中重用时，它们才会从使用XLA中受益。需要频繁XLA编译的模型(例如，张量具有动态形状的模型)不会从使用torch_xla中受益。引用<a class="ae kv" href="https://pytorch.org/blog/understanding-lazytensor-system-performance-with-pytorch-xla-on-cloud-tpu/" rel="noopener ugc nofollow" target="_blank">这篇推荐的博客文章</a>，其中包括使用torch_xla的最佳实践，“编译一次，经常执行”<strong class="kz ir">。</strong></p><p id="01c9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">代码修改，不管它们是必需的还是推荐的，都会使torch_xla的使用复杂化。如果你选择维持XLA和非XLA的资金流动，这一点尤其正确。</p><h1 id="74a7" class="ms lv iq bd lw mt mu mv lz mw mx my mc jw mz jx mf jz na ka mi kc nb kd ml nc bi translated">摘要</h1><p id="d82b" class="pw-post-body-paragraph kx ky iq kz b la mn jr lc ld mo ju lf lg mp li lj lk mq lm ln lo mr lq lr ls ij bi translated">XLA汇编有可能大大加快培训速度，进而节省培训成本。不幸的是，在撰写本文时，GPU对PyTorch中XLA编译的支持仍然处于TPU的次要地位。值得庆幸的是，像AWS这样的CSP已经创建了定制版本，让我们可以从XLA编译中获益。我们只能希望未来能提高torch_xla的可用性和易用性，特别是在GPU上。这种改进将包括:</p><ol class=""><li id="ed81" class="nd ne iq kz b la lb ld le lg nf lk ng lo nh ls ni nj nk nl bi translated">一座官方多用途火炬xla建筑，</li><li id="a1bb" class="nd ne iq kz b la nm ld nn lg no lk np lo nq ls ni nj nk nl bi translated">torch_xla和标准torch API流的一致性，</li><li id="54b0" class="nd ne iq kz b la nm ld nn lg no lk np lo nq ls ni nj nk nl bi translated">将XLA支持合并到官方PyTorch包中(如在TensorFlow中)，以及</li><li id="b96f" class="nd ne iq kz b la nm ld nn lg no lk np lo nq ls ni nj nk nl bi translated">改进了使用文档，特别是在GPU上。</li></ol><p id="1850" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">同时，“试一试，看看你的代码飞起来吧”:)。</p><p id="0e6d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">如有任何意见、问题或更正，请随时联系我。</p></div></div>    
</body>
</html>