<html>
<head>
<title>Leveraging Feature Importance to Predict Mushrooms’ Edibility in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用特征重要性预测Python中蘑菇的可食用性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-importance-to-predict-mushrooms-edibility-in-python-5aa133fea3f8#2022-09-14">https://towardsdatascience.com/feature-importance-to-predict-mushrooms-edibility-in-python-5aa133fea3f8#2022-09-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7726" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">利用特征重要性来预测蘑菇种类是可食用的还是有毒的</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6d38cd4c5c3cb1893092e86c86242aae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MynMFKZZfL5s7aK5"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@selzcc?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Jannik Selz </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="fb55" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="a49d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi mn translated"><span class="l mo mp mq bm mr ms mt mu mv di"> T </span>本文旨在利用特征重要性来评估<strong class="lt iu">数据集中的所有列</strong>是否需要用于<strong class="lt iu">预测</strong>。</p><p id="dc43" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">秋天快到了。想象一下，你正在树林中散步，发现路边有一些蘑菇。将它们的一些特性输入到一个ML驱动的应用程序中不是很好吗？我个人不喜欢吃蘑菇，但我绝对喜欢食物，走了很长一段路后，我已经可以闻到面前有一道美味的“tagliolini ai funghi”。</p><p id="64e3" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">蘑菇是真菌，是独立于植物和动物的独立王国的一部分。正如你可能想象的那样，在决定一个人是否可以食用之前，他们会给出无数的特征来评估。</p><p id="fa91" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">您的徒步旅行团队正在离开，我们需要尽快做出决定。</p><blockquote class="nb"><p id="1e6c" class="nc nd it bd ne nf ng nh ni nj nk mm dk translated">哪个信息是舒适地评估蘑菇品种的可食用性的基础？</p></blockquote><p id="231a" class="pw-post-body-paragraph lr ls it lt b lu nl ju lw lx nm jx lz ma nn mc md me no mg mh mi np mk ml mm im bi translated">让我们找出答案</p><h2 id="dde6" class="nq la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated">数据</h2><p id="64e1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">数据集</strong>可以在<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/mushroom" rel="noopener ugc nofollow" target="_blank">机器学习UCI知识库</a>【1】中检索。它包括关于<strong class="lt iu">蘑菇</strong>和<strong class="lt iu">蘑菇</strong>家族中<strong class="lt iu"> 23种蘑菇</strong>的假设信息。这些记录是假设的，因为根据蘑菇指南，它们是人工输入的。作者确定了<strong class="lt iu"> 22个独特的特征</strong>，不包括可食用或有毒的特征，其中一些是:</p><ol class=""><li id="fef7" class="oc od it lt b lu mw lx mx ma oe me of mi og mm oh oi oj ok bi translated"><code class="fe ol om on oo b">cap-shape</code>:钟形=b，锥形=c，凸形=x，扁平=f，圆球形=k，凹形=s</li><li id="5345" class="oc od it lt b lu op lx oq ma or me os mi ot mm oh oi oj ok bi translated"><code class="fe ol om on oo b">cap-surface</code>:纤维状=f，凹槽=g，鳞状=y，光滑=s</li><li id="0050" class="oc od it lt b lu op lx oq ma or me os mi ot mm oh oi oj ok bi translated"><code class="fe ol om on oo b">cap-color</code>:棕色=n，浅黄色=b，肉桂色=c，灰色=g，绿色=r，粉色=p，紫色=u，红色=e，白色=w，黄色=y</li><li id="158e" class="oc od it lt b lu op lx oq ma or me os mi ot mm oh oi oj ok bi translated"><code class="fe ol om on oo b">bruises?</code>:淤青=t，no=f</li><li id="0164" class="oc od it lt b lu op lx oq ma or me os mi ot mm oh oi oj ok bi translated"><code class="fe ol om on oo b">odor</code>:杏仁=a，茴香=l，杂酚油=c，鱼腥味=y，恶臭=f，霉味=m，无=n，刺鼻=p，辛辣=s</li><li id="689c" class="oc od it lt b lu op lx oq ma or me os mi ot mm oh oi oj ok bi translated"><code class="fe ol om on oo b">gill-attachment</code>:附着=a，下降=d，自由=f，缺口=n</li><li id="7c29" class="oc od it lt b lu op lx oq ma or me os mi ot mm oh oi oj ok bi translated"><code class="fe ol om on oo b">gill-spacing</code>:近=c，挤=w，远=d</li><li id="6892" class="oc od it lt b lu op lx oq ma or me os mi ot mm oh oi oj ok bi translated"><code class="fe ol om on oo b">gill-size</code>:宽=b，窄=n</li><li id="ec9b" class="oc od it lt b lu op lx oq ma or me os mi ot mm oh oi oj ok bi translated"><code class="fe ol om on oo b">gill-color</code>:黑色=k，棕色=n，浅黄色=b，巧克力色=h，灰色=g，绿色=r，橙色=o，粉色=p，紫色=u，红色=e，白色=w，黄色=y</li><li id="6c61" class="oc od it lt b lu op lx oq ma or me os mi ot mm oh oi oj ok bi translated"><code class="fe ol om on oo b">stalk-shape</code>:放大=e，缩小=t</li><li id="53c7" class="oc od it lt b lu op lx oq ma or me os mi ot mm oh oi oj ok bi translated"><code class="fe ol om on oo b">stalk-root</code>:球根=b，球杆=c，杯子=u，等号=e，根茎=z，有根=r，缺失=？</li><li id="e1d1" class="oc od it lt b lu op lx oq ma or me os mi ot mm oh oi oj ok bi translated"><code class="fe ol om on oo b">stalk-surface-above-ring</code>:纤维状=f，鳞状=y，丝状=k，光滑=s</li><li id="e6f4" class="oc od it lt b lu op lx oq ma or me os mi ot mm oh oi oj ok bi translated"><code class="fe ol om on oo b">stalk-color-above-ring</code>:棕色=n，浅黄色=b，肉桂色=c，灰色=g，橙色=o，粉色=p，红色=e，白色=w，黄色=y</li><li id="55a6" class="oc od it lt b lu op lx oq ma or me os mi ot mm oh oi oj ok bi translated"><code class="fe ol om on oo b">spore-print-color</code>:黑色=k，棕色=n，浅黄色=b，巧克力色=h，绿色=r，橙色=o，紫色=u，白色=w，黄色=y</li></ol><h2 id="4286" class="nq la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated">特征重要性</h2><p id="3dfe" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">您可以将一个要素标识为数据集中的多个列之一。计算<strong class="lt iu">特征重要性</strong>是指每个特征在训练一个模型后所拥有的分数。你会同意我的观点，即<strong class="lt iu">并不是</strong>所有的特征都是<strong class="lt iu">相等的</strong>，它们对输出预测有贡献。</p><p id="eec1" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">我们可以看到与<strong class="lt iu">帕累托法则</strong>的相似之处，其中某件事情的<strong class="lt iu"> 20% </strong>负责结果的<strong class="lt iu"> 80% </strong>，或者称为20:80法则。在<strong class="lt iu">销售环境</strong>中，帕累托原则的一个例子是，你的20%的客户创造了你80%的销售额。在一个<strong class="lt iu">机器学习场景</strong>中，我们可以把这个原则翻译成“<strong class="lt iu"> 20% </strong>的特征负责<strong class="lt iu"> 80% </strong>的正确预测”。特征重要性允许从业者识别出<strong class="lt iu"> 20% </strong>。</p><p id="4fa4" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">你是如何解读乐谱的？一个<strong class="lt iu">较高的分数</strong>表明一个特性对你的模型更有<strong class="lt iu">影响</strong>，而一个<strong class="lt iu">较低的分数</strong>表明信息<strong class="lt iu">对模型来说不那么重要</strong>。对于我们的项目，我们有21个特征，我们想计算每个特征的重要性分数，并且只选择几个[2]。</p><h1 id="9493" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">模特们</h1><p id="c628" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">特征选择考虑的模型有两个:<strong class="lt iu">随机森林</strong>和<strong class="lt iu">梯度提升</strong>。每一种都有自己的特点，我将简要解释一下。</p><h2 id="2136" class="nq la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated">随机森林</h2><p id="ca20" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">“<strong class="lt iu">随机决策森林</strong>属于<strong class="lt iu">集成学习</strong>算法的范畴。“<strong class="lt iu">森林</strong>这个名字来自于它“组成”的个体实体:<strong class="lt iu">决策树</strong>。在训练阶段，多个决策树从数据中学习，一个<strong class="lt iu">投票</strong>机制确保最常见的结果也是最终输出。该模型同时用于<strong class="lt iu">分类</strong>和<strong class="lt iu">回归</strong>任务。</p><p id="7b94" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated"><strong class="lt iu">随机森林</strong>在有大量变量时表现出了显著的<strong class="lt iu">性能</strong>。鉴于需要调整的超参数很少，企业认为这是一个“<strong class="lt iu">黑箱</strong>”。很难解释为什么一个<strong class="lt iu">随机森林</strong>会做出某些预测。另一方面，该特征提供了<strong class="lt iu">适应性</strong>和<strong class="lt iu">高精度</strong>，而不会陷入过度拟合的风险【3】。</p><h2 id="2b1c" class="nq la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated">梯度推进</h2><p id="ba70" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">梯度提升</strong>也是一种<strong class="lt iu">集成学习</strong>算法，可用于<strong class="lt iu">分类</strong>和<strong class="lt iu">回归任务</strong>。类似于随机森林，梯度增强预测基于应用于决策树的<strong class="lt iu">投票</strong>机制。根本区别在于，这些决策树被定义为“<strong class="lt iu">弱</strong>或“<strong class="lt iu">浅</strong>”，这意味着一个模型具有<strong class="lt iu">高偏差</strong>和<strong class="lt iu">低方差</strong>。换句话说，它利用了<strong class="lt iu">一致的预测</strong>(具有所有相似的结果)，但是不准确(显示错误的结果)。集合的目标是<strong class="lt iu">减少偏差</strong>同时保持<strong class="lt iu">方差低</strong>，使模型输出更接近目标结果【4】。</p><p id="92d1" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">你可以把它想象成一束指向镖靶的激光。起初，模型做出的所有预测都远离靶心，但它们都彼此接近，它们是一致的。梯度提升以<strong class="lt iu">减少误差</strong>为目标依次构建弱学习器，使目标越来越接近靶心，即最佳模型。与直觉相反，梯度推进仍然保留了很大的<strong class="lt iu">泛化</strong>能力，这使得它成为一个<strong class="lt iu">计算昂贵的</strong>但总体上很棒的学习模型。</p><h1 id="0828" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">Python中的代码部署</h1><h2 id="7a3a" class="nq la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated">数据探索</h2><p id="50f7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">与大多数数据科学项目一样，代码部署的第一部分侧重于<strong class="lt iu">数据探索</strong>和<strong class="lt iu">可视化</strong>。一个好的早期测试，检查数据集<strong class="lt iu">是否平衡。在分类问题中，重要的是两个目标变量呈现相似的计数。<strong class="lt iu">探索性数据分析</strong>因此是项目的第一步。</strong></p><ul class=""><li id="c80b" class="oc od it lt b lu mw lx mx ma oe me of mi og mm ou oi oj ok bi translated">我们根据需要导入<code class="fe ol om on oo b">pandas</code>和<code class="fe ol om on oo b">matplotlib</code>库</li><li id="eaaf" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">我们将csv文件作为熊猫<strong class="lt iu">数据帧</strong>读取</li><li id="cfaa" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated"><code class="fe ol om on oo b">category_dist</code>统计可食用或有毒的记录数量</li><li id="8704" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">此时，可以定义图表及其<strong class="lt iu">大小</strong>、<strong class="lt iu">颜色</strong>、<strong class="lt iu">字体</strong>、<strong class="lt iu">大小</strong>，以及我们是否想要一个网格。</li></ul><pre class="kj kk kl km gt ov oo ow ox aw oy bi"><span id="0504" class="nq la it oo b gy oz pa l pb pc">#Importing libraries<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span><span id="14cc" class="nq la it oo b gy pd pa l pb pc">#Reading dataset<br/>df = pd.read_csv('mushrooms.csv')</span><span id="7beb" class="nq la it oo b gy pd pa l pb pc">#Counting values within each class<br/>category_dist = df['class'].value_counts()</span><span id="1210" class="nq la it oo b gy pd pa l pb pc">#Defining chart<br/>plt.figure(figsize=(8,6))<br/>category_dist.plot(kind='bar', color = '#89b4a1')<br/>plt.grid()<br/>plt.xlabel("Edible/Poisonous", fontsize = 12)<br/>plt.ylabel("Number of Measurements", fontsize = 12)<br/>plt.title("Distribution of Measurements", fontsize = 15)<br/>plt.grid(False)</span><span id="36d1" class="nq la it oo b gy pd pa l pb pc">#Generating chart<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/e6f7d34f3e88842641ef1e1ee7aa67a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HvlvVdNQtggz9N8TeoecaA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">蘑菇可食用性的秋季主题分布——作者图表</p></figure><p id="21dd" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">对于蘑菇数据集，可食用物种的数量彼此之间只相差几百个。我们可以断定数据集是<strong class="lt iu">平衡的</strong>，我们可以继续进行项目的下一部分，标签编码。</p><h2 id="930f" class="nq la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated">标签编码</h2><p id="2b24" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">处理<strong class="lt iu">分类变量</strong>可能是几个机器学习算法的问题。在预处理阶段，标签编码通过将标签转换成数字来解决问题，以便机器<strong class="lt iu">能够读取它们。</strong></p><p id="412a" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">流程是如何运作的？让我们以cap-color特性为例，有十种可能性:</p><ul class=""><li id="0572" class="oc od it lt b lu mw lx mx ma oe me of mi og mm ou oi oj ok bi translated">棕色=n，</li><li id="7ad3" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">buff=b，</li><li id="5d07" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">肉桂=c，</li><li id="8d5b" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">灰色=g，</li><li id="7493" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">绿色=r，</li><li id="1f04" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">粉色=p，</li><li id="38cc" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">紫色=u，</li><li id="2feb" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">红色=e，</li><li id="3977" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">白色=w，</li><li id="f78b" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">黄色=y</li></ul><p id="57e1" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">标签编码算法按照字母顺序对它们进行分类:b、c、e、g……然后给每个字母分配一个从0开始的数字。字母“b”对应于0，“c”对应于1，“e”对应于2，依此类推。</p><p id="881c" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">多亏了<code class="fe ol om on oo b">sklearn</code>，这个过程相当容易。从业者需要导入<code class="fe ol om on oo b">LabelEncoder</code>包，创建一个名为<code class="fe ol om on oo b">labelencoder</code>的对象，并编写一个for循环语句，该对象将每一列的记录转换成一个<strong class="lt iu">数字</strong>形式。</p><pre class="kj kk kl km gt ov oo ow ox aw oy bi"><span id="f705" class="nq la it oo b gy oz pa l pb pc">from sklearn.preprocessing import LabelEncoder</span><span id="4527" class="nq la it oo b gy pd pa l pb pc">labelencoder=LabelEncoder()</span><span id="6a8f" class="nq la it oo b gy pd pa l pb pc">for col in df.columns:<br/>    df[col] = labelencoder.fit_transform(df[col])</span></pre><p id="09ee" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">为了更直观地了解这一过程，下面的图表显示了每一类(可食用或有毒)蘑菇的<strong class="lt iu">颜色。你可以看到大多数棕色和灰色蘑菇是可以食用的，但是你应该远离黄色蘑菇。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/3adb6f229efaaea95e617e68c67e42d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6RTU8Uai3Jt1YRhM1Jskpg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">热编码前蘑菇种类的颜色和类别的秋季主题分布-作者图表</p></figure><p id="38ef" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">在我们对数据集进行标签编码后，你会注意到类和帽颜色特征都已经被<strong class="lt iu">转换成数字</strong>。食用的字母“e”现在对应于数字0，而“p”对应于1。关于帽子的颜色，所有的棕色蘑菇现在被归类为4，因为它是按字母顺序排列的第5个字母。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/c8d0392480f3ab04eedfa9dadbc552e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wWvg4HDkGXbwx2DattnCrg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">热编码后蘑菇种类按颜色和类别的秋季主题分布-作者图表</p></figure><h2 id="dfaf" class="nq la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated">型号选择</h2><p id="5285" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">对于<strong class="lt iu">模型选择</strong>部分，我强烈推荐你看一下我之前的一篇文章中的<a class="ae ky" rel="noopener" target="_blank" href="/model-selection-and-hyperparameter-tuning-on-amazon-kindle-book-reviews-with-python-6d17ec46d318">，在这篇文章中，我解释了如何<strong class="lt iu">部署流水线</strong>并评估几个机器学习<strong class="lt iu">模型的性能</strong>。对于这个项目，我在将以下内容设置为输入和目标变量后应用了相同的管道:</a></p><pre class="kj kk kl km gt ov oo ow ox aw oy bi"><span id="8b36" class="nq la it oo b gy oz pa l pb pc">#Defining input and target variables</span><span id="f4f4" class="nq la it oo b gy pd pa l pb pc">x = df.drop('class', axis=1)<br/>y = df['class']</span></pre><p id="be52" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">我们的输入变量最初是数据集的所有21个特征。因此<code class="fe ol om on oo b">x</code> <strong class="lt iu"> </strong>由除<code class="fe ol om on oo b">class</code>之外的所有列组成。既然我们想预测一个蘑菇是否可以食用，那么目标变量就是class，它包含了我们需要的信息。</p><p id="1495" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">我决定交叉验证<strong class="lt iu"> 16种不同的算法</strong>，以在评估特性的重要性之前识别出具有最高性能的算法<strong class="lt iu"/>(yo-yo Eminem)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/13e0b7dbbf45744507e8e5f674ba3c25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P2bG1ky0mys5yqhb8LjMbQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模型选择过程后算法的准确性比较—作者提供的图表</p></figure><p id="bf8e" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">特别地，四个以最小的方差显示出最高的准确度:<strong class="lt iu">决策树分类器(CART) </strong>，<strong class="lt iu">随机森林(RF) </strong>，<strong class="lt iu"> AdaBoost分类器</strong> <strong class="lt iu"> (ADA) </strong>，<strong class="lt iu">梯度增强(XGB) </strong>。</p><pre class="kj kk kl km gt ov oo ow ox aw oy bi"><span id="c1aa" class="nq la it oo b gy oz pa l pb pc">LR: 0.839369 (0.142382) <br/>LDA: 0.843812 (0.159062) <br/>KNN: 0.908047 (0.094523) <br/>CART: 0.991626 (0.015547) <br/>NB: 0.713839 (0.189206) <br/>SVM: 0.824590 (0.187337) <br/>SGC: 0.782230 (0.159755) <br/>Perc: 0.760104 (0.150632) <br/>NC: 0.698081 (0.167803) <br/>Ridge: 0.843319 (0.158079) <br/>NuSVC: 0.746827 (0.189874) <br/>BNB: 0.800220 (0.136547) <br/>RF: 0.997168 (0.003473) <br/>ADA: 0.994582 (0.009653) <br/>XGB: 0.998277 (0.003446) <br/>PAC: 0.850076 (0.106792)</span></pre><p id="0e95" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">值得一提的是，这样的<strong class="lt iu">高性能</strong>可能是<strong class="lt iu">过度适应</strong>的迹象，这个问题我们不打算在本文中讨论。不过，在未来的项目中，这将是一个很好的解释主题。现在，让我们考虑一下将被带到下一阶段的两个最好的模型:<strong class="lt iu">随机森林</strong>和<strong class="lt iu">梯度提升</strong>。</p><h2 id="1ca5" class="nq la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated">特征重要性:随机森林</h2><p id="b485" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这个阶段，我们知道<strong class="lt iu">随机森林</strong>的表现已经很出色了，下一步就是评估<strong class="lt iu">的表现</strong>是否能在更少的信息下保持不变。这可以通过选择<strong class="lt iu">几个特征</strong>来完成，这些特征是进行预测的基础:</p><ul class=""><li id="2410" class="oc od it lt b lu mw lx mx ma oe me of mi og mm ou oi oj ok bi translated">首先，让我们导入所有我们需要的<code class="fe ol om on oo b">sklearn</code>子包:<code class="fe ol om on oo b">train_test_split</code>、<code class="fe ol om on oo b">classification_report</code>、<code class="fe ol om on oo b">RandomForestClassifier</code>和<code class="fe ol om on oo b">GradientBoostingClassifier</code></li></ul><pre class="kj kk kl km gt ov oo ow ox aw oy bi"><span id="e0f2" class="nq la it oo b gy oz pa l pb pc">#Importing libraries<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import classification_report<br/>from sklearn.ensemble import RandomForestClassifier,<br/>GradientBoostingClassifier</span></pre><ul class=""><li id="a472" class="oc od it lt b lu mw lx mx ma oe me of mi og mm ou oi oj ok bi translated">在运行单元之前，需要再次设置输入和目标变量，以避免任何问题</li><li id="9b3b" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">我们可以通过分配<strong class="lt iu"> 80% </strong>训练数据和<strong class="lt iu"> 20% </strong>测试来分割数据集。我决定将<code class="fe ol om on oo b">random_state=56</code>设置为结果再现性。它可以是你想要的任何自然数。</li><li id="e2a7" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">我们用<code class="fe ol om on oo b">RandomForestClassifier()</code>函数创建一个对象。</li><li id="81ac" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">此时，命令RE.fit开始特征(<code class="fe ol om on oo b">X_train</code>)和目标变量(<code class="fe ol om on oo b">y_train</code>)的训练阶段</li><li id="dcb4" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">一旦在80%的数据集上训练了分类器，我们就可以通过创建<code class="fe ol om on oo b">predicted_RF</code>对象在剩余的20%上测试它的准确性</li><li id="90d9" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">最后，分类报告告诉我们经过训练的分类器的性能</li></ul><pre class="kj kk kl km gt ov oo ow ox aw oy bi"><span id="5896" class="nq la it oo b gy oz pa l pb pc">#Defining input and target variable<br/>x = df.drop('class', axis=1)<br/>y = df['class']</span><span id="4892" class="nq la it oo b gy pd pa l pb pc">#Splitting dataset between training and testing with a 80-20 #allocation<br/>X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 56)</span><span id="68f2" class="nq la it oo b gy pd pa l pb pc">#Classifier Object<br/>RF = RandomForestClassifier()</span><span id="367d" class="nq la it oo b gy pd pa l pb pc">#Fitting classifier to data<br/>RF.fit(X_train, y_train)</span><span id="4dd9" class="nq la it oo b gy pd pa l pb pc">#Predicting testing records<br/>predicted_RF = RF.predict(X_test)</span><span id="75c6" class="nq la it oo b gy pd pa l pb pc">#Printing classification report<br/>print(classification_report(y_test, predicted_RF))</span></pre><p id="0045" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">从我们在选型阶段看到的情况来看，<strong class="lt iu">精度</strong>有了进一步的提高。该算法能够在100%的情况下识别蘑菇是可食用的还是有毒的。产生输出的信息目前来自<strong class="lt iu">所有特征</strong>。</p><p id="c041" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">现在的问题是:</p><blockquote class="pf pg ph"><p id="6146" class="lr ls pi lt b lu mw ju lw lx mx jx lz pj my mc md pk mz mg mh pl na mk ml mm im bi translated">“有没有比其他更根本的特征？当我看到一个我想收集的蘑菇时，我需要特别注意什么？”</p></blockquote><pre class="kj kk kl km gt ov oo ow ox aw oy bi"><span id="0ab2" class="nq la it oo b gy oz pa l pb pc">              precision    recall  f1-score   support<br/>      <br/>           0       1.00      1.00      1.00       841   <br/>           1       1.00      1.00      1.00       784</span><span id="245a" class="nq la it oo b gy pd pa l pb pc">    accuracy                           1.00      1625  <br/>   macro avg       1.00      1.00      1.00      1625<br/>weighted avg       1.00      1.00      1.00      1625</span></pre><p id="59e9" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">运行下面的代码单元格后，可以确定上面的答案。这看起来很复杂，但我们可以一点一点地分解它:</p><ul class=""><li id="07b4" class="oc od it lt b lu mw lx mx ma oe me of mi og mm ou oi oj ok bi translated">单元格创建两列:一列名为<code class="fe ol om on oo b">feature</code>，另一列名为<code class="fe ol om on oo b">feature_importance</code>，第一列列出了数据集中包含的所有列:<code class="fe ol om on oo b">Cap-shape</code>、<code class="fe ol om on oo b">Cap-surface</code>、<code class="fe ol om on oo b">Cap-color</code> …</li><li id="c8ae" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">第二列包含数据集每一列的所有关联要素重要性值。如果你只是想要这个值，你需要对随机森林算法使用命令<code class="fe ol om on oo b">RF.feature_importances_</code>(其他分类器可能有不同的名字)</li><li id="9800" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">然后这两列一起被转换成熊猫数据帧</li><li id="5407" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">结果以我可以用命令<code class="fe ol om on oo b">to_markdown</code>复制和粘贴的格式打印出来</li></ul><pre class="kj kk kl km gt ov oo ow ox aw oy bi"><span id="0502" class="nq la it oo b gy oz pa l pb pc">##RF feature importance</span><span id="2698" class="nq la it oo b gy pd pa l pb pc">feature_importance = pd.DataFrame(<br/>{'feature':list(X_train.columns),<br/>'feature_importance':[abs(i) for i in RF.feature_importances_]}<br/>)</span><span id="e464" class="nq la it oo b gy pd pa l pb pc">print(feature_importance.to_markdown())</span></pre><p id="bb47" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">可以看出，要素重要性报告的值从0到0.002一直到0.2。没有特别的原因，我选择了报告值至少为0.1或接近0.1的行。截止日期留给我们4个特征用于下一个模型:<code class="fe ol om on oo b">odor</code>、<code class="fe ol om on oo b">gill-size</code>、<code class="fe ol om on oo b">gill-color</code>和<code class="fe ol om on oo b">spore-print-color</code>。</p><pre class="kj kk kl km gt ov oo ow ox aw oy bi"><span id="0e29" class="nq la it oo b gy oz pa l pb pc">|    | feature                  |   feature_importance |<br/>|---:|:-------------------------|---------------------:|<br/>|  0 | cap-shape                |           0.00488162 |<br/>|  1 | cap-surface              |           0.00917991 |<br/>|  2 | cap-color                |           0.013382   |<br/>|  3 | bruises                  |           0.054723   |<br/>|  4 | odor                     |           0.183597   |<br/>|  5 | gill-attachment          |           0.00193266 |<br/>|  6 | gill-spacing             |           0.03089    |<br/>|  7 | gill-size                |           0.101708   |<br/>|  8 | gill-color               |           0.122184   |<br/>|  9 | stalk-shape              |           0.0202438  |<br/>| 10 | stalk-root               |           0.0601195  |<br/>| 11 | stalk-surface-above-ring |           0.0411767  |<br/>| 12 | stalk-surface-below-ring |           0.040044   |<br/>| 13 | stalk-color-above-ring   |           0.0141602  |<br/>| 14 | stalk-color-below-ring   |           0.0147517  |<br/>| 15 | veil-type                |           0          |<br/>| 16 | veil-color               |           0.00194969 |<br/>| 17 | ring-number              |           0.0151127  |<br/>| 18 | ring-type                |           0.0839357  |<br/>| 19 | spore-print-color        |           0.0978179  |<br/>| 20 | population               |           0.0604895  |<br/>| 21 | habitat                  |           0.0277208  |</span></pre><p id="a4c2" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">现在我们知道了最重要的特性，我们可以重复训练和测试过程。这次<code class="fe ol om on oo b">x</code>只有<strong class="lt iu">四列</strong>我们确定为最重要的特性。目标变量<code class="fe ol om on oo b">y</code>保持不变。</p><ul class=""><li id="3504" class="oc od it lt b lu mw lx mx ma oe me of mi og mm ou oi oj ok bi translated">我们使用相同的方法将数据集分为训练和测试。</li><li id="43a0" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">创建<code class="fe ol om on oo b">RF</code>对象</li><li id="8c9c" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated"><strong class="lt iu">训练</strong>模型<strong class="lt iu">测试</strong>20%的数据</li><li id="bc32" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">打印<strong class="lt iu">分类报告</strong></li></ul><pre class="kj kk kl km gt ov oo ow ox aw oy bi"><span id="cac0" class="nq la it oo b gy oz pa l pb pc">##RF after feature selection<br/></span><span id="e5dc" class="nq la it oo b gy pd pa l pb pc">#Defining input and target variable while dropping unimportant #features<br/>x = df[['odor', 'gill-size', 'gill-color', 'spore-print-color']]<br/>y = df['class']</span><span id="39d3" class="nq la it oo b gy pd pa l pb pc">#Splitting dataset into training and testing<br/>X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 56)</span><span id="d171" class="nq la it oo b gy pd pa l pb pc">#Calling classifier<br/>RF = RandomForestClassifier()</span><span id="2b9e" class="nq la it oo b gy pd pa l pb pc">#Fitting classifier to data<br/>RF.fit(X_train, y_train)</span><span id="67f2" class="nq la it oo b gy pd pa l pb pc">#Predicting testing records<br/>predicted_RF = RF.predict(X_test)</span><span id="8d43" class="nq la it oo b gy pd pa l pb pc">#Printing classification prediction<br/>print(classification_report(y_test, predicted_RF))</span></pre><p id="6358" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">足够令人惊讶的是，该模型保持了与处理<strong class="lt iu"> 21特征</strong>时相同的精度，这一次仅通过<strong class="lt iu">使用4个</strong>。这向我们表明，尤其是当我们将模型投入生产时，模型所依赖的特性的数量应该限制在最重要的特性上。一般来说，更多的特性对应于部署计算成本更高的模型。</p><pre class="kj kk kl km gt ov oo ow ox aw oy bi"><span id="615a" class="nq la it oo b gy oz pa l pb pc">              precision    recall  f1-score   support<br/>      <br/>           0       1.00      1.00      1.00       841   <br/>           1       1.00      1.00      1.00       784</span><span id="c3c4" class="nq la it oo b gy pd pa l pb pc">    accuracy                           1.00      1625  <br/>   macro avg       1.00      1.00      1.00      1625<br/>weighted avg       1.00      1.00      1.00      1625</span></pre><h2 id="24d4" class="nq la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated">特征重要性:梯度推进分类器</h2><p id="d6a3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这个最后阶段，我们必须重复随机森林模型的过程，但使用不同的算法，梯度推进，在模型选择阶段表现最好。</p><ul class=""><li id="65e7" class="oc od it lt b lu mw lx mx ma oe me of mi og mm ou oi oj ok bi translated">在前一个单元中，我们重新定义了<code class="fe ol om on oo b">x</code>和<code class="fe ol om on oo b">y</code>，使它们只包含最重要的特性。我们需要重新分配所有数据集列，以评估模型的整体性能。</li><li id="4b45" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">数据集遵循通常的80/20分割比率，具有相同的<code class="fe ol om on oo b">random_state</code>。</li><li id="e51d" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">对象<code class="fe ol om on oo b">XGB</code>包含函数<code class="fe ol om on oo b">GradientBoostingClassifier()</code></li><li id="2bb8" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">使用命令<code class="fe ol om on oo b">fit</code>，分类器开始基于训练数据进行学习</li><li id="cbdb" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">命令<code class="fe ol om on oo b">predict()</code>执行测试阶段</li><li id="e611" class="oc od it lt b lu op lx oq ma or me os mi ot mm ou oi oj ok bi translated">我们最终可以打印分类报告来评估<strong class="lt iu">性能</strong></li></ul><pre class="kj kk kl km gt ov oo ow ox aw oy bi"><span id="49f5" class="nq la it oo b gy oz pa l pb pc">#Defining input and target variable<br/>x = df.drop('class', axis=1)<br/>y = df['class']</span><span id="44a8" class="nq la it oo b gy pd pa l pb pc">#Splitting dataset between training and testing with a 80-20 #allocation<br/>X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 56)</span><span id="2dcb" class="nq la it oo b gy pd pa l pb pc">#Calling classifier<br/>XGB = GradientBoostingClassifier()</span><span id="4cff" class="nq la it oo b gy pd pa l pb pc">#Fitting classifier to data<br/>XGB.fit(X_train, y_train)</span><span id="be44" class="nq la it oo b gy pd pa l pb pc">#Predicting testing records<br/>predicted_XGB = XGB.predict(X_test)</span><span id="393f" class="nq la it oo b gy pd pa l pb pc">#Printing classification report<br/>print(classification_report(y_test, predicted_XGB))</span></pre><p id="2827" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">当对所有特征进行训练时，<strong class="lt iu">梯度增强</strong>分类器报告与随机森林分类器相同的分数。在这个阶段，我们可以问自己以前的问题:</p><blockquote class="pf pg ph"><p id="68db" class="lr ls pi lt b lu mw ju lw lx mx jx lz pj my mc md pk mz mg mh pl na mk ml mm im bi translated">“有没有比其他更根本的特征？当我看到一个我想收集的蘑菇时，我需要特别注意什么？”</p></blockquote><p id="b939" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">两个不同的分类器可能通过使用最相关的不同特征来得到相同的结果。</p><pre class="kj kk kl km gt ov oo ow ox aw oy bi"><span id="9d60" class="nq la it oo b gy oz pa l pb pc">              precision    recall  f1-score   support<br/>      <br/>           0       1.00      1.00      1.00       841   <br/>           1       1.00      1.00      1.00       784</span><span id="f30f" class="nq la it oo b gy pd pa l pb pc">    accuracy                           1.00      1625  <br/>   macro avg       1.00      1.00      1.00      1625<br/>weighted avg       1.00      1.00      1.00      1625</span></pre><p id="5b16" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">获取梯度增强算法的特征重要性的命令对于随机森林模型是相同的:<code class="fe ol om on oo b">feature_importances_</code>。代码背后的逻辑和以前一样。它创建了两列，一列包含所有的特性名称，另一列包含所有的<strong class="lt iu">特性重要性分数</strong>。</p><pre class="kj kk kl km gt ov oo ow ox aw oy bi"><span id="d40d" class="nq la it oo b gy oz pa l pb pc">##XGB feature importance</span><span id="0be2" class="nq la it oo b gy pd pa l pb pc">#Calculating feature importances<br/>feature_importance=pd.DataFrame({'feature':list(X_train.columns),'feature_importance':[abs(i) for i in XGB.feature_importances_]})</span><span id="859d" class="nq la it oo b gy pd pa l pb pc">#Printing dataframe<br/>print(feature_importance.to_markdown())</span></pre><p id="74cc" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">你可能会注意到这些值非常相似，如果我们使用我们之前建立的<strong class="lt iu"> &gt; 0.1截止值</strong>规则，同样的四个特征被确定为最相关:<code class="fe ol om on oo b">odor</code>、<code class="fe ol om on oo b">gill-size</code>、<code class="fe ol om on oo b">gill-color</code>和<code class="fe ol om on oo b">spore-print-color</code>。</p><pre class="kj kk kl km gt ov oo ow ox aw oy bi"><span id="95e4" class="nq la it oo b gy oz pa l pb pc">|    | feature                  |   feature_importance |<br/>|---:|:-------------------------|---------------------:|<br/>|  0 | cap-shape                |           0.00458413 |<br/>|  1 | cap-surface              |           0.0146785  |<br/>|  2 | cap-color                |           0.0127857  |<br/>|  3 | bruises                  |           0.0573168  |<br/>|  4 | odor                     |           0.181369   |<br/>|  5 | gill-attachment          |           0.00358367 |<br/>|  6 | gill-spacing             |           0.0492609  |<br/>|  7 | gill-size                |           0.115759   |<br/>|  8 | gill-color               |           0.0958662  |<br/>|  9 | stalk-shape              |           0.0229765  |<br/>| 10 | stalk-root               |           0.0531794  |<br/>| 11 | stalk-surface-above-ring |           0.0495843  |<br/>| 12 | stalk-surface-below-ring |           0.0367454  |<br/>| 13 | stalk-color-above-ring   |           0.0128033  |<br/>| 14 | stalk-color-below-ring   |           0.0138009  |<br/>| 15 | veil-type                |           0          |<br/>| 16 | veil-color               |           0.00169852 |<br/>| 17 | ring-number              |           0.0116543  |<br/>| 18 | ring-type                |           0.0784959  |<br/>| 19 | spore-print-color        |           0.104922   |<br/>| 20 | population               |           0.0464761  |<br/>| 21 | habitat                  |           0.0324597  |</span></pre><p id="7630" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">我们已经为<strong class="lt iu">梯度增强</strong>模型确定了四个最相关的特征，所以让我们重复我们为随机森林所做的相同过程。</p><pre class="kj kk kl km gt ov oo ow ox aw oy bi"><span id="ce7a" class="nq la it oo b gy oz pa l pb pc">##XGB after feature selection</span><span id="d2b9" class="nq la it oo b gy pd pa l pb pc">#Defining input and target variable while dropping unimportant #features<br/>x = df[['odor', 'gill-size', 'gill-color', 'spore-print-color']]<br/>y = df['class']</span><span id="393b" class="nq la it oo b gy pd pa l pb pc">#Splitting dataset between training and testing with a 80-20 #allocation<br/>X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 56)</span><span id="7468" class="nq la it oo b gy pd pa l pb pc">#Calling classifier<br/>XGB = GradientBoostingClassifier()</span><span id="767c" class="nq la it oo b gy pd pa l pb pc">#Fitting classifier to data<br/>XGB.fit(X_train, y_train)</span><span id="2060" class="nq la it oo b gy pd pa l pb pc">#Predicting testing records<br/>predicted_XGB = XGB.predict(X_test)</span><span id="3463" class="nq la it oo b gy pd pa l pb pc">#Printing classification report<br/>print(classification_report(y_test, predicted_XGB))</span></pre><p id="678e" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">具有<strong class="lt iu"> 4个基本特征</strong>的更新型号比具有所有21个特征的型号稍差。差异是0.01，一个难以置信的小实体，这意味着大约有<strong class="lt iu"> 16个记录</strong>在超过1600个记录中被<strong class="lt iu">错误分类</strong>。</p><pre class="kj kk kl km gt ov oo ow ox aw oy bi"><span id="4f59" class="nq la it oo b gy oz pa l pb pc">              precision    recall  f1-score   support<br/>      <br/>           0       0.99      1.00      1.00       841   <br/>           1       1.00      0.99      1.00       784</span><span id="1d1d" class="nq la it oo b gy pd pa l pb pc">    accuracy                           1.00      1625  <br/>   macro avg       1.00      1.00      1.00      1625<br/>weighted avg       1.00      1.00      1.00      1625</span></pre><p id="1123" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">同样，结果与上一个极其接近。主要区别在于使用的特征数量，因此我们可以假设该模型在计算上应该<strong class="lt iu">更便宜</strong>，因此从ML操作的角度来看更好。</p><p id="0a65" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">跟踪所有不需要进行预测和运行模型的要素可以节省万亿字节的空间。在目前的设置中，这没有什么区别，但对于大型科技公司来说，取消一整列可以节省数百万美元。</p><h1 id="8246" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="a284" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">很明显，在有很多特性的情况下，选择几个是很重要的。我们能够通过在两种情况下保持相同的性能来降低<strong class="lt iu">计算成本</strong>。我们用较少的努力获得了类似的结果。</p><p id="1b9e" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">当然，这并非没有挑战，需要解决这些挑战以进一步改进。</p><p id="ea5f" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">首先，一些基本特征甚至连专家都很难识别<strong class="lt iu">。更具体地说，值得一提的是“<strong class="lt iu">气味</strong>”。你能认出“茴香”和“杂酚油”的区别吗？我真的不会。</strong></p><p id="ed83" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">最重要的是，我们不要忘记<strong class="lt iu">过度配合</strong>。所有模型都有令人难以置信的高总体精度分数，接近完美。明确建议将模型部署在相似数据集中的未知记录上，以进一步测试其可靠性。</p><p id="a7d8" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated"><strong class="lt iu">需要物种id</strong>来将模型的范围扩展到除了蘑菇和蘑菇以外的新蘑菇家族。</p><p id="20d4" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">我确实有一个<strong class="lt iu">最终免责声明</strong>给那些读完这篇文章后想尝试<strong class="lt iu">蘑菇狩猎</strong>的人。没有专家陪你一起采摘蘑菇是<strong class="lt iu">根本不推荐的</strong>。美国的一些州和一些国家要求有执照才能练习。除了法律，一个好的经验法则是不要碰它，除非你100%确定它可以食用。每种蘑菇对生态系统都有一定的贡献，所以如果不确定，最好让它留在原地！</p></div><div class="ab cl pm pn hx po" role="separator"><span class="pp bw bk pq pr ps"/><span class="pp bw bk pq pr ps"/><span class="pp bw bk pq pr"/></div><div class="im in io ip iq"><p id="24b2" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated"><em class="pi">最后，如果您喜欢该内容，请考虑添加一个关注，以便在新文章发布时得到通知。如果你对这篇文章有什么要考虑的，写在评论里吧！我很想读读它们:)谢谢你的阅读！</em></p><p id="7524" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated"><em class="pi"> PS:如果你喜欢我写的东西，如果你能通过</em> <a class="ae ky" href="https://giovanni-valdata.medium.com/membership" rel="noopener"> <em class="pi">这个链接</em> </a> <em class="pi">订阅一个中等会员，那对我来说就是全世界。有了会员资格，你就获得了媒体文章提供的惊人价值，这是支持我的内容的一种间接方式！</em></p></div><div class="ab cl pm pn hx po" role="separator"><span class="pp bw bk pq pr ps"/><span class="pp bw bk pq pr ps"/><span class="pp bw bk pq pr"/></div><div class="im in io ip iq"><p id="9252" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">参考</p><p id="0f2d" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">[1] UCI机器学习知识库:蘑菇数据集。(2022).检索于2022年9月13日，来自Uci.edu网站:<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/Mushroom" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/datasets/Mushroom</a></p><p id="0448" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">[2]兹恩，a .，克雷默，n .，索南堡，s .，和勒施，G. (2009年9月)。特征重要性排序度量。在<em class="pi">关于数据库中的机器学习和知识发现的欧洲联合会议</em>(第694–709页)。斯普林格，柏林，海德堡。</p><p id="858b" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">[3] Biau，g .，&amp; Scornet，E. (2016)。随机森林导游。<em class="pi">测试</em>，<em class="pi"> 25 </em> (2)，197–227。<a class="ae ky" href="https://doi.org/10.1007/s11749-016-0481-7" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1007/s11749-016-0481-7</a></p><p id="fed1" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">[4]j . h .弗里德曼(2002年)。随机梯度增强。<em class="pi">计算统计&amp;数据分析</em>，<em class="pi"> 38 </em> (4)，367–378。<a class="ae ky" href="https://doi.org/10.1016/s0167-9473(01)00065-2" rel="noopener ugc nofollow" target="_blank">https://doi . org/10.1016/s 0167-9473(01)00065-2</a></p></div></div>    
</body>
</html>