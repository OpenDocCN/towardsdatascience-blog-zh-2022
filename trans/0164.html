<html>
<head>
<title>Implementing Stochastic Depth/Drop Path In PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 PyTorch 中实现随机深度/落差路径</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-stochastic-depth-drop-path-in-pytorch-291498c4a974#2022-02-07">https://towardsdatascience.com/implementing-stochastic-depth-drop-path-in-pytorch-291498c4a974#2022-02-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/67a7b92b738780f3fb4377638e4938a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eHAYNVl2iIok6jFDSXtNOg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><div class=""/><div class=""><h2 id="99ff" class="pw-subtitle-paragraph kf jh ji bd b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw dk translated">DropPath 在<a class="ae kx" href="https://github.com/FrancescoSaverioZuppichini/glasses" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">眼镜</strong> </a>我的计算机视觉库中</h2></div><p id="4bdd" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">代码是<a class="ae kx" href="https://github.com/FrancescoSaverioZuppichini/DropPath" rel="noopener ugc nofollow" target="_blank">这里</a>本文的互动版本可以从<a class="ae kx" href="https://github.com/FrancescoSaverioZuppichini/DropPath/blob/main/README.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><h1 id="9422" class="lu lv ji bd lw lx ly lz ma mb mc md me ko mf kp mg kr mh ks mi ku mj kv mk ml bi translated">介绍</h1><p id="a5e1" class="pw-post-body-paragraph ky kz ji la b lb mm kj ld le mn km lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">今天我们将在 PyTorch 中实现随机深度，也称为 Drop Path！<a class="ae kx" href="https://arxiv.org/abs/1603.09382" rel="noopener ugc nofollow" target="_blank">黄高等人引入的随机深度</a>是一种在训练过程中“去激活”某些层的技术。我们将坚持使用<strong class="la jj"> DropPath </strong>。</p><p id="f601" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们来看一个使用剩余连接的普通 ResNet 块(像现在几乎所有的模型一样)。如果你不熟悉 ResNet，我有一篇<a class="ae kx" rel="noopener" target="_blank" href="/residual-network-implementing-resnet-a7da63c7b278">文章</a>展示了如何实现它。</p><p id="f254" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">基本上，块的输出被添加到它的输入:<code class="fe mr ms mt mu b">output = block(input) + input</code>。这被称为<strong class="la jj">剩余连接</strong></p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/2aa6bbfbe66bc395abe05ae8a98400c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*sWt-ZPo_nwhQHdAXhBfSvg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="1770" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里我们看到四个类似 ResnNet 的块，一个接一个。</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi na"><img src="../Images/4acaa43e4394b3a0082988bad4f6407d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l_vO9sYr7E0RqzcbSc5KRA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="f9bc" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">随机的深度/下落路径会降低一些石块的重量</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/67cc5859298d3ca9c7e9fefb1defcdc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Ii-W1brHZ1ZuWeqLM-WzQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><h1 id="b810" class="lu lv ji bd lw lx ly lz ma mb mc md me ko mf kp mg kr mh ks mi ku mj kv mk ml bi translated">履行</h1><p id="d685" class="pw-post-body-paragraph ky kz ji la b lb mm kj ld le mn km lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">让我们从导入我们最好的朋友<code class="fe mr ms mt mu b">torch</code>开始</p><pre class="mw mx my mz gt nc mu nd ne aw nf bi"><span id="a235" class="ng lv ji mu b gy nh ni l nj nk">import torch<br/>from torch import nn<br/>from torch import Tensor</span></pre><p id="e084" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以定义一个 4D 张量(<code class="fe mr ms mt mu b">batch x channels x height x width</code>)，在我们的例子中，让我们只发送 4 个图像，每个图像一个像素，这样更容易看到发生了什么:)</p><pre class="mw mx my mz gt nc mu nd ne aw nf bi"><span id="856f" class="ng lv ji mu b gy nh ni l nj nk">x = torch.ones((4, 1, 1, 1))</span></pre><p id="9b04" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们需要一个形状为<code class="fe mr ms mt mu b">batch x 1 x 1 x 1</code>的张量，使用一个给定的 prob 来将批处理中的一些元素设置为零。伯努利来救援了。</p><pre class="mw mx my mz gt nc mu nd ne aw nf bi"><span id="75d6" class="ng lv ji mu b gy nh ni l nj nk">keep_prob: float = .5<br/>mask: Tensor = x.new_empty(x.shape[0], 1, 1, 1).bernoulli_(keep_prob)</span><span id="f86e" class="ng lv ji mu b gy nl ni l nj nk"># mask = </span><span id="ffc4" class="ng lv ji mu b gy nl ni l nj nk">tensor([[[[1.]]],<br/><br/>        [[[1.]]],<br/><br/>        [[[0.]]],<br/><br/>        [[[0.]]]])</span></pre><p id="60f7" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Btw，这相当于</p><pre class="mw mx my mz gt nc mu nd ne aw nf bi"><span id="1432" class="ng lv ji mu b gy nh ni l nj nk">mask: Tensor = (torch.rand(x.shape[0], 1, 1, 1) &gt; keep_prob).float()</span></pre><p id="742c" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们想把<code class="fe mr ms mt mu b">x</code>的一些元素设置为零，因为我们的遮罩是由<code class="fe mr ms mt mu b">0</code>和<code class="fe mr ms mt mu b">1</code>组成的，我们可以把它乘以<code class="fe mr ms mt mu b">x</code>。在我们这样做之前，我们需要用<code class="fe mr ms mt mu b">x</code>除以<code class="fe mr ms mt mu b">keep_prob</code>来降低训练期间输入的激活，参见<a class="ae kx" href="https://cs231n.github.io/neural-networks-2/#reg" rel="noopener ugc nofollow" target="_blank"> cs231n </a>。因此</p><pre class="mw mx my mz gt nc mu nd ne aw nf bi"><span id="dd2f" class="ng lv ji mu b gy nh ni l nj nk">x_scaled : Tensor = x / keep_prob</span></pre><p id="7f8c" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后</p><pre class="mw mx my mz gt nc mu nd ne aw nf bi"><span id="930a" class="ng lv ji mu b gy nh ni l nj nk">output: Tensor = x_scaled * mask<br/># output =<br/>tensor([[[[2.]]],<br/><br/>        [[[0.]]],<br/><br/>        [[[2.]]],<br/><br/>        [[[2.]]]])</span></pre><p id="702c" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看看批处理中的一些元素是如何被设置为零的。我们可以把它放在一个函数中</p><pre class="mw mx my mz gt nc mu nd ne aw nf bi"><span id="a8f1" class="ng lv ji mu b gy nh ni l nj nk">def drop_path(x: Tensor, keep_prob: float = 1.0) -&gt; Tensor:<br/>    mask: Tensor = x.new_empty(x.shape[0], 1, 1, 1).bernoulli_(keep_prob)<br/>    x_scaled: Tensor = x / keep_prob<br/>    return x_scaled * mask<br/><br/>drop_path(x, keep_prob=0.5)</span></pre><p id="ed11" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们也可以就地做手术</p><pre class="mw mx my mz gt nc mu nd ne aw nf bi"><span id="eaa3" class="ng lv ji mu b gy nh ni l nj nk">def drop_path(x: Tensor, keep_prob: float = 1.0) -&gt; Tensor:<br/>    mask: Tensor = x.new_empty(x.shape[0], 1, 1, 1).bernoulli_(keep_prob)<br/>    x.div_(keep_prob)<br/>    x.mul_(mask)<br/>    return x<br/><br/><br/>drop_path(x, keep_prob=0.5)</span></pre><p id="49c9" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，我们可能想在其他地方使用<code class="fe mr ms mt mu b">x</code>，用<code class="fe mr ms mt mu b">x</code>或<code class="fe mr ms mt mu b">mask</code>除以<code class="fe mr ms mt mu b">keep_prob</code>也是一样的。让我们来看看最终的实现</p><pre class="mw mx my mz gt nc mu nd ne aw nf bi"><span id="45a1" class="ng lv ji mu b gy nh ni l nj nk">def drop_path(x: Tensor, keep_prob: float = 1.0, inplace: bool = False) -&gt; Tensor:<br/>    mask: Tensor = x.new_empty(x.shape[0], 1, 1, 1).bernoulli_(keep_prob)<br/>    mask.div_(keep_prob)<br/>    if inplace:<br/>        x.mul_(mask)<br/>    else:<br/>        x = x * mask<br/>    return x<br/><br/>x = torch.ones((4, 1, 1, 1))<br/>drop_path(x, keep_prob=0.5)</span></pre><p id="cd1c" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe mr ms mt mu b">drop_path</code>仅适用于 2d 数据，我们需要从输入的尺寸中自动计算出维数，以使其适用于任何数据时间</p><pre class="mw mx my mz gt nc mu nd ne aw nf bi"><span id="9ea3" class="ng lv ji mu b gy nh ni l nj nk">def drop_path(x: Tensor, keep_prob: float = 1.0, inplace: bool = False) -&gt; Tensor:<br/>    mask_shape: Tuple[int] = (x.shape[0],) + (1,) * (x.ndim - 1) <br/>    # remember tuples have the * operator -&gt; (1,) * 3 = (1,1,1)<br/>    mask: Tensor = x.new_empty(mask_shape).bernoulli_(keep_prob)<br/>    mask.div_(keep_prob)<br/>    if inplace:<br/>        x.mul_(mask)<br/>    else:<br/>        x = x * mask<br/>    return x</span></pre><p id="cdd7" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们创建一个漂亮的<code class="fe mr ms mt mu b">DropPath</code> <code class="fe mr ms mt mu b">nn.Module</code></p><pre class="mw mx my mz gt nc mu nd ne aw nf bi"><span id="068f" class="ng lv ji mu b gy nh ni l nj nk">class DropPath(nn.Module):<br/>    def __init__(self, p: float = 0.5, inplace: bool = False):<br/>        super().__init__()<br/>        self.p = p<br/>        self.inplace = inplace<br/><br/>    def forward(self, x: Tensor) -&gt; Tensor:<br/>        if self.training and self.p &gt; 0:<br/>            x = drop_path(x, self.p, self.inplace)<br/>        return x<br/><br/>    def __repr__(self):<br/>        return f"{self.__class__.__name__}(p={self.p})"</span></pre><h1 id="69db" class="lu lv ji bd lw lx ly lz ma mb mc md me ko mf kp mg kr mh ks mi ku mj kv mk ml bi translated">使用剩余连接</h1><p id="4c4e" class="pw-post-body-paragraph ky kz ji la b lb mm kj ld le mn km lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">我们有我们的<code class="fe mr ms mt mu b">DropPath</code>，酷！我们如何使用它？我们需要一个剩余块，我们可以使用一个经典的 ResNet 块:老朋友<code class="fe mr ms mt mu b">BottleNeckBlock</code></p><pre class="mw mx my mz gt nc mu nd ne aw nf bi"><span id="b9df" class="ng lv ji mu b gy nh ni l nj nk">from torch import nn<br/><br/><br/>class ConvBnAct(nn.Sequential):<br/>    def __init__(self, in_features: int, out_features: int, kernel_size=1):<br/>        super().__init__(<br/>            nn.Conv2d(in_features, out_features, kernel_size=kernel_size, padding=kernel_size // 2),<br/>            nn.BatchNorm2d(out_features),<br/>            nn.ReLU()<br/>        )<br/>         <br/><br/>class BottleNeck(nn.Module):<br/>    def __init__(self, in_features: int, out_features: int, reduction: int = 4):<br/>        super().__init__()<br/>        self.block = nn.Sequential(<br/>            # wide -&gt; narrow<br/>            ConvBnAct(in_features, out_features // reduction, kernel_size=1),<br/>            # narrow -&gt; narrow<br/>            ConvBnAct( out_features // reduction, out_features // reduction, kernel_size=3),<br/>            # wide -&gt; narrow<br/>            ConvBnAct( out_features // reduction, out_features, kernel_size=1),<br/>        )<br/>        # I am lazy, no shortcut etc<br/>        <br/>    def forward(self, x: Tensor) -&gt; Tensor:<br/>        res = x<br/>        x = self.block(x)<br/>        return x + res<br/>    <br/>    <br/>BottleNeck(64, 64)(torch.ones((1,64, 28, 28)))</span></pre><p id="09c3" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要停用程序块，操作<code class="fe mr ms mt mu b">x + res</code>必须等于<code class="fe mr ms mt mu b">res</code>，因此我们的<code class="fe mr ms mt mu b">DropPath</code>必须在程序块之后应用。</p><pre class="mw mx my mz gt nc mu nd ne aw nf bi"><span id="8b04" class="ng lv ji mu b gy nh ni l nj nk">class BottleNeck(nn.Module):<br/>    def __init__(self, in_features: int, out_features: int, reduction: int = 4):<br/>        super().__init__()<br/>        self.block = nn.Sequential(<br/>            # wide -&gt; narrow<br/>            ConvBnAct(in_features, out_features // reduction, kernel_size=1),<br/>            # narrow -&gt; narrow<br/>            ConvBnAct( out_features // reduction, out_features // reduction, kernel_size=3),<br/>            # wide -&gt; narrow<br/>            ConvBnAct( out_features // reduction, out_features, kernel_size=1),<br/>        )<br/>        # I am lazy, no shortcut etc<br/>        self.drop_path = DropPath()<br/>        <br/>    def forward(self, x: Tensor) -&gt; Tensor:<br/>        res = x<br/>        x = self.block(x)<br/>        x = self.drop_path(x)<br/>        return x + res<br/>    <br/>BottleNeck(64, 64)(torch.ones((1,64, 28, 28)))</span></pre><p id="7b7a" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Tada！现在，随机地，我们的<code class="fe mr ms mt mu b">.block</code>将被完全跳过！</p><h1 id="3c98" class="lu lv ji bd lw lx ly lz ma mb mc md me ko mf kp mg kr mh ks mi ku mj kv mk ml bi translated">结论</h1><p id="8192" class="pw-post-body-paragraph ky kz ji la b lb mm kj ld le mn km lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">在本文中，我们看到了如何实现 DropPath 并在 residual 块中使用它。希望，当你阅读/看到下落路径/随机深度时，你知道它是如何制作的</p><p id="442b" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">保重:)</p><p id="aff8" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">弗朗西斯科</p></div></div>    
</body>
</html>