<html>
<head>
<title>Jump with deep learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用深度学习跳跃</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/jump-with-deep-learning-4d2fedb4a61#2022-02-04">https://towardsdatascience.com/jump-with-deep-learning-4d2fedb4a61#2022-02-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7dfb" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">捕捉跳跃不连续性的深度学习模型及其在偏微分方程中的应用实例</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/39f05cab7e16c86ccfb8fd90d53a80a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*o0zeEvHW4Gfj3Al1NVnHnA.gif"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片:跳跃中断</p></figure><p id="6fe0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当所有单侧极限都存在且有限，但相等或不相等时，就形成了函数中的跳跃不连续性，通常在分段函数中会遇到这种情况，因为分段函数的不同区域由不同的连续函数定义。正如万有逼近定理所说，在人工神经网络的数学理论中，一个单隐层包含有限个神经元的前馈网络，在对激活函数的温和假设下，可以逼近有限维实空间中紧致子集上的连续函数。然而，对于深度神经网络来说，学习具有跳跃不连续性的函数是困难的，因为值跳跃点周围的误差通常很大。</p><p id="70c9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在本文中，我将展示如何构建一个特定的模块来捕获跳跃不连续性，并使用网络来处理偏微分方程(PDE)问题。</p><h1 id="5bfa" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">神经网络不能很好地跳跃</h1><h2 id="9723" class="mf lo iq bd lp mg mh dn lt mi mj dp lx la mk ml lz le mm mn mb li mo mp md mq bi translated">经典网络</h2><p id="e4bb" class="pw-post-body-paragraph kr ks iq kt b ku mr jr kw kx ms ju kz la mt lc ld le mu lg lh li mv lk ll lm ij bi translated">让我们从一个只有一个跳跃不连续点的简单分段函数开始:当x为正时，函数等于sin(x ),否则等于sin(x ),如本文开头所示。</p><p id="c465" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们首先构建一个简单的经典神经网络，它包含三个密集层，这样两个隐藏层都有32个单元。输出层具有足够简单的线性激活函数。下图说明了该模型的架构。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/cd8bdfb35105ad69c4620e20bcb0dae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*DZUJuwBP61J8FIdjV12dJg.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片:三层模型</p></figure><p id="2171" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在20个训练时期之后，我们观察到当函数接近跳跃不连续点时，模型学习差得多，并且所学习的函数甚至不形成跳跃。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/0d0b726931bd3cb093f3d1bd70aab610.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*cFzl1x070s47Y2GXNWSPuw.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片:原始功能与习得功能</p></figure><h2 id="5789" class="mf lo iq bd lp mg mh dn lt mi mj dp lx la mk ml lz le mm mn mb li mo mp md mq bi translated">具有亥维赛活化的网络</h2><p id="9d4e" class="pw-post-body-paragraph kr ks iq kt b ku mr jr kw kx ms ju kz la mt lc ld le mu lg lh li mv lk ll lm ij bi translated">结果并不令人惊讶:所有层的激活函数都是连续的，它们使原始曲线规则化。因此，一个自然的想法是将输出层的激活函数设置为Heaviside函数。</p><p id="5077" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><a class="ae my" href="https://en.wikipedia.org/wiki/Heaviside_step_function" rel="noopener ugc nofollow" target="_blank">亥维赛函数</a>，也称单位阶跃函数，通常用H或θ表示，是一个阶跃函数，其零为负变元，一为正变元。我们可以用下面的方法用Keras定义Heaviside函数。注意，由于亥维赛函数的导数是<a class="ae my" href="https://en.wikipedia.org/wiki/Dirac_delta_function" rel="noopener ugc nofollow" target="_blank">狄拉克分布</a>，它不是在任何地方都定义的，我们简单地将梯度设为0。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="7e02" class="mf lo iq na b gy ne nf l ng nh">@tf.custom_gradient</span><span id="96ea" class="mf lo iq na b gy ni nf l ng nh">def heaviside(x):<br/>    ones = tf.ones(tf.shape(x), dtype=x.dtype.base_dtype)<br/>    zeros = tf.zeros(tf.shape(x), dtype=x.dtype.base_dtype)<br/>    def grad(dy):<br/>        return dy*0 <br/>    return keras.backend.switch(x &gt; 0.5, ones, zeros), grad</span></pre><p id="11c7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们现在建立一个与之前结构相同的模型，唯一的变化是输出层的激活函数被设置为我们刚刚定义的亥维赛函数。在20个训练时期之后，我们观察到该模型确实捕获了跳跃不连续性，但是根本没有能力预测平滑函数部分。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/9ee054f7b6c023e1f8f9b4ba482f90b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*3aXSsJi9RTLUD5aHQjaPiw.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图片由作者提供:原始功能与具有亥维赛激活的学习功能</p></figure><p id="862e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这种行为的原因来自于这样一个事实，即损失函数的极小值是通过计算梯度来搜索的，而Heaviside函数的导数是狄拉克分布，其值不能在实轴上的任何地方定义。由于基于梯度的方法未能达到期望的最小值，学习的函数停留在小平台上。</p><h1 id="3cb7" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">建立“良好的跳跃模型”</h1><p id="8b64" class="pw-post-body-paragraph kr ks iq kt b ku mr jr kw kx ms ju kz la mt lc ld le mu lg lh li mv lk ll lm ij bi translated">为了获得具有良好梯度的最小值并同时捕捉跳跃不连续性，我们现在试图构建一个“井跳跃模型”,该模型利用了经典方法和具有亥维赛激活的方法。其思想是建立一个输出线性函数和Heaviside函数的线性组合的层，如下式:output = w_s * S(input)+w_h * H(input)+b，其中S和H是线性函数，Heaviside函数，w _ S，w _ H，b是可训练参数。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="1e56" class="mf lo iq na b gy ne nf l ng nh">import keras</span><span id="0208" class="mf lo iq na b gy ni nf l ng nh">class Linear(keras.layers.Layer):<br/>      def __init__(self, units=32, input_dim=32):<br/>          super(Linear, self).__init__()<br/>          self.w = self.add_weight(shape=(input_dim, units),         initializer="random_normal", trainable=True)<br/>          self.b = self.add_weight(shape=(units,), initializer="zeros", trainable=True)</span><span id="2659" class="mf lo iq na b gy ni nf l ng nh">      def call(self, inputs):<br/>          return tf.matmul(inputs, self.w) + self.b</span><span id="0e22" class="mf lo iq na b gy ni nf l ng nh"><br/>class JumpBlock(keras.layers.Layer):<br/>      def __init__(self):<br/>          super(JumpBlock, self).__init__()<br/>          self.linear0 = Linear(1,1)<br/>          self.linear1 = Linear(1,1)<br/>          self.linear2 = Linear(1,1)<br/>          self.linear2.b.trainbale=False<br/>     <br/>      def call(self,inputs):<br/>          x=self.linear0(inputs)<br/>          h= self.linear1(heaviside(x))<br/>          s=self.linear2(x)<br/>          return h+s</span></pre><p id="2d44" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们将跳转块作为输出层添加到网络中。下图显示了“良好跳转模型”的架构:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/ce03664614dcf689429f52632d9bd861.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*DU9yNm_RV_n7VAxbH-vU8w.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><p id="b58d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们以“井跃模型”的好结果来结束这一节，它成功地捕捉到了给定函数的平滑部分和跳跃不连续性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/8199b16e8ce348a47753530add021854.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*Me1IDHNtxESJ4TpOfUdsBA.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片:“井跃模型”的结果</p></figure><p id="37d8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">关于代码的更多细节，请查看<a class="ae my" href="https://colab.research.google.com/drive/1MN3Ou6pQf9FWXXcQknHeZFkA9gQ0IHYf" rel="noopener ugc nofollow" target="_blank">笔记本</a>。</p><h1 id="cd56" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">双曲偏微分方程的应用</h1><p id="d8c0" class="pw-post-body-paragraph kr ks iq kt b ku mr jr kw kx ms ju kz la mt lc ld le mu lg lh li mv lk ll lm ij bi translated">我现在想把“好的跳跃模型”应用到一个双曲型偏微分方程中。为了简化这个故事，让我们考虑一个足够简单但不是试探性的方程:具有简单形式的<a class="ae my" href="https://en.wikipedia.org/wiki/Burgers%27_equation" rel="noopener ugc nofollow" target="_blank">无粘伯格斯方程</a>:u _ t+u * u _ x = 0。我们总是对初始问题感兴趣:对于给定的函数，找到一个满足方程并且初始等于函数的解。一般来说，即使一个足够光滑的初始条件也能形成激波解，作为跳跃间断处理。</p><p id="022e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这里，让我们考虑一个时间序列作为Burgers方程的解，该解是通过从时间0到时间1的<a class="ae my" href="https://www.cambridge.org/core/books/abs/systems-of-conservation-laws-1/glimm-scheme/C6641E20714641A3A1FB226FFC2586E7" rel="noopener ugc nofollow" target="_blank"> Glimm方案</a>离散化成500个时间步长而获得的，并且我们想使用该时间序列来预测时间经过1时的解。为此，我们使用一个结合了<a class="ae my" href="https://github.com/philipperemy/keras-tcn" rel="noopener ugc nofollow" target="_blank">时间卷积网络</a> (TCN)和我们的“良好跳跃模型”的网络。这个想法是让TCN捕捉解的时间演化，让“好的跳跃模型”强制冲击波的形成。</p><p id="39d2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">模型的输入是20个时间步内的解，输出是下一个时间步的解。这里的简短说明是，我们使用的损失函数是均方误差，因为Burgers方程通常在弱意义上满足，即不是在所有地方都满足，因此我们必须使用L2范数。我们对模型进行了100个时期的训练，并获得了e-4的验证损失。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="660c" class="mf lo iq na b gy ne nf l ng nh">from tcn import TCN<br/>from tensorflow.keras.layers import Densefrom tensorflow.keras.models import Sequential</span><span id="580e" class="mf lo iq na b gy ni nf l ng nh">tcn_layer = TCN(input_shape=(time_step,space_size))</span><span id="6087" class="mf lo iq na b gy ni nf l ng nh">m=Sequential()<br/>m.add(tcn_layer)<br/>m.add(Dense(32))<br/>m.add(Dense(32))<br/>m.add(Dense(space_size))<br/>m.add(JumpBlock())<br/>m.compile(optimizer='adam', loss='mse')</span><span id="f6a9" class="mf lo iq na b gy ni nf l ng nh">x_train=x[:400]<br/>y_train=y[:400]<br/>x_val=x[400:]<br/>y_val=y[400:]</span><span id="5739" class="mf lo iq na b gy ni nf l ng nh">m.fit(x_train, y_train, epochs=100, validation_data=(x_val,y_val))</span></pre><p id="5948" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">瞧，这就是模型在时间=1.2时预测的伯格斯方程的解。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/5d62e4c5ef26ad7e2d951d94ef3d4b5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*qECztgzl62C22nNtonfGMg.png"/></div></figure><h1 id="6f1e" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">后续步骤</h1><p id="b36c" class="pw-post-body-paragraph kr ks iq kt b ku mr jr kw kx ms ju kz la mt lc ld le mu lg lh li mv lk ll lm ij bi translated">在本文中，我们构建了一个跳转块，它结合了线性函数和Heaviside函数来学习具有跳转不连续性的函数。然后，我们使用Burgers方程中的块来预测更长时间的解，并得到或多或少令人满意的结果。然而，对于解的预测，仅使用数据作为数值解，而忽略了方程本身的结构。一种想法是将这样的块与<a class="ae my" href="https://en.wikipedia.org/wiki/Physics-informed_neural_networks" rel="noopener ugc nofollow" target="_blank">物理通知神经网络</a> (PINN)的逻辑相结合，这是一种深度学习方法，用于解决往往无法单独形成冲击波的PDEs。</p></div></div>    
</body>
</html>