<html>
<head>
<title>Learning Semantics-Enriched Representation via Self-discovery, Self-Classification, and Self-Restoration: A Summary</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过自我发现、自我分类和自我恢复学习语义丰富的表征:综述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-semantics-enriched-representation-via-self-discovery-self-classification-and-ec0b7a08e566#2022-09-17">https://towardsdatascience.com/learning-semantics-enriched-representation-via-self-discovery-self-classification-and-ec0b7a08e566#2022-09-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a55c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用新的迁移学习技术预训练深度学习模型，在稀缺的医学图像数据集上获得更好的结果</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5b61281f5deee809f6a51edb5c3c1465.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bLaNTtIRg62eFBm_"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">乔纳森·博尔巴在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="fed6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将机器学习和深度学习模型应用于医学成像任务的主要问题之一是<strong class="ky ir">缺乏足够的数据</strong>来训练模型。医学图像的手动生成和标记成本高且耗时，因为需要训练有素的专家来正确理解和标记医学图像。为了解决计算机视觉中缺乏数据的问题，通常使用迁移学习技术，例如预训练和微调，其中首先用另一个域(通常是有大量训练数据的域)中的数据训练模型，然后将这个预训练的模型微调到有少量标记数据的域。在大量未标记数据可用的情况下<strong class="ky ir">通常使用自我监督学习</strong>技术，利用训练数据中的有用信息对这些未标记图像的模型进行预训练。要了解更多关于自我监督学习与其他培训模式的不同之处，请参考这篇由Louis Bouchard<a class="ls lt ep" href="https://medium.com/u/f34bfe2bbaec?source=post_page-----ec0b7a08e566--------------------------------" rel="noopener" target="_blank">撰写的</a><a class="ae kv" href="https://medium.com/what-is-artificial-intelligence/what-is-self-supervised-learning-will-machines-be-able-to-learn-like-humans-d9160f40cdd1" rel="noopener">文章</a>。</p><p id="1c12" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">任何<strong class="ky ir">自监督</strong>学习算法中的一个重要步骤是确定<strong class="ky ir">学习信号</strong>和<strong class="ky ir">可用于模型训练的数据</strong>的属性。当数据是图像时，诸如彩色化[1，2]，拼图[3，4]，旋转[5，6]和许多其他技术被用于从未标记的数据预训练模型。彩色化技术通常试图从图像的灰度对应物预测图像的颜色属性。拼图技术破坏图像并训练网络来恢复原始图像。旋转技术试图预测图像旋转。</p><p id="3956" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管这些自监督技术对于自然图像工作得很好，但是，对于从医学数据集进行预训练来说，这些不是最佳的技术。医学数据具有<strong class="ky ir">重复的解剖模式</strong>，其也可被用作预训练模型的学习信号。这篇<a class="ae kv" href="https://arxiv.org/pdf/2007.06959.pdfhttps://arxiv.org/pdf/2007.06959.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>【8】介绍了一种自我监督的预训练方法，该方法利用医学图像中的重复模式来学习更适合各种医学成像任务的预训练模型。图1显示了医学图像中循环模式的一个例子。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lu"><img src="../Images/b22ca14e8832d6999977f92f04db3995.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ECeOJksco-QaQsaq4Gjl0g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图一。医学图像中的循环模式。来源<a class="ae kv" href="https://arxiv.org/pdf/2007.06959.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>。</p></figure><p id="d85c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文[8]中利用循环解剖模式的自我监督技术介绍了三个步骤，即:相似患者中解剖模式的自我发现<strong class="ky ir">、已学习解剖模式的自我分类</strong><strong class="ky ir"/>和转换模式的自我恢复<strong class="ky ir">。</strong>这个模型整体被称为<strong class="ky ir">语义生成</strong>。仅使用自我恢复模块而不进行自我分类和自我发现是同一研究组的早期论文之一，被称为<strong class="ky ir">Models Genesis【7】。</strong></p><p id="4001" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">自分类模块帮助模型学习图像的<strong class="ky ir">语义</strong>，自恢复模块帮助模型学习数据的<strong class="ky ir">视觉属性</strong>，如外观、纹理、几何形状等。接下来，我们将逐一介绍这些步骤。</p><p id="2f01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">自我发现— </strong>该步骤的目标是<strong class="ky ir">从未标记的图像中识别</strong>重复的解剖模式。这主要包括3个步骤——</p><ol class=""><li id="767b" class="lv lw iq ky b kz la lc ld lf lx lj ly ln lz lr ma mb mc md bi translated">用未标记的图像训练<strong class="ky ir">自动编码器</strong>。要了解更多关于自动编码器的信息，请参考由<a class="ls lt ep" href="https://medium.com/u/b89dbc0712c4?source=post_page-----ec0b7a08e566--------------------------------" rel="noopener" target="_blank"> Matthew Stewart </a>撰写的这篇综合<a class="ae kv" rel="noopener" target="_blank" href="/generating-images-with-autoencoders-77fd3a8dd368">文章</a>。图像的潜在表示被用作图像的标识符，这意味着对于未来的步骤，我们使用学习到的图像的潜在表示，而不是原始图像。</li><li id="f730" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">随机选择一个<strong class="ky ir">参考图像</strong>，然后在潜在空间中找到与参考图像最近的k个图像(距离是在图像的潜在表示上测量的，而不是在原始图像上)。<strong class="ky ir">注- </strong> k是一个超参数，本文中使用的值的选择在实验部分讨论。</li><li id="69d5" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">在所有这些相似的图像中随机选择n个点，然后裁剪一个补丁。将伪标签分配给补丁。这些补丁包含在步骤2中发现的相似图像中的重复模式。补片数和伪标签数(C)是另一个超参数，论文中使用的值在实验部分提到。</li></ol><p id="5a68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在自我发现过程的最后，我们有一个分配了伪标签的补丁集合，可能在每个补丁中捕获一些有用的解剖模式。图2显示了完整的自我发现过程。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mj"><img src="../Images/48a1c130f271d78bd2cc22c90e7b7323.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d3q-o2620giRg3qCK0ZQcw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图二。来源<a class="ae kv" href="https://arxiv.org/pdf/2007.06959.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>。</p></figure><p id="1b86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">自分类— </strong>该步骤<strong class="ky ir"> </strong>利用自发现步骤后获得的<strong class="ky ir">标记的小块</strong>来训练多类分类器以正确预测伪标签。分类器有一个类似编码器的网络，后面是一个完全连接的层。<strong class="ky ir">编码器与接下来讨论的自恢复步骤共享。</strong>想法是通过训练分类器来预测在自我发现步骤中发现的重现解剖模式的正确伪标签，模型的学习权重存储关于图像中这些<strong class="ky ir">语义结构</strong>的信息。</p><p id="3d8c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">自恢复— </strong>该步骤首先用某些变换来修改图像(稍后将讨论这些变换)，然后尝试使用编码器-解码器网络从变换后的图像重建原始图像。训练模型来重建原始图像有助于学习各种视觉表示。</p><p id="b552" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">编码器与自我分类步骤中使用的编码器相同。自分类和自恢复网络以多任务学习格式一起训练。图3显示了自我分类和自我恢复模块。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mk"><img src="../Images/8d04deaf75d0cae7f86c5682bc9ab185.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LwcX9GdeaeRsa-qP_BH0aQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3。自我分类和自我恢复模块。注意，编码器对于两个模块是公共的，并且转换仅仅是为了自恢复而进行的。来源<a class="ae kv" href="https://arxiv.org/pdf/2007.06959.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>。</p></figure><p id="a123" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由模型学习的<strong class="ky ir">视觉属性</strong>取决于在恢复之前对图像进行的变换的类型。本文讨论了4种类型的变换— <strong class="ky ir">非线性、局部像素混洗、出画和入画</strong>。</p><p id="624c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">通过<strong class="ky ir">非线性变换学习外观</strong>—</strong>本文使用贝塞尔曲线(<a class="ae kv" href="https://www.youtube.com/watch?v=aVwxzDHniEw" rel="noopener ugc nofollow" target="_blank">视频解说</a>)作为非线性变换，为每个像素赋予一个唯一的值。原始图像的恢复教导网络关于器官外观，因为医学图像中的强度值给出了对器官结构的洞察。</p><p id="50dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">通过<strong class="ky ir">局部像素重排</strong>学习局部边界和纹理</strong>——局部像素重排涉及从一个面片随机选择窗口中重排像素顺序，以获得一个变换的面片。选择窗口的大小，使得图像的全局内容不变。从这个变换中的恢复学习了图像的局部边界和纹理。</p><p id="fa58" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">通过<strong class="ky ir">外绘和内绘</strong>学习上下文</strong>t——在外绘和内绘中，通过<strong class="ky ir">将不同尺寸和纵横比的</strong>窗口叠加在彼此之上，获得复杂形状的单个窗口。</p><p id="f18c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">外绘</strong> —在窗口外分配随机像素，同时保留窗口内像素的原始亮度。画外复原学<strong class="ky ir">全局几何</strong>和<strong class="ky ir">空间布局</strong>。</p><p id="78a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> In-painting </strong> —保留窗口外的原始亮度，并替换内部像素的亮度值。器官的局部连续性是在修复过程中从内嵌图像中学习的。</p><p id="689b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图4显示了应用于CT图像的每个变换的可视化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ml"><img src="../Images/9595c45308ff4eea93f5b32ef8b69be3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VlD6fjhuLjmK00YwB7-AxA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4。在3D CT图像上完成的变换。来源<a class="ae kv" href="https://arxiv.org/pdf/2007.06959.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>。</p></figure><p id="8e63" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">训练— </strong>在多任务学习范例中，涉及自我分类和自我恢复模块的整个模型被一起训练。这实质上意味着用于训练整个模型的损失函数是自分类(分类交叉熵损失)和自恢复(重建损失)模块的损失函数的<strong class="ky ir">加权和</strong>。个体损失函数的权重是经验学习的超参数。</p><p id="1e82" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">微调和模型重用</strong> —在使用自我发现、自我分类和自我恢复来训练模型之后，模型的不同组件可以被重用并针对目标任务域进行微调。<strong class="ky ir">对于图像分类任务，模型的编码器被重用。对于图像分割任务，编码器和解码器都被重用。</strong></p><h1 id="e571" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated">实验</h1><p id="5651" class="pw-post-body-paragraph kw kx iq ky b kz ne jr lb lc nf ju le lf ng lh li lj nh ll lm ln ni lp lq lr ij bi translated">基于目标图像模态，在两个不同的数据集上训练该模型。公开可用的CT扫描用于3D图像模态，X射线用于2D图像模态。</p><p id="0f90" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">由623个CT扫描组成的训练数据集</strong>—<a class="ae kv" href="https://luna16.grand-challenge.org/Data/" rel="noopener ugc nofollow" target="_blank">LUNA 2016</a>【9】(<a class="ae kv" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank">Creative Commons Attribution 4.0国际许可</a>)和由75708个X射线图像组成的<a class="ae kv" href="https://www.med.upenn.edu/cbica/brats2019/data.html" rel="noopener ugc nofollow" target="_blank">胸部X射线14</a>【10】(<a class="ae kv" href="https://creativecommons.org/publicdomain/zero/1.0/" rel="noopener ugc nofollow" target="_blank">CC0:公共领域</a>)用于训练语义发生模型。</p><p id="50bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">超参数— </strong></p><ul class=""><li id="65ce" class="lv lw iq ky b kz la lc ld lf lx lj ly ln lz lr nj mb mc md bi translated"><strong class="ky ir">对于自我发现，选择前k个相似患者。对于2D/3D情况，k根据经验设置为200/1000。</strong></li><li id="0e3c" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr nj mb mc md bi translated">对于3D/2D图像，c(伪标签的数量)被设置为44/100，以覆盖整个图像，同时避免重叠。</li></ul><p id="a6f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">基线</strong> —在所有实验中，模型都在六个公开的医学成像应用程序上进行评估，涵盖分类和分割。图5显示了用于评估模型的不同任务。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/b0dc31fe4065cb26429cdabe1cbb4ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CjweccL6ESaHdfv_wahrSg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5。用于评估的数据集。来源<a class="ae kv" href="https://arxiv.org/pdf/2007.06959.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>。</p></figure><p id="65e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">评测/微调数据集-</strong><a class="ae kv" href="https://luna16.grand-challenge.org/Data/" rel="noopener ugc nofollow" target="_blank">LUNA-2016</a>【9】(<a class="ae kv" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank">知识共享署名4.0国际许可</a>)<a class="ae kv" href="https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI" rel="noopener ugc nofollow" target="_blank">LIDC-IDRI</a>【16】(<a class="ae kv" href="https://creativecommons.org/licenses/by/3.0/" rel="noopener ugc nofollow" target="_blank">知识共享署名3.0未署名许可</a>)<a class="ae kv" href="https://competitions.codalab.org/competitions/17094" rel="noopener ugc nofollow" target="_blank">LiTS-2017</a>【17】(<a class="ae kv" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" rel="noopener ugc nofollow" target="_blank">署名-非商业性-非商业性-非专有4.0国际</a>)<a class="ae kv" href="https://www.med.upenn.edu/cbica/brats2019/data.html" rel="noopener ugc nofollow" target="_blank">brats 20</a></p><p id="f7cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">3D迁移学习的预训练3D模型</strong> — NiftyNet[11]，MedicalNet[12]，Models Genesis[7]，Inflated 3D[13]。</p><p id="5a63" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">预训练的自我监督学习</strong> —图像嵌入绘画【14】，补丁洗牌【15】，模型生成【7】。</p><h1 id="2848" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated">结果</h1><ol class=""><li id="8bf2" class="lv lw iq ky b kz ne lc nf lf nl lj nm ln nn lr ma mb mc md bi translated"><strong class="ky ir">将自我分类和自我恢复添加到现有的自我监督学习方法中</strong></li></ol><p id="0a17" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图6比较了在现有的自我监督学习方法上添加语义(自我恢复+自我分类)的结果，所述自我监督学习方法包括修补[14]、修补洗牌[15]和模型生成[7]。注— <strong class="ky ir"> Models Genesis </strong>是同一个研究组的论文，只涉及<strong class="ky ir">自我恢复</strong>模块，没有自我发现和自我分类模块。</p><p id="af31" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">实验在3个不同的领域进行(NCC——CT图像上的肺结节分类，LCS——CT图像上的肝脏分割，BMS——MRI图像上的脑肿瘤分割)。在现有的自我监督学习技术的基础上添加语义导致了这3个领域的改进。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/348add2aa1375b02c3acf8754c596e42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3EJpww9plS3Py2gg_fVVgw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图6。来源<a class="ae kv" href="https://arxiv.org/pdf/2007.06959.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>。</p></figure><p id="0d40" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.<strong class="ky ir">将语义Genesis 3D与预训练的3D模型进行比较— </strong>该实验将语义Genesis与其他预训练(监督和自监督)的3D模型进行比较。对涉及3D图像(CT和MRI图像)的6个任务中的4个任务的结果(图7)进行了评估。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/06764ac3956b809d3b0960a1c9e79f98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Irm7DM6VsrbGFtJyrFTJOw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图7。来源<a class="ae kv" href="https://arxiv.org/pdf/2007.06959.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>。</p></figure><p id="fdff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.<strong class="ky ir">自我分类和自我恢复模块的比较— </strong>将自我恢复和自我分类分别与组合语义生成方法进行比较。结果(图7)显示了两个重要的结论。首先，在四个不同任务中的三个任务中，自我恢复和自我分类的组合优于单个组件。第二，自我分类在一些任务中表现更好，而自我恢复在其他任务中表现更好，这表明它们学习<strong class="ky ir">互补特征</strong>，将它们加在一起比单独使用它们中的每一个导致学习额外的特征。</p><p id="4678" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">4.<strong class="ky ir">语义成因3D与基于2D切片的方法的比较</strong>—3D成像模式中的任务通常在2D重新制定和解决。该实验比较了语义Genesis 3D和基于2D切片的方法。在两种3D成像模式中评估结果(NCC-CT上的肺结节检测，NCS-CT图像上的肺结节分割)。结果(图8中的前两个结果)显示语义Genesis 3D优于其他基于2D切片的方法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/f7b72590c319c46d8ed019fb3afbcf72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nd7rgkqLExeTJDsvSW-akw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图8。来源<a class="ae kv" href="https://arxiv.org/pdf/2007.06959.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>。</p></figure><p id="e373" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">5.<strong class="ky ir">语义成因2D与其他预训练2D模型的比较</strong> —比较是在两个医学成像任务(<strong class="ky ir">PXS</strong>—x射线图像上的气胸分割，<strong class="ky ir">DXC</strong>—x射线图像上的胸部疾病分类)包括2D x射线图像，以及两个3D医学成像任务(<strong class="ky ir"> NCC和NCS </strong>)上进行的。结果(图8)表明Semantic genesis在PXS中的性能优于ImageNet，在NCC和NCS中的性能与ImageNet相当。</p><h1 id="ddbe" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated">结论</h1><p id="712c" class="pw-post-body-paragraph kw kx iq ky b kz ne jr lb lc nf ju le lf ng lh li lj nh ll lm ln ni lp lq lr ij bi translated">本文提供了一种模型和训练算法来学习医学成像任务的更好的表示和更好的预训练模型，这些模型可以针对不同的医学图像领域进行微调，以应对医学应用任务中的数据稀缺问题。该论文设计了模型以利用医学图像中重复出现的解剖模式，并在自我监督的训练范例中利用它们。我觉得这个想法和结果非常有前途，可以用作医学分类/分割任务的预训练方法，尽管与公开可用的预训练图像净重相比，实现起来更加耗时和复杂。</p><p id="68d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在下面的网址找到这篇论文的官方GitHub实现——<a class="ae kv" href="https://github.com/fhaghighi/SemanticGenesis" rel="noopener ugc nofollow" target="_blank">https://github.com/fhaghighi/SemanticGenesis</a>。</p><p id="1b1e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这篇文章对你有所帮助和启发。我写过的其他论文总结<a class="ae kv" rel="noopener" target="_blank" href="/towards-controlled-generation-of-text-a-summary-7f4c954c1fad">这里</a>和<a class="ae kv" href="https://pub.towardsai.net/disentangled-representation-learning-for-non-parallel-text-style-transfer-paper-summary-aa862bc46349" rel="noopener ugc nofollow" target="_blank">这里</a>你都可以找到。</p><p id="4516" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请关注我的个人资料，以获得我未来文章的通知。</p><h1 id="1b2b" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated">参考</h1><ol class=""><li id="1878" class="lv lw iq ky b kz ne lc nf lf nl lj nm ln nn lr ma mb mc md bi translated">Larsson，g .，Maire，m .，Shakhnarovich，g .:自动着色的学习表示。年:欧洲计算机视觉会议。第577-593页。施普林格(2016)2。拉尔森、迈尔、沙赫纳罗维奇；</li><li id="bb5a" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">彩色化作为视觉理解的代理任务。IEEE计算机视觉和模式识别会议论文集。第6874至6883页(2017年)</li><li id="1a60" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">Kim，d .，Cho，d .，Yoo，d .，Kweon，I.S .:通过完成损坏的拼图学习图像表示。In: 2018年IEEE计算机视觉应用冬季会议(WACV)。第793-802页。电气和电子工程师协会(2018)</li><li id="2355" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">诺鲁齐，m .，法瓦罗，p .:通过解决七巧板视觉表征的无监督学习。年:欧洲计算机视觉会议。第69-84页。施普林格(2016年)</li><li id="835b" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">冯，钟，徐，c，陶，d:基于旋转特征解耦的自监督表示学习。IEEE计算机视觉和模式识别会议论文集。第10364至10374页(2019年)</li><li id="22ce" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">通过预测图像旋转的无监督表示学习。arXiv预印本arXiv:1803.07728 (2018)</li><li id="5266" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">Z.Zhou，V. Sodha，M. M. Rahman Siddiquee，R. Feng，N. Tajbakhsh，M. B. Gotway，和J. Liang，“模型生成:用于3d医学图像分析的通用自学模型”，医学图像计算和计算机辅助干预MICCAI 2019年。湛:施普林格国际出版公司，2019年，第384–393页。</li><li id="41d7" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">F.、M. R. Hosseinzadeh、Z. Zhou、M. B. Gotway和J. Liang，“通过自我发现、自我分类和自我恢复学习语义丰富的表示”，医学图像计算和计算机辅助干预MICCAI 2020。湛:施普林格国际出版公司，2020年，第137-147页。</li><li id="fe28" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">Setio，A.A.A .，Traverso，a .，De Bel，t .，Berens，M.S .，van den Bogaard，c .，Cerello，p .，Chen，h .，Dou，q .，Fantacci，M.E .，Geurts，b .，等人:计算机断层摄影图像中肺结节自动检测算法的验证、比较和组合:luna16挑战。医学图像分析42，1–13(2017)</li><li id="a8d7" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">王，x，彭，y，陆，l，陆，z，Bagheri，m，Summers，R.M.: Chestx-ray8:医院级胸部x线数据库和常见胸部疾病的弱监督分类和定位基准。IEEE计算机视觉和模式识别会议论文集。第2097至2106页(2017年)</li><li id="d7f2" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">Gibson，e .，Li，w .，Sudre，c .，Fidon，l .，Shakir，D.I .，Wang，g .，Eaton-Rosen，z .，Gray，r .，Doel，t .，Hu，y .，等人:Niftynet:一个用于医学成像的深度学习平台。生物医学中的计算机方法和程序158，113–122(2018)</li><li id="3b27" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">陈，s，马，k，郑，Y.: Med3d:用于三维医学图像分析的迁移学习。arXiv预印本arXiv:1904.00625 (2019)</li><li id="4617" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">卡雷拉，j .，齐塞曼，A.: Quo vadis，动作识别？一个新的模型和动力学数据集。IEEE计算机视觉和模式识别会议论文集。第6299至6308页(2017年)</li><li id="164b" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">Pathak，d .、Krahenbuhl，p .、Donahue，j .、Darrell，t .、Efros，A.A .:上下文编码器:通过修补进行特征学习。IEEE计算机视觉和模式识别会议论文集。第2536至2544页(2016年)</li><li id="a42f" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">Chen，l .，Bentley，p .，Mori，k .，Misawa，k .，m .，Rueckert，d .:使用图像上下文恢复进行医学图像分析的自我监督学习。医学图像分析58，101539 (2019)</li><li id="b6f7" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">Armato III，S.G .，g .，Bidaut，l .，McNitt-Gray，M.F .，Meyer，C.R .，Reeves，A.P .，Zhao，b .，Aberle，D.R .，Henschke，C.I .，Hoffman，E.A .等人:肺部图像数据库联盟(lidc)和图像数据库资源倡议(idri):一个完整的ct扫描肺结节参考数据库。医学物理学38(2)，915–931(2011)</li><li id="d5e9" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">Bilic，p .，Christ，P.F .，Vorontsov，e .，Chlebus，g .，Chen，h .，Dou，q .，Fu，C.W .，Han，x .，Heng，P.A .，Hesser，j .等人:肝肿瘤分割基准(lits)。arXiv预印本arXiv:1901.04056 (2019)</li><li id="d789" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">Bakas，s .，Reyes，m .，Jakab，a .，Bauer，s .，Rempfler，m .，克里米，a .，蛯原姫奈，R.T .，Berger，c .，Ha，S.M .，Rozycki，m .，等人:在brats挑战中识别用于脑肿瘤分割、进展评估和总体存活率预测的最佳机器学习算法。arXiv预印本arXiv:1811.02629 (2018)</li><li id="1ed3" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">Siim-acr气胸分割(2019)，<a class="ae kv" href="https://www.kaggle.com/competitions/siim-acr-pneumothorax-segmentation/data" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/ siim-ACR气胸分割/ </a></li></ol></div></div>    
</body>
</html>