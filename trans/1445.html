<html>
<head>
<title>Policy Gradients In Reinforcement Learning Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习中的策略梯度解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/policy-gradients-in-reinforcement-learning-explained-ecec7df94245#2022-04-09">https://towardsdatascience.com/policy-gradients-in-reinforcement-learning-explained-ecec7df94245#2022-04-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f4d8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解所有基于似然比的策略梯度算法(强化):直觉、推导、对数技巧，以及高斯和softmax策略的更新规则。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/eccb2cc6b0d2df0dfab56f2a48d437ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vNWzB_LDgEQf1DxF"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">斯科特·韦伯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="6405" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">当我第一次研究政策梯度算法时，我并不觉得它们特别容易理解。直觉上，它们看起来足够简单——<strong class="li iu">采取行动，观察奖励，调整政策</strong>——但在最初的想法之后，是许多冗长的推导、我早已忘记的微积分技巧和大量的注释。在某一点上，它只是变成了一个模糊的概率分布和梯度。</p><p id="2790" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在这篇文章中，我试图一步一步地解释这个概念，包括关键的思维过程和数学运算。诚然，这是一个有点长的阅读，并需要一定的强化学习(RL)的初步知识，但希望它揭示了一些政策梯度背后的想法。重点是<strong class="li iu">似然比策略梯度</strong>，这是诸如增强/普通策略梯度等经典算法的基础。</p><p id="878f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">考虑到篇幅，我们先来构建这篇文章:</p><ol class=""><li id="6c82" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb mh mi mj mk bi translated"><em class="ml">数值逼近:学习确定性策略</em></li><li id="c4a7" class="mc md it li b lj mm lm mn lp mo lt mp lx mq mb mh mi mj mk bi translated"><em class="ml">数值逼近:学习确定性策略</em></li><li id="b0c3" class="mc md it li b lj mm lm mn lp mo lt mp lx mq mb mh mi mj mk bi translated"><em class="ml">策略近似方法:转向随机策略</em></li><li id="b9d6" class="mc md it li b lj mm lm mn lp mo lt mp lx mq mb mh mi mj mk bi translated"><em class="ml">建立目标函数</em></li><li id="5380" class="mc md it li b lj mm lm mn lp mo lt mp lx mq mb mh mi mj mk bi translated"><em class="ml">定义轨迹概率</em></li><li id="b32b" class="mc md it li b lj mm lm mn lp mo lt mp lx mq mb mh mi mj mk bi translated"><em class="ml">推导政策梯度</em></li><li id="370e" class="mc md it li b lj mm lm mn lp mo lt mp lx mq mb mh mi mj mk bi translated"><em class="ml">对数概率函数的梯度</em></li><li id="dd7d" class="mc md it li b lj mm lm mn lp mo lt mp lx mq mb mh mi mj mk bi translated"><em class="ml">近似梯度</em></li><li id="c232" class="mc md it li b lj mm lm mn lp mo lt mp lx mq mb mh mi mj mk bi translated"><em class="ml">定义更新规则</em></li><li id="7cfe" class="mc md it li b lj mm lm mn lp mo lt mp lx mq mb mh mi mj mk bi translated"><em class="ml">示例:Softmax和高斯策略</em></li><li id="e77e" class="mc md it li b lj mm lm mn lp mo lt mp lx mq mb mh mi mj mk bi translated"><em class="ml">损失函数和自动梯度计算</em></li><li id="0a8a" class="mc md it li b lj mm lm mn lp mo lt mp lx mq mb mh mi mj mk bi translated"><em class="ml">算法实现(加强)</em></li></ol><h1 id="678d" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">一.价值近似值:学习确定性政策</h1><p id="1893" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">RL的目标是学习一个好的决策政策π，使回报随时间最大化。虽然(确定性)策略π的概念一开始看起来有点抽象，但它只是一个基于问题状态<em class="ml"> s </em>、<em class="ml"> π :s→a </em>返回动作<em class="ml"> a </em>的函数。</p><p id="b685" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果你对RL有一些经验，你可能从<strong class="li iu">值近似值</strong>开始。这类RL保持接近动态编程范式，旨在<em class="ml">近似</em>值函数——反映下游回报的<a class="ae ky" rel="noopener" target="_blank" href="/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e"> Q值</a>——而不是递归求解<a class="ae ky" rel="noopener" target="_blank" href="/why-reinforcement-learning-doesnt-need-bellman-s-equation-c9c2e51a0b7">贝尔曼方程</a>达到最优。</p><p id="d364" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对于值近似值方法，确定性策略工作得很好。通常，我们用概率1-ϵ选择最佳行动(给定我们的q值),用概率ϵ选择随机行动，允许对新行动进行一些探索。我们比较r(t)+Q_t+1和Q_t，并使用观察到的误差来改进价值函数。整个概念非常接近贝尔曼的最优条件。</p><p id="b0a9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="ml">想了解更多关于RL各种类的内容？看看这篇文章:</em></p><div class="no np gp gr nq nr"><a rel="noopener follow" target="_blank" href="/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd iu gy z fp nw fr fs nx fu fw is bi translated">强化学习的四个策略类别</h2><div class="ny l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">towardsdatascience.com</p></div></div><div class="nz l"><div class="oa l ob oc od nz oe ks nr"/></div></div></a></div><h1 id="18df" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">二。策略近似方法:转向随机策略</h1><p id="c624" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">在策略近似方法中，我们省略了学习值函数的概念，而是直接<strong class="li iu">调整策略</strong>。我们<strong class="li iu">用一组参数θ对策略</strong>进行参数化——例如，这些参数可以是神经网络权重——并调整θ以改进策略π_θ <em class="ml">。</em></p><p id="c6da" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这听起来很合理，但是我们如何评估一个给定政策的质量呢？我们如何更新θ？没有将相应的性能与其他东西进行对比的能力，就没有办法分辨。像ϵ-greedy的价值逼近方法一样，我们需要一些<strong class="li iu">探索机制</strong>。</p><p id="afdd" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">有许多策略近似方法(例如，遗传算法、爬山法)，但是<strong class="li iu">策略梯度</strong>由于其效率而被使用得最多。政策梯度算法有多种形式(例如，<em class="ml">有限差分法</em>对θ添加小扰动并测量差异)，但本文仅关注<strong class="li iu">似然比政策梯度</strong>。</p><p id="42e1" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">核心思想是用一个参数化的概率分布π_θ(a | s)= P(a | s；θ).我们<strong class="li iu">从θ调整的概率分布</strong>中抽取动作，而不是返回单个动作。</p><p id="4803" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">随机策略可能看起来不方便，但它为优化策略提供了基础。我们将很快进入数学领域，但是让我们从一个手动的直觉开始。各种政策样本使我们能够对比与某些行动相关的奖励。我们用这些样本来改变θ，<strong class="li iu">增加获得高回报的概率</strong>。这就是似然比梯度政策的本质。</p><h1 id="82f2" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">三。建立目标函数</h1><p id="ae82" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">当在一个连续的决策过程中移动时，我们遵循一个状态-行动<em class="ml">轨迹τ=</em>(S1，a1，…，s_T，a_T)。通过对行为进行抽样，策略影响了我们观察时间范围内每个可能的<strong class="li iu">状态和行为序列的概率</strong>。每一条轨迹都有一个对应的概率P(τ)和一个累积奖励R(τ)=∑γ^t R_t(奖励R_t的序列用γ折现)。</p><blockquote class="of og oh"><p id="dc2f" class="lg lh ml li b lj lk ju ll lm ln jx lo oi lq lr ls oj lu lv lw ok ly lz ma mb im bi translated">为了简单起见(不是必须的)，我们假设一组有限的轨迹，这样我们可以对概率求和而不是积分。</p></blockquote><p id="78a3" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">有了这些要素，我们就可以正式确定我们的<strong class="li iu">目标</strong>，即随着时间的推移使预期回报最大化。这里，τ~π_θ形式化了占优策略下的轨迹分布。同样，我们可以对所有轨迹概率求和，并乘以相应的回报。</p><p id="3b1b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">目标函数</strong> J(θ)如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/8ff202242939ecc4067f71376ae16a5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XzFFgr7yAqW8qaeMyM05Vw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">似然比策略梯度法的目标函数。由于抽样提供了对期望值的无偏估计，我们可以使用模拟来近似估计。</p></figure><p id="ddb6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">相应的<strong class="li iu">最大化问题</strong>表示为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/4a600919096191184afeca025277fde0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bdPitGihNA8_y0eoDBOPDg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">政策近似的最大化问题。通过调整θ，我们的目标是增加遵循产生高回报的轨迹τ的概率。</p></figure><h1 id="5052" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">四。定义轨迹概率</h1><p id="ffd5" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">从最大化问题来看，调整θ显然会影响轨迹概率。下一个问题是:<strong class="li iu">如何计算概率P(τ；θ)?</strong>回想一下，这个客观项受政策π_θ的影响。通过增加高回报轨迹的概率，我们提高了预期回报。</p><blockquote class="of og oh"><p id="9f21" class="lg lh ml li b lj lk ju ll lm ln jx lo oi lq lr ls oj lu lv lw ok ly lz ma mb im bi translated">如果你想要更多关于这部分的理论背景，值得阅读<strong class="li iu">似然比方法、评分函数</strong>和<strong class="li iu">重要性抽样</strong>。对于本文来说，当前的详细程度已经足够了。</p></blockquote><p id="6c48" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在完全确定的环境中，我们可以计算每个策略π_θ产生的轨迹，并找到产生最高累积回报的策略。然而，大多数RL问题不完全是确定性的，而是具有(实质上的)随机成分。因此，轨迹概率受政策的影响，但不完全由政策决定。给定我们的政策，我们计算某个回报轨迹发生的<strong class="li iu">概率。</strong></p><p id="3f4a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">总而言之，我们处理两种概率分布:</p><ul class=""><li id="5b99" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb on mi mj mk bi translated"><strong class="li iu">保单本身就是一个概率分布</strong>π_θ(a | s)= P(a | s；θ) .该策略规定了在给定状态下选择每个动作的概率，并且取决于参数设置θ。</li><li id="1487" class="mc md it li b lj mm lm mn lp mo lt mp lx mq mb on mi mj mk bi translated">一个<strong class="li iu">转移概率分布</strong> P(s_t+1|s_t，a_t)描述了环境中的状态转移。注意，这个概率分布部分受π_θ(行动选择)影响，部分受外源信息影响。</li></ul><p id="be39" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">有了这些信息，让我们试着计算在策略π_θ(a|s)下轨迹τ出现的概率。每一时间步<strong class="li iu">将采样动作π_θ(a_t|s_t)的概率</strong>乘以转移概率P(s_t+1|s_t，a_t)。随后，我们将每个时间步长的这些概率相乘，以找到完整轨迹实现的概率:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/4c015e7e9e6358a055b04de076644947.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*EDGD_PfiaqbxL0ZoLR5gOw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">轨迹概率是动作概率(由策略π_θ决定)和状态转移概率(由转移函数P(s_t+1)决定)的乘积。</p></figure><p id="be07" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">麻烦的部分是转移函数P(s_t+1)。这就是环境的模型，往好里说是工作复杂，往坏里说是完全未知。</p><p id="ce43" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这个模型的另一个缺点是，它是概率的产品。对于很长的时间范围和很小的概率，轨迹概率变得非常小。由于计算机语言只提供有限精度的浮点数，这导致了数值的不稳定性。</p><p id="4f29" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们以后再担心这些事情，首先重新考虑我们实际上旨在优化的函数。</p><h1 id="81b3" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">动词 （verb的缩写）引入政策梯度</h1><p id="058f" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">如前所述，我们寻求最大化我们的预期回报J(θ)。如何优化该函数，例如，确定使目标函数最大化的参数θ？</p><p id="cdc4" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">现在我们已经做了一些有益的观察。通过采用<em class="ml">随机</em>策略，我们对产生不同轨迹τ的各种行为进行采样，使我们能够看到哪些行为产生最佳回报。换句话说，<strong class="li iu">采样观测为策略参数θ提供了更新方向</strong>。</p><p id="8b87" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">是时候具体化这个“方向”了。高中的时候，我们学过如何取函数f(x)关于x的<strong class="li iu">导数</strong> δf/δx向其最大值(或最小值)移动。如果导数很大，就意味着斜率很陡——我们可能离最优值很远。如果它是0，我们降落在局部最优。我们可以对J(θ)做同样的事情，求目标函数J对θ的导数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/929f5e1cb2d74c47b10bb4cdcc96b08a.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/1*kos4NYG_bZKUdEPKO4O4_g.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用切线的导数斜率动画[图片来自<a class="ae ky" href="https://en.wikipedia.org/wiki/Derivative#/media/File:Tangent_animation.gif" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="bd8b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">一个<strong class="li iu">梯度</strong>概括了导数的概念，仅仅是偏导数的一个<strong class="li iu">向量。由于θ通常是一个向量θ=[θ_1，θ_2，…θ_N]，我们可以看到如何计算每个参数的偏导数(例如，δJ(θ)/δ θ_1):</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/07ae90e47b5df8933210dcd1c600e042.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*681-NlWCKsfvRxSBc6fj2A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">参数化累积报酬函数J(θ)的梯度。梯度是向量θ中每个参数θ_n的偏导数的向量。</p></figure><p id="2042" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为了计算梯度，我们必须能够对函数<strong class="li iu">J(θ)求导。我们看到改变πθ(a | s)会影响轨迹概率P(τ；θ).然而，我们还没有解决<em class="ml">如何</em>计算这些轨迹概率；记住我们可能甚至不知道转移函数P(s_t+1|s_t，a_t)！</strong></p><p id="42eb" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">一些数学的时间到了。</p><h1 id="0866" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">不及物动词推导政策梯度</h1><p id="1ccf" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">我们需要检索目标函数的显式梯度。让我们一步一步来。我们从预期回报的梯度开始:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/d2f91db1e60e627036c50080c2306f82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*UkAYzshWOMwF7B_ITVQk_A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第一步:表示为预期报酬的梯度</p></figure><p id="2cbf" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如前所述，我们可以将其改写为所有轨迹概率的总和<strong class="li iu">乘以轨迹回报:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/877d02929bd9b2c0229cebdd2d70f077.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*Cn6NXeQOtTFDmIHx9QA8dQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">步骤2:表示为概率加权回报轨迹的梯度</p></figure><p id="788b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">总和的<em class="ml">梯度等于<strong class="li iu">梯度总和</strong>，因此我们可以在总和内移动梯度。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/f5b6e6a8cb3b33bd52bfed04304fe08f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*PGSI-8N8SfPuIFaXYOPUEw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">步骤3:重写为渐变总和</p></figure><p id="856d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">下一步需要更多的关注。让我们回顾一下对梯度政策定理至关重要的一个恒等式——对数导数技巧。</p><blockquote class="of og oh"><p id="cd1a" class="lg lh ml li b lj lk ju ll lm ln jx lo oi lq lr ls oj lu lv lw ok ly lz ma mb im bi translated"><strong class="li iu">对数求导技巧</strong></p><p id="0854" class="lg lh ml li b lj lk ju ll lm ln jx lo oi lq lr ls oj lu lv lw ok ly lz ma mb im bi translated">数学中一个常见的技巧是将一个表达式乘以1，这显然不会改变它。在这种情况下，我们乘以<em class="it">P(τ；θ)/P(τ；θ) </em>，这样表达式变成:</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/deff44e1908126a63290e8b40cb43bd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LS1NOyh8ftIC7D7orqIeig.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">步骤4a:将表达式乘以P(τ；θ)/P(τ；θ)(等于1)</p></figure><blockquote class="of og oh"><p id="6533" class="lg lh ml li b lj lk ju ll lm ln jx lo oi lq lr ls oj lu lv lw ok ly lz ma mb im bi translated">我们可以重新排列成:</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/96b54bed6792d145733dc445f3fc5a29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nldbEwh2IZQaeQ6VnOY7AA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">步骤4b:重新排列表达式</p></figure><blockquote class="of og oh"><p id="babd" class="lg lh ml li b lj lk ju ll lm ln jx lo oi lq lr ls oj lu lv lw ok ly lz ma mb im bi translated">并随后:</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/71367c77af39d6d98c9275e9d6d90d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WmrMQg03-KJz-8lHZPCSpg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">步骤4c:再次重新排列表达式</p></figure><blockquote class="of og oh"><p id="3b35" class="lg lh ml li b lj lk ju ll lm ln jx lo oi lq lr ls oj lu lv lw ok ly lz ma mb im bi translated">何必呢？嗯，这是因为我们现在可以应用下面的恒等式:</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/1b7ef7120dbf3d14e18aad9cd0330ba2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*SVIn83uu_mNvGBCi5KQDGg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">日志标识。这个至关重要的结果——也称为似然比或得分函数——有助于轨迹概率函数的关键转换。</p></figure><blockquote class="of og oh"><p id="ce31" class="lg lh ml li b lj lk ju ll lm ln jx lo oi lq lr ls oj lu lv lw ok ly lz ma mb im bi translated">这个恒等式是整个定理的基石。其实，<strong class="li iu">这个结果就是似然比</strong>。</p><p id="ea50" class="lg lh ml li b lj lk ju ll lm ln jx lo oi lq lr ls oj lu lv lw ok ly lz ma mb im bi translated">我将不再详细解释——本文中已经有足够的数学知识了——但是可以使用(I)对数函数的导数和(ii)链式法则来重建恒等式。</p></blockquote><p id="4c3e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">使用对数导数技巧，我们将表达式改写为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/fa89f1c37f6535c8a33d1be94e13f591.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xoo9hjbgkQO1P4w10JirrA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">步骤4:使用对数导数技巧重写</p></figure><p id="e901" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这个表达是个好消息，原因有很多！我们现在将把自己限制在一个原因上，那就是项∑_θ(P(τ；θ).为什么这是好消息？在对数变换之前，我们对概率的<em class="ml">梯度</em>求和，现在我们对概率本身求和。有了这个结果，我们可以<strong class="li iu">把我们的梯度改写成期望值。</strong>最后一步变成了</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/a00e9ee64d6ca7f99c3f581ee462d0f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Snx8r7wSqtlFS9FEDqEjw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第五步:按照预期重写</p></figure><p id="b7bf" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">值得停下来评估一下我们的成就。我们已经知道我们可以把目标函数<em class="ml">写成期望值</em>，但是现在我们发现<strong class="li iu">也可以把梯度本身写成期望值</strong>。这一点对于应用RL等基于采样的方法至关重要。</p><p id="92d2" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">不幸的是，我们仍然不知道如何处理P(τ；θ)，(即如何实际计算梯度)但我们很快就会到达那里。</p><h1 id="a291" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">七。对数概率函数的梯度</h1><p id="62dd" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">还记得我们之前纠结轨迹概率函数吗，因为它需要环境的显式模型(P(s_t+1|s_t，a_t)部分)？原来我们已经解决了使用日志技巧！</p><p id="7bd9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们写出梯度部分∇_θ对数p(τ；θ).首先，我们简单地用P(τ；θ)我们之前已经确定:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/e09f9e3e37e0cde5623d35e341ca59eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Gd5zBIydhBg9hvlOL5OLQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">步骤1:用轨迹概率代替显式表达式</p></figure><p id="a7d9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们可以重写为<em class="ml">对数</em>概率，而不是取整个概率乘积的对数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/45ccbc42210c7ba8d770a2c141a8f00e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vhrea02br6DPoLo6LUyPyA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第二步:使用对数概率重写</p></figure><p id="4e49" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对数概率的一个方便的特性是它们是相加的而不是相乘的。对于数值稳定性来说，这是非常令人愉快的(考虑到浮点数的有限精度)。它还有助于防止通常困扰RL算法的爆炸/消失梯度。</p><p id="b5e5" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">然而，主要的结果是我们有效地<strong class="li iu">将转移函数</strong> P(s_t+1) <strong class="li iu">与策略</strong> π_θ(a_t)解耦。因为梯度是相对于θ取的，而P(s_t+1|s_t，a_t)不依赖于θ，所以我们简单地取它！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/af4c1e6ed99ae2d1b422abfb77ebb3ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zet9TjevFVHVxGdIMvvxdw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">步骤3a:移除状态转换功能。因为它与第二项分开，不依赖于θ，所以对梯度没有影响。</p></figure><p id="1ba6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">产生的结果看起来更清晰，我们的梯度只取决于政策:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/cb5976dafe499927a50c8f0a79ef9b62.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*V1EEPSnmbw66HJuvuehPEg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">步骤3b:移除过渡函数后的梯度</p></figure><p id="d56e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">最后一步，我们在求和中引入梯度符号(<em class="ml">‘和的梯度=梯度的和’</em>)，这样我们可以计算每个时间步长的梯度(例如，对于单个动作)。最终结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/a7922c4e5c5a80bf599f70d814e000a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*lLQHDXhRXqYZFaIjzDp5kg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">步骤4:表示为梯度和</p></figure><h1 id="50d9" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">八。近似梯度</h1><p id="fbb2" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">我们快到了。然而，到目前为止的计算涉及到真实的预期，包括所有可能的轨迹。典型的RL问题在计算上是难以处理的(否则我们可以只应用动态编程)，所以我们需要使用轨迹的<strong class="li iu">样本</strong>。</p><p id="a3c9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">事实上，我们基于有限数量的样本来计算一个<strong class="li iu">近似梯度</strong>。幸运的是，我们已经建立了梯度也是一个期望值，我们可以用模拟来估计。表达式看起来是这样的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/1af026995809ce579898e391892ddf3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*iF_3tcB_7DvQ7EoLHUwN-w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基于有限数量的轨迹样本m的梯度近似</p></figure><p id="0d11" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">正如你所看到的，这个表达式是完全容易处理的。我们可以对轨迹和相应的回报进行采样，即使我们不知道环境模型。我们所需要的是一个明确定义的政策，可微分的w.r.t. θ。</p><p id="2210" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为了简单起见，我将在本文的剩余部分继续讨论<em class="ml">渐变</em>，但是请记住，我们实际上使用的是<em class="ml">近似</em> <em class="ml">渐变</em>。</p><h1 id="3211" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">九。定义更新规则</h1><p id="6a69" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">一旦我们计算了(近似的)梯度，我们如何应用它来更新我们的策略？</p><p id="b13a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">基于采样观察，我们希望<strong class="li iu">逐渐更新策略参数</strong>。为此，我们定义了一个学习率α∈(0，1)，表示放在计算的梯度上的权重，以更新现有的参数向量。相应的更新规则如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/0f452ca259da52230dadd302edcc7084.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*JGlxC5Spfo04JlYDxGtgPg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">策略梯度更新规则。新的政策参数是旧参数和目标函数梯度的加权组合。</p></figure><p id="473e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">通常，更新规则由θ应改变的增量表示:δθ=α∇_θj(θ)。</p><p id="9e33" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">记住梯度只是偏导数的向量。因此，更新规则为θ中的每个元素提供了唯一的权重更新。通过这种方式，更新与单个功能的影响保持一致。</p><h1 id="6b10" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">X.示例:Softmax和Gaussian策略</h1><p id="a0dc" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">我们已经达成了一个明确的更新规则，但如果你对所有的曲折感到有点茫然，这是可以理解的。</p><p id="ccf8" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们看看是否可以将我们的抽象结果具体化，为<strong class="li iu">离散和连续动作空间提供规范的策略。</strong></p><p id="f923" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">一些符号。设ϕ(s,a)是基函数的向量(例如，从状态动作对导出的解释变量的向量)。如前所述，θ是相应的一组参数，可以解释为每个变量的权重。我们假设一个简单的线性函数ϕ(s,a)^⊤⋅θ——注意，我们可以很容易地用θ参数化的神经网络来代替它。</p><p id="f819" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对于离散动作，最常用的是<strong class="li iu"> softmax策略</strong>。其定义如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/9d0232ffdda55624f394bb981d8b2e9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4yuOYS48VDYgjheO0a8fZA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Softmax政策。这个策略通常用于离散的动作空间</p></figure><p id="5429" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">其梯度(此处<a class="ae ky" href="https://math.stackexchange.com/questions/2013050/log-of-softmax-function-derivative" rel="noopener ugc nofollow" target="_blank">的推导</a>)如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/e01ccab6919edf051bf0e5caaf5e7276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tVeBzTXZLrxkOQZKUoZDRA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">softmax策略的梯度</p></figure><p id="a29a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">结果可以解释为所有动作的<em class="ml">观察到的</em>特征向量(来自采样动作)减去<em class="ml">预期的</em>特征向量。因此，如果奖励信号很高，并且观察向量与预期向量相差很大，则提供了更新该动作的概率的强烈动机。</p><p id="a15d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对于连续动作空间，高斯策略最常见。这里，我们从参数化的正态分布中绘制动作。分布的平均值由μ_θ=ϕ(s,a)^⊤⋅θ.表示</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/4c235fa0cd9399bc66ff9d821b472c8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*m1wgXFEhx7ZF0gfkQCNW9Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">高斯政策。这个策略通常用于连续动作空间</p></figure><p id="151f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">相应的梯度:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/f6c7fda8f4d5e3ae996095c602a479ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*445IVB-d-BVWplcuPpvy6g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">高斯政策的梯度</p></figure><p id="e37b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">同样在这里，可以看出，在高回报的情况下，远离平均值的行为触发了强烈的更新信号。由于概率总和必须为1，增加某些轨迹的概率意味着减少其他轨迹的概率。因此，调整政策会影响预期回报。</p><h1 id="68f5" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">XI。损失函数和自动梯度计算</h1><p id="fcda" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">尽管策略需要是可微分的，并且梯度可以使用微积分来计算，但是手动计算偏导数是相当麻烦的。特别是当策略是深度神经网络时——其中θ表示网络权重——我们通常依赖于<strong class="li iu">自动梯度计算</strong>。</p><p id="d45f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对于自动梯度，我们只需定义一个<strong class="li iu">损失函数</strong>并让计算机求解所有导数。损失函数有效地代表了更新信号。我们添加一个负号(因为训练依赖于梯度<em class="ml">下降</em>而不是- <em class="ml">上升</em>)，并定义规范损失函数如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/5df7d5958f8104ff9b9a73f0069c3717.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mLabr8YlQ-1gMGf2zYZh2Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">策略梯度算法的损失函数。大多数实现都提供了自动微分，这样就可以计算出梯度。</p></figure><h1 id="45fd" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">十二。算法实现(加强)</h1><p id="4125" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">本文提供的信息解释了似然比策略梯度方法的背景，如Williams的经典<strong class="li iu">加强算法</strong>。</p><p id="50d3" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们把这些放在一起:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pk"><img src="../Images/60f69a5a59e435577d3b99663464ae81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-FOeTBdZkezT5ICEXmBs1w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">加强算法，也称为普通策略梯度或似然比策略梯度[图片由作者提供，基于Williams (1992)]</p></figure><p id="1ac7" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">虽然这需要一些数学知识，但实际的算法是简洁明了的。我们所需要的只是抽样奖励和我们政策的梯度。</p><p id="63d1" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu"> Python实现</strong>(例如，使用TensorFlow定义的策略网络)也不需要太多代码——查看我下面的文章中关于连续和离散变体的例子。</p><div class="no np gp gr nq nr"><a rel="noopener follow" target="_blank" href="/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd iu gy z fp nw fr fs nx fu fw is bi translated">TensorFlow 2.0中离散策略梯度的最小工作示例</h2><div class="pl l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">一个训练离散演员网络的多兵种土匪例子。在梯度胶带功能的帮助下…</h3></div><div class="ny l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">towardsdatascience.com</p></div></div><div class="nz l"><div class="pm l ob oc od nz oe ks nr"/></div></div></a></div><div class="no np gp gr nq nr"><a rel="noopener follow" target="_blank" href="/a-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd iu gy z fp nw fr fs nx fu fw is bi translated">TensorFlow 2.0中连续策略梯度的最小工作示例</h2><div class="pl l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">一个简单的训练高斯演员网络的例子。定义自定义损失函数并应用梯度胶带…</h3></div><div class="ny l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">towardsdatascience.com</p></div></div><div class="nz l"><div class="pn l ob oc od nz oe ks nr"/></div></div></a></div><div class="no np gp gr nq nr"><a rel="noopener follow" target="_blank" href="/cliff-walking-problem-with-the-discrete-policy-gradient-algorithm-59d1900d80d8"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd iu gy z fp nw fr fs nx fu fw is bi translated">基于离散策略梯度算法的悬崖行走问题</h2><div class="pl l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">用Python实现了一个完整的增强算法。手动执行这些步骤来说明内部…</h3></div><div class="ny l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">towardsdatascience.com</p></div></div><div class="nz l"><div class="po l ob oc od nz oe ks nr"/></div></div></a></div><p id="aff5" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果你想更进一步，也可以阅读我关于自然梯度和TRPO的文章，它们构成了当代政策梯度算法的基础:</p><div class="no np gp gr nq nr"><a rel="noopener follow" target="_blank" href="/natural-policy-gradients-in-reinforcement-learning-explained-2265864cf43c"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd iu gy z fp nw fr fs nx fu fw is bi translated">解释强化学习中的自然策略梯度</h2><div class="pl l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">传统的政策梯度方法存在根本缺陷。自然梯度收敛得更快更好，形成…</h3></div><div class="ny l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">towardsdatascience.com</p></div></div><div class="nz l"><div class="pp l ob oc od nz oe ks nr"/></div></div></a></div><div class="no np gp gr nq nr"><a rel="noopener follow" target="_blank" href="/trust-region-policy-optimization-trpo-explained-4b56bd206fc2"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd iu gy z fp nw fr fs nx fu fw is bi translated">解释了信任区域策略优化(TRPO)</h2><div class="pl l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">强化学习算法TRPO建立在自然策略梯度算法的基础上，确保更新保持…</h3></div><div class="ny l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">towardsdatascience.com</p></div></div><div class="nz l"><div class="pq l ob oc od nz oe ks nr"/></div></div></a></div><p id="d453" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果你成功了，恭喜你！掌握策略梯度算法需要一些时间，但一旦掌握，它们就为混合方法(如行动者-批评家方法)以及更高级的方法(如近似策略优化)打开了大门。因此，他们的理解对任何RL从业者都是至关重要的。</p><h1 id="0523" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">摘要</h1><ul class=""><li id="158c" class="mc md it li b lj nj lm nk lp pr lt ps lx pt mb on mi mj mk bi translated">给定政策下的预期回报由国家行动轨迹乘以相应回报的概率来定义。<strong class="li iu">似然比政策梯度</strong>通过增加高回报轨迹的概率，部署由θ参数化的随机政策，建立在这一定义之上。</li><li id="a7b8" class="mc md it li b lj mm lm mn lp mo lt mp lx mq mb on mi mj mk bi translated">我们可能不知道环境的转变和回报功能。然而，在经过<strong class="li iu">对数变换</strong>之后，我们使用的是加法(对数)概率，而不是概率的乘法。这种转换将策略从(可能未知的)状态转换功能中分离出来。</li><li id="25f0" class="mc md it li b lj mm lm mn lp mo lt mp lx mq mb on mi mj mk bi translated">如果我们有一个关于它的参数化θ的可微策略，我们可以计算它的<strong class="li iu">梯度</strong>。因为我们可以将这个梯度表示为一个期望值，所以我们可以使用模拟来近似它。典型的RL实现利用损失函数，从损失函数自动导出梯度。</li><li id="9341" class="mc md it li b lj mm lm mn lp mo lt mp lx mq mb on mi mj mk bi translated">在诸如<strong class="li iu">加强</strong>的算法中，我们从环境中采样转换和回报(使用随机策略)，并将轨迹回报乘以对数策略的梯度来更新参数θ。</li></ul></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="7f69" class="mr ms it bd mt mu pu mw mx my pv na nb jz pw ka nd kc px kd nf kf py kg nh ni bi translated">进一步阅读</h1><p id="13e3" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated"><strong class="li iu">加固算法</strong></p><p id="16a3" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">威廉姆斯，R. J. (1992年)。联结主义强化学习的简单统计梯度跟踪算法。<em class="ml">机器学习</em>，<em class="ml"> 8 </em> (3)，229–256。</p><p id="c521" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">不同政策梯度方法解释</strong></p><p id="b4b3" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">里德米勒，m .，彼得斯，j .，，S. (2007年4月)。在cart-pole基准上评估政策梯度方法和变体。在<em class="ml"> 2007年IEEE近似动态规划和强化学习国际研讨会上</em>(第254–261页)。IEEE。</p><p id="85f6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">有限差分、似然比和自然政策梯度的描述</strong></p><p id="217f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae ky" href="http://www.scholarpedia.org/article/Policy_gradient_methods#Likelihood_Ratio_Methods_and_REINFORCE" rel="noopener ugc nofollow" target="_blank">http://www.scholarpedia.org/article/Policy_gradient_methods</a></p><p id="1b23" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">政策梯度介绍(许宗盛)</strong></p><p id="8a53" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae ky" href="https://jonathan-hui.medium.com/rl-policy-gradients-explained-9b13b688b146" rel="noopener">https://Jonathan-hui . medium . com/rl-policy-gradients-explained-9b 13 b 688 b 146</a></p><p id="d2ca" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">政策梯度推导(Chris Yoon) </strong></p><p id="745d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae ky" href="https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63" rel="noopener">https://medium . com/@ thechrisyoon/derivating-policy-gradients-and-implementing-reinforce-f 887949 BD 63</a></p><p id="1457" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">对数导数技巧的解释(大卫·迈耶):</strong></p><p id="b55b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae ky" href="https://davidmeyer.github.io/ml/log_derivative_trick.pdf" rel="noopener ugc nofollow" target="_blank">https://davidmeyer.github.io/ml/log_derivative_trick.pdf</a></p><p id="20de" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">似然比政策梯度(戴维·迈耶):</strong></p><p id="29b1" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae ky" href="https://davidmeyer.github.io/ml/policy_gradient_methods_for_robotics.pdf" rel="noopener ugc nofollow" target="_blank">https://David Meyer . github . io/ml/policy _ gradient _ methods _ for _ robotics . pdf</a></p><p id="c4e9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">政策梯度定理的证明和大量的政策梯度算法(Lilian Weng): </strong></p><p id="362d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae ky" href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/" rel="noopener ugc nofollow" target="_blank">https://lilian Weng . github . io/posts/2018-04-08-policy-gradient/</a></p><p id="3036" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">似然比梯度(蒂姆·维埃拉):</strong></p><p id="9c00" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae ky" href="https://timvieira.github.io/blog/post/2019/04/20/the-likelihood-ratio-gradient/#:~:text=The%20likelihood%2Dratio%20method%20can%20be%20used%20to%20derive%20several,knowledge%20of%20the%20transition%20distribution" rel="noopener ugc nofollow" target="_blank">https://timvieira . github . io/blog/post/2019/04/20/the-likelihood-ratio-gradient/</a></p><p id="167d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu"> Sergey Levine(伯克利)关于政策梯度的演讲幻灯片</strong></p><p id="de20" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae ky" href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_4_policy_gradient.pdf" rel="noopener ugc nofollow" target="_blank">http://rail . eecs . Berkeley . edu/deeprlcourse-fa17/f17 docs/lecture _ 4 _ policy _ gradient . pdf</a></p><p id="f596" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu"> David Silver (Deepmind)关于政策梯度的演讲幻灯片</strong></p><p id="c4c6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae ky" href="https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf" rel="noopener ugc nofollow" target="_blank">https://www . David silver . uk/WP-content/uploads/2020/03/pg . pdf</a></p><p id="f43a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">彼得·阿比尔的4A深度RL训练营演讲:政策梯度</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pz qa l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">伯克利政策梯度讲座</p></figure><p id="a421" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">David Silver的RL课程——第7讲:政策梯度方法</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pz qa l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">关于政策梯度的DeepMind讲座</p></figure></div></div>    
</body>
</html>