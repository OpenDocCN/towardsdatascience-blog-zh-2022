<html>
<head>
<title>Correcting and Preventing Unfairness in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">纠正和防止机器学习中的不公平现象</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/approaches-for-addressing-unfairness-in-machine-learning-a31f9807cf31#2022-06-06">https://towardsdatascience.com/approaches-for-addressing-unfairness-in-machine-learning-a31f9807cf31#2022-06-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="911e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">预处理、加工中和后处理定量方法。以及非定量方法:限制ML的使用，可解释性，解释，解决根本原因，问题意识和团队多样性</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/026d60bde5e1ee44a37c38fc04a3828d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ezcv0U8NCWTs_RQVpVSWUQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(来源:<a class="ae ky" href="https://www.flaticon.com/premium-icon/repair-tools_1850687" rel="noopener ugc nofollow" target="_blank"> flaticon </a>)</p></figure><p id="278f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">机器学习中的公平性是一个复杂的问题。更糟糕的是，负责建立模型的人不一定有能力确保它们是公平的。这是因为不公平的原因超越了数据和算法。这意味着解决方案还需要超越定量方法。</p><p id="0ce6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了理解这一点，我们将从讨论不同的<strong class="lb iu">定量方法</strong>开始。我们可以将这些分为<strong class="lb iu">前处理</strong>、<strong class="lb iu">中处理</strong>和<strong class="lb iu">后处理</strong>。我们将关注这些限制，以理解为什么它们可能无法解决不公平问题。最终，我们需要将公平视为一个更广泛的问题。</p><p id="17f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是为什么我们将继续讨论<strong class="lb iu">非定量</strong>方法。包括<strong class="lb iu">不使用</strong>或<strong class="lb iu">限制使用</strong>ML。提供<strong class="lb iu">解释</strong>、<strong class="lb iu">解释</strong>和<strong class="lb iu">质疑决策</strong>的机会是一个重要方面。它们还包括解决不公平的<strong class="lb iu">根本原因</strong>、问题的<strong class="lb iu">意识</strong>以及团队的<strong class="lb iu">多样性。为了有效解决不公平问题，将采取定量和非定量相结合的方法。</strong></p><h1 id="9b72" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">定量方法</h1><p id="4593" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">公平的量化方法是数据科学家介入的地方。他们通过调整用于训练模型的数据或训练的算法来工作。查看图1，您可以看到我们可以将这些方法分为<strong class="lb iu">预处理</strong>、<strong class="lb iu">加工中</strong>和<strong class="lb iu">后处理</strong>方法。这种划分取决于在模型开发的哪个阶段应用它们。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/457c8cfb5c5a24b7b67e24e2f8753928.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dGnc87wbIKB2ZuyYDJi-Fw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:定量方法的类型(来源:作者)</p></figure><h2 id="cd99" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated"><strong class="ak">预处理</strong></h2><p id="4c6f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">不公平的模型可能是数据偏差的结果。<strong class="lb iu">预处理</strong>方法试图在数据用于训练模型之前<strong class="lb iu">去除数据</strong> <strong class="lb iu">中的偏差。数据被转换，算法被定型，就像在原始数据集上一样。希望最终的模型是公平的。即使它不太准确。</strong></p><p id="b0d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不公平模型的一个原因是目标变量反映了历史上不公平的决策。这对于主观目标变量来说更为常见。例如，性别歧视的招聘做法可能会导致更高比例的女性求职被拒。为了解决这个问题，一种预处理方法是<strong class="lb iu">交换先前决策的目标变量</strong>。也就是说，我们重新标记了不公平的招聘决定，以人为地增加我们数据集中女性雇员的数量。</p><p id="4974" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个问题是，改变目标变量并不总是可能的。目标变量通常是客观的。例如，我们不能将以前的贷款违约重新标记为非违约。对于一些主观的目标变量可能也很难。这是因为在做出决定时，我们可能没有所有可用的信息。对于招聘决定，我们可能有简历。然而，我们可能没有录音面试，这是一个申请的一大部分。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/0fbf65634c97fd624711a4dde597703c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9AcHf4pkRNq_cms_XZOEsw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(来源:<a class="ae ky" href="https://www.flaticon.com/free-icon/question_2353678?term=confused&amp;page=1&amp;position=26&amp;page=1&amp;position=26&amp;related_id=2353678&amp;origin=search" rel="noopener ugc nofollow" target="_blank"> flaticon </a>)</p></figure><p id="7ccb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不公平模型的另一个原因是<strong class="lb iu">代理变量</strong>。这些是与<strong class="lb iu">保护变量<em class="ng">相关或关联的模型特征。</em> </strong>其中受保护变量代表敏感特征，如种族或性别。其他方法着眼于从模型特征中“修复”或移除偏差。这些通过移除模型特征和受保护变量之间的关联来工作。</p><p id="9c57" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法的一个例子是<strong class="lb iu">不同影响消除</strong> (DIR)。受保护变量通常分为特权(例如男性)和非特权(例如女性)组。DIR通过修改特征来工作，因此两个组的分布变得相似。例如，参见图2。在这里，男性和女性的收入分配发生了变化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/3a4094aafda6122631e07b76ed2075f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*dXK44t2_LgLn5j6h5roeag.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:不同影响消除的例子(来源:作者)</p></figure><p id="9433" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DIR保持每个子群体内的等级顺序。也就是说，如果你是特权人群中收入最高的人，你将一直是这个人群中收入最高的人。只有两个群体之间的等级顺序受到影响。结果是，我们将不再能够用收入来区分这两个群体。同时，我们会保留一些收入预测目标变量的能力。</p><p id="5b2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DIR还有一个<strong class="lb iu">调优参数</strong>，它在准确性和公平性之间引入了一个折衷。它允许你控制分布的移动量。换句话说，它允许您仅移除模型特征和受保护变量之间的部分关联。</p><p id="1b09" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">预处理方法的一个优点是它们可以用于任何算法。这是因为只有数据被修改。一个主要的缺点是对我们特征的解释不再清晰。我们可能需要修复多个受保护变量的特征值(如性别、种族、原籍国)。在一个特征的分布发生如此多的变化之后，它就失去了它的解释。向非技术观众解释这样一个模型将是一个挑战。</p><h2 id="4e19" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">正在处理中</h2><p id="f254" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们可以<strong class="lb iu">调整ML算法</strong>，而不是转换数据。这些被称为<strong class="lb iu">在加工</strong>方法。通常，模型被训练成最大限度地提高某种程度的准确性。处理中方法通过调整目标来考虑公平性。这可以通过改变成本函数来考虑公平性或者通过对模型预测施加约束来实现。模型是根据有偏差的数据训练的，但最终结果是一个公平的模型。</p><p id="d712" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于回归，一种方法是向成本函数添加惩罚参数。这与正则化的工作方式类似。对于正则化，我们惩罚参数值以减少复杂性和过度拟合。现在我们引入一个减少不公平的惩罚参数。</p><p id="2d0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具体而言，该参数旨在满足公平性的数学定义。例如，参见图3 <strong class="lb iu">中的<strong class="lb iu">均衡赔率</strong>。</strong>这里下标0代表非特权组，1代表特权组。在这个定义下，我们要求相等的真阳性率(TPR)和假阳性率(FPR)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/9c70327aa5261bb5cbed97da57eea9bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ieXg5G52NQuQ8Zj4zBSPQA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:均等赔率的定义(来源:作者)</p></figure><p id="1f2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">均等的机会只是公平的一个潜在定义。其他定义包括<strong class="lb iu">平等机会</strong>和<strong class="lb iu">不同影响</strong>。我们将在下面的文章中讨论所有这些。我们还将讨论每个定义的合理性，并向您展示如何使用Python来应用它们。</p><div class="nj nk gp gr nl nm"><a rel="noopener follow" target="_blank" href="/analysing-fairness-in-machine-learning-with-python-96a9ab0d0705"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">分析机器学习中的公平性(用Python)</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">做一个探索性的公平性分析，并使用平等机会、均等优势和完全不同来衡量公平性…</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">towardsdatascience.com</p></div></div><div class="nv l"><div class="nw l nx ny nz nv oa ks nm"/></div></div></a></div><p id="bb76" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些方法的缺点是在实践中很难实施。它们需要调整成熟的算法。与预处理方法不同，这些调整也是算法特定的。对于回归、树方法或神经网络，我们引入惩罚参数的方式将是不同的。</p><p id="b326" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">试图从数学上定义公平也有问题。在这样做的时候，我们可能会忽略公平的细微差别。当追求一个定义时，这变得更加困难。如果我们根据一种定义实现公平，我们不一定根据另一种定义实现公平。试图将多个定义合并到一个惩罚参数中会极大地增加算法的复杂性。</p><h2 id="7b00" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">后处理</h2><p id="d40e" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated"><strong class="lb iu">后处理</strong>方法通过<strong class="lb iu">改变模型做出的预测</strong>来工作。我们不对数据或算法做任何调整。我们只是交换某些模型预测(例如，从正面到负面)。目标是只对不公平的预测这样做。</p><p id="4db7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一种方法是对特权群体和非特权群体设定不同的门槛。例如，假设我们使用逻辑回归来预测贷款申请的违约。我们将小于0.5的概率标记为正(1)。也就是说，他们预计不会违约，我们给客户贷款。假设使用阈值0.5，我们发现女性(0)的TPR明显低于男性(1)。</p><p id="79ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，图4中的<strong class="lb iu">机会均等</strong>没有得到满足。为了解决这个问题，我们可以将女性的概率阈值降低到0.4。也就是说，我们对男性(0.5)和女性(0.4)有不同的阈值。结果将是更多的女性接受贷款，导致更高的TPR。在实践中，我们可以调整阈值，以实现在某个临界值内的TPR差异。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/7f0486f7887b87500012ecb9926a2dca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BvfRVPArYEOUPl3Ers3Zug.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4:平等机会的定义(来源:作者)</p></figure><p id="b850" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法的一个问题是，它要求我们在预测时拥有关于受保护变量的信息。为了决定使用什么门槛，我们需要关于申请人性别的可靠信息。在许多情况下，由于法律或隐私原因，此类信息仅在培训期间可用。我们也面临着类似的加工方法的缺点。也就是说，我们将努力在一个定义的基础上实现公平。</p><h2 id="5f92" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">定量方法的缺点</h2><p id="038e" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">数据科学家倾向于关注这些量化的公平方法。这是我们的优势所在。然而，如果认为仅仅通过调整数据和算法就可以解决不公平问题，那就太天真了。公平是一个复杂的问题，我们需要将其视为超越数据集的东西。只使用定量方法就像缝合枪伤而不先取出子弹一样。</p><blockquote class="ob oc od"><p id="2afc" class="kz la ng lb b lc ld ju le lf lg jx lh oe lj lk ll of ln lo lp og lr ls lt lu im bi translated">“因此，不公正和有害的结果被视为副作用，可以通过“去偏置”数据集等技术解决方案来处理，而不是那些深深植根于模糊和偶然问题的数学化、历史不平等、不对称的权力等级或渗透到数据实践中的未经检验的有问题的假设的问题。”</p><p id="86a6" class="kz la ng lb b lc ld ju le lf lg jx lh oe lj lk ll of ln lo lp og lr ls lt lu im bi translated">— <a class="ae ky" href="https://www.sciencedirect.com/science/article/pii/S2666389921000155" rel="noopener ugc nofollow" target="_blank">阿贝巴·比尔哈尼</a></p></blockquote><p id="2264" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这可能是定量方法的最大缺点。通过满足公平的一些数学定义，我们使自己相信我们已经解决了问题。然而，这可能会错过公平的细微差别。它也无助于解决不公平的根源。这并不是说定量方法没有用。他们将成为解决方案的一部分。然而，要完全解决不公平问题，我们需要额外的非量化方法。</p><h1 id="25d7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">非定量方法</h1><p id="c556" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在本节中，我们将讨论其中的一些方法。您可以在图5中看到这些内容的概述。其中大多数要求数据科学家远离计算机，用更广阔的视角来看待公平。成功实施这些方法也需要非技术技能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/31026507c78493d0e1cf68f8277a0822.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s4NNtUvU9FLLwA-_njUJZQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5:算法公平性的非量化方法概述</p></figure><h2 id="2f5d" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">对问题的认识</h2><p id="7ddf" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">数据科学涉及技术工作。我们整天看着数字、代码和屏幕。我们很容易忘记我们建立的模型会影响真实的人。更糟糕的是，有些人甚至不知道模型会导致不公平的结果。为了解决不公平，我们首先需要理解和接受ML的潜在负面后果。</p><p id="f3ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们需要了解不公平模型的原因。我们已经提到了其中的一些。它们包括<strong class="lb iu">代理变量</strong>、<strong class="lb iu">倾斜数据集</strong>和<strong class="lb iu">嵌入数据的历史不公</strong>。不公平也可能来自我们的<strong class="lb iu">算法选择</strong>或者<strong class="lb iu">用户与模型的</strong>交互方式。我们将在下面的文章中对此进行更深入的讨论。</p><div class="nj nk gp gr nl nm"><a rel="noopener follow" target="_blank" href="/algorithm-fairness-sources-of-bias-7082e5b78a2c"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">你的模型做出不公平预测的5个原因</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">偏见的常见来源——历史偏见、代理变量、不平衡数据集、算法选择和用户交互</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">towardsdatascience.com</p></div></div><div class="nv l"><div class="oi l nx ny nz nv oa ks nm"/></div></div></a></div><p id="3e7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们需要做一个彻底的<a class="ae ky" rel="noopener" target="_blank" href="/analysing-fairness-in-machine-learning-with-python-96a9ab0d0705">公平分析</a>。这是为了了解上述原因普遍存在的程度。将这种分析扩展到数据和模型之外也很重要。模型可以拒绝或接受贷款申请。我们可以用1/0来量化这些，计算公平性度量。然而，这隐藏了我们模型的真实后果。拒绝可能导致企业破产或学生无法上大学。</p><p id="72ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">只有当我们充分了解不公平的风险时，我们才能做出适当的决定来降低这些风险。这样做可能需要数据科学家转变对自己角色的看法。它不再是纯粹的技术，而是影响到真实的人。</p><h2 id="0165" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">不要用ML</h2><p id="77e7" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们需要承认ML并不能解决我们所有的问题。即使它可以被使用，它仍然可能导致不公平的结果。还有许多不可接受的用途，例如使用面部识别来预测犯罪行为。这意味着最好的解决方案可能是根本不使用ML。我们不会自动化一个过程。相反，人类将对任何决定负责。</p><p id="b4a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">选择这条路线时，你需要考虑所有的费用。一方面，人类做出决定的成本可能很高。另一方面，有偏见的模型会给用户带来严重的负面后果。这会导致失去信任和名誉受损。这些风险可能超过自动化过程的任何好处。对这些风险的理解将来自上述的公平性分析。</p><h2 id="6d00" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">限制ML的使用</h2><p id="cb99" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">如果你努力构建了一个模型，你可能会想要使用它。因此，您可以限制它的使用方式，而不是完全抛弃这个模型。这可能涉及对ML做出的决策进行某些人工检查或干预。最终，我们将使用一个模型来部分自动化一个过程。</p><p id="aac6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，一个模型可以自动<strong class="lb iu">拒绝</strong>或者<strong class="lb iu">接受</strong>贷款申请。相反，我们可以引入一个新的结果——<strong class="lb iu">参考</strong>。在这种情况下，应用程序可以由人来手动检查。推荐申请的流程可以设计为减少不公平结果的影响。这些应该旨在帮助那些最脆弱的人。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/427b068e95c652f9bf3f6b73d4c722cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AhUcyvsVDcF1fDSps8z8Sg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(来源:<a class="ae ky" href="https://www.flaticon.com/free-icon/chatbot_2040946?term=robot&amp;page=1&amp;position=46&amp;page=1&amp;position=46&amp;related_id=2040946&amp;origin=search" rel="noopener ugc nofollow" target="_blank"> flaticon </a>)</p></figure><h2 id="3a85" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">解决根本原因</h2><p id="4cb5" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">数据的不公平是现实的反映。如果我们解决了真正的潜在问题，我们也可以解决数据中的问题。这将需要公司或政府政策的转变。这也意味着从定量方法中转移资源。这需要更广泛的组织甚至整个国家的合作。</p><p id="07e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，假设一个自动化的招聘过程导致更少的女性被雇佣。我们可以通过鼓励更多女性申请来帮助解决这个问题。该公司可以通过广告活动或让其环境成为一个更适合女性的地方来实现这一目标。在国家层面，可以通过加大对女性STEM教育的投资来实现。</p><h2 id="dc5a" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">花时间理解模型</h2><p id="028b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">模型<strong class="lb iu">可解释性</strong>对公平性很重要。可解释性包括理解模型如何做出预测。这可以是模型的整体(即全局解释)。这是通过查看多个预测的趋势来实现的。这使我们能够质疑这些趋势，并决定它们是否会导致不公平。</p><p id="7282" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这也意味着理解模型是如何做出个体预测的(即局部解释)。这些告诉我们哪些模型特征对特定的预测贡献最大。这允许我们决定模型是否导致了对特定用户不公平的结果。</p><p id="2a11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可解释性要求我们将理解置于性能之上。我们可以使用回归或决策树等本质上可解释的模型来做到这一点。这些可以通过查看模型参数来直接解释。对于非线性模型，如xgboost或神经网络，我们将需要<a class="ae ky" rel="noopener" target="_blank" href="/what-are-model-agnostic-methods-387b0e8441ef">模型不可知的</a>方法。一种常见的方法是SHAP，我们将在下面的文章中介绍。</p><div class="nj nk gp gr nl nm"><a rel="noopener follow" target="_blank" href="/introduction-to-shap-with-python-d27edc23c454"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">Python SHAP简介</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">如何创造和解释SHAP情节:瀑布，力量，决定，SHAP和蜂群</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">towardsdatascience.com</p></div></div><div class="nv l"><div class="ok l nx ny nz nv oa ks nm"/></div></div></a></div><h2 id="79d2" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">给出解释</h2><p id="6721" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">模型预测会给用户带来严重的后果。考虑到这一点，他们有权为那些预测得到一个<strong class="lb iu">的解释</strong>。解释可以很大程度上基于当地的解释。也就是说，我们可以解释哪些功能对影响用户的预测贡献最大。</p><p id="d44d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">重要的是要明白解释和说明不是一回事。解释是技术性的。我们关注模型参数或SHAP值。向非技术观众提供解释。这意味着以一种可以理解的方式给出它们是很重要的。我们将在下面的文章中讨论如何做到这一点。</p><div class="nj nk gp gr nl nm"><a rel="noopener follow" target="_blank" href="/the-art-of-explaining-predictions-22e3584ed7d8"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">解释预言的艺术</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">如何以人性化的方式解释你的模型</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">towardsdatascience.com</p></div></div><div class="nv l"><div class="ol l nx ny nz nv oa ks nm"/></div></div></a></div><p id="debe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解释也可以超越特性贡献。我们可以解释决策过程自动化的程度。我们还可以解释在这个过程中使用了哪些数据，以及这些数据是从哪里获得的。这些方面的解释可以由法律或客户需求来定义。</p><h2 id="2dca" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">给挑战决策的机会</h2><p id="0f71" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">一旦给用户一个解释，他们就可以决定这个解释是否合理。如果他们认为这是不合理的，他们必须被允许挑战这个决定。赋予用户这种决定权对于消除不公平至关重要。这意味着不公平的决定更有可能被质疑和纠正。</p><p id="8615" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这又回到了限制ML使用的问题上。我们需要建立程序，允许至少一些决策是人工做出的。就像我们的贷款模型示例一样，决策可以提交给贷方，而不是自动拒绝。申请人对这一过程有控制权是很重要的。</p><p id="97d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也可以回想一下我们的定量方法。像DIR这样的方法会增加复杂性和对可解释性的负面影响。这使得给出人性化的解释变得更加困难。换句话说，通过影响解释，一些定量方法可能对公平性产生负面影响。</p><h2 id="383c" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">团队多样性</h2><p id="3a4c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">数据是我们做出有效决策的最佳工具。然而，如前所述，它可以隐藏ML的真实后果。我们的生活经历可以更好地说明。这些经历与我们的文化背景有关。这就是我们体验世界的方式，技术取决于我们的性别、种族、宗教或原籍国。这使得从不同的人群中获得对我们ML系统的反馈变得非常重要。</p><p id="aa9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">雇佣一个多元化的团队也很重要。这些人实际上将构建模型和系统。这样做会带来一系列不同的生活经历。他们都将理解这个系统将如何影响他们自己的生活。这将使得在部署模型之前识别潜在的公平性问题变得更加容易。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/710104abf49149cdbc50fa10fbca4422.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p47r9RGOlb6uf69S9T2bjg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(来源:<a class="ae ky" href="https://www.flaticon.com/free-icon/chatbot_2040946?term=robot&amp;page=1&amp;position=46&amp;page=1&amp;position=46&amp;related_id=2040946&amp;origin=search" rel="noopener ugc nofollow" target="_blank"> flaticon </a>)</p></figure><p id="7121" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">多元化也意味着雇佣不同专业领域的人。正如我们提到的，要解决不公平问题，我们需要超越定量方法。换句话说，我们需要大多数数据科学家不具备的技能。一个关键的团队成员将是人工智能伦理方面的专家。他们会对我们上面概述的非定量方法有更深的理解。然后，数据科学家将能够专注于定量方法。</p></div><div class="ab cl on oo hx op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="im in io ip iq"><p id="78de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望这篇文章对你有帮助！你可以成为我的<a class="ae ky" href="https://conorosullyds.medium.com/membership" rel="noopener"> <strong class="lb iu">推荐会员</strong> </a> <strong class="lb iu">来支持我。你可以访问Medium上的所有文章，我可以得到你的部分费用。</strong></p><div class="nj nk gp gr nl nm"><a href="https://conorosullyds.medium.com/membership" rel="noopener follow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">通过我的推荐链接加入Medium康纳·奥沙利文</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">conorosullyds.medium.com</p></div></div><div class="nv l"><div class="ou l nx ny nz nv oa ks nm"/></div></div></a></div><p id="1338" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在|<a class="ae ky" href="https://twitter.com/conorosullyDS" rel="noopener ugc nofollow" target="_blank">Twitter</a>|<a class="ae ky" href="https://www.youtube.com/channel/UChsoWqJbEjBwrn00Zvghi4w" rel="noopener ugc nofollow" target="_blank">YouTube</a>|<a class="ae ky" href="https://mailchi.mp/aa82a5ce1dc0/signup" rel="noopener ugc nofollow" target="_blank">时事通讯</a>上找到我——注册免费参加<a class="ae ky" href="https://adataodyssey.com/courses/shap-with-python/" rel="noopener ugc nofollow" target="_blank"> Python SHAP课程</a></p><h2 id="1378" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">图像来源</h2><p id="4c46" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">所有图片都是我自己的或从<a class="ae ky" href="http://www.flaticon.com/" rel="noopener ugc nofollow" target="_blank">www.flaticon.com</a>获得的。在后者的情况下，我拥有他们的<a class="ae ky" href="https://support.flaticon.com/hc/en-us/articles/202798201-What-are-Flaticon-Premium-licenses-" rel="noopener ugc nofollow" target="_blank">高级计划</a>中定义的“完全许可”。</p><h2 id="b905" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">参考</h2><p id="688b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">Birhane，a .，(2021) <strong class="lb iu">算法的不公正:一种关系伦理方法</strong>。<a class="ae ky" href="https://www.sciencedirect.com/science/article/pii/S2666389921000155" rel="noopener ugc nofollow" target="_blank">https://www . science direct . com/science/article/pii/s 2666389921000155</a></p><p id="5fda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Pessach，d .和Shmueli，e .(2020)，<strong class="lb iu">算法公平性。</strong>T10】https://arxiv.org/abs/2001.09784</p><p id="74c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Mehrabi，n .、Morstatter，f .、Saxena，n .、Lerman，k .和Galstyan，A .，(2021)，<strong class="lb iu">关于机器学习中的偏见和公平的调查。</strong><a class="ae ky" href="https://arxiv.org/abs/1908.09635" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1908.09635</a></p><p id="0b82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Feldman，m .、Friedler，S.A .、Moeller，j .、Scheidegger，c .和Venkatasubramanian，s .，(2015)，<strong class="lb iu">证明和消除不同的影响</strong>。<a class="ae ky" href="https://dl.acm.org/doi/pdf/10.1145/2783258.2783311" rel="noopener ugc nofollow" target="_blank">https://dl.acm.org/doi/pdf/10.1145/2783258.2783311</a></p><p id="c9c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Bechavod，y .和Ligett，k .(2017)，<strong class="lb iu">惩罚二元分类中的不公平。<a class="ae ky" href="https://arxiv.org/abs/1707.00044" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1707.00044</a>T21</strong></p><p id="a29c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">钢琴独奏音乐会(2020)。机器学习和人工智能中的伦理原则:来自该领域的案例和可能的前进方向。<a class="ae ky" href="https://www.nature.com/articles/s41599-020-0501-9" rel="noopener ugc nofollow" target="_blank">https://www.nature.com/articles/s41599-020-0501-9</a></p><p id="2499" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">史密斯，g .(2020年)。<strong class="lb iu">对于机器学习系统来说，“公平”意味着什么？</strong><a class="ae ky" href="https://haas.berkeley.edu/wp-content/uploads/What-is-fairness_-EGAL2.pdf" rel="noopener ugc nofollow" target="_blank">https://Haas . Berkeley . edu/WP-content/uploads/What-is-fairness _-egal 2 . pdf</a></p><p id="3191" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">谷歌，(2022)，<strong class="lb iu">包容性数据如何打造更强大的品牌</strong><a class="ae ky" href="https://www.thinkwithgoogle.com/feature/ml-fairness-for-marketers/#what-we-learned" rel="noopener ugc nofollow" target="_blank">https://www . thinkwithggoogle . com/feature/ml-fairness-for-markets/# what-we-learned</a></p></div></div>    
</body>
</html>