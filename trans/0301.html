<html>
<head>
<title>Beginners Guide for Choosing the Correct Spark API: RDDs, DataFrames, and Datasets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正确选择Spark API的初学者指南:rdd、数据帧和数据集</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beginners-guide-for-choosing-the-correct-spark-api-rdds-dataframes-datasets-23c94041f575#2022-02-12">https://towardsdatascience.com/beginners-guide-for-choosing-the-correct-spark-api-rdds-dataframes-datasets-23c94041f575#2022-02-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6c14" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">何时使用rdd、数据帧和数据集？激发正确的选择！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6011f6b9b1e682dd5b104a1035fed451.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cf1N8jtWHUdTUY_X"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">📷<a class="ae ky" href="https://unsplash.com/@capturebylucy?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">露西·希斯</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><h1 id="51cd" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="573b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">当开始使用Spark编程时，我们将可以选择使用不同的抽象来表示数据——灵活地使用三种API(rdd、数据帧和数据集)中的一种。但是这个选择需要小心处理。随机选择API会影响ETL/ELT/ETLT管道和分布式集群的性能。</p><p id="4eb9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我在<strong class="lt iu">粗体</strong>中强调了Spark架构中使用的关键点和有用术语。</p><h1 id="cca6" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">动机</h1><p id="fb14" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我最近一直在与Spark合作，并有机会研究和体验不同Spark APIs的优缺点。</p><p id="f23e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我也使用过熊猫在进入Spark之前提供的数据结构。对于处理大数据用例，与小数据项目相比，我们需要有不同的思维模式。</p><p id="f547" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">本文将对那些经历类似的向分布式数据领域过渡的人有所帮助。因此，我想分享我在处理跨多个节点的大数据项目时如何选择合适的API的经验。</p><h1 id="4845" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">Spark中使用的基本术语</h1><p id="86ec" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Spark中使用的API利用了这两个基本术语:</p><h2 id="aa53" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">A.分布式执行-驱动程序和工作程序节点</h2><p id="42cd" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在Spark架构中，使用两种类型的机器/节点/计算基础设施，即驱动程序和工作程序来支持并行执行。</p><p id="0c8f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">把它们想象成我们如何解决一个大型拼图游戏:</p><p id="d3fc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">a)我们可以同时开始处理它的不同部分。在Spark架构中，它是一组同时处理任务的机器/节点(也称为<strong class="lt iu">工作节点</strong>)。</p><p id="c0f6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">b)然后我们连接/还原结果，以从拼图中获得完整的图片。在Spark中，最终结果被发送回用户应用程序可以使用的<strong class="lt iu">驱动节点</strong>。</p><h2 id="17cc" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated"><strong class="ak"> B .惰性评估-转换&amp;动作</strong></h2><p id="81d5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">用Spark编写的代码被分成<strong class="lt iu">转换</strong>和<strong class="lt iu">动作</strong>。两者都是基于spark函数的形式，但不同之处在于它们的使用方式。<br/>转换<strong class="lt iu">的谱系</strong>以<strong class="lt iu">有向无环图(DAG) </strong>的形式收集，并在驱动节点触发<strong class="lt iu">动作</strong>时执行。在DAG的帮助下，Spark可以进行关键优化，以减少执行时间并提高节点集群的性能。</p><p id="3e3c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在并行计算中，考虑下面的Spark命令，它过滤一个大的文本文件，并在第一行搜索一个特殊的单词。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/c44df7b99aa747747cc4662ec9381d39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KDoxPDhP4JUgJkC2EV1s1g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">为什么懒在火花好！[图片由作者提供]</p></figure><p id="2362" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">所需的数据量远远少于查询的数据量，在这种情况下，第一行是需要考虑可以应用过滤器的数据。因此，通过整理<strong class="lt iu">转换</strong> Spark将构建一个最佳计划来一起执行它们。这也减少了驱动程序和工作节点之间的往返次数，减少了它们之间所需的数据I/O量。</p></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><h1 id="db86" class="kz la it bd lb lc nm le lf lg nn li lj jz no ka ll kc np kd ln kf nq kg lp lq bi translated">在Spark中与三个火枪手(API)合作</h1><p id="1246" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这一节深入探究了三个火枪手(即RDDs、Dataframes和Datasets)的细微差别。引入它们是为了处理将跨节点分布的数据。</p><p id="187c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">它们作为Spark中的特性被引入的时间线同样重要。它解释了一个API的缺点如何成为优化后续API的触发因素。</p><p id="262e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在接下来的四个小节中，我们从rdd的用例开始，然后分别阐述Dataframe和Dataset API，最后讨论Dataset与Dataframe API的统一以及何时使用这些API。</p><h1 id="b387" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">1.RDD(弹性分布式数据集)</h1><p id="594f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">rdd是Matei Zaharia等人在2012年作为内存计算的容错抽象提出的(<a class="ae ky" href="https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf" rel="noopener ugc nofollow" target="_blank">会议资源</a>)。</p><p id="4fe0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">随着RDD在Spark体系结构中的引入，与使用细粒度读/写转换的分布式共享内存系统相比，它显得更加有效。</p><p id="3142" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">缩写RDD可以拆分来理解它的定义:</p><ul class=""><li id="67db" class="nr ns it lt b lu mn lx mo ma nt me nu mi nv mm nw nx ny nz bi translated"><strong class="lt iu"><em class="oa"/></strong>:数据的处理方式是容错的，Spark能够在数据损坏的情况下随时重建数据。这是由于在Spark操作中使用的<strong class="lt iu">有向无环图(DAGs) </strong>。</li><li id="8e16" class="nr ns it lt b lu ob lx oc ma od me oe mi of mm nw nx ny nz bi translated"><strong class="lt iu"> <em class="oa">分布式数据集</em> </strong>:我们通过Spark上下文馈入Spark的数据将被存储为分区。</li></ul><p id="aecb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">它们是Spark优化引擎使用的低级API。需要注意的重要一点是，Spark中的大多数查询最终都会简化成这种形式，因此它确实是Spark中的低级API。</p><p id="c0b0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是为开发人员在分布式架构中实现<strong class="lt iu">容错</strong>而引入的，这是通过在节点集群之间使用受限形式的共享内存来实现的，基于形成谱系的粗粒度转换，而不是依赖于细粒度更新。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/eadbabafe3740a3d7d55f859ec4148f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R_8630rMXIsJFAoWSUkCQA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">DAGs在Spark中带来容错行为[图片由作者提供]</p></figure><h2 id="cd31" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">何时使用rdd？</h2><ul class=""><li id="c314" class="nr ns it lt b lu lv lx ly ma oh me oi mi oj mm nw nx ny nz bi translated">当您处理需要在行/记录级别转换特殊数据结构(文本或媒体文件)的操作时。</li><li id="2228" class="nr ns it lt b lu ob lx oc ma od me oe mi of mm nw nx ny nz bi translated">当在粒度级别修改数据的灵活性更重要时。</li><li id="6290" class="nr ns it lt b lu ob lx oc ma od me oe mi of mm nw nx ny nz bi translated">当模式变得与您的用例无关，但并行化会有所帮助。</li><li id="17c5" class="nr ns it lt b lu ob lx oc ma od me oe mi of mm nw nx ny nz bi translated">当您不打算使用<strong class="lt iu">特定于领域的表达式时(<em class="oa">想想Spark SQL抽象)</em> </strong></li></ul><h2 id="940b" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated"><strong class="ak">何时避免使用rdd？</strong></h2><ul class=""><li id="9471" class="nr ns it lt b lu lv lx ly ma oh me oi mi oj mm nw nx ny nz bi translated">当您想要强加模式并使用Spark catalyst优化以及列名访问时。</li><li id="90a7" class="nr ns it lt b lu ob lx oc ma od me oe mi of mm nw nx ny nz bi translated">当你想为简单的API操作避免复杂的编码结构时，例如寻找一个文件中单词的平均频率。</li></ul><h1 id="becc" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">2.数据帧</h1><p id="32a8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Spark 1.3.0版本(2015年初)中引入了数据帧。它是RDDs的高级抽象，由一个模式提供支持，该模式还允许Spark使用<strong class="lt iu"> Catalyst optimizer </strong>在运行时执行更多的自动化优化。</p><p id="7c3d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">当连接两个rdd时，由于<strong class="lt iu">催化剂优化器</strong>的存在，将它们转换成数据帧可能是有益的。它使用Scala中的准引号和模式匹配来加速执行过程。</p><p id="cdff" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">可以使用以下方式创建数据帧:</p><ul class=""><li id="ca65" class="nr ns it lt b lu mn lx mo ma nt me nu mi nv mm nw nx ny nz bi translated">从RDDs使用<em class="oa"> inferSchema </em>选项(或)使用自定义模式。</li><li id="d4ff" class="nr ns it lt b lu ob lx oc ma od me oe mi of mm nw nx ny nz bi translated">来自不同格式的文件(JSON、Parquet、CSV、Avro等。).</li><li id="2969" class="nr ns it lt b lu ob lx oc ma od me oe mi of mm nw nx ny nz bi translated">从数据集中使用<strong class="lt iu"> <em class="oa">隐式转换</em></strong><code class="fe ok ol om on b"><strong class="lt iu"><em class="oa">toDF</em></strong></code><strong class="lt iu"><em class="oa"/></strong>的方法。</li></ul><h2 id="5f10" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">数据帧的非类型化本质——运行时的类型检查</h2><p id="115e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">数据帧是一组保存数据的通用行对象，它们有类型。单词<strong class="lt iu"> untyped </strong>表示对数据帧进行类型检查的时间。仅在<strong class="lt iu"> <em class="oa">运行期间</em> </strong>根据用户推断或定义的模式完成。</p><h2 id="6947" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">数据框架的缺点</h2><ul class=""><li id="03d7" class="nr ns it lt b lu lv lx ly ma oh me oi mi oj mm nw nx ny nz bi translated">它无法通过优化有效地使用UDF。</li><li id="22b5" class="nr ns it lt b lu ob lx oc ma od me oe mi of mm nw nx ny nz bi translated">缺乏Scala/Java中可以实现的强类型。</li></ul><h1 id="73a0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">3.数据集</h1><p id="67b9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Spark 1 . 6 . 0版(2016年初)引入了数据集。它本身带来了在<strong class="lt iu"> <em class="oa">编译时</em> </strong>进行强类型检查的优势。</p><p id="f278" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">引入类型安全的基本概念是通过<strong class="lt iu">编码器</strong>的引入，它可以将类型T的JVM对象转换成内部二进制表示。<strong class="lt iu"> </strong>也是序列化&amp;反序列化(<strong class="lt iu"> SerDe </strong>)框架<strong class="lt iu">。</strong>编码器表示记录的模式，这避免了JVM对象的不必要的转换。它强制执行从域对象到内部二进制表示的映射。与Java或Kryo序列化相比，它们提供了超快的转换。</p><h2 id="7943" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">数据集的类型化本质——编译时的类型检查</h2><p id="aa43" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">强类型数据集API带来了使用Scala中的case类或Java中的bean对象来创建将在编译时强制执行的模式的优势。</p><p id="8891" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">只有数据集才有检查语法和分析错误的能力。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/62145147c9eca5c8affe869b9906b03a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*--gZxNRQ0C8b77PiCeQKXw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据框架和数据集API中的类型检查[图片由作者提供]</p></figure><p id="da1f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这种强大的类型检查将节省开发人员的时间和成本。</p><h2 id="e7bb" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">数据集的特征</h2><ul class=""><li id="538c" class="nr ns it lt b lu lv lx ly ma oh me oi mi oj mm nw nx ny nz bi translated">对半结构化数据的无缝支持。</li><li id="347b" class="nr ns it lt b lu ob lx oc ma od me oe mi of mm nw nx ny nz bi translated">编译时类型安全(语法+分析错误)。</li><li id="f38e" class="nr ns it lt b lu ob lx oc ma od me oe mi of mm nw nx ny nz bi translated">编码器提高了序列化/反序列化的速度。</li><li id="f94e" class="nr ns it lt b lu ob lx oc ma od me oe mi of mm nw nx ny nz bi translated">Java和Scala的单一API。</li></ul><h1 id="3354" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">4.数据集和数据框架API的统一</h1><p id="cb40" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Dataset和Dataframe是独立的API，直到最后两个<em class="oa">火枪手</em>在Spark 2.0.0版本(2016年末)中组合成了<strong class="lt iu">统一的Dataset API </strong>。</p><p id="3c91" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这种统一将rdd和Dataframe APIs的优点结合在一起。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/2beff2e53581bd3123379026db60e3b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fDHOG79twHtNtluSybd2sg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据框架和数据集API的统一(Spark 2.0+)[图片由作者提供]</p></figure><p id="ba02" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Dataframe成为了<code class="fe ok ol om on b"><strong class="lt iu">Dataset[Row]</strong></code>的类型别名。就语言而言，数据帧仍然是Python语言的主要抽象，因为它们类似于单节点数据帧。在Scala &amp; Java中，datasets代表API的类型化版本，dataframe是非类型化版本。</p><h2 id="bbe4" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">何时使用统一数据集[Dataframe / Dataset] API？</h2><ul class=""><li id="3b3f" class="nr ns it lt b lu lv lx ly ma oh me oi mi oj mm nw nx ny nz bi translated">当我们计划在特定领域的抽象(聚合、连接等)上使用高层抽象时。)和模式实施。</li><li id="d508" class="nr ns it lt b lu ob lx oc ma od me oe mi of mm nw nx ny nz bi translated">列访问，lambda作用于半结构化数据。</li><li id="27a0" class="nr ns it lt b lu ob lx oc ma od me oe mi of mm nw nx ny nz bi translated">为了在编译时进行更高级别的类型安全检查，我们可以使用统一数据集API的类型化版本。</li><li id="60f1" class="nr ns it lt b lu ob lx oc ma od me oe mi of mm nw nx ny nz bi translated">受益于<strong class="lt iu">钨代码生成(<em class="oa">使用dataframe/SQL操作符</em> ) </strong>的更快表达式求值。</li><li id="3042" class="nr ns it lt b lu ob lx oc ma od me oe mi of mm nw nx ny nz bi translated">r建议用户使用数据框架(数据集不可用)。</li><li id="8328" class="nr ns it lt b lu ob lx oc ma od me oe mi of mm nw nx ny nz bi translated">Python用户也可以使用数据框架(数据集不可用)。</li></ul><h1 id="ef7e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结束语</h1><p id="728e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">毫无疑问，对于大数据用例来说，一种通用的API是不存在的。关键的一点是，我们要根据我们面临的场景选择正确的API:输入文件的类型、选择的编程语言、将要执行的处理类型以及流程中的操作量。</p><p id="bbac" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">另外，请注意，这三种类型之间的转换是无缝的，可以使用简单的方法调用来完成。因此，如果您注意到使用特定的API在速度和效率方面是有益的，那么转换将是利用其优势的最佳选择。</p><p id="802a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在撰写本文时——在Spark (3.2.X)的最新版本中，有更多关于API进一步标准化的讨论，Python中熊猫UDF的优化以及PySpark中对熊猫API的支持。谈到Apache Spark，变化的可能性是无穷的。</p><p id="3974" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">你可以在<a class="ae ky" href="https://www.linkedin.com/in/lingeshwaran-kanniappan-157455117/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与我联系，进行简短的交谈。</p><p id="f07a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="oa">你可以在这里</em>  <em class="oa">阅读我的其他文章</em> <a class="ae ky" href="https://medium.com/@lingeshk" rel="noopener"> <em class="oa">。下次见！</em></a></p></div></div>    
</body>
</html>