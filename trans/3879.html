<html>
<head>
<title>Make The Most of Your Small NER Data Set by Fine-tuning Bert</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过微调Bert充分利用您的小型NER数据集</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/make-the-most-of-your-small-ner-data-set-by-fine-tuning-bert-98a2c8b544f7#2022-08-29">https://towardsdatascience.com/make-the-most-of-your-small-ner-data-set-by-fine-tuning-bert-98a2c8b544f7#2022-08-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a278" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">利用大型语言模型来提高模型的性能</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/e3324d897071c06499fba56c8095d87c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*AbiQnkNadv0wZZWCzNz2PA.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><p id="2a1b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在小数据集上训练NER模型可能是一件痛苦的事情。当只对几百个样本进行训练时，很难建立一个模型来很好地概括非平凡的NER任务。这类问题我们将在本帖中尝试解决。</p><p id="4256" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">首先，我们将定义任务。然后，我们将描述模型架构的不同变化，然后比较预训练模型与从头开始训练的模型之间的结果。</p><h1 id="7c1e" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">问题是</h1><p id="ac4c" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">我们将解决一个应用于食谱数据集的NER(命名实体识别)问题。这项任务的目标是从原始文本中提取标记实体形式的结构化信息。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/96ac6c4cf72245dd3fdf59e2fb6f5793.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*VwdHOFHjOkMMaaRd5Ne7bg.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">原始文本</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mm"><img src="../Images/892648e7c1f1a1119e75c4f77f8e453a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7uENEuh5szbbkIf03yT2rA.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">标记文本</p></figure><p id="b7f1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们的目标是给原始文本的每个实体分配正确的标签。</p><p id="616a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们将使用的数据集很小，大约有700个句子。这是这项任务的主要挑战，也是预训练模型可以大放异彩的地方，因为对它们进行微调可以将预训练步骤中学习到的表示重新用于手头的任务。</p><h1 id="5efb" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">模特们</h1><h2 id="60fc" class="mr lo iq bd lp ms mt dn lt mu mv dp lx la mw mx lz le my mz mb li na nb md nc bi translated">建筑:</h2><p id="e120" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">我们模型的基本架构是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/0235918618568aebe2e6bac42dd71333.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*EtsFOU6Jn_nWLAl0_kmbAg.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">模型架构(图片由作者提供)</p></figure><h2 id="1ab6" class="mr lo iq bd lp ms mt dn lt mu mv dp lx la mw mx lz le my mz mb li na nb md nc bi translated">标记方案:</h2><p id="2080" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">上面提到的生物标签是一种在从原始文本生成的标记序列上编码标签的方法，如下所述:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/45fd47512385786e0ebea2c5284708e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*4bxSk5WvY5AeDQeAbbt5uw.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">来源:https://x-wei.github.io/notes/xcs224n-lecture3.html<a class="ae mk" href="https://x-wei.github.io/notes/xcs224n-lecture3.html" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="0dcb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">每个{LABEL}实体的起始标记被标记为B-{LABEL}，同一实体的每个其他标记被标记为I-{LABEL}。所有其他令牌都标记为o。</p><h2 id="573f" class="mr lo iq bd lp ms mt dn lt mu mv dp lx la mw mx lz le my mz mb li na nb md nc bi translated">实施:</h2><p id="4794" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">我们将为伯特使用拥抱脸变形金刚库。我们可以像插入任何其他火炬层一样插入预训练模型，因为它是nn.Module的一个实例。</p><p id="2cca" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在模型的__init__中，我们可以这样做-&gt;</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="94f7" class="mr lo iq ng b gy nk nl l nm nn">from transformers import BertConfig, BertModel</span><span id="2b5c" class="mr lo iq ng b gy no nl l nm nn">class BertNerModel(BaseModel):<br/>    def __init__(<br/>        self,<br/>        ...<br/>        dropout=0.2,<br/>        bert_path=None,<br/>        ...<br/>    ):<br/>        super().__init__()<br/><br/>        ...</span><span id="c7b3" class="mr lo iq ng b gy no nl l nm nn">        self.bert = BertModel(config=CONFIG)<br/><br/>        ...<br/><br/>        if bert_path:<br/>            state_dict = torch.load(bert_path)<br/>            self.bert.load_state_dict(state_dict)<br/><br/>        self.do = nn.Dropout(p=dropout)<br/><br/>        self.out_linear = nn.Linear(CONFIG.hidden_size, n_classes)</span></pre><p id="25ef" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">其中CONFIG是BertConfig的实例，从JSON文件初始化。</p><p id="31ea" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">然后，在向前的方法中，我们做:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="6b86" class="mr lo iq ng b gy nk nl l nm nn">def forward(self, x):<br/><br/>    mask = (x != self.pad_idx).int()<br/>    x = self.bert(<br/>        x, attention_mask=mask, encoder_attention_mask=mask<br/>    ).last_hidden_state<br/>    # [batch, Seq_len, CONFIG.hidden_size]<br/><br/>    x = self.do(x)<br/><br/>    out = self.out_linear(x)<br/><br/>    return out</span></pre><p id="ceb1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们首先为填充标记生成一个掩码，然后将输入提供给BERT模型。我们从BERT的输出中提取最后一个隐藏层，然后通过线性分类器层来产生每个类的分数。</p><p id="a29e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">作为比较，我们也将使用nn.TransformerEncoder从头开始训练一个模型。你可以在这里找到所有的<a class="ae mk" href="https://github.com/CVxTz/ner_playground/blob/master/src/ner_playground/models.py" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="91f0" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">结果呢</h1><p id="5a94" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">我们使用Tensorboard跟踪损失指标:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi np"><img src="../Images/4797dc21fb52bc84afb5ae877ddf1f62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y5TZ55PxSXluhd1pFcCVVw.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">验证损失(橙色:BERT，蓝色:nn。变压器编码器)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nq"><img src="../Images/2bb2bfa4f2a15b7bf7d719eea97b7830.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kk1R1Ks0Rfu7o5WkCE7jjQ.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">令牌级精度(橙色:BERT，蓝色:nn。变压器编码器)</p></figure><p id="9e6b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们可以看到，基于微调BERT的模型比从零开始训练的模型概括得更好。以下F1分数进一步证实了这一点:</p><h2 id="978c" class="mr lo iq bd lp ms mt dn lt mu mv dp lx la mw mx lz le my mz mb li na nb md nc bi translated">验证集的F1分数:</h2><p id="e655" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">BERT初始化:<strong class="kt ir"> 92.9% </strong></p><p id="6372" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">nn。变压器编码器:<strong class="kt ir"> 76.3% </strong></p><h2 id="f833" class="mr lo iq bd lp ms mt dn lt mu mv dp lx la mw mx lz le my mz mb li na nb md nc bi translated">在3080 RTX GPU上的推理速度(批量大小为1):</h2><p id="5467" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">BERT初始化:43句/秒</p><p id="bd86" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">nn。TransformerEncoder: 303句/秒</p><h1 id="f9f7" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">结论</h1><p id="e977" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">通过使用预先训练的模型，我们能够获得额外的<strong class="kt ir"> 16% F1 </strong>积分。由于拥抱脸的变形金刚库和PyTorch，这很容易做到，但代价是推理速度较慢。然而，我们仍然可以使用权重量化或模型提取来缓解推理延迟问题，同时保持F1的大部分性能。</p><p id="633d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">您可以在这个库中找到重现所有这些结果的代码:<a class="ae mk" href="https://github.com/CVxTz/ner_playground" rel="noopener ugc nofollow" target="_blank">https://github.com/CVxTz/ner_playground</a></p><p id="ed17" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">数据集:<a class="ae mk" href="https://github.com/taisti/TASTEset/tree/main/data" rel="noopener ugc nofollow" target="_blank">https://github.com/taisti/TASTEset/tree/main/data</a>(麻省理工学院许可)</p><p id="50ae" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">感谢阅读！</p></div></div>    
</body>
</html>