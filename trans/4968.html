<html>
<head>
<title>Ace your Machine Learning Interview — Part 5</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">赢得机器学习面试——第五部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ace-your-machine-learning-interview-part-5-3de48703cd65#2022-11-04">https://towardsdatascience.com/ace-your-machine-learning-interview-part-5-3de48703cd65#2022-11-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/dd1b46c39828a69784f9ac6c4b63f068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Z9tjHhe5MQRL4p58"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae jd" href="https://unsplash.com/@austindistel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Austin Distel </a>拍摄的照片</p></figure><div class=""/><div class=""><h2 id="e553" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">使用Python深入研究内核支持向量机</h2></div><p id="1a2b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如我在本系列的前一篇文章中提到的，我现在将继续谈论支持向量机，以及如何使用<strong class="kx jh">核技巧</strong>将它们用于对非线性可分数据集进行分类。</p><p id="65c2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你错过了这个<em class="lr"> Ace your Machine Learning面试系列</em>的前几篇文章，我留下了下面的链接:</p><ol class=""><li id="137b" class="ls lt jg kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/ace-your-machine-learning-interview-part-1-e6a5897e6844"> <em class="lr"> Ace your Machine Learning面试—第一部分</em> </a> <em class="lr">:深入线性、套索和岭回归及其假设</em></li><li id="5512" class="ls lt jg kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/ace-your-machine-learning-interview-part-2-c58526b5faba"><em class="lr">Ace your Machine Learning访谈—第二部分</em> </a> <em class="lr">:使用Python深入研究分类问题的逻辑回归</em></li><li id="fc68" class="ls lt jg kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/ace-your-machine-learning-interview-part-3-af432f922aa7"><em class="lr">Ace your Machine Learning面试—第三部分</em> </a> <em class="lr">:使用Python深入研究朴素贝叶斯分类器</em></li><li id="0e86" class="ls lt jg kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/ace-your-machine-learning-interview-part-4-e30b695ce63"><em class="lr">Ace your Machine Learning访谈—第四部分</em> </a> <em class="lr">:深入研究使用Python的支持向量机</em></li></ol><h2 id="e8da" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">介绍</h2><p id="0731" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">在上一篇文章中，我们看到了如何通过最大化间隔来获得良好的概括，从而将支持向量机用于分类问题。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ne"><img src="../Images/027a696844cd95eb795ce1d33d1bb510.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7bBrzpSJco9_t0R5fPpJjQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">逻辑回归VS SVM(图片由作者提供)</p></figure><p id="bfbf" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我们有一个不能用直线(或n维超平面)分类的数据集，我们该怎么办？</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nj"><img src="../Images/430ebbb8587762ce8b62e42f3dfde165.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*G4joM0eEz9xP-TR0AKrKuA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">数据集不可线性分离(图片由作者提供)</p></figure><p id="303e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请注意，在上图所示的数据集中，我们有两个类，red和blue。但是没有办法用一条直线把这两个类分开。唯一的方法是使用一种“循环”函数，绿色部分描述的函数。</p><p id="8d45" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这就是SVM核仁的神奇之处。它们允许我们<strong class="kx jh">将数据集投射到更高维度的空间</strong>。在这个新的空间中，很容易找到一个超平面来正确划分这两个类。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/a8ac521ebb5e95079812ffa1af3c85f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*veUTDmgRaeJv2xyiCThIhg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">从2D到3D的转变(图片由作者提供)</p></figure><p id="3dd9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，处理非线性可分数据的核方法的基本思想是创建原始特征的非线性组合，以便使用φ函数将数据集投影到新空间。让我们来看一个将我们从2D空间带入3D空间的<strong class="kx jh">φ函数的例子。</strong></p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/f7f8bb6ffad073ace50088ac8046dddf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*PyjWE75MawewEfn86PS_1A.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">φ映射(图片由作者提供)</p></figure><p id="27c3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种方法的问题是构建这些新功能在计算上非常昂贵。这就是所谓的内核技巧帮助我们的地方。首先，我们必须找出如何解决前一章中训练支持向量机的优化问题，但是数学是非常复杂的，并且会花费大量的时间。</p><p id="b315" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">总而言之，我们只需要明白我们需要<strong class="kx jh">计算每个向量对<em class="lr">x _ I * x _ j</em>T5】的点积。那么在我们的例子中，<strong class="kx jh">我们应该首先使用函数φ将每个向量投影到新的空间中，然后计算<em class="lr">φ(x _ I)*φ(x _ j)</em></strong>。</strong></p><p id="de4e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以有两件事要做:</p><ul class=""><li id="a126" class="ls lt jg kx b ky kz lb lc le lu li lv lm lw lq nm ly lz ma bi translated">将每个向量投影到新的空间</li><li id="75af" class="ls lt jg kx b ky mb lb mc le md li me lm mf lq nm ly lz ma bi translated">计算新空间中向量对之间的点积</li></ul><p id="9bef" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">内核技巧是一个函数，它允许我们直接获得新空间中向量之间的点积结果，而不必将每个单独的向量投影到新空间中</strong>。这将节省我们大量的时间和计算。更正式地说:</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/ea802c53eb508b8048d74c5ad545afc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*locOoRNz7FBEb0HQT052xg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">内核技巧(图片由作者提供)</p></figure><p id="ab72" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是这个内核技巧函数是什么样子的呢？不要担心，你不是必须找到这个函数的人，但它们是你可以在网上找到的众所周知的函数。让我们来看看最著名的<strong class="kx jh">高斯核或者也叫径向基函数</strong>。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi no"><img src="../Images/6598e4d4e8f9bfff0a77437d4ed26ec4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZiJo2PJldVO8i7BDz-loxw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">高斯核(图片由作者提供)</p></figure><p id="ca99" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中<em class="lr"> y </em>为待优化的自由参数。</p><blockquote class="np nq nr"><p id="c527" class="kv kw lr kx b ky kz kh la lb lc kk ld ns lf lg lh nt lj lk ll nu ln lo lp lq ij bi translated">术语核可以被解释为一对例子之间的相似性函数。负号将距离度量转化为相似性得分，并且由于指数项，所得的相似性得分将落在1(对于完全相似的示例)和0(对于非常不相似的示例)之间的范围内。</p></blockquote><h2 id="6e59" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">我们来编码吧！</h2><p id="25de" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">首先，让我们使用<a class="ae jd" href="https://en.wikipedia.org/wiki/XOR_gate" rel="noopener ugc nofollow" target="_blank"> xor </a>函数创建我们的线性不可分数据集。</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nv nw l"/></div></figure><figure class="nf ng nh ni gt is gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/866ba68ad6e146a7988cb5ca97227945.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*60TAGVDvRQHfgsykiC0WbQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">线性不可分数据集(图片由作者提供)</p></figure><p id="900c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们可以使用参数<strong class="kx jh">kernel =‘rbf’</strong>训练一个SVM，其中RBF代表径向基函数。我们可以看到分类器创建的决策边界。</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nv nw l"/></div></figure><figure class="nf ng nh ni gt is gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/d4d668e126b62e697c6d8a741d02fc0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*tS0PfwRKKMvXjy9sgTDCfw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">决策界限(图片由作者提供)</p></figure><p id="7216" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">您可以看到，SVM能够使用rbf内核正确地分离这两个类！</p><p id="ef6e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我们增加gamma的值，我们将增加训练数据对决策边界的影响。因此<strong class="kx jh">过高的gamma将产生决策边界，这些边界将形成我们训练数据的一种轮廓，从而失去泛化能力</strong>。</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nv nw l"/></div></figure><figure class="nf ng nh ni gt is gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/9c7b326c95878080477c53c0ca3c4654.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*iCJqDwDPCDzpflSrqT1B6w.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">过度拟合(图片由作者提供)</p></figure><h1 id="70eb" class="oa mh jg bd mi ob oc od ml oe of og mo km oh kn mr kp oi kq mu ks oj kt mx ok bi translated">最后的想法</h1><p id="3e4f" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">在本文中，我们看到了内核支持向量机背后的理论以及如何使用sklearn实现它们。我们看到了输出如何随着gamma参数的变化而受到影响，这为训练集数据提供了更多的权重。</p><p id="b851" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">SVM核长期以来一直是包括计算机视觉在内的许多领域中机器学习的艺术状态。它们今天仍然被广泛使用，所以了解它们是很重要的！我希望这篇文章对你有用。如果是的话，就跟着我一起看这个<em class="lr">《王牌你机器学习面试》</em>系列的下篇文章吧！😁</p><h1 id="8c39" class="oa mh jg bd mi ob oc od ml oe of og mo km oh kn mr kp oi kq mu ks oj kt mx ok bi translated">结束了</h1><p id="e546" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated"><em class="lr">马赛洛·波利蒂</em></p><p id="c15d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" href="https://www.linkedin.com/in/marcello-politi/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>，<a class="ae jd" href="https://twitter.com/_March08_" rel="noopener ugc nofollow" target="_blank"> Twitter </a>，<a class="ae jd" href="https://march-08.github.io/digital-cv/" rel="noopener ugc nofollow" target="_blank"> CV </a></p></div></div>    
</body>
</html>