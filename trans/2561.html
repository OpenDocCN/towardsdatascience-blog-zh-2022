<html>
<head>
<title>XGBoost Alternative Base Learners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XGBoost备选基础学习者</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/xgboost-alternative-base-learners-a2dc72d97e64#2022-06-03">https://towardsdatascience.com/xgboost-alternative-base-learners-a2dc72d97e64#2022-06-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6dcc" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">dart、gblinear和XGBoost随机森林简介</h2></div><h1 id="87ac" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">介绍</h1><p id="6e72" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">XGBoost是“极限梯度提升”的缩写，是处理表格数据的最强的机器学习算法之一，由于成功赢得了众多Kaggle比赛，因此当之无愧的声誉。</p><p id="a1cd" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">XGBoost是一种通常由决策树组成的集成机器学习算法。组成XGBoost的决策树分别被称为<em class="mb"> gbtree </em>，是“梯度增强树”的缩写。XGBoost集成中的第一个决策树是基础学习器，所有后续的树都从它的错误中学习。</p><p id="68bc" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">尽管决策树由于其出色的集成分数而通常被首选为基础学习器，但是在某些情况下，备选的基础学习器可能会胜过它们。xd boost包括<em class="mb"> gblinear </em>、<em class="mb"> dart </em>和XGBoost Random Forests作为备选的基础学习器，我们将在本文中探讨所有这些。</p><p id="2022" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这篇文章是基于我的书第8章<a class="ae mc" href="https://www.amazon.com/Hands-Gradient-Boosting-XGBoost-scikit-learn/dp/1839218355" rel="noopener ugc nofollow" target="_blank">用XGBoost和Scikit-learn </a>实践梯度增强的新例子。鼓励不熟悉XGBoost的读者查看我以前的文章，<a class="ae mc" rel="noopener" target="_blank" href="/getting-started-with-xgboost-in-scikit-learn-f69f5f470a97">在scikit-learn中开始使用XGBoost</a>。下面的所有代码都可以在<a class="ae mc" href="https://colab.research.google.com/drive/16Nh7x5ZpP49oOhCcTBa_hMHppD8cd6CC?usp=sharing" rel="noopener ugc nofollow" target="_blank">这个Colab笔记本</a>里找到。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi md"><img src="../Images/93a03aabc468cfdec4a90017de81123d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P-FY-BYIsTAZYPlQoiaPFw.jpeg"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">照片由邓肯C，Flickr<a class="ae mc" href="https://www.flickr.com/photos/duncan/99863704" rel="noopener ugc nofollow" target="_blank">https://www.flickr.com/photos/duncan/99863704</a></p></figure><h1 id="9b93" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">什么是基础学习者？</h1><p id="4748" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">理解基础学习者首先需要理解助推。<a class="ae mc" href="https://www.cis.upenn.edu/~mkearns/papers/boostnote.pdf" rel="noopener ugc nofollow" target="_blank">根据Michael Kearns 1998年的文章《关于假设增强的思考》，</a>增强器是一种机器学习算法，旨在将弱学习者转化为强学习者。这是什么意思？</p><p id="0a28" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">假设一个数据集将所有输出分类为红色或蓝色，您构建一个决策树，该决策树的正确预测率为50%。显然，这并不比随机猜测好多少。但是如果决策树51%的时候是正确的呢？假设真正的学习已经发生，这个弱学习者可以通过建立在已经发生的学习之上而转变成强学习者。</p><p id="d31e" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">所有的提升组合都是从基础学习者开始的。它被称为“基础”是因为它排在第一位，被称为“学习者”是因为集合中的所有后续模型都从它的结果中学习。</p><p id="2685" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在XGBoost中，在构建第一个决策树模型(称为基础学习器)之后，第二个模型根据第一个模型的错误(也称为残差)进行训练。然后根据第二个模型的误差训练第三个模型。然后，第四个模型根据第三个模型的错误进行训练，以此类推，大约一百棵树。</p><p id="092e" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">与随机森林不同，随机森林采用多个决策树的平均值，XGBoost基于先前树的错误构建新的决策树。随着训练的继续，XGBoost变得更加精确，因为误差随着集合的增长而被校正。</p><p id="8871" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">Boosting是一个通用的概念，所以有各种各样的boosting风格，比如在XGBoost之前风靡一时的AdaBoost。类似地，基础学习器是一个通用的概念，因此除了决策树之外，还可以使用不同的基础学习器。</p><p id="97de" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在继续之前，我想强调的是，强化能让弱学习者变成强学习者。如果基础学习者太强，在随后的几轮中学习可能会受到限制，并且收获最多也是微乎其微。一个基础学习者当然可以做得比51%更好才有效，但是当重要的学习发生在第一轮训练之后时，梯度推进算法工作得最佳。</p><h1 id="09e4" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">数据和代码准备</h1><p id="bb47" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">打开一个<a class="ae mc" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a>开始在实践中使用XGBoost基础学习者。我们将使用佩斯·r·凯利和罗纳德·巴里从美国人口普查中提取的<a class="ae mc" href="https://developers.google.com/machine-learning/crash-course/california-housing-data-description" rel="noopener ugc nofollow" target="_blank">加州住房数据集</a>。插入下面的代码片段，从Colab提供的<em class="mb"> sample_data </em>文件夹中加载数据，然后按Shift-Enter运行单元格。</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="3602" class="my kj it mu b gy mz na l nb nc">import pandas as pd<br/>df=pd.read_csv('/content/sample_data/california_housing_train.csv')<br/>df.head()</span></pre><p id="33ee" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">加州住房数据的标准是使用机器学习来预测最后一列，即<em class="mb"> median_house_value </em>，如下图所示</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nd"><img src="../Images/13f3e27a32cd331efff0b4c72ef7b909.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QdvY2S1wMh5g71E_9HZ50g.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">加州住房数据框架。作者提交的照片。</p></figure><p id="c32e" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">现在，将数据分成机器学习输入X和机器学习输出y。我们将实现<em class="mb"> cross_val_score </em>来将模型分割成5个部分，以防止在测试模型时出现不均匀分割。</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="d179" class="my kj it mu b gy mz na l nb nc">y = df.iloc[:, -1]<br/>X = df.iloc[:, :-1]<br/>from sklearn.model_selection import cross_val_score</span></pre><p id="3f8d" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">最后的初步步骤是编写一个函数，该函数将机器学习模型作为输入，并将5次折叠的平均分数作为输出返回。</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="a473" class="my kj it mu b gy mz na l nb nc">def cv_score(model):<br/>  cv = cross_val_score(model, X, y)<br/>  cv_mean_score = cv.mean()<br/>  return cv_mean_score</span></pre><p id="a340" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">注意，默认情况下<em class="mb"> cross_val_score </em>将返回R2作为回归的评分标准。R2评分标准返回一个百分比，该百分比表示模型可以解释多少预测变化，因此越接近1越好。</p><h1 id="c897" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">gbtree</h1><p id="36c1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">所有XGBoost模型都有基础学习者。如果你没有对默认设置做任何改变，那么基础学习者就是<em class="mb"> gbtree </em>，是“梯度提升树”的缩写，它的操作就像一个标准的sklearn决策树。</p><p id="e14a" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">让我们使用XGBRegressor默认值对XGBoost模型进行评分。将<em class="mb">objective =</em>'<em class="mb">reg:squarederror '</em>指定为一个参数，以防止错误消息让所有人都知道XGBoost已经更改<em class="mb">objective =</em>'<em class="mb">reg:linear '</em>。</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="9458" class="my kj it mu b gy mz na l nb nc">from xgboost import XGBRegressor<br/>model = XGBRegressor(objective='reg:squarederror')<br/>cv_score(model)</span><span id="def8" class="my kj it mu b gy ne na l nb nc"><em class="mb">0.49547514549498306</em></span></pre><p id="7327" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">默认的XGBRegressor解释了预测中几乎50%的变化，这是一个相当不错的分数。</p><p id="3949" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">现在运行相同的模型，但是指定<em class="mb"> gbtree </em>作为基本学习者，这是XGBoost的默认设置。用于指定基础学习者的参数是'<em class="mb"> booster' </em>，如下面的代码片段所示。</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="87fe" class="my kj it mu b gy mz na l nb nc">model = XGBRegressor(objective=’reg:squarederror’, booster=’gbtree’)<br/>cv_score(model)</span><span id="ecfa" class="my kj it mu b gy ne na l nb nc"><em class="mb">0.49547514549498306</em></span></pre><p id="700b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">不出所料，比分完全一样。</p><p id="6a51" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">本节的目的不是深入讨论<em class="mb"> gbtree </em>的细节，而是揭示XGBoost如何使用<em class="mb"> booster </em>参数实现基础学习器，以便在接下来的章节中对其进行更改。</p><h1 id="0a24" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">飞镖</h1><p id="460f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">dart 背后的想法源于Rashmi Korlakai Vinayak和Ran Gilad-Bachrach 的文章<a class="ae mc" href="http://proceedings.mlr.press/v38/korlakaivinayak15.pdf" rel="noopener ugc nofollow" target="_blank">“DART:辍学者遇到多重加法回归树”。</a></p><p id="3de2" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">基础学习器<em class="mb"> dart </em>和基础学习器<em class="mb"> gbtree </em>都是梯度提升树，这意味着<em class="mb"> dart </em>继承了所有<em class="mb"> gbtree </em>的参数。主要区别是<em class="mb">镖</em>移除树木，称为脱落，以帮助防止过度拟合。</p><p id="5bd3" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">要将<em class="mb"> dart </em>实现为XGBoost的基础学习器，请按如下方式设置参数<em class="mb">booster =</em>'<em class="mb">dart '</em>。</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="fd0f" class="my kj it mu b gy mz na l nb nc">model = XGBRegressor(objective='reg:squarederror', booster='dart')<br/>cv_score(model)</span><span id="ed07" class="my kj it mu b gy ne na l nb nc"><em class="mb">0.49547514549498306</em></span></pre><p id="0dd0" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">如果你留心的话，你可能会惊讶于<em class="mb"> dart </em>给出的分数与<em class="mb"> gbtree </em>给出的分数完全相同。据推测，该算法实际上并没有丢弃树，这违背了使用<em class="mb"> dart </em>的目的。为确保至少有一棵树被丢弃，设置参数<em class="mb"> one_drop=1 </em>如下。</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="cf5c" class="my kj it mu b gy mz na l nb nc">model = XGBRegressor(objective='reg:squarederror', booster='dart', one_drop=1)<br/>cv_score(model)</span><span id="d1c3" class="my kj it mu b gy ne na l nb nc"><em class="mb">0.48632337772457224</em></span></pre><p id="2b49" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">XGBoost使用许多默认参数来防止过度拟合，因此至少删除一棵树并不能提高分数也就不足为奇了。要点是你可以掉树，设置<em class="mb"> booster=dart </em>就是这么做的。</p><p id="43d9" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">一个重要的<em class="mb"> dart </em>调整:您可以通过将参数<em class="mb"> rate_drop </em>设置为所需的百分比来控制树木掉落的百分比，如以下代码片段所示。</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="18bc" class="my kj it mu b gy mz na l nb nc">model = XGBRegressor(booster='dart', rate_drop=0.1, objective='reg:squarederror')<br/>cv_score(model)</span><span id="dee1" class="my kj it mu b gy ne na l nb nc"><em class="mb">0.4344857301387063</em></span></pre><p id="45fe" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">尽管通过减少10%的树，分数并没有提高，但是很明显，在XGBoost中，减少树数是防止过度拟合的有效工具。</p><p id="4fe8" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">有关<em class="mb">飞镖</em>的更多信息，请查看<a class="ae mc" href="https://xgboost.readthedocs.io/en/stable/tutorials/dart.html" rel="noopener ugc nofollow" target="_blank">飞镖助推器XGBoost官方文档页面</a>。</p><h2 id="8e7d" class="my kj it bd kk nf ng dn ko nh ni dp ks lj nj nk ku ln nl nm kw lr nn no ky np bi translated">gblinear</h2><p id="f6be" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">决策树是非线性数据的理想选择，但在某些情况下，数据本身可能是线性的，或者最好用线性算法建模。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nq"><img src="../Images/834532b933daedfd2ca7cd12751775bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hFdJRRJh0oig0ZIs7wXc8A.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">3.0&lt;<a class="ae mc" href="https://creativecommons.org/licenses/by/3.0" rel="noopener ugc nofollow" target="_blank">https://creativecommons.org/licenses/by/3.0</a>&gt;gyassinemrabettalk✉，CC，通过维基共享。<a class="ae mc" href="https://commons.wikimedia.org/wiki/File:Linear_programming_graphical_solution.png" rel="noopener ugc nofollow" target="_blank">https://commons . wikimedia . org/wiki/File:Linear _ programming _ graphical _ solution . png</a></p></figure><p id="4e12" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">XGBoost包含了<em class="mb"> gblinear </em>作为处理线性数据的基础学习器。boosting实现是相同的，这意味着每个模型都是根据以前模型的误差进行训练的，只是在这种情况下，模型本身是线性的。</p><p id="f3e9" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">检查数据是否线性的一个好方法是应用线性回归，如下面的代码片段所示。</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="3e98" class="my kj it mu b gy mz na l nb nc">from sklearn.linear_model import LinearRegression<br/>model = LinearRegression()<br/>cv_score(model)</span><span id="4c14" class="my kj it mu b gy ne na l nb nc"><em class="mb">0.54829300976933</em></span></pre><p id="fc3b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这是迄今为止最强的得分，表明加州住房数据可以很好地用线性模型来表示。</p><p id="92ad" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">现在让我们看看XGBoost如何使用<em class="mb"> gblinear </em>作为基础模型。</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="283f" class="my kj it mu b gy mz na l nb nc">model=XGBRegressor(booster='gblinear',objective='reg:squarederror')<br/>cv_score(model)</span><span id="1846" class="my kj it mu b gy ne na l nb nc"><em class="mb">0.40932317082444925</em></span></pre><p id="069e" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">令人有点失望的是，对于加州住房数据集，gblinear  R2分数比线性回归和XGBoost树基学习器差。</p><p id="a8e0" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在我的<a class="ae mc" href="https://www.amazon.com/Hands-Gradient-Boosting-XGBoost-scikit-learn/dp/1839218355/" rel="noopener ugc nofollow" target="_blank"> XGBoost书</a>中，我生成了一个随机散射的线性数据集，并且<em class="mb"> gblinear </em>在第五位小数上的表现优于<em class="mb"> LinearRegression </em>！在下面的截图中，我使用了RMSE(均方根误差)评分标准，所以越低越好。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nr"><img src="../Images/c8fc0577f631e19b6924281915e0f868.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lZiWZDvXzXV9Mlp9sr6OcQ.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated"><a class="ae mc" href="https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/blob/master/Chapter08/Alternative_Base_Learners.ipynb" rel="noopener ugc nofollow" target="_blank">摘自Jupyter笔记本</a>发表在<a class="ae mc" href="https://www.amazon.com/Hands-Gradient-Boosting-XGBoost-scikit-learn/dp/1839218355/" rel="noopener ugc nofollow" target="_blank">用XGBoost和Scikit-learn </a>实际操作梯度增强。作者提交的照片。</p></figure><p id="13b2" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在我看来，当使用一系列线性算法时，值得尝试一下gblinear 。</p><p id="2227" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">欲了解更多关于<em class="mb"> gblinear </em>的信息，请查阅<a class="ae mc" href="https://xgboost.readthedocs.io/en/stable/parameter.html?highlight=gblinear#parameters-for-linear-booster-booster-gblinear" rel="noopener ugc nofollow" target="_blank">xgb boost关于线性助力器</a>参数的文档。</p><h1 id="4cf6" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">XGBoost随机森林</h1><p id="fd31" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">存在XGBoost随机森林的事实令人敬畏，也令人惊讶。在机器学习集成方面，随机森林是XGBoost的主要竞争对手之一，这些机器学习集成通常在默认参数下表现良好。回想一下，随机森林也是由许多决策树构建的，但是它们取所有树的平均值，而XGBoost训练树的误差(残差)。</p><p id="e889" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">XGBoost随机森林实际上有两种风格:1)它们可能被用作基础学习者；以及2)它们本身可以用作算法。</p><p id="10e4" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">虽然XGBoost随机森林作为基础学习者听起来或多或少势不可挡，但请记住第一节的教训:<strong class="lc iu">助推器是用来将弱学习者转化为强学习者的。</strong></p><p id="0c8b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">随机森林通常是强大的基础学习者，因此boosting算法在后续的训练回合中可能会发现有限的增益。然而，可能会有随机森林基础学习者闪耀的边缘案例，在机器学习中，实验胜过一切。</p><p id="f019" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">XGBoost随机森林基学习器通过不同参数实现，<em class="mb"> num_parallel_trees </em>。换句话说，您可以指定您希望在每轮boosting中携带多少棵平行树，这些平行树组合起来，在每轮训练中为您提供一个随机的森林分数。</p><p id="cfef" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">例如，具有10个梯度提升树的随机森林基础学习器意味着基础学习器是10树随机森林，并且每一轮后续训练都包括10树随机森林。</p><p id="0dee" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">让我们用10个梯度增强树实现一个随机森林基学习器，如下所示:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="f566" class="my kj it mu b gy mz na l nb nc">model = XGBRegressor(booster='gbtree', num_parallel_tree=10, objective='reg:squarederror')<br/>cv_score(model)</span><span id="88d9" class="my kj it mu b gy ne na l nb nc"><em class="mb">0.4954762095888313</em></span></pre><p id="f91c" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在这种情况下，除了在运行时，添加并行树不会产生显著的差异，因为树的数量是运行时的10倍。</p><p id="6c69" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">接下来让我们试试XGBoost随机森林算法。这些不是备选的基础学习者，而是备选的XGBoost模型。这些是XGBoost构建的随机森林，它们作为单独的类存在，如<em class="mb"> XGBRFRegressor </em>和<em class="mb"> XGBRFClassifier </em>。我们需要前者，因为我们正在处理回归。</p><p id="8bca" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在应用下面的代码之前，请注意，在实现<em class="mb"> XGBRFRegressor </em>和<em class="mb"> XGBRFClassifier时，不会发生任何提升。相反，他们使用bagging (bootstrap aggregation ),这是随机森林中的标准。要了解更多信息，请查看关于XGBoost随机森林的官方文档。</em></p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="1d44" class="my kj it mu b gy mz na l nb nc">from xgboost import XGBRFRegressor<br/>model = XGBRFRegressor(objective='reg:squarederror')<br/>cv_score(model)</span><span id="3f5b" class="my kj it mu b gy ne na l nb nc"><em class="mb">0.3533693725331441</em></span></pre><p id="f8fa" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这不是最好的成绩，但也不是最差的。我们来对比一下sklearn的<em class="mb"> RandomForestRegressor </em>。</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="9daf" class="my kj it mu b gy mz na l nb nc">from sklearn.ensemble import RandomForestRegressor<br/>model = RandomForestRegressor()<br/>cv_score(model)</span><span id="e224" class="my kj it mu b gy ne na l nb nc"><em class="mb">0.3827582499069725</em></span></pre><p id="4e8c" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">根据我的经验，RandomForestRegressor 在大多数时候都比<em class="mb"> XGBRFRegressor </em>表现更好。然而，随着时间的推移，我已经看到XGBoost的随机森林有所改进，我喜欢不时地尝试一下，看看会发生什么。</p><h1 id="3241" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">结论</h1><p id="7d00" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在本文中，您了解了基础学习者在提升算法中的重要作用。此外，您已经看到了XGBoost如何默认使用<em class="mb"> gbtree </em>作为它的基本学习器。值得注意的是，您已经使用<em class="mb"> booster </em>和<em class="mb"> num_parallel_tree </em>参数实现了XGBoost备选基础学习器，以应用<em class="mb"> dart </em>、<em class="mb"> gblinear </em>和XGBoost随机森林。XGBoost随机森林类是作为奖励提供的。</p><p id="3cca" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">根据我的经验，我大部分时间使用XGBoost缺省值<em class="mb"> gbtree </em>,因为它通常会产生最好的结果。我的建议是尝试<em class="mb"> gblinear </em>作为线性回归的替代方案，如果您的XGBoost模型过度拟合，并且您认为删除树可能会有所帮助，请尝试<em class="mb"> dart </em>。最后，享受XGBoost随机森林带来的乐趣，因为它们可能会改进，并且当创造性地与其他参数一起使用时，可能会带来小的收益。</p><p id="9275" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">有关调优XGBoost备选基础模型的更多信息，请查看关于<a class="ae mc" href="https://xgboost.readthedocs.io/en/stable/tutorials/dart.html" rel="noopener ugc nofollow" target="_blank"> dart </a>、<a class="ae mc" href="https://xgboost.readthedocs.io/en/stable/parameter.html?highlight=gblinear#parameters-for-linear-booster-booster-gblinear" rel="noopener ugc nofollow" target="_blank"> gblinear </a>和<a class="ae mc" href="https://xgboost.readthedocs.io/en/stable/tutorials/rf.html" rel="noopener ugc nofollow" target="_blank"> XGBoost随机森林</a>、T26】的官方文档。</p><p id="623e" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">编码快乐！</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi ns"><img src="../Images/c75b8fc0e372a25ace5405d08242b43c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_5zQeNoGrjoYUTbApfj-rw.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">作者提交的照片。</p></figure><p id="cd0e" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><em class="mb">作者科里·韦德是伯克利编码学院</em> <a class="ae mc" href="https://www.berkeleycodingacademy.com/" rel="noopener ugc nofollow" target="_blank"> <em class="mb">的主任和创始人，他在这里向来自世界各地的青少年教授Python、数据科学、机器学习&amp;人工智能。</em></a></p></div></div>    
</body>
</html>