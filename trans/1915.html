<html>
<head>
<title>Implementing SegFormer in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在PyTorch中实现SegFormer</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-segformer-in-pytorch-8f4705e2ed0e#2022-05-03">https://towardsdatascience.com/implementing-segformer-in-pytorch-8f4705e2ed0e#2022-05-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/f4623c8b642bae84e02726b3ef6186a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H6HvF16oGKKDCygayHdJnA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><div class=""/><div class=""><h2 id="f7f4" class="pw-subtitle-paragraph kf jh ji bd b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw dk translated"><em class="kx">一种快速、高效、轻量级的图像分割模型</em></h2></div><p id="7f57" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你好。！今天我们将看看如何在PyTorch中实现SegFormer，这是在<a class="ae lu" href="https://arxiv.org/abs/2105.15203" rel="noopener ugc nofollow" target="_blank"> SegFormer:用变形器进行语义分割的简单有效的设计</a>中提出的。</p><p id="0cce" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里的代码是<a class="ae lu" href="https://github.com/FrancescoSaverioZuppichini/SegFormer" rel="noopener ugc nofollow" target="_blank"/>，这篇文章的互动版本可以从<a class="ae lu" href="https://github.com/FrancescoSaverioZuppichini/SegFormer/blob/main/README.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><p id="c7ae" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们开始吧！</p><p id="d132" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">提出了一种新的基于变换的图像分割模型。即使“变形金刚”是时下的流行语，模型本身也只有基本的注意机制。该模型有两个主要优点，首先<em class="lv"> SegFormer包括一个新颖的分级结构的变压器编码器，它输出多尺度特征</em>。然后，<em class="lv">不需要位置编码，从而避免了当测试分辨率不同于训练时导致性能下降的位置代码插值</em>。</p><p id="0ed6" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有趣的是，我们正在研究中倒退，这两个优点从一开始就存在于convnet中，我们将看到SegFormer最终只是一个convnet + attention。</p><p id="dc4c" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下图显示了SegFormer在ADE20K数据集上针对不同模型/大小的性能，他们有sota。</p><figure class="lx ly lz ma gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lw"><img src="../Images/1e87171174a4bd2c0548cf1ff26b6f9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AAh9fPkJ1-5AtmsM.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片由<a class="ae lu" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xie%2C+E" rel="noopener ugc nofollow" target="_blank">恩泽</a>、<a class="ae lu" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+W" rel="noopener ugc nofollow" target="_blank">王文海</a>、<a class="ae lu" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu%2C+Z" rel="noopener ugc nofollow" target="_blank">余</a>、<a class="ae lu" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Anandkumar%2C+A" rel="noopener ugc nofollow" target="_blank">阿尼玛</a>、<a class="ae lu" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Alvarez%2C+J+M" rel="noopener ugc nofollow" target="_blank">何塞·阿尔瓦雷斯</a>、<a class="ae lu" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Luo%2C+P" rel="noopener ugc nofollow" target="_blank">平罗</a></p></figure><p id="4113" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它比老式的FCN-R50要好，速度快两倍。既然它少了24次翻牌，我想知道为什么它的速度只有两倍。</p><h1 id="1f34" class="mb mc ji bd md me mf mg mh mi mj mk ml ko mm kp mn kr mo ks mp ku mq kv mr ms bi translated">体系结构</h1><p id="f968" class="pw-post-body-paragraph ky kz ji la b lb mt kj ld le mu km lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">该模型是一个典型的编码器-解码器/主干-颈部。附加一个头部来预测最终的分割掩模。</p><figure class="lx ly lz ma gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi my"><img src="../Images/717c79f59493577fb67be9bb65a9d547.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6O6ZyCVgs4-7XSDJ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片由<a class="ae lu" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xie%2C+E" rel="noopener ugc nofollow" target="_blank">恩泽</a>、<a class="ae lu" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+W" rel="noopener ugc nofollow" target="_blank">、</a>、<a class="ae lu" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu%2C+Z" rel="noopener ugc nofollow" target="_blank">余</a>、<a class="ae lu" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Anandkumar%2C+A" rel="noopener ugc nofollow" target="_blank">阿尼玛</a>、<a class="ae lu" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Alvarez%2C+J+M" rel="noopener ugc nofollow" target="_blank">何塞</a>、<a class="ae lu" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Luo%2C+P" rel="noopener ugc nofollow" target="_blank">平罗</a></p></figure><p id="62c6" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将以自下而上的方式实现它，从解码器内部的最低模块开始。</p><p id="0efd" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">报纸上的图像是错误的🤦，我不明白为什么有评论者指出，也许我错了。在官方的<a class="ae lu" href="https://github.com/NVlabs/SegFormer/blob/master/mmseg/models/backbones/mix_transformer.py" rel="noopener ugc nofollow" target="_blank">实现中</a>没有第一次补丁嵌入。重叠补丁合并块(紫色的那个)应该在自我效能注意块之前。</p><p id="121c" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它应该是这样的:</p><figure class="lx ly lz ma gt iv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/dc6419b95449cdb895899d0560ca7131.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/0*o2wXWoNYJzclQQ87.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="703a" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">用一点photoshop</p><figure class="lx ly lz ma gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/409cf88d15c23d6555235b4a54576d5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/0*kXAg8PJNyTgtRzLW.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="fed0" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">参见处的代码<a class="ae lu" href="https://github.com/NVlabs/SegFormer/blob/9454025f0e74acbbc19c65cbbdf3ff8224997fe3/mmseg/models/backbones/mix_transformer.py#L318" rel="noopener ugc nofollow" target="_blank"/></p><h1 id="1c92" class="mb mc ji bd md me mf mg mh mi mj mk ml ko mm kp mn kr mo ks mp ku mq kv mr ms bi translated">解码器</h1><p id="be54" class="pw-post-body-paragraph ky kz ji la b lb mt kj ld le mu km lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">使用的解码器叫做<code class="fe nb nc nd ne b">MixVisionTransformer</code> ( <code class="fe nb nc nd ne b">MiT</code>)，另一个<code class="fe nb nc nd ne b">ViT</code>，中间加了一些随机的东西，我们就叫它<code class="fe nb nc nd ne b">SegFormerDecoder</code>。让我们从积木本身的第一个独立组件<code class="fe nb nc nd ne b">OverlapPatchMerging</code>开始。</p><h2 id="b102" class="nf mc ji bd md ng nh dn mh ni nj dp ml lh nk nl mn ll nm nn mp lp no np mr nq bi translated">重叠合并</h2><figure class="lx ly lz ma gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/31c493f18dadce88a31bea6d5ca08757.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/0*NyYJHULluA-GHkBR.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="bd9f" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe nb nc nd ne b">OverlapPatchMerging</code>块可以用一个<code class="fe nb nc nd ne b">stride</code>小于<code class="fe nb nc nd ne b">kernel_size</code>的卷积层来实现，因此它与不同的面片重叠。这与几年前提出的使用大于1的<code class="fe nb nc nd ne b">stride</code>来减少输入的空间维度是一样的。在<code class="fe nb nc nd ne b">SegFormer</code>中，conv层之后是层规范。</p><p id="e93d" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于PyTorch中的<code class="fe nb nc nd ne b">nn.LayerNorm</code>适用于形状为<code class="fe nb nc nd ne b">batch, ...., channels</code>的张量，我们可以创建一个<code class="fe nb nc nd ne b">LayerNorm2d</code>，首先将<code class="fe nb nc nd ne b">channels</code>轴与最后一个轴交换，然后应用层规范，并将其交换回来。我将使用<code class="fe nb nc nd ne b"><a class="ae lu" href="https://github.com/arogozhnikov/einops" rel="noopener ugc nofollow" target="_blank">einops</a></code>使代码更具可读性</p><figure class="lx ly lz ma gt iv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="d72b" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那么我们的<code class="fe nb nc nd ne b">OverlapPatchMerging</code>只是一个conv层，后面跟着我们的层范数</p><figure class="lx ly lz ma gt iv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><h2 id="8eed" class="nf mc ji bd md ng nh dn mh ni nj dp ml lh nk nl mn ll nm nn mp lp no np mr nq bi translated">高效的自我关注</h2><figure class="lx ly lz ma gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/4bd99465ab40d2c155eb18af946faabc.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/0*fIS2KzYBMIgnhkOY.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="2165" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们都知道注意力有一个平方复杂度<code class="fe nb nc nd ne b">O(N^2)</code>,而在我们的例子中<code class="fe nb nc nd ne b">N=H*W</code>。我们可以将<code class="fe nb nc nd ne b">N</code>减少<code class="fe nb nc nd ne b">R</code>的一个因子，复杂度变成<code class="fe nb nc nd ne b">O(N^2/R)</code>。一个简单的方法是平坦的空间维度，并使用线性层。</p><figure class="lx ly lz ma gt iv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="7dc6" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将空间大小减少了<code class="fe nb nc nd ne b">r=4</code>，因此在每个维度上减少了<code class="fe nb nc nd ne b">2</code>(<code class="fe nb nc nd ne b">height</code>和<code class="fe nb nc nd ne b">width</code>)。如果你考虑一下，你可以用一个<code class="fe nb nc nd ne b">kernel_size=r</code>和一个<code class="fe nb nc nd ne b">stride=r</code>的卷积层来达到同样的效果。</p><p id="f885" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于注意力等于<code class="fe nb nc nd ne b">softmax((QK^T/scale)V)</code>，我们需要使用约化张量计算<code class="fe nb nc nd ne b">K</code>和<code class="fe nb nc nd ne b">V</code>，否则，形状不会匹配。我们可以用PyTorch的<code class="fe nb nc nd ne b">MultiheadAttention</code>来计算注意力。</p><figure class="lx ly lz ma gt iv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><pre class="lx ly lz ma gt nt ne nu nv aw nw bi"><span id="58f5" class="nf mc ji ne b gy nx ny l nz oa">torch.Size([1, 8, 64, 64])</span></pre><h2 id="1c10" class="nf mc ji bd md ng nh dn mh ni nj dp ml lh nk nl mn ll nm nn mp lp no np mr nq bi translated">MixMLP</h2><figure class="lx ly lz ma gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/df312c1587656df9836c4de7318fce0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/0*bz0RIBXYGTURlDvc.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="6281" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">细心的读者可能已经注意到我们没有使用位置编码。SegFormer使用了一个<code class="fe nb nc nd ne b">3x3</code>深度conv。引用论文<em class="lv">我们认为位置编码对于语义分割是不必要的。相反，我们引入了混合FFN，它考虑了零填充的影响，以泄漏位置信息</em>。我不知道这意味着什么，所以我们会认为这是理所当然的。</p><p id="0404" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我很确定它被称为Mix，因为它使用<code class="fe nb nc nd ne b">3x3</code> conv来混合信息。</p><p id="eb43" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">图层由<code class="fe nb nc nd ne b">dense layer</code>-&gt;-<code class="fe nb nc nd ne b">3x3 depth-wise conv</code>-&gt;-<code class="fe nb nc nd ne b">GELU</code>-&gt;-<code class="fe nb nc nd ne b">dense layer</code>组成。像在ViT中，这是一个逆瓶颈层，信息在中间层展开。</p><figure class="lx ly lz ma gt iv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><h1 id="6b12" class="mb mc ji bd md me mf mg mh mi mj mk ml ko mm kp mn kr mo ks mp ku mq kv mr ms bi translated">编码器(变压器)模块</h1><figure class="lx ly lz ma gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/43d9d1bb5b38c3242c4bc7a79db85848.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/0*4GDj9SUZwPqoBTO_.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="27e5" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们把所有东西放在一起，创建我们的编码器模块。我们将遵循一个更好的(imho)命名惯例，我们称<code class="fe nb nc nd ne b">SegFormerEncoderBlock</code>具有自我关注和混合fpn的部分和<code class="fe nb nc nd ne b">SegFormerEncoderStage</code>整个重叠片合并+ N x <code class="fe nb nc nd ne b">SegFormerEncoderBlock</code></p><p id="3a15" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">与<code class="fe nb nc nd ne b">ViT</code>非常相似，我们有跳过连接和标准化层+随机深度，也称为丢弃路径，(我有一篇关于它的<a class="ae lu" rel="noopener" target="_blank" href="/implementing-stochastic-depth-drop-path-in-pytorch-291498c4a974">文章</a>)。</p><figure class="lx ly lz ma gt iv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><pre class="lx ly lz ma gt nt ne nu nv aw nw bi"><span id="e585" class="nf mc ji ne b gy nx ny l nz oa">torch.Size([1, 8, 64, 64])</span></pre><p id="51e0" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">好吧，让我们创造一个舞台。我不知道为什么，他们在最后应用层规范，所以我们也这样做:)</p><figure class="lx ly lz ma gt iv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="a928" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后的<code class="fe nb nc nd ne b">SegFormerEncoder</code>由多个阶段组成。</p><figure class="lx ly lz ma gt iv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="7e55" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我添加了函数<code class="fe nb nc nd ne b">chunks</code>来保持代码的整洁。它是这样工作的</p><figure class="lx ly lz ma gt iv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><pre class="lx ly lz ma gt nt ne nu nv aw nw bi"><span id="cefb" class="nf mc ji ne b gy nx ny l nz oa">[[1, 2], [3, 4, 5]]</span></pre><p id="57e6" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这很方便，因为<code class="fe nb nc nd ne b">drop_probs</code>是一个列表，包含每个阶段的块的下落路径概率，我们需要将一个列表和正确的值传递给每个阶段。</p><p id="e5f9" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从编码器中，我们返回一个内部特征列表，每个阶段一个。</p><h1 id="20e8" class="mb mc ji bd md me mf mg mh mi mj mk ml ko mm kp mn kr mo ks mp ku mq kv mr ms bi translated">解码器/颈部</h1><p id="98d9" class="pw-post-body-paragraph ky kz ji la b lb mt kj ld le mu km lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">幸运的是，解码器/neck的图片与原始代码相匹配。他们称解码器部分为<code class="fe nb nc nd ne b">MLP Layer</code>。</p><figure class="lx ly lz ma gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ob"><img src="../Images/7635a26c35b71b9b91ffd5c7a9d8ae56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tbk79jewsgXpHh3R.png"/></div></div></figure><p id="d474" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它所做的非常简单，它采用大小为<code class="fe nb nc nd ne b">batch, channels_i, height_i, width_i</code>的<code class="fe nb nc nd ne b">F</code>特征，并输出相同空间和通道大小的<code class="fe nb nc nd ne b">F'</code>特征。空间大小固定为<code class="fe nb nc nd ne b">first_features_spatial_size / 4</code>。在我们的例子中，由于我们的输入是一个<code class="fe nb nc nd ne b">224x224</code>图像，输出将是一个<code class="fe nb nc nd ne b">56x56</code>遮罩。</p><p id="f3ec" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，单个<code class="fe nb nc nd ne b">SegFormerDecoderBlock</code>包含一个上采样层(用于空间维度)和一个conv层(用于通道)。需要<code class="fe nb nc nd ne b">scale_factor</code>参数来告诉它我们想要对特征进行多大程度的上采样。</p><figure class="lx ly lz ma gt iv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="37c6" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这种情况下，我们没有阶段，所以我们的<code class="fe nb nc nd ne b">SegFormerDecoder</code>只是一个块列表。它获取要素列表，并返回具有相同空间大小和通道的新要素列表。</p><figure class="lx ly lz ma gt iv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><h1 id="0645" class="mb mc ji bd md me mf mg mh mi mj mk ml ko mm kp mn kr mo ks mp ku mq kv mr ms bi translated">SegFormer头</h1><figure class="lx ly lz ma gt iv gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/5d7e22dda02f613d38a4492dc954532e.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/0*j4iX6A801tCjFNC2.png"/></div></figure><p id="52e1" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们快到了！解码器的功能在通道轴上串联(记住它们都具有相同的通道和空间维度)。然后，它们被传递到分段头，以将它们从<code class="fe nb nc nd ne b">channels * number of features</code>减少到<code class="fe nb nc nd ne b">channels</code>。最后，密集层输出最终分割。</p><figure class="lx ly lz ma gt iv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><h1 id="7dd0" class="mb mc ji bd md me mf mg mh mi mj mk ml ko mm kp mn kr mo ks mp ku mq kv mr ms bi translated">SegFormer</h1><p id="c301" class="pw-post-body-paragraph ky kz ji la b lb mt kj ld le mu km lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">嗯，我们的最终模型只是<code class="fe nb nc nd ne b">encoder + decoder + head</code>。容易的事</p><figure class="lx ly lz ma gt iv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="7658" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们试试吧！</p><figure class="lx ly lz ma gt iv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><pre class="lx ly lz ma gt nt ne nu nv aw nw bi"><span id="be4b" class="nf mc ji ne b gy nx ny l nz oa">torch.Size([1, 100, 56, 56])</span></pre><p id="32bb" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出是正确的，我们期望一个空间形状的遮罩<code class="fe nb nc nd ne b">image_size // 4</code>和<code class="fe nb nc nd ne b">224 // 4 = 56</code>。</p><p id="7497" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们做到了！🎉🎉🎉</p><h1 id="7812" class="mb mc ji bd md me mf mg mh mi mj mk ml ko mm kp mn kr mo ks mp ku mq kv mr ms bi translated">结论</h1><p id="adf1" class="pw-post-body-paragraph ky kz ji la b lb mt kj ld le mu km lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">在本文中，我们已经一步一步地看到了如何创建SegFormer一种快速有效的图像分割模型。</p><p id="b6ee" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢您的阅读！</p><p id="0fc6" class="pw-post-body-paragraph ky kz ji la b lb lc kj ld le lf km lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">弗朗西斯科</p></div></div>    
</body>
</html>