<html>
<head>
<title>What I learned about Graph Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我对图形神经网络的了解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-i-learned-about-graph-neural-networks-e17bb0d70a7f#2022-08-28">https://towardsdatascience.com/what-i-learned-about-graph-neural-networks-e17bb0d70a7f#2022-08-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="bbe1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">卷积和注意机制在图中的应用</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5c564a08155a332abfa3b11abb180298.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*z8w3MvtGqgqeBvL9"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">一个美学网络，图片来自<a class="ae kv" href="https://unsplash.com/photos/ZiQkhI7417A" rel="noopener ugc nofollow" target="_blank"> Alina Grubnyak </a></p></figure><p id="0fe0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">近年来，图形神经网络在机器学习领域迅速获得牵引力，变得适用于各种任务。毫无疑问，社交网络的出现在GNNs的成功中发挥了重要作用，然而它们也适用于生物学、医学和其他领域，在这些领域中，图代表了基本的实体。</p><p id="7e85" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我总是被新兴的技术和方法所吸引，因此我决定尝试并学习一些相关知识。此外，为了让知识沉淀下来，我决定写这篇文章来记下我在这次旅程中遇到的所有概念。</p><p id="63d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有条不紊地组织所有的内容可能对将来的快速复习有用，或者有希望为其他需要学习资源的人服务。</p><p id="7885" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在说的够多了，让我们从基本面开始吧！</p><h1 id="b1ce" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">什么是图？</h1><p id="ac2b" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如果你正在读这篇文章，你很可能已经知道什么是图，所以我不会在这上面花太多时间。</p><p id="5c65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图是由<strong class="ky ir">节点</strong> <em class="mp"> V </em>组成的结构<em class="mp"> G </em>，它们之间通过<strong class="ky ir">边</strong> <em class="mp"> E </em>连接。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/e9b494167fe8ef8d154d9c42191b2f51.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*gTuCwqkkdnOu_fSiZwoByg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图表示例</p></figure><p id="622e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">节点可以有<em class="mp">特性</em>，更好地描述节点本身。例如，如果节点代表人，那么他们的特征可以是年龄、性别、身高等等。</p><p id="4918" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">节点之间的边表示关系，即两个节点如果有关系就是连接的(想想Instagram上的<em class="mp">跟</em>的概念)，如果关系对称，它们可以是无向的，如果关系不对称，它们可以是有向的。</p><p id="1335" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我所说的，我不打算深入讨论这个问题，因为这不在本文的范围之内，但是为了便于阅读，我需要介绍一些术语。</p><h1 id="14f4" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">图形神经网络简介</h1><p id="ba2f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">鉴于图形结构在我们的日常生活中如此丰富(社交网络、地图、用户-产品交互……)，研究人员开始寻找能够处理它们的深度学习架构。在过去10年的DL研究中，卷积神经网络成为最幸运的架构之一，它工作于图形的一个特殊例子:图像。</p><p id="3ab4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图像可以被认为是图形的特殊情况，其中像素代表以网格组织的节点，它们的灰度/RGB值是它们的特征。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/323d51ff73ca8e3c2e0f3e7861bacf8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*QMRKMTWzWtvFlq6xaHCgKQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">网格形式的图像示例。卷积的工作原理是聚合邻域的像素值。</p></figure><p id="6eed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当您对一组像素应用卷积时，您将汇总从卷积居中的像素及其所有相邻像素获得的信息。</p><p id="8c4c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，<strong class="ky ir">您不能直接在图形</strong>上应用卷积，因为图形节点没有固有的顺序(与图像不同，图像中的像素由其在图像中的坐标唯一确定)。</p><p id="e129" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此研究人员想知道:<em class="mp">我们能推广图上的卷积运算吗？</em></p><p id="a38e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">他们提出了两类方法:</p><ul class=""><li id="5492" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated"><strong class="ky ir">频谱法</strong>:顾名思义，它们与频域有关。它们保留了卷积的严格概念，但理解起来有点棘手。尽管在数学上更合理，但由于计算成本，它们很少被使用。</li><li id="505a" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">空间方法:它们代表了光谱方法的一个体面的近似，尽管没有数学上的严格。更容易理解的是，它们基于这样一个概念，即每个节点都应该从自己和它的K跳邻居那里收集信息。</li></ul><p id="b534" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就我个人而言，我看了看光谱方法，但由于它们不是今天最常用的，我决定不给它们太多的空间，而支持空间方法，我将在这篇文章中涉及更多。</p><h1 id="b6b4" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">我们开始吧:图卷积网络</strong></h1><p id="554d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">深度学习和卷积在图上应用的第一个成功例子出现在<a class="ae kv" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank"> Kipf &amp; Welling，2017 </a>中，其中介绍了<strong class="ky ir">图卷积网络</strong>。</p><p id="2a59" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">他们算法背后的主要思想如下:</p><ul class=""><li id="cfe4" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated">将线性投影应用于节点的所有特征向量</li><li id="f083" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">你汇总它们(平均值、总和、串联…)</li><li id="ad81" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">将一个节点的投影与其相邻节点的投影组合在一起。</li></ul><p id="200e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是公式中的过程:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/4d1e3f5b5b204463c003d54491b241c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*okaaKZYkmwOfDnjbWDu09w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">方程式1，GCN层，归功于<a class="ae kv" href="https://distill.pub/2021/understanding-gnns/" rel="noopener ugc nofollow" target="_blank">https://distill.pub/2021/understanding-gnns/</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/ea26a6c0850e3d8f7c5ae6ea5710a953.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PpWNw09y_ueOcRHsUwOdhA.png"/></div></div></figure><p id="f461" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">等式1是前面列出的步骤的数学表达式。每个节点的嵌入是通过将每个邻居投影到另一个空间，平均(但也可以使用其他类型的聚合)，并将它们与节点本身的投影相结合而获得的。最后一步，像在DL中一样，是激活函数传递。</p><p id="4124" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这太棒了，不是吗？然而，当我读它的时候，我对你是如何编码的感到有点困惑。好的，我有一个图，但是我如何有效地找到每个节点的邻居，以便组合它们的特征？简单，你用<strong class="ky ir">邻接矩阵</strong>一<em class="mp">一</em>！</p><p id="180f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">邻接矩阵<em class="mp"> A </em>，大小为<em class="mp"> N </em> ( <em class="mp"> N </em>为节点数)，描述节点如何连接:<em class="mp"> A(v，u) </em> = 1如果节点<em class="mp"> v，u </em>连接。<br/>例如，如果您只想汇总一个节点的所有邻居(也称为单跳节点)的功能，您只需:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/2485cfd4cbfb66511379cd1771283d0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/format:webp/1*XPsp_csdKlY07aJzNDF1pQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">等式2，1跳功能聚合</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/7116c130d7d977d08bc72df017c448a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*WQzFKbHE-HRYIQuDqk3_-g.png"/></div></figure><p id="ce37" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，<strong class="ky ir">如果您在k步上不断迭代该过程，您将从k跳邻居</strong>聚集特征，并且上面的表达式可以容易地表达如下，其中已经添加了线性投影:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/2fca9596a5c6ed4b4ed81a8918e285c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*Gh18JC680nGxnwMcNC19cA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">方程3，矩阵形式的GCN层</p></figure><p id="2d7c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mp">请注意，原始论文没有使用简单邻接矩阵a。作者应用了一些提高性能的规范化技巧，但为了便于学习，这足以理解概念。</em></p><p id="f32f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此<strong class="ky ir">参数<em class="mp"> k </em>调节你想从</strong>学习的层数和跳数。这是一件不太明显但相当聪明的事情！但是……我该如何选择<em class="mp"> k </em>？</p><p id="6b55" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> GCNs </strong>的一个奇特之处在于，它们<strong class="ky ir">通常不是深层网络</strong>，相反，它们更可能是非常浅层的(大多数时候2层就够了！).到今天为止，为什么浅层网络工作得更好还不清楚，但这背后有一些直觉:</p><ul class=""><li id="656d" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated">如果网络是强连接的，单个节点只需几跳就可以到达大多数其他节点</li><li id="ff35" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">许多学习任务依赖于这样的假设，即来自近节点的信息比来自远节点的信息更相关</li></ul><p id="c8f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这很酷，对吧？看起来几个层(以及参数)就能达到目的！是的，但是由于图形的大小，gcn通常会处理可伸缩性问题。想想在一个有数百万个节点的图上应用等式3:邻接矩阵将是巨大的！幸运的是，有论文探索了这种大图的学习技巧，比如<a class="ae kv" href="https://arxiv.org/abs/1706.02216" rel="noopener ugc nofollow" target="_blank"> GraphSAGE </a>。</p><h1 id="a9d0" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">你能用GCNs做什么？</h1><p id="1404" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">一旦您建立了自己的网络，您就可以开始解决以下任务了:</p><ul class=""><li id="2ae1" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated"><strong class="ky ir">节点分类</strong>，即对图中每个节点的分类</li><li id="7597" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated"><strong class="ky ir">图形分类</strong>，即对整个图形的分类</li><li id="4df1" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated"><strong class="ky ir">链路预测</strong>，即预测两个节点是否连通</li><li id="46b4" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated"><strong class="ky ir">节点聚类</strong>，即根据节点的特征和/或连接性对节点集合进行分组</li></ul><p id="8537" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我发现图形网络特别吸引人的地方是它们可以在两种不同的环境中使用:</p><ul class=""><li id="60d2" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated">归纳学习:在训练时，你完全不知道测试集节点，就像你处理标准的机器学习问题一样</li><li id="eaa1" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">转导学习:在训练时，你确实能看到你的测试集节点，因为它们是你的图形结构的一部分。但是，您不能使用它们的标签来计算和最小化您的成本函数</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/1d7df6a39d61c8dbd9225545059abca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OsILFH7zi7aU4ERZtogxeA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">归纳学习(左)与直推学习(右)。在归纳设置中，数据集由多个图形组成。在直推式设置中，数据集由单个图的所有节点组成，但其中一些节点的标签被屏蔽掉了。</p></figure><p id="b4e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">直推式方法是我不习惯的，因为在我通常的ML项目中，我从不使用我的验证/测试集进行训练。然而，图学习可能也需要来自这些集合的信息，因为它们是图结构的一部分，并且它们的特征被组合来计算每个节点的嵌入！</p><p id="bced" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，如果给你一个用户网络，你的目标是预测他们是否是机器人，你可能会直推式地做:你将通过使用整个网络作为输入，对每个节点进行分类。然而，只有标签的子集(对应于训练集)将用于计算和最小化成本函数。</p><p id="5ecf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一方面，对于图分类，您通常通过归纳来学习:您的数据集由一组图(而不是单个节点)组成，您可以将这些图分为训练集、验证集和测试集。您的网络将被优化以将每个图分配到正确的类。</p><p id="dcb9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">成功的GCNs应用示例如下:</p><ul class=""><li id="18f7" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated"><a class="ae kv" href="https://arxiv.org/abs/1806.01973ù" rel="noopener ugc nofollow" target="_blank">用于网络规模推荐系统的图卷积神经网络</a></li><li id="3de6" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated"><a class="ae kv" href="https://ebooks.iospress.nl/volumearticle/2775" rel="noopener ugc nofollow" target="_blank">用于目标定位的图形神经网络</a></li><li id="0d97" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated"><a class="ae kv" href="https://arxiv.org/abs/1802.00543" rel="noopener ugc nofollow" target="_blank">用图卷积网络建模多药副作用</a></li></ul><h1 id="8d32" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">GCNs中的注意机制:图注意网络</h1><p id="96a2" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">正如我们之前解释的，GCNs学习的步骤之一包括从邻居那里聚集信息。<strong class="ky ir">这个聚合步骤可以以这样一种方式加权，以便将重要性分配给邻居</strong>，这就是<strong class="ky ir">注意机制</strong>和<a class="ae kv" href="https://arxiv.org/abs/1710.10903" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">图形注意网络</strong> </a>背后的思想。</p><p id="7522" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该方法允许每个节点了解要关注哪些邻居，并在聚集步骤中为每个邻居指定不同的关注权重。</p><p id="63fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意力层是这样工作的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/6dd1b484f649b48db76f5f8cfa91ff98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*GYP8ICqg1kevIr0jDxOcFg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">等式4，图形注意力网络中的注意力层，归功于<a class="ae kv" href="https://distill.pub/2021/understanding-gnns/" rel="noopener ugc nofollow" target="_blank">https://distill.pub/2021/understanding-gnns/</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/d7ca60103901b07dd92d98a9499b59b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DNfb0GfbD5nUx7aOqGwb_g.png"/></div></div></figure><p id="904f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基本上，<strong class="ky ir">注意力机制为网络的每一条边学习一个权重</strong>。</p><p id="09e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个例子中，我们将注意力权重的计算简化为单个注意力头的情况。然而，你可能在同一层有多个注意力头，以不同的方式关注邻居。</p><p id="6f85" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与标准的GCNs不同，<strong class="ky ir">聚合系数是动态计算的</strong>，允许网络决定收集信息的最佳方式，尽管投入了一些计算能力。</p><p id="e52f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些系数是如何计算的？这个想法是学习一个评分函数<em class="mp"> S </em>，它给每条边分配一个分数。这是通过对由给定边链接的节点的线性投影求和，将它们通过评分函数<em class="mp"> S </em>和<em class="mp"/>a激活步骤来完成的。最后，对一个给定节点的所有邻居的分数进行归一化，获得注意力权重。</p><p id="2a4f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在公式中:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/f1fcf68ff68ffbf35cab9a3958176606.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*AheOvu-Uj4zBqsFE1A9NYg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">等式5，边缘分数计算，归功于<a class="ae kv" href="https://distill.pub/2021/understanding-gnns/" rel="noopener ugc nofollow" target="_blank">https://distill.pub/2021/understanding-gnns/</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/13c70a6ef88968cb851cfc0d1debc0e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*E_6bekxtEiRzEEH4xaTFwA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">等式6，注意力分数计算，归功于<a class="ae kv" href="https://distill.pub/2021/understanding-gnns/" rel="noopener ugc nofollow" target="_blank">https://distill.pub/2021/understanding-gnns/</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/95380cac8253fb3e0a4b9e563e0b91ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OZtRjt9S7w-Qu8zxW-TYFQ.png"/></div></div></figure><p id="b306" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">实现的方式有点复杂，不过，我建议看看<a class="ae kv" href="https://github.com/gordicaleksa/pytorch-GAT" rel="noopener ugc nofollow" target="_blank">这个回购</a>。</p><h1 id="d4bd" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="73c4" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">暂时就这样了。而且只是现在。我希望在接下来的几周里浏览更多的资源，因为仍然有一些方法我还没有探索到，甚至还没有看到。</p><p id="6537" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望我让你相信图形学习是值得你花时间的。谁知道呢，也许它可以成为你下一个项目的可能选择。我花了一些时间来吸收所有这些东西，因为它不像其他DL主题那样熟悉，所以不要责怪自己，如果不是一开始一切都非常清楚(但如果是的话，也许我做得很好！).</p><p id="96bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢阅读！</p></div><div class="ab cl nq nr hu ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="ij ik il im in"><p id="41d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">除非另有说明，所有图片均为作者所有。</p><p id="07fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">访问我的<a class="ae kv" href="https://alessandropaticchio.github.io/" rel="noopener ugc nofollow" target="_blank">个人页面</a>或通过<a class="ae kv" href="https://www.linkedin.com/in/alessandro-paticchio-a3b6b7138/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我！</p></div></div>    
</body>
</html>