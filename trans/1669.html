<html>
<head>
<title>A new tool for explainable AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释人工智能的新工具</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-new-tool-for-explainable-ai-65834e757c28#2022-04-20">https://towardsdatascience.com/a-new-tool-for-explainable-ai-65834e757c28#2022-04-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8b7a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过反事实解释在Julia、Python和R中训练的模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/fe94dacb618042d90d461ac3b14ce010.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/0*-z3BzPxAXqMaXkpq.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">把9变成4。图片作者。</p></figure><p id="3651" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我在之前的<a class="ae lq" rel="noopener" target="_blank" href="/individual-recourse-for-black-box-models-5e9ed1e4b4cc">帖子</a>中介绍的反事实解释，提供了一种简单直观的方式来解释黑盒模型，而无需打开它们。尽管如此，到今天为止，只有一个开源库提供了一种统一的方法来为用Python构建和训练的模型生成和测试反事实解释(Pawelczyk et al. 2021)。这很棒，但是对于其他编程语言🥲.的用户来说用处有限</p><p id="b114" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">输入<code class="fe lr ls lt lu b"><a class="ae lq" href="https://www.paltmeyer.com/CounterfactualExplanations.jl/stable/" rel="noopener ugc nofollow" target="_blank">CounterfactualExplanations.jl</a></code>:一个Julia包，可以用来解释用Julia、Python和r开发和训练的机器学习算法，反事实解释属于更广泛的可解释人工智能范畴(XAI)。</p><p id="b7d3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">可解释的人工智能通常涉及模型，这些模型本身不可解释，但需要额外的工具才能对人类进行解释。后者的例子包括集成、支持向量机和深度神经网络。这不要与可解释的人工智能混淆，后者涉及本质上可解释和透明的模型，如通用加法模型(GAM)、决策树和基于规则的模型。</p><p id="4336" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一些人会认为，我们最好避免解释黑盒模型(Rudin 2019)，而是只关注可解释的人工智能。虽然我同意最初的努力应该总是面向可解释的模型，停止在那里将意味着错过机会，而且无论如何在达尔-E 公司的时代可能是不太现实的</p><blockquote class="lv lw lx"><p id="32ad" class="ku kv ly kw b kx ky ju kz la lb jx lc lz le lf lg ma li lj lk mb lm ln lo lp im bi translated"><em class="it">尽管[……]可解释性非常重要，并且应该加以追求，但原则上可以在不打开“黑匣子”的情况下提供解释。</em></p><p id="114c" class="ku kv ly kw b kx ky ju kz la lb jx lc lz le lf lg ma li lj lk mb lm ln lo lp im bi translated"><em class="it"> -沃希特、米特斯塔特、罗素(</em><a class="ae lq" href="https://www.paltmeyer.com/blog/posts/a-new-tool-for-explainable-ai/#ref-wachter2017counterfactual" rel="noopener ugc nofollow" target="_blank"><em class="it">2017</em></a><em class="it">)</em></p></blockquote><p id="5813" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这篇文章介绍了新的Julia包的主要功能。通过一个使用Julia中训练的模型的激励性示例，我们将看到如何轻松地调整该包，以与Python和r中训练的模型一起工作。因为这篇文章的动机也是希望吸引贡献者，所以最后一节概述了我们计划的一些令人兴奋的开发。</p><h1 id="ba76" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">图像数据🖼的反事实</h1><p id="7af6" class="pw-post-body-paragraph ku kv it kw b kx mu ju kz la mv jx lc ld mw lf lg lh mx lj lk ll my ln lo lp im bi translated">为了介绍反事实的解释，我在之前的<a class="ae lq" rel="noopener" target="_blank" href="/individual-recourse-for-black-box-models-5e9ed1e4b4cc">帖子</a>中使用了一个简单的二元分类问题。它包括一个线性分类器和一个线性可分的合成数据集，只有两个特征。这一次我们将更进一步:我们将从MNIST数据中得出反事实的解释。MNIST数据集包含60，000个28×28像素灰度图像形式的手写数字训练样本(LeCun 1998)。每个图像都与一个标签相关联，该标签指示图像所代表的数字(0–9)。</p><p id="89eb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe lr ls lt lu b"><a class="ae lq" href="https://www.paltmeyer.com/CounterfactualExplanations.jl/stable/" rel="noopener ugc nofollow" target="_blank">CounterfactualExplanations.jl</a></code>包附带了两个黑盒模型，它们被训练来预测这些数据的标签:首先，一个简单的多层感知器(MLP ),其次，一个相应的深度集成。最初由Lakshminarayanan、Pritzel和Blundell (2016)提出，深度集成实际上只是深度神经网络的集成。它们仍然是贝叶斯深度学习最流行的方法之一。有关贝叶斯深度学习的更多信息，请参见我之前的帖子:[ <a class="ae lq" rel="noopener" target="_blank" href="/go-deep-but-also-go-bayesian-ab25efa6f7b"> TDS </a>，[ <a class="ae lq" href="https://www.paltmeyer.com/blog/posts/effortsless-bayesian-dl/" rel="noopener ugc nofollow" target="_blank">博客</a>。</p><h1 id="3afc" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">黑盒模型</h1><p id="55a9" class="pw-post-body-paragraph ku kv it kw b kx mu ju kz la mv jx lc ld mw lf lg lh mx lj lk ll my ln lo lp im bi translated">虽然该包目前可以处理一些简单的分类模型，但它被设计成可以通过用户和贡献者轻松扩展。扩展包以处理自定义模型通常只涉及两个简单的步骤:</p><ol class=""><li id="0c9f" class="mz na it kw b kx ky la lb ld nb lh nc ll nd lp ne nf ng nh bi translated"><strong class="kw iu">子类型</strong>:自定义模型需要声明为包内部类型<code class="fe lr ls lt lu b">AbstractFittedModel</code>的子类型。</li><li id="9f02" class="mz na it kw b kx ni la nj ld nk lh nl ll nm lp ne nf ng nh bi translated"><strong class="kw iu">多分派</strong>:包内部函数<code class="fe lr ls lt lu b">logits</code>和<code class="fe lr ls lt lu b">probs</code>需要通过新型号类型的自定义方法进行扩展。</li></ol><p id="2216" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">实现这两个步骤的代码可以在我自己博客上相应的<a class="ae lq" href="https://www.paltmeyer.com/blog/posts/a-new-tool-for-explainable-ai/#black-box-models" rel="noopener ugc nofollow" target="_blank">帖子</a>中找到。</p><h1 id="74ca" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">反事实生成器</h1><p id="edab" class="pw-post-body-paragraph ku kv it kw b kx mu ju kz la mv jx lc ld mw lf lg lh mx lj lk ll my ln lo lp im bi translated">接下来，我们需要指定我们想要使用的反事实生成器。该软件包目前附带了两个默认生成器，都需要梯度访问:首先，由沃希特、米特斯塔特和罗素(2017)引入的通用生成器，其次，由舒特等人(2021)引入的贪婪生成器。</p><p id="b1e5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">greedy generator设计用于预测中包含不确定性的模型，如上面介绍的deep ensemble。它适用于概率(贝叶斯)模型，因为它们只在由训练样本填充的特征域区域中产生高置信度的预测。只要模型具有足够的表现力和良好的指定性，这些区域中的反事实将总是现实的和明确的，因为通过构造它们应该看起来非常类似于训练样本。其他流行的反事实解释方法，如REVISE (Joshi等人，2019年)和CLUE (Antorán等人，2020年)也在玩这个简单的想法。</p><p id="d965" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面两行代码实例化了当前问题的两个生成器:</p><pre class="kj kk kl km gt nn lu no np aw nq bi"><span id="cdba" class="nr md it lu b gy ns nt l nu nv">generic = GenericGenerator(;loss=:logitcrossentropy) <br/>greedy = GreedyGenerator(;loss=:logitcrossentropy)</span></pre><h1 id="aa0c" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">说明</h1><p id="8f51" class="pw-post-body-paragraph ku kv it kw b kx mu ju kz la mv jx lc ld mw lf lg lh mx lj lk ll my ln lo lp im bi translated">一旦指定了模型和反事实生成器，使用这个包运行反事实搜索就非常容易了。对于给定的事实(<code class="fe lr ls lt lu b">x</code>)、目标类(<code class="fe lr ls lt lu b">target</code>)和数据集(<code class="fe lr ls lt lu b">counterfactual_data</code>)，只需运行</p><pre class="kj kk kl km gt nn lu no np aw nq bi"><span id="ecf1" class="nr md it lu b gy ns nt l nu nv">generate_counterfactual(x, target, counterfactual_data, M, generic)</span></pre><p id="dd1a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">将生成结果，在本例中使用通用生成器(<code class="fe lr ls lt lu b">generic</code>)生成MLP ( <code class="fe lr ls lt lu b">M</code>)。因为我们已经指定了两个不同的黑盒模型和两个不同的反事实生成器，所以我们总共有一个模型和一个生成器的四种组合。对于这些组合中的每一个，我都使用了<code class="fe lr ls lt lu b">generate_counterfactual</code>函数来产生图1中的结果。</p><p id="dabd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在每种情况下，期望的标签转换实际上都实现了，但从人类的角度来看，可以说只有深度系综的反事实看起来像4。通用生成器在从人类角度看似乎不相关的区域产生轻微的扰动，但仍然产生可以作为4的反事实。贪婪的方法明显地瞄准了手写的9的顶部的像素，并且产生了总体上最好的结果。对于非贝叶斯MLP，一般方法和贪婪方法都会产生看起来很像对立例子的反事实:它们扰乱了图像上看似随机区域的像素。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi nw"><img src="../Images/7a4cccb6e4999eb5d522b136709b4725.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pjpox8zetlMMtjI1.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图1:对MNIST的反事实解释:把9变成4。图片作者。</p></figure><h1 id="97d1" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">语言互用性👥</h1><p id="5cbc" class="pw-post-body-paragraph ku kv it kw b kx mu ju kz la mv jx lc ld mw lf lg lh mx lj lk ll my ln lo lp im bi translated">Julia语言为编程语言互操作性提供了独特的支持。例如，分别通过<code class="fe lr ls lt lu b">RCall.jl</code>和<code class="fe lr ls lt lu b">PyCall.jl</code>调用R或Python变得非常容易。这个功能可以用来使用<code class="fe lr ls lt lu b">CounterfactualExplanations.jl</code>为用其他编程语言开发的模型生成解释。目前还没有对外国编程语言的本地支持，但是下面的例子涉及一个在<code class="fe lr ls lt lu b">R</code>训练的<code class="fe lr ls lt lu b">torch</code>神经网络，展示了这个包是多么的通用。涉及<code class="fe lr ls lt lu b">PyTorch</code>的相应例子是类似的，因此省略，但在这里<a class="ae lq" href="https://www.paltmeyer.com/CounterfactualExplanations.jl/dev/tutorials/interop/" rel="noopener ugc nofollow" target="_blank">可用</a>。</p><h1 id="4638" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">解释在R中训练的模型</h1><p id="803b" class="pw-post-body-paragraph ku kv it kw b kx mu ju kz la mv jx lc ld mw lf lg lh mx lj lk ll my ln lo lp im bi translated">我们将考虑为二进制分类任务训练的简单MLP。和以前一样，我们首先需要修改这个定制模型以用于我们的包。两个必要步骤(子类型和方法扩展)下面的代码。Logits由<code class="fe lr ls lt lu b">torch</code>模型返回，并从R环境复制到Julia范围。然后通过sigmoid函数传递logits，在Julia范围内计算概率。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">采用在R中训练的定制火炬模型用于反事实解释。</p></figure><p id="7be2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">与在Julia中训练的模型相比，我们需要在这一点上做更多的工作。由于我们的反事实生成器需要梯度访问，我们本质上需要允许我们的包与R <code class="fe lr ls lt lu b">torch</code>库通信。虽然这可能听起来令人生畏，但事实证明这是非常容易管理的:我们所要做的就是重新指定计算关于反事实损失函数的梯度的函数，以便它可以处理我们上面定义的<code class="fe lr ls lt lu b">TorchNetwork</code>类型。下面的代码实现了这一点。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">调整反事实损失函数的梯度，以对r中训练的模型使用反事实解释。</p></figure><p id="a7ad" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这就是为我们的定制R模型使用<code class="fe lr ls lt lu b">CounterfactualExplanations.jl</code>所需的所有调整。图2显示了随机选择的样本相对于r中训练的MLP的反事实路径</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi od"><img src="../Images/a1e1385f9acf217a742a05455e3f9c25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZXBXUD9LqQtEJA9d.gif"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图2:作者对R. Image中训练的模型使用通用反事实生成器的反事实路径。</p></figure><h1 id="1718" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">我们需要你！🫵</h1><p id="1b63" class="pw-post-body-paragraph ku kv it kw b kx mu ju kz la mv jx lc ld mw lf lg lh mx lj lk ll my ln lo lp im bi translated"><code class="fe lr ls lt lu b">CounterfactualExplanations.jl</code>的目标是为Julia社区和其他社区提供一个反事实解释的去处。这是一个宏伟的目标，尤其是对于一个迄今为止由一个之前对Julia没有什么经验的开发人员构建的包来说。因此，我们非常希望邀请机构群体做出贡献。如果你对可信赖的人工智能、开源社区和Julia感兴趣，请参与进来！这个包仍然处于开发的早期阶段，所以任何类型的贡献都是受欢迎的:关于核心包架构的建议，拉请求，问题，讨论，甚至只是下面的评论都将不胜感激。</p><p id="b0c7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了让您了解我们所设想的未来发展类型，以下是一个不完整的列表:</p><ol class=""><li id="4a33" class="mz na it kw b kx ky la lb ld nb lh nc ll nd lp ne nf ng nh bi translated">原生支持额外的反事实生成器和预测模型，包括那些用Python或r构建和训练的。</li><li id="7d2f" class="mz na it kw b kx ni la nj ld nk lh nl ll nm lp ne nf ng nh bi translated">用于测试、评估和基准测试的附加数据集。</li><li id="4428" class="mz na it kw b kx ni la nj ld nk lh nl ll nm lp ne nf ng nh bi translated">改进的预处理，包括对分类特性的本机支持。</li><li id="e570" class="mz na it kw b kx ni la nj ld nk lh nl ll nm lp ne nf ng nh bi translated">支持回归模型。</li></ol><p id="e41a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，如果你喜欢这个项目，但没有太多时间，那么简单地分享这篇文章或在GitHub上主演<a class="ae lq" href="https://github.com/pat-alt/CounterfactualExplanations.jl" rel="noopener ugc nofollow" target="_blank">回购</a>也将大有帮助。</p><h1 id="b569" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">进一步阅读📚</h1><p id="b7c9" class="pw-post-body-paragraph ku kv it kw b kx mu ju kz la mv jx lc ld mw lf lg lh mx lj lk ll my ln lo lp im bi translated">如果您有兴趣了解这一发展的更多信息，请随时查阅以下资源:</p><ul class=""><li id="2166" class="mz na it kw b kx ky la lb ld nb lh nc ll nd lp oe nf ng nh bi translated">打包文件:<a class="ae lq" href="https://pat-alt.github.io/CounterfactualExplanations.jl/stable" rel="noopener ugc nofollow" target="_blank">【稳定】</a><a class="ae lq" href="https://pat-alt.github.io/CounterfactualExplanations.jl/dev" rel="noopener ugc nofollow" target="_blank">【开发】</a>。</li><li id="9d1a" class="mz na it kw b kx ni la nj ld nk lh nl ll nm lp oe nf ng nh bi translated"><a class="ae lq" href="https://www.paltmeyer.com/CounterfactualExplanations.jl/stable/contributing/" rel="noopener ugc nofollow" target="_blank">投稿指南</a>。</li><li id="ea8c" class="mz na it kw b kx ni la nj ld nk lh nl ll nm lp oe nf ng nh bi translated"><a class="ae lq" href="https://github.com/pat-alt/CounterfactualExplanations.jl" rel="noopener ugc nofollow" target="_blank"> GitHub回购</a>。</li></ul><h1 id="3fc1" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">谢谢💐</h1><p id="bacb" class="pw-post-body-paragraph ku kv it kw b kx mu ju kz la mv jx lc ld mw lf lg lh mx lj lk ll my ln lo lp im bi translated">《舒特》(2021)的通讯作者丽莎·舒特<a class="ae lq" href="https://twitter.com/miouantoinette?lang=en" rel="noopener ugc nofollow" target="_blank">和</a><a class="ae lq" href="https://oatml.cs.ox.ac.uk/members/oscar_key/" rel="noopener ugc nofollow" target="_blank">给了我很大的帮助，他们为这篇文章提供了反馈，并回答了我对他们论文的一些问题。谢谢大家！</a></p><h1 id="8035" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">参考</h1><p id="3ec9" class="pw-post-body-paragraph ku kv it kw b kx mu ju kz la mv jx lc ld mw lf lg lh mx lj lk ll my ln lo lp im bi translated">安托万、哈维尔、乌曼·巴特、塔米姆·阿德尔、阿德里安·韦勒和何塞·米格尔·埃尔南德斯·洛巴托。2020."获得线索:解释不确定性估计的方法."arXiv预印本arXiv:2006.06848 。</p><p id="923c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Joshi、Shalmali、Oluwasanmi Koyejo、Warut Vijitbenjaronk、Been Kim和Joydeep Ghosh。2019."黑箱决策系统中现实的个人求助和可操作的解释."<em class="ly"> arXiv预印本arXiv:1907.09615 </em>。</p><p id="33fc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">拉克什米纳拉亚南、巴拉吉、亚历山大·普里策尔和查尔斯·布伦德尔。2016."使用深度集成的简单和可扩展的预测不确定性估计."<em class="ly"> arXiv预印本arXiv:1612.01474 </em>。</p><p id="8303" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">勒昆，扬恩。1998." MNIST手写数字数据库."<a class="ae lq" href="http://Http://Yann." rel="noopener ugc nofollow" target="_blank">T5。 </a> <em class="ly">乐存。Com/Exdb/Mnist/ </em>。</p><p id="deb6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Pawelczyk、Martin、Sascha Bielawski、Johannes van den Heuvel、Tobias Richter和Gjergji Kasneci。2021."卡拉:一个Python库，用于测试算法资源和反事实解释算法."<em class="ly"> arXiv预印本arXiv:2108.00783 </em>。</p><p id="a4a9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">鲁丁辛西娅。2019."停止解释高风险决策的黑盒机器学习模型，转而使用可解释的模型."<em class="ly">自然机器智能</em>1(5):206–15。</p><p id="6069" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">舒特，丽莎，奥斯卡·基，罗里·麦克·格拉斯，卢卡·科斯塔贝洛，波格丹一世·萨卡利安努，亚林·加尔，等。"通过隐含最小化认知和随机不确定性产生可解释的反事实解释."在<em class="ly">人工智能和统计国际会议</em>中，1756–64。PMLR。</p><p id="c9f1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">沃希特、桑德拉、布伦特·米特斯塔特和克里斯·拉塞尔。2017."不打开黑盒的反事实解释:自动化决策和GDPR . "<em class="ly">哈维。JL理工大学。</em> 31: 841。</p></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><p id="eb89" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="ly">原载于2022年4月20日https://www.paltmeyer.com</em><em class="ly">T21</em><a class="ae lq" href="https://www.paltmeyer.com/blog/posts/a-new-tool-for-explainable-ai/" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>