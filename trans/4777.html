<html>
<head>
<title>How to choose the number of estimators for Gradient Boosting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度推进中如何选择估计量的个数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-choose-the-number-of-estimators-for-gradient-boosting-8d06920ab891#2022-10-24">https://towardsdatascience.com/how-to-choose-the-number-of-estimators-for-gradient-boosting-8d06920ab891#2022-10-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="adac" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个简单的循环来调整你的模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a397d673c11bca6cad9e574589225303.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cDjF657dRt-tqpsTQN6YLw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">布莱登·特罗洛普在<a class="ae ky" href="https://unsplash.com/s/photos/sequence?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="f1d1" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">介绍</h1><p id="e817" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">在数据科学中，目前有许多算法可供使用。因此，一种有用的技术是将它们结合在一个模型中，以获得各自的优点，从而得到一个更准确的模型。</p><p id="df94" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">使用Scikit-Learn，您会发现随机森林算法，即bagging类集成模型。另一方面，您还会发现Boosting模型，它按顺序训练估计器，将一个模型的结果传递给下一个模型，试图改进预测，直到它们达到最佳结果。</p><p id="1b54" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">创建梯度增强估计器时，您会发现这个超参数<code class="fe mz na nb nc b">n_estimator=100</code>的默认值为100棵树，需要创建这些树才能得到结果。很多时候，我们只是将它设置为默认值，或者根据需要增加，甚至使用网格搜索技术。</p><div class="nd ne gp gr nf ng"><a rel="noopener follow" target="_blank" href="/grid-search-or-random-search-for-model-tuning-f09edab6aaa3"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd iu gy z fp nl fr fs nm fu fw is bi translated">模型调整的网格搜索或随机搜索</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">了解如何在SciKit-Learn的GridSearchCV或RandomizedSearchCV之间进行选择</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">towardsdatascience.com</p></div></div><div class="np l"><div class="nq l nr ns nt np nu ks ng"/></div></div></a></div><p id="2de4" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">在这篇文章中，我们将找到一个简单的方法来得到一个单一的数字来训练我们的模型。</p><h1 id="6eea" class="lg lh it bd li lj nv ll lm ln nw lp lq jz nx ka ls kc ny kd lu kf nz kg lw lx bi translated">梯度推进</h1><p id="c78f" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">使用这个类<code class="fe mz na nb nc b"><strong class="ma iu">from </strong>sklearn.ensemble <strong class="ma iu">import </strong>GradientBoostingRegressor</code>可以从Scikit-Learn加载梯度增强。梯度推进算法可用于分类或回归模型。它是一个基于树的估计器，这意味着它由许多决策树组成。</p><p id="dbf8" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><em class="oa">树1 </em>的结果会产生错误。这些误差将被用作<em class="oa">采油树2 </em>的输入。同样，上一个模型的误差将被用作下一个模型的输入，直到它达到<code class="fe mz na nb nc b">n_estimators</code>值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/5ba4ed0215f38ee53568ef79ea4bd93f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NT9YEj6sIMviPEIlBwO1Bw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">每个模型都会符合前一个模型的误差。图片由作者提供。</p></figure><p id="b153" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">由于每个估计量都将符合前一个估计量的误差，因此预期预测的组合将优于任何一个单独的估计量。每次迭代后，我们都使模型变得更复杂，减少了偏差，但增加了方差。所以我们必须知道何时停止。</p><p id="ccf1" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">让我们看看现在该如何做。</p><h1 id="d90f" class="lg lh it bd li lj nv ll lm ln nw lp lq jz nx ka ls kc ny kd lu kf nz kg lw lx bi translated">准备模型</h1><p id="a511" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">这个练习的代码很简单。我们所要做的就是在每次迭代后循环一次，并检查哪一次我们有最低的误差。</p><p id="d5e2" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">让我们从选择数据集开始。我们将使用来自seaborn库的<a class="ae ky" href="https://github.com/mwaskom/seaborn-data/blob/master/car_crashes.csv" rel="noopener ugc nofollow" target="_blank"> <em class="oa"> car_crashes </em> </a>数据集(这是BDS许可下的一个开放数据)。</p><pre class="kj kk kl km gt oc nc od oe aw of bi"><span id="2419" class="og lh it nc b gy oh oi l oj ok"># Dataset<br/>df = sns.load_dataset('car_crashes')</span></pre><p id="4a29" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">下面快速看一下数据。我们将尝试使用其他特征作为预测值来估计<code class="fe mz na nb nc b">total</code>的数量。既然是实数输出，我们就说回归模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/40ef7eb2e4a9f3b0305ba1c747365ac1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qk7Z5rdTvymfpFa1pD9Shg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://github.com/mwaskom/seaborn-data/blob/master/car_crashes.csv" rel="noopener ugc nofollow" target="_blank">车祸数据集</a>，来自seaborn。图片由作者提供。</p></figure><p id="872d" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">快速查看相关性。</p><pre class="kj kk kl km gt oc nc od oe aw of bi"><span id="bd3d" class="og lh it nc b gy oh oi l oj ok"># Correlations<br/>df.corr().style.background_gradient(cmap='coolwarm')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/1ddad2d80328978b6480ed7ebe6d5815.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wa4A8hVAtP7RaW5KWOEpmw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据集中的相关性。图片由作者提供。</p></figure><p id="8008" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">好的，没有严重的多重共线性。我们可以看到<code class="fe mz na nb nc b">ins_premium</code>和<code class="fe mz na nb nc b">ins_losses</code>与<code class="fe mz na nb nc b">total</code>没有很好的关联，所以我们在模型中不考虑它们。</p><p id="fbfd" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">如果我们检查丢失的数据，没有</p><pre class="kj kk kl km gt oc nc od oe aw of bi"><span id="c611" class="og lh it nc b gy oh oi l oj ok"># Missing<br/>df.isnull().sum()<br/>0</span></pre><p id="349d" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">很好，现在让我们拆分数据。</p><pre class="kj kk kl km gt oc nc od oe aw of bi"><span id="fb02" class="og lh it nc b gy oh oi l oj ok"># X and y<br/>X = df.drop(['ins_premium', 'ins_losses', 'abbrev', 'total'], axis=1)<br/>y = df['total']</span><span id="d993" class="og lh it nc b gy on oi l oj ok"># Train test<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22)</span></pre><p id="b4ad" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们可以创建一个管道来缩放数据并对其建模(<em class="oa">缩放这些数据真的不是很有必要，因为它们已经处于相同的缩放比例，基于十进制</em>)。接下来，我们将数据拟合到模型中，并预测结果。</p><p id="95cd" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我使用了500个T4为0.3的估值器。</p><blockquote class="oo op oq"><p id="a54d" class="ly lz oa ma b mb mu ju md me mv jx mg or mw mj mk os mx mn mo ot my mr ms mt im bi translated">学习率是我们达到最小误差所采取的步长。如果我们使用的值太高，我们可能会超过最小值。如果我们用一个太小的数字，我们甚至可能无法接近它。所以，你可以考虑的一个经验法则是:如果你有大量的评估者，你可以使用较低的学习率值。如果你只有几个估值器，最好使用更高的学习率值。</p></blockquote><pre class="kj kk kl km gt oc nc od oe aw of bi"><span id="b800" class="og lh it nc b gy oh oi l oj ok">steps = [('scale', StandardScaler()),<br/>         ('GBR', GradientBoostingRegressor(n_estimators=500, learning_rate=0.03)) ]</span><span id="84eb" class="og lh it nc b gy on oi l oj ok"># Instance Pipeline and fit<br/>pipe = Pipeline(steps).fit(X_train, y_train)</span><span id="9c59" class="og lh it nc b gy on oi l oj ok"># Predict<br/>preds = pipe.predict(X_test)</span></pre><p id="30d6" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">现在评估。</p><pre class="kj kk kl km gt oc nc od oe aw of bi"><span id="2990" class="og lh it nc b gy oh oi l oj ok"># RMSE of the predictions<br/>print(f'RMSE: { round(np.sqrt(mean_squared_error(y_test, preds)),1 )}')</span><span id="1635" class="og lh it nc b gy on oi l oj ok"><strong class="nc iu">[OUT]: RMSE: 1.1</strong></span><span id="31fd" class="og lh it nc b gy on oi l oj ok"># Mean of the true Y values<br/>print(f'Data y mean: {round( y.mean(),1 )}')</span><span id="411f" class="og lh it nc b gy on oi l oj ok"><strong class="nc iu">[OUT]: Data y mean: 15.8</strong></span></pre><p id="6571" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">很好。我们的RMSE大约是平均值的6.9%。平均来说，我们就差这么多。</p><h1 id="071b" class="lg lh it bd li lj nv ll lm ln nw lp lq jz nx ka ls kc ny kd lu kf nz kg lw lx bi translated">确定估计量的数量</h1><p id="c193" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">现在，让我们检查一种方法来调整我们的模型，选择最佳数量的估计量来训练，这将使我们的错误率最低。</p><p id="cc75" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">就像我说的，我们真的不需要缩放这个数据，因为它已经是相同的比例了。所以我们来拟合一下模型。</p><pre class="kj kk kl km gt oc nc od oe aw of bi"><span id="1c52" class="og lh it nc b gy oh oi l oj ok">#Model<br/>gbr = GradientBoostingRegressor(n_estimators=500, learning_rate=0.3).fit(X_train, y_train)</span></pre><p id="68b7" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">现在是好东西了。梯度推进中有一种方法，允许我们迭代每个训练好的估计器的预测，从1到500。因此，我们将创建一个循环，遍历<code class="fe mz na nb nc b">gbr</code>模型中的500个估计器，使用方法<code class="fe mz na nb nc b">staged_predict()</code>预测结果，计算均方误差，并将结果存储在列表<code class="fe mz na nb nc b">errors</code>中。</p><pre class="kj kk kl km gt oc nc od oe aw of bi"><span id="4694" class="og lh it nc b gy oh oi l oj ok"># Loop for the best number<br/>errors = [ mean_squared_error(y_test, preds) for preds in gbr.staged_predict(X_test)]</span><span id="6006" class="og lh it nc b gy on oi l oj ok"># Optimal number of estimators<br/>optimal_num_estimators = np.argmin(errors) + 1</span></pre><p id="9df7" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">接下来，我们可以绘制结果。</p><pre class="kj kk kl km gt oc nc od oe aw of bi"><span id="09ba" class="og lh it nc b gy oh oi l oj ok">#Plot<br/>g=sns.lineplot(x=range(500), y=errors)<br/>g.set_title(f'Best number of estimators at {optimal_num_estimators}', size=15);</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/53e7f4f880b8490fb39da7019524b46e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*07-6VDX27iVbwK-3HHno3w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最佳估计数。图片由作者提供。</p></figure><p id="d6a1" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们看到最低的错误率是用34个估值器。因此，让我们用34个估计器重新训练我们的模型，并与用管道训练的模型的结果进行比较。</p><pre class="kj kk kl km gt oc nc od oe aw of bi"><span id="d45d" class="og lh it nc b gy oh oi l oj ok"># Retrain<br/>gbr = GradientBoostingRegressor(n_estimators=34, learning_rate=0.3).fit(X_train, y_train)</span><span id="dfc5" class="og lh it nc b gy on oi l oj ok"># Predictions<br/>preds2 = gbr.predict(X_test)</span></pre><p id="ae86" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">正在评估…</p><pre class="kj kk kl km gt oc nc od oe aw of bi"><span id="3579" class="og lh it nc b gy oh oi l oj ok"># RMSE of the predictions<br/>print(f'RMSE: { round(np.sqrt(mean_squared_error(y_test, preds2)),1 )}')<br/><strong class="nc iu">[OUT]: RMSE: 1.0</strong></span><span id="751c" class="og lh it nc b gy on oi l oj ok"># Data Y mean<br/>print(f'Data y mean: {round( y.mean(),1 )}')</span><span id="012f" class="og lh it nc b gy on oi l oj ok"><strong class="nc iu">[OUT]: Data y mean: 15.8</strong></span></pre><p id="b9ca" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们从6.9%降到了现在的6.3%。近似的好9%。我们来看几个预测。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/cbcb9e5a940e35e1a0e5b4f7a50564ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*Ph93enk4gE1uuNGOBMfxrA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">两个模型的预测。图片由作者提供。</p></figure><p id="f35a" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">有趣的结果。第二个模型的一些预测比第一个要好。</p><h1 id="0214" class="lg lh it bd li lj nv ll lm ln nw lp lq jz nx ka ls kc ny kd lu kf nz kg lw lx bi translated">在你走之前</h1><p id="bbd3" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">我们从Scikit-Learn中学习了如何确定调整a <code class="fe mz na nb nc b">GradientBoostingRegressor</code>的最佳估计数。这是一个超参数，可以在这种集合模型中发挥作用，它按顺序训练估计量。</p><p id="5e54" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">有时，在几次迭代之后，模型可能会开始过度拟合，因此它会开始过多地增加方差，从而影响预测。</p><p id="3581" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们看到，在这种情况下，一个简单的循环可以帮助我们找到最优解。但是，当然，对于大型数据集来说，计算起来可能很昂贵，所以一个想法是首先尝试一个较低的<code class="fe mz na nb nc b">n_estimators</code>,看看您是否能尽快达到最小误差。</p><p id="4348" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">下面是GitHub 中的<a class="ae ky" href="https://github.com/gurezende/Studying/blob/master/Python/sklearn/GradientBoosting.ipynb" rel="noopener ugc nofollow" target="_blank">完整代码。</a></p><p id="a71d" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">如果你喜欢这个内容，请关注我的博客。</p><div class="nd ne gp gr nf ng"><a href="http://gustavorsantos.medium.com/" rel="noopener follow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd iu gy z fp nl fr fs nm fu fw is bi translated">古斯塔沃·桑托斯-中等</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">阅读古斯塔夫·桑托斯在媒介上的作品。数据科学家。我从数据中提取见解，以帮助个人和公司…</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">gustavorsantos.medium.com</p></div></div><div class="np l"><div class="ow l nr ns nt np nu ks ng"/></div></div></a></div><p id="2233" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">在<a class="ae ky" href="https://www.linkedin.com/in/gurezende/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上找到我。</p><p id="a7f3" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">这个练习是基于参考文献中Aurélien Géron的优秀教科书。</p><h1 id="dcab" class="lg lh it bd li lj nv ll lm ln nw lp lq jz nx ka ls kc ny kd lu kf nz kg lw lx bi translated">参考</h1><div class="nd ne gp gr nf ng"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd iu gy z fp nl fr fs nm fu fw is bi translated">sk learn . ensemble . gradientboostingclassifier</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">用于分类的梯度增强。该算法以前向逐级的方式建立一个附加模型；它…</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">scikit-learn.org</p></div></div><div class="np l"><div class="ox l nr ns nt np nu ks ng"/></div></div></a></div><div class="nd ne gp gr nf ng"><a href="https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1098125975/ref=pd_lpo_2?pd_rd_w=DBmLe&amp;content-id=amzn1.sym.116f529c-aa4d-4763-b2b6-4d614ec7dc00&amp;pf_rd_p=116f529c-aa4d-4763-b2b6-4d614ec7dc00&amp;pf_rd_r=81QQEFQYR0K7ZTFRHMJD&amp;pd_rd_wg=iPGdg&amp;pd_rd_r=1b48e671-181a-44bc-8d3b-7b6384e9edcd&amp;pd_rd_i=1098125975&amp;psc=1" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd iu gy z fp nl fr fs nm fu fw is bi translated">使用Scikit-Learn、Keras和TensorFlow进行机器实践学习:概念、工具和技术…</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">用Scikit-Learn、Keras和TensorFlow进行机器学习:概念、工具和技术来构建…</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">www.amazon.com</p></div></div><div class="np l"><div class="oy l nr ns nt np nu ks ng"/></div></div></a></div></div></div>    
</body>
</html>