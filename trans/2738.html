<html>
<head>
<title>Neural Networks: Forward pass and Backpropagation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络:正向传递和反向传播</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-forward-pass-and-backpropagation-be3b75a1cfcc#2022-06-14">https://towardsdatascience.com/neural-networks-forward-pass-and-backpropagation-be3b75a1cfcc#2022-06-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0fd7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用PyTorch的逐步解释和示例</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/0e286b70a9021aa92b2c910caf5cdd37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hdD_5IuKWyMbwkbAAzBZCw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="2cfb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">内容:</p><ol class=""><li id="2a31" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">介绍</li><li id="8e68" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">功能组合</li><li id="4782" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">一个简单的神经网络</li><li id="e0e1" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">前进传球</li><li id="6557" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">在PyTorch中建立简单的神经网络</li><li id="ba64" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">反向传播</li><li id="7235" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">与PyTorch结果的比较</li><li id="708f" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">结论</li><li id="e1e8" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">参考</li></ol><p id="268a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">简介:</strong></p><p id="b005" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">神经网络是最广泛使用的机器学习算法之一。神经网络在图像分类、时间序列预测等领域的成功应用为其在商业和研究中的应用铺平了道路。可以说，神经网络是最重要的机器学习算法之一。对算法的清晰理解将有助于诊断问题，也有助于理解其他高级深度学习算法。本文的目标是解释神经网络的工作原理。我们将一步一步地检查算法，并解释如何在PyTorch中建立一个简单的神经网络。我们还将比较我们的计算结果和PyTorch的输出。</p><p id="1175" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 1.0功能组合:</strong></p><p id="53cb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们从考虑以下两个任意线性函数开始:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/9185c5ff7cb961797aa6ab4ab4211f93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*Mo07FiVsqTkaWm0k5jvreA.png"/></div></figure><p id="2d43" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">系数-1.75、-0.1、0.172和0.15是为了说明的目的而任意选择的。接下来，我们定义两个新函数a₁和a₂，它们分别是z₁和z₂的函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/88b5c23d07d50093408cfdfea5be7d1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*3lJbWUsfArf5_zFVfWfgCQ.png"/></div></figure><p id="460a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该功能</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/608331684af760ecdf46be400303061f.png" data-original-src="https://miro.medium.com/v2/resize:fit:286/format:webp/1*hv1nXND21fHe0myWibmuJg.png"/></div></figure><p id="6962" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上面使用的称为sigmoid函数。这是一条S形曲线。函数f(x)在神经网络中有特殊的作用。我们将在随后的章节中更详细地讨论它。现在，我们简单地用它来构造函数a₁和a₂.</p><p id="e53e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，我们定义另一个函数，它是a₁和a₂:函数的线性组合</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/287c64a5d024dd6141eddc09f3117938.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*XGNOTMbmwjvHJl5sjQ2OBA.png"/></div></figure><p id="9162" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">同样，系数0.25、0.5和0.2是任意选择的。图1显示了a₁、a₂和z₃.三个函数的曲线图</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/d65302694b6b4063518366f8c77e1fe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*f8hjhvDmSeIAYv4zXTlhoQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图一。组合功能(图片由作者提供)</p></figure><p id="7f4b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从图1中我们可以看到，a₁和a₂函数的线性组合是一条看起来更复杂的曲线。换句话说，通过线性组合曲线，我们可以创建能够捕捉更复杂变化的函数。我们可以将sigmoid函数应用于z₃，并将其与另一个类似的函数线性组合，以表示一个更复杂的函数。从理论上讲，通过组合足够多的这样的函数，我们可以表现出极其复杂的数值变化。上述等式中的系数是任意选择的。如果我们可以通过调整系数来改变最终函数的形状，会怎么样呢？这将允许我们将最终函数拟合到一个非常复杂的数据集。这是神经网络背后的基本思想。神经网络为我们提供了一个框架，将简单的函数组合起来，构建一个能够表示数据中复杂变化的复杂函数。现在让我们检查神经网络的框架。</p><p id="866d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 2.0一个简单的神经网络:</strong></p><p id="bd4e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">图2是简单神经网络的示意图。在本文的后续讨论中，我们将使用这个简单的网络。网络将单个值(x)作为输入，并产生单个值y作为输出。网络中还有四个附加节点，标记为1到4。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mk"><img src="../Images/c6f70ce7251ddde0068c6c09114501e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0RP2z_jewe_IPaJhAxPAwQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:简单的神经网络(图片由作者提供)</p></figure><p id="af7e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">输入节点为节点1和节点2供电。节点1和节点2分别为节点3和节点4供电。最后，节点3和节点4向输出节点供电。w₁到w₈是网络的权重，b₁到b₈是偏差。权重和偏差用于在节点处创建值的线性组合，然后将这些值提供给下一层中的节点。例如，结合了权重w₁和偏差b₁的输入x是节点1的输入。类似地，结合了权重w₂和偏差b₂的输入x是节点2的输入。节点处的AF代表激活功能。上一节介绍的sigmoid函数就是这样一种激活函数。我们将很快讨论更多的激活函数。现在，让我们跟随信息在网络中的流动。由节点1和节点2处的激活函数产生的输出然后分别与权重w₃和w₅以及偏置b₃.线性组合线性组合是节点3的输入。类似地，节点1和节点2的输出分别与权重w₆和w₄组合，并且偏置b₄以馈送到节点4。最后，来自节点3和节点4的激活函数的输出分别与权重w₇和w₈线性组合，并偏置b₅以产生网络输出yhat。</p><p id="67eb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种从输入到输出的信息流也被称为前向传递。在我们为我们的简单网络制定出向前传递的细节之前，让我们来看看激活函数的一些选择。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ml"><img src="../Images/b5c4fae47896c9f2b2403307e47b3327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3YlNRSvp15qWu_a0iss41A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表1:激活功能(图片由作者提供)</p></figure><p id="4a16" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">表1显示了三种常见的激活功能。还显示了每个激活函数及其导数的曲线图。虽然sigmoid和tanh是平滑函数，但RelU在x=0处有一个拐点。激活函数的选择取决于我们试图解决的问题。存在神经网络的应用，其中期望具有激活函数的连续导数。对于这样的应用，导数连续的函数是一个很好的选择。tanh和sigmoid激活函数在原点附近具有较大的导数。因此，如果我们在这个区域中操作，这些函数将产生更大的梯度，导致更快的收敛。相反，远离原点，双曲正切函数和sigmoid函数具有非常小的导数值，这将导致解中非常小的变化。我们将在下一节讨论梯度的计算。还有许多其他的激活功能，我们不会在本文中讨论。由于RelU函数是一个简单的函数，我们将使用它作为简单神经网络的激活函数。我们现在准备执行向前传球。</p><p id="a588" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 3.0向前传球:</strong></p><p id="23d9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">图3显示了我们的简单神经网络向前传递的计算。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mm"><img src="../Images/99d31e549e0e5bbfd0bc85d89efa4888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kNOa3-rGup6t_qspu9ZJtA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3:向前传球(图片由作者提供)</p></figure><p id="dcd7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过将输入x分别与w₁和b₁以及w₂和b₂进行线性组合来获得z₁和z₂。a₁和a₂是分别对z₁和z₂应用RelU激活函数的输出。z₃和z₄是通过将前一层中的a₁和a₂分别与w₃、w₅、b₃和w₄进行线性组合而得到的。最后，通过将前一层中的a₃和a₄与w₇、w₈和b₅.组合，获得输出yhat实际上，函数z₁、z₂、z₃和z₄是通过矩阵向量乘法获得的，如图4所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/84f06e1393433d3548de0b8ac03668c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*mv0UB-GJjWNwrA4eIfjl3g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4:矩阵向量乘积(图片由作者提供)</p></figure><p id="019f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里我们将矩阵中的偏置项结合起来。一般来说，如图5所示，对于一个r节点层馈送一个s节点层，矩阵矢量积将是(s×r+1)*(r+1×1)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/70962c4df2309d122e466ec91625a92c.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*B12JDH7JvGYbSQNS3W_nQw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5:一般配置的矩阵矢量积(图片由作者提供)</p></figure><p id="e280" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正向传递的最后一步是计算损耗。由于我们的示例中只有一个数据点，因此损失L是输出值yhat和已知值y之间的差值的平方。通常，对于回归问题，损失是每个数据点的网络输出值和已知值之间的差值的平方的平均和。它被称为均方误差。这就完成了神经网络的两个重要步骤中的第一步。在讨论下一步之前，我们先描述如何在PyTorch中建立简单的网络。</p><p id="41d0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 4.0在PyTorch中设置简单的神经网络:</strong></p><p id="f75f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的目标是使用我们的简单网络示例来展示在PyTorch中建立神经网络的基础。这里假设用户已经在他们的机器上安装了PyTorch。我们将使用torch.nn模块来设置我们的网络。我们从导入nn模块开始，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/f0d1cdad32b0671677b7bb2584c4a18d.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*SF9Qw6wE76WAdPcvHN6r1A.png"/></div></figure><p id="f580" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了建立我们的简单网络，我们将在nn模块中使用顺序容器。我们的网络中的三层是按照上面图3所示的顺序指定的。以下是我们简单网络的完整规格:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/81ebacad4759ba6775ecc4f7d661bc9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*ERe9lmCOikXvlBSHaUwiuw.png"/></div></figure><p id="40ad" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">nn。线性分类用于应用权重和偏差的线性组合。线性类有两个参数。第一个参数指定了该层的节点数。层中的节点数被指定为第二个参数。例如，输入层中的(1，2)规范意味着它由单个输入节点提供，并且该层有两个节点。隐藏层由输入层的两个节点馈电，有两个节点。值得注意的是，前一层的输出节点数必须与当前层的输入节点数相匹配。输出层的(2，1)规范告诉PyTorch我们只有一个输出节点。激活功能在层之间指定。如前所述，我们使用RelU函数。使用这个简单的方法，我们可以构建一个深度和广度都适合手头任务的网络。通过提供如下输入值获得网络输出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/5c6a0227f9d85a0503fad19d2911a947.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*7ivxFrqU7pKH6ooAUcqE7A.png"/></div></figure><p id="fd05" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们的例子中，t_u1是单一的x值。为了计算损失，我们首先定义损失函数。损失函数的输入是神经网络的输出和已知值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/f0767fed72c29bbbd24f15cc6a36ffae.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*tn9PaE0p5fX3VjfZ7Y60jg.png"/></div></figure><p id="3613" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们的例子中，t_c1是y值。这就完成了PyTorch中正向传递的设置。接下来，我们讨论神经网络的第二个重要步骤，反向传播。</p><p id="2391" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 5.0反向传播:</strong></p><p id="1ec7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">神经网络的权重和偏差是我们模型中的未知数。我们希望确定最适合我们数据集的权重和偏差的值。当损失(即误差)最小时，达到最佳拟合。请注意，损耗L(见图3)是未知权重和偏差的函数。想象一个多维空间，其中轴是权重和偏差。损失函数是这个空间中的一个曲面。在最小化过程的开始，用随机权重和偏差来播种神经网络，即，我们从损失表面上的随机点开始。为了到达地面的最低点，我们开始沿着最陡的下坡方向前进。这就是梯度下降算法在每个训练时期或迭代期间所实现的。在任何第n次迭代中，权重和偏差更新如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/8daa573c5005a73b5c4d63d2f87888a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*bHYg3GhoDuxG49e4MIYzeg.png"/></div></figure><p id="4eac" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">m是网络中权重和偏差的总数。请注意，这里我们使用wᵢ来表示权重和偏差。学习率η决定了每一步的大小。在反向传播步骤中计算损失相对于每个权重/偏差的偏导数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/bee2cd259a243b3ad79aca9d88a5e098.png" data-original-src="https://miro.medium.com/v2/resize:fit:310/format:webp/1*l4iOiKXTbfscSTFAXJU-IA.png"/></div></figure><p id="9e33" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该过程从输出节点开始，系统地向后通过各层，一直到输入层，因此称为反向传播。每一步都要用到计算导数的链式法则。我们现在计算简单神经网络的偏导数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mv"><img src="../Images/95f8c47472977ca7d82768fda34bc27a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1zg5wA49rYpyolcjQ3_-Dg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图wrt w7、w8和b5的偏导数(图片由作者提供)</p></figure><p id="f2a1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们首先从损耗L wrt对输出yhat的偏导数开始(参见图6)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/a18068605820775dbe1ffcf889bdd726.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*XJVeMfJxSLrZz-MisvkM9Q.png"/></div></figure><p id="d1f8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们用它来计算w₇.损耗的偏导数</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/4f71edaf3f526718ece2010a28113238.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*5pIm541OowoQAmIimMcFPw.png"/></div></figure><p id="3a70" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里，我们使用了图6中的yhat方程来计算yhat wrt对w₇.的偏导数类似地计算w₈和b₅的偏导数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mx"><img src="../Images/2a60d354d2b4c866bc096a4be2cdad88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z3n7b9AjtcqkmNFRhfX0uQ.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/224e1e89cb81c3b657766832c7c04080.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sPQmGRXhI72xp6fFn1de4w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图7:w3、w5和b3的偏导数(图片来自作者)</p></figure><p id="27e8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们回到上一层。链式法则再次被用来计算导数。参考图7，了解w₃、w₅和b₃:的偏导数</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/d976bcd29dfc3d80099bf390e30c78f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3iqfemgszZkFDpRmNzXH0w.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/f314a2d08f18ebc9dc2fe39097a3e0a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IH4o7N7ja8uEMC0Cjm_OWg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图wrt w6、w4和b4的偏导数(图片由作者提供)</p></figure><p id="e155" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">参考图8，了解w₄、w₆和b₄:的偏导数</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/1900498cd16a3e3330b1a1bce807c2ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*02kfP92RRNvlQ0bhH4uFGg.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/f51befa6c15de81dbbe2965ed437812a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yL5r8LC5ebjuJZeRKsCLEQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图wrt w1和b1的偏导数(图片来自作者)</p></figure><p id="e038" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">关于w₁和b₁的下一组偏导数，参见图9。我们首先将输出重写为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/c6252d6b202e4773aeb66491b13ffced.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8WqQP3MPKe21QC44tCLwGg.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/da2c1919d8c1df982a0f639dbf1ef090.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*58iIlu0aFVoJNihTTwPe9w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图wrt w2和b2的偏导数(图片由作者提供)</p></figure><p id="7411" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">同样，w₂和b₂:的偏导数见图10</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/082c3e98bd646eca2868d0a41e17f5b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GWmvr220wUaUGFZ-QD1X4w.png"/></div></div></figure><p id="0e9f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">PyTorch通过计算图执行所有这些计算。在PyTorch中，损耗wrt权重和偏差的梯度计算如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/e4b748cb7072ddccaaa17c612ec9702f.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*qfOcdcM9qLhY_ZKyzvkRVA.png"/></div></figure><p id="932a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">首先，我们广播所有梯度项的零。optL是优化器。的。向后触发PyTorch中的渐变计算。</p><p id="d7b2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">既然我们已经为简单的神经网络导出了正向传递和反向传播的公式，那么让我们将我们的计算输出与PyTorch的输出进行比较。</p><p id="e314" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 6.0与PyTorch结果的比较:</strong></p><p id="c38a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一个完整的历元由正向传递、反向传播和权重/偏差更新组成。我们将使用导出的公式，使用Excel执行一个完整时期的计算。我们将首先比较正向传递的结果，然后比较反向传播的结果。最后，我们将使用来自反向传播的梯度来更新权重和偏差，并将其与Pytorch输出进行比较。在实践中，我们很少在训练中关注重量或梯度。这里，我们在PyTorch中执行两次迭代，并输出这些信息进行比较。但是首先，我们需要从PyTorch中提取初始随机权重和偏差。我们需要这些权重和偏差来执行我们的计算。这是按如下方式逐层完成的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/d26368f1a2b4839232d69616e49c28b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*pumblp9dB2immcg2PJRJuA.png"/></div></figure><p id="2994" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意，我们提取偶数层的权重和偏差，因为我们的神经网络中的奇数层是激活函数。提取的初始权重和偏差被传输到Excel中适当标记的单元格。</p><p id="ed4d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">图11显示了我们的正向传递计算与PyTorch时段输出的比较。PyTorch的输出显示在图的右上角，而Excel中的计算显示在图的左下角。输出值和损耗值分别用适当的颜色圈出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/e55d9c22af529fc0c830f5f9beb1b420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kPejieytF246w3IoMxDMRA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图11:与PyTorch的向前传球结果比较(图片由作者提供)</p></figure><p id="d506" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，我们计算梯度项。就像权重一样，任何训练时期的梯度也可以在PyTorch中逐层提取，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/8297eef0df9d20e62b5feae540dfd8f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*EhAshsKeFZc-uMKXGlk5EQ.png"/></div></figure><p id="d246" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">图12显示了我们在Excel中的反向传播计算与PyTorch输出的比较。损失权重和偏差的梯度的不同项被适当地标注。请注意，我们在Excel计算中使用了表1中RelU的导数(当x &lt; 0时，RelU的导数为零，否则为1)。PyTorch的输出显示在图的右上角，而Excel中的计算显示在图的左下角。除了三个梯度项外，其余都为零。w₈、b₄和b₅的损耗梯度是三个非零分量。这三个非零梯度项用适当的颜色圈出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/0a030819fd23db7b8c823ee8aff73007.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*myxml0jLrGeyReEOqQGXGw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图12:与PyTorch的反向传播比较(图片由作者提供)</p></figure><p id="be28" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们现在准备在第一个训练周期结束时更新权重。在PyTorch中，这是通过调用optL.step()来完成的。对于我们的计算，我们将使用第5节开头提到的权重更新公式。</p><p id="cf4c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">图13示出了在时段1开始时更新的权重的比较。PyTorch的输出显示在图的右上角，而Excel中的计算显示在图的左下角。请注意，只有一个权重w₈和两个偏差b₄和b₅值发生变化，因为只有这三个梯度项是非零的。我们的例子中使用的学习率是0.01。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/0b48a0a902d108bff1057392897b57ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r6UjyX0NgWhPNuu92NNtWw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图13:用PyTorch比较更新的权重和偏差(图片由作者提供)</p></figure><p id="6056" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">感兴趣的读者可以在下面找到PyTorch笔记本和电子表格(Google Sheets)。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="f334" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">链接到google工作表:</p><p id="9293" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae no" href="https://docs.google.com/spreadsheets/d/1njvMZzPPJWGygW54OFpX7eu740fCnYjqqdgujQtZaPM/edit#gid=1501293754" rel="noopener ugc nofollow" target="_blank">https://docs . Google . com/spreadsheets/d/1 njvmzzppjwgygw 54 ofp x7e u 740 fcnyjqqdgujqtzapm/edit # GID = 1501293754</a></p><p id="61a1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 7.0结论:</strong></p><p id="9fc4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本文中，我们研究了如何建立神经网络，以及如何执行正向传递和反向传播计算。我们使用一个简单的神经网络来导出正向传递过程中每个节点的值。使用链式法则，我们导出了损失函数wrt对权重和偏差的梯度项。我们使用Excel执行正向传递、反向传播和权重更新计算，并将Excel的结果与PyTorch输出进行比较。虽然我们在本文中使用的神经网络非常小，但是其基本概念可以扩展到任何一般的神经网络。</p><p id="6839" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">参考文献:</strong></p><p id="19be" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">1.0 PyTorch文档:<a class="ae no" href="https://pytorch.org/docs/stable/index.html" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/index.html</a>。</p><p id="06a2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">2.0深度学习，PyTorch，Eli Stevens，Luca Antiga和Thomas Viehmann，2020年7月，曼宁出版，ISBN 9781617295263。</p></div></div>    
</body>
</html>