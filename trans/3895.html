<html>
<head>
<title>All You Need to Know About Bag of Words and Word2Vec — Text Feature Extraction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于单词包和Word2Vec —文本特征提取，您需要知道的一切</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/all-you-need-to-know-about-bag-of-words-and-word2vec-text-feature-extraction-e386d9ed84aa#2022-08-30">https://towardsdatascience.com/all-you-need-to-know-about-bag-of-words-and-word2vec-text-feature-extraction-e386d9ed84aa#2022-08-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="5ca3" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">数据科学</h2><div class=""/><div class=""><h2 id="9b70" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">Word2Vec为什么更好，为什么不够好</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/4e0311e9114729b6aa8e572fced51938.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gd6Wd2j1b4VEtXVgwI1BUw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@tamarabellis?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">塔玛拉·比利斯</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄，由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a>编辑</p></figure><p id="57ab" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di"> W </span>虽然图像数据可以直接用于深度学习模型(RGB值作为输入)，但文本数据却不是这样。深度学习模型只对数字起作用，而不是像文本一样的符号序列。所以，你需要一种方法从文本中提取有意义的数字特征向量。这被称为<strong class="lk jd">特征提取</strong>。</p><p id="1616" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从现在开始，我们将通过<strong class="lk jd">文档</strong>来调用单个文本观察，通过<strong class="lk jd">语料库</strong>来调用文档集合。</p><pre class="ks kt ku kv gt mn mo mp mq aw mr bi"><span id="754f" class="ms mt it mo b gy mu mv l mw mx"><strong class="mo jd">Table of Contents</strong></span><span id="c6ca" class="ms mt it mo b gy my mv l mw mx"><strong class="mo jd">· </strong><a class="ae lh" href="#38a3" rel="noopener ugc nofollow"><strong class="mo jd">Bag of Words</strong></a><br/>  ∘ <a class="ae lh" href="#0e83" rel="noopener ugc nofollow">The Basics</a><br/>  ∘ <a class="ae lh" href="#c38c" rel="noopener ugc nofollow">Example on Data</a><br/>  ∘ <a class="ae lh" href="#f0dd" rel="noopener ugc nofollow">Advantages &amp; Limitations</a></span><span id="ca7e" class="ms mt it mo b gy my mv l mw mx"><strong class="mo jd">· </strong><a class="ae lh" href="#263a" rel="noopener ugc nofollow"><strong class="mo jd">Word2Vec</strong></a><br/>  ∘ <a class="ae lh" href="#6400" rel="noopener ugc nofollow">The Basics</a><br/>  ∘ <a class="ae lh" href="#b396" rel="noopener ugc nofollow">Creating Train Data</a><br/>  ∘ <a class="ae lh" href="#2c27" rel="noopener ugc nofollow">Continuous Bag of Words &amp; Skip-Gram</a><br/>  ∘ <a class="ae lh" href="#b3bc" rel="noopener ugc nofollow">Advantages &amp; Limitations</a></span><span id="c26c" class="ms mt it mo b gy my mv l mw mx"><strong class="mo jd">· </strong><a class="ae lh" href="#b0f4" rel="noopener ugc nofollow"><strong class="mo jd">Summary</strong></a></span></pre><h1 id="38a3" class="mz mt it bd na nb nc nd ne nf ng nh ni ki nj kj nk kl nl km nm ko nn kp no np bi translated">一袋单词</h1><h2 id="0e83" class="ms mt it bd na nq nr dn ne ns nt dp ni lr nu nv nk lv nw nx nm lz ny nz no iz bi translated">基础知识</h2><p id="522a" class="pw-post-body-paragraph li lj it lk b ll oa kd ln lo ob kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">要创建的最直观的特性之一是每个单词在文档中出现的次数。所以，你需要做的是:</p><ol class=""><li id="a8f3" class="of og it lk b ll lm lo lp lr oh lv oi lz oj md ok ol om on bi translated"><strong class="lk jd">对每个文档进行标记化</strong>,并给每个标记一个整数id。标记分隔符可以是空格和标点符号。</li><li id="1813" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated"><strong class="lk jd">统计</strong>令牌在每个文档中的出现次数。</li></ol><p id="e747" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">记号出现的次数称为<strong class="lk jd">词频</strong> (tf)。尽管简单，术语频率不一定是最好的语料库表示。根据<a class="ae lh" href="https://en.wikipedia.org/wiki/Zipf%27s_law" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> Zipf定律</strong> </a>，像“the”、“a”、“to”这样的常用词几乎总是文档中出现频率最高的术语/标记。如果你将频率这个词直接输入到分类器中，那些非常频繁的记号将会掩盖那些更罕见但更有趣的记号的频率。</p><p id="d0d1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了解决这个问题，一种最流行的“标准化”术语频率的方法是用文档频率 (idf)的<strong class="lk jd">倒数对每个标记进行加权，其计算公式如下</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/f6ff0833841146bc5a8339e00a67a380.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/format:webp/1*7V2y9TpNfAhhOpD9f3QuzQ.png"/></div></figure><p id="8303" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<code class="fe ou ov ow mo b">m</code>是语料库中的文档总数，df( <em class="ox"> t </em>)是语料库中包含标记<em class="ox"> t </em>的文档数。加权tf称为tf-idf，由下式给出</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/6893428dfc2ab319c4525b8553a02bc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*JCyuHJqMcSIEzR88aV_yXA.png"/></div></figure><p id="5af9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于语料库中文档<em class="ox"> d </em>的令牌<em class="ox"> t </em>。<a class="ae lh" href="https://en.wikipedia.org/wiki/Norm_(mathematics)" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">欧几里德范数</strong> </a>然后归一化所得到的tf-idf向量，即，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/9accd7014a67d8e76edaf92b2eefc5b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*vsg6jfoy2OD3nYT6m0UoSA.png"/></div></figure><p id="3ddb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于任何tf-idf矢量<em class="ox"> v </em>。</p><h2 id="c38c" class="ms mt it bd na nq nr dn ne ns nt dp ni lr nu nv nk lv nw nx nm lz ny nz no iz bi translated">数据示例</h2><p id="eae3" class="pw-post-body-paragraph li lj it lk b ll oa kd ln lo ob kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">作为一个具体的例子，假设您有以下语料库。</p><pre class="ks kt ku kv gt mn mo mp mq aw mr bi"><span id="fdb4" class="ms mt it mo b gy mu mv l mw mx">corpus = [<br/>    'This is the first document.',<br/>    'This is the second second document.',<br/>    'And the third one.',<br/>    'Is this the first document?',<br/>]</span></pre><p id="6783" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，<code class="fe ou ov ow mo b">m</code> <em class="ox"> </em> = 4。经过标记化，语料库中共有9个标记:“和”、“文档”、“第一”、“是”、“一”、“第二”、“the”、“第三”、“this”。因此，术语频率可以表示为大小为4×9的矩阵:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/a053c71ebe21e1d1ecccedd7d65fa875.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yIGcOWrC64gjwJPiaIadOg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">词频|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="79fe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">df( <em class="ox"> t </em>)然后可以通过计算每个令牌的非零值的数量，根据术语频率来计算，idf( <em class="ox"> t </em>)使用上面的公式来计算:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/245c070ea261e98d487ff7d142d7c414.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dYMDjAOfDv3j2YqBkdDrwA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">文档频率和文档频率的倒数|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="f396" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">tf-idf( <em class="ox"> t </em>，<em class="ox"> d </em>)通过将上面的tf矩阵乘以每个令牌的idf来获得。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/fa22c74d2bcd2ce8109861247d6cf5f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GgF4FFb0fzOqJZohl_F6dg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">tf-idf |图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="8545" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于每个文档，tf-idf的欧几里德范数分别显示如下。</p><pre class="ks kt ku kv gt mn mo mp mq aw mr bi"><span id="8018" class="ms mt it mo b gy mu mv l mw mx">2.97, 5.36, 4.25, 2.97</span></pre><p id="d866" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，通过将原始tf-idf除以每个文档的适当欧几里德范数来计算归一化的tf-idf。您可以获得标准化的tf-idf，如下所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/50cebe2b1908e09aa2eca0c3b9ee2eaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CZ8mO1x-rx-JTJfEEpQihA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">归一化的tf-idf |图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="a25c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这些是输入模型的最终特征。</p><h2 id="f0dd" class="ms mt it bd na nq nr dn ne ns nt dp ni lr nu nv nk lv nw nx nm lz ny nz no iz bi translated">优势和局限性</h2><p id="b088" class="pw-post-body-paragraph li lj it lk b ll oa kd ln lo ob kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">因为每个单词都由一个标量表示，所以文本的单词包表示是非常轻量级的，并且容易理解。然而，它至少有两个明显的缺点:</p><ol class=""><li id="29c7" class="of og it lk b ll lm lo lp lr oh lv oi lz oj md ok ol om on bi translated">特征维度线性依赖于唯一标记的数量(姑且称之为<code class="fe ou ov ow mo b">vocab_size</code>)，当你有一个大的语料库时，这是一个坏消息。您可以丢弃一些最少出现的标记以节省空间，但这样您就从数据中丢弃了一些潜在有用的信息。</li><li id="3b48" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated">如果您查看上面关于数据的示例中的第一个和最后一个文档，您会发现它们是不同的文档，但是具有相同的特征向量。这是因为单词包没有保留标记之间的关系。</li></ol><p id="8ef3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了解决限制2，您可以添加<strong class="lk jd"> n-grams </strong>作为新特性，它捕获<em class="ox"> n个</em>连续的标记(以及它们之间的关系)。然而，这又导致了限制1，您需要为额外的功能节省额外的空间。</p><h1 id="263a" class="mz mt it bd na nb nc nd ne nf ng nh ni ki nj kj nk kl nl km nm ko nn kp no np bi translated">Word2Vec</h1><h2 id="6400" class="ms mt it bd na nq nr dn ne ns nt dp ni lr nu nv nk lv nw nx nm lz ny nz no iz bi translated">基础知识</h2><p id="f2c3" class="pw-post-body-paragraph li lj it lk b ll oa kd ln lo ob kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">Word2Vec同时解决了单词包表示的两个限制:</p><ol class=""><li id="8a03" class="of og it lk b ll lm lo lp lr oh lv oi lz oj md ok ol om on bi translated">不是每个文档都有一个长度等于<code class="fe ou ov ow mo b">vocab_size</code>的特征向量，现在每个标记都变成了一个长度由您决定的数字(通常为100–1000，姑且称之为<code class="fe ou ov ow mo b">embed_dim</code>)的向量。</li><li id="8786" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated">Word2Vec通过考虑邻近的标记来对标记的<strong class="lk jd">上下文</strong>进行矢量化，而不是对标记本身进行矢量化。</li></ol><p id="3f94" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">结果是一个<code class="fe ou ov ow mo b">vocab_size × embed_dim</code>矩阵。那么，Word2Vec是如何学习一个令牌的上下文的呢？</p><blockquote class="pc pd pe"><p id="752a" class="li lj ox lk b ll lm kd ln lo lp kg lq pf ls lt lu pg lw lx ly ph ma mb mc md im bi translated">注意:在继续之前，最好知道什么是密集神经网络和激活函数。这里有一个故事。</p></blockquote><div class="pi pj gp gr pk pl"><a rel="noopener follow" target="_blank" href="/5-most-well-known-cnn-architectures-visualized-af76f1f0065e"><div class="pm ab fo"><div class="pn ab po cl cj pp"><h2 class="bd jd gy z fp pq fr fs pr fu fw jc bi translated">理解和可视化5种卷积神经网络架构</h2><div class="ps l"><h3 class="bd b gy z fp pq fr fs pr fu fw dk translated">为什么盗梦空间看起来像三叉戟？！</h3></div><div class="pt l"><p class="bd b dl z fp pq fr fs pr fu fw dk translated">towardsdatascience.com</p></div></div><div class="pu l"><div class="pv l pw px py pu pz lb pl"/></div></div></a></div><p id="860d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Word2Vec采用了具有单个隐藏层的<a class="ae lh" rel="noopener" target="_blank" href="/5-most-well-known-cnn-architectures-visualized-af76f1f0065e#c7c5"> <strong class="lk jd">密集神经网络</strong> </a>的使用，该隐藏层没有激活函数，在给定另一个独热码编码令牌的情况下预测一个独热码编码令牌。</p><p id="57b9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">输入层有<code class="fe ou ov ow mo b">vocab_size</code>个神经元，隐藏层有<code class="fe ou ov ow mo b">embed_dim</code>个神经元，输出层也有<code class="fe ou ov ow mo b">vocab_size</code>个神经元。输出图层通过softmax激活函数传递，该函数将问题视为多类。</p><p id="c798" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下面是网络的架构，其中<em class="ox">xᵢ</em>∑{ 0，1}对令牌进行一热编码后，∑表示上一层输出的加权和，s表示softmax。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qa"><img src="../Images/24fed4625a1411f1616c5ce35fe775e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j4RnqRjcd8rmeuUzO-jQLg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Word2Vec训练的密集神经网络架构|图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="fd9d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">与来自输入层的隐藏层相关联的权重矩阵被称为<strong class="lk jd">字嵌入</strong>，并且具有维度<code class="fe ou ov ow mo b">vocab_size × embed_dim</code>。当您在NLP任务中使用它时，它充当一个查找表，将单词转换为向量(因此得名)。在训练Word2Vec的最后，你把除了嵌入这个词以外的东西都扔掉了。</p><blockquote class="pc pd pe"><p id="3ca6" class="li lj ox lk b ll lm kd ln lo lp kg lq pf ls lt lu pg lw lx ly ph ma mb mc md im bi translated">你有神经网络模型。现在，火车数据怎么样？</p></blockquote><h2 id="b396" class="ms mt it bd na nq nr dn ne ns nt dp ni lr nu nv nk lv nw nx nm lz ny nz no iz bi translated">创建列车数据</h2><p id="028d" class="pw-post-body-paragraph li lj it lk b ll oa kd ln lo ob kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">创建数据来训练神经网络包括将每个单词分配为中心单词<strong class="lk jd">和其相邻单词</strong>作为上下文单词。相邻单词的数量由窗口(超参数)定义。</p><p id="b27a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">具体来说，让我们回到之前的例子。我们将使用window = 1(中心单词左右各一个上下文单词)。生成列车数据的过程如下所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qb"><img src="../Images/7840a9f4a5029d35120d8f286ca0cd50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n9Xx4qlkDQZRzeqFvjUWrw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">创建列车数据|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="f74b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">绿色单词是中心单词，橙色单词是上下文单词。一次一个单词，你正在创建<strong class="lk jd">(中心，上下文)</strong>对。对语料库中的每个文档重复这个步骤。</p><p id="ac08" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Word2Vec的想法是，相似的中心词将出现在相似的上下文中，您可以通过使用<strong class="lk jd">(中心，上下文)</strong>对重复训练您的模型来了解这种关系。</p><h2 id="2c27" class="ms mt it bd na nq nr dn ne ns nt dp ni lr nu nv nk lv nw nx nm lz ny nz no iz bi translated">连续的单词包和跳格</h2><p id="4b55" class="pw-post-body-paragraph li lj it lk b ll oa kd ln lo ob kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">Word2Vec有两种方法学习令牌的上下文。两者的区别在于输入数据和使用的标签。</p><p id="1e94" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 1。连续单词包(CBOW) </strong></p><p id="be19" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">给定上下文单词，CBOW将取其独热编码的平均值，并预测中心单词的独热编码。下图解释了这一过程。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qc"><img src="../Images/3af22f8344ac93859c03250c82ae4108.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NVlwnWoawacW4V0Xz5cFng.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">CBOW如何学习|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="3331" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 2。跳过程序(SG) </strong></p><p id="0451" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">给定一个中心单词，SG将对其进行一次性编码，并在输出时最大化上下文单词的概率。为每个上下文单词计算误差，然后求和。下面是训练过程。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qd"><img src="../Images/6b141a80db66b2caae6ef3cf8b2f5cdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MJiEwXdpBIwQPOCOVimyTA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">SG如何学习|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="2cd6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于softmax用于计算输出层中所有单词的概率分布(可能是数百万或更多)，因此训练过程在计算上非常昂贵。</p><p id="6150" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了解决这个问题，您可以将问题重新表述为一组独立的二进制分类任务，并使用<a class="ae lh" href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">负采样</strong> </a>。新的目标是对于任何给定的<strong class="lk jd">(单词，上下文)</strong>对，预测<strong class="lk jd">单词</strong>是否在<strong class="lk jd">中心</strong>单词的<strong class="lk jd">上下文</strong>窗口中。</p><p id="0bc7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">负采样<strong class="lk jd"> </strong>只更新正确的类和少数任意(一个超参数)不正确的类。我们之所以能够做到这一点，是因为在大量的训练数据中，我们会多次看到同一个单词作为目标类。</p><h2 id="b3bc" class="ms mt it bd na nq nr dn ne ns nt dp ni lr nu nv nk lv nw nx nm lz ny nz no iz bi translated">优势和局限性</h2><p id="d350" class="pw-post-body-paragraph li lj it lk b ll oa kd ln lo ob kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">SG可以很好地处理少量的训练数据，并且可以很好地表示不常用的单词或短语。但这是以增加计算成本为代价的。</p><p id="296e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">CBOW的训练速度比SG快几倍，对常用词的准确率略高。如果训练时间是一个大问题，并且您有足够大的数据来克服预测不常用词的问题，CBOW可能是一个更可行的选择。</p><p id="abb3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于输入层和输出层之间只有线性关系(在softmax之前)，Word2Vec产生的特征向量可以是线性相关的。比如<strong class="lk jd"> vec(国王)</strong>–<strong class="lk jd">vec(男人)+ vec(女人)≈ vec(女王)</strong>，这种对于我们这种小糊状人脑来说是有道理的。</p><p id="e8a8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，Word2Vec仍然有一些限制，其中四个是:</p><ol class=""><li id="0f53" class="of og it lk b ll lm lo lp lr oh lv oi lz oj md ok ol om on bi translated">Word2Vec依赖于关于单词的本地信息，即单词的上下文仅依赖于其邻居。</li><li id="d0eb" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated">单词嵌入是训练神经网络的副产品，因此特征向量之间的线性关系是一个黑盒(某种)。</li><li id="6fde" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated">Word2Vec无法理解词汇外(OOV)单词，即不存在于训练数据中的单词。您可以分配一个用于所有OOV单词的UNK令牌，或者您可以使用对OOV单词稳定的其他模型。</li><li id="5c15" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated">通过给每个单词分配不同的向量，Word2Vec忽略单词的<a class="ae lh" href="https://en.wikipedia.org/wiki/Morphology_(linguistics)" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd"/></a>。比如<em class="ox">吃</em>、<em class="ox">吃</em>、吃<em class="ox">吃</em>被Word2Vec认为是独立不同的词，但它们来自同一个词根:<em class="ox">吃</em>。这个信息可能有用。</li></ol><p id="5402" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在下一个故事中，我们将提出并解释理论上可以解决这些限制的嵌入模型。敬请期待！</p><h1 id="b0f4" class="mz mt it bd na nb nc nd ne nf ng nh ni ki nj kj nk kl nl km nm ko nn kp no np bi translated">摘要</h1><p id="31eb" class="pw-post-body-paragraph li lj it lk b ll oa kd ln lo ob kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">感谢到达终点！</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qe"><img src="../Images/949d8816918fd3b6f14032a6cb863e0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AcUHP38janvJZ0eT"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@markusspiske?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Markus Spiske </a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="8fef" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这个故事中，向您介绍了两种可以从文本数据中提取特征的方法:</p><ol class=""><li id="a7df" class="of og it lk b ll lm lo lp lr oh lv oi lz oj md ok ol om on bi translated"><strong class="lk jd">单词包</strong>标记每个文档并统计每个标记的出现次数。</li><li id="fbc2" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated"><strong class="lk jd"> Word2Vec </strong>使用具有单个隐藏层的密集神经网络来从一个热编码的单词中学习单词嵌入。</li></ol><p id="1966" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">虽然单词包很简单，但它没有捕捉到标记之间的关系，并且对于大型语料库来说，所获得的特征维数变得非常大。Word2Vec通过使用<strong class="lk jd">(中心，上下文)</strong>单词对解决了这个问题，并允许我们定制特征向量的长度。</p><p id="5984" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，Word2Vec并不完美。它不能理解OOV的文字，忽略了文字的形态学。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qf"><img src="../Images/e1a6e3674ab93bcb99796285f9d0175c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*6HsoGpmIb1oibJc_JWbqJA.gif"/></div></div></figure></div><div class="ab cl qg qh hx qi" role="separator"><span class="qj bw bk qk ql qm"/><span class="qj bw bk qk ql qm"/><span class="qj bw bk qk ql"/></div><div class="im in io ip iq"><p id="4859" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">🔥你好！如果你喜欢这个故事，想支持我这个作家，可以考虑 <a class="ae lh" href="https://dwiuzila.medium.com/membership" rel="noopener"> <strong class="lk jd"> <em class="ox">成为会员</em> </strong> </a> <em class="ox">。每月只需5美元，你就可以无限制地阅读媒体上的所有报道。如果你注册使用我的链接，我会赚一小笔佣金。</em></p><p id="bd58" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">🔖<em class="ox">想了解更多关于经典机器学习模型的工作原理，以及它们如何优化参数？或者MLOps大型项目的例子？有史以来最优秀的文章呢？继续阅读:</em></p><div class="pi pj gp gr pk"><div role="button" tabindex="0" class="ab bv gv cb fp qn qo bn qp lb ex"><div class="qq l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qr qs fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qr qs fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----e386d9ed84aa--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="qv qw gw l"><h2 class="bd jd vw of fp vx fr fs pr fu fw jc bi translated">从零开始的机器学习</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi vy au vz wa wb so wc an eh ei wd we wf el em eo de bk ep" href="https://dwiuzila.medium.com/list/machine-learning-from-scratch-b35db8650093?source=post_page-----e386d9ed84aa--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wg l fo"><span class="bd b dl z dk">8 stories</span></div></div></div><div class="ri dh rj fp ab rk fo di"><div class="di ra bv rb rc"><div class="dh l"><img alt="" class="dh" src="../Images/4b97f3062e4883b24589972b2dc45d7e.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*CWNoicci28F2TUQc-vKijw.png"/></div></div><div class="di ra bv rd re rf"><div class="dh l"><img alt="" class="dh" src="../Images/b1f7021514ba57a443fe0db4b7001b26.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*wSRsSHYnIiGJFAqC"/></div></div><div class="di bv rg rh rf"><div class="dh l"><img alt="" class="dh" src="../Images/deb73e42c79667024a46c2c8902b81fa.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HVEz7KwzO0tv1Q4d"/></div></div></div></div></div><div class="pi pj gp gr pk"><div role="button" tabindex="0" class="ab bv gv cb fp qn qo bn qp lb ex"><div class="qq l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qr qs fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qr qs fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----e386d9ed84aa--------------------------------" rel="noopener follow" target="_top">艾伯斯乌兹拉</a></p></div></div><div class="qv qw gw l"><h2 class="bd jd vw of fp vx fr fs pr fu fw jc bi translated">高级优化方法</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi vy au vz wa wb so wc an eh ei wd we wf el em eo de bk ep" href="https://dwiuzila.medium.com/list/advanced-optimization-methods-26e264a361e4?source=post_page-----e386d9ed84aa--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wg l fo"><span class="bd b dl z dk">7 stories</span></div></div></div><div class="ri dh rj fp ab rk fo di"><div class="di ra bv rb rc"><div class="dh l"><img alt="" class="dh" src="../Images/15b3188b0f29894c2bcf3d0965515f44.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*BVMamoNudzn9UlAE"/></div></div><div class="di ra bv rd re rf"><div class="dh l"><img alt="" class="dh" src="../Images/3249ba2cf680952e2ccdff36d8ebf4a7.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*C1fv3HJdh1RBspwN"/></div></div><div class="di bv rg rh rf"><div class="dh l"><img alt="" class="dh" src="../Images/a73f0494533d8a08b01c2b899373d2b9.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*QZvzgiM2VnhYyx8M"/></div></div></div></div></div><div class="pi pj gp gr pk"><div role="button" tabindex="0" class="ab bv gv cb fp qn qo bn qp lb ex"><div class="qq l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qr qs fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qr qs fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----e386d9ed84aa--------------------------------" rel="noopener follow" target="_top">艾伯斯乌兹拉</a></p></div></div><div class="qv qw gw l"><h2 class="bd jd vw of fp vx fr fs pr fu fw jc bi translated">MLOps大型项目</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi vy au vz wa wb so wc an eh ei wd we wf el em eo de bk ep" href="https://dwiuzila.medium.com/list/mlops-megaproject-6a3bf86e45e4?source=post_page-----e386d9ed84aa--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wg l fo"><span class="bd b dl z dk">6 stories</span></div></div></div><div class="ri dh rj fp ab rk fo di"><div class="di ra bv rb rc"><div class="dh l"><img alt="" class="dh" src="../Images/41b5d7dd3997969f3680648ada22fd7f.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*EBS8CP_UnStLesXoAvjeAQ.png"/></div></div><div class="di ra bv rd re rf"><div class="dh l"><img alt="" class="dh" src="../Images/41befac52d90334c64eef7fc5c4b4bde.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*XLpRKnIMcJzBzCwvXrLvsw.png"/></div></div><div class="di bv rg rh rf"><div class="dh l"><img alt="" class="dh" src="../Images/80908ef475e97fbc42efe3fae0dfcff5.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*K_gzBmjv-ZHlU0Q6HeXclQ.jpeg"/></div></div></div></div></div><div class="pi pj gp gr pk"><div role="button" tabindex="0" class="ab bv gv cb fp qn qo bn qp lb ex"><div class="qq l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qr qs fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qr qs fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----e386d9ed84aa--------------------------------" rel="noopener follow" target="_top">艾伯斯乌兹拉</a></p></div></div><div class="qv qw gw l"><h2 class="bd jd vw of fp vx fr fs pr fu fw jc bi translated">我最好的故事</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi vy au vz wa wb so wc an eh ei wd we wf el em eo de bk ep" href="https://dwiuzila.medium.com/list/my-best-stories-d8243ae80aa0?source=post_page-----e386d9ed84aa--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wg l fo"><span class="bd b dl z dk">24 stories</span></div></div></div><div class="ri dh rj fp ab rk fo di"><div class="di ra bv rb rc"><div class="dh l"><img alt="" class="dh" src="../Images/0c862c3dee2d867d6996a970dd38360d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*K1SQZ1rzr4cb-lSi"/></div></div><div class="di ra bv rd re rf"><div class="dh l"><img alt="" class="dh" src="../Images/392d63d181090365a63dc9060573bcff.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*hSKy6kKorAfHjHOK"/></div></div><div class="di bv rg rh rf"><div class="dh l"><img alt="" class="dh" src="../Images/f51725806220b60eccf5d4c385c700e9.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HiyGwoGOMI5Ao_fd"/></div></div></div></div></div><div class="pi pj gp gr pk"><div role="button" tabindex="0" class="ab bv gv cb fp qn qo bn qp lb ex"><div class="qq l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qr qs fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qr qs fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----e386d9ed84aa--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="qv qw gw l"><h2 class="bd jd vw of fp vx fr fs pr fu fw jc bi translated">R中的数据科学</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi vy au vz wa wb so wc an eh ei wd we wf el em eo de bk ep" href="https://dwiuzila.medium.com/list/data-science-in-r-0a8179814b50?source=post_page-----e386d9ed84aa--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wg l fo"><span class="bd b dl z dk">7 stories</span></div></div></div><div class="ri dh rj fp ab rk fo di"><div class="di ra bv rb rc"><div class="dh l"><img alt="" class="dh" src="../Images/e52e43bf7f22bfc0889cc794dcf734dd.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*10B3radiyQGAp-QA"/></div></div><div class="di ra bv rd re rf"><div class="dh l"><img alt="" class="dh" src="../Images/945fa9100c2a00b46f8aca3d3975f288.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*o6A863Vdwq7ThlmW"/></div></div><div class="di bv rg rh rf"><div class="dh l"><img alt="" class="dh" src="../Images/3ca9e4b148297dbc4e7da0a180cf9c99.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*ekmX89TW6N8Bi8bL"/></div></div></div></div></div></div><div class="ab cl qg qh hx qi" role="separator"><span class="qj bw bk qk ql qm"/><span class="qj bw bk qk ql qm"/><span class="qj bw bk qk ql"/></div><div class="im in io ip iq"><p id="6c05" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[1]托马斯·米科洛夫、程凯、格雷戈·科拉多、杰弗里·迪恩(2013): <em class="ox">向量空间中单词表征的有效估计</em>。<a class="ae lh" href="https://arxiv.org/abs/1301.3781v3" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1301.3781v3</a></p><p id="5dcf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[2]拉迪姆·řehůřek(2022):<em class="ox">教程:面向学习的课程</em>。<a class="ae lh" href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py" rel="noopener ugc nofollow" target="_blank">Gensim上的Word2Vec模型</a></p><p id="39f3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[3]悟空·莫汉达斯(2021): <em class="ox">嵌入——用ML制作</em>。<a class="ae lh" href="https://madewithml.com/courses/foundations/embeddings/" rel="noopener ugc nofollow" target="_blank">https://madewithml.com</a></p><p id="8cf0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[4] Eric Kim (2019): <em class="ox">揭秘Skip-Gram语言建模中的神经网络</em>。<a class="ae lh" href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling" rel="noopener ugc nofollow" target="_blank">https://aegis 4048 . github . io</a></p></div></div>    
</body>
</html>