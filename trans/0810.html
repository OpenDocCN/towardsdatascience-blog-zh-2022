<html>
<head>
<title>Q-learning for beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">q-初学者学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/q-learning-for-beginners-2837b777741#2022-03-07">https://towardsdatascience.com/q-learning-for-beginners-2837b777741#2022-03-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6930" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">训练一个 AI 来解决冰冻的湖泊环境</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9d5de0f0b725a36bd5d3dbc7bc28f6fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ytpUzFLslVd8pk7D3jQRA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="5883" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi lr translated"><span class="l ls lt lu bm lv lw lx ly lz di"> T </span>这篇文章的目标是<strong class="kx ir">教人工智能如何使用强化学习解决❄️Frozen 湖环境问题</strong>。我们不再阅读维基百科的文章和解释公式，而是从零开始，尝试重新创建🤖q-自学算法。我们不仅要了解<strong class="kx ir">它是如何工作的</strong>，更重要的是<strong class="kx ir">它为什么会工作</strong>:为什么要这样设计？有哪些隐藏的假设，常规课程和教程中从来不解释的细节？</p><p id="7ead" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本文结束时，您将<strong class="kx ir">掌握 Q 学习算法</strong>，并能够<strong class="kx ir">将其应用于其他环境和现实世界的问题</strong>。这是一个很酷的迷你项目，让人们更好地了解强化学习是如何工作的，并且有望激发原创应用的想法。</p><p id="d71c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们首先安装❄️ <strong class="kx ir">冰封湖</strong>环境并导入必要的库:<code class="fe ma mb mc md b">gym</code>用于游戏，<code class="fe ma mb mc md b">random</code>用于生成随机数，<code class="fe ma mb mc md b">numpy</code>用于做一些数学运算。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="me mf l"/></div></figure></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="c053" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">❄️岛冰封湖</h1><p id="c69e" class="pw-post-body-paragraph kv kw iq kx b ky nf jr la lb ng ju ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi lr translated">现在，让我们来谈谈我们将在本教程中解决的游戏。❄️ <strong class="kx ir">冰湖</strong>是一个由瓷砖组成的简单环境，在这里 AI 必须<strong class="kx ir">从最初的瓷砖</strong>移动到<strong class="kx ir">目标</strong>。瓷砖可以是一个安全的✅冰湖，也可以是一个让你永远陷进去的❌。人工智能或代理有 4 种可能的动作:走◀️ <strong class="kx ir">左</strong>，🔽<strong class="kx ir">向下</strong>，▶️ <strong class="kx ir">向右</strong>，或者🔼<strong class="kx ir">涨</strong>。代理必须学会避开洞，以便<strong class="kx ir">在<strong class="kx ir">最少的动作</strong>中达到目标</strong>。默认情况下，环境<strong class="kx ir">总是处于相同的配置</strong>。在环境代码中，<strong class="kx ir">每个图块由字母</strong>表示，如下所示:</p><pre class="kg kh ki kj gt nk md nl nm aw nn bi"><span id="7b8b" class="no mo iq md b gy np nq l nr ns">S F F F       (S: starting point, safe)<br/>F H F H       (F: frozen surface, safe)<br/>F F F H       (H: hole, stuck forever)<br/>H F F G       (G: goal, safe)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/8ce03e3513d9d81a6b09ba91a669d7c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*QVMC98MU5aE5dgGCT_IvLQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="1e83" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以尝试手动解决上面的例子来理解游戏。看看下面的动作顺序是不是正确的解法:<strong class="kx ir">右</strong> → <strong class="kx ir">右</strong> → <strong class="kx ir">右</strong> → <strong class="kx ir">下</strong> → <strong class="kx ir">下</strong> → <strong class="kx ir">下</strong>。我们的代理人从瓦<strong class="kx ir">s</strong>开始，所以我们在冰冻的表面上向右移动✅，然后再一次是✅，然后再一次是✅，然后我们下去找一个洞❌.</p><p id="8abf" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其实找几个正确的解法真的很容易:<strong class="kx ir">右</strong> → <strong class="kx ir">右</strong> → <strong class="kx ir">下</strong> → <strong class="kx ir">下</strong> → <strong class="kx ir">下</strong> → <strong class="kx ir">右</strong>就是很明显的一个。但是我们可以做一系列动作，在到达目标之前绕着一个洞转 10 圈。这个序列是有效的，但是它不满足我们的最终需求:<strong class="kx ir">代理需要在最少的动作数内达到目标</strong>。在这个例子中，完成游戏的最少动作数是<strong class="kx ir"> 6 </strong>。我们需要记住这个事实，以检查我们的代理商是否真的掌握了❄️ <strong class="kx ir">冰封湖</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/4bbcd9ca5df0e260095ea49e1600ba0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*oUW1prwRzihD4ebeS7iu7w.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="6765" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们通过<code class="fe ma mb mc md b">gym</code>库来初始化环境。游戏有两个版本:一个有<strong class="kx ir">滑冰</strong>，其中选择的动作有<strong class="kx ir">随机几率被代理</strong>忽略；和一个<strong class="kx ir">防滑器</strong>，其中<strong class="kx ir">的动作不能忽略</strong>。我们将使用<strong class="kx ir">防滑</strong>开始，因为它更容易理解。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="me mf l"/></div></figure><pre class="kg kh ki kj gt nk md nl nm aw nn bi"><span id="de5e" class="no mo iq md b gy np nq l nr ns">🟥FFF<br/>FHFH<br/>FFFH<br/>HFFG</span></pre><p id="3376" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以看到，创建的游戏<strong class="kx ir">与我们的示例</strong>具有完全相同的配置:这是相同的谜题。我们代理的位置由一个红色矩形<strong class="kx ir">表示。解决这个难题可以用一个简单的脚本和 if…else 条件来完成，这实际上对于比较我们的人工智能和一个更简单的方法是有用的。但是，我们想尝试一个更刺激的方案:<strong class="kx ir">强化学习</strong>。</strong></p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="c44f" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">🏁二。q 表</h1><p id="bfbc" class="pw-post-body-paragraph kv kw iq kx b ky nf jr la lb ng ju ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi lr translated"><span class="l ls lt lu bm lv lw lx ly lz di">在</span> ❄️ <strong class="kx ir">冰封湖</strong>中，有 16 个瓷砖，这意味着我们的代理可以在 16 个不同的位置找到，称为<strong class="kx ir">状态</strong>。对于每个状态，有 4 种可能的动作:走◀️ <strong class="kx ir">左</strong>，🔽<strong class="kx ir">向下</strong>，▶️ <strong class="kx ir">向右</strong>，以及🔼<strong class="kx ir">向上</strong>。学习如何玩冰封湖就像<strong class="kx ir">学习在每个状态下你应该选择哪个动作</strong>。为了知道在给定的状态下哪一个动作是最好的，我们想给我们的动作分配一个<strong class="kx ir">质量值</strong>。我们有 16 个状态和 4 个动作，所以要计算 16 x 4 = 64 个值。</p><p id="c797" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">表示它的一个好方法是使用一个表格，称为 Q-table，其中<strong class="kx ir">行列出每个州的</strong>，而<strong class="kx ir">列列出每个动作 a </strong>。在这个 Q-table 中，每个单元格都包含一个值 Q(s，a)，它是状态 s 中动作 a 的<strong class="kx ir">值(如果是可能的最佳动作，则为 1；如果真的很差，则为 0)。当我们的代理处于特定状态 s 时，它<strong class="kx ir">只需检查这个表，看看哪个动作具有最高值</strong>。采取价值最高的行动是有道理的，但是我们稍后会看到我们可以设计出更好的东西</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="me mf l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nu">Q 表示例，其中每个单元格包含给定状态</em> s <em class="nu">(行)</em>下动作 a <em class="nu">(列)的值</em> Q(a，s) <em class="nu"/></p></figure><p id="e3cd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们创建我们的 Q 表并用零填充它，因为<strong class="kx ir">我们仍然不知道每个状态</strong>中每个动作的值。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="me mf l"/></div></figure><pre class="kg kh ki kj gt nk md nl nm aw nn bi"><span id="c381" class="no mo iq md b gy np nq l nr ns">Q-table =<br/>[[0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]]</span></pre><p id="d4c0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">太好了！我们的 Q-table 有 16 行(我们的 16 个州)和 4 列<strong class="kx ir"/>(我们的 4 个动作)，正如预期的那样。让我们试着看看下一步我们能做什么:每个值都被设置为零，所以我们完全没有信息。假设代理采取了一个<strong class="kx ir">随机动作</strong> : ◀️ <strong class="kx ir">左</strong>，🔽<strong class="kx ir">向下</strong>，▶️ <strong class="kx ir">向右</strong>，或者🔼<strong class="kx ir">向上</strong>。</p><p id="c536" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以使用带有<code class="fe ma mb mc md b">choice</code>方法的<code class="fe ma mb mc md b">random</code>库来随机选择一个动作。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="me mf l"/></div></figure><pre class="kg kh ki kj gt nk md nl nm aw nn bi"><span id="be3d" class="no mo iq md b gy np nq l nr ns">'LEFT'</span></pre><p id="b0cb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">等等，实际上代理当前处于初始状态<strong class="kx ir"> S </strong>，这意味着只有两个动作是可能的:▶️ <strong class="kx ir">右</strong>和🔽<strong class="kx ir">向下</strong>。代理也可以采取措施🔼<strong class="kx ir">向上</strong>和◀️ <strong class="kx ir">向左</strong>，但是它不动:它的状态不变。因此，我们<strong class="kx ir">不会对可能的行为</strong>施加任何约束:代理人<strong class="kx ir">自然会理解他们中的一些人不做任何事情</strong>。</p><p id="21c8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以继续使用<code class="fe ma mb mc md b">random.choice()</code>，但是<code class="fe ma mb mc md b">gym</code>库<strong class="kx ir">已经实现了随机选择一个动作</strong>的方法。以后可能会省去我们一些麻烦，所以让我们试试吧。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="me mf l"/></div></figure><pre class="kg kh ki kj gt nk md nl nm aw nn bi"><span id="902f" class="no mo iq md b gy np nq l nr ns">0</span></pre><p id="248f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">哎哟...这次是一个<strong class="kx ir">号</strong>。我们可以阅读<code class="fe ma mb mc md b"><a class="ae nv" href="https://gym.openai.com/docs/" rel="noopener ugc nofollow" target="_blank">gym</a></code> <a class="ae nv" href="https://gym.openai.com/docs/" rel="noopener ugc nofollow" target="_blank">的文档</a>，但不幸的是，这是非常稀缺的。不过不用担心，<a class="ae nv" href="https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py#L10" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">我们可以查看 GitHub </strong> </a>上的源代码来理解<strong class="kx ir">这些数字的含义</strong>。这其实非常简单:</p><pre class="kg kh ki kj gt nk md nl nm aw nn bi"><span id="992e" class="no mo iq md b gy np nq l nr ns">◀️ LEFT = 0<br/>🔽 DOWN = 1<br/>▶️ RIGHT = 2<br/>🔼 UP = 3</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/df10679c99217ed70f83d123fa41d779.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*LnBPWvGkgBP10I1Z6S6o0Q.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="1d9c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">好了，现在<strong class="kx ir">我们了解了</strong> <code class="fe ma mb mc md b"><strong class="kx ir">gym</strong></code> <strong class="kx ir">如何将数字与方向</strong>联系起来，让我们试着用它来<strong class="kx ir">将我们的代理移动到右边的</strong> ▶️.这一次，可以使用<code class="fe ma mb mc md b">step(action)</code>方法来执行。我们可以尝试<strong class="kx ir">直接给它提供数字 2 </strong>，对应我们选择的方向(右)，检查代理是否移动。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="me mf l"/></div></figure><pre class="kg kh ki kj gt nk md nl nm aw nn bi"><span id="c4ba" class="no mo iq md b gy np nq l nr ns">(Right)<br/>S🟥FF<br/>FHFH<br/>FFFH<br/>HFFG</span></pre><p id="2a44" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">万岁</strong>！红色方块从初始状态<strong class="kx ir"> S </strong>向右移动:<strong class="kx ir">我们的预测是正确的</strong>。为了与环境互动，我们只需要知道这些:</p><ol class=""><li id="84d5" class="nw nx iq kx b ky kz lb lc le ny li nz lm oa lq ob oc od oe bi translated">如何<strong class="kx ir">使用<code class="fe ma mb mc md b">action_space.sample()</code>随机选择一个动作</strong>；</li><li id="af70" class="nw nx iq kx b ky of lb og le oh li oi lm oj lq ob oc od oe bi translated">如何<strong class="kx ir">执行该动作，并使用<code class="fe ma mb mc md b">step(action)</code>将我们的代理移动到期望的方向</strong>。</li></ol><p id="3b01" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了完全详尽，我们可以加上:</p><ol class=""><li id="9409" class="nw nx iq kx b ky kz lb lc le ny li nz lm oa lq ob oc od oe bi translated">如何<strong class="kx ir">显示当前地图，看看我们在用<code class="fe ma mb mc md b">render()</code>做什么</strong>；</li><li id="d515" class="nw nx iq kx b ky of lb og le oh li oi lm oj lq ob oc od oe bi translated">如何<strong class="kx ir">重新开始游戏</strong>当代理人掉进一个洞或者用<code class="fe ma mb mc md b">reset()</code>到达目标<strong class="kx ir"> G </strong>。</li></ol><p id="825e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">既然我们理解了如何与我们的<code class="fe ma mb mc md b">gym</code>环境交互，让我们回到我们的算法。在强化学习中，<strong class="kx ir">智能体在完成预定目标</strong>时会得到环境的奖励。在❄️ <strong class="kx ir">冰封湖</strong>中，代理人只有达到状态<strong class="kx ir"> G </strong>才会获得奖励(见<a class="ae nv" href="https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py#L85" rel="noopener ugc nofollow" target="_blank">源代码</a>)。我们无法控制这个奖励，它是在环境中设置的:<strong class="kx ir">当代理达到 G 时为 1，否则为 0</strong>。</p><p id="d244" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们每次执行一个动作就把它打印出来。奖励通过<code class="fe ma mb mc md b">step(action)</code>的方式给出。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="me mf l"/></div></figure><pre class="kg kh ki kj gt nk md nl nm aw nn bi"><span id="944f" class="no mo iq md b gy np nq l nr ns">(Left)<br/>🟥FFF<br/>FHFH<br/>FFFH<br/>HFFG<br/>Reward = 0.0</span></pre><p id="2b15" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">奖励确实是 0…😱哇，我想我们陷入困境了，因为在整个游戏中只有一个州能给我们正奖励。当我们唯一的确认是在最后的时候，我们怎么能从一开始就选择正确的方向呢？如果我们想要看到 1 的奖励，我们需要足够幸运，能够<strong class="kx ir">偶然找到正确的行动顺序。不幸的是，这正是它的工作方式…<strong class="kx ir">Q 表将保持填充零，直到代理随机达到目标 G </strong>。</strong></p><p id="f165" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我们能有中间的、较小的奖励来引导我们走向目标，问题会简单得多。唉，这实际上是强化学习的<strong class="kx ir">主要问题之一:这种被称为<strong class="kx ir">稀疏奖励</strong>的现象，使得代理很难在问题<strong class="kx ir">上接受训练，因为唯一的奖励是在一长串动作</strong>的结尾。人们提出了不同的技术来缓解这个问题，但我们将在另一个时间讨论它。</strong></p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="752e" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">🤖三。q 学习</h1><p id="07ec" class="pw-post-body-paragraph kv kw iq kx b ky nf jr la lb ng ju ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi lr translated">让我们回到我们的问题上来。好吧，我们需要足够幸运，偶然发现目标<strong class="kx ir"> G </strong>。但是一旦完成，如何将信息反向传播到初始状态？这🤖Q 学习算法为这个问题提供了一个巧妙的解决方案。我们需要更新我们的状态-动作对(Q 表中的每个单元格)的值，考虑 1/到达下一个状态的<strong class="kx ir">奖励</strong>，以及 2/下一个状态中的<strong class="kx ir">最高可能值。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/e6c739ac53a7e7e53ac3e9f45351111f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Sb0uYW0DWAJN72l7GZMZKQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="986e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们知道，当我们移动到<strong class="kx ir"> G </strong>时，我们得到的回报是 1。正如我们刚才所说的，G (姑且称之为<strong class="kx ir"> G-1 </strong>)与<strong class="kx ir">到达 G </strong>的相关动作的<strong class="kx ir">的值由于奖励而增加。好的，这一集的结尾:代理赢了，我们重新开始游戏。现在，下一次代理处于 G-1 </strong>旁边的<strong class="kx ir"> a 状态时，它会用<strong class="kx ir">的相关动作增加这个状态的值(姑且称之为<strong class="kx ir"> G-2 </strong>)到达 G-1 </strong>。下一次代理处于<strong class="kx ir"> G-2 </strong>旁边的状态时，它也会这样做。清洗并重复，直到更新达到初始状态<strong class="kx ir"> S </strong>。</strong></p><p id="a80a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们试着找到<strong class="kx ir">更新公式</strong>将值从<strong class="kx ir"> G </strong>反向传播到<strong class="kx ir"> S </strong>。记住:数值表示在特定状态下<strong class="kx ir">一个动作的<strong class="kx ir">质量</strong>(0 表示很糟糕，1 表示在这种状态下可能是最好的动作)。我们尝试<strong class="kx ir">在 sₜ状态下(例如，当代理处于初始状态<strong class="kx ir"> S </strong>时，sₜ = 0)更新动作 aₜ的值</strong>(例如，如果动作为左，则 aₜ=为 0)。这个<strong class="kx ir">值只是我们的 q 表</strong>中的一个单元格，对应于<strong class="kx ir">行号 s </strong> ₜ <strong class="kx ir">和列号 a </strong> ₜ:这个值的正式名称是 aₜ).q(sₜ</strong></p><p id="2131" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如前所述，我们需要使用 1/ <strong class="kx ir">下一个状态</strong>(正式标注为 rₜ)的奖励，以及 2/ <strong class="kx ir">下一个状态</strong> (maxₐ <em class="ok"> Q(s </em> ₜ₊₁，a))的最大可能值来更新它。因此，更新公式必须如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/9b63ab11f753b7deebbf64ffc8bcceeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zh5TsI0ASrVgJjllW26Llw.png"/></div></div></figure><p id="d480" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">新值是当前值+奖励值+下一个状态的最高值。我们可以手动尝试我们的公式来检查它看起来是否正确:让我们假设我们的代理人第一次在目标 G 旁边的状态 G-1 中<strong class="kx ir">。我们可以用以下方式更新对应于此状态下获胜动作的值<strong class="kx ir"> G-1 </strong>:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/2a4ea004c1a87a6d9356542ae9f9514a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nQ8Lsyp8CkQ_3XZ3IZxSUQ.png"/></div></div></figure><p id="d599" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中 Q(G-1，aₜ) = 0 和 maxₐ <em class="ok"> Q(G </em>，a) = 0 是因为 q 表是空的，rₜ <em class="ok"> = 1 </em>是因为我们在这个环境中得到唯一的奖励。我们得到 Q{new}(G-1，aₜ) = 1。下次代理处于下一个状态(<strong class="kx ir"> G-2 </strong>)时，我们也使用公式更新它，得到相同的结果:<em class="ok"> Q </em> {new}(G-2，aₜ) = 1。最后，<strong class="kx ir">我们将 Q 表</strong>中的 1 从<strong class="kx ir"> G </strong>反向传播到<strong class="kx ir"> S </strong>。好吧，它工作了，但是结果是<strong class="kx ir">二元的</strong>:要么是<strong class="kx ir">错误的状态-动作对，要么是最好的状态-动作对</strong>。我们想要更多的细微差别…</p><p id="781e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其实我们差不多<strong class="kx ir">用常识找到了真正的 Q-learning 更新公式</strong>。我们寻找的细微差别增加了两个参数:</p><ul class=""><li id="bcdb" class="nw nx iq kx b ky kz lb lc le ny li nz lm oa lq on oc od oe bi translated"><strong class="kx ir"> α </strong>是💡<strong class="kx ir">学习率</strong>(在 0 和 1 之间)，也就是我们应该改变多少原来的 q(sₜ(aₜ)值。如果α = 0，数值<strong class="kx ir">永远不变</strong>，但如果α = 1，数值<strong class="kx ir">变化极快</strong>。在我们的尝试中，我们没有限制学习速率，所以α = 1。但这在现实中太快了:下一个状态的奖励和最大值很快<strong class="kx ir">压倒了当前值</strong>。我们需要在过去和新知识的重要性之间找到一个平衡。</li><li id="bf78" class="nw nx iq kx b ky of lb og le oh li oi lm oj lq on oc od oe bi translated"><strong class="kx ir"> γ </strong>是📉<strong class="kx ir">贴现因子</strong>(0 到 1 之间)，决定了相对于眼前的回报，代理人对未来回报的关心程度(俗话说“一鸟在手胜过双鸟在林”)。如果γ = 0，代理只关注<strong class="kx ir">即时奖励</strong>，但是如果γ = 1，任何<strong class="kx ir">潜在的未来奖励与当前奖励</strong>具有相同的价值。在❄️ <strong class="kx ir">冰封湖</strong>，我们想要一个高折扣系数，因为在游戏的最后只有一个可能的奖励。</li></ul><p id="6e08" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用实数 Q 学习算法，新值计算如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/94e9b631f08a1579d274194648eebbd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hEl3nsWIsqi4fB51halCXw.png"/></div></div></figure><p id="f508" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">好吧，让我们在实施这个新公式之前先试一试。再一次，我们可以假装我们的代理人是第一次目标 G 旁边的<strong class="kx ir"/>。我们可以使用我们的公式更新状态-动作对以赢得游戏:Q{new}(G-1，aₜ)= 0+α(1+γ00)<em class="ok">。</em>我们可以给α和γ赋值任意值来计算结果。当α = 0.5，γ = 0.9 时，我们得到 Q{new}(G-1，aₜ)= 0+0.5(1+0.9 0)= 0.5。代理第二次处于这种状态时，我们会得到:Q{new}(G-1，aₜ)= 0.5+0.5(1+0.9 0.5)= 0.75，然后是 0.875，0.9375，0.96875，等等。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/feb29bdf031f44c5ff4eed7f8db02f34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*3c5t2lubdnSlZI77DP4K6w.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="5399" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以用代码训练我们的特工意味着:</p><ol class=""><li id="f758" class="nw nx iq kx b ky kz lb lc le ny li nz lm oa lq ob oc od oe bi translated"><strong class="kx ir">选择一个随机动作</strong>(使用<code class="fe ma mb mc md b">action_space.sample()</code>)如果当前状态的值正好为零。否则，我们用函数<code class="fe ma mb mc md b">np.argmax()</code>取当前状态下值最高的<strong class="kx ir">动作；</strong></li><li id="ed84" class="nw nx iq kx b ky of lb og le oh li oi lm oj lq ob oc od oe bi translated"><strong class="kx ir">通过<code class="fe ma mb mc md b">step(action)</code>向所需方向移动来执行此动作</strong>；</li><li id="3b39" class="nw nx iq kx b ky of lb og le oh li oi lm oj lq ob oc od oe bi translated"><strong class="kx ir">使用关于新状态的信息和<code class="fe ma mb mc md b">step(action)</code>给出的奖励，用我们采取的动作更新原始状态的值</strong>；</li></ol><p id="ba81" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们不断重复这 3 个步骤，直到代理<strong class="kx ir">卡在洞内</strong>或<strong class="kx ir">到达目标 G </strong>。当它发生时，我们只需<strong class="kx ir">用<code class="fe ma mb mc md b">reset()</code>重启环境</strong>，开始新的一集，直到我们达到 1000 集。此外，我们可以绘制每次运行的<strong class="kx ir">结果(如果没有达到目标，则为失败，否则为成功),以便<strong class="kx ir">观察我们代理的进度</strong>。</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="me mf l"/></div></figure><pre class="kg kh ki kj gt nk md nl nm aw nn bi"><span id="5f8f" class="no mo iq md b gy np nq l nr ns">Q-table before training:<br/>[[0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]]<br/><br/>===========================================<br/>Q-table after training:<br/>[[0.         0.         0.59049    0.        ]<br/> [0.         0.         0.6561     0.        ]<br/> [0.         0.729      0.         0.        ]<br/> [0.         0.         0.         0.        ]<br/> [0.         0.02050313 0.         0.        ]<br/> [0.         0.         0.         0.        ]<br/> [0.         0.81       0.         0.        ]<br/> [0.         0.         0.         0.        ]<br/> [0.         0.         0.17085938 0.        ]<br/> [0.         0.         0.49359375 0.        ]<br/> [0.         0.9        0.         0.        ]<br/> [0.         0.         0.         0.        ]<br/> [0.         0.         0.         0.        ]<br/> [0.         0.         0.         0.        ]<br/> [0.         0.         1.         0.        ]<br/> [0.         0.         0.         0.        ]]</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/76ad154f065fb685d5f67ae61b0d5de0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rx5_OYgmdnYpnBYpRcLAAg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="2f27" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">特工训练有素！图上的每个蓝条对应一次胜利，因此我们可以看到代理在培训开始时很难找到目标。但是一旦它连续几次发现，就开始<strong class="kx ir">一致赢</strong>。🥳训练过的 q 表也很有趣:这些值表明了代理为达到目标而学习的独特的行动顺序。</p><p id="0d9a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在通过 100 集的评测来看看它的表现如何。我们认为培训已经结束，因此<strong class="kx ir">我们不再需要更新 Q 表</strong>。要查看代理的表现，我们可以<strong class="kx ir">计算它设法达到目标</strong>的次数百分比(成功率)。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="me mf l"/></div></figure><pre class="kg kh ki kj gt nk md nl nm aw nn bi"><span id="0ab7" class="no mo iq md b gy np nq l nr ns">Success rate = 100.0%</span></pre><p id="eb77" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的代理不仅接受了培训，而且成功地达到了 100%的成功率。大家干得好，防滑的❄️ <strong class="kx ir">冰湖</strong>解决了！</p><p id="6ac4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们甚至可以<strong class="kx ir">通过执行下面的代码来可视化代理在地图</strong>上的移动，并打印<strong class="kx ir">所采取的动作序列</strong>来检查它是否是最好的一个。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="me mf l"/></div></figure><pre class="kg kh ki kj gt nk md nl nm aw nn bi"><span id="3f66" class="no mo iq md b gy np nq l nr ns">(Right)<br/>SFFF<br/>FHFH<br/>FFFH<br/>HFF🟥<br/>Sequence = [2, 2, 1, 1, 1, 2]</span></pre><p id="06a1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">代理可以学习几种正确的动作顺序:[2，2，1，1，1，2]，[1，1，2，2，1，2]等。好的一面是在我们的序列中只有<strong class="kx ir">6 个动作，这是我们统计的</strong>最小可能动作数<strong class="kx ir">:这意味着我们的代理学会了以最佳方式解决游戏。在[2，2，1，1，1，2]的情况下，对应的是右→右→下→下→下→右，正是我们在文章最开始预测的顺序。📣</strong></p><h1 id="2bea" class="mn mo iq bd mp mq oq ms mt mu or mw mx jw os jx mz jz ot ka nb kc ou kd nd ne bi translated">📐四。ε-贪婪算法</h1><p id="614e" class="pw-post-body-paragraph kv kw iq kx b ky nf jr la lb ng ju ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi lr translated"><span class="l ls lt lu bm lv lw lx ly lz di"> D </span>尽管取得了成功，但我们之前的方法有一点让我感到困扰:代理总是选择具有最高<strong class="kx ir">值的动作。因此，每当一个状态-动作对<strong class="kx ir">开始具有非零值时，代理将总是选择它</strong>。其他的动作将永远不会被执行，这意味着我们永远不会更新它们的值…但是如果这些动作中的一个比代理总是执行的那个更好呢？难道我们不应该鼓励代理人时不时地尝试新的东西，看看是否可以改进吗？</strong></p><p id="092b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">换句话说，我们希望允许我们的代理:</p><ul class=""><li id="efc4" class="nw nx iq kx b ky kz lb lc le ny li nz lm oa lq on oc od oe bi translated"><strong class="kx ir">采取值最高的动作</strong>(剥削)；</li><li id="d822" class="nw nx iq kx b ky of lb og le oh li oi lm oj lq on oc od oe bi translated"><strong class="kx ir">选择一个随机动作，尝试找到更好的动作</strong>(探索)。</li></ul><p id="6d27" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这两种行为之间的权衡很重要:如果代理只关注于<strong class="kx ir">开发</strong>，它不能尝试新的解决方案，因此<strong class="kx ir">不再学习</strong>。另一方面，如果代理只采取<strong class="kx ir">随机行动</strong>，那么<strong class="kx ir">培训是没有意义的</strong>，因为它不使用 Q 表。所以我们想<strong class="kx ir">随着时间的推移改变这个参数</strong>:在训练开始的时候，我们想<strong class="kx ir">尽可能的探索环境</strong>。但是探索变得越来越没意思，因为代理已经知道每一个可能的状态-动作对。该参数代表动作选择中的<strong class="kx ir">随机量。</strong></p><p id="37e8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种技术通常被称为<strong class="kx ir">ε贪婪算法</strong>，其中ε是我们的参数。这是一个<strong class="kx ir">简单但极其有效的</strong>方法来找到一个好的折衷方案。每次代理必须采取行动时，它有一个<strong class="kx ir">概率ε选择随机的一个</strong>，和一个<strong class="kx ir">概率 1-ε选择具有最高值的一个</strong>。我们可以在每集结束时将ε<strong class="kx ir">的值减少一个固定的量(<strong class="kx ir">线性衰减</strong>，或者基于ε的当前值(<strong class="kx ir">指数衰减</strong>)。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/a09f523c5ee345c9973e094ee4ba4486.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*6hbfNr415PCrFS_DJJaikw.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="eccd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们实现一个线性衰减 T2。在此之前，我想看看任意参数下的曲线。我们将从ε = 1 开始进入完全探索模式，并在每集之后将该值减少 0.001。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/c62beff2a25ce3514cb66c979c3103b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bio8GnpIz1PDzR-mxZzB3Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="4691" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">好了，现在我们对它有了很好的理解，我们可以真正实现它，看看<strong class="kx ir">它如何改变代理的行为</strong>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="me mf l"/></div></figure><pre class="kg kh ki kj gt nk md nl nm aw nn bi"><span id="673f" class="no mo iq md b gy np nq l nr ns">Q-table before training:<br/>[[0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]]<br/><br/>===========================================<br/>Q-table after training:<br/>[[0.531441   0.59049    0.59049    0.531441  ]<br/> [0.531441   0.         0.6561     0.56396466]<br/> [0.58333574 0.729      0.56935151 0.65055117]<br/> [0.65308668 0.         0.33420534 0.25491326]<br/> [0.59049    0.6561     0.         0.531441  ]<br/> [0.         0.         0.         0.        ]<br/> [0.         0.81       0.         0.65519631]<br/> [0.         0.         0.         0.        ]<br/> [0.6561     0.         0.729      0.59049   ]<br/> [0.6561     0.81       0.81       0.        ]<br/> [0.72899868 0.9        0.         0.72711067]<br/> [0.         0.         0.         0.        ]<br/> [0.         0.         0.         0.        ]<br/> [0.         0.81       0.9        0.729     ]<br/> [0.81       0.9        1.         0.81      ]<br/> [0.         0.         0.         0.        ]]</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/86b9edffc61a6b13784ff49fdd1f18eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eas5yW6SZnherik03rMoUw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="6b17" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">嘿，<strong class="kx ir">代理现在需要更多时间来持续赢得游戏</strong>！Q 表中的非零值<strong class="kx ir">比前一个多得多，这意味着代理已经学习了<strong class="kx ir">几个动作序列</strong>来达到目标。这是可以理解的，因为这个新的代理<strong class="kx ir">被迫探索状态-动作对，而不是总是利用具有非零值的状态-动作对</strong>。</strong></p><p id="e7c7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们看看<strong class="kx ir">是否和上一个</strong>一样成功赢得比赛。在评估模式中，我们<strong class="kx ir">不想再探索了</strong>，因为代理现在已经训练好了。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="me mf l"/></div></figure><pre class="kg kh ki kj gt nk md nl nm aw nn bi"><span id="8ce0" class="no mo iq md b gy np nq l nr ns">Success rate = 100.0%</span></pre><p id="59fe" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">唷，又是一个<strong class="kx ir"> 100%的成功率</strong>！我们没有贬低这个模型。😌这种方法的好处在这个例子中可能并不明显，但是我们的模型变得不那么静态了，而 T21 变得更加灵活了。它学习了从<strong class="kx ir"> S </strong>到<strong class="kx ir"> G </strong>的不同路径(动作序列),而不是像以前的方法那样只有一条路径。更多的探索<strong class="kx ir">会降低性能</strong>但是有必要训练能够<strong class="kx ir">适应新环境的代理</strong>。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="9b11" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">❄️四世。挑战:湿滑的冰湖</h1><p id="a5b1" class="pw-post-body-paragraph kv kw iq kx b ky nf jr la lb ng ju ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi lr translated"><span class="l ls lt lu bm lv lw lx ly lz di">我们</span>没有解决<strong class="kx ir">整个❄️Frozen 湖环境</strong>:我们只是在防滑版本上训练了一个代理，在初始化的时候使用<code class="fe ma mb mc md b">is_slippery = False</code>。在易变型中，代理采取的行动只有<strong class="kx ir"> 33%的机会成功</strong>。如果失败，则随机采取其他三个动作中的一个。这个特点给训练增加了很多随机性，给我们的代理增加了难度。让我们看看我们的代码在这个新环境中表现如何...</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="me mf l"/></div></figure><pre class="kg kh ki kj gt nk md nl nm aw nn bi"><span id="0006" class="no mo iq md b gy np nq l nr ns">Q-table before training:<br/>[[0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]<br/> [0. 0. 0. 0.]]<br/><br/>===========================================<br/>Q-table after training:<br/>[[0.06208723 0.02559574 0.02022059 0.01985828]<br/> [0.01397208 0.01425862 0.01305446 0.03333396]<br/> [0.01318348 0.01294602 0.01356014 0.01461235]<br/> [0.01117016 0.00752795 0.00870601 0.01278227]<br/> [0.08696239 0.01894036 0.01542694 0.02307306]<br/> [0.         0.         0.         0.        ]<br/> [0.09027682 0.00490451 0.00793372 0.00448314]<br/> [0.         0.         0.         0.        ]<br/> [0.03488138 0.03987256 0.05172554 0.10780482]<br/> [0.12444437 0.12321815 0.06462294 0.07084008]<br/> [0.13216145 0.09460133 0.09949734 0.08022573]<br/> [0.         0.         0.         0.        ]<br/> [0.         0.         0.         0.        ]<br/> [0.1606242  0.18174032 0.16636549 0.11444442]<br/> [0.4216631  0.42345944 0.40825367 0.74082329]<br/> [0.         0.         0.         0.        ]]</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/6f4d61939b54b9934b18d0f9a911fb3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6FZiH7nxlhWXOZpw-kCVIg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><pre class="kg kh ki kj gt nk md nl nm aw nn bi"><span id="4ad3" class="no mo iq md b gy np nq l nr ns">Success rate = 17.0%</span></pre><p id="bab7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">不太好。但是你能通过调整我们谈到的不同参数来提高性能吗？我鼓励你接受这个<strong class="kx ir">小挑战</strong>，自己动手<strong class="kx ir">享受强化学习的乐趣</strong>，并检查你是否理解了<strong class="kx ir">我们所说的关于 Q-learning 的一切</strong>。为什么不为ε贪婪算法实现指数衰减呢？在这个快速练习中，您可能会意识到<strong class="kx ir">稍微修改超参数会完全破坏结果</strong>。这是强化学习的另一个怪癖:超参数是相当情绪化的，如果你想调整它们，理解它们的含义是很重要的。测试和尝试新的组合来建立你的直觉并变得更有效率总是好的。祝你好运，玩得开心！</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="f2bd" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">🔚五.结论</h1><p id="d159" class="pw-post-body-paragraph kv kw iq kx b ky nf jr la lb ng ju ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">Q-learning 是一个简单而强大的算法，是强化学习的核心。在这篇文章中，</p><ul class=""><li id="6d68" class="nw nx iq kx b ky kz lb lc le ny li nz lm oa lq on oc od oe bi translated">我们学会了<strong class="kx ir">与</strong> <code class="fe ma mb mc md b"><strong class="kx ir">gym</strong></code> <strong class="kx ir">环境</strong>交互来选择动作和移动我们的代理；</li><li id="6236" class="nw nx iq kx b ky of lb og le oh li oi lm oj lq on oc od oe bi translated">我们引入了一个<strong class="kx ir"> Q 表</strong>的概念，其中<strong class="kx ir">行是状态</strong> , <strong class="kx ir">列是动作</strong>,<strong class="kx ir">单元格是给定状态下动作的值</strong>；</li><li id="c012" class="nw nx iq kx b ky of lb og le oh li oi lm oj lq on oc od oe bi translated">我们实验性地重新创建了<strong class="kx ir"> Q 学习更新公式</strong>来解决<strong class="kx ir">稀疏回报问题</strong>；</li><li id="d62a" class="nw nx iq kx b ky of lb og le oh li oi lm oj lq on oc od oe bi translated">我们实施了一整套培训和评估流程，以 100%的成功率解决了<strong class="kx ir"> ❄️Frozen 湖</strong>的环境问题；</li><li id="9b08" class="nw nx iq kx b ky of lb og le oh li oi lm oj lq on oc od oe bi translated">我们实现了著名的<strong class="kx ir">ε-贪婪算法</strong>，以便在未知状态-动作对的<strong class="kx ir">探索和最成功状态-动作对</strong>的<strong class="kx ir">利用之间建立一个折衷。</strong></li></ul><p id="1437" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> ❄️Frozen 湖</strong>是一个非常简单的环境，但是其他人可以有<strong class="kx ir">如此多的状态和动作，以至于不可能在内存</strong>中存储 q 表。在事件<strong class="kx ir">不是离散的，而是连续的</strong>的环境中尤其如此(像《超级马里奥兄弟》或《《我的世界》》)。当问题出现时，一种流行的技术包括训练一个<strong class="kx ir">深度神经网络来逼近 Q 表</strong>。这种方法增加了几层复杂性，因为神经网络<strong class="kx ir">不是很稳定</strong>。但是我将在另一个教程中用不同的技术来稳定它们。</p><p id="3b2b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在此之前，<strong class="kx ir">分享这篇文章</strong>如果对你有帮助的话<a class="ae nv" href="https://twitter.com/maximelabonne" rel="noopener ugc nofollow" target="_blank">和<strong class="kx ir">在 Twitter 上关注我</strong>T5 和</a><a class="ae nv" href="https://medium.com/@mlabonne" rel="noopener">T7】中 T9】了解更多关于机器学习和深度学习的<strong class="kx ir">实用内容</strong>。📣</a></p></div></div>    
</body>
</html>