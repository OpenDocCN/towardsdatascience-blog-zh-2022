<html>
<head>
<title>XGBoost: its Genealogy, its Architectural Features, and its Innovation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XGBoost:它的谱系、建筑特色和创新</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/xgboost-its-genealogy-its-architectural-features-and-its-innovation-bf32b15b45d2#2022-10-20">https://towardsdatascience.com/xgboost-its-genealogy-its-architectural-features-and-its-innovation-bf32b15b45d2#2022-10-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1bf3" class="pw-subtitle-paragraph jo ip iq bd b jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dk translated">顺序和并行架构合二为一</h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/c98ee200cd360a2df6e7dfa03db88e10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Wn0SFKipak7Vm63f"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">照片由<a class="ae kw" href="https://unsplash.com/@mariolagr?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">玛丽奥拉·格罗贝尔斯卡</a>在<a class="ae kw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="d1b0" class="kx ky iq bd kz la lb lc ld le lf lg lh jx li jy lj ka lk kb ll kd lm ke ln lo bi translated">介绍</h1><p id="88e7" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated"><strong class="lr ir"> <em class="ml"> XGBoost </em> </strong>(极限梯度提升)是一种强大的学习算法，在过去的许多比赛中胜过了许多传统的机器学习算法。</p><p id="0804" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">简而言之，XGBoost <strong class="lr ir"> <em class="ml"> </em> </strong>集顺序和并行架构于一身:虽然它是一种顺序学习算法(<a class="ae kw" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html#additive-training" rel="noopener ugc nofollow" target="_blank">加法策略</a>)，但它将并行计算融入其架构中，以提高系统效率。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mr"><img src="../Images/ca076e986a8c600848eb78761f747423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tSaJ_yv8yDt4XU0ZJEaMeg.jpeg"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">制作人:杉尾道夫</p></figure><p id="2c3b" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">这篇文章是针对初学者的XGBoost的介绍性概述，是一篇一站式的文章，它将向您提供关于XGBoost的整体情况(如果不是细节的话)——它的谱系、架构特性和创新特性。在文章的最后，我还会给出一个简短的补充资源列表，这样读者就可以更详细地了解文章中涉及的主题。</p><p id="5386" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">现在，我们开始吧。</p><p id="66fd" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">为了理解XGBoost的特性，我们可以从快速概述它的系谱开始。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="982d" class="kx ky iq bd kz la mz lc ld le na lg lh jx nb jy lj ka nc kb ll kd nd ke ln lo bi translated">讨论</h1><h1 id="cdeb" class="kx ky iq bd kz la lb lc ld le lf lg lh jx li jy lj ka lk kb ll kd lm ke ln lo bi translated">A.XGBoost系谱</h1><p id="dbcf" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">从自上而下的角度来看，XGBoost是<strong class="lr ir"> <em class="ml">监督机器学习</em> </strong>的子类。而且顾名思义，XGBoost是<strong class="lr ir"> <em class="ml"> Boosting机</em> </strong>的高级变种，是<strong class="lr ir"> <em class="ml">基于树的系综</em> </strong>算法的子类，像随机森林。</p><p id="5405" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">然而，Boosting Machine在操作学习过程的方式上与Random Forest有着根本的不同。</p><h2 id="5407" class="ne ky iq bd kz nf ng dn ld nh ni dp lh ly nj nk lj mc nl nm ll mg nn no ln np bi translated"><strong class="ak"> <em class="jn">助推机</em> </strong></h2><p id="ffd9" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">随机森林并行运行多个独立的决策树，并通过平均所有结果来组合它们的结果。这种方法使用随机引导抽样，通常被称为bagging。从这个意义上说，随机森林是一种并行学习算法。</p><p id="6750" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">相反，Boosting Machine使用<a class="ae kw" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html#additive-training" rel="noopener ugc nofollow" target="_blank">一个加法策略</a>:即“<em class="ml">一次增加一棵新树</em>”(<a class="ae kw" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html#additive-training" rel="noopener ugc nofollow" target="_blank">xgboost developers，2022 </a>)。Boosting机器依次运行名为<strong class="lr ir"> <em class="ml">的单个弱/简单决策树，基学习器</em> </strong>。简单来说，概念上的Boosting Machine是建立在顺序学习架构上的。</p><p id="5e1b" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">从这个意义上说，Boosting Machine是顺序学习的，而Random Forest是并行学习的。</p><p id="d4f3" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">作为增压机的参考，这里有一个麻省理工学院关于增压的讲座:<a class="ae kw" href="https://www.youtube.com/watch?v=UHBmv7qCey4" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=UHBmv7qCey4</a></p><p id="2604" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">也就是说，为了避免混淆，我应该从<strong class="lr ir"> <em class="ml">系统优化</em> </strong>的角度在这里做一个脚注。XGBoost还设计用于运行并行计算，以提高计算资源的使用效率(<a class="ae kw" href="https://xgboost.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> xgboost developers，n.d. </a>)。总的来说，XGBoost虽然继承了Boosting Machine的顺序学习架构，但却通过并行计算来优化系统。</p><h2 id="df20" class="ne ky iq bd kz nf ng dn ld nh ni dp lh ly nj nk lj mc nl nm ll mg nn no ln np bi translated"><strong class="ak"> <em class="jn">渐变助推机</em> </strong></h2><p id="6e2c" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">顾名思义，XGBoost(极限梯度推进)是梯度推进机(GBM)的高级变种，是推进机家族成员。</p><p id="dad7" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">作为其<strong class="lr ir"> <em class="ml">加法策略</em>的一部分，</strong>梯度推进机(GBM)使用梯度下降进行优化。为了减少计算量，GBM利用泰勒展开式的一阶项逼近目标函数，并忽略高阶项进行学习优化。换句话说，它使用目标函数(损失函数)的一阶导数(<strong class="lr ir"> <em class="ml">梯度</em> </strong>)来确定下一个弱学习器预测器。通过这种方式，梯度提升在保留现有弱预测器的同时，在它们之上添加新的预测器以减少当前误差，从而逐步提高性能。(弗里德曼，2000年)</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="8a8f" class="kx ky iq bd kz la mz lc ld le na lg lh jx nb jy lj ka nc kb ll kd nd ke ln lo bi translated"><strong class="ak">b .</strong><strong class="ak">XGBoost</strong>的算法特点</h1><h2 id="720a" class="ne ky iq bd kz nf ng dn ld nh ni dp lh ly nj nk lj mc nl nm ll mg nn no ln np bi translated"><strong class="ak"> <em class="jn">牛顿助推机</em> </strong></h2><p id="3ee6" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">XGBoost扩展了梯度增强的思想，除了它的一阶导数(梯度)之外，它还使用目标函数的二阶导数(<strong class="lr ir"><em class="ml">Hessian:Curvature</em></strong>)来进一步优化它的学习过程。这个方法叫做<strong class="lr ir"> <em class="ml">牛顿拉夫逊法</em> </strong>。而采用牛顿拉夫逊法的增压机叫做<a class="ae kw" href="https://arxiv.org/abs/1808.03064" rel="noopener ugc nofollow" target="_blank"> <strong class="lr ir"> <em class="ml">牛顿增压</em> </strong> </a>。关于梯度下降和牛顿提升之间的差异的进一步讨论，您可以阅读由<a class="ae kw" href="https://arxiv.org/search/stat?searchtype=author&amp;query=Sigrist%2C+F" rel="noopener ugc nofollow" target="_blank"> Fabio Sigrist </a>撰写的论文<a class="ae kw" href="https://arxiv.org/abs/1808.03064" rel="noopener ugc nofollow" target="_blank">Gradient and Newton Boosting for class ification and Regression</a>。</p><p id="db34" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">由于加法策略的特定结构，二阶近似产生多个有益的数学特性，以简化算法，进一步提高计算效率。(Guestrin和陈，2016年)</p><h2 id="8e45" class="ne ky iq bd kz nf ng dn ld nh ni dp lh ly nj nk lj mc nl nm ll mg nn no ln np bi translated"><strong class="ak"> <em class="jn">规则化:解决方差-偏差权衡</em> </strong></h2><p id="541c" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated"><a class="ae kw" href="https://jerryfriedman.su.domains/" rel="noopener ugc nofollow" target="_blank"> Jerome Friedman </a>，梯度推进机的设计者(<a class="ae kw" href="https://www.jstor.org/stable/2699986" rel="noopener ugc nofollow" target="_blank"> Friedman，2000 </a>)阐述了正则化对于解决<strong class="lr ir"> <em class="ml">偏差-方差权衡</em> </strong>、欠拟合-过拟合权衡问题的重要性，特别推荐用户调整梯度推进机的三个元参数:迭代次数、学习速率和终端节点/叶子数。(<a class="ae kw" href="https://www.researchgate.net/publication/2424824_Greedy_Function_Approximation_A_Gradient_Boosting_Machine" rel="noopener ugc nofollow" target="_blank">弗里德曼，2000年，第1203、1214–1215页</a>)</p><p id="c31c" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">在这种背景下，XGBoost继承了梯度推进机的正则化重点，并对其进行了进一步扩展。</p><ul class=""><li id="56bc" class="nq nr iq lr b ls mm lv mn ly ns mc nt mg nu mk nv nw nx ny bi translated">首先，XGBoost使用户能够调整各种超参数来约束树:例如，树的数量、单个树的深度、分区的实例权重的最小和、提升轮的最大数量以及节点/叶的数量。</li><li id="d69c" class="nq nr iq lr b ls nz lv oa ly ob mc oc mg od mk nv nw nx ny bi translated">第二，它允许用户在学习过程中应用学习速率，收缩率。(<a class="ae kw" href="https://doi.org/10.48550/arXiv.1603.02754" rel="noopener ugc nofollow" target="_blank"> Guestrin &amp;陈2016第3页</a>)</li><li id="fffc" class="nq nr iq lr b ls nz lv oa ly ob mc oc mg od mk nv nw nx ny bi translated">第三，它使用户能够使用随机采样技术，如列子采样。(<a class="ae kw" href="https://doi.org/10.48550/arXiv.1603.02754" rel="noopener ugc nofollow" target="_blank"> Guestrin &amp;陈2016第3页</a></li><li id="d0ad" class="nq nr iq lr b ls nz lv oa ly ob mc oc mg od mk nv nw nx ny bi translated">第四，它使用户能够调整L1和L2正则项。</li></ul></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="cbe2" class="kx ky iq bd kz la mz lc ld le na lg lh jx nb jy lj ka nc kb ll kd nd ke ln lo bi translated">C.创新:</h1><h2 id="f260" class="ne ky iq bd kz nf ng dn ld nh ni dp lh ly nj nk lj mc nl nm ll mg nn no ln np bi translated">稀疏感知算法和加权分位数草图</h2><p id="a543" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">更重要的是，XGBoost引入了两项创新:稀疏感知算法和加权分位数草图。(<a class="ae kw" href="https://doi.org/10.48550/arXiv.1603.02754" rel="noopener ugc nofollow" target="_blank">陈&amp;guest rin 2016 p10</a></p><p id="f6eb" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">首先，XGBoost有一个内置的特性叫做<strong class="lr ir"> <em class="ml">默认方向。</em> </strong>该功能捕获稀疏数据结构的模式，并根据该模式确定每个节点的分割方向。Guestrin &amp; Chen 提出了稀疏性的三个典型原因:</p><blockquote class="oe of og"><p id="61c6" class="lp lq ml lr b ls mm js lu lv mn jv lx oh mo ma mb oi mp me mf oj mq mi mj mk ij bi translated">“1)数据中存在缺失值；2)统计中经常出现零条目；以及3)特征工程的人工制品，例如一键编码(<a class="ae kw" href="https://arxiv.org/abs/1603.02754" rel="noopener ugc nofollow" target="_blank"> Guestrin &amp;陈2016 </a></p></blockquote><p id="e6f3" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">原则上，这个特性使得XGBoost <strong class="lr ir"> <em class="ml">稀疏感知算法</em> </strong>能够处理缺失数据:用户不需要估算缺失数据。</p><p id="2a5f" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">默认方向决定了分割的方向，<strong class="lr ir"> <em class="ml">加权分位数草图</em> </strong>提出候选分割点。下面节选自<a class="ae kw" href="https://arxiv.org/abs/1603.02754" rel="noopener ugc nofollow" target="_blank">陈和Guestrin的论文</a>总结了它是什么。</p><blockquote class="oe of og"><p id="8647" class="lp lq ml lr b ls mm js lu lv mn jv lx oh mo ma mb oi mp me mf oj mq mi mj mk ij bi translated">“一种新的分布式加权分位数草图算法……能够以可证明的理论保证处理加权数据。总体思路是提出一种支持合并和剪枝操作的数据结构，每个操作都被证明能保持一定的精度水平。”(<a class="ae kw" href="https://arxiv.org/abs/1603.02754" rel="noopener ugc nofollow" target="_blank"> Guestrin &amp;陈2016 </a></p></blockquote><h2 id="2a9f" class="ne ky iq bd kz nf ng dn ld nh ni dp lh ly nj nk lj mc nl nm ll mg nn no ln np bi translated">系统优化:效率和可扩展性</h2><p id="a2de" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">到目前为止，我们从学习算法架构的角度看到了XGBoost的框架。现在，我们可以从系统优化的角度来看待。</p><p id="879e" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated"><a class="ae kw" href="https://xgboost.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">原生的XGBoost API </a>在追求计算效率，或者说系统优化方面也是创新的。该API被称为eXtreme (X ),因为XGBoost旨在通过在给定的计算资源(处理器(CPU、GPU)、内存和核外(磁盘空间):缓存访问、块数据压缩和分片)之间有效地分配计算任务，使用户能够利用给定系统计算能力的极限。(<a class="ae kw" href="https://github.com/databricks/xgboost-linux64/blob/master/doc/model.md" rel="noopener ugc nofollow" target="_blank"> databricks，2017 </a></p><p id="dc03" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">关于原生XGBoost API的更多创新方面，这里有一个由XGBoost的发明者(Chen &amp; Guestrin)，<em class="ml"> XGBoost:一个可扩展的树提升系统</em> 概述的伟大作品。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="978d" class="kx ky iq bd kz la mz lc ld le na lg lh jx nb jy lj ka nc kb ll kd nd ke ln lo bi translated"><strong class="ak"> <em class="jn">结论</em> </strong></h1><p id="8468" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">这个对XGBoost的快速概述回顾了它的系谱、架构特性和创新，但没有深入细节。</p><p id="5399" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">简而言之，XGBoost具有顺序-并行混合体系结构，从某种意义上说，它继承了Boosting机器谱系中的顺序学习体系结构，同时将并行计算融入其体系结构中，以提高系统效率。</p><p id="5273" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">由于Boosting Machine有过度拟合的倾向，所以原生的XGBoost API非常注重解决偏差-方差权衡<strong class="lr ir"> <em class="ml"> </em> </strong>问题，并通过超参数调整方便用户应用各种正则化技术。</p><p id="3b75" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">如果您对原生XGBoost API的实现示例感兴趣，可以阅读我的另一篇文章，<a class="ae kw" href="https://medium.com/p/2f40a2e382fa/edit?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="lr ir">用原生XGBoost API </strong> </a> <strong class="lr ir">进行成对超参数调优。</strong></p><p id="1c75" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">感谢你阅读这篇文章。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h2 id="6704" class="ne ky iq bd kz nf ng dn ld nh ni dp lh ly nj nk lj mc nl nm ll mg nn no ln np bi translated">建议的外部资源</h2><p id="b7be" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">对于那些想探索XGBoost更多细节的人来说，这里有一个我最喜欢的关于该算法的参考资料的简短列表:</p><ul class=""><li id="eecd" class="nq nr iq lr b ls mm lv mn ly ns mc nt mg nu mk nv nw nx ny bi translated">为了快速概述XGBoost，<a class="ae kw" href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/" rel="noopener ugc nofollow" target="_blank"><em class="ml">Jason Brownlee对机器学习的梯度推进算法</em> g </a>的简要介绍非常简洁地捕捉到了XGBoost的谱系和主要特性:</li><li id="c29f" class="nq nr iq lr b ls nz lv oa ly ob mc oc mg od mk nv nw nx ny bi translated">对于XGBoost的数学解释，<a class="ae kw" href="https://datasciblog.github.io/2020/02/26/tree-boosting-03/" rel="noopener ugc nofollow" target="_blank"> <em class="ml"> TreeBoosting-03:为什么每次机器学习比赛XGBoost都会赢？</em> </a> by哈阮给了我一个很好的补充资源。</li><li id="3ddb" class="nq nr iq lr b ls nz lv oa ly ob mc oc mg od mk nv nw nx ny bi translated">XGBoost的创新方面，<a class="ae kw" href="https://arxiv.org/abs/1603.02754" rel="noopener ugc nofollow" target="_blank"><em class="ml">XGBoost:XGBoost的发明人陈&amp; Guestrin的一个可扩展的树提升系统</em> </a>给大家做一个简单的总结。</li><li id="9391" class="nq nr iq lr b ls nz lv oa ly ob mc oc mg od mk nv nw nx ny bi translated">官方原生XGBoost API的在线文档给你一个官方的<a class="ae kw" href="https://xgboost.readthedocs.io/en/latest/tutorials/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="ml"> XGBoost教程</em> </a>。它是创新算法的重要基础资源。</li><li id="38ef" class="nq nr iq lr b ls nz lv oa ly ob mc oc mg od mk nv nw nx ny bi translated">如果你有时间和灵魂去读一篇负荷很重的论文，你应该读一读杰罗姆·h·弗里德曼的论文《关于梯度推进机》，<a class="ae kw" href="https://jerryfriedman.su.domains/ftp/trebst.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ml">贪婪函数逼近:一台梯度推进机</em> </a>:</li></ul></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="a5cb" class="kx ky iq bd kz la mz lc ld le na lg lh jx nb jy lj ka nc kb ll kd nd ke ln lo bi translated">确认</h1><p id="abe6" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">我要感谢TDS的编辑团队，特别是<a class="ae kw" href="https://www.linkedin.com/in/ACoAAArEpWwBHvFKeyTA3JKk6gApzplqwM2hjKY" rel="noopener ugc nofollow" target="_blank">凯瑟琳·普雷里</a>，感谢他们在编辑阶段提供的宝贵意见。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="c614" class="kx ky iq bd kz la mz lc ld le na lg lh jx nb jy lj ka nc kb ll kd nd ke ln lo bi translated">参考</h1><ul class=""><li id="f6e7" class="nq nr iq lr b ls lt lv lw ly ok mc ol mg om mk nv nw nx ny bi translated">Brownlee，J. <a class="ae kw" href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/" rel="noopener ugc nofollow" target="_blank"> <em class="ml">对机器学习的梯度推进算法的温和介绍</em> </a> <em class="ml">。</em> (2016)。检索自机器学习掌握:<a class="ae kw" href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://Machine Learning Mastery . com/gentle-introduction-gradient-boosting-algorithm-Machine-Learning/</a></li><li id="0c41" class="nq nr iq lr b ls nz lv oa ly ob mc oc mg od mk nv nw nx ny bi translated">数据砖。<a class="ae kw" href="https://github.com/databricks/xgboost-linux64/blob/master/doc/model.md" rel="noopener ugc nofollow" target="_blank"><em class="ml">xgboost-Linux 64</em></a>T30】。 (2017)。从github检索:<a class="ae kw" href="https://github.com/databricks/xgboost-linux64/blob/master/doc/model.md" rel="noopener ugc nofollow" target="_blank">https://github . com/databricks/xgboost-Linux 64/blob/master/doc/model . MD</a></li><li id="16ea" class="nq nr iq lr b ls nz lv oa ly ob mc oc mg od mk nv nw nx ny bi translated">j . Friedman<em class="ml"/><a class="ae kw" href="https://www.researchgate.net/publication/2424824_Greedy_Function_Approximation_A_Gradient_Boosting_Machine" rel="noopener ugc nofollow" target="_blank"><em class="ml">贪婪函数逼近:一台梯度助推机</em> </a>。(2000).<em class="ml">统计年鉴，29 </em> (5)，1189–1232。doi:10.1214/aos/1013203451</li><li id="9b45" class="nq nr iq lr b ls nz lv oa ly ob mc oc mg od mk nv nw nx ny bi translated">Guestrin，c .，&amp; Chen，T. <em class="ml"> XGBoost: </em> <a class="ae kw" href="https://doi.org/10.48550/arXiv.1603.02754" rel="noopener ugc nofollow" target="_blank"> <em class="ml">一个可扩展的树提升系统</em> </a> <em class="ml">。</em> (2016)。https://doi.org/10.48550/arXiv.1603.02754</li><li id="3516" class="nq nr iq lr b ls nz lv oa ly ob mc oc mg od mk nv nw nx ny bi translated">Nguyen，H. <a class="ae kw" href="https://datasciblog.github.io/2020/02/26/tree-boosting-03" rel="noopener ugc nofollow" target="_blank"> <em class="ml"> TreeBoosting-03:为什么XGBoost每次机器学习比赛都赢？</em> </a>(未注明)。检索自数据科学博客:<a class="ae kw" href="https://datasciblog.github.io/2020/02/26/tree-boosting-03" rel="noopener ugc nofollow" target="_blank">https://datasciblog.github.io/2020/02/26/tree-boosting-03</a></li><li id="0ec6" class="nq nr iq lr b ls nz lv oa ly ob mc oc mg od mk nv nw nx ny bi translated">xgboost开发人员。<a class="ae kw" href="https://xgboost.readthedocs.io/: https://xgboost.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> <em class="ml"> XGBoost文档</em> </a> <em class="ml">。</em>(未注明)。检索自<a class="ae kw" href="https://xgboost.readthedocs.io/:" rel="noopener ugc nofollow" target="_blank">https://xgboost.readthedocs.io/:</a>T32】https://xgboost.readthedocs.io/en/stable/</li></ul></div></div>    
</body>
</html>