<html>
<head>
<title>What? When? How?: ExtraTrees Classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么？什么时候？怎么会？:树外分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-when-how-extratrees-classifier-c939f905851c#2022-08-06">https://towardsdatascience.com/what-when-how-extratrees-classifier-c939f905851c#2022-08-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ca06" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">什么是树外量词？什么时候用？如何实施？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/18faa17df5b3d4a38ea7763910e6e131.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BSu8BlnxWaBEpV_TyrkUHA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/s/photos/questioning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kv" href="https://unsplash.com/@euniveeerse?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Eunice lituaas</a>拍摄的照片</p></figure><p id="8a72" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在过去的十年中，基于树的模型越来越受欢迎，主要是因为它们的鲁棒性。基于树的模型可以用于任何类型的数据(分类/连续)，可以用于非正态分布的数据，并且几乎不需要任何数据转换(可以处理缺失值/比例问题等)。)</p><p id="ed63" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然决策树和随机森林通常是基于树的模型，但不太为人所知的是ExtraTrees。(如果你不熟悉基于树的模型，一定要看看下面的<a class="ae kv" rel="noopener" target="_blank" href="/understanding-random-forest-58381e0602d2">帖子</a>)。</p><h1 id="6def" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">什么是树外模型？</h1><p id="708d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">与随机森林类似，ExtraTrees是一种集合ML方法，它训练大量决策树并聚集来自决策树组的结果以输出预测。然而，额外的树和随机森林之间几乎没有区别。</p><p id="33bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随机森林使用<a class="ae kv" rel="noopener" target="_blank" href="/understanding-random-forest-58381e0602d2">打包</a>来选择训练数据的不同变化，以确保决策树足够不同。但是，Extra Trees使用整个数据集来训练决策树。因此，为了确保各个决策树之间有足够的差异，它会随机选择分割特征和创建子节点的值。相比之下，在随机森林中，我们使用算法进行贪婪搜索，并选择分割要素的值。除了这两个区别，随机森林和额外的树基本上是相同的。那么这些变化有什么影响呢？</p><ul class=""><li id="0d4b" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">使用整个数据集(这是默认设置，可以更改)允许提取树减少模型的偏差。但是，分割时特征值的随机化会增加偏差和方差。介绍额外树模型的<a class="ae kv" href="https://orbi.uliege.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf" rel="noopener ugc nofollow" target="_blank">论文对不同基于树的模型进行了偏差-方差分析。<strong class="ky ir">从论文中我们看到，在大多数分类和回归任务(分析了六个)中，抽提树比随机森林有更高的偏差和更低的方差。</strong>然而，论文继续说这是因为额外树中的随机化将无关的特征包括到模型中。这样，当不相关特征被排除时，比方说通过特征选择预建模步骤，额外的树得到类似于随机森林的偏差分数。</a></li><li id="82d6" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated"><strong class="ky ir">就计算成本而言，额外的树比随机森林要快得多。</strong>这是因为Extra Trees随机选择分割要素的值，而不是随机森林中使用的贪婪算法。</li></ul><h1 id="da89" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">什么时候应该使用提取物？</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/45fc505fd01360e44e995f2ae15eb80a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9nDKZA8HWaDhxx8qEYpk7w.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@madebyjens?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Jens Lelie </a>在<a class="ae kv" href="https://unsplash.com/s/photos/decision?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="cf7c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随机森林仍然是基于集成树的模型(最近有来自<a class="ae kv" href="https://xgboost.readthedocs.io/en/stable/tutorials/model.html" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>模型的竞争)。然而，从我们之前对随机森林和额外树之间的差异的讨论中，我们看到了Extra Trees的价值，尤其是当计算成本是一个问题时。<strong class="ky ir">具体来说，在构建具有大量特征工程/特征选择预建模步骤的模型时，计算成本是一个问题，相比其他基于系综树的模型，ExtraTrees是一个不错的选择。</strong></p><h1 id="6f70" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">如何建立一个ExtraTrees模型？</h1><p id="ebd7" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">ExtraTrees可用于构建分类模型或回归模型，可通过Scikit-learn获得。在本教程中，我们将介绍分类模型，但是代码可以用于稍加调整的回归(例如，从ExtraTreesClassifier切换到ExtraTreesRegressor)</p><p id="e8c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">建立模型</strong></p><p id="3109" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用Scikit-learn中的make_classification来创建虚拟分类数据集。为了评估该模型，我们将使用10重交叉验证，以准确性作为评估标准。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="d9c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">超参数调谐</strong></p><p id="ec5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">额外树模型的详细参数列表可在<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html" rel="noopener ugc nofollow" target="_blank"> Scikit-learn页面</a>上找到。<a class="ae kv" href="https://orbi.uliege.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf" rel="noopener ugc nofollow" target="_blank"> Extra Trees研究论文</a>明确提出了三个关键参数，陈述如下。</p><p id="a6ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ng">“参数K、nmin和M具有不同的效果:K确定属性选择过程的强度，nmin确定平均输出噪声的强度，M确定集合模型聚合的方差减少的强度。”</em></p><p id="e666" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们从实现的角度更仔细地看看这些参数。</p><ul class=""><li id="f496" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated"><strong class="ky ir"> K </strong>是Scikit-learn文档中的max_feature，指每个决策节点要考虑的特性数量。K值越高，每个决策节点考虑的特征越多，因此模型的偏差越低。然而，过高的K值降低了随机化，否定了系综的效果。</li><li id="f8e0" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated"><strong class="ky ir"> nmin </strong>映射到min_sample_leaf，并且是在叶节点所需的最小样本数。其值越高，模型越不可能过度拟合。样本数量越少，分裂越多，树越深，越专门化。</li><li id="998b" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated"><strong class="ky ir"> M </strong>映射到n_estimators，是森林中树的数量。其值越高，模型的方差越低。</li></ul><p id="13c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如下所示，可通过<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank"> GridSearchCV </a>选择最佳参数集。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ne nf l"/></div></figure><h1 id="6de8" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">最终外卖</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/0404d12dfa4fe695274b9ce9ab0f45b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qidEh7dbb5U1S2i29Wt0ag.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@pavement_special?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Riccardo Annandale </a>在<a class="ae kv" href="https://unsplash.com/s/photos/result?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><ul class=""><li id="1160" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">ExtraTrees分类器是一种基于系综树的机器学习方法，它使用依赖随机化来减少方差和计算成本(与随机森林相比)。</li><li id="ec14" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">树外分类器可用于分类或回归，在这种情况下，计算成本是一个问题，特征已仔细选择和分析。</li><li id="7763" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">额外的树可以从Scikit-learn实现。对于调优很重要的三个超参数是max_feature、min_samples_leaf和n_estimators。</li></ul><p id="e625" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样！树外之物，何时，如何！</p></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><h1 id="677f" class="ls lt iq bd lu lv np lx ly lz nq mb mc jw nr jx me jz ns ka mg kc nt kd mi mj bi translated">参考</h1><div class="nu nv gp gr nw nx"><a href="https://www.summitllc.us/blog/advantages-of-tree-based-modeling#:~:text=Are%20easy%20to%20represent%20visually,because%20variable%20transformations%20are%20unnecessary" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd ir gy z fp oc fr fs od fu fw ip bi translated">基于树的建模的优势</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">基于树的建模是线性回归分析的一个很好的替代方法。但是是什么让它如此有利…</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">www.summitllc.us</p></div></div></div></a></div><div class="nu nv gp gr nw nx"><a href="https://machinelearningmastery.com/extra-trees-ensemble-with-python/" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd ir gy z fp oc fr fs od fu fw ip bi translated">如何用Python开发额外的树集合——机器学习掌握</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">Extra Trees是一种集成机器学习算法，它结合了来自许多决策树的预测。这是…</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">machinelearningmastery.com</p></div></div><div class="og l"><div class="oh l oi oj ok og ol kp nx"/></div></div></a></div><div class="nu nv gp gr nw nx"><a href="https://stats.stackexchange.com/questions/175523/difference-between-random-forest-and-extremely-randomized-trees" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd ir gy z fp oc fr fs od fu fw ip bi translated">随机森林和极度随机树的区别</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">begin group $ extractreesclassifier类似于RandomForest的兄弟，但有两个重要的区别。我们正在建立…</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">stats.stackexchange.com</p></div></div><div class="og l"><div class="om l oi oj ok og ol kp nx"/></div></div></a></div></div></div>    
</body>
</html>