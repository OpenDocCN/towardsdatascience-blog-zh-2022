<html>
<head>
<title>Why do we minimize the mean squared error?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么我们要最小化均方差？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-do-we-minimize-the-mean-squared-error-3b97391f54c#2022-05-27">https://towardsdatascience.com/why-do-we-minimize-the-mean-squared-error-3b97391f54c#2022-05-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cba5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">你有没有想过为什么我们要最小化平方误差？在这篇文章中，我将展示最著名的损失函数背后的数学原因。</h2></div></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/69161d13236f40221ce4bcdebb1840e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9t8eqhmwhEeQ3bmh"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">彼得·塞坎在<a class="ae lc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。最小化平方误差就像试图到达谷底。</p></figure></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><p id="4163" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">学习机器学习时首先遇到的话题之一是线性回归。它通常被认为是回归的最简单算法之一。就我而言，当我学习线性回归时——早在我攻读物理学学位的时候——有人告诉我，线性回归试图找到最小化数据平方距离之和的直线。数学上这是<em class="lz"> y_p=ax+b </em>，其中<em class="lz"> x </em>是将用于预测<em class="lz"> y_t </em>的独立变量，<em class="lz"> y_p </em>是相应的预测，<em class="lz"> a </em>和<em class="lz"> b </em>是斜率和截距。为了简单起见，让我们假设x∈R和y∈R分别为T16和T17和T18。我们想要最小化的量——也就是损失函数——是</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/a66232801b4ab8f2aecab52026d824f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*YFYeu6Ej46t_RTh2r_cVfQ.png"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated">MSE损失函数</p></figure><p id="051e" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这种损失背后的直觉是，我们希望惩罚更多的大错误，而不是小错误，这就是为什么我们要平方错误项。对于损失函数的这种特殊选择，10个1单位的误差比1个10单位的误差更好，在第一种情况下，我们将损失增加10个平方单位，而在第二种情况下，我们将损失增加100个平方单位。</p><p id="cfca" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">然而，虽然这很直观，但似乎也很武断。为什么使用平方函数，而不使用指数函数或任何其他具有类似属性的函数？答案是这个损失函数的选择没有那么随意，可以从更根本的原理推导出来。我给你介绍一下<em class="lz">最大似然估计</em>！</p><h1 id="7946" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">最大似然估计</h1><p id="9e05" class="pw-post-body-paragraph ld le iq lf b lg mt jr li lj mu ju ll lm mv lo lp lq mw ls lt lu mx lw lx ly ij bi translated">在这一节中，我将介绍最大似然估计，这是我在机器学习中最喜欢的技术之一，我将展示我们如何将这一技术用于统计学习。</p><h1 id="675d" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">基础</h1><p id="1e38" class="pw-post-body-paragraph ld le iq lf b lg mt jr li lj mu ju ll lm mv lo lp lq mw ls lt lu mx lw lx ly ij bi translated">首先是一些理论。考虑一个数据集<strong class="lf ir"> <em class="lz"> X </em> </strong> <em class="lz"> ={x1，…，xn} </em>从分布<em class="lz"> p_real(x) </em>独立抽取的<em class="lz"> n </em>个数据点。我们还有分布<em class="lz"> p_model(θ，x) </em>，它由参数<em class="lz"> θ </em>索引。这意味着对于每个<em class="lz"> θ </em>，我们有不同的分布。例如，可以有<em class="lz"> p_model(θ，x)=θ* exp()-θ* x)</em>，也就是指数分布。</p><p id="f1a6" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们要解决的问题是找到使X被<em class="lz"> p_model(θ*，x) </em>生成的概率最大的<em class="lz"> θ* </em>。对于所有可能的<em class="lz"> p_model </em>分布，这是最有可能生成<em class="lz"> X </em>的分布。这可以形式化为</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/c5d0bec83d51ae667ee43fa6dc06a930.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*Ix2rCSTS_FnYyKxq3ZQjhA.png"/></div></figure><p id="1145" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">由于来自<strong class="lf ir"> <em class="lz"> X </em> </strong>的观测值是独立提取的，我们可以将等式改写为</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/a3308dc47a5858e847099d050d004260.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*xLrn7LiBF_MY2wat7ow7ig.png"/></div></figure><p id="95ea" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">虽然从数学的角度来看，这个方程完全没问题，但它存在一些数值问题。特别是，我们在乘以概率密度，密度有时可能非常小，所以总乘积可能会有下溢问题——即:我们无法用CPU的精度来表示值。好消息是，这个问题可以通过一个简单的技巧来解决:只需将<em class="lz"> log </em>应用于乘积，并将乘积转换为总和。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/536c15b197b837cb471562bbe246c6d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*bRTCO2jB5xU9orLvYSaFEA.png"/></div></figure><p id="94fd" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">由于对数是一个单调递增的函数，这个技巧不会改变<em class="lz"> argmax </em>。</p><h1 id="88b7" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">例子</h1><p id="4dca" class="pw-post-body-paragraph ld le iq lf b lg mt jr li lj mu ju ll lm mv lo lp lq mw ls lt lu mx lw lx ly ij bi translated">让我们设计一个例子，看看如何在实际问题中使用这种技术。假设您有两个遵循高斯分布的数据点，或者至少这是您所怀疑的，并且您想要找到这些数据点的最可能的中心。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/28666bfb5f286cede7c71be654d8856f.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/0*k2RlKi6ciFx2XRtF.jpeg"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated">点由两个分别以(3，3)和(-3，-3)为中心的高斯分布生成。</p></figure><p id="275d" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们的假设是这些分布遵循一个单位协方差的高斯分布，即:<strong class="lf ir"><em class="lz">【σ</em></strong><em class="lz">=[[1，0]，[0，1]]。</em>因此，我们要最大化</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/64251c24e81dbfe7b17c3c4870294217.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*-gjWh-eQdHbX_TtHBdkpCQ.png"/></div></figure><p id="fbaf" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">其中<strong class="lf ir"><em class="lz">θ</em></strong><em class="lz">=(</em><strong class="lf ir"><em class="lz">θ_</em></strong><em class="lz">1、</em><strong class="lf ir"><em class="lz">θ_</em></strong><em class="lz">2)</em>和<strong class="lf ir"><em class="lz">θ_</em></strong><em class="lz">1<strong class="lf ir">θ_</strong></em><em class="lz">2</em>是中心</p><p id="7381" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">根据MLE，使用下面的片段可以得到最合理的中心</p><pre class="kn ko kp kq gt nc nd ne nf aw ng bi"><span id="5f2e" class="nh mc iq nd b gy ni nj l nk nl"><strong class="nd ir">from</strong> scipy.optimize <strong class="nd ir">import</strong> minimize<br/><strong class="nd ir">import</strong> numpy <strong class="nd ir">as</strong> np<br/><strong class="nd ir">class</strong> <strong class="nd ir">ExampleMLE</strong>:<br/>    <strong class="nd ir">def</strong> <strong class="nd ir">__init__</strong>(self, x1, x2):<br/>        self.x1 <strong class="nd ir">=</strong> x1<br/>        self.x2 <strong class="nd ir">=</strong> x2<br/>    <strong class="nd ir">def</strong> <strong class="nd ir">loss</strong>(self, x):<br/>        mu1 <strong class="nd ir">=</strong> (x[0], x[1])<br/>        mu2 <strong class="nd ir">=</strong> (x[2], x[3])<br/>        log_likelihood <strong class="nd ir">=</strong> (<strong class="nd ir">-</strong> 1<strong class="nd ir">/</strong>2 <strong class="nd ir">*</strong> np.sum((self.x1 <strong class="nd ir">-</strong> mu1)<strong class="nd ir">**</strong>2) <br/>                          <strong class="nd ir">-</strong> 1<strong class="nd ir">/</strong>2 <strong class="nd ir">*</strong> np.sum((self.x2 <strong class="nd ir">-</strong> mu2)<strong class="nd ir">**</strong>2))<br/>        <strong class="nd ir">return</strong> <strong class="nd ir">-</strong> log_likelihood <em class="lz"># adding - to make function minimizable<br/># x1 and x2 are arrays with the coordinates of point for blob 1 and 2 respectively.<br/></em>p <strong class="nd ir">=</strong> ExampleMLE(x1<strong class="nd ir">=</strong>x1, x2<strong class="nd ir">=</strong>x2) <br/>res <strong class="nd ir">=</strong> minimize(p.loss, x0<strong class="nd ir">=</strong>(0, 0, 0, 0))<br/><strong class="nd ir">print</strong>(res.x)</span></pre><p id="30ad" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在下图中，我们可以看到最合理的最大似然生成分布。在这种特定情况下，真正的中心在<code class="fe nm nn no nd b">(3, 3)</code>和<code class="fe nm nn no nd b">(-3, -3)</code>中，最优发现值分别是<code class="fe nm nn no nd b">(3.003, 3.004)</code>和<code class="fe nm nn no nd b">(-3.074, -2.999)</code>，因此该方法似乎是可行的。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/9de7842af15ab8f2faa7fe131d2e1550.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/0*1pNmzSPV7E_3GDKo.jpeg"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated">最大似然法找到的最优高斯分布的原点和等高线图。</p></figure><h1 id="0cd7" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">使用最大似然法进行预测</h1><p id="5f98" class="pw-post-body-paragraph ld le iq lf b lg mt jr li lj mu ju ll lm mv lo lp lq mw ls lt lu mx lw lx ly ij bi translated">在上一节中，我们已经看到了如何使用MLE来估计分布的参数。但是同样的原理可以推广到预测给定<em class="lz"> x </em>的<em class="lz"> y </em>，利用条件概率<em class="lz">P(</em><strong class="lf ir"><em class="lz">Y</em></strong><em class="lz">|</em><strong class="lf ir"><em class="lz">X</em></strong><em class="lz">；θ) </em>。在这种情况下，我们想要估计我们的模型的参数，在给定<em class="lz"> x </em>的情况下更好地预测<em class="lz"> y </em>，这是</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi np"><img src="../Images/a5f9fef4b9d914e28f53fe18b05fb784.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QXlLg85LpWa22FUX6j8TFg.png"/></div></div></figure><p id="e1ed" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">使用和以前一样的技巧</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nq"><img src="../Images/e5458704bb01b54eb41485f7152df6a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aBQHDwXBIQFGaWMV-6rn2A.png"/></div></div></figure><p id="9a35" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">现在，产生真实数据的过程可以写成<em class="lz"> y=f(x)+ϵ </em>，其中<em class="lz"> f(x) </em>是我们要估计的函数，<em class="lz"> ϵ </em>是过程的固有噪声。这里我们假设<em class="lz"> x </em>有足够的信息来预测<em class="lz"> y </em>，再多的额外信息也无法帮助我们预测噪声<em class="lz"> ϵ </em>。一个常见的假设是这种噪声是正态分布的，即:<em class="lz"> ϵ∼N(0,σ ) </em>。这种选择背后的直觉是，即使知道描述系统的所有变量，你总会有一些噪声，而这种噪声通常遵循高斯分布。例如，如果你取同一个城镇中所有相同年龄的女性的身高分布，你会发现一个正态分布。</p><p id="7c05" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">因此，对于我们的情况，条件概率的一个好选择是</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nr"><img src="../Images/e098fce41c2b0ff4e35422dbcc29b482.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fP5w9zuQaqYet3W8DLMx9g.png"/></div></div></figure><p id="ce6b" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">其中<em class="lz"> f^(x) </em>是我们的模型，由<em class="lz"> θ </em>索引。这意味着我们的模型<em class="lz"> f </em>将预测高斯的平均值。</p><p id="ab61" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">现在把条件概率代入我们想要最大化的方程，我们得到</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ns"><img src="../Images/f5cf8cf259ab4d7651bb93e70eae1dc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LcNQK87ArJxm_yi1KdxWGg.png"/></div></div></figure><p id="1e49" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">其中<em class="lz"> y_ip </em>是输入<em class="lz"> x_i </em>的回归模型的输出。请注意，<em class="lz"> n </em>和<em class="lz"> σ </em>都是常量值，因此我们可以将它们从等式中去掉。因此，我们想要解决的函数是</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nt"><img src="../Images/43fcf217433378c72fd8ee69cf26d660.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U429cwnMs-JynG8NGaaJ5w.png"/></div></div></figure><p id="7d9a" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这相当于最小化平方误差损失！</p><p id="9bab" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">但是为什么呢？我们在损失函数背后的直觉是，它惩罚大的小的错误，但是这与条件概率和正态分布有什么关系呢？关键是极值在正态分布中不太可能出现，所以它们会对可能性产生负面影响。例如，对于<em class="lz">p(x)= N(x；0，1) </em>、<em class="lz">log⁡p(1)∶1.42</em>，而<em class="lz">对数⁡p(10)≈−50.92 </em>。因此，在最大化可能性时，我们将首选避免极值<em class="lz">(y _ t</em>-<em class="lz">y _ p)</em>的<em class="lz"> θ </em>值。那么问题<em class="lz">的答案是为什么我们应该最小化MSE？</em>是<em class="lz">，因为我们假设噪声呈正态分布。</em></p><h1 id="e24a" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">结论</h1><p id="c63a" class="pw-post-body-paragraph ld le iq lf b lg mt jr li lj mu ju ll lm mv lo lp lq mw ls lt lu mx lw lx ly ij bi translated">我们刚刚看到，最小化平方误差不是一个任意的选择，而是有理论基础的。我们还看到，它来自于假设噪声呈正态分布。我们在这篇文章中研究的同样的过程可以用来获得多种结果，例如，方差的无偏估计、维特比算法、逻辑回归、机器学习分类等等。</p></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><p id="7d4c" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这个故事最初发表在这里:<a class="ae lc" href="https://www.amolas.dev/posts/mean-squared-error/" rel="noopener ugc nofollow" target="_blank">amolas.dev/posts/mean-squared-error/</a></p></div></div>    
</body>
</html>