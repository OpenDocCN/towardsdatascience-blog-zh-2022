<html>
<head>
<title>EfficientNetV2: Faster, Smaller, and Higher Accuracy than Vision Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">EfficientNetV2:比视觉变压器更快、更小、精度更高</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/efficientnetv2-faster-smaller-and-higher-accuracy-than-vision-transformers-98e23587bf04#2022-10-08">https://towardsdatascience.com/efficientnetv2-faster-smaller-and-higher-accuracy-than-vision-transformers-98e23587bf04#2022-10-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="533d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于EfficientNetV2模型的详细说明，及其架构和培训方法的开发</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0bab5fa2d5c5750a75295e7932093c8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JOoga42iuLkWxj_tKIzYYg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片来源:克里斯里德(Chris Ried)在<a class="ae ky" href="https://unsplash.com/s/photos/python-programming?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a>—<a class="ae ky" href="https://unsplash.com/photos/ieic5Tq8YMk" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/ieic5Tq8YMk</a>拍摄的照片)</p></figure><p id="cf05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">EfficientNets是目前最强大的卷积神经网络(CNN)模型之一。随着视觉变压器的兴起，它实现了比EfficientNets更高的精确度，出现了CNN是否正在消亡的问题。EfficientNetV2不仅提高了准确性，还减少了训练时间和延迟，从而证明了这一点。</p><p id="4f34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我已经详细讨论了这些CNN被开发出来，它们有多强大，以及它对CNN在计算机视觉中的未来意味着什么。</p><h1 id="3177" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">内容</h1><ol class=""><li id="6526" class="mn mo it lb b lc mp lf mq li mr lm ms lq mt lu mu mv mw mx bi translated"><strong class="lb iu">简介</strong></li><li id="ef73" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated"><strong class="lb iu"> EfficientNetV2 </strong></li></ol><p id="8ba5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">………….2.1 efficient net(版本1)的问题</p><p id="2d20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">………….2.2 EfficientNetV2 —为克服问题和进一步改进所做的更改</p><p id="4b75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">………….2.3结果</p><p id="5eef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 3。结论</strong></p><h1 id="01c6" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">1.介绍</h1><p id="2c3e" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">使用神经结构搜索来设计高效网络模型。第一个神经架构搜索是在2016年的论文中提出的—<a class="ae ky" href="https://arxiv.org/abs/1611.01578" rel="noopener ugc nofollow" target="_blank">‘具有强化学习的神经架构搜索</a>’。</p><p id="dfda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其思想是使用一个控制器(一个网络，如RNN)和来自概率为“p”的搜索空间的样本网络结构。然后，通过首先训练网络，然后在测试集上验证它以获得准确度“R”来评估该架构。“p”的梯度通过精度“R”计算和缩放。结果(奖励)被反馈给控制器RNN。控制者充当代理，网络的训练和测试充当环境，结果充当报酬。这是常见的强化学习(RL)循环。这个循环运行多次，直到控制器找到给出高回报(高测试精度)的网络架构。这如图1所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/ccaea686828e277be3f4248fd73f7cd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NL15Qu2HGBuvMKVNEEJWYg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。神经架构搜索概述(来源:图片来自<a class="ae ky" href="https://arxiv.org/abs/1611.01578" rel="noopener ugc nofollow" target="_blank">神经架构搜索论文</a></p></figure><p id="965f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">控制器RNN对各种网络架构参数进行采样，例如每层的过滤器数量、过滤器高度、过滤器宽度、步幅高度和步幅宽度。对于网络的每一层，这些参数可以是不同的。最后选择奖励最高的网络作为最终的网络架构。这如图2所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/3f3236d2f54bd07a36362720cfd6b63a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wDiyan6Hj-4p5Pn4JUra0g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。控制器在网络的每一层中搜索的所有不同参数(来源:图片来自<a class="ae ky" href="https://arxiv.org/abs/1611.01578" rel="noopener ugc nofollow" target="_blank">神经架构搜索论文</a></p></figure><p id="4612" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管这种方法效果很好，但这种方法的一个问题是需要大量的计算能力和时间。</p><p id="2fd0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了克服这个问题，2017年，论文中提出了一种新的方法——<a class="ae ky" href="https://arxiv.org/abs/1707.07012" rel="noopener ugc nofollow" target="_blank">‘学习可扩展图像识别的可转移架构’</a>。</p><p id="f311" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇论文中，作者研究了以前著名的卷积神经网络(CNN)架构，如VGG或ResNet，并指出这些架构在每一层中没有不同的参数，而是有一个具有多个卷积和池层的块，并且在整个网络架构中，这些块被多次使用。作者利用这一想法，使用RL控制器找到这样的块，并重复这些块N次，以创建可扩展的NASNet架构。</p><p id="056a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这在2018年的“<a class="ae ky" href="https://arxiv.org/abs/1807.11626" rel="noopener ugc nofollow" target="_blank"> MnasNet:面向移动设备的平台感知神经架构搜索</a>”论文中得到进一步改进。</p><p id="ece1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个网络中，作者选择了7个区块，并对每个区块的一层进行采样和重复。这如图3所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/ac2785bb2024b0af170940105aff6798.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7rT96rAlxhfGRq9JjBSoag.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3。MnasNet架构中的采样参数。所有用蓝色书写的内容都用RL搜索(来源:图片来自<a class="ae ky" href="https://arxiv.org/pdf/1807.11626.pdf" rel="noopener ugc nofollow" target="_blank"> MnasNet论文</a>)</p></figure><p id="a04f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了这些参数之外，在决定进入控制器的奖励时，还要考虑一个非常重要的参数，那就是“等待时间”。因此，对于MnasNet，作者同时考虑了准确性和延迟，以找到最佳的模型架构。这如图4所示。这使得架构很小，可以在移动或边缘设备上运行。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/9550f831c827636be2369ad4c22f91f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iKZM1JfWIwT_MQQ_bq7pkQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4。寻找模型架构的工作流，同时考虑准确性和延迟以决定对控制者的最终奖励(来源:图片来自<a class="ae ky" href="https://arxiv.org/pdf/1807.11626.pdf" rel="noopener ugc nofollow" target="_blank"> MnasNet论文</a>)</p></figure><p id="859b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，在论文中提出了EfficientNet架构——2020年<a class="ae ky" href="https://arxiv.org/abs/1905.11946" rel="noopener ugc nofollow" target="_blank">‘efficient net:反思卷积神经网络的模型缩放’</a>。</p><p id="f36a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">寻找EfficientNet架构的工作流程与MnasNet非常相似，但没有将“延迟”作为奖励参数，而是考虑了“FLOPs(每秒浮点运算次数)”。这个标准搜索给了作者一个基础模型，他们称之为EfficientNetB0。接下来，他们扩大了基本模型的深度、宽度和图像分辨率(使用网格搜索)以创建另外6个模型，从EfficientNetB1到EfficientNetB7。这种缩放如图5所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/269f97b740dde142b96128a599bf2b11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IGwW3i2a-ojbA3b-__tb2w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5。缩放深度、宽度和图像分辨率，以创建不同的EfficientNet模型(来源:图片来自<a class="ae ky" href="https://arxiv.org/pdf/1905.11946.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientNet论文</a>)</p></figure><p id="f444" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我已经写了一篇关于EfficientNet版本1的文章。要详细了解这个版本，请点击下面的链接—</p><div class="nl nm gp gr nn no"><a href="https://medium.com/mlearning-ai/understanding-efficientnet-the-most-powerful-cnn-architecture-eaeb40386fad" rel="noopener follow" target="_blank"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd iu gy z fp nt fr fs nu fu fw is bi translated">了解高效网络——最强大的CNN架构</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">了解目前最好和最有效的CNN模型EfficientNet</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">medium.com</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc ks no"/></div></div></a></div></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="c104" class="lv lw it bd lx ly ok ma mb mc ol me mf jz om ka mh kc on kd mj kf oo kg ml mm bi translated">2.高效网络V2</h1><p id="31c9" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated"><strong class="lb iu">论文-</strong><a class="ae ky" href="https://arxiv.org/pdf/2104.00298.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="op">efficient net v2:更小的模型和更快的训练</em></strong></a><strong class="lb iu"><em class="op">(2021)</em></strong></p><p id="3a64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">EfficientNetV2比EfficientNet更进一步，提高了训练速度和参数效率。该网络通过使用缩放(宽度、深度、分辨率)和神经架构搜索的组合来生成。主要目标是优化训练速度和参数效率。此外，这次搜索空间还包括新的卷积块，如Fused-MBConv。最终，作者获得了EfficientNetV2架构，它比以前和更新的最新模型快得多，而且体积也小得多(高达6.8倍)。这如图6所示。</p><p id="9269" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图6(b)清楚地显示了EfficientnetV2有2400万个参数，而Vision Transformer (ViT)有8600万个参数。V2版本也有原始EfficientNet的将近一半的参数。虽然它确实显著减小了参数大小，但它在ImageNet数据集上保持了与其他模型相似或更高的精度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/910b4dc3efcf5a2543e7130a50bb46a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WfiQo2x0WFX5JgfEqSnoFA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6。与其他先进模型相比，EfficientNetV2模型的训练和参数效率(来源:<a class="ae ky" href="https://arxiv.org/pdf/2104.00298.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientNetV2论文</a>)</p></figure><p id="e1e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者还进行了渐进式学习，即一种逐渐增加图像大小以及正则化(如删除和数据增加)的方法。这种方法进一步加快了训练速度。</p><h2 id="ad84" class="or lw it bd lx os ot dn mb ou ov dp mf li ow ox mh lm oy oz mj lq pa pb ml pc bi translated">2.1 efficient net(版本1)的问题</h2><p id="8f1e" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">EfficientNet(原始版本)有以下瓶颈—</p><p id="1d39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">a.效率网络通常比其他大型CNN模型训练得更快。但是，当使用大图像分辨率来训练模型(B6或B7模型)时，训练是缓慢的。这是因为更大的EfficientNet模型需要更大的图像大小来获得最佳结果，当使用更大的图像时，需要降低批处理大小以适应GPU/TPU内存中的这些图像，从而使整个过程变慢。</p><p id="8d52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">b.在网络架构的早期，深度方向的卷积层(MBConv)速度很慢。深度方向卷积层通常比常规卷积层具有更少的参数，但问题是它们不能充分利用现代加速器。为了解决这个问题，EfficientNetV2结合使用了MBConv和Fused MBConv，在不增加参数的情况下加快了训练速度(本文稍后将对此进行讨论)。</p><p id="6c6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">c.从B0到B7，高度、宽度和图像分辨率都采用了相同的比例来创建各种高效网络模型。所有层的这种等比例缩放并不是最佳的。例如，如果深度被缩放2倍，网络中的所有块被放大2倍，使得网络非常大/深。将一个块缩放两次，将另一个块缩放1.5倍(非均匀缩放)可能更好，这样可以在保持良好精度的同时减小模型大小。</p><h2 id="ec3c" class="or lw it bd lx os ot dn mb ou ov dp mf li ow ox mh lm oy oz mj lq pa pb ml pc bi translated">2.2 EfficientNetV2 —为克服问题和进一步改进所做的更改</h2><p id="cfb5" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated"><strong class="lb iu"> a .添加MBConv和融合MBConv模块的组合</strong></p><p id="b21b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如2.1(b)中所述，MBConv block通常无法充分利用现代加速器。融合MBConv层可以更好地利用服务器/移动加速器。</p><p id="d446" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MBConv层最初是在MobileNets中引入的。如图7所示，MBConv和Fused-MBConv结构的唯一区别是最后两个模块。MBConv使用深度方向卷积(3x3 ),后跟一个1x1卷积层，而Fused-MBConv用一个简单的3x3卷积层替换/融合这两层。</p><p id="b852" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">融合的MBConv层可以使训练速度更快，只需增加很少的参数，但如果使用许多这样的模块，就会因添加更多的参数而大大降低训练速度。为了克服这个问题，作者在神经架构搜索中通过了MBConv和Fused-MBConv，该搜索自动决定这些块的最佳组合，以获得最佳性能和训练速度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/b6e616c1d1d56c9ebadb82c1b0d648fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*DOf_v9EJuOhldHugkB8dAA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7。MBConv和融合MBConv模块的结构(来源:<a class="ae ky" href="https://arxiv.org/pdf/2104.00298.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientNetV2 paper </a></p></figure><p id="761c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> b .优化准确性、参数效率和培训效率的NAS搜索</strong></p><p id="65df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="op">进行神经结构搜索是为了联合优化准确性、参数效率和训练效率。</em>efficient net模型被用作主干，搜索是在各种设计选择下进行的，如卷积块、层数、过滤器尺寸、扩展比等。将近1000个模型作为样本，训练10个时期，并比较它们的结果。在准确性、训练步骤时间和参数大小方面优化得最好的模型被选为EfficientNetV2的最终基础模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/9f7b107c6a46ab26b1b21ef75027d24e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*kquwaOFiitEuYbwOmP8nTA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图8。EfficientNetV2-S的架构(来源:<a class="ae ky" href="https://arxiv.org/pdf/2104.00298.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientNetV2论文</a>)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/51c308300316a19cfc40da56c6e2ee56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*tjTpEAC1Kpf-KW2G5rICDw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图9。EfficientNet-B0的架构(来源:<a class="ae ky" href="https://arxiv.org/pdf/1905.11946.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientNet论文</a>)</p></figure><p id="f326" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图8显示了EfficientNetV2模型(EfficientNetV2-S)的基本模型架构。该模型开始时包含融合MBConv层，但后来切换到MBConv层。为了比较，我还在图9中展示了之前EfficientNet论文的基本模型架构。以前的版本只有MBConv层，没有熔断MBConv层。</p><p id="aecd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与EfficientNet-B0相比，EfficientNetV2-S的膨胀率也较小。EfficeinetNetV2不使用5x5滤镜，只使用3x3滤镜。</p><p id="271f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> c .智能模型缩放</strong></p><p id="d08e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦获得了EfficientNetV2-S型号，就可以按比例增加以获得EfficientNetV2-M和EfficientNetV2-L型号。使用了一种复合缩放方法，类似于EfficientNet，但是做了更多的改变，使模型更小更快——</p><p id="1cb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">I .最大图像大小被限制为480x480像素，以减少GPU/TPU内存的使用，从而提高训练速度。</p><p id="ea33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">二。更多的层被添加到后面的阶段(图8中的阶段5和阶段6)，以在不增加太多运行时开销的情况下增加网络容量。</p><p id="7a13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> d .渐进式学习</strong></p><p id="0764" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">较大的图像尺寸通常倾向于给出更好的训练结果，但是增加了训练时间。一些论文先前已经提出了动态改变图像尺寸，但是这经常导致训练准确性的损失。</p><p id="6b88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">EfficientNetV2的作者表明，由于在训练网络时图像大小是动态变化的，因此正则化也应该相应地改变。改变图像尺寸，但保持相同的正则化会导致精度损失。此外，较大的模型比较小的模型需要更多的正则化。</p><p id="914c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者使用不同的图像尺寸和不同的放大来测试他们的假设。如图10所示，当图像尺寸较小时，较弱的放大会产生较好的结果，但当图像尺寸较大时，较强的放大会产生较好的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/c6f1352cb157af00dd4759c774759fc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*kdAngq7Juvu7qfT8DAoFOA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图10。使用不同的图像大小和不同的增强参数测试ImageNet top-1准确性(来源:<a class="ae ky" href="https://arxiv.org/pdf/2104.00298.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientNetV2论文</a></p></figure><p id="2708" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑到这一假设，EfficientNetV2的作者使用了带有自适应正则化的<strong class="lb iu">渐进学习</strong>。想法很简单。在前面的步骤中，网络在小图像和弱正则化上被训练。这允许网络快速学习特征。然后图像尺寸逐渐增大，正则化也是如此。这使得网络很难学习。总的来说，这种方法具有更高的精度、更快的训练速度和更少的过拟合。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/2f5618937dbd222055dcccc43826d958.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*omNIf9UmFD5cyLJapRxg1g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图11。具有自适应正则化的渐进学习算法(来源:<a class="ae ky" href="https://arxiv.org/pdf/2104.00298.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientNetV2论文</a>)</p></figure><p id="018d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">初始图像尺寸和正则化参数是用户定义的。然后，在特定阶段(M)之后，应用线性插值来增加图像大小和正则化，如图11所示。这在图12中有更好的直观解释。随着历元数的增加，图像尺寸和放大倍数也逐渐增加。EfficicentNetV2使用三种不同类型的正则化— Dropout、RandAugment和Mixup。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/04b438d8ff42bbddfe7217550be6beb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*wzN13K0KBtZZBVDmrnGf7A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图12。具有自适应正则化视觉解释的渐进学习(来源:<a class="ae ky" href="https://arxiv.org/pdf/2104.00298.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientNetV2论文</a></p></figure><h2 id="84ea" class="or lw it bd lx os ot dn mb ou ov dp mf li ow ox mh lm oy oz mj lq pa pb ml pc bi translated">2.3结果</h2><p id="6d5d" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">i. EfficientNetV2-M实现了与EfficientNetB7(以前最好的EfficientNet型号)相似的精度。此外，EfficientNetV2-M的训练速度比EfficientNetB7快近11倍。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/6cc2b9fd22508752b3256fffc7806d4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*TnrRHeh1S82NjCmg-3kpHg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图13 a .与其他先进模型的参数效率比较(来源:<a class="ae ky" href="https://arxiv.org/pdf/2104.00298.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientNetV2论文</a>)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/d56cfee7caf36a8ec0e20bbe5102f37d.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*VVmBpIASaSn4kkNS-QE2zw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图13 b. FLOPs与其他先进模型的效率比较(来源:<a class="ae ky" href="https://arxiv.org/pdf/2104.00298.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientNetV2论文</a>)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/f32d5c80fe0f0bccb52b66570e60a14f.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*nNP7pmXIYre2HiZq7D4ECw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图13 c .与其他先进模型的延迟比较(来源:<a class="ae ky" href="https://arxiv.org/pdf/2104.00298.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientNetV2论文</a>)</p></figure><p id="4846" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如图13 a、13 b和13 c所示，EfficientNetV2模型优于所有其他最先进的计算机视觉模型，包括视觉变压器。</p><p id="09b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要了解更多关于视觉变形金刚的信息，请访问下面的链接</p><div class="nl nm gp gr nn no"><a rel="noopener follow" target="_blank" href="/are-transformers-better-than-cnns-at-image-recognition-ced60ccc7c8"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd iu gy z fp nt fr fs nu fu fw is bi translated">变形金刚在图像识别方面比CNN强吗？</h2><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">towardsdatascience.com</p></div></div><div class="nx l"><div class="pm l nz oa ob nx oc ks no"/></div></div></a></div><p id="ceef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图14显示了在ImageNet21k上预训练有1300万张图像的EfficientNetV2模型和在ImageNet ILSVRC2012上预训练有128万张图像的一些模型与所有其他最先进的CNN和transformer模型的详细比较。除了ImageNet数据集，模型还在其他公共数据集上进行测试，如CIFAR-10、CIFAR-100、Flowers数据集和Cars数据集，在每种情况下，模型都显示出非常高的准确性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pn"><img src="../Images/12594fba42e79d2480ef7824aa88dca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O7RLatfLA1rgFOpEOXJdlA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图14。EfficientNetV2模型与其他CNN和变压器模型在精度、参数和FLOPs方面的比较(来源:<a class="ae ky" href="https://arxiv.org/pdf/2104.00298.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientNetV2论文</a></p></figure></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="3e17" class="lv lw it bd lx ly ok ma mb mc ol me mf jz om ka mh kc on kd mj kf oo kg ml mm bi translated">3.结论</h1><p id="ee28" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">EfficientNetV2型号比大多数最先进的型号更小、更快。这个CNN模型表明，即使视觉变形金刚已经席卷了计算机视觉世界，但通过获得比其他CNN更高的精度，具有改进训练方法的更好结构的CNN模型仍然可以获得比变形金刚更快更好的结果，这进一步证明了CNN将继续存在。</p></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h2 id="f601" class="or lw it bd lx os ot dn mb ou ov dp mf li ow ox mh lm oy oz mj lq pa pb ml pc bi translated">参考文献—</h2><p id="4296" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">EfficientNetV2纸张—</p><p id="2bc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">谭，明星&amp;乐，郭。(2021).EfficientNetV2:更小的模型和更快的训练。arXiv，doi:10.48550/arXiv . 2104.00298 https://arxiv.org/abs/2104.00298<a class="ae ky" href="https://arxiv.org/abs/2104.00298" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>