<html>
<head>
<title>Outlier Detection with Simple and Advanced Techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用简单和先进的技术检测异常值</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/detecting-outliers-with-simple-and-advanced-techniques-cb3b2db60d03#2022-11-17">https://towardsdatascience.com/detecting-outliers-with-simple-and-advanced-techniques-cb3b2db60d03#2022-11-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="87b7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于如何使用标准差、四分位数范围、隔离森林、DBSCAN 和局部异常值因子检测异常值的教程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/965c462b6efe8684d956cbe211302068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HD2QNGmyckBibnJV"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@vorosbenisop?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">本杰明·沃罗斯</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="639e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Outlier" rel="noopener ugc nofollow" target="_blank">离群值</a>是远离数据集中大多数观察值的数据点。异常值的出现有多种原因，例如群体行为的自然偏差、欺诈活动以及人为或系统错误。然而，在运行任何统计分析或为训练机器学习模型准备数据之前，检测和识别异常值是必不可少的。</p><p id="8fb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将介绍单变量和多变量异常值，它们之间的区别，以及如何使用统计方法和自动化异常检测技术来识别它们。我们将看到用于检测单变量异常值和隔离森林的四分位距和标准差方法、用于检测多变量异常值的 DBSCAN(带噪声的基于密度的应用程序空间聚类)和 LOF(局部异常值因子)。</p><p id="38a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在阅读本文的同时，我鼓励您查看我的 GitHub 上的<a class="ae ky" href="https://github.com/Idilismiguzel/Machine-Learning/blob/master/Outlier%20Detection/Outlier_Detection.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本</a>以获得完整的分析和代码。</p><p id="b494" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有很多要讲的，让我们开始吧！🚀</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="07d3" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">数据</h1><p id="b050" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在本文中，我们将使用来自 UCI 的<a class="ae ky" href="https://www.kaggle.com/uciml/glass" rel="noopener ugc nofollow" target="_blank">玻璃识别数据集，该数据集具有 8 个与玻璃含量和玻璃类型相关的属性(即“Na”)。</a></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="d109" class="ne md it na b gy nf ng l nh ni">import pandas as pd</span><span id="f910" class="ne md it na b gy nj ng l nh ni">glass = pd.read_csv('glass.csv')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/b30f2d1152612ca07edfe39ac0836d71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vyFH-fPxLf8Ix42v.png"/></div></figure><h1 id="7b76" class="mc md it bd me mf nl mh mi mj nm ml mm jz nn ka mo kc no kd mq kf np kg ms mt bi translated">单变量和多变量异常值</h1><p id="ae93" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">通过使用<code class="fe nq nr ns na b">seaborn’s pairplot</code>,我们可以绘制玻璃含量之间的成对关系，通过这种视觉，我们可以看到我们的基础数据的分布情况。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="9495" class="ne md it na b gy nf ng l nh ni">import seaborn as sns</span><span id="b15f" class="ne md it na b gy nj ng l nh ni">sns.pairplot(glass, diag_kws={'color':'red'})</span></pre><div class="kj kk kl km gt ab cb"><figure class="nt kn nu nv nw nx ny paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/84f9d5e8c34d05305ce622a1601060fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*g48S_M28wZ1NC0a927iYrw.png"/></div></figure><figure class="nt kn nu nv nw nx ny paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/34fb56b9d7a6e6688f92c665304eac83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*cmEFJkUPYpxXHXQYYJK-mw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk nz di oa ob translated">变量之间的成对关系</p></figure></div><p id="4206" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们仔细观察上面的图，我们会发现玻璃属性位于 x 轴和 y 轴上。沿着红色的对角线，我们可以看到显示分布的直方图。</p><p id="e494" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，并非所有属性都遵循钟形曲线的正态分布，但事实上，大多数属性都偏向较低值(如 Ba、Fe)或较高值(如 Mg)。为了检测<strong class="lb iu">单变量异常值</strong>，我们应该关注单个属性的分布，并找到远离该属性大部分数据的数据点。例如，如果我们选择“Na”并绘制一个箱线图，我们可以找到哪些数据点在胡须之外，可以标记为异常值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/a5ec0b4bf91acf215dee7815706fa19c.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*zar4b70v9CrJx79eQXqVGQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Na 的箱线图—显示晶须外部的数据点。</p></figure><p id="2560" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相反，为了检测多元异常值，我们应该关注 n 维空间中至少两个变量的组合。例如，在玻璃数据集中，我们可以使用玻璃的所有八个属性，并在 n 维空间中绘制它们，并通过检测哪些数据点落在远处来找到多元异常值。</p><p id="3887" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于绘制超过三个维度是不可能的，我们应该找到一种方法将八个维度转换到一个更低维度的空间。PCA —主成分分析是一种降维技术，它通过最大化低维表示中的方差来执行高维空间到低维空间的线性映射。我们可以通过用<code class="fe nq nr ns na b">n_components=3.</code>执行 PCA 将八个维度转换成三维空间</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/719801f8f2ceb044a74483a1e25d3ee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*8fBDBdgDITzmuTW2s32HCg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有 3 种成分的 PCA 可视化(颜色代表玻璃的“类型”)</p></figure><p id="e88a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以在图中看到，有些数据点彼此靠近(构成密集区域)，有些远离，可能是<em class="og">多元</em>异常值。</p><p id="e960" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要遵循不同的程序来检测单变量和多变量异常值。让我们从单变量异常值开始，学习标准差和四分位距方法来检测它们。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="4eee" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">单变量异常检测</h1><h2 id="2eb4" class="ne md it bd me oh oi dn mi oj ok dp mm li ol om mo lm on oo mq lq op oq ms or bi translated">1.标准偏差法</h2><p id="306b" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">假设一个变量(几乎)是正态分布的。在这种情况下，其直方图应遵循钟形曲线，68.3%的数据值位于平均值的一个标准偏差内，95.4%的数据值位于平均值的两个标准偏差内，99.7%的数据值位于平均值的三个标准偏差内。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/c4e19dcef43eb059c9aab7beb3ad5967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9igRzhHx7eMSlALOsPbdcA.jpeg"/></div></div></figure><p id="5a98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，如果数据点距离平均值超过三个标准差，我们就可以检测出异常值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="3ebf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过使用标准偏差技术，我们根据“Na”变量的分布(这是一个极值)删除了两个记录。您可以扩展并覆盖所有其他属性，以移除单变量异常值。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="c40b" class="ne md it na b gy nf ng l nh ni">Shape of original dataset: (213, 9) <br/>Shape of dataset after removing outliers in Na column: (211, 9)</span></pre><h2 id="d5de" class="ne md it bd me oh oi dn mi oj ok dp mm li ol om mo lm on oo mq lq op oq ms or bi translated">2.四分位极差法</h2><p id="4880" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">四分位距方法(最好使用箱线图显示)通过定义三个点将数据分成四分位:</p><blockquote class="ot ou ov"><p id="c6c4" class="kz la og lb b lc ld ju le lf lg jx lh ow lj lk ll ox ln lo lp oy lr ls lt lu im bi translated">四分位数 1 (Q1)代表第 25 百分位<br/>四分位数 2 (Q2)代表第 50 百分位<br/>四分位数 3 (Q3)代表第 75 百分位</p></blockquote><p id="0778" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">箱线图中的方框表示 IQR 范围，其被定义为 Q1 和 Q3 之间的范围；<code class="fe nq nr ns na b">IQR = Q3 — Q1</code>和低于<code class="fe nq nr ns na b">Q1 - 1.5*IQR</code>或高于<code class="fe nq nr ns na b">Q3 + 1.5*IQR</code>的数据点被定义为异常值。</p><p id="d717" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在方框图中，<code class="fe nq nr ns na b">Q1 - 1.5*IQR</code>和<code class="fe nq nr ns na b">Q3 + 1.5*IQR</code>用触须表示，异常值用上方或下方的点表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/730335250a406e09b9225f39a660363f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZrRgmtVHMVLknr7BmezXlg.jpeg"/></div></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="2feb" class="ne md it na b gy nf ng l nh ni">Shape of original dataset: (213, 9) <br/>Shape of dataset after removing outliers in Na column: (206, 9)</span></pre><p id="ec15" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过使用 IQR 技术，我们根据“Na”变量的分布删除了七个记录。正如你可以注意到的，标准差方法只能找到 2 个异常值，这是真正的极值点，但使用 IQR 方法，我们能够检测到更多(5 个不那么极端的记录)。由您和您的用例来决定哪种方法对数据集更好，以及您是否有空间丢弃更多的数据点。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="3937" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们继续讨论多元异常值，了解隔离森林、DBSCAN(含噪声的基于密度的应用程序空间聚类)和 LOF(本地异常值因子)。</p><h1 id="0e69" class="mc md it bd me mf nl mh mi mj nm ml mm jz nn ka mo kc no kd mq kf np kg ms mt bi translated">多元异常检测</h1><h2 id="5404" class="ne md it bd me oh oi dn mi oj ok dp mm li ol om mo lm on oo mq lq op oq ms or bi translated">1.隔离森林法</h2><p id="6887" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html" rel="noopener ugc nofollow" target="_blank">隔离森林</a>是一种基于随机森林的无监督机器学习算法。你可能知道，随机森林是一个<a class="ae ky" href="https://medium.com/towards-data-science/practical-guide-to-ensemble-learning-d34c74e022a0" rel="noopener">集成学习</a>模型，它使用基本模型(比如 100 棵决策树)和在最终决策中权重较高的模型的集成。</p><blockquote class="ot ou ov"><p id="e3a9" class="kz la og lb b lc ld ju le lf lg jx lh ow lj lk ll ox ln lo lp oy lr ls lt lu im bi translated">如果你需要一个关于集合学习的复习，你可以看看这篇文章。</p></blockquote><div class="pa pb gp gr pc pd"><a rel="noopener follow" target="_blank" href="/practical-guide-to-ensemble-learning-d34c74e022a0"><div class="pe ab fo"><div class="pf ab pg cl cj ph"><h2 class="bd iu gy z fp pi fr fs pj fu fw is bi translated">集成学习实用指南</h2><div class="pk l"><h3 class="bd b gy z fp pi fr fs pj fu fw dk translated">通过投票、打包、提升和堆叠来改进您的模型</h3></div><div class="pl l"><p class="bd b dl z fp pi fr fs pj fu fw dk translated">towardsdatascience.com</p></div></div><div class="pm l"><div class="pn l po pp pq pm pr ks pd"/></div></div></a></div><p id="7203" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">隔离林遵循随机林的方法，但相反，它检测(或换句话说隔离)异常数据点。为了做到这一点，它做了两个基本假设:离群值是少数，他们是不寻常的。</p><p id="5b46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">隔离林通过随机选择一个功能，然后随机选择一个拆分规则来分隔所选功能的值，从而创建决策树。这个过程一直持续到达到设定的超参数值。然后，隔离林考虑如果树更短并且分区更少，则相应的值是异常值(少数和不寻常)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ps"><img src="../Images/cba421998158d926570d406d386e1300.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_QDLToxDsoiCv42-VA1Nlw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">孤立森林异常检测图解</p></figure><p id="38da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看使用来自<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>的<code class="fe nq nr ns na b">IsolationForest</code>类的实现。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="159d" class="ne md it na b gy nf ng l nh ni">from sklearn.ensemble import IsolationForest</span><span id="4525" class="ne md it na b gy nj ng l nh ni">IsolationForest(<em class="og">n_estimators=100</em>, <em class="og">max_samples='auto'</em>, <em class="og">contamination='auto'</em>, <em class="og">max_features=1.0</em>, <em class="og">bootstrap=False</em>, <em class="og">n_jobs=None</em>, <em class="og">random_state=None</em>, <em class="og">verbose=0</em>, <em class="og">warm_start=False</em>)</span></pre><p id="1ca4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">隔离森林算法有几个超参数，例如用于集成的基本估计量数量的<strong class="lb iu"> n_estimators </strong>、用于训练模型的样本数量的<strong class="lb iu"> max_samples </strong>、用于定义数据中异常值比例的<strong class="lb iu"> contamination </strong>、用于从数据中提取用于训练的特征数量的<strong class="lb iu"> max_features </strong>。其余的可以看<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="39ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们通过将基本模型的数量设置为 100，将最大特征的数量设置为特征的总数，将污染设置为<code class="fe nq nr ns na b">'auto'</code>来启动隔离林，该隔离林使用原始文件中确定的偏移和污染阈值。如果污染为 0.1，那么数据集的 10%将被定义为异常值。</p><p id="168f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过调用<code class="fe nq nr ns na b">glass['outlier'].value_counts()</code>，我们可以看到有 19 条记录被标记为<code class="fe nq nr ns na b">-1</code>——异常值，其余 195 条记录被标记为<code class="fe nq nr ns na b">1</code>——非异常值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/4c8b78c2eaef9c681e114622ef876a3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*-rTVHt7vymX2L77R-MK0iA.png"/></div></figure><p id="5903" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如前所述，我们可以通过主成分分析将特征数量减少到三个分量来可视化异常值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/8bdf2c5af21b6097496788c1399dc539.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*Ea6Zepcgasly7HDkMfO84A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">隔离林检测到异常值(黄色)(n 估计值=100，污染=“自动”)</p></figure><blockquote class="ot ou ov"><p id="f56e" class="kz la og lb b lc ld ju le lf lg jx lh ow lj lk ll ox ln lo lp oy lr ls lt lu im bi translated">为了保持内容的重点，我不会展示如何调优超参数，但是如果您感兴趣，可以查看本文。</p></blockquote><h2 id="4ede" class="ne md it bd me oh oi dn mi oj ok dp mm li ol om mo lm on oo mq lq op oq ms or bi translated">2.基于密度的含噪声应用空间聚类(DBSCAN)方法</h2><p id="5574" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html?highlight=dbscan#sklearn.cluster.DBSCAN" rel="noopener ugc nofollow" target="_blank"> DBSCAN </a>是一种流行的聚类算法，通常用作 K-means 的替代算法。它是<em class="og">基于密度的</em>，这意味着它侧重于许多数据点所在的高密度区域。它通过测量数据之间的特征空间距离(即欧几里德距离)来执行<em class="og">空间聚类</em>，以识别哪些可以被聚类在一起。它允许有噪声的应用，这意味着我们可以对有噪声的数据使用 DBSCAN。但这还不是全部，DBSCAN 的最大优势之一是我们不需要预先定义集群的数量。</p><p id="08b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看使用<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>中的<code class="fe nq nr ns na b">DBSCAN</code>类的实现。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="830e" class="ne md it na b gy nf ng l nh ni">from sklearn.cluster import DBSCAN</span><span id="b2b3" class="ne md it na b gy nj ng l nh ni">DBSCAN(<em class="og">eps=0.5</em>, <em class="og">min_samples=5</em>, <em class="og">metric=’euclidean’</em>, <em class="og">metric_params=None</em>, <em class="og">algorithm=’auto’</em>, <em class="og">leaf_size=30</em>, <em class="og">p=None</em>, <em class="og">n_jobs=None</em>)</span></pre><p id="4fef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DBSCAN 有几个超参数，例如用于在同一个聚类中考虑的两个数据点之间的最大距离的<strong class="lb iu">EPS(ε)</strong>、用于被认为是核心点的点的接近数据点的数量的<strong class="lb iu"> min_samples </strong>、用于计算点之间距离的<strong class="lb iu">度量</strong>。其余的可以看<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html?highlight=dbscan#sklearn.cluster.DBSCAN" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="c7d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">启动 DBSCAN 时，仔细选择超参数非常重要。例如，如果 eps 值选择得太小，那么大多数数据可以被归类为异常值，因为邻域面积被定义为更小。相反，如果 eps 值选择得太大，则大多数点可以聚集在一起，因为它们可能位于相同的邻域中。这里，我们使用<a class="ae ky" href="https://stackoverflow.com/questions/43160240/how-to-plot-a-k-distance-graph-in-python" rel="noopener ugc nofollow" target="_blank">k-距离图选择 eps 为 0.4。</a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pv"><img src="../Images/8fa71b4e02ada7c7cf1b7a2f8459fdb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8w8trQ-SD63pqKO-DgBRtw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">k-距离图，其中大部分数据距离第 10 个最近的邻居不超过 0.4 个单位</p></figure><p id="2a3b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，min_samples 是一个重要的超参数，通常等于或大于 3，大多数情况下选择为 D+1，其中 D 是数据集的维数。在我们的示例中，我们将 min_samples 设置为 10。</p><p id="a33b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为 DBSCAN 通过密度来识别聚类，所以高密度区域是聚类发生的地方，低密度区域是异常值发生的地方。通过调用<code class="fe nq nr ns na b">glass['outlier'].value_counts()</code>，我们可以看到有 22 条记录被标记为<code class="fe nq nr ns na b">-1</code>——异常值，其余 192 条记录被标记为<code class="fe nq nr ns na b">1</code>——非异常值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/0d8841d9e0db60c1c715efd6fe4d9419.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*wLX76UXfH_JY_WrMRI7tPw.png"/></div></figure><p id="c8ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用主成分分析来可视化异常值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi px"><img src="../Images/d882d686d830de44613a85d92f46dce0.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*Epuw0lcoPaAw72Q5jNYDyw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">DBSCAN 检测到异常值(黄色)(eps=0.4，min_samples=10)</p></figure><h2 id="454c" class="ne md it bd me oh oi dn mi oj ok dp mm li ol om mo lm on oo mq lq op oq ms or bi translated">3.本地异常因素(LOF)</h2><p id="c8f3" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/neighbors/plot_lof_outlier_detection.html#sphx-glr-auto-examples-neighbors-plot-lof-outlier-detection-py" rel="noopener ugc nofollow" target="_blank"> LOF </a>是一种流行的无监督异常检测算法，它计算数据点相对于其邻居的局部密度偏差。计算后，密度较低的点被视为异常值。</p><p id="fe5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看使用来自<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>的<code class="fe nq nr ns na b">LOF</code>类的实现。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="d132" class="ne md it na b gy nf ng l nh ni">from sklearn.neighbors import LocalOutlierFactor</span><span id="fe6c" class="ne md it na b gy nj ng l nh ni"><em class="og">LocalOutlierFactor(n_neighbors=20, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, contamination='auto', novelty=False, n_jobs=None)</em></span></pre><p id="747f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LOF 有几个超参数，如<strong class="lb iu"> n_neighbors </strong>用于选择默认等于 20 的邻域数，以及<strong class="lb iu">contaminance</strong>用于定义异常值的比例，它可以等于(0，0.5)范围内的<code class="fe nq nr ns na b">'float'</code>或使用原始论文中确定的偏移和污染阈值的<code class="fe nq nr ns na b">'auto'</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="2896" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过调用<code class="fe nq nr ns na b">glass['outlier'].value_counts()</code>，我们可以看到有 34 条记录被标记为<code class="fe nq nr ns na b">-1</code>——异常值，其余 180 条记录被标记为<code class="fe nq nr ns na b">1</code>——非异常值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi py"><img src="../Images/dab1e636e00802a740c34544bc44df2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*aLCKS1a_7vsCYEM4SmTrnQ.png"/></div></figure><p id="d32d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们可以使用主成分分析来可视化这些异常值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/e13c79ea6358b6aa76a31b5901144174.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*v7C5SyVDJ0O9nrEYyTCjzw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">LOF 检测到异常值(黄色)(n_neighbors=20，污染=“自动”)</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="96cb" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">结论</h1><p id="2d40" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在本文中，我们探索了不同的方法来检测数据集中的异常值。我们从单变量异常值检测技术开始，涵盖了标准差和四分位间距方法。我们对玻璃鉴定数据集中的“Na”列执行了这些方法。然后，我们转向多元异常检测技术，涵盖了隔离森林、DBSCAN 和局部异常因素。通过这些方法，我们学会了如何使用特征空间中的所有维度来检测异常值。除了异常值检测，我们还学习了如何使用 PCA——一种降维技术来可视化 n 维数据。</p><p id="7c00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在介绍这些方法时，我们没有花太多时间调优超参数。然而，超参数调整是 ML 模型开发中的一个重要步骤。如果你想学习或者刷新一下知识，可以看看下面这篇文章。</p><div class="pa pb gp gr pc pd"><a rel="noopener follow" target="_blank" href="/hyperparameter-tuning-with-grid-search-and-random-search-6e1b5e175144"><div class="pe ab fo"><div class="pf ab pg cl cj ph"><h2 class="bd iu gy z fp pi fr fs pj fu fw is bi translated">基于网格搜索和随机搜索的超参数调谐</h2><div class="pk l"><h3 class="bd b gy z fp pi fr fs pj fu fw dk translated">并深入探讨如何将它们结合起来</h3></div><div class="pl l"><p class="bd b dl z fp pi fr fs pj fu fw dk translated">towardsdatascience.com</p></div></div><div class="pm l"><div class="qa l po pp pq pm pr ks pd"/></div></div></a></div><p id="9fcf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望您喜欢阅读异常值检测，并发现这篇文章对您的工作有用！</p><p id="a7a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="og">如果你喜欢这篇文章，你可以</em><strong class="lb iu"><em class="og"/></strong><a class="ae ky" href="https://medium.com/@idilismiguzel" rel="noopener"><strong class="lb iu"><em class="og">在这里阅读我的其他文章</em></strong></a><strong class="lb iu"><em class="og"/></strong><em class="og">和</em> <a class="ae ky" href="http://medium.com/@idilismiguzel/follow" rel="noopener"> <strong class="lb iu"> <em class="og">关注我上媒</em></strong></a><strong class="lb iu"><em class="og"/></strong>如果您有任何问题或建议，请告诉我。✨</p><p id="ee52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">喜欢这篇文章吗？ <a class="ae ky" href="https://idilismiguzel.medium.com/membership" rel="noopener"> <strong class="lb iu">成为会员求更！</strong> </a></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="c20d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">参考</p><ol class=""><li id="baa5" class="qb qc it lb b lc ld lf lg li qd lm qe lq qf lu qg qh qi qj bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html" rel="noopener ugc nofollow" target="_blank">隔离森林由 Scikit-Learn </a></li><li id="02ab" class="qb qc it lb b lc qk lf ql li qm lm qn lq qo lu qg qh qi qj bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html?highlight=dbscan#sklearn.cluster.DBSCAN" rel="noopener ugc nofollow" target="_blank">通过 Scikit-Learn 进行数据库扫描</a></li><li id="e08f" class="qb qc it lb b lc qk lf ql li qm lm qn lq qo lu qg qh qi qj bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/neighbors/plot_lof_outlier_detection.html#sphx-glr-auto-examples-neighbors-plot-lof-outlier-detection-py" rel="noopener ugc nofollow" target="_blank">sci kit-Learn 的局部异常因子</a></li><li id="db57" class="qb qc it lb b lc qk lf ql li qm lm qn lq qo lu qg qh qi qj bi translated">由<a class="ae ky" href="https://unsplash.com/@vorosbenisop?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">本杰明·沃罗斯</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的标题照片</li><li id="b2ea" class="qb qc it lb b lc qk lf ql li qm lm qn lq qo lu qg qh qi qj bi translated">所有其他图片均由作者提供</li><li id="f74b" class="qb qc it lb b lc qk lf ql li qm lm qn lq qo lu qg qh qi qj bi translated"><a class="ae ky" href="https://www.kaggle.com/datasets/uciml/glass" rel="noopener ugc nofollow" target="_blank">来自 Kaggle </a>的玻璃鉴定数据集作者:Dua，d .和 Graff，C. (2019)。UCI 机器学习知识库[http://archive . ics . UCI . edu/ml]。加州欧文:加州大学信息与计算机科学学院。</li></ol></div></div>    
</body>
</html>