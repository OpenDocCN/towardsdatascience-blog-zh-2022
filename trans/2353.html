<html>
<head>
<title>Let Data Science Make Choices for You</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让数据科学为你做出选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/let-data-science-make-choices-for-you-5702bd759803#2022-05-23">https://towardsdatascience.com/let-data-science-make-choices-for-you-5702bd759803#2022-05-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="134b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">多臂强盗入门</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c7c86e119d52dd6edd37002c01a447b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mAHJm-AsW3h2Xn9vZgr9lQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@burst?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">上的</a>爆裂<a class="ae ky" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">未爆裂</a></p></figure><p id="7dd5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">想象一下，你必须从一组结果不确定的互斥选项中做出多个选择。你如何最大化你的收益？</p><p id="a6d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">反复的过程，一次又一次的选择，不是抽象的东西，而是我们每天都在做的事情:我们吃什么？给定时间和交通状况，我们走哪条路回家？我们如何最大限度地减少花在路上的时间？我应该去我的城市的什么餐馆？</p><p id="7113" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些问题由一类称为<strong class="lb iu">多臂强盗(MAB)</strong>的算法解决，这是<strong class="lb iu">强化学习</strong> (RL)算法的一个子领域，旨在通过在没有监督的情况下执行行动，随着时间的推移做出更好的选择。</p><p id="cead" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们一起来回顾一下最常见和最简单的算法，从具体的例子开始，逐步发展到数学形式化。在我最近对RL感兴趣之前(近两年)，我对它知之甚少。在博士期间，我学习的书籍主要集中在监督和非监督学习上，强化学习要么被简单提及，要么被彻底抛弃。只是在最近几年，很大程度上由于深度思维的创新，RL正在成为一个更相关和研究的领域，我个人对此非常感兴趣。MAB算法是这个迷人领域的第一步，我想你个人会对它们很感兴趣，而且很快也会变得专业。</p><h1 id="1229" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">代理人、奖励和环境</h1><p id="86b9" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在强化学习中，我们有一个代理(<em class="ms"> A </em>)，它必须在一个环境中执行<em class="ms">动作</em>。通过采取行动，代理获得<em class="ms">奖励</em>或受到处罚，随着时间的推移，确保最大<em class="ms">奖励</em>与特定<em class="ms">环境</em>中代理的最佳行为相关联。没有训练数据——代理通过反复试验和交互过程来学习。</p><p id="9569" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个领域中最简单的问题之一(多臂强盗问题的例子就是由此而来)是吃角子老虎。开始游戏需要支付一定的费用，玩家从一组<em class="ms"> n </em>选项中选择一个手柄即可获得奖励。游戏的目的是选择正确的杠杆，最大化总回报，这可以通过使用MAB算法来实现。</p><p id="bffe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用老虎机的相同概念和一个更简单的例子来解释MAB算法试图解决什么样的问题。</p><h1 id="2b0a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">一个例子</h1><p id="f99f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">假设我们有四个按钮可供选择，分别标为A、B、C和d。每个按钮按下时都会释放不同的奖励。</p><p id="e374" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还假设我们可以按四个按钮的次数是有限制的，比如说100次。并且每个按钮每次都给出相同的奖励:A总是给出1，B，2，C，3和D 4的奖励。</p><p id="4d23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最大化我们总回报的策略是什么？</p><p id="06d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以选择的两个最简单的策略是</p><p id="f978" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1.<strong class="lb iu">随机选择一个按钮，并坚持选择</strong>。这意味着我们只有25%的机会选择D并最大化我们的奖励，如果有更多的按钮，这个概率会更低</p><p id="ce12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.<strong class="lb iu">每次随机按键</strong>。这里的结果是每个按钮的平均结果乘以100。如果我们重复实验，我们的结果总是一样的，但永远不会是最好的。</p><p id="5193" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两种选择在所谓的<em class="ms">开采-勘探困境</em>中处于对立的两端。对于第一个，我们只做了一个决定，并坚持下去(只有<strong class="lb iu">利用了</strong>)。另一方面，我们只探索了<strong class="lb iu">和</strong>，而没有使用通过其他选择获得的洞察力。</p><p id="e5a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，我们必须在探索最佳选择和利用最佳选择之间取得平衡。</p><p id="c13f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有四种算法试图达到<em class="ms">开发-探索</em>的平衡:</p><ol class=""><li id="1bbc" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">ε-贪婪算法</li><li id="825a" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">乐观的初始值</li><li id="83a5" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">梯度bandit算法</li><li id="616e" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">置信上限(UCB)动作选择</li></ol><h1 id="41ee" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">1.<strong class="ak">ε</strong>-贪婪算法</h1><p id="0e9f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">继续上面的例子，我们有4个固定奖励的按钮。</p><p id="a349" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，策略很简单:每个按钮尝试一次，并一直选择奖励最高的按钮，直到结束。</p><p id="1366" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">四次探险和96次剥削。总回报是1+2+3+4*97=394，而最高分是400。理论最高分与回报之差称为<strong class="lb iu">后悔</strong>。</p><p id="a733" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设奖励不是固定的，但是每个按钮都关联着一个正态分布的奖励。你会如何在最大化回报的同时解决这个问题？</p><p id="cc68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ε-贪婪算法采用了这种解决方案:以概率按下奖励最高的按钮，选择一个概率为1-ε的随机按钮<em class="ms">。同时跟踪结果并计算每个选择的平均奖励。</em></p><blockquote class="nh ni nj"><p id="d96d" class="kz la ms lb b lc ld ju le lf lg jx lh nk lj lk ll nl ln lo lp nm lr ls lt lu im bi translated">1.将每个按钮的平均值设置为零<br/> 2。按下每个按钮一次，并记录其值。<br/> 3。继续到最后:<br/> a .用概率选择平均值(<strong class="lb iu">贪心</strong>)最高的按钮，否则任意一个按钮(注意，这个阶段甚至可以按下值最高的按钮)。<br/> b .更新与每个按钮相关的平均值。c .更新总回报</p></blockquote><p id="e0e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用这种方法，系统将很快知道哪个分布提供最高的平均报酬，并且它将选择它ε+1-ε/n次。</p><p id="44e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与每个行动相关的平均回报被称为<strong class="lb iu">行动值</strong>。</p><p id="248a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也有可能有一个随时间增加的灵活性，因此它只选择最佳选项，停止任何探索过程。</p><p id="e4a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，对于90%，我们示例中的最佳按钮选择为90+100–90/4 = 92.5%。过一会儿，这可能会增加到100%。</p><p id="2cd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，只有当分布不随时间变化时，这种策略才有意义，因为如果它们发生变化，最佳选择可能会改变。</p><h1 id="ff61" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">2.乐观初始值</h1><p id="56b3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在ε-greedy算法中，我们看到每个按钮的第一个值被认为是第一个平均值，它通常被设置为零。</p><p id="69b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最初的零值策略会给整个过程带来偏差的风险。我们看到，ε-贪婪算法在第一轮探索后立即开始选择最高平均值。但是，如果一个分布具有很大的标准差，并且它的第一个值与真实平均值相差很远，会发生什么呢？</p><p id="660c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，系统需要一段时间进行调整，如果这个具有较大标准差的假设分布的真实均值接近最高平均分布，调整会更加困难。</p><p id="39bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在达到我们可以按下按钮的最大时间之前，系统可能找不到最佳选项。这种对进程的限制被称为<strong class="lb iu">事件</strong>。</p><p id="3071" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了解决这个问题，我们可以将初始值设定为一个远大于我们的回报的值，这个值被称为<strong class="lb iu">乐观值</strong>，因为我们将初始值设定为远大于真正的强盗行动值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/a285f6d37af28d793ffbb0b27290f8b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iUc12AbdcJg_QmChKjQLIA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在该图中，我们可以看到乐观算法与ε-贪婪算法的对比示例。后者开始利用它发现的最佳行动，而乐观者在探索阶段花费更多的时间来更好地了解最佳行动</p></figure><p id="192f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">无论最初选择哪种行动，回报都小于我们的乐观值，因此得到的平均值将小于其他值。因此，在值估计收敛之前，所有动作都要尝试几次，并且该方法必须进行更长时间的探索，因为具有最高平均值的动作经常改变，并且需要很长时间来选择具有一致的更高值的动作。</p><h1 id="b72d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">3)置信上限动作选择</h1><p id="3e62" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">正如我们提到的，任何ε-贪婪算法都在当前时间步采取最佳行动，只考虑该点的最高平均值(行动值估计)。但是，它没有考虑估计的不确定性。</p><p id="173d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在开发和探索之间的权衡中，贪婪算法给出了一种选择非贪婪动作的方式，但是它的选择在贪婪和非贪婪动作之间是随机的。</p><p id="1db5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相反，置信上限动作选择(UCB)试图评估相对于其成为真正最优动作(面对不确定性时乐观)的潜力而言不具有最高值(非贪婪)的动作。这是通过以下公式实现的，该公式考虑了动作值估计和这些估计的不确定性:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/80ee43d6b6de41d56697b7acfc763989.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*zRAycqa7V9pYL5TxhF7TAQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd np">置信上限动作选择</strong>公式。对于Qt(a)，a的最大自变量加上超参数c，探索度时间t的自然对数的平方根除以在时间t之前已经选择的动作的次数。Qt(a)表示等式的<strong class="bd np"> <em class="nq">剥削</em> </strong>部分，选择的动作将是具有最高估计奖励的一个。等式的第二部分代表<strong class="bd np">探索</strong>方，这为行动值提供了不确定性的度量。[图片由作者提供]</p></figure><p id="50f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每次采取行动时，行动值估计值都会变得更准确(Qt(a)更接近真实值)。如果<em class="ms"> Nt(a) </em>增加，被选中用于勘探目的的概率会降低，尽管仍有可能被选中用于开发。对于未选择的动作，等式左侧的值缓慢增长，直到其中一个动作触发另一个探索动作。</p><p id="3468" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">UCB可以判断什么时候探索，什么时候开发，并且在寻找最佳行动方面相当快。由于这个原因，UCB是最有效的算法之一，我们将在实验部分看到。</p><h1 id="c5bf" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">4)梯度bandit算法</h1><p id="66a1" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">到目前为止，我们已经看到了基于动作值来估计要采取的动作的方法。还有其他可能的不同方法，其中之一是使用<strong class="lb iu">软最大分布</strong>(即吉布斯或玻尔兹曼分布)，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/38ca1263db2c44b20d42316be7e5b6be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IOVr-e1PTKfcWtpIj73qHQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="nq">渐变土匪公式(soft-max)更多信息</em> <a class="ae ky" href="https://deepai.org/machine-learning-glossary-and-terms/softmax-layer" rel="noopener ugc nofollow" target="_blank"> <em class="nq">此处</em> </a>。[图片由作者提供]</p></figure><p id="1738" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Soft-max函数计算所有动作的概率分布，我们可以选择得分最高的动作。</p><p id="30a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可能已经在神经网络领域中遇到过软最大分布的概念，因为它经常在输出层每个类有一个节点时使用。</p><p id="521d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要解释GBA背后的数学原理，可能需要另一篇比这篇文章更长的文章。我仍然选择把它放在这里，因为我想表明使用平均奖励值作为行动值并不是唯一可能的选择。</p><h1 id="5ac5" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">总结和测试</h1><p id="e857" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在这里，我们看到了最流行的多臂土匪问题的算法，探索不同的方法来探索与开发的困境:</p><ol class=""><li id="f4fd" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><strong class="lb iu">仅探索</strong>，它在每个时间步选择一个随机动作</li><li id="7181" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu">仅利用</strong>，第一次选择一个随机动作并重复</li><li id="f6da" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu">ε-贪婪</strong>方法，随机选择一小部分时间或者探索</li><li id="a7fb" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu">乐观</strong>，迫使系统有更长的探索初始阶段。</li><li id="acd6" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu"> UCB </strong>确定性地选择，但通过偏好接收到较少样本的动作来实现探索。</li><li id="6598" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu"> GBA </strong>是一种基于概率分布的完全不同的方法。</li></ol><p id="358a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如您可能已经发现的那样，没有一种方法对所有可能的场景都是最好的。UCB可能是使用最广泛的，但这并不能保证。</p><p id="7090" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，我认为最好用我给出的第一个例子来展示这些算法的行为，我已经对75个步骤进行了250次实验，这是结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/3054d7d5c8ca93e235af6e2765dbb430.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UUhlO8WBgy3K5EPaFdNyWQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="nq">我已经为epsilon选择了0.75的值作为每种方法的中间点。UCB的c是2，乐观的初始值是5。标准差为1。</em>【作者图片】</p></figure><p id="172e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GBA只能在25%的情况下找到最佳操作——这有点令人惊讶，所以在这种特定的场景中使用它可能是一种糟糕的算法。</p><p id="f156" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">表现最好的工具是UCB，它能在很短的时间内找到最佳动作，并一直跟踪到剧集结束。乐观算法利用早期探索阶段并找到t个最优动作，而贪婪算法需要更多的时间步骤。</p><p id="912a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">只有勘探和开采有25%的最佳行动被预测。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/4d1b5ead9bd207630dd5b494dc8bd475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qut77AkJ2xQr-bW7qcEW1A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">250个实验的每个时间步的平均回报。[图片由作者提供]</p></figure><p id="aeb1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，如果我们看每个时间步的平均奖励，UCB得到最高的奖励稳定。乐观一开始有个学习曲线，贪婪滞后。</p><h1 id="d848" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">把这一切都编码起来</h1><p id="1e34" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">MAB和这四种算法只是强化学习研究中一个漫长旅程的开始。</p><p id="e0c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为这是RL的一个复杂子领域，我决定创建一个最重要算法的python实现，你可以在这里找到:<a class="ae ky" href="https://github.com/MattiaCinelli/AlgoRL" rel="noopener ugc nofollow" target="_blank">https://github.com/MattiaCinelli/AlgoRL</a></p><p id="83c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我没有提到Thompson的采样(针对Bernoulli或Gaussian分布)或Bayesian方法，这些方法可以在代码中找到，我鼓励您尝试一下。</p><p id="6ef3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">欢迎大家参与并改进代码，如果您有任何问题，请随时联系我们。</p><h1 id="8865" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">来源</h1><ol class=""><li id="a1c1" class="mt mu it lb b lc mn lf mo li nu lm nv lq nw lu my mz na nb bi translated">强化学习:导论。莎顿和巴尔托。第二版。</li><li id="3767" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">米格尔·莫拉莱斯探索深度强化学习。</li></ol></div></div>    
</body>
</html>