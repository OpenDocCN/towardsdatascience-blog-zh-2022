<html>
<head>
<title>Meet HistGradientBoostingClassifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">认识histgradientsboostingclassifier</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/meet-histgradientboostingclassifier-54a9df60d066#2022-06-09">https://towardsdatascience.com/meet-histgradientboostingclassifier-54a9df60d066#2022-06-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ef94" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">计算机编程语言</h2><div class=""/><div class=""><h2 id="fbdf" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">更加灵活和可扩展的梯度增强分类器</h2></div><p id="62b7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Scikit-learn的<a class="ae ln" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifierhtml" rel="noopener ugc nofollow" target="_blank">GradientBoostingClassifier</a>(GBM从这里开始)是最受欢迎的集成算法之一，在许多数据集上表现良好。<a class="ae ln" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html" rel="noopener ugc nofollow" target="_blank">histgradientsboostingclassifier</a>(从这里开始的HGBM)，一个基于直方图的GBM替代实现，在<a class="ae ln" href="https://scikit-learn.org/stable/whats_new/v0.21.html#version-0-21-0" rel="noopener ugc nofollow" target="_blank"> v0.21.0 </a>中作为实验估计器引入。从<a class="ae ln" href="https://scikit-learn.org/stable/whats_new/v1.0.html#version-1-0-0" rel="noopener ugc nofollow" target="_blank"> v1.0.0 </a>开始，这个估计量已经成为一个稳定的估计量。在本帖中，我们将探讨使用HGBM相对于GBM的两个主要优势。</p><p id="2092" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="lo">还有回归等价:</em><a class="ae ln" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html" rel="noopener ugc nofollow" target="_blank"><em class="lo">histgradientsboostingregressor</em></a><em class="lo">。然而，由于同样的逻辑适用，为了避免重复，我们将不涉及它。</em></p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/377028a0cbfa752f55c568b89a4d8981.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yi9W8EIEKI4qo8Mz"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">由<a class="ae ln" href="https://unsplash.com/@sebastiansvenson?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">塞巴斯蒂安·斯文森</a>在<a class="ae ln" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="3cd8" class="mm mn it bd mo mp mq mr ms mt mu mv mw ki mx kj my kl mz km na ko nb kp nc nd bi translated">📦 1.处理丢失的数据</h1><p id="b27e" class="pw-post-body-paragraph kr ks it kt b ku ne kd kw kx nf kg kz la ng lc ld le nh lg lh li ni lk ll lm im bi translated">该估计器可以处理缺失数据，因为它内置了对缺失值的支持。让我们来看看实际情况。我们将从导入库开始，并创建一个带有缺失值的样本数据:</p><pre class="lq lr ls lt gt nj nk nl nm aw nn bi"><span id="4558" class="no mn it nk b gy np nq l nr ns">import numpy as np<br/>import pandas as pd<br/>from time import perf_counter<br/>pd.options.display.max_columns = 6</span><span id="2e11" class="no mn it nk b gy nt nq l nr ns">from sklearn.datasets import make_classification<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.ensemble import (GradientBoostingClassifier, <br/>                              HistGradientBoostingClassifier)<br/>from sklearn.metrics import accuracy_score, roc_auc_score, f1_score</span><span id="4075" class="no mn it nk b gy nt nq l nr ns">import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>sns.set(style='darkgrid', context='talk', palette='rainbow')</span><span id="dfdd" class="no mn it nk b gy nt nq l nr ns">n = 10**4<br/>X, y = make_classification(n, random_state=42)<br/>X = pd.DataFrame(X, columns=[f'feature{i}' for i in range(X.shape[1])])</span><span id="d98a" class="no mn it nk b gy nt nq l nr ns"># Randomly add missing data for all columns<br/>for i, col in enumerate(X.columns):<br/>    np.random.seed(i)<br/>    X.loc[np.random.choice(range(n), 1000, replace=False), col] = np.nan</span><span id="c9aa" class="no mn it nk b gy nt nq l nr ns">print(f"Target shape: {y.shape}")<br/>print(f"Features shape: {X.shape}")<br/>X.head()</span></pre><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/087f6242059e486445f165cef3af902d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*4OV61rGeEM1CsU13icN5sg.png"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">作者图片</p></figure><p id="752a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们现在将对数据进行分区，并尝试适应GBM:</p><pre class="lq lr ls lt gt nj nk nl nm aw nn bi"><span id="b42d" class="no mn it nk b gy np nq l nr ns">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)<br/>print("========== Training data ========== ")<br/>print(f"Features: {X_train.shape} | Target:{y_train.shape}")<br/>print("========== Test data ========== ")<br/>print(f"Features: {X_test.shape} | Target:{y_test.shape}")</span><span id="124b" class="no mn it nk b gy nt nq l nr ns">gbm = GradientBoostingClassifier(random_state=42)<br/>gbm.fit(X_train, y_train)<br/>gbm.score(X_test, y_test)</span></pre><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/09bdcf62bd0457dfe1453c7c0d1c75f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*aSGDemliu4ouQqKBoK55lg.png"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">作者图片|部分输出</p></figure><p id="d588" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">像大多数Scikit-learn的估计器一样，试图用缺失值来拟合模型会触发<code class="fe nw nx ny nk b">ValueError: Input contains NaN, infinity or a value too large for dtype('float32')</code>。</p><p id="3b1b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，让我们看看如果我们使用HGBM会发生什么:</p><pre class="lq lr ls lt gt nj nk nl nm aw nn bi"><span id="47b8" class="no mn it nk b gy np nq l nr ns">hgbm = HistGradientBoostingClassifier(random_state=42)<br/>hgbm.fit(X_train, y_train)<br/>hgbm.score(X_test, y_test)</span></pre><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/77c08f9c5aac5f8a33fa93aa67cf2abc.png" data-original-src="https://miro.medium.com/v2/resize:fit:160/format:webp/1*VUbaL9n5hlsnpsopjEObeQ.png"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">作者图片</p></figure><p id="5111" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">太棒了，这非常有效，因为估计器本身就可以处理缺失数据。这是HGBM相对于GBM的一个优势。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="d825" class="mm mn it bd mo mp mq mr ms mt mu mv mw ki mx kj my kl mz km na ko nb kp nc nd bi translated">📊 2.能够很好地适应更大的数据</h1><p id="e4d9" class="pw-post-body-paragraph kr ks it kt b ku ne kd kw kx nf kg kz la ng lc ld le nh lg lh li ni lk ll lm im bi translated">HGBM是一种实现速度更快的GBM，可以很好地适应更大的数据集。让我们来看看这两种估计量如何对不同大小的样本数据进行比较:</p><pre class="lq lr ls lt gt nj nk nl nm aw nn bi"><span id="f6a4" class="no mn it nk b gy np nq l nr ns">n_samples = 10**np.arange(2,7)<br/>tuples = [*zip(np.repeat(n_samples,2), np.tile(['gbm', 'hgbm'], 2))]<br/>summary = pd.DataFrame(<br/>    index=pd.MultiIndex.from_tuples(tuples, <br/>                                    names=["n_records", "model"])<br/>)</span><span id="e7ce" class="no mn it nk b gy nt nq l nr ns">models = [('gbm', GradientBoostingClassifier(random_state=42)), <br/>          ('hgbm', HistGradientBoostingClassifier(random_state=42))]</span><span id="9d1f" class="no mn it nk b gy nt nq l nr ns">for n in n_samples:<br/>    X, y = make_classification(n, random_state=42)<br/>    X_train, X_test, y_train, y_test = train_test_split(<br/>        X, y, random_state=42<br/>    )<br/>    <br/>    for name, model in models:<br/>        start = perf_counter()<br/>        model.fit(X_train, y_train)<br/>        end = perf_counter()<br/>        summary.loc[(n, name), 'fit_time'] = end-start</span><span id="5c0d" class="no mn it nk b gy nt nq l nr ns">start = perf_counter()<br/>        y_proba = model.predict_proba(X_test)[:,1]<br/>        end = perf_counter()<br/>        summary.loc[(n, name), 'score_time'] = end-start<br/>        <br/>        summary.loc[(n, name), 'roc_auc'] = roc_auc_score(y_test, <br/>                                                          y_proba)<br/>        y_pred = np.round(y_proba)<br/>        summary.loc[(n, name), 'accuracy'] = accuracy_score(y_test, <br/>                                                            y_pred)<br/>        summary.loc[(n, name), 'f1'] = f1_score(y_test, y_pred)<br/>summary</span></pre><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/4044c31cbfdf95050e935c7e4b4c7875.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*BVXD6KnduABintMjYa3FPA.png"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">作者图片</p></figure><p id="2820" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这里，总记录的75%用于训练，剩余的25%用于测试。我们可以看到，随着训练数据的增长，HGBM的训练时间要快得多。数据越大，HGBM的速度就越令人印象深刻。HGBM通过将数据粗化为分箱的要素来实现其惊人的速度。让我们更仔细地看一下摘要:</p><pre class="lq lr ls lt gt nj nk nl nm aw nn bi"><span id="3662" class="no mn it nk b gy np nq l nr ns">fig, ax = plt.subplots(2, 1, figsize=(9,6), sharex=True)<br/>sns.lineplot(data=summary['fit_time'].reset_index(), <br/>             x='n_records', y='fit_time', hue='model', ax=ax[0])<br/>ax[0].legend(loc='upper right', bbox_to_anchor=(1.3, 1))<br/>sns.lineplot(data=summary['score_time'].reset_index(), <br/>             x='n_records', y='score_time', hue='model', <br/>             legend=False, ax=ax[1])<br/>ax[1].set_xscale('log')<br/>fig.tight_layout();</span></pre><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/90e99f01f0b7d6bfb2f0ed0fb39d7482.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*l7pRfmMAOs2sElS2LSKWAQ.png"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">作者图片</p></figure><p id="b012" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">随着训练样本数量的增加，GBM的训练时间显著增加，而h GBM在更大的数据集上仍然相对较快。两者的得分时间非常接近。</p><pre class="lq lr ls lt gt nj nk nl nm aw nn bi"><span id="932f" class="no mn it nk b gy np nq l nr ns">fig, ax = plt.subplots(3, 1, figsize=(9,9), sharex=True)<br/>sns.lineplot(data=summary['roc_auc'].reset_index(), <br/>             x='n_records', y='roc_auc', hue='model', ax=ax[0])<br/>ax[0].legend(loc='upper right', bbox_to_anchor=(1.3, 1))<br/>sns.lineplot(data=summary['accuracy'].reset_index(), <br/>             x='n_records', y='accuracy', hue='model', <br/>             legend=False, ax=ax[1])<br/>sns.lineplot(data=summary['f1'].reset_index(), <br/>             x='n_records', y='f1', hue='model', <br/>             legend=False, ax=ax[2])<br/>ax[2].set_xscale('log')</span><span id="5292" class="no mn it nk b gy nt nq l nr ns">fig.tight_layout();</span></pre><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/2beeb313fe1c840174623bb203475d10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*kIJRTSCFWaoRionV9yC8Jg.png"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">作者图片</p></figure><p id="1b2d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">总的来说，两者之间的预测性能非常相似，尽管当训练数据在75和750处较小时有一点差异。</p><p id="62d7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">正如您现在所知，HGBM的第二个好处是，与GBM相比，它可以很好地扩展大数据集。</p><p id="3e2f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这就是这篇文章的全部内容！希望您喜欢了解这个灵活且可扩展的估计器，并乐于进一步探索它。如果您想了解更多，HGBM也有对分类特性的本地支持。<a class="ae ln" href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_categorical.html" rel="noopener ugc nofollow" target="_blank">本文档</a>展示了一些关于此功能的优秀示例。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi oc"><img src="../Images/9e3475cf5a93af88bc18727083c31441.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sGJGmfwKmh6GGjRj"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">由<a class="ae ln" href="https://unsplash.com/@fakurian?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">米拉德·法库里安</a>在<a class="ae ln" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="a3be" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="lo">您想访问更多这样的内容吗？媒体会员可以无限制地访问媒体上的任何文章。如果你使用</em> <a class="ae ln" href="https://zluvsand.medium.com/membership" rel="noopener"> <em class="lo">我的推荐链接</em> </a>，<em class="lo">成为会员，你的一部分会费会直接去支持我。</em></p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="fd43" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">感谢您阅读这篇文章。如果你感兴趣，这里有我的一些其他帖子的链接:<br/> ◼️️ <a class="ae ln" rel="noopener" target="_blank" href="/from-ml-model-to-ml-pipeline-9f95c32c6512">从ML模型到ML管道</a> <br/> ◼️️ <a class="ae ln" rel="noopener" target="_blank" href="/explaining-scikit-learn-models-with-shap-61daff21b12a">解释scikit-用SHAP </a> <br/> ◼️️ <a class="ae ln" rel="noopener" target="_blank" href="/4-simple-tips-for-plotting-multiple-graphs-in-python-38df2112965c"> 4个在Python中绘制多个图形的简单技巧</a> <br/> ◼️ <a class="ae ln" rel="noopener" target="_blank" href="/prettifying-pandas-dataframes-75c1a1a6877d">美化熊猫数据帧</a><br/>◼<a class="ae ln" rel="noopener" target="_blank" href="/simple-data-visualisations-in-python-that-you-will-find-useful-5e42c92df51e">python中的简单数据可视化你会发现有用的</a> ️ <br/> ◼️ <a class="ae ln" rel="noopener" target="_blank" href="/6-simple-tips-for-prettier-and-customised-plots-in-seaborn-python-22f02ecc2393"> 6个简单技巧</a></p><p id="ef2f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">再见🏃 💨</p></div></div>    
</body>
</html>