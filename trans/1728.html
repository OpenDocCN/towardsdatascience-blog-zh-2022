<html>
<head>
<title>Intergenerational Mobility in the US — Data Preprocessing (2/5)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">美国的代际流动——数据预处理(2/5)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/intergenerational-mobility-in-the-us-data-preprocessing-2-5-829ace38df41#2022-04-22">https://towardsdatascience.com/intergenerational-mobility-in-the-us-data-preprocessing-2-5-829ace38df41#2022-04-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><blockquote class="jn jo jp"><p id="893f" class="jq jr js jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ij bi translated">本系列由Ibukun Aribilola和Valdrin Jonuzi共同撰写，是社会公益数据科学教程的一部分。</p></blockquote><p id="b5d9" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">在跨越了数据收集的障碍之后，我们得到了一系列不同形式的数据，这些数据的原始形式不适合我们的预期分析。为了解决这个问题，我们将合并数据集并解决丢失数据的问题。因为我们所有的数据集都有县级数据，所以我们将根据县id进行合并。</p><h1 id="c7cf" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">合并单个数据集</h1><p id="f237" class="pw-post-body-paragraph jq jr iq jt b ju lq jw jx jy lr ka kb kp ls ke kf kq lt ki kj kr lu km kn ko ij bi translated">这个项目的数据合并可以分为三个阶段。我们从合并单个机会图谱数据集开始，以创建一个大的机会图谱数据集。然后，我们将Opportunity Atlas数据集与Data Commons数据集合并，后者又与Chetty &amp; Hendren数据集合并。</p><h1 id="6af0" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">合并机会图谱数据集</h1><p id="7533" class="pw-post-body-paragraph jq jr iq jt b ju lq jw jx jy lr ka kb kp ls ke kf kq lt ki kj kr lu km kn ko ij bi translated">在“收集数据”部分，我们描述了如何从机会地图集下载七个特征的县和区域级数据。正如在“数据预处理”一节的介绍中提到的，我们将只使用县级数据，因此我们将在这一节中介绍七个县级数据集的合并。</p><p id="8a74" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">首先，我们需要导入所有必要的库，然后我们将各个数据集从Google Drive加载到Python笔记本中。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="0732" class="me kt iq ma b gy mf mg l mh mi">import pandas as pd<br/>import functools</span><span id="07ac" class="me kt iq ma b gy mj mg l mh mi"><em class="js"># County-level data</em><br/><em class="js">## Household income at age 35</em><br/>c_household_income = pd.read_csv('https://drive.google.com/uc?id=1dRLKhKqBjs2ARcynSUT99Y8CoJPoC5IZ')<br/><em class="js">## Incerceration rate</em><br/>c_jail_rate = pd.read_csv('https://drive.google.com/uc?id=14K-QJ_bZi8Dtvod9J_s72tLE8TpWf25T')<br/><em class="js">## Fraction married at age 35</em><br/>c_married = pd.read_csv('https://drive.google.com/uc?id=1MY_ulJnGFSBotgDtbf4dSYRR_SqW5rNq')<br/><em class="js">## Employment rate at age 35</em><br/>c_working = pd.read_csv('https://drive.google.com/uc?id=1sZxD3OqA0IqbiDn1L22HXHfFE3J_Q6yX')<br/><em class="js">## High school graduation rate</em><br/>c_hs = pd.read_csv('https://drive.google.com/uc?id=1cOFWFjPNyYn-dBdBH5s8PZ7V8B1ZNpC9')<br/><em class="js">## College graduation rate</em><br/>c_coll = pd.read_csv('https://drive.google.com/uc?id=1CquuVRG_c-7wgW6P7yjXL6uUxt3VBb2v')<br/><em class="js">## Poverty rate in 2012-16</em><br/>c_poor = pd.read_csv('https://drive.google.com/uc?id=166vtYSczyKAHMXP1qbjaMZBNIZlO3DjB')</span></pre><p id="65e0" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">当我们使用<code class="fe mk ml mm ma b">df.shape</code>命令检查数据集的形状时，我们发现它们有218行，其中两行是县ID和名称。剩余的216个是6个父母收入百分位数水平、6个种族亚组、3个儿童性别亚组和2个儿童群组亚组的组合(6×6×3×2 = 216)。</p><p id="05d8" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">我们还注意到,“c_poor”数据集中的“county name”列是以标题“name”的形式书写的，而其他数据集中的列名是以小写“Name”标记的。因为Python是一种区分大小写的语言，所以我们希望确保所有数据集中的county name列的标签都是相似的，所以我们只需更改列名。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="923c" class="me kt iq ma b gy mf mg l mh mi">c_poor = c_poor.rename(columns = {"Name": "name"})</span></pre><p id="1cde" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">现在，所有的数据集都可以使用Pandas的“合并”功能进行合并了。我们将合并县名和ID上的数据集，以便大型数据集将包含唯一的县名和ID(没有重复)。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="9bac" class="me kt iq ma b gy mf mg l mh mi"><em class="js"># Make a list of all the datasets that need to be merged</em><br/>c_datasets = [c_household_income,<br/>        c_jail_rate,<br/>        c_married,<br/>        c_working,<br/>        c_hs,<br/>        c_coll,<br/>        c_poor]</span><span id="71de" class="me kt iq ma b gy mj mg l mh mi"><em class="js"># Batch merging the datasets into one</em><br/>oa_county_dataset = functools.reduce(lambda  left, right: <br/>                        pd.merge(left, right, on = ["cty", "name"], how = "outer"), c_datasets)</span></pre><p id="b3a7" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">现在可以将合并的数据集导出到您选择的文件夹中。(点击<a class="ae mn" href="https://share.streamlit.io/ibukunlola/dssg_final_project/main/finalised_notebooks/DataCleaning/data_preprocessing.py" rel="noopener ugc nofollow" target="_blank">此处</a>查看完整的牌桌。)</p><p id="9e58" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">合并后的数据集包含多达1300列，这意味着可能有太多我们不需要的变量。快速检查显示，大多数列是不同收入百分位数的居民的数据条目和码本中未描述的“晚期群体”的数据条目。因为我们想要删除的变量比保留的要多，所以我们可以列出想要保留的变量。对于这个项目，我们感兴趣的是整个样本、种族和性别分组的收入、结婚率、就业率、高中毕业率、大学毕业率和贫困率。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="d34c" class="me kt iq ma b gy mf mg l mh mi">oa_data.info()<br/><em class="js"># the output of the cell shows that there are 1300 columns</em></span></pre><p id="7f25" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">我们使用<code class="fe mk ml mm ma b">df.columns.difference</code>命令来选择opportunity atlas数据集的所有列，通过列出我们想要保留的列来删除这些列。因此，如果你想选择一组不同的变量来进行分析，在这一阶段必须将它们列出来。然后，我们使用<code class="fe mk ml mm ma b">df.drop</code>命令删除那些不需要的变量。</p><h1 id="cc3e" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">数据共享空间+机会图谱</h1><p id="07b7" class="pw-post-body-paragraph jq jr iq jt b ju lq jw jx jy lr ka kb kp ls ke kf kq lt ki kj kr lu km kn ko ij bi translated">在为Opportunity Atlas数据创建单个数据集并对其进行修整以仅包含我们进行分析所需的变量之后，我们将把它与Data Commons数据合并。两个数据集都有一个专用于县ID的列，即Opportunity Atlas数据集中的<code class="fe mk ml mm ma b">cty</code>列和Data Commons数据集中的“地点”列。我们希望将跨数据集的每个县的属性合并到一个数据集，因此我们将基于县id进行合并。为此，两个县ID列必须采用相同的格式。目前，县Id以不同的格式书写，因为它们以不同的字母为前缀，即“ctyxxxxx”和“geoId/xxxxx”。我们可以通过分割每个字符串并删除前缀来确保县id具有相同的格式。我们还将每个数据集的县ID变量重命名为“geo_id ”,以确保变量名称在数据集之间保持一致。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="139d" class="me kt iq ma b gy mf mg l mh mi"><em class="js"># slice the elements of the 'cty' and 'place' columns in oa_data and dc_data, respectively</em><br/><em class="js">## the goal is to isolate the geo IDs of each county</em><br/>cty_new = []<br/>place_new = []<br/>for i in oa_data['cty']:<br/>  cty_new.append(i[3:])</span><span id="c9dc" class="me kt iq ma b gy mj mg l mh mi">for j in dc_data['place']:<br/>  place_new.append(j[6:])</span><span id="7288" class="me kt iq ma b gy mj mg l mh mi"><em class="js"># replace the geo ID columns without surrounding text</em><br/>oa_data['cty'] = cty_new<br/>dc_data['place'] = place_new<br/>oa_data = oa_data.rename(columns={"cty":"geo_id"})<br/>dc_data = dc_data.rename(columns={"place":"geo_id"})</span></pre><p id="b17f" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">有了一致的县ID格式，我们就可以使用<code class="fe mk ml mm ma b">pd.merge</code>命令合并Opportunity Atlas和Data commons数据集了。我们将对<code class="fe mk ml mm ma b">geo_id</code>列进行外部合并，合并两个数据集中的县id，这样两个数据集中表示的每个县在合并的数据集中只出现一次。查看<a class="ae mn" href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html" rel="noopener ugc nofollow" target="_blank"> pd.merge文档</a>以了解合并数据集的不同方法。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="27fa" class="me kt iq ma b gy mf mg l mh mi"><em class="js"># merge the datasets</em><br/>merged_data = pd.merge(oa_data, dc_data, how="outer", on=["geo_id", "geo_id"])<br/>merged_data = merged_data.drop(columns="name_y")<br/>merged_data = merged_data.rename(columns = {"name_x": "name"})<br/>merged_data.geo_id = merged_data.geo_id.astype(str)<br/>merged_data</span></pre><p id="fbea" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">(点击<a class="ae mn" href="https://share.streamlit.io/ibukunlola/dssg_final_project/main/finalised_notebooks/DataCleaning/data_preprocessing.py" rel="noopener ugc nofollow" target="_blank">此处</a>查看完整表格。)</p><h1 id="f417" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">Chetty &amp; Hendren +数据共享+机会图谱</h1><p id="f472" class="pw-post-body-paragraph jq jr iq jt b ju lq jw jx jy lr ka kb kp ls ke kf kq lt ki kj kr lu km kn ko ij bi translated">合并过程的最后一步是将Chetty &amp; Hendren的数据集与之前合并的数据共享空间和机会地图集数据集合并。为了做到这一点，我们进一步削减了每个数据集中的变量列表。例如，我们放弃了所有与永久居民代际流动相关的变量，因为我们对永久居民和流动者之间的区别不感兴趣。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="4289" class="me kt iq ma b gy mf mg l mh mi">IGM = pd.read_stata("../data/raw/online_table4.dta")<br/>df = pd.read_csv("../data/processed/merged_df.csv")</span><span id="ea61" class="me kt iq ma b gy mj mg l mh mi"><em class="js"># Remove permanent resident intergenerational mobility from Chetty's 2014 </em><br/><em class="js"># because we aren't interested in the permanent residents vs movers distinction</em><br/>perm_res_IGM = list(filter(re.compile("perm_res_.*").match, IGM.columns))<br/>IGM = IGM.drop(columns=["csa", "csa_name", "cbsa", "cbsa_name", "intersects_msa"]+ perm_res_IGM)</span></pre><p id="ff1f" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">我们还从合并的(数据共享+机会地图集)数据集中删除了一些变量，如所有种族和性别子群的县数据。</p><p id="0ca2" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">最后一个预合并步骤是确保县ID列在数据集之间的命名一致。如果列名不一致，我们只需重命名其中一个以匹配另一个。</p><p id="013e" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">现在，数据集已准备好进行合并和导出。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="58eb" class="me kt iq ma b gy mf mg l mh mi">df_subset.geo_id = df_subset.geo_id.astype(int)<br/>df_subset.rename(columns={"geo_id":"cty2000"}, inplace=True)</span><span id="abdb" class="me kt iq ma b gy mj mg l mh mi">merged_data = pd.merge(IGM, df_subset, on=["cty2000", "cty2000"])<br/>merged_data = merged_data.drop(columns=['name'])</span><span id="e396" class="me kt iq ma b gy mj mg l mh mi">merged_data</span></pre><p id="6dbf" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">(点击<a class="ae mn" href="https://share.streamlit.io/ibukunlola/dssg_final_project/main/finalised_notebooks/DataCleaning/data_preprocessing.py" rel="noopener ugc nofollow" target="_blank">此处</a>查看完整表格。)</p><h1 id="e70f" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">电报密码本</h1><p id="8619" class="pw-post-body-paragraph jq jr iq jt b ju lq jw jx jy lr ka kb kp ls ke kf kq lt ki kj kr lu km kn ko ij bi translated">这是一条漫长的道路，但我们最终获得了一个全面的数据集，并很高兴开始分析。然而，我们必须创建一个数据码本，定义数据集中的每个变量，并使浏览数据集的任何人都容易理解每个变量的含义。</p><p id="2376" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">当构造码本时，有不同的格式可供选择。例如，<a class="ae mn" href="http://www.equality-of-opportunity.org/data/neighborhoods/online_table4.pdf" rel="noopener ugc nofollow" target="_blank"> Chetty </a>混合使用了第1-3页的列表式变量描述和第4-5页的表格格式。<a class="ae mn" href="https://github.com/valdrinj/dssg_final_project/blob/main/data/processed/codebook.csv" rel="noopener ugc nofollow" target="_blank">这里的</a>是我们在这个分析中使用的数据集的码本。</p><h1 id="f912" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">因变量选择</h1><p id="3093" class="pw-post-body-paragraph jq jr iq jt b ju lq jw jx jy lr ka kb kp ls ke kf kq lt ki kj kr lu km kn ko ij bi translated">当前的数据集包含一个结果变量的洗衣清单，所有这些我们都不一定感兴趣。因此，我们去掉了所有变量，只有一个除外——搬到一个县对26岁时父母收入排在第75百分位的孩子的收入排名的因果影响(“因果_p75_cty_kr26”)。虽然这是我们分析感兴趣的结果变量，但还有其他有趣的结果变量值得探索，所以不要犹豫，进一步探索数据吧！</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="e332" class="me kt iq ma b gy mf mg l mh mi">IGM_df = pd.read_pickle("../data/processed/IGM_merged_df.pkl")</span><span id="6bbc" class="me kt iq ma b gy mj mg l mh mi"><em class="js"># Drop irrelevant columns</em><br/>other_causal = list(filter(re.compile("causal_.*").match, IGM_df.columns))<br/>perm_res = list(filter(re.compile("perm_.*").match, IGM_df.columns))<br/>df = IGM_df.drop(columns=other_causal+perm_res+ ['cty1990', 'cz_name', 'cz_pop2000', 'csa', 'csa_name', 'stateabbrv',<br/>       'cbsa', 'cbsa_name', 'intersects_msa'])</span><span id="92ab" class="me kt iq ma b gy mj mg l mh mi"><em class="js"># Select the outcome variable</em><br/>pred = "causal_p75_cty_kr26"</span><span id="eb81" class="me kt iq ma b gy mj mg l mh mi">pred_df = df.copy() <br/>pred_df.insert(0, pred, IGM_df[pred])</span></pre><p id="f04b" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">另一个数据预处理最佳实践是指定丢失数据问题的处理方式。在该数据集中，我们以分层格式处理缺失值。如果缺少与某个县相关的特定值，我们将输入通勤区域中值。如果通勤区中间值不存在，我们插入州中间值，如果州中间值为空，我们输入国家中间值。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="af3c" class="me kt iq ma b gy mf mg l mh mi">df_cz_median = df.groupby("cz").median()<br/>df_state_median = df.groupby("state_id").median()<br/>df_country_median = df.median()</span><span id="bf0e" class="me kt iq ma b gy mj mg l mh mi"><em class="js"># Impute by commuting zone median</em><br/>for col in df.columns[np.where(df.isna().sum()&gt;0)]:<br/>    df[col] = df.apply(<br/>        lambda row: df_cz_median.loc[row['cz']][col] if np.isnan(row[col]) else row[col],<br/>        axis=1<br/>    )</span><span id="d1f4" class="me kt iq ma b gy mj mg l mh mi"><em class="js"># Impute by state median</em><br/>for col in df.columns[np.where(df.isna().sum()&gt;0)]:<br/>    df[col] = df.apply(<br/>        lambda row: df_state_median.loc[row['state_id']][col] if np.isnan(row[col]) else row[col],<br/>        axis=1<br/>    )</span><span id="6d66" class="me kt iq ma b gy mj mg l mh mi"><em class="js"># Impute with country median</em><br/>for col in df.columns[np.where(df.isna().sum()&gt;0)]:<br/>    df[col] = df.apply(<br/>        lambda row: df_country_median[col] if np.isnan(row[col]) else row[col],<br/>        axis=1<br/>    )</span></pre><p id="b1f2" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">替换NA值后，3221行中仍有767行缺少结果变量值，因此我们将它们与其他不必要的列一起删除，并导出清理后的数据集。</p><h1 id="12b0" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">结论</h1><p id="37d0" class="pw-post-body-paragraph jq jr iq jt b ju lq jw jx jy lr ka kb kp ls ke kf kq lt ki kj kr lu km kn ko ij bi translated">从变量选择到数据清理，许多必要的工作都是为了准备用于数据分析的数据集，每个步骤都必须仔细完成并记录在案，以便于与受众顺利沟通。在下一篇文章中，我们将解释如何执行探索性数据分析来理解数据，并将其用作对数据进行头脑风暴分析的工具。</p></div></div>    
</body>
</html>