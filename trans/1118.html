<html>
<head>
<title>Creating Word Embeddings with Jax</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Jax 创建单词嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-word-embeddings-with-jax-c9f144901472#2022-03-22">https://towardsdatascience.com/creating-word-embeddings-with-jax-c9f144901472#2022-03-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="12d9" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在 Jax 中实现经典的自然语言处理算法 Word2Vec</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b0fda523c17f49e470102514c5e15beb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mR7Y7IRBnbhsV1UR"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">克里斯托夫·高尔在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="fd52" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">深度学习中的一些新想法比其他想法更有影响力，这是众所周知的。Word2Vec 是这些极具影响力的想法之一，因为它给 NLP word 带来了变化，之后又给复杂网络世界带来了变化，成为了 Node2Vec 和 Deep Walk 等其他算法的基础。</p><p id="148e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Word2Vec [1]于 2013 年发布，是一种用于从文本语料库(或向量的节点)中生成嵌入的算法。它变得有影响力，因为它允许 NLP 任务考虑单词的关系，而不仅仅是语料库的单个标记，从而生成更健壮的模型。</p><p id="5758" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我们将对 Word2Vec 进行一些探索，并使用 Jax 库实现它。如果你有兴趣了解更多关于 Jax 库的知识，请参考我的 Jax 介绍<a class="ae kv" rel="noopener" target="_blank" href="/jax-numpy-on-gpus-and-tpus-9509237d9194">这里</a>。</p><p id="3c67" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">包含这篇文章代码的笔记本可以在 Github 和 Kaggle 上找到。</p><h2 id="910c" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">介绍 Node2Vec</h2><p id="3a46" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">Node2Vec 是一种深度学习算法，旨在从单词(或节点)中生成嵌入。但是你可能会问，什么是嵌入，嵌入是这个世界在 d 维空间的向量表示。</p><p id="4fe7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个想法是，我们希望经常一起出现并且与相同意思相关的单词在这个空间上应该更近(它们的向量之间的距离应该小)，而与它们没有任何关系的单词应该更远。</p><p id="d44d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">拥有这种嵌入表示是有用的，因为它允许我们用这些词进行一些计算，还允许我们在其他机器学习算法上使用这些嵌入来进行预测。</p><p id="87d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文提出了 Word2Vec 的两种架构，如下图所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/ed94c5962b329eb030ca23a7bd3e2c72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*Ixkb6puCdJiw02nizfpwgg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Node2Vec 架构。来自[1]</p></figure><p id="39b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在 CBOW 架构中，我们将每个单词的上下文(即短语中该单词的相邻单词)插入到神经网络中，并尝试预测目标单词。</p><p id="018e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，如果短语是“女王非常强大”，而我们试图预测的单词是“是”，那么单词“the”、“queen”、“very”、“powerful”将是网络的输入。</p><p id="6ee2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，这一过程是在我们的数据集的短语上的滑动窗口中发生的，被视为上下文的单词的数量是一个可配置的参数。</p><p id="a750" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于跳格结构，我们做相反的事情。我们在网络中插入目标单词，并试图预测该单词的上下文。所以在这个例子中，在我们插入“is”之前，试着预测其他单词的。</p><h2 id="ae78" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">如何训练 Node2Vec</h2><p id="1766" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">这可能会引起混淆，因为我们在这里试图预测单词。毕竟，我们对生成嵌入不感兴趣？确实如此。这里不使用预测任务，因为这不是我们的目标。然而，我们使用梯度下降法进行这些预测来训练我们的网络。</p><p id="66ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">梯度下降法将改善每个时期的嵌入(希望如此)。</p><p id="b379" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，请注意，预测的良好准确性并不是生成嵌入的必要条件。</p><p id="fa7e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，在每个时期，预测词与预期词进行比较，生成一个损失函数，我们用它来更新网络的权重。</p><p id="3a6d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们正在寻找的嵌入将是我们网络的投影层的权重。</p><h2 id="a268" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">履行</h2><p id="e6aa" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们现在将实现 Word2Vec。为此，我们将使用 Jax 框架，并将重点放在 CBOW 架构上。</p><p id="bc77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些是您将需要的库和方法:</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="8d26" class="ls lt iq ms b gy mw mx l my mz">import numpy as np</span><span id="1776" class="ls lt iq ms b gy na mx l my mz">import jax.numpy as jnp</span><span id="9858" class="ls lt iq ms b gy na mx l my mz">from jax import grad, jit, vmap</span><span id="f32b" class="ls lt iq ms b gy na mx l my mz">from jax import random</span><span id="1b3b" class="ls lt iq ms b gy na mx l my mz">from jax.nn import softmax, one_hot</span></pre><p id="1cb5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，让我们获取我们将用于该任务的数据。我们将使用 wikitext-2-v1 [2]数据集，该数据集可以免费下载<a class="ae kv" href="https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/" rel="noopener ugc nofollow" target="_blank">到这里</a>。</p><p id="0da5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下载完数据集后，我们会做一些非常基础的处理。请注意，由于这不是本任务的重点，因此不包括停用词的处理和删除以及一些常见的 NLP 任务。</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="0e11" class="ls lt iq ms b gy mw mx l my mz">with open('wiki.train.tokens') as fp:</span><span id="9863" class="ls lt iq ms b gy na mx l my mz">   corpus = fp.read()</span><span id="77ec" class="ls lt iq ms b gy na mx l my mz">corpus = corpus.replace('=', '').replace('@', '').replace('.', '').replace(',', '').replace('(', '').replace(')', '').replace('"', '')</span><span id="408e" class="ls lt iq ms b gy na mx l my mz">corpus = corpus.split('\n')</span><span id="c2fd" class="ls lt iq ms b gy na mx l my mz">corpus = [i for i in corpus if len(i.split(' ')) &gt; 5]</span></pre><p id="06bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这里，我们读取数据，用空格替换一些标记，然后通过用分隔线分割数据集来生成语料库。最后，我们删除少于 5 个单词的每个短语。</p><p id="8774" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们创建一些函数来帮助我们处理这些数据。首先，让我们标记我们的语料库:</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="f6ef" class="ls lt iq ms b gy mw mx l my mz">def tokenize_corpus(corpus):</span><span id="8e18" class="ls lt iq ms b gy na mx l my mz">    tokens = [x.split() for x in corpus]</span><span id="542c" class="ls lt iq ms b gy na mx l my mz">    return tokens</span></pre><p id="7dfa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还需要一个函数来创建我们的标记化语料库的词汇:</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="bce1" class="ls lt iq ms b gy mw mx l my mz">def create_vocabulary(tokenized_corpus):<br/>    vocab = {}<br/>    vocab['&lt;PAD&gt;'] = 0<br/>    i = 1<br/>    for sentence in tokenized_corpus:<br/>        for word in sentence:<br/>            if word in vocab:<br/>                continue<br/>            vocab[word] = i<br/>            i += 1<br/>            <br/>    return vocab</span></pre><p id="d73e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个函数中，我们为语料库中的每个单词设置一个整数。我们还创建了单词“<pad>”，当窗口大小没有包含我们需要的所有单词时，我们将使用它来填充训练数据。</pad></p><p id="e165" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们使用现有的语料库和刚刚创建的词汇来生成我们的训练数据:</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="cd86" class="ls lt iq ms b gy mw mx l my mz">def generate_train_data(tokenized_corpus, vocab, window_size):<br/>    X = []<br/>    y = []<br/>    for phrase in tokenized_corpus:<br/>        for i in range(len(phrase)):<br/>            target = i<br/>            context = []</span><span id="7be3" class="ls lt iq ms b gy na mx l my mz">for j in range(-window_size, window_size + 1):<br/>                if j == 0:<br/>                    continue<br/>                try:<br/>                    if i + j &lt; 0:<br/>                        context.append('&lt;PAD&gt;')<br/>                        continue<br/>                    context.append(phrase[i + j])<br/>                except Exception:<br/>                    context.append('&lt;PAD&gt;')<br/>            X.append([vocab[k] for k in context])<br/>            y.append(vocab[phrase[target]])<br/>            <br/>    return jnp.array(X), jnp.array(y)</span></pre><p id="edf1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个函数的结果将是我们的神经网络的输入。生成的 X 的一些例子是:</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="ca7d" class="ls lt iq ms b gy mw mx l my mz">DeviceArray([[0, 0, 2, 3],<br/>             [0, 1, 3, 0],<br/>             [1, 2, 0, 0],<br/>             [0, 0, 5, 1],<br/>             [0, 4, 1, 6],<br/>             [4, 5, 6, 7],<br/>             [5, 1, 7, 8],<br/>             [1, 6, 8, 2],<br/>             [6, 7, 2, 9],<br/>             [7, 8, 9, 7]], dtype=int32)</span></pre><p id="8818" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，让我们应用刚刚创建的函数:</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="935b" class="ls lt iq ms b gy mw mx l my mz">tokenized_corpus = tokenize_corpus(corpus)</span><span id="b45a" class="ls lt iq ms b gy na mx l my mz">vocab = create_vocabulary(tokenized)</span><span id="8152" class="ls lt iq ms b gy na mx l my mz">X, y = generate_train_data(tokenized_corpus, vocab, window_size)</span></pre><p id="e3e7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们将窗口大小定义为 2，例如:</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="c8df" class="ls lt iq ms b gy mw mx l my mz">window_size = 2</span></pre><p id="8249" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">神经网络的类别如下:</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="a9d6" class="ls lt iq ms b gy mw mx l my mz">class Word2VecCBOW():<br/>    def __init__(self, window_size, embed_dim, vocab_size, random_state):<br/>        # Defines the key to be used for the random creation of the weights<br/>        self.key = random.PRNGKey(random_state)<br/>        self.vocab_size = vocab_size</span><span id="b53f" class="ls lt iq ms b gy na mx l my mz">self.linear = self._create_random_matrix(vocab_size, embed_dim)<br/>        self.soft = self._create_random_matrix(embed_dim, vocab_size)<br/>        <br/>        # Vectorizes the predict method<br/>        self.predict = vmap(self._predict, in_axes=(None, 0))<br/>    <br/>    def train(self, X, y, num_epochs, batch_size):<br/>        y = one_hot(y, self.vocab_size)<br/>        X = one_hot(X, self.vocab_size)</span><span id="97ed" class="ls lt iq ms b gy na mx l my mz">params = [self.linear, self.soft]<br/>        for epoch in range(num_epochs):<br/>            print(f'Epoch: {epoch}')<br/>              for X_batch, y_batch in self.generate_batches(X, y, batch_size):<br/>                    params = self.update(params, X_batch, y_batch)<br/>            print(f'Loss: {float(self.l.primal)}')<br/>        self.linear = params[0]<br/>        self.soft = params[1]</span><span id="3b38" class="ls lt iq ms b gy na mx l my mz">def _predict(self, params, X):<br/>        activations = []<br/>        for x in X:<br/>            activations.append(jnp.dot(x, params[0]))</span><span id="5de2" class="ls lt iq ms b gy na mx l my mz"># Averages the activations<br/>        activation = jnp.mean(jnp.array(activations), axis=0)</span><span id="e640" class="ls lt iq ms b gy na mx l my mz">logits = jnp.dot(activation, params[1])<br/>        result = softmax(logits)<br/>        <br/>        return result<br/>    <br/>    def _create_random_matrix(self, window_size, embed_dim):<br/>        w_key = random.split(self.key, num=1)<br/>        <br/>        return 0.2 * random.normal(self.key, (window_size, embed_dim))</span><span id="1038" class="ls lt iq ms b gy na mx l my mz">def loss(self, params, X, y):<br/>        preds = self.predict(params, X)<br/>        l = -jnp.mean(preds * y)<br/>        self.l = l<br/>        return l</span><span id="f146" class="ls lt iq ms b gy na mx l my mz">def update(self, params, X, y, step_size=0.02):<br/>        grads = grad(self.loss)(params, X, y)</span><span id="4ee9" class="ls lt iq ms b gy na mx l my mz">return [params[0] - step_size * grads[0],<br/>                params[1] - step_size * grads[1]]<br/>    <br/>    def get_embedding(self):<br/>        return self.linear</span><span id="b201" class="ls lt iq ms b gy na mx l my mz">def generate_batches(self, X, y, batch_size):<br/>        for index, offset in enumerate(range(0, len(y), batch_size)):<br/>            yield X[offset: offset + batch_size], y[offset: offset + batch_size]</span></pre><p id="64fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这里，我们创建一个辅助函数来在训练期间生成批次。除此之外，就神经网络应该做什么而言，实现非常简单。</p><p id="bde5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们可以训练我们的网络并检索嵌入内容:</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="fbee" class="ls lt iq ms b gy mw mx l my mz">w2v = Word2VecCBOW(2, 32, len(vocab), 42)</span><span id="e3fa" class="ls lt iq ms b gy na mx l my mz">w2v.train(X, y, 10, 32)</span><span id="777f" class="ls lt iq ms b gy na mx l my mz">w2v.get_embedding()</span></pre><h2 id="f327" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">结论</h2><p id="73dd" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">后续论文进一步改进了该论文的结果。此外，skip-gram 体系结构变得更加流行，并产生了更好的结果。还提出了针对复杂网络的新方法，例如 de Deep Walk 和 Node2Vec。</p><p id="1f94" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章旨在介绍这种方法，并举例说明如何从头开始用 Jax 开发神经网络。</p><p id="c106" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">希望它能对你的数据科学家之旅有所帮助。</p><p id="4047" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1]米科洛夫，托马斯&amp;陈，凯&amp;科拉多，G.s &amp;迪恩，杰弗里。(2013).向量空间中单词表示的有效估计。ICLR 研讨会会议录。2013.</p><p id="b0f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]梅里蒂，斯蒂芬&amp;熊，&amp;布拉德伯里，詹姆斯&amp;索彻，理查德。(2017).指针哨兵混合模型。</p></div></div>    
</body>
</html>