<html>
<head>
<title>SQL to PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">SQL到PySpark</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sql-to-pyspark-d7966e3c15b3#2022-05-06">https://towardsdatascience.com/sql-to-pyspark-d7966e3c15b3#2022-05-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="75c2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从SQL到PySpark的快速指南。</h2></div><p id="22ff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你知道SQL但是需要在PySpark中工作，这篇文章就是为你准备的！</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/7e3c41a95fb61fe1664494479eec3381.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0wneZPokqW_fjbcS"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">照片由<a class="ae lr" href="https://unsplash.com/@m_fath?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Miki Fath </a>在<a class="ae lr" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="5337" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Spark正迅速成为大数据处理最广泛采用的框架之一。但是为什么要使用本机PySpark而不是SQL呢？</p><p id="d96e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">嗯，你不需要。PySpark允许您<a class="ae lr" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.createOrReplaceTempView.html" rel="noopener ugc nofollow" target="_blank">创建一个不牺牲运行时性能的tempView </a>。在后端，spark以完全相同的方式运行相同的转换，而不考虑语言。因此，如果你想坚持使用SQL，你的代码不会有任何不同。</p><p id="ba17" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，当在DataFrame API中工作时，您会得到编译时错误，而使用原始SQL时，您会得到运行时错误。<strong class="kh ir">如果您正在处理大量数据，那么在使用本机PySpark时，同样的错误可能会更早出现。</strong></p><p id="6417" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我们将利用<a class="ae lr" href="https://www.amazon.com/Spark-Definitive-Guide-Processing-Simple/dp/1491912219" rel="noopener ugc nofollow" target="_blank"> Spark:权威指南</a>，顺序处理基本SQL查询中的每个子句，并解释如何在PySpark中复制这个逻辑。</p><p id="0991" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">事不宜迟，让我们开始吧…</p><h1 id="716c" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">挑选</h1><p id="3326" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">任何好的SQL查询都是从SELECT语句开始的——它决定了哪些列将被提取，以及它们是否应该被转换或重命名。</p><h2 id="a64d" class="mp lt iq bd lu mq mr dn ly ms mt dp mc ko mu mv me ks mw mx mg kw my mz mi na bi translated">结构化查询语言</h2><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="edc0" class="mp lt iq nc b gy ng nh l ni nj">SELECT <br/>  column_1,<br/>  CASE WHEN column_2 IS NULL THEN 0 ELSE 1 END AS is_not_null,<br/>  SUM(column_3) OVER(PARTITION BY column_1)</span></pre><h2 id="5046" class="mp lt iq bd lu mq mr dn ly ms mt dp mc ko mu mv me ks mw mx mg kw my mz mi na bi translated">PySpark</h2><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="4e53" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如上所示，SQL和PySpark的结构非常相似。<code class="fe nm nn no nc b">df.select()</code>方法接受作为位置参数传递的一系列字符串。每一个SQL关键字在PySpark中都有对应的符号:点符号，例如<code class="fe nm nn no nc b">df.method()</code>、<code class="fe nm nn no nc b">pyspark.sql</code>或<code class="fe nm nn no nc b">pyspark.sql.functions</code>。</p><p id="062b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">几乎任何SQL select结构都很容易复制，只需搜索一些SQL关键字。</p><blockquote class="np nq nr"><p id="3644" class="kf kg ns kh b ki kj jr kk kl km ju kn nt kp kq kr nu kt ku kv nv kx ky kz la ij bi translated">提示:使用<code class="fe nm nn no nc b"><a class="ae lr" href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.selectExpr.html" rel="noopener ugc nofollow" target="_blank"><em class="iq">df.selectExpr()</em></a></code>来运行带有SQL字符串的SQL命令。</p></blockquote><h1 id="417b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">从</h1><p id="d2fe" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">现在，如果没有一个好的FROM子句，我们的SELECT语句就毫无价值。</p><h2 id="4406" class="mp lt iq bd lu mq mr dn ly ms mt dp mc ko mu mv me ks mw mx mg kw my mz mi na bi translated">结构化查询语言</h2><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="727e" class="mp lt iq nc b gy ng nh l ni nj">FROM df</span></pre><h2 id="5256" class="mp lt iq bd lu mq mr dn ly ms mt dp mc ko mu mv me ks mw mx mg kw my mz mi na bi translated">PySpark</h2><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="79e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很复杂，对吧？</p><p id="9bc9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如上所示，FROM表是由方法之前引用的数据帧定义的。</p><p id="43ff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您习惯于在SQL代码中使用CTE，您可以通过将一组转换赋值给变量来复制CTE逻辑。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h1 id="040d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">在哪里</h1><p id="6481" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">WHERE子句是一个被低估的子句，它可以显著提高查询时间。在PySpark中，有两个<a class="ae lr" href="https://stackoverflow.com/questions/38867472/spark-select-where-or-filtering" rel="noopener ugc nofollow" target="_blank">相同的方法</a>允许你过滤数据:<code class="fe nm nn no nc b">df.where()</code>和<code class="fe nm nn no nc b">df.filter()</code>。</p><h2 id="0a75" class="mp lt iq bd lu mq mr dn ly ms mt dp mc ko mu mv me ks mw mx mg kw my mz mi na bi translated">结构化查询语言</h2><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="56b5" class="mp lt iq nc b gy ng nh l ni nj">WHERE column_2 IS NOT NULL <br/>  AND column_1 &gt; 5</span></pre><h2 id="ab0b" class="mp lt iq bd lu mq mr dn ly ms mt dp mc ko mu mv me ks mw mx mg kw my mz mi na bi translated">PySpark</h2><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="23db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如上所述，两者都支持SQL字符串和本机PySpark，因此利用SQL语法有助于平稳过渡到PySpark。但是，出于可读性和减少错误的目的，完全原生的PySpark应该(可能)是最终目标。</p><h1 id="5bce" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">加入</h1><p id="6c6d" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">连接是另一个被低估的子句——如果你真的很擅长连接，你代码中的bug数量会大大减少。根据<a class="ae lr" href="https://www.amazon.com/Spark-Definitive-Guide-Processing-Simple/dp/1491912219" rel="noopener ugc nofollow" target="_blank"> Spark:权威指南</a>，有8大类连接，其中一些包括内连接和左外连接。</p><p id="76d3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们不会一一介绍，但通常PySpark连接遵循以下语法:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="f090" class="mp lt iq nc b gy ng nh l ni nj">&lt;LEFT&gt;.join(&lt;RIGHT&gt;, &lt;JOIN_EXPRESSION&gt;, &lt;JOIN_TYPE&gt;)</span></pre><ul class=""><li id="6c1e" class="nw nx iq kh b ki kj kl km ko ny ks nz kw oa la ob oc od oe bi translated"><code class="fe nm nn no nc b">&lt;LEFT&gt;</code>和<code class="fe nm nn no nc b">&lt;RIGHT&gt;</code>是PySpark数据帧</li><li id="e79a" class="nw nx iq kh b ki of kl og ko oh ks oi kw oj la ob oc od oe bi translated"><code class="fe nm nn no nc b">&lt;JOIN_EXPRESSION&gt;</code>是两个数据帧中的列之间的布尔比较</li><li id="0fe0" class="nw nx iq kh b ki of kl og ko oh ks oi kw oj la ob oc od oe bi translated"><code class="fe nm nn no nc b">&lt;JOIN_TYPE&gt;</code>是确定连接类型的字符串</li></ul><h2 id="b6f6" class="mp lt iq bd lu mq mr dn ly ms mt dp mc ko mu mv me ks mw mx mg kw my mz mi na bi translated">结构化查询语言</h2><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="7b3b" class="mp lt iq nc b gy ng nh l ni nj">FROM table_1<br/>INNER JOIN table_2<br/>  ON table_1.x = table_2.y</span></pre><h2 id="105c" class="mp lt iq bd lu mq mr dn ly ms mt dp mc ko mu mv me ks mw mx mg kw my mz mi na bi translated">PySpark</h2><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="nk nl l"/></div></figure><blockquote class="np nq nr"><p id="5268" class="kf kg ns kh b ki kj jr kk kl km ju kn nt kp kq kr nu kt ku kv nv kx ky kz la ij bi translated">提示:使用<code class="fe nm nn no nc b">&lt;DF&gt;.dropDuplicates().count() == &lt;DF&gt;.count()</code>来检查左表、右表或连接表中是否有重复项。这些错误有时很难被发现，因为你并没有在寻找它们。</p></blockquote><h1 id="6431" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">分组依据</h1><p id="96dd" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">转到更复杂的SQL分组概念，PySpark的语法与这个领域中的pandas非常相似。</p><h2 id="8fa8" class="mp lt iq bd lu mq mr dn ly ms mt dp mc ko mu mv me ks mw mx mg kw my mz mi na bi translated">结构化查询语言</h2><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="48ef" class="mp lt iq nc b gy ng nh l ni nj">SELECT<br/>  column_1,<br/>  SUM(column_3) AS col_3_sum<br/>FROM df<br/>GROUP BY 1</span></pre><h2 id="5826" class="mp lt iq bd lu mq mr dn ly ms mt dp mc ko mu mv me ks mw mx mg kw my mz mi na bi translated">PySpark</h2><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="6ead" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在PySpark中有许多不同的方法来对数据进行分组，但是最通用的语法是上面的方法。我们利用<code class="fe nm nn no nc b">.agg()</code>并传递许多定义如何转换列的位置参数。注意，我们可以链接<code class="fe nm nn no nc b">.alias()</code>来重命名我们的列，使其比<code class="fe nm nn no nc b">sum(column_3)</code>更有用。</p><p id="6dba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你记住了这个语法，你就可以随时进行任何你想要的转换。非常清楚地说，语法是…</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="54ec" class="mp lt iq nc b gy ng nh l ni nj">df.groupBy(['&lt;col_1&gt;','&lt;col_2&gt;',...]).agg(<br/>  F.&lt;agg_func&gt;('&lt;col_3&gt;').alias('&lt;name_3&gt;'),<br/>  F.&lt;agg_func&gt;('&lt;col_4&gt;').alias('&lt;name_4&gt;'),<br/>  ...<br/>)</span></pre><p id="98a7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有关聚合函数的列表和每个函数的示例，请查看<a class="ae lr" href="https://sparkbyexamples.com/pyspark/pyspark-aggregate-functions/" rel="noopener ugc nofollow" target="_blank"> sparkbyexamples </a>。</p><h1 id="fa0d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="a809" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">在这里，我们讨论了从SQL迁移到PySpark的基础知识。有了上面的结构和google的一些帮助，您可以用本机PySpark编写几乎任何SQL查询。</p><p id="4965" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，有很多SELECT语句关键字，比如CASE、COALESCE或NVL，所有这些都可以使用<code class="fe nm nn no nc b">df.selectExpr()</code>编写。如果你想迁移到原生的PySpark，对google来说很简单。</p><p id="b231" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">希望这有所帮助，祝你好运！</p></div><div class="ab cl ok ol hu om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="ij ik il im in"><p id="3efc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="ns">感谢阅读！我会再写13篇文章，把学术研究带到DS行业。查看我的评论，链接到这篇文章的主要来源和一些有用的资源。</em></p></div></div>    
</body>
</html>