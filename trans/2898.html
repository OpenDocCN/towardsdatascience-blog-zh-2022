<html>
<head>
<title>Gradient Descent VS Regularization: Which One to Use?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降VS正则化:使用哪一个？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-descent-or-regularization-which-one-to-use-f02adc5e642f#2022-06-23">https://towardsdatascience.com/gradient-descent-or-regularization-which-one-to-use-f02adc5e642f#2022-06-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="54e8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">为了更好地理解，对梯度下降和正则化进行概述</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a6837b940759efd490199dbfc96c88a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q194pZ21o05Gp9tVmu9IjA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">巴勃罗·加西亚·萨尔达尼亚在<a class="ae ky" href="https://unsplash.com/s/photos/ways?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="5f45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> W </span>当迈出机器学习的第一步时，有很多东西需要学习和理解。此外，一些ML模型可能看起来彼此非常相似；有时候，很难真正理解它们之间的区别。这是我第一次接触正则化和梯度下降时的情况:它们对我来说太相似了，以至于不能理解它们之间的区别。事实上，正则化和梯度下降都使用成本函数，但它们之间的差异是成本函数的类型和我们对这些方法的使用。</p><p id="0e96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我将试图阐明它们的区别，目的是帮助你更好地理解这些方法。</p><h1 id="d09e" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">1.正规化</h1><p id="9b2e" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">在机器学习中，我们通常应用套索和脊正则化方法(也分别称为L1和L2)，但当“标准”模型过度拟合时，我们会使用它们。</p><p id="5e4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，考虑一个简单的线性回归模型；如果在训练集上得到一个好的决定系数值(例如，接近1 ),但在测试集上得到一个差的决定系数值(例如，接近0 ),那么您将面临过度拟合，应用Lasso或Ridge回归模型甚至可以在测试集上得到好的结果。此外，如果你不知道是否使用套索或脊方法，你可以在这里阅读我的文章:</p><div class="nb nc gp gr nd ne"><a rel="noopener follow" target="_blank" href="/understanding-l1-and-l2-regularization-93918a5ac8d0"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">理解l1和l2正则化</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">线性回归模型中的正则化综述。</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns ks ne"/></div></div></a></div><p id="5507" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在机器学习的背景下，正则化是将系数缩小到零的过程，“阻止”学习复杂的问题；这个过程是通过所谓的成本函数来实现的。</p><p id="9e6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">套索和岭方法具有“预先固定的”成本函数；这意味着它们是“内置”成本函数的这意味着Lasso是用线性回归模型的方程加上等于系数大小绝对值的成本函数构建的。相反，岭是用线性回归模型的方程加上等于系数大小的平方<strong class="lb iu"> </strong>值的成本函数构建的。</p><p id="0acb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于这些是监督学习技术(如果你想了解更多关于监督和非监督学习的知识，你可以在这里阅读我的文章<a class="ae ky" rel="noopener" target="_blank" href="/what-is-a-trained-model-5c872cfa8448"/>)，我们的目标是找到最佳超参数以避免过度拟合，这个超参数可以是0(因此，我们“返回”到简单的线性回归模型)或无穷大(意味着模型已经被算法高度惩罚)。</p><h1 id="0f2c" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">2.梯度下降</h1><p id="d4c3" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">梯度下降是一种学习算法，通过最小化给定的成本函数来工作；所以梯度下降和正则化之间的主要区别是:</p><ul class=""><li id="ee70" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu ny nz oa ob bi translated">正则化方法有一个“预定义的”成本函数，不像梯度下降(它有一个“给定的”成本函数，但我们将在后面看到它是如何工作的)</li><li id="702b" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">与梯度下降不同，当模型过拟合时，使用正则化</li></ul><p id="387b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有一个重要的概念要记住:<strong class="lb iu">模型通过最小化成本函数来学习，</strong>这就是为什么梯度下降是有用的。</p><p id="58a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可能知道，在数学中，当我们需要找到一个函数的最小值时，我们会用到导数。当我们有一个多变量的问题时，我们必须进行多重求导:简化很多，这就是梯度。</p><p id="6373" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，梯度下降算法使用梯度(也称为多维导数)来寻找函数的最小值，并通过多次迭代来实现，直到找到函数的最小值。</p><p id="178c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比如，假设我们有一个函数f(x)，其中x是一个多元的元组:x = (x1，x2，…，xn)。假设f(x)的梯度由∇f(x给出)(倒三角形是标识梯度的符号)。在任何迭代t，我们将用x[t]表示元组x的值。对于每个迭代t，梯度下降的训练工作如下:</p><p id="189f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">x[t] = x[t-1] — 𝜂∇f(x[t-1])</p><p id="ee03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中𝜂被称为学习率(这是一个超参数)。在这个过程的最后，算法将找到使函数f(x)最小的变量(x1，x2，…，xn)的值(更多细节参见<a class="ae ky" href="https://machinelearningmastery.com/a-gentle-introduction-to-gradient-descent-procedure/" rel="noopener ugc nofollow" target="_blank">这里</a>)。</p><p id="0696" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，最后，我们发现了什么？像在机器学习中一样，我们找到了最适合给定数据的函数，但我们发现它最小化了一个成本函数。</p><h1 id="7356" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated"><strong class="ak">结论</strong></h1><p id="7939" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">本文的目的是阐明梯度下降和正则化之间的区别；概括地说，这些差异是:</p><ul class=""><li id="18b4" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu ny nz oa ob bi translated">正则化方法有一个“预定义”的成本函数，不像梯度下降法，成本函数是给定函数的梯度。</li><li id="36f0" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">当模型过度拟合时，使用正则化，这与梯度下降不同，梯度下降不考虑过度拟合(我们可能会在验证后发现模型过度拟合，但<strong class="lb iu">我们不使用梯度下降来防止过度拟合</strong>)。</li></ul></div><div class="ab cl oh oi hx oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="im in io ip iq"><p id="4d9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="oo">让我们连在一起！</em></p><p id="ccb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://federicotrotta.medium.com/" rel="noopener"> <em class="oo">中等</em> </a></p><p id="eb28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://www.linkedin.com/in/federico-trotta/" rel="noopener ugc nofollow" target="_blank"> <em class="oo"> LINKEDIN </em> </a> <em class="oo">(向我发送连接请求)</em></p><p id="a07f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="oo">如果你愿意，你可以</em> <a class="ae ky" href="https://federicotrotta.medium.com/subscribe" rel="noopener"> <em class="oo">订阅我的邮件列表</em> </a> <em class="oo">这样你就可以一直保持更新了！</em></p></div><div class="ab cl oh oi hx oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="im in io ip iq"><p id="56e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑成为会员:你可以免费支持我和其他像我一样的作家。点击 <a class="ae ky" href="https://federicotrotta.medium.com/membership" rel="noopener"> <em class="oo">这里的</em> </a> <em class="oo">成为会员。</em></p></div></div>    
</body>
</html>