<html>
<head>
<title>Understanding AIPW, the Doubly-Robust Estimator</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解双重稳健估计量AIPW</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-aipw-ed4097dab27a#2022-07-19">https://towardsdatascience.com/understanding-aipw-ed4097dab27a#2022-07-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="ed92" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/causal-data-science" rel="noopener" target="_blank">因果数据科学</a></h2><div class=""/><div class=""><h2 id="05f9" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated"><em class="ko">模型错误设定下条件平均治疗效果(CATE)估计指南</em></h2></div><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/f77f4dcc4ff5dfa5fa4c119105c10448.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gf1u19ivnFptsErKIRJ2hA.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">封面，图片来自我的好朋友<a class="ae lf" href="https://chiaraaina.github.io/" rel="noopener ugc nofollow" target="_blank">基亚拉·艾纳</a></p></figure><p id="5480" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在估计因果关系时，黄金标准是<strong class="li ja">随机对照试验或AB测试</strong>。通过随机将单位暴露于治疗，我们确保两组中的个体在平均水平上是可比较的，并且我们观察到的任何差异都可以单独归因于治疗效果。</p><p id="aad4" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">然而，通常治疗组和对照组<strong class="li ja">并不完全可比</strong>。这可能是由于随机化不完善或不可用的事实。出于伦理或实践的原因，我们并不总是能够随机选择治疗方法。即使我们可以，有时我们也没有足够的个人或单位来捕捉群体间的差异。这种情况经常发生，例如，当随机化不是在个体水平上进行，而是在更高的聚集水平上进行时，例如，邮政编码、县甚至州。</p><p id="a5b7" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在之前的<a class="ae lf" rel="noopener" target="_blank" href="/99bf5cffa0d9">帖子</a>中，我介绍并比较了一系列从观察或实验数据中计算<strong class="li ja">条件平均治疗效果(CATE) </strong>的方法。在给定治疗和可观察特征的情况下，这些方法中的一些要求研究者指定和估计感兴趣的结果的分布(例如<a class="ae lf" rel="noopener" target="_blank" href="/8a9c1e340832">元学习者</a>)。其他方法要求研究人员根据可观察的特征(例如<a class="ae lf" rel="noopener" target="_blank" href="/99bf5cffa0d9"> IPW </a>)指定和估计被治疗的概率。</p><p id="6ad8" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在这篇文章中，我们将看到一个程序，它结合了两种方法，并且对两种模型的错误设定都是鲁棒的(T21):增强逆概率加权估计(AIPW)。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/59e3bafb0788e4b96f9c42dc66244749.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*lZkOh5lOY9FjUcaQPYdR-A.gif"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">融合，gif由作者剪切</p></figure><p id="23a2" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">TLDR；</strong> AIPW是IPW和元学习者两者的概括，比他们每一个都好用！</p></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="033b" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">例子</h1><p id="b403" class="pw-post-body-paragraph lg lh iq li b lj nc ka ll lm nd kd lo lp ne lr ls lt nf lv lw lx ng lz ma mb ij bi translated">假设我们有一个关于统计和因果推理的博客😇。为了改善用户体验，我们正在考虑<strong class="li ja">发布一个黑暗模式</strong>，我们想了解这个新功能是否会增加用户在我们博客上花费的时间。</p><p id="eb11" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">这个例子是从我上一篇关于估计条件平均治疗效果(CATE)的文章中借用的。你可以在这里找到<a class="ae lf" rel="noopener" target="_blank" href="/99bf5cffa0d9">原帖</a>。如果你记得设置，你可以跳过这个介绍。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="ab gu cl nh"><img src="../Images/4373e14a07944f20090fdf5fc4f78121.png" data-original-src="https://miro.medium.com/v2/format:webp/1*Hp7STI5b8nCfgJlLtvCGmg.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">不使用和使用黑暗模式的博客外观，由作者提供的图像</p></figure><p id="bc98" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们不是一家成熟的公司，因此我们不进行AB测试，而是简单地发布黑暗模式，我们观察用户是否选择它以及他们在博客上花费的时间。我们知道可能会有<strong class="li ja">选择</strong>:偏好黑暗模式的用户可能会有不同的阅读偏好，这可能会使我们的因果分析变得复杂。</p><p id="c750" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们可以用下面的<a class="ae lf" rel="noopener" target="_blank" href="/b63dc69e3d8c"> <strong class="li ja">有向无环图(DAG) </strong> </a>来表示数据生成过程。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ni"><img src="../Images/fe94796174eab828ac682c72d84e8bbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j0zbE3_i5LPmOedhZ-aFCg.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">数据生成过程的DAG，按作者排序的图像</p></figure><p id="c3dc" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们使用来自<code class="fe nj nk nl nm b"><a class="ae lf" href="https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py" rel="noopener ugc nofollow" target="_blank">src.dgp</a></code>的数据生成过程<code class="fe nj nk nl nm b">dgp_darkmode()</code>生成模拟数据。我还从<code class="fe nj nk nl nm b"><a class="ae lf" href="https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py" rel="noopener ugc nofollow" target="_blank">src.utils</a></code>引进了一些绘图函数和库。</p><pre class="kq kr ks kt gt nn nm no np aw nq bi"><span id="64ef" class="nr ml iq nm b gy ns nt l nu nv">from src.utils import *<br/>from src.dgp import dgp_darkmode</span><span id="337f" class="nr ml iq nm b gy nw nt l nu nv">dgp = dgp_darkmode()<br/>df = dgp.generate_data()<br/>df.head()</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="ab gu cl nh"><img src="../Images/de583901ef325575c3c6c6286c3c280c.png" data-original-src="https://miro.medium.com/v2/format:webp/1*NpQcqKCllpR1hVvfgWbZyQ.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">数据快照，图片由作者提供</p></figure><p id="c04e" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们有300名用户的信息，我们观察他们是否选择了<code class="fe nj nk nl nm b">dark_mode</code>(治疗)、他们每周的<code class="fe nj nk nl nm b">read_time</code>(感兴趣的结果)以及一些特征，如<code class="fe nj nk nl nm b">gender</code>、<code class="fe nj nk nl nm b">age</code>和以前在博客上花费的总<code class="fe nj nk nl nm b">hours</code>。</p><p id="0967" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们想估计一下新的<code class="fe nj nk nl nm b">dark_mode</code>对用户<code class="fe nj nk nl nm b">read_time</code>的影响。作为第一种方法，我们可能天真地将效果计算为平均值的差异，假设治疗和对照样本是可比的。我们可以通过在<code class="fe nj nk nl nm b">dark_mode</code>上回归<code class="fe nj nk nl nm b">read_time</code>来估计均值的差异。</p><pre class="kq kr ks kt gt nn nm no np aw nq bi"><span id="1c0f" class="nr ml iq nm b gy ns nt l nu nv">smf.ols("read_time ~ dark_mode", data=df).fit().summary().tables[1]</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nx"><img src="../Images/4c45967169e682dcb374026934039e66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wBri6966Cll92NfOdbe7vw.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">线性回归结果，图片由作者提供</p></figure><p id="998a" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">选择<code class="fe nj nk nl nm b">dark_mode</code>的人平均每周花在博客上的时间少了0.44小时。我们应该断定<code class="fe nj nk nl nm b">dark_mode</code>是一个<strong class="li ja">坏主意</strong>吗？这是因果关系吗？</p><p id="37ba" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">问题是我们做了<strong class="li ja">而不是</strong>运行<a class="ae lf" href="https://de.wikipedia.org/wiki/A/B-Test" rel="noopener ugc nofollow" target="_blank"> <strong class="li ja"> AB测试</strong> </a>或随机对照试验，因此选择了<code class="fe nj nk nl nm b">dark_mode</code>的用户可能无法与没有选择的用户直接<strong class="li ja">比较</strong>。我们能证实这种担忧吗？部分地。在我们的设置中，我们只能检查我们观察到的特征、<code class="fe nj nk nl nm b">gender</code>、<code class="fe nj nk nl nm b">age</code>和总计<code class="fe nj nk nl nm b">hours</code>。我们无法检查用户是否在我们没有观察到的其他维度上存在差异。</p><p id="8ccb" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">让我们使用优步<code class="fe nj nk nl nm b"><a class="ae lf" href="https://causalml.readthedocs.io/" rel="noopener ugc nofollow" target="_blank">causalml</a></code>包中的<code class="fe nj nk nl nm b">create_table_one</code>函数来生成一个<strong class="li ja">协变量平衡表</strong>，包含我们在治疗组和对照组中可观察特征的平均值。顾名思义，这应该永远是你在因果推断分析中呈现的第一张表。</p><pre class="kq kr ks kt gt nn nm no np aw nq bi"><span id="5fc6" class="nr ml iq nm b gy ns nt l nu nv">from causalml.match import create_table_one<br/><br/>X = ['male', 'age', 'hours']<br/>create_table_one(df, 'dark_mode', X)</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="ab gu cl nh"><img src="../Images/5c9e09eebf94b01b027cb245971ff612.png" data-original-src="https://miro.medium.com/v2/format:webp/1*SqbhEnAkxnjRvJY09yEp_A.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">平衡表，作者图片</p></figure><p id="29d1" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">治疗组(<code class="fe nj nk nl nm b">dark_mode</code>)和对照组之间似乎有<strong class="li ja">一些差异</strong>。特别是，选择<code class="fe nj nk nl nm b">dark_mode</code>的用户年龄较大，花在博客上的时间较少，他们更有可能是男性。</p><p id="d67a" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们能做什么？如果我们假设治疗组和对照组之间的所有差异都是<strong class="li ja">可观察的</strong>，我们可以通过执行<strong class="li ja">条件分析</strong>来解决问题。</p></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="938e" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">条件分析</h1><p id="2820" class="pw-post-body-paragraph lg lh iq li b lj nc ka ll lm nd kd lo lp ne lr ls lt nf lv lw lx ng lz ma mb ij bi translated">我们假设对于一组主题<em class="ny"> i = 1，…，n </em>，我们观察到一个元组<em class="ny"> (Dᵢ，Yᵢ，Xᵢ) </em>包括</p><ul class=""><li id="3c17" class="nz oa iq li b lj lk lm ln lp ob lt oc lx od mb oe of og oh bi translated">一个治疗分配<em class="ny"> Dᵢ ∈ {0，1} </em> ( <code class="fe nj nk nl nm b">dark_mode</code>)</li><li id="f09c" class="nz oa iq li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">一个回应<em class="ny"> Yᵢ ∈ ℝ </em> ( <code class="fe nj nk nl nm b">read_time</code>)</li><li id="a350" class="nz oa iq li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">一个特征向量<em class="ny"> Xᵢ ∈ ℝⁿ </em> ( <code class="fe nj nk nl nm b">gender</code>、<code class="fe nj nk nl nm b">age</code>和<code class="fe nj nk nl nm b">hours</code>)</li></ul><p id="8968" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们感兴趣的是<strong class="li ja">估计条件平均治疗效果(CATE) </strong>。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi on"><img src="../Images/8196bc12a73dde3925b67dec189f4f5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5fgAHbc7qKt2QQ2WlDm26w.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">条件平均治疗效果(CATE)，图片由作者提供</p></figure><p id="50d7" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">其中<em class="ny"> Yᵢ </em> ⁽ᵈ⁾表示个体<em class="ny"> i </em>在治疗状态<em class="ny"> d </em>下的潜在结果。我们还做了以下假设。</p><p id="07a2" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">假设1:未发现</strong>(或可忽略，或可观的选择)</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi oo"><img src="../Images/6706a3e20628386002aa59bea2de7758.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2w7wQZazNWVs2cCFnqNPkw.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">无根据假设，作者的图像</p></figure><p id="4c0d" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">即以可观察的特征<em class="ny"> X </em>为条件，治疗分配<em class="ny"> D </em>几乎是随机的。我们实际假设的是，没有其他我们没有观察到的特征会影响用户是否选择<code class="fe nj nk nl nm b">dark_mode</code>和他们的<code class="fe nj nk nl nm b">read_time</code>。这是一个强有力的假设，我们观察到的个人特征越多，这个假设就越有可能得到满足。</p><p id="3322" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">假设2:重叠</strong>(或共同支撑)</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi op"><img src="../Images/72fe29fd4119ac17bb742ec24bedba14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*96DwL5WJM6t4fpPJj7hU0g.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">重叠假设，作者图片</p></figure><p id="ebc9" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">即没有观察结果被确定地分配给治疗组或对照组。这是一个更技术性的假设，基本上意味着对于任何级别的<code class="fe nj nk nl nm b">gender</code>、<code class="fe nj nk nl nm b">age</code>或<code class="fe nj nk nl nm b">hours</code>，都可能存在选择<code class="fe nj nk nl nm b">dark_mode</code>的个体和不选择<code class="fe nj nk nl nm b">dark_mode</code>的个体。与未发现假设不同，总体假设是<strong class="li ja">可检验的</strong>。</p><p id="14ae" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">假设3:稳定单位治疗值(SUTVA) </strong></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi oq"><img src="../Images/b1b15a12df4006226da5a9e71beb3928.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*agQO-VSHle1zqWR2_FVV8w.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">稳定单位治疗值假设(SUTVA)，作者图片</p></figure><p id="e26b" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">即潜在的结果不取决于治疗状态。在我们的例子中，我们排除了另一个用户选择<code class="fe nj nk nl nm b">dark_mode</code>可能会影响我的<code class="fe nj nk nl nm b">dark_mode</code>对<code class="fe nj nk nl nm b">read_time</code>的影响。违反SUTVA的最常见的情况是存在<strong class="li ja">网络效应</strong>:如果我的一个朋友使用一个社交网络，会增加我使用它的效用。</p><h2 id="10d3" class="nr ml iq bd mm or os dn mq ot ou dp mu lp ov ow mw lt ox oy my lx oz pa na iw bi translated">IPW和元学习者</h2><p id="af3d" class="pw-post-body-paragraph lg lh iq li b lj nc ka ll lm nd kd lo lp ne lr ls lt nf lv lw lx ng lz ma mb ij bi translated">执行条件分析的两种替代方法是</p><ol class=""><li id="66b2" class="nz oa iq li b lj lk lm ln lp ob lt oc lx od mb pb of og oh bi translated"><a class="ae lf" rel="noopener" target="_blank" href="/99bf5cffa0d9"/></li><li id="12a1" class="nz oa iq li b lj oi lm oj lp ok lt ol lx om mb pb of og oh bi translated"><a class="ae lf" rel="noopener" target="_blank" href="/8a9c1e340832"/></li></ol><p id="b8ee" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">这两个替代程序利用了我们以不同方式观察个体特征<em class="ny"> X </em>的事实:</p><ol class=""><li id="35b5" class="nz oa iq li b lj lk lm ln lp ob lt oc lx od mb pb of og oh bi translated">IPW利用<em class="ny"> X </em>来预测治疗分配<em class="ny"> D </em>并估计<strong class="li ja">倾向得分</strong><em class="ny">e(x)=𝔼[d|x】</em></li><li id="8f01" class="nz oa iq li b lj oi lm oj lp ok lt ol lx om mb pb of og oh bi translated">元学习者利用<em class="ny"> X </em>来预测反事实结果<em class="ny"> Y </em> ⁽ᵈ⁾并估计<strong class="li ja">响应函数</strong>T6】μ⁽ᵈ⁾<em class="ny">(x)</em>=<em class="ny">𝔼[y|d，x】</em></li></ol><p id="abdd" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们能不能把这两个过程结合起来，从而两全其美呢？</p><p id="9832" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">是的，使用<strong class="li ja"> AIPW或双稳健估计器</strong>。</p></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="2b22" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">AIPW估计量</h1><p id="d20e" class="pw-post-body-paragraph lg lh iq li b lj nc ka ll lm nd kd lo lp ne lr ls lt nf lv lw lx ng lz ma mb ij bi translated"><strong class="li ja">增强的反向倾向加权</strong>估计量由下式给出</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi pc"><img src="../Images/96522503bc5a2f246005a13853f3a6cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S8er96iIzhLPdej4G7aFGQ.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">AIPW估算器，作者图片</p></figure><p id="19f8" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">其中<em class="ny"> μ </em> ⁽ᵈ⁾ <em class="ny"> (x) </em>为<strong class="li ja">反应函数</strong>，即结果的期望值，以可观察特征<em class="ny"> x </em>和治疗状态<em class="ny"> d </em>为条件，<em class="ny"> e(x) </em>为<strong class="li ja">倾向得分</strong>。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi pd"><img src="../Images/058fb16314afc2fe61da26d889499e59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qoOY1jjDGh8yogyZ0tF33g.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">响应函数和倾向得分，按作者分类的图像</p></figure><p id="7408" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">AIPW估计量的公式乍一看似乎非常神秘，所以让我们深入研究并试图理解它。</p><h2 id="405e" class="nr ml iq bd mm or os dn mq ot ou dp mu lp ov ow mw lt ox oy my lx oz pa na iw bi translated">分解</h2><p id="ac3b" class="pw-post-body-paragraph lg lh iq li b lj nc ka ll lm nd kd lo lp ne lr ls lt nf lv lw lx ng lz ma mb ij bi translated">理解AIPW公式的最好方法是<strong class="li ja">将</strong>分解成两部分。</p><p id="ae02" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">第一种方式</strong>是将AIPW估计器分解成一个<a class="ae lf" rel="noopener" target="_blank" href="/8a9c1e340832"> <strong class="li ja"> S-learner估计器</strong> </a>和一个调整因子。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi pe"><img src="../Images/8ad130a918a270b7a0dd8452cbe3ff40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*udRtS08BZ4V-sBkMyjXIrw.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">第一次AIPW分解，图片作者</p></figure><p id="975b" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在哪里</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi pf"><img src="../Images/9bfa6eab0e1a46de9698d371fdc10c02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G0kovPFYeNXBsHr4PaMGfA.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">第一次AIPW分解的详细信息，图片由作者提供</p></figure><p id="9b93" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">这种调整实质上是对S学习器的<strong class="li ja">残差</strong>执行的IPW估计器。</p><p id="ca23" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">第二种方式</strong>将AIPW估算器分解成<strong class="li ja"> IPW估算器</strong>和一个调整因子。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi pg"><img src="../Images/cf663784675194f377f2e3873e77d8ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zBDL3ro8vw0fRdyFtkCKQg.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">第二次AIPW分解，作者图片</p></figure><p id="c355" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在哪里</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ph"><img src="../Images/2c71cbd46fa646da43415bca9af68bea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PEkHMZBCAX4vB8toGedI_g.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">第二次AIPW分解的详细信息，图片由作者提供</p></figure><p id="a757" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">该调整本质上是一个由剩余治疗概率加权的S-学习器估计器。</p><h2 id="36ad" class="nr ml iq bd mm or os dn mq ot ou dp mu lp ov ow mw lt ox oy my lx oz pa na iw bi translated">双重鲁棒性</h2><p id="b924" class="pw-post-body-paragraph lg lh iq li b lj nc ka ll lm nd kd lo lp ne lr ls lt nf lv lw lx ng lz ma mb ij bi translated">为什么AIPW估算器如此引人注目？原因是它只需要两个预测<em class="ny"> μ̂ </em>或<em class="ny"> ê </em>中的一个是正确的，以便<strong class="li ja">无偏</strong>(即平均正确)。让我们检查一下。</p><p id="8b18" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">如果<em class="ny"> μ̂ </em>指定正确，即𝔼[<em class="ny">μ̂</em>⁽ᵈ⁾<em class="ny">(x)</em><em class="ny">=</em>𝔼[<em class="ny">yᵢ| xᵢ=x，Dᵢ=d </em>，那么<em class="ny">τ\̘</em>aipw是无偏的，<strong class="li ja">即使</strong><em class="ny">e\792;</em>拼写错误。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi pi"><img src="../Images/fa7e6446e9dad491a7b6f89ecbd6210b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QqdjJQDCuXOgHzYf0-VgGQ.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">倾向分数错误设定下的AIPW无偏性，作者图片</p></figure><p id="d88f" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">直觉</strong>是，如果<em class="ny"> μ̂ </em>被正确指定，<em class="ny"> τ̂ </em> S-learn是<strong class="li ja">无偏的</strong>并且调整因子<strong class="li ja">消失</strong>，因为残差<em class="ny">(yᵢ−μ̂</em>【⁽ᵈ⁾<em class="ny">(xᵢ)</em>收敛到零。</p><p id="96bf" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">另一方面，如果<em class="ny"> ê </em>被正确指定，即𝔼[<em class="ny">ê(x)</em>]= 𝔼[<em class="ny">dᵢ=1 | xᵢ=x</em>，那么<em class="ny"> τ̂ </em> AIPW是无偏的，<strong class="li ja">即使</strong> <em class="ny"> μ̂ </em>被错误指定。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi pj"><img src="../Images/9ecc68361e855f7c3aeae93a0d37c6cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sZPivzyYJt3BYy4EY_8qCQ.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">响应函数错误设定下的AIPW无偏性，图片由作者提供</p></figure><p id="f629" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">直觉</strong>是，如果<em class="ny"> ê </em>被正确指定，<em class="ny">τ̂</em>ipw<strong class="li ja">无偏</strong>，并且调整因子<strong class="li ja">消失</strong>，因为残差<em class="ny"> (Dᵢ−ê (Xᵢ)) </em>收敛到零。</p><h2 id="235e" class="nr ml iq bd mm or os dn mq ot ou dp mu lp ov ow mw lt ox oy my lx oz pa na iw bi translated">最佳实践</h2><p id="ee9c" class="pw-post-body-paragraph lg lh iq li b lj nc ka ll lm nd kd lo lp ne lr ls lt nf lv lw lx ng lz ma mb ij bi translated">下面是实现AIPW估计器时的最佳实践列表(有些更通用)。</p><p id="5b4f" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> 1。检查协变量平衡</strong></p><p id="cbdc" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">IPW和AIPW都是为治疗<em class="ny"> D </em>不是无条件随机分配，而是可能依赖于一些可观察的<em class="ny"> X </em>的设置而构建的。可以通过两种方式检查该信息:</p><ol class=""><li id="735d" class="nz oa iq li b lj lk lm ln lp ob lt oc lx od mb pb of og oh bi translated">制作平衡表，总结各治疗组的协变量。如果无条件随机化不成立，我们期望在一些可观察的方面看到显著的差异</li><li id="affe" class="nz oa iq li b lj oi lm oj lp ok lt ol lx om mb pb of og oh bi translated">画出估计的倾向分数。如果无条件随机化成立，我们期望倾向得分是常数</li></ol><p id="3ca6" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> 2。检查重叠假设</strong></p><p id="31ec" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们可以检查的另一个假设是<strong class="li ja">重叠</strong>假设。为了检验这个假设，我们可以简单地检验预测倾向得分的界限。如果重叠假设被违反，我们最终将估计量的某些项除以零。</p><p id="4f86" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> 3。使用十字接头</strong></p><p id="b6b4" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">每当我们建立一个预测时，当估计响应函数<em class="ny"> μ̂ </em> ⁽ᵈ⁾ <em class="ny"> (Xᵢ) </em>或倾向得分<em class="ny">ê(xᵢ</em>时，最好的做法是排除观察值<em class="ny"> i </em>。这个程序在机器学习文献中一般称为<a class="ae lf" href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)" rel="noopener ugc nofollow" target="_blank"> <strong class="li ja">交叉拟合</strong> </a>。虽然有许多可能的方法来执行交叉拟合，但最简单的方法如下:</p><ol class=""><li id="ed17" class="nz oa iq li b lj lk lm ln lp ob lt oc lx od mb pb of og oh bi translated">将样本随机分成两份</li><li id="1469" class="nz oa iq li b lj oi lm oj lp ok lt ol lx om mb pb of og oh bi translated">用样本1估计<em class="ny"> μ̂ </em> ⁽ᵈ⁾ <em class="ny"> (x) </em>和<em class="ny">ê(x</em></li><li id="9d02" class="nz oa iq li b lj oi lm oj lp ok lt ol lx om mb pb of og oh bi translated">使用样本2估计<em class="ny"> τ̂₁ </em> AIPW</li><li id="34dd" class="nz oa iq li b lj oi lm oj lp ok lt ol lx om mb pb of og oh bi translated">重复(2)和(3)交换样本以估计<em class="ny"> τ̂₂ </em> AIPW</li><li id="12c2" class="nz oa iq li b lj oi lm oj lp ok lt ol lx om mb pb of og oh bi translated">计算<em class="ny"> τ̂ </em> AIPW作为两个估计值的平均值</li></ol><p id="aac8" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">步骤(2)和(3)确保估计器是<strong class="li ja">而不是过拟合</strong>。步骤(4)和(5)确保估计器是<strong class="li ja">有效的</strong>，使用所有步骤的所有数据，而不仅仅是一半。<a class="ae lf" href="https://arxiv.org/abs/2004.14497" rel="noopener ugc nofollow" target="_blank"> Kennedy (2022) </a>表明这种方法比现有方法产生更精确的估计，并提供关于误差界限的正式结果。特别是，他们的主要结果如下:</p><blockquote class="pk pl pm"><p id="e49e" class="lg lh ny li b lj lk ka ll lm ln kd lo pn lq lr ls po lu lv lw pp ly lz ma mb ij bi translated">定理2中给出的DR-Learner误差的界限表明，它最多只能偏离oracle误差一个(平滑的)倾向得分和回归估计量的误差乘积，因此即使当NUS ance估计以较慢的速率收敛时，也允许以较快的速率估计CATE。重要的是，结果与所用的方法无关，不需要特殊的调整或不必要的修饰。</p></blockquote></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="a040" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">回到数据</h1><p id="8029" class="pw-post-body-paragraph lg lh iq li b lj nc ka ll lm nd kd lo lp ne lr ls lt nf lv lw lx ng lz ma mb ij bi translated">现在，让我们在博客阅读时间和黑暗模式的数据集中构建和探索AIPW估计器。</p><h2 id="57a7" class="nr ml iq bd mm or os dn mq ot ou dp mu lp ov ow mw lt ox oy my lx oz pa na iw bi translated">倾向得分</h2><p id="58a5" class="pw-post-body-paragraph lg lh iq li b lj nc ka ll lm nd kd lo lp ne lr ls lt nf lv lw lx ng lz ma mb ij bi translated">首先，我们来估计一下<strong class="li ja">倾向得分</strong> <em class="ny"> e(X) </em>。</p><pre class="kq kr ks kt gt nn nm no np aw nq bi"><span id="19b2" class="nr ml iq nm b gy ns nt l nu nv">def estimate_e(df, X, D, model_e):<br/>    e = model_e.fit(df[X], df[D]).predict_proba(df[X])[:,1]<br/>    return e</span></pre><p id="2295" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们使用<code class="fe nj nk nl nm b">sklearn</code>包中的<code class="fe nj nk nl nm b">LogisticRegression</code>方法，通过<a class="ae lf" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank">逻辑回归</a>来估计它们。</p><pre class="kq kr ks kt gt nn nm no np aw nq bi"><span id="40c0" class="nr ml iq nm b gy ns nt l nu nv">from sklearn.linear_model import LogisticRegression<br/><br/>df['e'] = estimate_e(df, X, "dark_mode", LogisticRegression())</span></pre><p id="3943" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">让我们通过绘制治疗组和对照组的估计倾向得分来检查是否满足<strong class="li ja">有界支持</strong>假设。</p><pre class="kq kr ks kt gt nn nm no np aw nq bi"><span id="0809" class="nr ml iq nm b gy ns nt l nu nv">sns.histplot(data=df, x='e', hue='dark_mode', bins=30, stat='density', common_norm=False).\<br/>    set(ylabel="", title="Distribution of Propensity Scores");</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="ab gu cl nh"><img src="../Images/9dfd4eecd0a641c02b71e317048ec6fc.png" data-original-src="https://miro.medium.com/v2/format:webp/1*wcyexTvwr7z234fUvcN0TQ.png"/></div></figure><p id="99c3" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">倾向得分的分布在两组之间是不同的，但它通常是重叠的。</p><p id="276a" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们现在可以使用倾向分数来构建IPW估计量。</p><pre class="kq kr ks kt gt nn nm no np aw nq bi"><span id="65b7" class="nr ml iq nm b gy ns nt l nu nv">w = 1 / (e * df["dark_mode"] + (1-e) * (1-df["dark_mode"]))<br/>smf.wls("read_time ~ dark_mode", weights=w, data=df).fit().summary().tables[1]</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi pq"><img src="../Images/bf964aaaadce0a7d7a5c4cac9c4cb06d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-QZZ_9dKl0YFFCj209G38g.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">IPW回归结果，图片由作者提供</p></figure><p id="2dac" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">请注意，计算出的标准误差并不精确，因为我们忽略了来自倾向得分估计的额外不确定性<em class="ny"> e(x) </em>。</p><h2 id="c643" class="nr ml iq bd mm or os dn mq ot ou dp mu lp ov ow mw lt ox oy my lx oz pa na iw bi translated">响应函数</h2><p id="7300" class="pw-post-body-paragraph lg lh iq li b lj nc ka ll lm nd kd lo lp ne lr ls lt nf lv lw lx ng lz ma mb ij bi translated">现在让我们来估计AIPW估计器的第二个构件:<strong class="li ja">响应函数</strong> <em class="ny"> μ(x) </em>。</p><pre class="kq kr ks kt gt nn nm no np aw nq bi"><span id="32b7" class="nr ml iq nm b gy ns nt l nu nv">def estimate_mu(df, X, D, y, model_mu):<br/>    mu = model_mu.fit(df[X + [D]], df[y])<br/>    mu0 = mu.predict(df[X + [D]].assign(dark_mode=0))<br/>    mu1 = mu.predict(df[X + [D]].assign(dark_mode=1))<br/>    return mu0, mu1</span></pre><p id="ec8c" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">先用线性回归估计<em class="ny"> μ(x) </em>吧。</p><pre class="kq kr ks kt gt nn nm no np aw nq bi"><span id="d009" class="nr ml iq nm b gy ns nt l nu nv">from sklearn.linear_model import LinearRegression<br/><br/>mu0, mu1 = estimate_mu(df, X, "dark_mode", "read_time", LinearRegression())<br/>print(np.mean(mu1-mu0))</span><span id="81aa" class="nr ml iq nm b gy nw nt l nu nv">1.3858099131476969</span></pre><p id="a25c" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们已经将平均治疗效果的元学习者估计值计算为两个估计响应函数<em class="ny"> μ </em> ⁽ ⁾ <em class="ny"> (x) </em>和<em class="ny"> μ </em> ⁽⁰⁾ <em class="ny"> (x) </em>之间的均值差。</p><p id="21c8" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">注意</strong>我们可以使用任何估计量来获得响应函数，为了简单起见，我使用了线性回归。</p><h2 id="1a49" class="nr ml iq bd mm or os dn mq ot ou dp mu lp ov ow mw lt ox oy my lx oz pa na iw bi translated">估计AIPW</h2><p id="9bfc" class="pw-post-body-paragraph lg lh iq li b lj nc ka ll lm nd kd lo lp ne lr ls lt nf lv lw lx ng lz ma mb ij bi translated">我们现在有了<strong class="li ja">计算AIPW估计量的所有构件</strong>！</p><pre class="kq kr ks kt gt nn nm no np aw nq bi"><span id="6ba7" class="nr ml iq nm b gy ns nt l nu nv">aipw = mu1 - mu0 + df["dark_mode"] / e * (df["read_time"] - mu1) - (1-df["dark_mode"]) / (1-e) * (df["read_time"] - mu0)<br/>print(np.mean(aipw))</span><span id="f7fb" class="nr ml iq nm b gy nw nt l nu nv">1.3153774511905783</span></pre><p id="81e0" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们也可以使用微软<code class="fe nj nk nl nm b"><a class="ae lf" href="https://econml.azurewebsites.net/index.html" rel="noopener ugc nofollow" target="_blank">EconML</a></code>库中的<code class="fe nj nk nl nm b">LinearDRLearner</code>函数直接计算。</p><pre class="kq kr ks kt gt nn nm no np aw nq bi"><span id="e4f8" class="nr ml iq nm b gy ns nt l nu nv">from econml.drlearner import LinearDRLearner<br/><br/>model = LinearDRLearner(model_propensity=LogisticRegression(), <br/>                        model_regression=LinearRegression(),<br/>                        random_state=1)<br/>model.fit(Y=df["read_time"], T=df["dark_mode"], X=df[X]);</span></pre><p id="fb07" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">模型直接给我们平均治疗效果。</p><pre class="kq kr ks kt gt nn nm no np aw nq bi"><span id="677f" class="nr ml iq nm b gy ns nt l nu nv">model.ate_inference(X=df[X].values, T0=0, T1=1).summary().tables[0]</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi pr"><img src="../Images/89cb894f70467c60ac25b1fe948bf3de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3KXPPfbe4d1HCO4gtmSmdw.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">AIPW评估结果，图片由作者提供</p></figure><p id="1fba" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">该估计在统计上不同于零，并且置信区间包括真值2。</p><p id="d20f" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">注意</strong>我们得到了一个不同的估计，因为<code class="fe nj nk nl nm b">LinearDRLearner</code>函数也在后台执行<strong class="li ja">交叉拟合</strong>，这是我们以前没有做过的。</p><h2 id="7d05" class="nr ml iq bd mm or os dn mq ot ou dp mu lp ov ow mw lt ox oy my lx oz pa na iw bi translated">评价</h2><p id="410d" class="pw-post-body-paragraph lg lh iq li b lj nc ka ll lm nd kd lo lp ne lr ls lt nf lv lw lx ng lz ma mb ij bi translated">现在让我们来评估AIPW估计器的主要特性:它的<strong class="li ja">双重鲁棒性</strong>。为此，我们将它与它的两个双亲:IPW估计器和S学习器进行比较。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="ps pt l"/></div></figure><p id="a7d3" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们使用<code class="fe nj nk nl nm b"><a class="ae lf" href="https://joblib.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">joblib</a></code>库来并行运行模拟，并加快进程。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="ps pt l"/></div></figure><p id="b700" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">首先，假设我们对两个模型都使用<strong class="li ja">所有变量</strong>,<em class="ny">μ(x)</em>和<em class="ny"> e(x) </em>。在这种情况下，两个模型都是<strong class="li ja">良好指定的</strong>，我们期望所有的估计器都表现良好。</p><p id="ef26" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们绘制了100次模拟中三个估计量的分布。</p><pre class="kq kr ks kt gt nn nm no np aw nq bi"><span id="1ade" class="nr ml iq nm b gy ns nt l nu nv">simulate_estimators(X_e=X, X_mu=X, D="dark_mode", y="read_time")</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="ab gu cl nh"><img src="../Images/02b45d39c80fc7b081346715f1caaf24.png" data-original-src="https://miro.medium.com/v2/format:webp/1*DJpC3x7QulHUNG5K7oq1-A.png"/></div></figure><p id="226e" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">事实上，所有的估计都是无偏的，并且给出非常相似的估计。</p><p id="bceb" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">如果我们错误地指定了两个模型中的一个会怎么样？让我们首先(正确地)假设<code class="fe nj nk nl nm b">gender</code>和<code class="fe nj nk nl nm b">age</code>影响选择<code class="fe nj nk nl nm b">dark_mode</code>的概率，并(错误地)假设只有先前的<code class="fe nj nk nl nm b">hours</code>影响每周的<code class="fe nj nk nl nm b">read_time</code>。在这种情况下，倾向得分<em class="ny"> e(x) </em>被很好地指定，而响应函数<em class="ny"> μ(x) </em>被错误地指定。</p><pre class="kq kr ks kt gt nn nm no np aw nq bi"><span id="4645" class="nr ml iq nm b gy ns nt l nu nv">simulate_estimators(X_e=['male', 'age'], X_mu=['hours'], D="dark_mode", y="read_time")</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="ab gu cl nh"><img src="../Images/4d24b16c4855516b85e1be3596f03a60.png" data-original-src="https://miro.medium.com/v2/format:webp/1*P1SHh70lWECnhzG6BX3u5Q.png"/></div></figure><p id="d489" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">不出所料，S-learner有偏见，因为我们拼错了<em class="ny"> μ(x) </em>，而IPW没有。AIPW选择两个世界中最好的<strong class="li ja"/>并且不偏不倚。</p><p id="4a4f" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">现在让我们来探究另一种<strong class="li ja">错误设定</strong>。我们(错误地)假设只有<code class="fe nj nk nl nm b">age</code>影响选择<code class="fe nj nk nl nm b">dark_mode</code>的概率，并且(正确地)假设<code class="fe nj nk nl nm b">gender</code>和之前的<code class="fe nj nk nl nm b">hours</code>都影响每周的<code class="fe nj nk nl nm b">read_time</code>。在这种情况下，倾向得分<em class="ny"> e(X) </em>被错误指定，而响应函数<em class="ny"> μ(x) </em>被正确指定。</p><pre class="kq kr ks kt gt nn nm no np aw nq bi"><span id="1456" class="nr ml iq nm b gy ns nt l nu nv">simulate_estimators(['age'], ['male', 'hours'], D="dark_mode", y="read_time")</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="ab gu cl nh"><img src="../Images/764edb7d92da64e2bacfa63ae2e6714a.png" data-original-src="https://miro.medium.com/v2/format:webp/1*b3nbjipI1agNo1jv3NCsSg.png"/></div></figure><p id="7565" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在这种情况下，S-learner是无偏的，而IPW不是，因为我们错误地指定了<em class="ny"> e(X) </em>。同样，AIPW选择了两个世界中最好的<strong class="li ja"/>并且没有偏见。</p></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="f4df" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">结论</h1><p id="8f06" class="pw-post-body-paragraph lg lh iq li b lj nc ka ll lm nd kd lo lp ne lr ls lt nf lv lw lx ng lz ma mb ij bi translated">在本文中，我们已经看到了一种估计条件平均治疗效果(CATE)的方法，即<strong class="li ja">对模型设定错误</strong>具有鲁棒性的方法:增强型反向倾向加权(AIPW)估计器。AIPW估计器从现有的两种估计器中取长补短:IPW估计器和S学习器。它需要估计倾向得分函数𝔼[D <em class="ny"> |X </em>和响应函数𝔼[ <em class="ny"> Y|X，D </em>，并且它是<strong class="li ja">无偏的</strong>，即使两个函数中的一个被错误指定。</p><p id="4e23" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">这个估计器现在是一个标准，它包含了所有最重要的因果推理包，如微软的<a class="ae lf" href="https://econml.azurewebsites.net/" rel="noopener ugc nofollow" target="_blank"> EconML </a>，优步的<a class="ae lf" href="https://causalml.readthedocs.io/" rel="noopener ugc nofollow" target="_blank"> causalml </a>和斯坦福研究人员的R包<a class="ae lf" href="https://grf-labs.github.io/grf/" rel="noopener ugc nofollow" target="_blank"> grf </a>。</p><h2 id="4546" class="nr ml iq bd mm or os dn mq ot ou dp mu lp ov ow mw lt ox oy my lx oz pa na iw bi translated">参考</h2><p id="e8dd" class="pw-post-body-paragraph lg lh iq li b lj nc ka ll lm nd kd lo lp ne lr ls lt nf lv lw lx ng lz ma mb ij bi translated">[1] J. Robins，A. Rotzniski，J. P. Zhao，<a class="ae lf" href="https://www.tandfonline.com/doi/abs/10.1080/01621459.1994.10476818" rel="noopener ugc nofollow" target="_blank">某些回归变量不总是被观测时回归系数的估计</a> (1994)，<em class="ny">美国统计协会杂志</em>。</p><p id="94f9" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">[2] A. Glyn，K. Quinn，<a class="ae lf" href="https://www.cambridge.org/core/journals/political-analysis/article/abs/an-introduction-to-the-augmented-inverse-propensity-weighted-estimator/4B1B8301E46F4432C4DCC91FE20780DB" rel="noopener ugc nofollow" target="_blank">增广逆倾向加权估计量介绍</a> (2010)，<em class="ny">政治分析</em>。</p><p id="0d5c" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">[3] E. Kennedy，<a class="ae lf" href="https://arxiv.org/abs/2004.14497" rel="noopener ugc nofollow" target="_blank">对异质因果效应的最优双重稳健估计</a> (2022)，<em class="ny">工作论文</em>。</p><h2 id="e569" class="nr ml iq bd mm or os dn mq ot ou dp mu lp ov ow mw lt ox oy my lx oz pa na iw bi translated">相关文章</h2><ul class=""><li id="7666" class="nz oa iq li b lj nc lm nd lp pu lt pv lx pw mb oe of og oh bi translated"><a class="ae lf" rel="noopener" target="_blank" href="/b63dc69e3d8c">Dag和控制变量</a></li><li id="bd22" class="nz oa iq li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">匹配、加权还是回归？</li><li id="45cc" class="nz oa iq li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated"><a class="ae lf" rel="noopener" target="_blank" href="/8a9c1e340832">了解元学习者</a></li></ul><h2 id="cdc5" class="nr ml iq bd mm or os dn mq ot ou dp mu lp ov ow mw lt ox oy my lx oz pa na iw bi translated">密码</h2><p id="f6a1" class="pw-post-body-paragraph lg lh iq li b lj nc ka ll lm nd kd lo lp ne lr ls lt nf lv lw lx ng lz ma mb ij bi translated">你可以在这里找到Jupyter的原始笔记本:</p><div class="px py gp gr pz qa"><a href="https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/aipw.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="qb ab fo"><div class="qc ab qd cl cj qe"><h2 class="bd ja gy z fp qf fr fs qg fu fw iz bi translated">Blog-Posts/aipw . ipynb at main matter courthoud/Blog-Posts</h2><div class="qh l"><h3 class="bd b gy z fp qf fr fs qg fu fw dk translated">我的中型博客文章的代码和笔记本。为matteocourthoud/Blog-Posts的发展作出贡献</h3></div><div class="qi l"><p class="bd b dl z fp qf fr fs qg fu fw dk translated">github.com</p></div></div><div class="qj l"><div class="qk l ql qm qn qj qo kz qa"/></div></div></a></div></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h2 id="28ab" class="nr ml iq bd mm or os dn mq ot ou dp mu lp ov ow mw lt ox oy my lx oz pa na iw bi translated">感谢您的阅读！</h2><p id="2f5a" class="pw-post-body-paragraph lg lh iq li b lj nc ka ll lm nd kd lo lp ne lr ls lt nf lv lw lx ng lz ma mb ij bi translated"><em class="ny">我真的很感激！</em>🤗<em class="ny">如果你喜欢这个帖子并想看更多，可以考虑</em> <a class="ae lf" href="https://medium.com/@matteo.courthoud" rel="noopener"> <strong class="li ja"> <em class="ny">关注我</em> </strong> </a> <em class="ny">。我每周发布一次与因果推断和数据分析相关的主题。我尽量让我的帖子简单而精确，总是提供代码、例子和模拟。</em></p><p id="7102" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><em class="ny">还有，一个小小的</em> <strong class="li ja"> <em class="ny">免责声明</em> </strong> <em class="ny">:我写作是为了学习所以出错是家常便饭，尽管我尽了最大努力。当你发现他们的时候，请告诉我。也很欣赏新话题的建议！</em></p></div></div>    
</body>
</html>