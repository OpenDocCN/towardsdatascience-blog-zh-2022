<html>
<head>
<title>Fine-Tuning BERT for Text Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于文本分类的微调BERT</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tuning-bert-for-text-classification-54e7df642894#2022-05-14">https://towardsdatascience.com/fine-tuning-bert-for-text-classification-54e7df642894#2022-05-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="184a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Python的分步教程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/97b20cbad74bb201b89e2271d61c40ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QSRyotwGm0jAmI4Deyjl7w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">意大利贝加莫皮佐科诺。图片作者。</p></figure><h1 id="5bea" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">目录</h1><ol class=""><li id="44f4" class="lq lr it ls b lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated"><a class="ae mi" href="#15b3" rel="noopener ugc nofollow">简介</a></li><li id="d224" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md me mf mg mh bi translated"><a class="ae mi" href="#e7cb" rel="noopener ugc nofollow">环境设置</a></li><li id="4fb5" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md me mf mg mh bi translated"><a class="ae mi" href="#8f53" rel="noopener ugc nofollow">数据集</a></li><li id="72e7" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md me mf mg mh bi translated"><a class="ae mi" href="#6ba6" rel="noopener ugc nofollow">预处理</a></li><li id="87a7" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md me mf mg mh bi translated"><a class="ae mi" href="#12df" rel="noopener ugc nofollow">数据分割</a></li><li id="4e01" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md me mf mg mh bi translated"><a class="ae mi" href="#ec34" rel="noopener ugc nofollow">列车</a></li><li id="1461" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md me mf mg mh bi translated"><a class="ae mi" href="#5067" rel="noopener ugc nofollow">预测</a></li><li id="c9ad" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md me mf mg mh bi translated"><a class="ae mi" href="#f0e7" rel="noopener ugc nofollow">结论</a></li><li id="d0c0" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md me mf mg mh bi translated"><a class="ae mi" href="#96e0" rel="noopener ugc nofollow">参考文献</a></li></ol><h1 id="15b3" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">1.介绍</h1><p id="308e" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated"><strong class="ls iu">BERT</strong><strong class="ls iu"/>(<strong class="ls iu">B</strong>I directional<strong class="ls iu">E</strong>n coder<strong class="ls iu">R</strong>presentations from<strong class="ls iu">T</strong>transformers)是一个基于transformers的机器学习模型，即能够学习单词之间上下文关系的注意力组件。</p><p id="c59f" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">自然语言处理(NLP)社区可以(至少)以两种方式利用像BERT这样强大的工具:</p><ol class=""><li id="d420" class="lq lr it ls b lt nb lv nc lx ng lz nh mb ni md me mf mg mh bi translated"><strong class="ls iu">基于特征的方法</strong> <br/> 1.1下载预训练的BERT模型。<br/> 1.2用BERT把自然语言句子变成向量表示。<br/> 1.3将预先训练好的矢量表示输入到模型中，用于下游任务(如文本分类)。</li><li id="8e91" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md me mf mg mh bi translated"><strong class="ls iu">执行微调<br/> </strong> 2.1下载一个预先训练好的BERT模型。<br/> 2.2更新下游任务的模型权重。</li></ol><p id="aa84" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">在本帖中，我们将遵循二进制文本分类示例的微调方法。我们将分享可以在<strong class="ls iu">谷歌实验室</strong>上轻松复制和执行的代码片段。</p><h1 id="e7cb" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">2.环境设置</h1><p id="cdb6" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated">虽然这不是必要的，但训练过程将受益于GPU的可用性。在Colab中，我们可以通过选择<code class="fe nj nk nl nm b">Runtime &gt; Change runtime type</code>来<strong class="ls iu">启用GPU </strong>。</p><p id="6f0a" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">然后，我们安装拥抱Face⁴ <code class="fe nj nk nl nm b">transformers</code>库如下:</p><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="8930" class="nr kz it nm b gy ns nt l nu nv">!pip install transformers</span></pre><p id="6965" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">我们导入所需的依赖项:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><h1 id="8f53" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">3.资料组</h1><p id="2b4f" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated">我们使用公共的<strong class="ls iu">垃圾短信收集数据集</strong>⁵<strong class="ls iu">⁶.<strong class="ls iu">UCI机器学习库</strong>中的</strong>这些数据由一个文本文件组成，其中包含一组被标记为垃圾短信的文本文件。来自Colab笔记本:</p><ul class=""><li id="cf58" class="lq lr it ls b lt nb lv nc lx ng lz nh mb ni md nz mf mg mh bi translated">将数据集下载为zip文件夹:</li></ul><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="6a2c" class="nr kz it nm b gy ns nt l nu nv">!wget 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'</span></pre><ul class=""><li id="cdb7" class="lq lr it ls b lt nb lv nc lx ng lz nh mb ni md nz mf mg mh bi translated">打开文件夹:</li></ul><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="54c1" class="nr kz it nm b gy ns nt l nu nv">!unzip -o smsspamcollection.zip</span></pre><ul class=""><li id="e2a5" class="lq lr it ls b lt nb lv nc lx ng lz nh mb ni md nz mf mg mh bi translated">检查数据文件的前几行:</li></ul><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="6a71" class="nr kz it nm b gy ns nt l nu nv">!head -10 SMSSpamCollection</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl oa"><img src="../Images/c705e4b16c458d93db43fbd4209b2a24.png" data-original-src="https://miro.medium.com/v2/format:webp/1*X09TphxFeEaGrEJ47uJmhA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><ul class=""><li id="f412" class="lq lr it ls b lt nb lv nc lx ng lz nh mb ni md nz mf mg mh bi translated">对于每一行，标签后跟一个制表符和原始文本消息。我们选择处理该文件以得到一个<code class="fe nj nk nl nm b">pandas.DataFrame</code> <strong class="ls iu"> </strong>对象，因为这是数据科学实验中的一个常见起点:</li></ul><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="7419" class="nr kz it nm b gy ns nt l nu nv">file_path = '/content/SMSSpamCollection'<br/>df = pd.DataFrame({'label':int(), 'text':str()}, index = [])<br/>with open(file_path) as f:<br/>  for line in f.readlines():<br/>    split = line.split('\t')<br/>    df = df.append({'label': 1 if split[0] == 'spam' else 0,<br/>                    'text': split[1]},<br/>                    ignore_index = True)<br/>df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl oa"><img src="../Images/fc7a0f5de06df7558a6ee93f16d6a9fa.png" data-original-src="https://miro.medium.com/v2/format:webp/1*PIJ-0j6_NiKQsIhUGzkRMQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><ul class=""><li id="c5e0" class="lq lr it ls b lt nb lv nc lx ng lz nh mb ni md nz mf mg mh bi translated">我们提取文本和标签值:</li></ul><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="6afb" class="nr kz it nm b gy ns nt l nu nv">text = df.text.values<br/>labels = df.label.values</span></pre><h1 id="6ba6" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">4.预处理</h1><p id="1bcb" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated">我们需要在将文本源提供给BERT之前对其进行预处理。为此，我们下载了<code class="fe nj nk nl nm b">BertTokenizer</code>:</p><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="79c9" class="nr kz it nm b gy ns nt l nu nv">tokenizer = BertTokenizer.from_pretrained(<br/>    'bert-base-uncased',<br/>    do_lower_case = True<br/>    )</span></pre><p id="1ecd" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">让我们观察一下记号赋予器如何将一个随机句子分割成单词级记号，并将它们映射到BERT词汇表中它们各自的id:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/1dc7a0aca36e8863329ddb822b9483a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*avn3Tc-28GPTKpft7Ulk5g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="ae41" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">BERT需要以下预处理步骤:</p><ol class=""><li id="19b3" class="lq lr it ls b lt nb lv nc lx ng lz nh mb ni md me mf mg mh bi translated">添加<strong class="ls iu">特殊记号</strong> : <br/> - <code class="fe nj nk nl nm b">[CLS]</code>:每句话首(ID <code class="fe nj nk nl nm b">101</code> ) <br/> - <code class="fe nj nk nl nm b">[SEP]</code>:每句话尾(ID <code class="fe nj nk nl nm b">102</code>)</li><li id="5966" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md me mf mg mh bi translated">使<strong class="ls iu">的句子具有相同的长度</strong>:<br/>——这是通过<em class="ny">填充</em>来实现的，即向较短的序列添加方便的值，以匹配期望的长度。较长的序列被截断。<br/> -填充(<code class="fe nj nk nl nm b">[PAD]</code>)令牌具有ID <code class="fe nj nk nl nm b">0</code>。<br/> -允许的最大序列长度为512个令牌。</li><li id="7dd2" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md me mf mg mh bi translated">创建一个<strong class="ls iu">注意力屏蔽</strong>:<br/>-0/1列表，指示模型在学习它们的上下文表示时是否应该考虑记号。我们期望<code class="fe nj nk nl nm b">[PAD]</code>令牌具有值<code class="fe nj nk nl nm b">0</code>。</li></ol><p id="62ea" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">该过程可以表示如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/c63c36df1b7ed47b0efae511ae9117db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vaw98m1VVncgKxNFWI0d2Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="4452" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">我们可以通过使用<code class="fe nj nk nl nm b">tokenizer.encode_plus</code> ⁷方法来执行所有需要的步骤。当被调用时，它返回一个带有以下字段的<code class="fe nj nk nl nm b">transformers.tokenization.tokenization-utils_base.BatchEncoding</code>对象:</p><ul class=""><li id="56eb" class="lq lr it ls b lt nb lv nc lx ng lz nh mb ni md nz mf mg mh bi translated"><code class="fe nj nk nl nm b">input_ids</code>:令牌id列表。</li><li id="14ad" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md nz mf mg mh bi translated"><code class="fe nj nk nl nm b">token_type_ids</code>:令牌类型id列表。</li><li id="5af8" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md nz mf mg mh bi translated"><code class="fe nj nk nl nm b">attention_mask</code>:0/1列表，指示模型应该考虑哪些令牌(<code class="fe nj nk nl nm b">return_attention_mask = True</code>)。</li></ul><p id="a8c3" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">当我们选择<code class="fe nj nk nl nm b">max_length = 32</code>时，较长的句子将被截断，而较短的句子将用<code class="fe nj nk nl nm b">[PAD]</code>标记(id: <code class="fe nj nk nl nm b">0</code>)填充，直到它们达到期望的长度。</p><p id="b783" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><em class="ny">注意</em>:使用<code class="fe nj nk nl nm b">tokenizer.encode_plus</code>方法的想法(以及它的代码)是从这篇文章中借用的:<em class="ny">克里斯·麦考密克和尼克·瑞恩的《PyTorch⁸的伯特微调教程</em>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="7b97" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">我们可以观察文本样本的标记id，并识别特殊标记<code class="fe nj nk nl nm b">[CLS]</code>和<code class="fe nj nk nl nm b">[SEP]</code>的存在，以及达到期望的<code class="fe nj nk nl nm b">max_length</code>的填充<code class="fe nj nk nl nm b">[PAD]</code>:</p><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="6387" class="nr kz it nm b gy ns nt l nu nv">token_id[6]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/b536f83ebc6c876114760d37e5c95712.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I--QXIaxEu9kJT2_UQQK5w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="fba6" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">我们还可以通过检查令牌、它们的id和随机文本样本的注意掩码来验证<code class="fe nj nk nl nm b">tokenizer.encode_plus</code>的输出，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/eec81c38b2cf9d5d6dd8ba861a9ce34d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*8m64rQkaIBRs76rP74tJuQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="9c37" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><em class="ny">注意</em> : BERT是一种具有绝对位置嵌入的模型，因此通常建议将输入填充在右边(序列的结尾)而不是左边(序列的开头)。在我们的例子中，<code class="fe nj nk nl nm b">tokenizer.encode_plus</code>负责所需的预处理。</p><h1 id="12df" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">5.数据分割</h1><p id="eb64" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated">我们将数据集分为训练集(80%)和验证集(20%)，并将它们包装在一个<code class="fe nj nk nl nm b">torch.utils.data.DataLoader</code>对象周围。凭借其直观的语法，<code class="fe nj nk nl nm b">DataLoader</code>在给定的数据集上提供了一个iterable。</p><p id="8884" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">有关<code class="fe nj nk nl nm b">DataLoader</code>的更多信息，请点击此处:</p><ul class=""><li id="c8f1" class="lq lr it ls b lt nb lv nc lx ng lz nh mb ni md nz mf mg mh bi translated"><em class="ny">数据集&amp;数据加载器— Pytorch教程</em> ⁹</li><li id="995b" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md nz mf mg mh bi translated"><em class="ny">数据加载器文档</em> ⁰</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><h1 id="ec34" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">6.火车</h1><p id="d08c" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated">现在是微调任务的时候了:</p><ul class=""><li id="a6dc" class="lq lr it ls b lt nb lv nc lx ng lz nh mb ni md nz mf mg mh bi translated">根据BERT论文中的建议选择超参数:</li></ul><blockquote class="of og oh"><p id="3916" class="mo mp ny ls b lt nb ju mq lv nc jx mr oi nd mt mu oj ne mw mx ok nf mz na md im bi translated">最佳超参数值因任务而异，但我们发现以下可能值范围适用于所有任务:</p><p id="c6f1" class="mo mp ny ls b lt nb ju mq lv nc jx mr oi nd mt mu oj ne mw mx ok nf mz na md im bi translated">-批量:16个，32个</p><p id="3766" class="mo mp ny ls b lt nb ju mq lv nc jx mr oi nd mt mu oj ne mw mx ok nf mz na md im bi translated">-学习率(Adam): 5e-5，3e-5，2e-5</p><p id="94b5" class="mo mp ny ls b lt nb ju mq lv nc jx mr oi nd mt mu oj ne mw mx ok nf mz na md im bi translated">-历元数:2、3、4</p></blockquote><ul class=""><li id="4842" class="lq lr it ls b lt nb lv nc lx ng lz nh mb ni md nz mf mg mh bi translated">定义一些函数来评估培训过程中的验证指标(准确度、精密度、召回率和特异性):</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/f2a15edc23194c0d28c8b466956d8fa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qX3sjK1ghynPrbL2tMRQNQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><ul class=""><li id="fa9c" class="lq lr it ls b lt nb lv nc lx ng lz nh mb ni md nz mf mg mh bi translated">下载<code class="fe nj nk nl nm b">transformers.BertForSequenceClassification</code>，这是一个BERT模型，在汇集的输出之上有一个用于句子分类(或回归)的线性层:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="07c8" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><em class="ny">注意</em>:最好在有GPU的情况下运行本笔记本。为了在CPU上执行它，我们应该在上面的代码片段中注释<code class="fe nj nk nl nm b">model.cuda()</code>以避免运行时错误。</p><ul class=""><li id="b5f3" class="lq lr it ls b lt nb lv nc lx ng lz nh mb ni md nz mf mg mh bi translated">执行培训程序:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/859104e6b70bdd5fd9d97a046eed5d7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vcduBc3eUMJuYtWfdTLDzg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">训练日志。图片作者。</p></figure><h1 id="5067" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">7.预测</h1><p id="d82b" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated">在训练过程之后，在测试集上评估模型的性能是一个很好的实践。出于这个例子的目的，我们简单地预测一个新文本样本的类别(<em class="ny">火腿</em>对<em class="ny">垃圾邮件</em>):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/07a17ee9ce2c771bb08ec0a94b424d21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lq0LxeIHJ28LzU4aOWkLOA.png"/></div></div></figure><h1 id="f0e7" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">8.结论</h1><p id="8bc2" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated">在本文中，我们对分类任务的BERT进行了微调。我们分享了可以在<strong class="ls iu"> Google Colab </strong>(或其他环境)上轻松复制和执行的代码片段。</p><p id="4aa4" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">深度学习框架已经配备了像BERT这样流行的NLP转换器的实现。<strong class="ls iu"> TensorFlow </strong>和<strong class="ls iu"> PyTorch </strong>提供了一组预先训练好的模型和直观的API，以方便它们的采用和执行微调任务。此外，像<strong class="ls iu">拥抱脸</strong> ⁴这样的人工智能社区使得访问大型模型中枢和简单的界面成为可能。</p><p id="7471" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">最后，我们分享一些有用的资源，从中可以找到更多与该主题相关的示例和信息:</p><ul class=""><li id="c7e3" class="lq lr it ls b lt nb lv nc lx ng lz nh mb ni md nz mf mg mh bi translated"><em class="ny"> TensorFlow教程:微调BERT模型</em></li><li id="72d6" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md nz mf mg mh bi translated"><em class="ny">tensor flow Hub上的BERT模型</em></li><li id="33f2" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md nz mf mg mh bi translated"><em class="ny"> PyTorch变形金刚⁴ </em></li><li id="144b" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md nz mf mg mh bi translated"><em class="ny">拥抱脸:变形金刚笔记本⁵ </em></li><li id="f3f0" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md nz mf mg mh bi translated"><em class="ny">拥抱脸:模特枢纽⁶ </em></li><li id="2edb" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md nz mf mg mh bi translated"><em class="ny">伯特与PyTorch⁸微调教程</em>:借用了本帖中<code class="fe nj nk nl nm b">tokenizer.encode_plus</code>的用法。</li><li id="7b87" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md nz mf mg mh bi translated">第一次使用BERT的视觉指南，⁷，作者Jay Alammar。</li></ul><p id="c5b4" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">在之前的帖子⁸中，我们也在一个<strong class="ls iu">多类</strong>文本分类任务中使用了BERT和<strong class="ls iu"> TensorFlow </strong>。</p><h1 id="96e0" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">9.参考</h1><p id="ee96" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated">[1]德夫林，雅各布；张明伟；李，肯顿；图塔诺娃、克里斯蒂娜、<em class="ny">伯特:用于语言理解的深度双向变压器的预训练</em>，2018、<a class="ae mi" href="https://en.wikipedia.org/wiki/ArXiv_%28identifier%29" rel="noopener ugc nofollow" target="_blank">arXiv</a>:<a class="ae mi" href="https://arxiv.org/abs/1810.04805v2" rel="noopener ugc nofollow" target="_blank">1810.04805 v2</a></p><p id="901c" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[2]阿希什·瓦斯瓦尼、诺姆·沙泽尔、尼基·帕尔马、雅各布·乌兹科雷特、利永·琼斯、艾丹·戈麦斯、卢卡兹·凯泽、伊利亚·波洛舒欣，“<em class="ny">关注是你所需要的全部</em>”，2017年，<a class="ae mi" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> arXiv:1706.03762 </a></p><p id="e0a0" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><a class="ae mi" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank">https://colab.research.google.com/</a></p><p id="0624" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><a class="ae mi" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/</a></p><p id="1114" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><a class="ae mi" href="https://archive.ics.uci.edu/ml/datasets/sms+spam+collection" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/datasets/sms+spam+collection</a></p><p id="1dd6" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[6]<a class="ae mi" href="https://archive.ics.uci.edu/ml/index.php" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/index.php</a></p><p id="3032" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[7]<a class="ae mi" href="https://huggingface.co/docs/transformers/v4.18.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode_plus" rel="noopener ugc nofollow" target="_blank">https://hugging face . co/docs/transformers/v 4 . 18 . 0/en/internal/token ization _ utils # transformers。pretrainedtokenizerbase . encode _ plus</a></p><p id="ece6" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[8]克里斯·麦考密克和尼克·瑞安。(2019年7月22日)。<em class="ny">使用PyTorch的BERT微调教程</em>。从http://www.mccormickml.com<a class="ae mi" href="http://www.mccormickml.com/" rel="noopener ugc nofollow" target="_blank">取回</a>。</p><p id="c2d3" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[9]<a class="ae mi" href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#preparing-your-data-for-training-with-dataloaders" rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/beginner/basics/data _ tutorial . html #使用数据加载器准备培训数据</a></p><p id="4756" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[10]<a class="ae mi" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" rel="noopener ugc nofollow" target="_blank">https://py torch . org/docs/stable/data . html # torch . utils . data . data loader</a></p><p id="444f" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[11]<a class="ae mi" href="https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#bertforsequenceclassification" rel="noopener ugc nofollow" target="_blank">https://hugging face . co/transformers/v 3 . 0 . 2/model _ doc/Bert . html # bertforsequenceclassification</a></p><p id="5729" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[12]<a class="ae mi" href="https://www.tensorflow.org/text/tutorials/fine_tune_bert" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/text/tutorials/classify _ text _ with _ Bert</a></p><p id="7131" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><a class="ae mi" href="https://tfhub.dev/s?q=bert" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/s?q=bert</a></p><p id="25ee" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><a class="ae mi" href="https://pytorch.org/hub/huggingface_pytorch-transformers/" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/hub/huggingface_pytorch-transformers/</a></p><p id="6a85" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><a class="ae mi" href="https://huggingface.co/docs/transformers/notebooks" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/docs/transformers/notebooks</a></p><p id="8899" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><a class="ae mi" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/models</a></p><p id="d12e" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[17]<a class="ae mi" href="https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/" rel="noopener ugc nofollow" target="_blank">https://jalammar . github . io/a-visual-guide-to-use-Bert-for-first-time/</a></p><p id="0146" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[18]<a class="ae mi" rel="noopener" target="_blank" href="/multi-label-text-classification-using-bert-and-tensorflow-d2e88d8f488d">https://towards data science . com/multi-label-text-class ification-using-Bert-and-tensor flow-D2 e88d 8 f 488d</a></p></div></div>    
</body>
</html>