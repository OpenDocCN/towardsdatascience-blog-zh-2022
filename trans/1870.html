<html>
<head>
<title>Create a Gradient Descent Algorithm with Regularization from Scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python从头开始创建正则化梯度下降算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/create-a-gradient-descent-algorithm-with-regularization-from-scratch-in-python-571cb1b46642#2022-05-01">https://towardsdatascience.com/create-a-gradient-descent-algorithm-with-regularization-from-scratch-in-python-571cb1b46642#2022-05-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d512" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过自己实现来巩固你的梯度下降知识</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/bf1f1f022b969fbdaae913132f50c76f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*d9uP3lRksNU9fnAM"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">安德烈·伯恩哈特在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="bff2" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="eb54" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">梯度下降是用于机器学习和优化问题的基本算法。因此，充分理解它的功能和局限性对于任何学习机器学习或数据科学的人来说都是至关重要的。本教程将实现一个从头开始的梯度下降算法，在一个简单的模型优化问题上测试它，最后进行调整以演示参数正则化。</p><h1 id="180f" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">背景</h1><p id="f5a3" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">梯度下降试图通过调整模型参数找到<strong class="lq ir">成本函数</strong>的局部最小值。<strong class="lq ir">成本函数</strong>(或损失函数)将变量映射到一个代表“成本”或要最小化的值的实数上。</p><p id="270e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对于我们的模型优化，我们将执行<strong class="lq ir">最小二乘优化</strong>，其中我们寻求最小化我们的预测值和数据值之间的差异总和。<strong class="lq ir">等式1 </strong>表示我们将使用的二次成本函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/9eab838264c36d2ec3ce9856eb6974a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JCSegVSqJ253Xvmdb44WQA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="5dfa" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">等式1: </strong> <em class="mq">最小二乘优化代价函数。</em></p><p id="4987" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这里，yhat是独立变量的模型预测。对于此分析，我们将使用一个通用多项式模型，如<strong class="lq ir">等式2 </strong>所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/5fedd57d03acf27e8152b1b970d9b89e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F5h7ctKkqcVL6K-1neVFWw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="8320" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">方程2: </strong> <em class="mq">本次分析使用的一般多项式模型。</em></p><p id="81c3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为简单起见，我们将保持这些方程为矩阵形式。这样做在<strong class="lq ir">等式3 </strong>中呈现了我们的新模型，在<strong class="lq ir">等式4 </strong>中呈现了X矩阵结构。请注意，yhat的大小为(n，)，beta的大小为(m，)，X的大小为(n，m)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/3b4d2876a95ead7f85a7951160dff471.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*j9I4EAc7354HEpxFAMjcBg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="b458" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">方程3: </strong> <em class="mq">我们模型的矩阵形式。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/c4edab8d8f2f32cfd0fbb078dc9c2252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VXF5XjgLboYP0-tkugW8tA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="c3f8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">方程4: </strong> <em class="mq">多项式矩阵，X. </em></p><p id="aee0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，有了成本函数和我们将要部署的模型的背景，我们终于可以开始研究梯度下降算法了。</p><h1 id="3ffe" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">算法</h1><p id="1a5b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">梯度下降的工作原理是计算成本的<em class="mq">梯度</em>，调整参数使<em class="mq">像斜坡一样下降</em>梯度。</p><p id="8128" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">链式法则(回想一下多变量微积分)为我们提供了一种方法来估算给定参数变化的成本变化。这种关系在<strong class="lq ir">等式5 </strong>中给出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/4c64cab57d2aaff5f776ff500bbc0545.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fbWVovPzUKbTktpm8dnQRw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="f497" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">方程式5: </strong> <em class="mq">应用链式法则来确定因参数变化而引起的成本变化。</em></p><p id="5547" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">知道了这一点，我们可以定义一个与成本梯度成比例的参数变化，如<strong class="lq ir">等式6 </strong>所示。学习率(eta)被选择为一个小的正数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/89239ed7e012de756c2bade4468e5136.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*5TbUlnF5qN3Liop_splBVQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="7ec3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">方程式6: </strong> <em class="mq">参数更新规则。</em></p><p id="9257" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">当这个更新参数的规则被插入<strong class="lq ir">等式5 </strong>时，我们得到我们的证明，即所选择的参数更新规则将总是降低成本。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/8816d357b9237dccf52e70afc71825f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XiEQ-QAHIlUyTQgq27C0wg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="04b4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">等式7: </strong> <em class="mq">证明参数更新规则会降低成本。</em></p><p id="9004" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果我们回忆一下线性代数，我们可以记住成本梯度向量的平方永远是正的。因此，假设学习率足够小，这种更新方法将<em class="mq">降低成本函数的梯度</em>。</p><p id="515a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，为了最终实现这个算法，我们需要一个数值计算梯度的方法。对于这个例子，我们可以拿着笔和纸做导数，但是我们希望我们的算法适用于任何模型和成本函数。<strong class="lq ir">等式8 </strong>给出了我们这样做的方法，我们将通过一个小值调整每个参数，并观察成本的变化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mx"><img src="../Images/8430b0684e85fd1179cd86dc6364bcb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HZTrn86sSFHmRQd0wbovFg.png"/></div></div></figure><p id="23b6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">方程式8: </strong> <em class="mq">计算成本梯度的数值方法。</em></p><h1 id="9ab8" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">数据</h1><p id="806e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们将为这个项目生成自己的数据集。我们将简单地生成一个独立值的线性间隔向量，并根据这些向量计算因变量，同时引入一些噪声。我设置了一个随机种子，让你看看你是否得到同样的结果。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="a031" class="nd kx iq mz b gy ne nf l ng nh">import numpy as np<br/>import matplotlib.pyplot as plt<br/>np.random.seed(1234)</span><span id="9c16" class="nd kx iq mz b gy ni nf l ng nh">def polynomial_model(beta, x):<br/>    '''<br/>    A polynomial model.<br/>    beta: numpy array of parameters of size (m,)<br/>    x: numpy array of size (n,)</span><span id="c3ce" class="nd kx iq mz b gy ni nf l ng nh">    return yhat: prediction of the model of size (n,)<br/>    '''<br/>    # Turn x (n,) to X (n, m) where m is the order of the polynomial<br/>    # The second axis is the value of x**m<br/>    X = x[:, np.newaxis] ** np.arange(0, len(beta))</span><span id="40ac" class="nd kx iq mz b gy ni nf l ng nh">    # Perform model prediction<br/>    yhat = np.sum(beta * X, axis=1)</span><span id="21d3" class="nd kx iq mz b gy ni nf l ng nh">    return yhat</span><span id="4ac4" class="nd kx iq mz b gy ni nf l ng nh"># Construct a dataset<br/>x = np.arange(-2, 3)<br/>beta_actual = [1, 2, 1]<br/>y = polynomial_model(beta_actual, x) + np.random.normal(size=x.size, scale=1)</span></pre><p id="01ab" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我选择实际的模型参数为[1，2，3]，噪声为标准偏差为1的正态分布。让我们看看下面的数据。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="1666" class="nd kx iq mz b gy ne nf l ng nh"># Plot results<br/>fig, ax = plt.subplots()<br/>ax.plot(x, y, '.')<br/>xplt = np.linspace(min(x), max(x), 100)<br/>yplt = polynomial_model(beta_actual, xplt)<br/>plt.plot(xplt, yplt, '-')</span><span id="1f28" class="nd kx iq mz b gy ni nf l ng nh">ax.legend(['Data', 'Actual Relationship'])</span><span id="7976" class="nd kx iq mz b gy ni nf l ng nh">plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/145fe7217b0636d4a88ddd30ecba8610.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*xjZIkDFOKfGmjL4iTcOKaQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="c3a7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">图1: </strong> <em class="mq">我们的数据和实际模型。</em></p><h1 id="50a9" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">模型创建</h1><h1 id="b8ff" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">功能</h1><p id="e04e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们的成本函数定义如下。请注意，我们只将beta设为位置参数，其余的我们将通过关键字参数传递。这是为了提高最终梯度下降算法的可读性，我们将在后面看到。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="2735" class="nd kx iq mz b gy ne nf l ng nh">def cost(beta, **kwargs):<br/>    """<br/>    Calculates the quadratic cost, with an optional regularization<br/>    :param beta: Model Parameters<br/>    :param kwargs:<br/>    :return:<br/>    """<br/>    x = kwargs['x']<br/>    y = kwargs['y']<br/>    model = kwargs['model']</span><span id="4e00" class="nd kx iq mz b gy ni nf l ng nh">    # Calculate predicted y given parameters<br/>    yhat = model(beta, x)</span><span id="74a4" class="nd kx iq mz b gy ni nf l ng nh">    # Calculate the cost<br/>    C = sum((y-yhat)**2) / len(y)<br/>    return C</span></pre><h1 id="adce" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">算法</h1><p id="e258" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们的梯度下降类需要我们的模型、成本函数、初始参数猜测和我们的数据。我们还可以调整参数，如学习率或步进参数，以计算梯度，但对于这个分析，我将它们设置为足够小的数字，并没有优化它们的值。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="714a" class="nd kx iq mz b gy ne nf l ng nh">class GradDescent:</span><span id="bafb" class="nd kx iq mz b gy ni nf l ng nh">    def __init__(self, model, C, beta0, x, y, dbeta=1E-8, eta=0.0001, ftol=1E-8):<br/>        self.model = model<br/>        self.C = C<br/>        self.beta = beta0<br/>        self.x = x<br/>        self.y = y<br/>        self.dbeta = dbeta<br/>        self.eta = eta<br/>        self.ftol = ftol</span></pre><p id="29c3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在我们终于可以实现梯度下降算法了。我们将首先创建一个成本函数的输入字典，它不会因迭代而改变。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="1ce6" class="nd kx iq mz b gy ne nf l ng nh">def descend(self):<br/>        # This dict of cost parameters does not change between calls<br/>        cost_inputs = {'x': self.x,<br/>                       'y': self.y,<br/>                       'model': self.model<br/>                       }</span></pre><p id="b4f5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">接下来，我们将初始化一个成本列表，并开始迭代。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="6d83" class="nd kx iq mz b gy ne nf l ng nh"># Initialize a list of costs, with the indices being the iteration<br/>        costs = [self.C(self.beta, **cost_inputs)]</span><span id="5ed3" class="nd kx iq mz b gy ni nf l ng nh">        run_condition = True</span></pre><p id="a7d5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对于每次迭代，我们必须:</p><ol class=""><li id="bdbb" class="nk nl iq lq b lr mk lu ml lx nm mb nn mf no mj np nq nr ns bi translated">计算梯度</li><li id="f26f" class="nk nl iq lq b lr nt lu nu lx nv mb nw mf nx mj np nq nr ns bi translated">更新参数</li><li id="4c6b" class="nk nl iq lq b lr nt lu nu lx nv mb nw mf nx mj np nq nr ns bi translated">计算新成本</li><li id="5290" class="nk nl iq lq b lr nt lu nu lx nv mb nw mf nx mj np nq nr ns bi translated">评估我们的运行状况</li></ol><p id="8403" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这一过程如下所示</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="75cf" class="nd kx iq mz b gy ne nf l ng nh">while run_condition:<br/>            # Get the gradient of the cost<br/>            delC = []</span><span id="5d08" class="nd kx iq mz b gy ni nf l ng nh">            for n, beta_n in enumerate(self.beta):<br/>                # Create a temporary parameters vector, to change the nth parameter<br/>                temp_beta = self.beta<br/>                temp_beta[n] = beta_n + self.dbeta  # Adjusts the nth parameter by dbeta<br/>                C_n = self.C(temp_beta, **cost_inputs)<br/>                dC = C_n - costs[-1]<br/>                delC.append(dC / self.dbeta)</span><span id="3b64" class="nd kx iq mz b gy ni nf l ng nh">            # Update the parameters<br/>            self.beta = self.beta - self.eta * np.array(delC)</span><span id="9423" class="nd kx iq mz b gy ni nf l ng nh">            # Re calc C<br/>            costs.append(self.C(self.beta, **cost_inputs))</span><span id="4d73" class="nd kx iq mz b gy ni nf l ng nh">            # Evaluate running condition<br/>            run_condition = abs(costs[-1] - costs[-2]) &gt; self.ftol</span></pre><p id="7006" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在我们准备实现我们的模型。让我们初始化初始参数，创建一个梯度下降对象，优化我们的模型，并绘制结果。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="3925" class="nd kx iq mz b gy ne nf l ng nh"># Initialize parameters, use a polynomial of order 5<br/>beta0 = np.random.normal(size=(5,), scale=1)</span><span id="829e" class="nd kx iq mz b gy ni nf l ng nh"># Initialize a GradDescent object, perform descent and get parameters<br/>gd = GradDescent(polynomial_model, cost, beta0, x, y)<br/>gd.descend()</span><span id="3dae" class="nd kx iq mz b gy ni nf l ng nh">beta = gd.beta</span><span id="b754" class="nd kx iq mz b gy ni nf l ng nh"># Make model prediction with parameters<br/>yhat = polynomial_model(beta, x)</span><span id="521b" class="nd kx iq mz b gy ni nf l ng nh"># Plot results<br/>fig, ax = plt.subplots()<br/>ax.plot(x, y, '.')<br/>ax.plot(x, yhat, 'x')<br/>xplt = np.linspace(min(x), max(x), 100)<br/>yplt = polynomial_model(beta_actual, xplt)<br/>plt.plot(xplt, yplt, '-')<br/>yplt = polynomial_model(beta, xplt)<br/>plt.plot(xplt, yplt, '--')</span><span id="4bd5" class="nd kx iq mz b gy ni nf l ng nh">ax.legend(['Data', 'Predicted Values', 'Actual Relationship', 'Predicted Model'])</span><span id="101b" class="nd kx iq mz b gy ni nf l ng nh">plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/9d314f66bd798b5ff9c376636f766b07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*4cfEi_60lo7weCUIAMgLug.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="e16b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">图2: </strong> <em class="mq">我们数据的4阶多项式拟合。</em></p><p id="d650" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">请注意，我们的模型是故意过度拟合的。我们有一个适合5个数据点的4阶多项式，回想一下，一个n阶多项式总是可以完美地预测n+1个数据点，而无需考虑任何基础模型。</p><p id="da0f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们稍微修改我们的成本函数，以惩罚参数的大小。这个过程被称为<strong class="lq ir">正则化</strong>，正则化被定义为添加信息以解决不适定问题以防止过度拟合的过程。我们将执行两种类型的正则化，<strong class="lq ir"> L1 </strong>或<strong class="lq ir">套索回归</strong>(最小绝对收缩和选择算子)和<strong class="lq ir"> L2 </strong>或<strong class="lq ir">岭回归</strong>。这些技术的修改成本函数在下面的<strong class="lq ir">等式9 &amp; 10 </strong>中给出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/bed2a0e23a7380e63cfcf8f6cfbe847f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OU-fS_Q9UZxFgoxKAtzUNA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="2aac" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">方程9:</strong><em class="mq">L1正则化的代价函数。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/fdba0c16d76b62c492140de817786bff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ABjSpJQN7mzIhOgP6bwHXQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="fbfa" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">方程10:</strong><em class="mq">L2正则化的代价函数。</em></p><p id="a0ee" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们可以很容易地修改代码来处理这些正则化技术。唯一的变化将发生在成本函数和GradDescent对象中，如下所示。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="c26e" class="nd kx iq mz b gy ne nf l ng nh">class GradDescent:</span><span id="02e4" class="nd kx iq mz b gy ni nf l ng nh">    def __init__(self, model, C, beta0, x, y, reg=None, lmda=0, dbeta=1E-8, eta=0.0001, ftol=1E-8):<br/>        self.model = model<br/>        self.C = C<br/>        self.beta = beta0<br/>        self.x = x<br/>        self.y = y<br/>        self.reg = reg<br/>        self.lmda = lmda<br/>        self.dbeta = dbeta<br/>        self.eta = eta<br/>        self.ftol = ftol<br/></span><span id="f292" class="nd kx iq mz b gy ni nf l ng nh">    def descend(self):<br/>        # This dict of cost parameters does not change between calls<br/>        cost_inputs = {'x': self.x,<br/>                       'y': self.y,<br/>                       'reg': self.reg,<br/>                       'lmda': self.lmda,<br/>                       'model': self.model<br/>                       }<br/>        # Initialize a list of costs, with the indices being the iteration<br/>        costs = [self.C(self.beta, **cost_inputs)]</span><span id="aec3" class="nd kx iq mz b gy ni nf l ng nh">        run_condition = True<br/>        while run_condition:</span><span id="f7e7" class="nd kx iq mz b gy ni nf l ng nh">            # Get the gradient of the cost<br/>            delC = []</span><span id="afb8" class="nd kx iq mz b gy ni nf l ng nh">            for n, beta_n in enumerate(self.beta):<br/>                # Create a temporary parameters vector, to change the nth parameter<br/>                temp_beta = self.beta<br/>                temp_beta[n] = beta_n + self.dbeta  # Adjusts the nth parameter by dbeta<br/>                C_n = self.C(temp_beta, **cost_inputs)<br/>                dC = C_n - costs[-1]<br/>                delC.append(dC / self.dbeta)</span><span id="4c7f" class="nd kx iq mz b gy ni nf l ng nh">            # Update the parameters<br/>            self.beta = self.beta - self.eta * np.array(delC)</span><span id="2098" class="nd kx iq mz b gy ni nf l ng nh">            # Re calc C<br/>            costs.append(self.C(self.beta, **cost_inputs))</span><span id="02f5" class="nd kx iq mz b gy ni nf l ng nh">            # Evaluate running condition<br/>            run_condition = abs(costs[-1] - costs[-2]) &gt; self.ftol</span><span id="f903" class="nd kx iq mz b gy ni nf l ng nh">def cost(beta, **kwargs):<br/>    """<br/>    Calculates the quadratic cost, with an optional regularization<br/>    :param beta: Model Parameters<br/>    :param kwargs:<br/>    :return:<br/>    """<br/>    x = kwargs['x']<br/>    y = kwargs['y']<br/>    reg = kwargs['reg']<br/>    lmda = kwargs['lmda']<br/>    model = kwargs['model']</span><span id="6714" class="nd kx iq mz b gy ni nf l ng nh">    # Calculate predicted y given parameters<br/>    yhat = model(beta, x)</span><span id="76e6" class="nd kx iq mz b gy ni nf l ng nh">    # Calculate the cost<br/>    C = sum((y-yhat)**2) / len(y)<br/>    if reg is not None:<br/>        if reg == 'L1':  # For Lasso Regression (L1), add the magnitudes<br/>            C += lmda * sum(abs(beta))<br/>        elif reg == 'L2':  # For Ridge Regression (L2), add the squared magnitude<br/>            C += lmda * sum(beta**2)<br/>    return C</span></pre><p id="9e01" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们做一点调整，看看这如何影响我们预测模型的偏差和方差。该代码如下所示，比较图见图3<strong class="lq ir">。</strong></p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="c8ce" class="nd kx iq mz b gy ne nf l ng nh">fig, axs = plt.subplots(1, 3, figsize=(15, 5))<br/></span><span id="dd9e" class="nd kx iq mz b gy ni nf l ng nh">for i, (reg, lmda) in enumerate(zip([None, 'L1', 'L2'], [0, 1, 1])):<br/>    # Initialize a GradDescent object, perform descent and get parameters<br/>    gd = GradDescent(polynomial_model, cost, beta0, x, y, reg=reg, lmda=lmda)<br/>    gd.descend()</span><span id="d10b" class="nd kx iq mz b gy ni nf l ng nh">    beta = gd.beta</span><span id="20e9" class="nd kx iq mz b gy ni nf l ng nh">    # Make model prediction with parameters<br/>    yhat = polynomial_model(beta, x)</span><span id="9b9c" class="nd kx iq mz b gy ni nf l ng nh">    axs[i].plot(x, y, '.')<br/>    axs[i].plot(x, yhat, 'x')<br/>    xplt = np.linspace(min(x), max(x), 100)<br/>    yplt = polynomial_model(beta_actual, xplt)<br/>    axs[i].plot(xplt, yplt, '--')<br/>    yplt = polynomial_model(beta, xplt)<br/>    axs[i].plot(xplt, yplt, '--')</span><span id="c4f7" class="nd kx iq mz b gy ni nf l ng nh">    # Set title<br/>    if reg is not None:<br/>        axs[i].set_title(reg)<br/>    else:<br/>        axs[i].set_title("No Regularization")</span><span id="3abb" class="nd kx iq mz b gy ni nf l ng nh">    # Clean up the plots - remove x,y ticks and labels<br/>    axs[i].axes.xaxis.set_ticklabels([])<br/>    axs[i].axes.yaxis.set_ticklabels([])<br/>    axs[i].axes.xaxis.set_visible(False)<br/>    axs[i].axes.yaxis.set_visible(False)<br/></span><span id="0627" class="nd kx iq mz b gy ni nf l ng nh">fig.legend(['Data', 'Predicted Values', 'Actual Relationship', 'Predicted Model'])</span><span id="9301" class="nd kx iq mz b gy ni nf l ng nh">plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/8b77b6f4492e235d888bc34eede6847f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SDI0OqqrXDlYJaAK0AEFzA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="8530" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">图3: </strong> <em class="mq">正则化方法对比。</em></p><p id="6a8c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们可以定性地看到，调整我们的参数提高了我们的拟合度(即，使其更接近实际的基础模型。)尝试迭代参数权重，看看它对最终模型的影响。</p><h1 id="2616" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="5b03" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们学习了梯度下降的基本原理，并用Python实现了一个简单的算法。这样做后，我们做了最小的改变，增加正则化方法到我们的算法，并了解了L1和L2正则化。我希望你喜欢。</p><p id="efbf" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><a class="ae kv" href="https://github.com/turnerluke/ML-algos/blob/main/gradient_descent/GradDescent.py" rel="noopener ugc nofollow" target="_blank">在GitHub上看到这个项目</a> <br/> <a class="ae kv" href="https://www.linkedin.com/in/turnermluke/" rel="noopener ugc nofollow" target="_blank">在LinkedIn上与我联系</a> <br/> <a class="ae kv" href="https://medium.com/@turnermluke" rel="noopener">阅读我的一些其他数据科学文章</a></p></div></div>    
</body>
</html>