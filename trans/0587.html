<html>
<head>
<title>How does linear regression really work?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归到底是怎么工作的？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-does-linear-regression-really-work-2387d0f11e8#2022-02-23">https://towardsdatascience.com/how-does-linear-regression-really-work-2387d0f11e8#2022-02-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3f82" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">普通最小二乘法背后的数学和直觉(OLS)</h2></div><p id="7c62" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你知道线性回归如何在“保持其他一切不变”的情况下衡量效果吗？或者它如何最小化误差平方和？在这篇文章中，我们将讨论如何，以及更多。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/718453f1567b212c63bdf079400ba584.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DUrPH_BZdju6sUgq"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">照片由<a class="ae lr" href="https://unsplash.com/@datingscout?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Datingscout </a>在<a class="ae lr" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="1608" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将利用矩阵代数和 python 来理解正在发生的事情。可能的话，我们会深入。但是，由于主题的复杂性，我们还会链接更多信息的外部资源。</p><p id="ac0e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章的主要来源是斯坦福大学统计课的课堂笔记。</p><p id="a6f8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">事不宜迟，让我们开始吧…</p><h1 id="5184" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">技术 TLDR</h1><p id="f7e7" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">普通最小二乘(OLS)线性回归通过最小化残差的平方来拟合数据。为此，我们…</p><ol class=""><li id="3daa" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la mu mv mw mx bi translated">用公式表示线性回归方程的残差。</li><li id="493d" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">求残差平方和的导数。</li><li id="9799" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">求解上面导数为零的β系数值。</li></ol><p id="34bd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过平方误差，我们的误差曲线变成一条向上的抛物线。因此，如果我们求解最优解，我们保证会求解全局最小值。</p><h1 id="9c7e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">1 —但是实际上发生了什么呢？</h1><p id="227d" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">好了，我们将放慢速度，浏览一些演示，以便真正理解幕后发生的事情。</p><h2 id="f265" class="nd lt iq bd lu ne nf dn ly ng nh dp mc ko ni nj me ks nk nl mg kw nm nn mi no bi translated">1.1 —线性回归的目的</h2><p id="5a16" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">在本帖中，我们将寻找熊猫宝宝的体重模型(照片用于科学目的)。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi np"><img src="../Images/ef9df044d5fcda56c3cf4c791b466020.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fu8HLUfE1tX6AF75"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated"><a class="ae lr" href="https://unsplash.com/@millerthachiller?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">帕斯卡·米勒</a>在<a class="ae lr" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="85f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的自变量的一些例子包括<code class="fe nq nr ns nt b">age</code>(数字)、<code class="fe nq nr ns nt b">amount of bamboo eaten</code>(数字)和<code class="fe nq nr ns nt b">whether they had siblings</code>(布尔)。我们还将添加常量值<code class="fe nq nr ns nt b">1</code>来计算 y 截距。</p><p id="5b1a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">甚至在开始之前，为了建立<code class="fe nq nr ns nt b">weight ~ predictors</code>模型，我们必须做一些基本的假设，比如线性、正态性和同方差。这里有一个关于 OLS 的<a class="ae lr" rel="noopener" target="_blank" href="/assumptions-in-ols-regression-why-do-they-matter-9501c800787d">假设的非常深入的资源，但是为了简洁，我们不打算覆盖这些假设。</a></p><h2 id="c799" class="nd lt iq bd lu ne nf dn ly ng nh dp mc ko ni nj me ks nk nl mg kw nm nn mi no bi translated">1.2 —数学目标</h2><p id="85ba" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">现在有许多方法来拟合我们的 X 和 y 之间的关系。普通最小二乘法(OLS)是最简单和最常用的方法。它寻求最小化残差平方和。</p><p id="4862" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">残差是我们的线性回归线和数据点之间的<strong class="kh ir">距离，如图 1 中的红线所示。</strong></p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nu"><img src="../Images/48f603b8a5cdd6b07f741444100f586e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jnd4QIfOF8pGn9zC9F8g8w.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 1:残差的可视化(红色)。图片作者。</p></figure><p id="351c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">平方残差是一种测量模型拟合的常用方法，因为它简单，易于数学处理，并且保证为正。在某些情况下，我们可能希望将高估(正残差)与低估(负残差)区别对待，但这本身就是一个完整的主题。</p><p id="0122" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总之，<strong class="kh ir">OLS 的目标是找到使残差最小化的直线的斜率。</strong></p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nv"><img src="../Images/4060e0b58fb6d8c03360919644bfe133.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W14-3WSwqCuGuEglOoGgYw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 2:2D 超平面的例子— <a class="ae lr" href="https://gist.github.com/lambdalisue/7201028" rel="noopener ugc nofollow" target="_blank"> src </a>。图片作者。</p></figure><p id="a95b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们的例子中，我们有多个预测器，所以我们将拟合一个高维超平面，如图 2 所示。很难想象三维以外的任何东西，但希望这个有 3 个轴(独立变量)的例子可以在概念上进一步外推。</p><p id="7653" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">太好了！有了这个设置，让我们进入数学。</p><h1 id="1cf3" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">2—OLS 的矩阵代数</h1><p id="ef4f" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">我们将假设没有矩阵代数的先验知识。此外，因为许多读者可能熟悉代码，我们将利用 python 来理解矩阵符号。</p><p id="a348" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在开始之前，这里是这篇文章的所有<a class="ae lr" href="https://github.com/mberk06/DS_academic_papers/blob/master/36_math_of_ols.py" rel="noopener ugc nofollow" target="_blank">源代码</a>。如果需要，请随意使用。</p><h2 id="b3b5" class="nd lt iq bd lu ne nf dn ly ng nh dp mc ko ni nj me ks nk nl mg kw nm nn mi no bi translated">步骤 0:设置我们的数据</h2><p id="d8ab" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">我们将<strong class="kh ir">创建上面提到的独立变量</strong>。为了确保一定程度的线性，我们将使用这些来<em class="nw">创建</em>我们的因变量<code class="fe nq nr ns nt b">baby_panda_weight</code>。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="b1e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，让我们绘制<code class="fe nq nr ns nt b">age</code>对<code class="fe nq nr ns nt b">baby_panda_weight</code>(图 3)来看看我们做得如何。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nz"><img src="../Images/59ff8d63377b0eb5c46c36b5f051594b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mW1GIhsNv6yiyfiuvc06Aw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 3:使用虚假数据绘制的年龄与体重图。图片作者。</p></figure><p id="39e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总的来说，剧情上似乎有一种比较线性的关系。</p><p id="3323" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来让我们用数学符号做同样的事情(不实际创建真实数据)…</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi oa"><img src="../Images/ec182a6b48aa24616db8dfa9e7a38843.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gjBHFYVm_dYeMjN_7vpdDw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 4:自变量(X_ij)和因变量(Y_i)的矩阵符号。图片作者。</p></figure><p id="dd3e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在图 4 中，每行对应一个独特的观察，每列对应一个变量。矩阵<code class="fe nq nr ns nt b">Y</code>有一列对应于<code class="fe nq nr ns nt b">baby_panda_weight</code>，矩阵<code class="fe nq nr ns nt b">X</code>有 4 列对应于我们的 4 个自变量。</p><p id="e49b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在继续之前，先简单说明一下。python 中的 Pandas 数据框使用相同的结构-列是要素，行是唯一的观察值。然而，<a class="ae lr" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"> numpy </a>数组的默认实现是相反的。</p><pre class="lc ld le lf gt ob nt oc od aw oe bi"><span id="c484" class="nd lt iq nt b gy of og l oh oi">import pandas as pd</span><span id="e308" class="nd lt iq nt b gy oj og l oh oi">a, b = [1,2,3], [4,5,6]</span><span id="cde4" class="nd lt iq nt b gy oj og l oh oi">numpy_arr = np.array([a, b])<br/>pandas_df = pd.DataFrame(dict(a=a, b=b))</span><span id="4178" class="nd lt iq nt b gy oj og l oh oi">numpy_arr.shape         # (2,3)<br/>pandas_df.shape         # (3,2)</span></pre><p id="e7d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在 numpy 中工作时，转置数据以返回到一个<em class="nw">行 x 列</em>形状通常很有帮助。</p><h2 id="9a03" class="nd lt iq bd lu ne nf dn ly ng nh dp mc ko ni nj me ks nk nl mg kw nm nn mi no bi translated">步骤 1:定义优化问题</h2><p id="f81b" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">现在我们有了用矩阵表示的小熊猫数据，让我们用简单的矩阵符号定义 OLS…</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ok"><img src="../Images/e38e81303329996bdc3768805b182aca.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*pny06lI4wBc9oDiqWzaDZQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 5:线性回归的最简单公式。作者图片</p></figure><p id="5ccf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，我们有四个术语。<code class="fe nq nr ns nt b">y</code>和<code class="fe nq nr ns nt b">X</code>同上——它们分别是因变量和自变量的向量。但是，我们也添加了两个新术语:beta ( <code class="fe nq nr ns nt b">β</code>)和 epsilon ( <code class="fe nq nr ns nt b">ε</code>)。β是我们自变量的线性系数矩阵——它显示了 X 的一个单位变化对 Y 的影响程度。ε是一个误差矩阵——它显示了<strong class="kh ir">我们的预测有多错误</strong>。</p><p id="00e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在不要担心翻译。我们稍后再来。</p><h2 id="db98" class="nd lt iq bd lu ne nf dn ly ng nh dp mc ko ni nj me ks nk nl mg kw nm nn mi no bi translated">第二步:制定残差</h2><p id="0ac8" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">酷毙了。现在我们已经定义了我们的术语，让我们开始寻找最好的测试集。</p><p id="165b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如上所述，贝塔需要<strong class="kh ir">最小化残差平方和</strong>。但是，我们如何计算残差呢？</p><p id="68fa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们从 python 开始，然后转到矩阵符号…</p><pre class="lc ld le lf gt ob nt oc od aw oe bi"><span id="5cc8" class="nd lt iq nt b gy of og l oh oi">beta = 1.0</span><span id="1798" class="nd lt iq nt b gy oj og l oh oi">true_value = baby_panda_weight<br/>fitted_value = (age * beta)</span><span id="95ed" class="nd lt iq nt b gy oj og l oh oi">resid = true_value - fitted_value</span></pre><p id="b6e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如 python 代码所示，残差就是我们的<strong class="kh ir"> <em class="nw">真实值</em>减去<em class="nw">拟合值</em>。相当简单。</strong></p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/847fa2feecf78aa6f504d8393f7131a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*kw2PXP0zN4C9WKVMtvToaw.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 6:剩余公式。图片作者。</p></figure><p id="a614" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如图 6 所示，残差也可以写成<strong class="kh ir"> OLS 公式，其中误差项被隔离</strong>。<code class="fe nq nr ns nt b">Xβ</code>是我们的拟合值，我们的模型预测的值，而<code class="fe nq nr ns nt b">y</code>是我们的真实值。</p><h2 id="c884" class="nd lt iq bd lu ne nf dn ly ng nh dp mc ko ni nj me ks nk nl mg kw nm nn mi no bi translated">第三步:平方残差并求和</h2><p id="177d" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">现在，我们已经得到了我们的残差，让我们平方和求和。平方的目的是让我们能够轻松求解贝塔系数的最佳值，我们将在第 3 节中讨论这一点。</p><pre class="lc ld le lf gt ob nt oc od aw oe bi"><span id="04c5" class="nd lt iq nt b gy of og l oh oi">sum_of_squares_python_style = np.sum(resid ** 2)</span></pre><p id="af15" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的 python 代码中，我们首先对残差求平方，然后求和。这很完美，但是线性代数有另一种方法。</p><p id="8dd1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">任何时候你把一个向量乘以它自身的</strong> <a class="ae lr" href="https://www.youtube.com/watch?v=TZrKrNVhbjI&amp;ab_channel=KhanAcademy" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">转置</strong> </a> <strong class="kh ir">，你就在计算平方和</strong>。让我们看一个例子…</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi om"><img src="../Images/26eb16be515c73d4baa965771c99b793.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HCg04o496vo4QWKhpZ7ohw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 7:矩阵符号中残差平方和。图片作者。</p></figure><p id="61d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如图 7 所示，向量乘法要求我们转置第一个向量，将其转换为“行”而不是“列”然后，通过等号右边的数学公式，我们得到这个向量的平方和。这个符号可以简化为图 8 中的值。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi on"><img src="../Images/e53dd663cfdb85a51757ada09863a870.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qxqG8eVPhxg1dJByr309jA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 8:残差平方和的矩阵符号，即转置(误差)*误差。图片作者。</p></figure><h2 id="a658" class="nd lt iq bd lu ne nf dn ly ng nh dp mc ko ni nj me ks nk nl mg kw nm nn mi no bi translated">第四步:求导</h2><p id="bfcf" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">到目前为止还好。我们有一个强大的数学方法来确定我们的平方和。</p><p id="8019" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下一步是公式化我们的平方和项相对于β的<a class="ae lr" href="https://www.youtube.com/watch?v=N2PpRnFqnqY&amp;ab_channel=KhanAcademy" rel="noopener ugc nofollow" target="_blank">导数</a>。我们将在接下来的步骤中了解为什么这是相关的。</p><pre class="lc ld le lf gt ob nt oc od aw oe bi"><span id="aed2" class="nd lt iq nt b gy of og l oh oi">term_1 = matmul(-2 * x.T, y)<br/>term_2 = matmul(matmul(2 * x.T, x), beta)<br/>derivative = term_1 + term_2</span></pre><p id="5d16" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面的 python 代码显示了我们的<strong class="kh ir">残差平方和导数</strong>的公式。如果你想知道我们是如何得到这个公式的，看看下面的矩阵符号…</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi oo"><img src="../Images/ea2880a07fb8a85350647a4dec9bfec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X3qHSahQErZXr8NkDUAeQA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 9:我们的 SSQ 残差的导数。图片作者。</p></figure><p id="6423" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">出于实用目的，我们不需要知道如何求导，所以我们要移动下一节。但是，你需要知道的是导数代表什么。<strong class="kh ir">导数是直线在给定点的瞬时斜率。</strong></p><p id="8f1e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你想了解更多，这里有一个很好的教程，作者是 YouTubers 上最好的数学专家之一。</p><h2 id="4798" class="nd lt iq bd lu ne nf dn ly ng nh dp mc ko ni nj me ks nk nl mg kw nm nn mi no bi translated">第五步:简化和解决</h2><p id="4963" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">最后一步，为了找到使误差平方和最小的β值，我们将上述导数设置为零，并求解β(图 10)。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi op"><img src="../Images/50ae989d3506406d0cf5683b4c8e7403.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dz-njP6ZsDKCGNqqmFv3Cw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 10:求解最佳 beta。图片作者。</p></figure><p id="cac6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了简洁起见，代数步骤被省略了，但是最后一行显示了获得任何 OLS 模型的贝塔系数的公式——就这么简单。</p><p id="b85c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们一步一步地完成这个计算…</p><pre class="lc ld le lf gt ob nt oc od aw oe bi"><span id="2dbc" class="nd lt iq nt b gy of og l oh oi">a = matmul(x.T, x)<br/>b = inv(a)<br/>c = matmul(b, x.T)<br/>best_betas = matmul(c, y)</span><span id="6d16" class="nd lt iq nt b gy oj og l oh oi">best_betas_one_line = matmul(matmul(inv(matmul(x.T, x)), x.T), y)</span></pre><ol class=""><li id="6548" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la mu mv mw mx bi translated"><code class="fe nq nr ns nt b">a</code>是方差/协方差矩阵。协方差在对角线上，方差在非对角线上。</li><li id="f8ce" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated"><code class="fe nq nr ns nt b">b</code>是这个矩阵的逆矩阵——它本质上允许我们进行除法。</li><li id="8005" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated"><code class="fe nq nr ns nt b">c</code>是我们的自变量“除以”方差/协方差矩阵。</li><li id="6fa6" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated"><code class="fe nq nr ns nt b">best_betas</code>是<code class="fe nq nr ns nt b">c</code>缩放到我们的 y 值。</li></ol><p id="d8c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种计算背后的直觉是，我们实际上是取 X 和 Y 的协方差，然后除以 X 的方差。<strong class="kh ir">我们分离出 Y 因 X 而变化的程度。</strong></p><p id="7502" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在你知道了！使用上面的 python 代码，您可以求解任何 beta 系数。</p><h1 id="e75e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">3—OLS 的直觉</h1><p id="add1" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">现在我们已经理解了数学，让我们更深入地了解它为什么会起作用。</p><h2 id="89b1" class="nd lt iq bd lu ne nf dn ly ng nh dp mc ko ni nj me ks nk nl mg kw nm nn mi no bi translated">3.1-为什么我们知道残差平方和是凸的？</h2><p id="f988" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">在上面的步骤 5 中，我们求解β，其中导数等于零。通过平方误差，我们保证有一个全局最小值，所以只要导数为零，我们就能最小化误差。</p><p id="6d97" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有两种不用证明的方法。</p><p id="0cda" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，通过对我们的残差求平方，我们产生一个二维抛物线。函数形式为<code class="fe nq nr ns nt b">y=a(x²)</code>，其中<code class="fe nq nr ns nt b">a</code>为比例参数。根据定义，抛物线只有一个最优值(最大值或最小值)。</p><p id="7080" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">朝下的抛物线要求上式中的<code class="fe nq nr ns nt b">a</code>为负。然而，因为我们平方所有的残差，T3 就是 T4，我们保证对于β的某个值有一个全局最小值。</p><p id="1ebe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其次，让我们用简单的代码做一个简单的线性回归的例子。</p><pre class="lc ld le lf gt ob nt oc od aw oe bi"><span id="36a0" class="nd lt iq nt b gy of og l oh oi">import numpy as np</span><span id="cb2f" class="nd lt iq nt b gy oj og l oh oi">def ssq(x, y, beta):<br/>    return np.sum((y - x * beta)**2)</span><span id="0968" class="nd lt iq nt b gy oj og l oh oi">x = np.array([4,3,2])<br/>y = np.array([3,2.2,1])</span><span id="7673" class="nd lt iq nt b gy oj og l oh oi">betas = np.arange(-2, 2, 0.05)<br/>errors = [ssq(x, y, b) for b in betas]</span></pre><p id="68b0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的代码中，我们创建了两个向量，然后为我们的回归系数 beta 尝试了很多可能的值。如果我们绘制不同β值的误差图(图 11)，我们会得到一个向上的抛物线。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi oq"><img src="../Images/6a5404bf0bdaa013944b0e74ccd4ba8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LW0kLkATW4rIXd23VsRghg.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 11:简单的可视化显示 ssq 误差是凸的。图片作者。</p></figure><p id="bd9d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本例中，误差实际上为零，但这并不总是有保证的。但是，通过利用 OLS 有单个最优值(在这种情况下是最小值)的事实，我们知道<strong class="kh ir">我们的最小值的位置是我们的 SSQ 的导数为 0 的地方。</strong></p><h2 id="598a" class="nd lt iq bd lu ne nf dn ly ng nh dp mc ko ni nj me ks nk nl mg kw nm nn mi no bi translated">3.2—你应该如何看待贝塔系数？</h2><p id="49ae" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">解释 OLS 系数非常困难。我们来分解一下…</p><ul class=""><li id="dce3" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la or mv mw mx bi translated">每个自变量系数的<strong class="kh ir">大小给出了该变量对因变量影响的大小。</strong></li><li id="74c5" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la or mv mw mx bi translated">系数(正或负)上的<strong class="kh ir">符号给你效果的方向。</strong></li></ul><p id="7a12" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是，这里有一些细节，所以我们来看一下每种系数类型的系数解释。</p><p id="4d1f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 3.2.1 — Y 轴截距(β0) </strong></p><blockquote class="os ot ou"><p id="75d8" class="kf kg nw kh b ki kj jr kk kl km ju kn ov kp kq kr ow kt ku kv ox kx ky kz la ij bi translated">Y 截距系数(β0) <strong class="kh ir"> </strong>是 Y 的平均值，所有其他预测值为零。</p></blockquote><p id="09dc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以把β0 当做基线值。请注意，预测值为 0 并不总是有意义——如果我们将<code class="fe nq nr ns nt b">year</code>作为一个特征包括在内会怎样。我们肯定没有 0 年的熊猫宝宝数据，所以考虑 y 截距是否有解释意义很重要。</p><p id="e5ad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">示例解释:y 截距系数 400.0 意味着当熊猫的<code class="fe nq nr ns nt b">age</code>、<code class="fe nq nr ns nt b">amount of bamboo eaten</code>和<code class="fe nq nr ns nt b">siblings</code>为 0 时，我们数据集中熊猫的平均体重为 400.0。</p><p id="a696" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 3.2.2 —连续自变量</strong></p><blockquote class="os ot ou"><p id="f773" class="kf kg nw kh b ki kj jr kk kl km ju kn ov kp kq kr ow kt ku kv ox kx ky kz la ij bi translated">连续可变系数是在其他因素保持不变的情况下，X 变化一个单位时 Y 均值的预期变化。</p></blockquote><p id="5fb1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">连续变量的系数代表<code class="fe nq nr ns nt b">X</code>一个单位的变化，我们的<code class="fe nq nr ns nt b">Y</code>应该变化多少。如上所述，数值越大，<code class="fe nq nr ns nt b">Y</code>相对于<code class="fe nq nr ns nt b">X</code>的变化就越大。如果符号为正，<code class="fe nq nr ns nt b">Y</code>将随<code class="fe nq nr ns nt b">X</code>增加，反之，如果符号为负，<code class="fe nq nr ns nt b">Y</code>将随<code class="fe nq nr ns nt b">X</code>减少。</p><p id="6393" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">示例解释:<code class="fe nq nr ns nt b">age</code>上的系数 10.0 表示在其他因素保持不变的情况下，熊猫每年平均生长 10.0 个单位。</p><p id="19be" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 3.2.3 —二元独立变量</strong></p><blockquote class="os ot ou"><p id="7744" class="kf kg nw kh b ki kj jr kk kl km ju kn ov kp kq kr ow kt ku kv ox kx ky kz la ij bi translated">二进制可变系数是在其他因素保持不变的情况下，X 变化一个单位时 Y 均值的预期变化。</p></blockquote><p id="9190" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">二进制变量可以用与连续变量相同的方式解释。它们只有两个值:0 或 1。还要注意，我们经常使用一种叫做<a class="ae lr" href="https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/" rel="noopener ugc nofollow" target="_blank">一键编码</a>的方法，通过几个二进制变量来表示分类变量。</p><p id="4156" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">举例解释:系数-4.0 表示如果熊猫宝宝有兄弟姐妹，它的体重平均会减少 4 个单位。</p><h2 id="4d9d" class="nd lt iq bd lu ne nf dn ly ng nh dp mc ko ni nj me ks nk nl mg kw nm nn mi no bi translated">互动</h2><p id="c0ff" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">变量之间的相互作用是一个更复杂的话题，我们不会涉及，但有很好的资源。这里有一个可靠的<a class="ae lr" href="http://If you would like to go deeper into interpreting coefficients, here's a good stackoverflow post that discusses interactions." rel="noopener ugc nofollow" target="_blank">堆栈溢出 post </a>。</p><h2 id="fe78" class="nd lt iq bd lu ne nf dn ly ng nh dp mc ko ni nj me ks nk nl mg kw nm nn mi no bi translated">3.3——为什么它保持一切不变？</h2><p id="0ac7" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">对于我们的最后一部分，让我们来谈谈“保持其他一切不变”的概念</p><p id="895d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">OLS 的一个关键优势是<strong class="kh ir">通过控制所有其他独立变量来隔离给定变量的影响。</strong></p><p id="9f8e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">思考这个问题的一个很好的方法是使用残差的概念。实际上，每个预测值在所有其他预测值上回归，只留下残差作为因变量的预测值。让我们看一个例子…</p><p id="75e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们正在寻找一个模型来确定<code class="fe nq nr ns nt b">age</code>对<code class="fe nq nr ns nt b">baby_panda_weight</code>的影响，保持<code class="fe nq nr ns nt b">amount_of_bamboo_eaten</code>不变。为此，我们可以创建以下模型…</p><pre class="lc ld le lf gt ob nt oc od aw oe bi"><span id="7929" class="nd lt iq nt b gy of og l oh oi">baby_panda_weight ~ age + amount_of_bamboo_eaten</span></pre><p id="0921" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，我们也可以执行以下步骤，并为<code class="fe nq nr ns nt b">age</code>获得相同的回归系数。</p><ol class=""><li id="7883" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la mu mv mw mx bi translated">拟合<code class="fe nq nr ns nt b">age ~ amount_of_bamboo_eaten</code>并计算残差。残差，姑且称之为<code class="fe nq nr ns nt b">R_x</code>，对应于<code class="fe nq nr ns nt b">age</code>中<strong class="kh ir">无法用<code class="fe nq nr ns nt b">amount_of_bamboo_eaten</code>解释的趋势。</strong></li><li id="a049" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">安装<code class="fe nq nr ns nt b">baby_panda_weight ~ amount_of_bamboo_eaten</code>。残差，姑且称之为<code class="fe nq nr ns nt b">R_y</code>，对应于<code class="fe nq nr ns nt b">baby_panda_weight</code>中<strong class="kh ir">无法用<code class="fe nq nr ns nt b">amount_of_bamboo_eaten</code>解释的趋势。</strong></li><li id="9dfc" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">安装<code class="fe nq nr ns nt b">R_y ~ B3*R_x</code>。</li></ol><p id="b832" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">我们剩余值的贝塔系数(</strong> <code class="fe nq nr ns nt b"><strong class="kh ir">B3</strong></code> <strong class="kh ir">)将等于我们上面完整模型中</strong> <code class="fe nq nr ns nt b"><strong class="kh ir">age</strong></code> <strong class="kh ir"> ( </strong> <code class="fe nq nr ns nt b"><strong class="kh ir">B1</strong></code> <strong class="kh ir">)的贝塔系数。这里有一个 python 中的例子…</strong></p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="8ef3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是 OLS 保持物质不变的方法。数学中隐含的是将所有其他预测值相对于感兴趣的预测值进行回归的过程。然后，它会寻找与该回归过程的残差相匹配的系数。</p><p id="2cbb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很酷吧？</p><p id="cc0e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">顺便说一下，这就是为什么<a class="ae lr" href="https://www.investopedia.com/terms/m/multicollinearity.asp#:~:text=Multicollinearity%20is%20the%20occurrence%20of,in%20a%20multiple%20regression%20model.&amp;text=In%20general%2C%20multicollinearity%20can%20lead,independent%20variables%20in%20a%20model." rel="noopener ugc nofollow" target="_blank">多重共线性</a>是这样一个问题——如果我们将一个变量与另一个非常相似的变量进行回归，残差中就不会留下多少信号。</p><p id="f7af" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，你可能想知道这种“剩余化”是如何工作的。嗯，我也是所以我写了一个<a class="ae lr" href="https://math.stackexchange.com/questions/4388961/whats-the-intuition-behind-holding-everything-constant-in-ordinary-least-square" rel="noopener ugc nofollow" target="_blank">关于栈交换的问题</a>。如果你能为线性代数公式如何“剩余化”描述一个直观的解释，那将是惊人的。我也会相应地更新这个帖子。</p><h1 id="29d3" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">4 —摘要</h1><p id="6f68" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">普通最小二乘回归是最常见的线性建模技术。它速度快，实现简单，易于解释。然而，很少有人真正了解幕后发生的事情。</p><p id="f48c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了找到最佳β系数，我们将误差公式化为平方和(SSQ)，从而产生一个向上的抛物线。然后，我们求解β的值，其中 SSQ 的导数为 0，表示拐点。该拐点处的β系数保证使平方误差最小。</p><p id="e07f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">建立这个模型后，我们可以将系数解释为自变量的一个单位变化引起的因变量的变化，其他一切保持不变。</p></div><div class="ab cl oy oz hu pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="ij ik il im in"><p id="517f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nw">感谢阅读！我会再写 14 篇文章，把学术研究带到 DS 行业。查看我的评论，链接到这篇文章的主要来源和一些有用的资源。</em></p></div></div>    
</body>
</html>