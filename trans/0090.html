<html>
<head>
<title>A Low Variance Sample Estimator for Mutual Information</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">互信息的低方差样本估计量</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-low-variance-sample-estimator-for-mutual-information-c4aa479003aa#2022-02-03">https://towardsdatascience.com/a-low-variance-sample-estimator-for-mutual-information-c4aa479003aa#2022-02-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fa28" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">与相关性竞争</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ce00dc0a36050f71bd5fc19bc58f1ad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YcRoRYDdjDY6ERq7"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@jeshoots?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">JESHOOTS.COM</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="8873" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我那篇关于<a class="ae kv" rel="noopener" target="_blank" href="/approximating-kl-divergence-4151c8c85ddd"> <strong class="ky ir">逼近 KL 发散</strong> e </a>的文章的续篇，遵循相同的思路。我想这应该有它自己的一部分，因为我还没有在互联网上的其他地方看到过这个具体的想法。</p><h1 id="a4fe" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">什么是互信息？</h1><p id="9517" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">首先快速回顾一下，互信息描述了在一个数字中两个随机变量是如何相互依赖的。如果互信息为 0，则变量是独立的，否则存在一定的依赖性。你可能会认为这听起来很像相关性，但关键区别在于相关性衡量的是线性相关性，而互信息也考虑了非线性相关性。例如，给定随机变量<strong class="ky ir"> X </strong>假设<strong class="ky ir"> Y=X </strong>。很明显<strong class="ky ir"> X </strong>和<strong class="ky ir"> Y </strong>是相关的，但是相关性可能不会注意到它，因为关系是非线性的，而互信息会。</p><p id="faa1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那么它到底是如何工作的呢？从概率论中我们知道，如果两个随机变量的单个概率密度函数的乘积等于联合概率密度函数，则这两个随机变量是独立的(因此也是不相关的):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/6cbda43d7b4cec6b07df38fcb0e7b270.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*-Llwko27TqKnV4GWZQpkjA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片:X 和 Y 独立的情况。</p></figure><p id="e7d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以为了测量独立性，我们真的需要找到一种方法来测量联合概率密度函数和个体密度函数的乘积之间的相似性。幸运的是，我们有信息论的工具，KL 散度！为了简洁起见，我不会在这里深入研究 KL 散度，但如果你想知道更多，请在这里 查看我关于主题<a class="ae kv" rel="noopener" target="_blank" href="/forward-and-reverse-kl-divergence-906625f1df06"> <strong class="ky ir">的文章。因此，完整的互信息定义如下:</strong></a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/00b79741c2a1712eefae6ebb8cc06974.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*XzoyinV-nTi1Cdha02dkPA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="6c30" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">为什么首先要估计？</h1><ol class=""><li id="14de" class="ms mt iq ky b kz mk lc ml lf mu lj mv ln mw lr mx my mz na bi translated"><strong class="ky ir">无解析解</strong>:互信息的完整形式解析解可能未知。例如，高斯混合分布就是这种情况。</li><li id="0d75" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated"><strong class="ky ir">高计算复杂度</strong>:计算完整的互信息通常需要对整个分布空间求和。使用不需要这样做的近似是有用的，因为它可以更快。</li></ol><h1 id="d3c2" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">估计量的标准</h1><p id="df24" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">直觉上，一个估计量应该具有与被估计的原始度量相似的行为。我们可以用两种方法来衡量这种相似性:</p><ol class=""><li id="e012" class="ms mt iq ky b kz la lc ld lf ng lj nh ln ni lr mx my mz na bi translated"><strong class="ky ir">偏差</strong>:理想情况下，估计量应该是无偏的；也就是说，估计量的期望值应该等于原始度量。</li><li id="ae80" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated"><strong class="ky ir">方差</strong>:方差为 0 的无偏估计量(确定性的)将完全等于原始度量！当然，这是不现实的，但理想情况下，方差应该尽可能低，从而增加获得更接近原始度量的值的可能性。</li></ol><h1 id="9c69" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">估计互信息</h1><p id="80a7" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">首先，我们需要计算出在我们的估计量中，我们的样本是来自哪个分布。假设我们有随机变量<strong class="ky ir"> X </strong>和<strong class="ky ir"> Y </strong>，在每个时间步，我们将同时采样一个<strong class="ky ir"> x </strong>和<strong class="ky ir"> y </strong>。因此，样本是从联合分布中生成的。这在处理估计量的第一个标准时是很重要的，即无偏性，知道在哪个分布上计算期望。让我们从目前使用的标准解决方案开始:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/0c6d346d49d20b22a52e707d17e4af61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*oRK2Wxh76JzSorivUW2P2Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="353d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这很好，但它有很高的方差，因为它可以取负值，而实际的互信息不能:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/73401840a058caa83676d262b4d7bfaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*xh-kkhjsLAHUsRqP8Jm7bA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者:当比值 f_{XY}(x，y)/f_X(x)f_Y(y)小于 1 时，上面给出的样本互信息估计量为负😞</p></figure><p id="ec2a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">改善这种情况的一种方法是添加一个期望值为 0 的项，该项与上面的原始近似值负相关。我们提出以下解决方案:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/e448e67621de3b510c78c0fea5481ede.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w0hIlGaWU-nAcRJsy5kqaA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="f463" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设<strong class="ky ir"> f_X(x) </strong>和<strong class="ky ir"> f_Y(y) </strong>是有效的概率函数(质量总和为 1)，这是可行的。然后，我们可以将估计值更新为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/3ecff3030fffccede1d80e3829975c17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z0DESNQvNu35uoObuvIxMw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="66f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中我们可以求解<strong class="ky ir"> λ </strong>以找到最小方差。不幸的是，这取决于具体情况，很难分析计算。但是，通过选择值 1，我们仍然可以找到一个很好的折衷方案。这导致无论如何都是一个半正定的近似！如果我们把比值<strong class="ky ir"> f_X(x)f_Y(y)/f_{XY}(x，y) </strong>作为自己的变量，并把它与近似值<strong class="ky ir"> λ=1 </strong>相对照，我们得到下面的正定图:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/31e8fedabe8a98266e5e0e14ac9979dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*jjAGaH61bOiUkF4C_iJGtg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="cde1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这产生了最终的估计量:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/e16d390346cb324f30804904be67ce0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*q7_4jb8c4IYQETAl87-0UA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><p id="7efd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下一次你发现自己需要使用一个样本来近似交互信息时，请记住这个技巧！</p><p id="cec6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您觉得这篇文章有用，请考虑:</p><ul class=""><li id="8d0d" class="ms mt iq ky b kz la lc ld lf ng lj nh ln ni lr nw my mz na bi translated">跟踪我🙌</li><li id="9d03" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr nw my mz na bi translated"><a class="ae kv" href="https://medium.com/subscribe/@rohan.tangri" rel="noopener"> <strong class="ky ir">订阅我的邮件通知</strong> </a>永不错过上传📧</li><li id="4f37" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr nw my mz na bi translated">使用我的媒介<a class="ae kv" href="https://medium.com/@rohan.tangri/membership" rel="noopener"> <strong class="ky ir">推荐链接</strong> </a> <strong class="ky ir"> </strong>直接支持我并获得无限量的优质文章🤗</li></ul><p id="e4d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">推广的方式，我真的希望你觉得这篇文章有趣，让我知道你的想法！！</p></div></div>    
</body>
</html>