<html>
<head>
<title>Analysing Fairness in Machine Learning (with Python)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分析机器学习中的公平性(用Python)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/analysing-fairness-in-machine-learning-with-python-96a9ab0d0705#2022-04-29">https://towardsdatascience.com/analysing-fairness-in-machine-learning-with-python-96a9ab0d0705#2022-04-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1118" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">进行探索性的公平性分析，并使用平等机会、均等优势和不同影响来衡量公平性</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f0e8fe25a210672c3ab5ff326390d7be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rJypRcPEOK_s-madSvNl4g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(来源:<a class="ae ky" href="https://www.flaticon.com/premium-icon/measure-tape_4080045" rel="noopener ugc nofollow" target="_blank"> flaticon </a>)</p></figure><p id="6b51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">仅仅建立做出<strong class="lb iu">精确</strong>预测的模型已经不够了。我们还需要确保这些预测是公平的<strong class="lb iu"/>。</p><p id="8d13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这样做会减少预测偏差的危害。因此，你将在建立对你的人工智能系统的信任方面走很长的路。为了纠正偏见，我们需要从分析数据和模型的公平性开始。</p><p id="4b3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">衡量公平很简单。</p><p id="a8ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理解一个模型为什么不公平更复杂。</p><p id="22d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是为什么我们将:</p><ul class=""><li id="3500" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">首先做一个<strong class="lb iu">探索性公平分析——在你开始建模之前</strong>识别潜在的偏见来源。</li><li id="80c5" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">然后，我们将继续<strong class="lb iu">衡量公平—</strong>应用不同的公平定义。</li></ul><p id="10b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以看到我们将在下面介绍的方法的摘要。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/47b23ab3e40fe891ee07047ba351f710.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EYzh1dgAuKFQrh4Ms7Nzrg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">公平性分析方法概述(来源:作者)</p></figure><p id="ab81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将讨论这些方法背后的理论。我们也将使用<strong class="lb iu"> Python来应用它们。</strong>我们将讨论关键的代码片段，你可以在<a class="ae ky" href="https://github.com/conorosully/medium-articles/blob/master/src/algorithm%20fairness/Measuring%20Bias.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> GitHub </strong> </a>上找到完整的项目。</p><h1 id="aa52" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">资料组</h1><p id="13a5" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">我们将使用<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/adult" rel="noopener ugc nofollow" target="_blank">成人数据集</a> <strong class="lb iu">建立一个模型。你可以在<strong class="lb iu">表1 </strong>中看到这个的快照。经过一点功能工程，我们将使用前6列作为模型<strong class="lb iu">功能</strong>。接下来的2个，种族和性别，是敏感属性</strong>。我们将基于这些来分析对群体的偏见。最后是我们的<strong class="lb iu">目标变量</strong>。我们将尝试预测一个人的收入是高于还是低于5万美元。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/01b24f620d454f81b83cd2e82de39bf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g3gY1fkLMLQXr1I7kA_aTw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表1:成人收入数据集快照(图片来源:作者)(数据集:<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/adult" rel="noopener ugc nofollow" target="_blank"> UCI </a>)(许可证:CC0:公共领域)</p></figure><p id="8383" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在加载这个数据集之前，我们需要导入一些Python包。我们用下面的代码做到了这一点。我们使用<strong class="lb iu"> NumPy </strong>和<strong class="lb iu">熊猫</strong>进行数据操作(第1-2行)。<strong class="lb iu"> Matplotlib </strong>用于一些数据可视化(第3行)。我们将使用<strong class="lb iu"> xgboost </strong>来构建我们的模型(第5行)。我们还从<strong class="lb iu"> scikit-learn </strong>导入了一些函数来评估我们的模型(第7–9行)。确保你已经安装了这些。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="3068" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在下面导入数据集(第7行)。我们还会删除任何缺少值的行(第8行)。注意这里加载了一些额外的列。查看列名(第1-4行)。在此分析中，我们将只考虑我们在<strong class="lb iu">表1 </strong>中提到的那些。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div></figure><h1 id="28fb" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated"><strong class="ak">算法公平性的探索性分析</strong></h1><p id="cb80" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">当你有了最终的模型时，评估公平性并没有开始。这也应该是你探索性分析的一部分。一般来说，我们这样做是为了围绕数据集建立一些直觉。所以，当谈到建模时，你对预期的结果有一个很好的想法。具体来说，为了公平，你想了解你的数据的哪些方面可能导致不公平的模型。</p><p id="b270" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于不公平的不同<a class="ae ky" rel="noopener" target="_blank" href="/algorithm-fairness-sources-of-bias-7082e5b78a2c">原因</a>，你的模型可能变得不公平。在我们的探索性分析中，我们将关注与数据相关的3个关键来源。这些是<strong class="lb iu">历史偏差</strong>、<strong class="lb iu">代理变量</strong>和<strong class="lb iu">不平衡数据集</strong>。我们想了解这些在我们的数据中存在的程度。了解原因将有助于我们选择解决不公平的最佳方法。</p><h2 id="e5d0" class="nk ml it bd mm nl nm dn mq nn no dp mu li np nq mw lm nr ns my lq nt nu na nv bi translated">不平衡的数据集</h2><p id="645c" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">我们将从查看我们的数据集是否不平衡开始。具体来说，我们指的是敏感属性方面的不平衡。请看图1，我们有按种族和性别划分的人口。你可以看到我们确实有一个不平衡的数据集。第一张图表显示我们86%的人口是白人。同样，<strong class="lb iu"/>68%的人口是男性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/d8b1e12e6208e3cc5e993593b5610206.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4GffmSZ07IP5O_lOK1K4Zw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:按种族和性别分列的人口(资料来源:作者)</p></figure><p id="eda5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以看到我们是如何为下面的种族属性创建饼图的。我们从按种族统计人口开始(第2行)。我们使用索引定义标签(第3行)。这些是我们在图1 中看到的不同种族的名称。然后，我们使用matplotlib中的<strong class="lb iu"> <em class="nx"> pie </em> </strong>函数绘制计数(第6行)。我们还使用标签创建了一个图例(第7行)。性别属性饼图的代码非常相似。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="51aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不平衡数据集的问题是模型参数可能会偏向大多数。例如，女性和男性的趋势可能不同。所谓趋势，我们指的是特征和目标变量之间的关系。一个模型将试图最大化整个群体的准确性。这样做可能有利于男性人口的趋势。因此，我们对女性人口的准确性可能较低。</p><h2 id="3bed" class="nk ml it bd mm nl nm dn mq nn no dp mu li np nq mw lm nr ns my lq nt nu na nv bi translated">定义受保护的功能</h2><p id="b9d2" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">在我们继续之前，我们需要定义我们的<strong class="lb iu">保护特性</strong>。我们通过使用敏感属性创建二进制变量来做到这一点。我们定义变量，使1代表一个<strong class="lb iu">特权</strong>组，0代表一个<strong class="lb iu">非特权</strong>组。通常，弱势群体在过去会面临历史不公。换句话说，这个群体最有可能面临来自一个有偏见的模型的不公平的决定。</p><p id="c264" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用下面的代码定义这些特性。对于种族，我们定义了受保护的特征，因此“白人”是特权群体(第4行)。也就是说，如果这个人是白人，这个变量的值为1，否则为0。对于性，“男性”是特权群体(第5行)。接下来，我们将使用这些二进制变量来代替原来的敏感属性。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="e709" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的代码中，我们还定义了一个目标变量(第8行)。其中，如果个人收入超过5万美元，则该值为1，如果收入低于5万美元，则该值为0。在第1行中，我们用原始的敏感属性创建了<strong class="lb iu"> <em class="nx"> df_fair </em> </strong>数据集。我们已经将目标变量和受保护的要素添加到该数据集中。它将被用作剩余公平性分析的基础。</p><h2 id="1380" class="nk ml it bd mm nl nm dn mq nn no dp mu li np nq mw lm nr ns my lq nt nu na nv bi translated">流行</h2><p id="0575" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">对于目标变量，<strong class="lb iu">患病率</strong>是阳性病例占全部病例的比例。其中肯定的情况是当目标变量的值为1时。我们的数据集的总体患病率为<strong class="lb iu"> 24.8% </strong>。在我们的数据集中，大约有1/4的人收入超过5万美元(T21)。我们也可以用流行度作为一个公平的衡量标准。</p><p id="116a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们通过计算不同特权(1)组和非特权(0)组的患病率来做到这一点。你可以在下面的<strong class="lb iu">表2 </strong>中看到这些值。注意，特权群体的患病率要高得多。事实上，如果你是男性，你挣5万美元以上的可能性几乎是女性的三倍<strong class="lb iu"/>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/282a0baf41526f02a86a7ece827c5e15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*54xjAXarrFBzMjf9gDqw8g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表2:受保护特征的流行率(资料来源:作者)</p></figure><p id="6975" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以通过计算受保护特征的交叉点的普遍性来更进一步。你可以在<strong class="lb iu">表3 </strong>中看到这些数值。如果你属于两个特权群体，左上角给出了患病率(即性别= 1 &amp;种族= 1)。同样，右下方给出了不属于任何特权群体的患病率(即性别= 0 &amp;种族= 0)。这告诉我们，白人男性收入超过5万美元的可能性是非白人女性的4倍。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/1b1b9daa7a524f40b15608ba8dde844a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B36e4aF5LcKQF4WChQk8oA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表3:受保护特征交叉的流行率(来源:作者)</p></figure><p id="093e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用下面的代码计算这些值。你可以看到总体患病率只是目标变量的平均值(第1行)。同样，我们可以对不同的受保护功能组合取平均值(第3-5行)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="f41f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这一点上，你应该问自己为什么我们在患病率上有这么大的差异。该数据集是利用1994年美国人口普查数据建立的。这个国家有基于性别和种族歧视的历史。最终，目标变量反映了这种歧视。从这个意义上说，患病率可以用来理解<strong class="lb iu">历史不公正</strong>在多大程度上嵌入了我们的目标变量。</p><h2 id="52a7" class="nk ml it bd mm nl nm dn mq nn no dp mu li np nq mw lm nr ns my lq nt nu na nv bi translated">代理变量</h2><p id="8453" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">我们可以分析偏差潜在来源的另一种方法是通过寻找<strong class="lb iu">代理变量</strong>。这些是与我们的受保护特征高度相关或关联的模型特征。使用代理变量的模型可以有效地使用受保护的特性来做出决策。</p><p id="68dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以找到代理变量，就像在特征选择过程中找到重要特征一样。也就是说，我们在特征和目标变量之间使用一些关联的度量。除了现在，我们使用受保护的特性，而不是目标变量。我们将研究两种关联的度量方法——<strong class="lb iu">互信息</strong>和<strong class="lb iu">特征重要性</strong>。</p><p id="408b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在此之前，我们需要做一些功能工程。我们从创建一个目标变量开始，就像前面一样(第2行)。然后我们创建<strong class="lb iu"> 6个模型特征</strong>。首先，我们让<strong class="lb iu">的年龄</strong>、<strong class="lb iu">的教育编号</strong>和<strong class="lb iu">的每周工作时间</strong>保持不变(第5行)。我们从<strong class="lb iu">婚姻状况</strong>和<strong class="lb iu">本国</strong>创建二元特征(第6-7行)。最后，我们通过将原始职业分成5组来创建<strong class="lb iu">职业</strong>特征(第9-16行)。在下一节中，我们将使用这些相同的特性来构建我们的模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="9437" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">互信息</strong>是两个变量之间非线性关联的度量。它表明一个变量的不确定性通过观察另一个变量减少了多少。在<strong class="lb iu">图2 </strong>中，您可以看到6个特征和受保护特征之间的互信息值。注意婚姻状况和性别之间的高值。这表明了这些变量之间可能的关系。换句话说，婚姻状况可能是性别的一个替代变量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/e490e3d6dcd3f1120d7024a9966e0f08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pAUEwydLlCNnrvdeOLSTsw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:具有受保护特征的交互信息(来源:作者)</p></figure><p id="25f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用下面的代码计算互信息值。这是使用<strong class="lb iu"> mutual_info_classif </strong>函数完成的。对于种族，我们传递我们的特征矩阵(第2行)和种族保护特征(第3行)。我们还告诉函数6个特征中哪些是离散的(第4行)。性的代码是相似的(第5行)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="c0ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以采取的另一种方法是使用受保护的特征来构建模型。也就是说，我们尝试使用6个模型特征来预测受保护的特征。然后我们可以使用来自这个模型的<strong class="lb iu">特征重要性</strong>分数作为我们的关联度量。你可以在图3 中看到这个过程的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/7178f0285898201f101448b7440b6f6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B_tkUqitCnXIGvOkqp9KQw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:预测受保护特性时的特性重要性(来源:作者)</p></figure><p id="eb23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个过程的另一个结果是我们有了模型的准确性。这些可以给我们一个整体关联的度量。预测种族的准确率为<strong class="lb iu"> 72.7% </strong>，预测性别的准确率为<strong class="lb iu"> 78.9% </strong>。如果我们回到<strong class="lb iu">图2 </strong>中的互信息值，这种差异是有意义的。你可以看到性的价值通常更高。最终，我们可以预期代理变量对于性别来说比种族更成问题。</p><p id="b633" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以在下面看到我们是如何计算种族指标的。我们从获得一个平衡的样本开始(第2-7行)。这就是为什么我们的数据集中有相同数量的特权和非特权。然后，我们使用这个数据集来构建一个模型(第10 -11行)。请注意，我们使用竞争保护特性作为目标变量。然后我们得到模型预测(第12行)，计算准确性(第15行)并得到特征重要性分数(第18行)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="bbfc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们已经看到数据集是不平衡的，T2的患病率比T4的高。我们还发现了一些潜在的代理变量。然而，这一分析并没有告诉我们我们的模型是否会不公平。它只是强调了可能导致不公平模式的问题。在接下来的部分，我们将建立一个模型，并表明它的预测是不公平的。最后，我们将回到这个探索性的分析。我们将看到它如何帮助解释是什么导致了不公平的预测。</p><h1 id="e8c8" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">系统模型化</h1><p id="9d6c" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">我们使用下面的代码构建我们的模型。我们正在使用<strong class="lb iu">xgbclassifier</strong>函数(第2行)。我们使用前面在代理变量一节中定义的特性和目标变量来训练模型。然后我们得到预测(第6行)并将它们添加到我们的<strong class="lb iu"> df_fair </strong>数据集(第7行)。最终，这个模型的准确率为85%。准确率为<strong class="lb iu"> 73% </strong>召回率为<strong class="lb iu"> 60%。我们现在想衡量一下这些预测有多公平。</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="ee52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们继续之前，如果你想的话，你可以用你自己的模型替换这个模型。或者您可以尝试不同的模型特征。这是因为我们将使用的所有公平性度量都是<a class="ae ky" rel="noopener" target="_blank" href="/what-are-model-agnostic-methods-387b0e8441ef">模型不可知的</a>。这意味着它们可以用于任何型号。他们通过将预测与原始目标变量进行比较来工作。最终，您将能够在大多数应用程序中应用这些指标。</p><h1 id="7cb3" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">公平的定义</h1><p id="cbf7" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">我们通过应用不同的公平定义来衡量公平。大多数定义涉及将人口分为特权群体和非特权群体。然后，您使用一些度量标准(例如准确性、FPR、FNR)来比较各组。我们将会看到，最佳指标显示了谁从该模型中受益于<strong class="lb iu">。</strong></p><p id="e241" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常，一个模型的预测要么会给一个人带来好处，要么没有好处。例如，一个银行模型可以预测一个人不会拖欠贷款。这将导致<strong class="lb iu">获得贷款</strong>的收益。另一个好处的例子是<strong class="lb iu">得到了一份工作</strong>。对于我们的模型，我们将假设Y = 1会带来收益。也就是说，如果模型预测这个人的收入超过5万美元，他们就会以某种方式受益。</p><h2 id="d844" class="nk ml it bd mm nl nm dn mq nn no dp mu li np nq mw lm nr ns my lq nt nu na nv bi translated">准确(性)</h2><p id="8079" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">首先，让我们讨论一下准确性，以及为什么它不是公平的理想衡量标准。我们可以根据<strong class="lb iu">图4 </strong>中的混淆矩阵进行精度计算。这是用于比较模型预测和实际目标变量的标准混淆矩阵。这里Y = 1是正预测，Y=0是负预测。当我们计算其他公平性指标时，我们还将参考这个矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/f376d16b078e6c44b2bdb99e488ecd68.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*-hnHerS0oitp3X0yEjgCHw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4:困惑矩阵(来源:作者)</p></figure><p id="19e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">查看<strong class="lb iu">图5 </strong>，您可以看到我们如何使用混淆矩阵来计算精确度。也就是说，准确度是在所有的观察中，真阴性和真阳性的数量。换句话说，准确性是正确预测的百分比。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/88927d8c4311f80d067b3e7e5a349ada.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TAgP-j_02dPpfRiL0fbP-Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5:精确度计算(来源:作者)</p></figure><p id="7983" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">表4 </strong>通过受保护的特征给出了我们模型的精确度。比率列给出了非特权(0)到特权(1)的精确度。对于这两个受保护的功能，您可以看到，非特权组的准确性实际上更高。这些结果可能会误导你，让你认为这种模式是在惠及无特权群体。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/da0183c42faab98e215784ffdf1c8bbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*E4m_GrdskqLLQnncDnp7Yg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表4:受保护特征的模型精度(来源:作者)</p></figure><p id="fc07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">问题是准确性可能会掩盖模型的结果。例如，不正确的肯定预测(FP)会降低准确性。然而，这个人仍然会从这个预测中受益。例如，即使预测是错误的，他们仍然会得到贷款或工作机会。</p><h2 id="bc44" class="nk ml it bd mm nl nm dn mq nn no dp mu li np nq mw lm nr ns my lq nt nu na nv bi translated">机会均等(真实阳性率)</h2><p id="8f9b" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">为了更好地捕捉模型的好处，我们可以使用真实正利率(TPR)。你可以在<strong class="lb iu">图6 </strong>中看到我们是如何计算TPR的。分母是<strong class="lb iu">实际阳性数</strong>。分子是<strong class="lb iu">正确预测阳性</strong>的数量。换句话说，TPR是被正确预测为阳性的实际阳性的百分比。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/11408966a964a4956c89f3b094f7f84b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tkiWVGD5nP77eb1cOO5diw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6: TPR计算(来源:作者)</p></figure><p id="a0b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">记住，我们假设积极的预测会带来一些好处。这意味着分母可以看作是<strong class="lb iu">应该从模型中受益</strong>的人数。分子是<strong class="lb iu">应该并且已经受益的人数。</strong>所以TPR可以解释为<strong class="lb iu">从该模型中正当获益</strong>的人的百分比。</p><p id="11a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，以一个贷款模型为例，其中Y=1表示客户没有违约。分母是没有违约的人数。分子是没有违约的数字，我们预测他们不会违约。这意味着TPR是我们贷款给的好客户的百分比。对于招聘模型，它将被解释为收到工作邀请的优秀候选人的百分比。</p><p id="5296" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">表5 </strong>给出了我们模型的TPR。同样，该比率给出了非特权(0)对特权(1)的TPR。与准确性相比，您可以看到非特权群体的TPR较低。这表明，较少比例的弱势群体从这一模式中受益。也就是说，高收入人群中被正确预测为高收入的比例较小。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/df074737c9b7d44381abe751f6dad179.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*b6pkia8si4MPMMY0Yx1jUw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表5:受保护特征的TPR(来源:作者)</p></figure><p id="0bb8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与流行性一样，我们可以通过在受保护特征的交叉点找到TPR来更进一步。您可以在<strong class="lb iu">表6 </strong>中看到这些值。请注意，当这个人同时属于两个非特权群体时，TPR甚至更低。事实上，白人男性的总生育率比非白人女性高50%以上。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/1bb0bd194acbd1aa95ca5b9c354c9dce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*Y_EOodLgu3ruHWlTtMoluA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表6:受保护特征交集的TPR(来源:作者)</p></figure><p id="13e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用TPRs让我们在<strong class="lb iu">等式1 </strong>中首次定义公平。在<strong class="lb iu">机会均等</strong>下，如果特权群体和非特权群体的TPR相等，我们认为模型是公平的。在实践中，我们会给统计不确定性留有余地。我们可以要求差值小于某个截止值(<strong class="lb iu">等式2 </strong>)。在我们的分析中，我们采用了比率。在这种情况下，我们要求比率大于某个截止值(<strong class="lb iu">等式3 </strong>)。这确保了非特权群体的TPR不会明显小于特权群体。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/7f0486f7887b87500012ecb9926a2dca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BvfRVPArYEOUPl3Ers3Zug.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7:平等机会的定义(来源:作者)</p></figure><p id="b2c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">问题是我们应该使用什么截止值？这个问题实际上没有好的答案。这取决于你的行业和应用。如果你的模型有重大影响，比如抵押贷款申请，你将需要一个更严格的截止日期。截止时间甚至可以由法律来定义。无论哪种方式，在衡量公平性之前，定义临界值是很重要的。</p><h2 id="78f7" class="nk ml it bd mm nl nm dn mq nn no dp mu li np nq mw lm nr ns my lq nt nu na nv bi translated">假阴性率</h2><p id="433e" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">在某些情况下，您可能想要捕捉模型的负面结果。您可以使用图8 中的FNR来完成此操作。同样，分母给出了<strong class="lb iu">实际阳性</strong>的数量。除了现在我们用<strong class="lb iu">错误预测的负数</strong>作为分子。换句话说，FNR是被错误预测为阴性的实际阳性的百分比。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/43489e86a96e36bdf78fd3ea7cf7ec0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6f3Gn0l0mg3QEkdbck_4ng.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图8: FNR定义(来源:作者)</p></figure><p id="8ecb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">FNR可以解释为错误地没有从模型中获益的人的百分比。例如，<strong class="lb iu">应该有</strong>但<strong class="lb iu">没有</strong>获得贷款的客户的百分比。对于我们的模型，它是被预测为低收入的高收入者的百分比。</p><p id="3dba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以在<strong class="lb iu">表7 </strong>中看到我们模型的FNR。现在，非特权群体的净生育率更高。换句话说，更高比例的特权群体错误地没有受益。在这个意义上，我们有一个类似的结论，当使用TPRs与均衡的赔率。这种模式似乎对弱势群体不公平。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/29e1f3307d360257b71f720b3b734603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*RSjLpyS5WDALaLpnIRXwCg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表7:受保护特征的FNR(来源:作者)</p></figure><p id="17ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实上，要求fnr平等会给我们带来与机会平等相同的定义。这是因为在<strong class="lb iu">等式1 </strong>中可以看到线性关系。换句话说，平等的TPR意味着我们也有平等的fnr。你应该记住，我们现在要求这个比率小于某个临界值(<strong class="lb iu">等式2 </strong>)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/79622ab68948a5c494472ac550059bca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*19sxaC2KU3y11Hvc0GtTUA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图9:用FNR评估公平性(来源:作者)</p></figure><p id="a90e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用fnr来定义<strong class="lb iu">平等机会</strong>似乎没有必要。然而，在某些情况下，使用负面结果来定义框架可以更好地说明你的观点。例如，假设我们建立一个模型来预测皮肤癌。FNR将给出患有癌症但未被诊断为癌症的人的百分比。这些错误可能是致命的。最终，以这种方式构建公平可以更好地强调不公平模型的后果。</p><h2 id="0320" class="nk ml it bd mm nl nm dn mq nn no dp mu li np nq mw lm nr ns my lq nt nu na nv bi translated">均等的赔率</h2><p id="5686" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">另一种获取模型益处的方法是通过观察假阳性率(FPR)。如<strong class="lb iu">图10 </strong>所示，分母为<strong class="lb iu">实际底片</strong>的数量。这意味着TPR是被错误预测为阳性的实际阴性的百分比。这可以解释为从模型中不正当获益的人的百分比。例如，它将是收到工作邀请的不合格人员的百分比。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/f76343048a5148a606fc89c48cdc0cb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*32bxzJpx5jq9hL9fzFrYjQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图10: FPR计算(来源:作者)</p></figure><p id="a299" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的模型中，FPR会给出被预测为高收入的低收入者的数量。你可以在<strong class="lb iu">表8 </strong>中看到这些数值。同样，特权群体的税率更高。这告诉我们，更高比例的特权群体<strong class="lb iu">错误地从该模式中获益</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/b44672bff598dbef97d802ec28d91599.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*Z0faLyjzABTiONmjpNfwzg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表8:受保护特征的FPR(来源:作者)</p></figure><p id="0ba3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就引出了公平的第二个定义，<strong class="lb iu">均等几率</strong>。与平等机会一样，这一定义要求贸易保护率是平等的。现在我们还要求FPR相等。这意味着均等的赔率<strong class="lb iu"> </strong>可以被认为是公平的更严格定义。同样有道理的是，一个模型要公平，整体利益应该是平等的。这是一个相似比例的群体应该合法和非法受益。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/9c70327aa5261bb5cbed97da57eea9bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ieXg5G52NQuQ8Zj4zBSPQA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图11:均等赔率定义(来源:作者)</p></figure><p id="5229" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">均等赔率的一个优点是，我们如何定义目标变量并不重要。假设Y = 0会带来收益。在这种情况下，TPR和FPR互换的解释。TPR现在获得了非法利益，而FPR现在获得了合法利益。均等赔率已经使用了这两个比率，因此解释保持不变。相比之下，对平等机会的解释发生了变化，因为它只考虑TPR。</p><h2 id="46e1" class="nk ml it bd mm nl nm dn mq nn no dp mu li np nq mw lm nr ns my lq nt nu na nv bi translated"><strong class="ak">完全不同的影响</strong></h2><p id="83d4" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">我们对公平的最后一个定义是<strong class="lb iu">不同的影响</strong> (DI)。我们首先计算图12 中的购买力平价率。这是被正确(TP)或错误(FP)预测为阳性的人的百分比。我们可以将此解释为将从模型中受益的人的百分比。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/aaafc04640611deb1a1b4636cc0a5c58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I4o6LH0amoNn6fltVB8c_Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图12:购买力平价计算(来源:作者)</p></figure><p id="f969" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于我们的模型，它是我们预测的高收入人群的百分比。您可以在<strong class="lb iu">表9 </strong>中看到这些值。这些数字再次表明，这种模式对弱势群体不公平。也就是说，他们中受益于这种模式的比例较小。虽然，当解释这些值时，我们应该考虑这个定义的一个缺点。我们将在本节的最后讨论这一点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/d8939285df71f897ce724fe5c4d06288.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*rdDsPId11u7P_sqNjwgYEA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表9:不同的影响(来源:作者)</p></figure><p id="80e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据<strong class="lb iu"> DI </strong>，如果我们有相等的购买力平价率，我们认为模型是公平的(<strong class="lb iu">等式1 </strong>)。同样，在实践中，我们使用一个截止值来给出一些余地。这个定义应该代表不同影响的法律概念。在美国，有一个将临界值设定为0.8(T21)的法律先例。即非特权群体的购买力平价不得低于特权群体的购买力平价的80%。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/8d1eefca58d3ef7044698aafce99fd97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zBJ4ogoJUJ7zdZe_0BRnQg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图13:不同的影响定义(来源:作者)</p></figure><p id="eab8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DI的问题在于它没有考虑地面的真实情况。回想一下探索性分析中的流行值。我们看到这些是倾斜的。特权群体的价值更高。对于一个完全精确的模型，我们不会有假阳性。这意味着流行率将与不同的影响率相同。换句话说，即使对于一个完全精确的模型，我们可能仍然有一个低的不同影响比。</p><p id="36f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在某些情况下，期待相同的患病率或DI可能是有意义的。例如，我们会期望一个模型预测同等比例的男性和女性是某个工作的高质量候选人。在其他情况下，没有意义。例如，肤色较浅的人更容易患皮肤癌。我们预计肤色较浅的人患皮肤癌的几率较高。在这种情况下，低DI比率并不表示模型不公平。</p><h2 id="2340" class="nk ml it bd mm nl nm dn mq nn no dp mu li np nq mw lm nr ns my lq nt nu na nv bi translated"><strong class="ak">公平性定义代码</strong></h2><p id="7f40" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">我们使用函数<strong class="lb iu">【公平性_度量】</strong>来得到上面所有的结果。这需要一个带有实际值(y)和预测目标值(y_pred)的数据帧。它使用这些来创建一个混淆矩阵(第5行)。这与我们在<strong class="lb iu">图4 </strong>中看到的4个值相同。我们得到这4个值(第6行)，并使用它们来计算公平性度量(第8–13行)。然后，我们将这些指标作为数组返回(第15行)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="40b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以在下面看到我们如何使用这个函数来实现竞争保护功能。我们首先将人口的子群传递给<strong class="lb iu">公平性度量</strong>函数<strong class="lb iu">。</strong>具体来说，我们得到特权(第2行)和非特权(第3行)组的指标。然后，我们可以获得非特权与特权指标的比率(第6行)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div></figure><h1 id="52df" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">为什么我们的模型有偏差？</h1><p id="b3e4" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">基于对公平的不同定义，我们发现我们的模型对弱势群体是不公平的。然而，这些定义并没有告诉我们为什么我们的模型是不公平的。为此，我们需要做进一步的分析。一个好的起点是回顾我们最初的探索性分析。</p><p id="e51d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，利用相互信息，我们发现婚姻状况是性别的潜在代理变量。通过查看<strong class="lb iu">表10 </strong>中的细分，我们可以开始理解这是为什么。记住，婚姻状况= 1表示这个人已经结婚了。我们可以看到，62%的男性已婚。而人口中只有15%的女性已婚。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f78d082d9f07e9b3ab282a409f7d0dc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*gBfAqmR7yMLAi9cZl29SAA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表10:按性别和婚姻状况分列的人口(资料来源:作者)</p></figure><p id="fa80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<strong class="lb iu">表11 </strong>中，我们可以看到已婚人群的患病率比<strong class="lb iu">高6倍</strong>。模型在进行预测时将使用这种关系。也就是说，它更有可能预测那些收入超过5万美元的已婚人士。问题是，正如我们在上面看到的，大多数已婚人士将是男性。换句话说，女性不太可能结婚，因此模型不太可能预测她们的收入超过5万美元。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/d295b2b68ec1b8e32e4e89697dfd0470.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*JxIwTIhM29SVAv7Szzth7A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表11:按婚姻状况分列的流行率(资料来源:作者)</p></figure><p id="45c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，还有更多的工作要做，以充分解释为什么模型是不公平的。这样做的时候，我们需要考虑所有不公平的潜在<strong class="lb iu">原因。</strong>我们在这篇文章中已经提到了一些。你也可以在下面的第一篇文章中深入了解它们。下一步是<strong class="lb iu">纠正不公平</strong>。在下面的第二篇文章中，我们将探讨定量和非定量方法。</p><div class="og oh gp gr oi oj"><a rel="noopener follow" target="_blank" href="/algorithm-fairness-sources-of-bias-7082e5b78a2c"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd iu gy z fp oo fr fs op fu fw is bi translated">你的模型做出不公平预测的5个原因</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">偏见的常见来源——历史偏见、代理变量、不平衡数据集、算法选择和用户交互</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">towardsdatascience.com</p></div></div><div class="os l"><div class="ot l ou ov ow os ox ks oj"/></div></div></a></div><div class="og oh gp gr oi oj"><a rel="noopener follow" target="_blank" href="/approaches-for-addressing-unfairness-in-machine-learning-a31f9807cf31"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd iu gy z fp oo fr fs op fu fw is bi translated">解决机器学习中的不公平问题</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">预处理、加工中和后处理定量方法。以及非定量方法…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">towardsdatascience.com</p></div></div><div class="os l"><div class="oy l ou ov ow os ox ks oj"/></div></div></a></div></div><div class="ab cl oz pa hx pb" role="separator"><span class="pc bw bk pd pe pf"/><span class="pc bw bk pd pe pf"/><span class="pc bw bk pd pe"/></div><div class="im in io ip iq"><p id="78de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望这篇文章对你有帮助！如果你想看更多，你可以成为我的<a class="ae ky" href="https://conorosullyds.medium.com/membership" rel="noopener"> <strong class="lb iu">推荐会员</strong> </a> <strong class="lb iu">来支持我。你可以访问medium上的所有文章，我可以得到你的部分费用。</strong></p><div class="og oh gp gr oi oj"><a href="https://conorosullyds.medium.com/membership" rel="noopener follow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd iu gy z fp oo fr fs op fu fw is bi translated">通过我的推荐链接加入Medium康纳·奥沙利文</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">conorosullyds.medium.com</p></div></div><div class="os l"><div class="pg l ou ov ow os ox ks oj"/></div></div></a></div><h2 id="1378" class="nk ml it bd mm nl nm dn mq nn no dp mu li np nq mw lm nr ns my lq nt nu na nv bi translated">图像来源</h2><p id="4c46" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">所有图片都是我自己的或从<a class="ae ky" href="http://www.flaticon.com/" rel="noopener ugc nofollow" target="_blank">www.flaticon.com</a>获得的。在后者的情况下，我拥有他们的<a class="ae ky" href="https://support.flaticon.com/hc/en-us/articles/202798201-What-are-Flaticon-Premium-licenses-" rel="noopener ugc nofollow" target="_blank">保费计划</a>中定义的“完全许可”。</p><h2 id="672e" class="nk ml it bd mm nl nm dn mq nn no dp mu li np nq mw lm nr ns my lq nt nu na nv bi translated"><strong class="ak">数据集</strong></h2><p id="d293" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">Kohavi和b . Barry Becker，(1996年)，<strong class="lb iu">成人数据集，</strong> <em class="nx">加州欧文:加州大学信息与计算机科学学院</em>(许可证:CC0:公共领域)<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/adult" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/datasets/adult</a></p><h2 id="b905" class="nk ml it bd mm nl nm dn mq nn no dp mu li np nq mw lm nr ns my lq nt nu na nv bi translated">参考</h2><p id="c5ad" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">Pessach，d .和Shmueli，e .(2020)，<strong class="lb iu">算法公平性。</strong><a class="ae ky" href="https://arxiv.org/abs/2001.09784" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2001.09784</a></p><p id="9fb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Mehrabi，n .、Morstatter，f .、Saxena，n .、Lerman，k .和Galstyan，A .，(2021)，<strong class="lb iu">关于机器学习中的偏见和公平的调查。https://arxiv.org/abs/1908.09635</strong>T2</p><p id="4958" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Hardt，e . Price和and Srebro，2016年。<strong class="lb iu">监督学习中的机会均等。</strong><a class="ae ky" href="https://proceedings.neurips.cc/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf" rel="noopener ugc nofollow" target="_blank">https://proceedings . neur IPS . cc/paper/2016/file/9d 2682367 c 3935 defcb 1 f 9 e 247 a 97 c 0d-paper . pdf</a></p><p id="7478" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Besse，p .，del Barrio，e .，Gordaliza，p .，Loubes，J.M .和Risser，l .，2021年。<strong class="lb iu">通过统计奇偶性的棱镜调查机器学习中的偏差。</strong>T10】https://arxiv.org/abs/2003.14263</p></div></div>    
</body>
</html>