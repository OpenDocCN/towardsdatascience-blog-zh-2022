<html>
<head>
<title>Deep-dive on ML techniques for feature selection in Python - Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入探讨Python中特性选择的ML技术——第1部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-dive-on-ml-techniques-for-feature-selection-in-python-part-1-3574269d5c69#2022-07-10">https://towardsdatascience.com/deep-dive-on-ml-techniques-for-feature-selection-in-python-part-1-3574269d5c69#2022-07-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="0013" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">Python中AI驱动的特征选择！</h2><div class=""/><div class=""><h2 id="10dc" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">基于最大似然的特征选择系列的第一部分，其中我们讨论了流行的过滤方法，如皮尔逊、斯皮尔曼、点双序列相关、克莱姆v和信息值</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/d9dadbc345efd15c7e5353bdae84df84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CmKYIQyo231jZWb9"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@edgr" rel="noopener ugc nofollow" target="_blank"> <strong class="bd lf"> Edu格兰德</strong> </a> <strong class="bd lf"> </strong>上<a class="ae le" href="https://unsplash.com/photos/0vY082Un2pk" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><blockquote class="lg"><p id="ad19" class="lh li iq bd lj lk ll lm ln lo lp lq dk translated">“垃圾进，垃圾出！”</p></blockquote><p id="d333" class="pw-post-body-paragraph lr ls iq lt b lu lv ka lw lx ly kd lz ma mb mc md me mf mg mh mi mj mk ml lq ij bi mm translated">这是一个任何建立机器学习模型的人都可以尽早使用的短语。简单地说，如果模型的输入数据有很多噪声，那么输出数据也会有噪声。</p><p id="8887" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">如今，数据集拥有数百万行和数千个要素，我们需要工具来有效地识别为我们的模型选择哪些要素。这就是特征选择算法适合模型开发旅程的地方。这里有一个它的好处的快速总结—</p><ul class=""><li id="cc24" class="na nb iq lt b lu mv lx mw ma nc me nd mi ne lq nf ng nh ni bi translated">随着过度拟合和<a class="ae le" href="https://www.sciencedirect.com/topics/mathematics/spurious-correlation" rel="noopener ugc nofollow" target="_blank">虚假关系</a>的机会减少，模型精度提高</li><li id="97e8" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq nf ng nh ni bi translated">可以解决<a class="ae le" href="https://en.wikipedia.org/wiki/Multicollinearity" rel="noopener ugc nofollow" target="_blank">多重共线性</a>的问题</li><li id="de22" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq nf ng nh ni bi translated">减少数据量，有助于模型训练更快</li><li id="1054" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq nf ng nh ni bi translated">具有少量相关特征的模型更容易解释</li></ul><p id="dd5f" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">我已经在不同的公司建立数据科学模型5年多了，我发现特征选择算法和用于预测的实际模型一样重要。在机器学习建模的这个关键方面，最近已经有了很多改进。将所有最新和最流行的要素选择方法连同它们的python实现一起放在一个地方不是很好吗？嗯，这是这个博客的灵感！</p><h1 id="dc4d" class="no np iq bd nq nr ns nt nu nv nw nx ny kf nz kg oa ki ob kj oc kl od km oe of bi translated">博客系列部分</h1><ul class=""><li id="89d8" class="na nb iq lt b lu og lx oh ma oi me oj mi ok lq nf ng nh ni bi translated">特征选择方法的类型(上)</li><li id="feb4" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq nf ng nh ni bi translated">相关性:皮尔逊，点双序列，克莱姆的V(第一部分)</li><li id="e4b5" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq nf ng nh ni bi translated">证据权重和信息价值(上)</li><li id="7b02" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq nf ng nh ni bi translated">贝塔系数(第二部分)</li><li id="97ed" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq nf ng nh ni bi translated">套索回归(下)</li><li id="c533" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq nf ng nh ni bi translated">递归特征选择和顺序特征选择器(第二部分)</li><li id="d150" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq nf ng nh ni bi translated">博鲁塔:博鲁塔比，博鲁塔沙普(第三部分)</li><li id="79a9" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq nf ng nh ni bi translated">将所有内容整合在一起(第3部分)</li></ul><h2 id="fe2d" class="ol np iq bd nq om on dn nu oo op dp ny ma oq or oa me os ot oc mi ou ov oe iw bi translated">a)特征选择方法的类型</h2><p id="64c6" class="pw-post-body-paragraph lr ls iq lt b lu og ka lw lx oh kd lz ma ow mc md me ox mg mh mi oy mk ml lq ij bi translated">关于特征选择算法，首先要注意的是，它们分为3类:</p><ol class=""><li id="bff7" class="na nb iq lt b lu mv lx mw ma nc me nd mi ne lq oz ng nh ni bi translated"><strong class="lt ja">过滤方法</strong>:根据单变量统计指标对每个特征进行排名，并挑选排名最高的特征。<br/> <br/> <strong class="lt ja">优点</strong> : <em class="pa">它们是模型不可知的，最容易计算和解释<br/> </em> <strong class="lt ja">缺点</strong> : <em class="pa">最大的缺点是它们不能识别本身是弱预测器但当与其他特征组合时是重要预测器的特征<br/> </em> <strong class="lt ja">示例:</strong>相关性，信息值</li><li id="d072" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq oz ng nh ni bi translated"><strong class="lt ja">包装方法</strong>:使用特征子集，然后使用它们训练用户定义的模型。根据模型的性能，它会添加或删除子集中的特征，并训练另一个模型。这个过程一直持续到达到期望的特征数量或者性能度量达到期望的阈值。<br/></li><li id="41de" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq oz ng nh ni bi translated"><strong class="lt ja">嵌入方法:</strong>像Lasso回归这样的模型有它们自己的内置特征选择方法，它们在回归方程中添加一个惩罚项以减少过度拟合<br/> <br/> <strong class="lt ja">优点</strong> : <em class="pa">比过滤方法更快的计算和更好的准确性</em> <br/> <strong class="lt ja">缺点</strong> : <em class="pa">具有内置特征选择方法的有限模型<br/> </em> <strong class="lt ja">示例:</strong> Lasso回归</li></ol><p id="1551" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">下面是这些方法的简要总结:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pb"><img src="../Images/1e4ccf9f1b4992a36f1877b6fa818a4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6mvNaERwEDKqbg54aZLBSg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="adee" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">对于这个博客系列，我们使用来自UCI机器学习资源库的信用卡客户数据Se的"<a class="ae le" href="https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients" rel="noopener ugc nofollow" target="_blank"> <strong class="lt ja"> <em class="pa">默认值</em></strong></a><strong class="lt ja">"</strong>。它很好地结合了记录(30K)和特征数(24)。每个特性的具体细节可以在<a class="ae le" href="https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients" rel="noopener ugc nofollow" target="_blank">网站</a>上找到。我已经完成了数据集的一些基本准备工作(分配列类型、分类变量的编码等)。)并且代码可以在<a class="ae le" href="https://github.com/IDB-FOR-DATASCIENCE/ML-based-feature-selection.git" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h2 id="81ef" class="ol np iq bd nq om on dn nu oo op dp ny ma oq or oa me os ot oc mi ou ov oe iw bi translated">b)相关性:皮尔逊、点双序列、克拉默V</h2><p id="b820" class="pw-post-body-paragraph lr ls iq lt b lu og ka lw lx oh kd lz ma ow mc md me ox mg mh mi oy mk ml lq ij bi translated">相关性是两个变量之间关系的强度和方向的量化(在我们的例子中，是特征和目标变量之间的量化)。最常见的相关类型是皮尔逊相关，使用以下公式计算:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/a4a277e46d8b25f46c32980778de3713.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*r8SuIzE70kovUEFQvc2Mfg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="aeae" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">但是皮尔逊相关只能计算两个连续变量之间的关系，这是一个主要的限制(特别是对于目标变量是分类变量的分类问题)。没有一个相关性度量可以单独量化所有分类变量对和连续变量对之间的关系。因此，我们需要根据变量类型使用不同的指标。我们还需要记住，出于特性选择的目的，指标应该是可比较的。考虑到这些因素，我倾向于以下组合:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pd"><img src="../Images/4154f8ea071433020bb89910d3d8cacb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*etdrS2gdSAMmFZoPYa0ZYQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="0784" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">我们已经查看了人员关联公式，让我们快速浏览一下表格中提到的其他内容:</p><p id="d5da" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">我)斯皮尔曼:</p><p id="848a" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">第一步是分别计算每列的每个观察值的等级(最高值为等级1)。然后使用以下公式:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/38cdb3c7f49401ea282aac7e9bc99d99.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*Xx0rQLpAyh9aTX4T-eITDA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="f19f" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">计算斯皮尔曼相关的例子可以在<a class="ae le" href="https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide-2.php#:~:text=The%20Spearman%20correlation%20coefficient%2C%20rs%2C%20can%20take%20values%20from,the%20association%20between%20the%20ranks." rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="5e7e" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated"><strong class="lt ja"> ii)点双连载:</strong></p><p id="4603" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">点双序列相关假设分类变量有两个值0和1。我们首先将数据分为两组:</p><p id="d8d8" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated"><em class="pa">组0:其中分类变量= 0 <br/>组1:其中分类变量= 1 </em></p><p id="f47e" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">然后我们使用下面的公式</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pf"><img src="../Images/f1ad1621efa9256469300df67c527a59.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*TJ-f87QVLyghu1a4u5mlSQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="758a" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">计算点双序列相关性的例子可以在<a class="ae le" href="https://statistics.laerd.com/spss-tutorials/point-biserial-correlation-using-spss-statistics.php#:~:text=For%20example%2C%20you%20could%20use,%22%20and%20%22females%22)." rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="6927" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated"><strong class="lt ja">三)克莱姆氏V: </strong></p><p id="52de" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">其计算方法为:<strong class="lt ja"> √(X2/n) / min(c-1，r-1) </strong></p><p id="5b7f" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">其中:</p><ul class=""><li id="89ae" class="na nb iq lt b lu mv lx mw ma nc me nd mi ne lq nf ng nh ni bi translated"><strong class="lt ja"> n: </strong>观察次数</li><li id="ac56" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq nf ng nh ni bi translated"><strong class="lt ja"> c: </strong>列数</li><li id="e370" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq nf ng nh ni bi translated">行数</li><li id="0b9f" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq nf ng nh ni bi translated"><strong class="lt ja"> X2: </strong>卡方统计</li></ul><p id="b982" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">计算克莱姆V的例子可以在<a class="ae le" href="https://www.statology.org/cramers-v-in-excel/" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="7ca6" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated"><em class="pa">根据优缺点，可以考虑各种其他相关性指标(有关更多详细信息，请参考此处的</em><a class="ae le" href="https://medium.com/@outside2SDs/an-overview-of-correlation-measures-between-categorical-and-continuous-variables-4c7f85610365" rel="noopener"><em class="pa"/></a><em class="pa">),但是上述指标是可比的，并且具有相同的范围。</em></p><p id="1d05" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated"><strong class="lt ja"> <em class="pa">这里是一个python函数，用于实现所有的相关性指标:</em> </strong></p><p id="02ba" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">为了计算皮尔逊和双序列相关性，<code class="fe pg ph pi pj b">scipy</code>包有函数<code class="fe pg ph pi pj b">corr</code>和<code class="fe pg ph pi pj b">pointbiserialr</code>。python中没有计算Cramer的V的直接函数，因此我在下面的代码片段中添加了一个函数。功能<code class="fe pg ph pi pj b">corr_feature_selection</code>基于用户指定的相关性类型计算相关性，并且还基于用户提供的阈值选择特征。例如，如果<code class="fe pg ph pi pj b">pearson_threshold</code>是. 5，它将选择绝对皮尔逊相关超过50%的特征。</p><pre class="kp kq kr ks gt pk pj pl pm aw pn bi"><span id="32fe" class="ol np iq pj b gy po pp l pq pr">#1.Select the top n features based on absolute correlation with train_target variable</span><span id="4a45" class="ol np iq pj b gy ps pp l pq pr"># Correlation</span><span id="8401" class="ol np iq pj b gy ps pp l pq pr">pearson_list = []<br/>point_bi_serial_list = ['LIMIT_BAL', 'AGE', 'BILL_AMT1', <br/>                        'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4',<br/>                        'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', <br/>                        'PAY_AMT2', 'PAY_AMT3','PAY_AMT4', <br/>                        'PAY_AMT5', 'PAY_AMT6']</span><span id="19ab" class="ol np iq pj b gy ps pp l pq pr">cramer_list = ['SEX_woe', 'EDUCATION_woe',<br/>               'MARRIAGE_woe', 'PAY_0_woe', <br/>               'PAY_2_woe', 'PAY_3_woe', 'PAY_4_woe',<br/>               'PAY_5_woe', 'PAY_6_woe']</span><span id="3460" class="ol np iq pj b gy ps pp l pq pr">pearson_threshold = .5<br/>point_bi_serial_threshold = .5<br/>cramer_threshold = .1</span><span id="5e00" class="ol np iq pj b gy ps pp l pq pr">################################ Functions #############################################################</span><span id="9496" class="ol np iq pj b gy ps pp l pq pr"># Function to calculate Cramer's V<br/>def cramers_V(var1,var2) :<br/>  crosstab=np.array(pd.crosstab(var1,var2, <br/>                                 rownames=None, colnames=None))<br/>  stat = chi2_contingency(crosstab)[0]<br/>  obs = np.sum(crosstab) <br/>  mini = min(crosstab.shape)-1 <br/>  return (stat/(obs*mini))</span><span id="6591" class="ol np iq pj b gy ps pp l pq pr"># Overall Correlation Function<br/>def corr_feature_selection(data,target,pearson_list,<br/>                           point_bi_serial_list,cramer_list,<br/>                           pearson_threshold,<br/>                           point_bi_serial_threshold,<br/>                           cramer_threshold):<br/>    <br/>    #Inputs<br/>    # data - Input feature data<br/>    # target - Target Variable<br/>    # pearson_list - list of continuous features (if target is continuous)<br/>    # point_bi_serial_list - list of continuous features (if target is categorical)/<br/>    #                        list of categorical features (if target is continuous)   <br/>    # cramer_list - list of categorical features (if target is categorical)<br/>    # pearson_threshold - select features if pearson corrrelation is above this<br/>    # point_bi_serial_threshold - select features if biserial corrrelation is above this<br/>    # cramer_threshold - select features if cramer's v is above this  <br/>    <br/>    corr_data = pd.DataFrame()</span><span id="00ea" class="ol np iq pj b gy ps pp l pq pr"># Calculate point bi-serial<br/>    for i in point_bi_serial_list:<br/>        # Manual Change in Parameters - Point Bi-Serial<br/>        # Link to function parameters - <a class="ae le" href="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pointbiserialr.html" rel="noopener ugc nofollow" target="_blank">https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pointbiserialr.html</a><br/>        pbc = pointbiserialr(target, data[i])   <br/>        corr_temp_data = [[i,pbc.correlation,"point_bi_serial"]]<br/>        corr_temp_df = pd.DataFrame(corr_temp_data, <br/>                                    columns = ['Feature', <br/>                                               'Correlation',<br/>                                               'Correlation_Type'])<br/>        corr_data = corr_data.append(corr_temp_df)</span><span id="43e5" class="ol np iq pj b gy ps pp l pq pr"># Calculate cramer's v<br/>    for i in cramer_list:<br/>        cramer = cramers_V(target, data[i])<br/>        corr_temp_data = [[i,cramer,"cramer_v"]]<br/>        corr_temp_df = pd.DataFrame(corr_temp_data,<br/>                                    columns = ['Feature',<br/>                                               'Correlation',<br/>                                               'Correlation_Type'])<br/>        corr_data = corr_data.append(corr_temp_df)</span><span id="b4f4" class="ol np iq pj b gy ps pp l pq pr"># Calculate pearson correlation<br/>    for i in pearson_list:<br/>        # Manual Change in Parameters - Perason<br/>        # Link to function parameters - <a class="ae le" href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html" rel="noopener ugc nofollow" target="_blank">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html</a><br/>        pearson = target.corr(data[i])<br/>        corr_temp_data = [[i,pearson,"pearson"]]<br/>        corr_temp_df = pd.DataFrame(corr_temp_data,<br/>                                    columns = ['Feature',<br/>                                               'Correlation',<br/>                                               'Correlation_Type'])<br/>        corr_data = corr_data.append(corr_temp_df)</span><span id="9ec3" class="ol np iq pj b gy ps pp l pq pr"># Filter NA and sort based on absolute correlation<br/>    corr_data = corr_data.iloc[corr_data.Correlation.abs().argsort()] <br/>    corr_data = corr_data[corr_data['Correlation'].notna()]<br/>    corr_data = corr_data.loc[corr_data['Correlation'] != 1]<br/>    <br/>    # Add thresholds<br/>    <br/>    # initialize list of lists<br/>    data = [['pearson', pearson_threshold],<br/>            ['point_bi_serial', point_bi_serial_threshold],<br/>            ['cramer_v', cramer_threshold]]</span><span id="9e19" class="ol np iq pj b gy ps pp l pq pr">    threshold_df = pd.DataFrame(data,<br/>                                columns=['Correlation_Type',<br/>                                         'Threshold'])<br/>    corr_data = pd.merge(corr_data,threshold_df,<br/>                         on=['Correlation_Type'],how = 'left')</span><span id="8ba5" class="ol np iq pj b gy ps pp l pq pr"># Select Features with greater than user dfined absolute correlation<br/>    corr_data2 = corr_data.loc[corr_data['Correlation'].abs() &gt; corr_data['Threshold']]<br/>    corr_top_features = corr_data2['Feature'].tolist()<br/>    print(corr_top_features)<br/>    corr_top_features_df = pd.DataFrame(corr_top_features,columns = ['Feature'])<br/>    corr_top_features_df['Method'] = 'Correlation'<br/>    return corr_data,corr_top_features_df</span><span id="4107" class="ol np iq pj b gy ps pp l pq pr">################################ Calculate Correlation #############################################################</span><span id="2871" class="ol np iq pj b gy ps pp l pq pr">corr_data,corr_top_features_df = corr_feature_selection(train_features_v2,train_target,<br/>                                   pearson_list,point_bi_serial_list,<br/>                                   cramer_list,pearson_threshold,<br/>                                   point_bi_serial_threshold,cramer_threshold)<br/>    <br/>    <br/>corr_data.tail(30)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/f316d16ecc7f9d6babfde5c2c98646e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*WVoaI6XFiF7_9KMkV1fIMA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><h1 id="1bb4" class="no np iq bd nq nr ns nt nu nv nw nx ny kf nz kg oa ki ob kj oc kl od km oe of bi translated">c)证据权重和信息价值</h1><p id="4785" class="pw-post-body-paragraph lr ls iq lt b lu og ka lw lx oh kd lz ma ow mc md me ox mg mh mi oy mk ml lq ij bi translated">这两个术语已经广泛用于多个领域的特征选择(尤其是信用评分模型)。WOE表示一个特征的预测能力的程度。它假设模型中的目标变量是二分法的(即有两个值，如事件和非事件)。使用以下步骤进行计算:</p><ol class=""><li id="e695" class="na nb iq lt b lu mv lx mw ma nc me nd mi ne lq oz ng nh ni bi translated">对于连续特征，将数据分割成箱</li><li id="1dd1" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq oz ng nh ni bi translated">对于每个箱，计算事件下的观察百分比和非事件下的观察百分比。</li><li id="20e0" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq oz ng nh ni bi translated">使用以下公式计算每个箱的权重</li></ol><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pu"><img src="../Images/f3f7cac56e5855918c131f326e97677f.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*i9rn4L1Nkiba7kFKFd45jw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="fbd3" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">如上所示，WOE计算特征中每个条柱的预测能力。然后，我们可以使用IV来聚合WOE，以获得该特征作为一个整体的预测能力。其计算方法如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pv"><img src="../Images/a82286a21caa614c1b67a5a2bd7d3052.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*50OFJFnSmm6EtALFb5NEWw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="981e" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">其中h是仓的数量。</p><p id="0f26" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated"><strong class="lt ja"> <em class="pa">下面是一个计算WOE和IV的python函数:</em> </strong></p><p id="4019" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">类似于函数<code class="fe pg ph pi pj b">corr_feature_selection</code>，<code class="fe pg ph pi pj b">iv_woe</code>计算WOE和IV，并基于用户提供的阈值选择特征。例如，如果<code class="fe pg ph pi pj b">iv_threshold</code>是. 1，它将选择IV大于. 1的特征。用户还可以选择连续变量需要多少个箱。</p><pre class="kp kq kr ks gt pk pj pl pm aw pn bi"><span id="4353" class="ol np iq pj b gy po pp l pq pr">#2. Select top features based on information value</span><span id="8f94" class="ol np iq pj b gy ps pp l pq pr"># Information value</span><span id="fb7c" class="ol np iq pj b gy ps pp l pq pr">show_woe = True<br/>iv_bins = 10<br/>iv_threshold = .1</span><span id="57cd" class="ol np iq pj b gy ps pp l pq pr">################################ Functions #############################################################</span><span id="5bc1" class="ol np iq pj b gy ps pp l pq pr">def iv_woe(data, target, iv_bins,iv_threshold, show_woe):<br/>    <br/>    #Inputs<br/>    # data - Input Data including target variable<br/>    # target - Target Variable name<br/>    # iv_bins - Number of iv_bins<br/>    # show_woe - show all the iv_bins and features<br/>    # iv_threshold - select features with IV greater than this<br/>    <br/>    #Empty Dataframe<br/>    newDF,woeDF = pd.DataFrame(), pd.DataFrame()<br/>    <br/>    #Extract Column Names<br/>    cols = data.columns<br/>    <br/>    #Run WOE and IV on all the independent variables<br/>    for ivars in cols[~cols.isin([target])]:<br/>        if (data[ivars].dtype.kind in 'bifc') and (len(np.unique(data[ivars]))&gt;10):<br/>            binned_x = pd.qcut(data[ivars], iv_bins,  duplicates='drop')<br/>            d0 = pd.DataFrame({'x': binned_x, 'y': data[target]})<br/>        else:<br/>            d0 = pd.DataFrame({'x': data[ivars], 'y': data[target]})</span><span id="19fa" class="ol np iq pj b gy ps pp l pq pr"># Calculate the number of events in each group (bin)<br/>        d = d0.groupby("x", as_index=False).agg({"y": ["count", "sum"]})<br/>        d.columns = ['Cutoff', 'N', 'Events']<br/>        <br/>        # Calculate % of events in each group.<br/>        d['% of Events'] = np.maximum(d['Events'], 0.5) / d['Events'].sum()</span><span id="ed24" class="ol np iq pj b gy ps pp l pq pr"># Calculate the non events in each group.<br/>        d['Non-Events'] = d['N'] - d['Events']<br/>        # Calculate % of non events in each group.<br/>        d['% of Non-Events'] = np.maximum(d['Non-Events'], 0.5) / d['Non-Events'].sum()</span><span id="871a" class="ol np iq pj b gy ps pp l pq pr"># Calculate WOE by taking natural log of division of % <br/>        # of non-events and % of events<br/>        d['WoE'] = np.log(d['% of Events']/d['% of Non-Events'])<br/>        d['IV'] = d['WoE'] * (d['% of Events'] - d['% of Non-Events'])<br/>        d.insert(loc=0, column='Variable', value=ivars)<br/>        print("Information value of " + ivars + " is " + <br/>              str(round(d['IV'].sum(),6)))<br/>        temp =pd.DataFrame({"Variable" : [ivars],<br/>                            "IV" : [d['IV'].sum()]},<br/>                           columns = ["Variable", "IV"])<br/>        newDF=pd.concat([newDF,temp], axis=0)<br/>        woeDF=pd.concat([woeDF,d], axis=0)</span><span id="fab3" class="ol np iq pj b gy ps pp l pq pr">#Show WOE Table<br/>        if show_woe == True:<br/>            print(d)<br/>    <br/>    # Aggregate IV at feature level<br/>    woeDF_v2 = pd.DataFrame(woeDF.groupby('Variable')['IV'].agg('sum'),<br/>                            columns= ['IV']).reset_index()<br/>    woeDF_v3 = woeDF_v2.sort_values(['IV'], ascending = False)<br/>    IV_df = woeDF_v2[woeDF_v2['IV']&gt; iv_threshold]<br/>    woe_top_features = IV_df['Variable'].tolist()<br/>    print(woe_top_features)<br/>    woe_top_features_df = pd.DataFrame(woe_top_features,columns = ['Feature'])<br/>    woe_top_features_df['Method'] = 'Information_value'<br/>    return newDF, woeDF,IV_df, woe_top_features_df</span><span id="772e" class="ol np iq pj b gy ps pp l pq pr">################################ Calculate IV #############################################################</span><span id="70dc" class="ol np iq pj b gy ps pp l pq pr">train_features_v3_temp = pd.concat([train_target, train_features_v2],<br/>                                   axis =1)</span><span id="eb3a" class="ol np iq pj b gy ps pp l pq pr">newDF, woeDF,IV_df, woe_top_features_df = iv_woe(train_features_v3_temp, <br/>                                              target,iv_bins,iv_threshold, <br/>                                              show_woe)<br/>woeDF.head(n=50)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pw"><img src="../Images/54f88672fdc7850de35ff8848e10a214.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pyUO7cczv20yewrrJfod8A.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="952b" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">以下是基于IV的常见特征分类:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi px"><img src="../Images/2a7d79f9b1fb4818bcf71c69203e272c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*fP8e3DOHx1xc-8Et5KTxYg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="e3fd" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">鉴于WoE和IV善于解释线性关系，记住使用IV选择的特征可能不是非线性模型的最佳特征集是很重要的。此外，这种特征选择方法应该仅用于分类问题。</p><h1 id="ee39" class="no np iq bd nq nr ns nt nu nv nw nx ny kf nz kg oa ki ob kj oc kl od km oe of bi translated">最后的话</h1><p id="2eb6" class="pw-post-body-paragraph lr ls iq lt b lu og ka lw lx oh kd lz ma ow mc md me ox mg mh mi oy mk ml lq ij bi translated">我希望这个博客系列能够帮助其他数据科学家使用最大似然法提供的最佳方法来识别他们数据集中真正的瑰宝。整个端到端的分析可以在<a class="ae le" href="https://github.com/IDB-FOR-DATASCIENCE/ML-based-feature-selection.git" rel="noopener ugc nofollow" target="_blank">这里</a>找到。在第一部分中，我们讨论了以下内容:</p><ul class=""><li id="09fb" class="na nb iq lt b lu mv lx mw ma nc me nd mi ne lq nf ng nh ni bi translated"><em class="pa">各类特征选择方法概述</em></li><li id="7d2b" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq nf ng nh ni bi translated"><em class="pa">用于特征选择的相关性指标类型——理论和python实现</em></li><li id="8bb4" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq nf ng nh ni bi translated"><em class="pa">什么是WOE和IV，如何计算？和他们的python实现</em></li></ul><p id="ae34" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">尽管我们在博客中讨论的过滤方法易于计算和理解，但它们并不是多变量建模(具有多个特征的模型)的最佳选择。这就是为什么我会敦促读者去看看下一个<a class="ae le" href="https://indraneeldb1993ds.medium.com/deep-dive-on-ml-techniques-for-feature-selection-in-python-part-2-c258f8a2ac43" rel="noopener">博客</a>，它关注一些有趣的包装器和嵌入式方法，比如套索回归、Beta系数、递归特征选择等等..</p><p id="869b" class="pw-post-body-paragraph lr ls iq lt b lu mv ka lw lx mw kd lz ma mx mc md me my mg mh mi mz mk ml lq ij bi translated">你对这个博客有什么问题或建议吗？请随时留言。</p><h1 id="9578" class="no np iq bd nq nr ns nt nu nv nw nx ny kf nz kg oa ki ob kj oc kl od km oe of bi translated">参考材料</h1><ul class=""><li id="72b0" class="na nb iq lt b lu og lx oh ma oi me oj mi ok lq nf ng nh ni bi translated"><a class="ae le" href="https://medium.com/@outside2SDs/an-overview-of-correlation-measures-between-categorical-and-continuous-variables-4c7f85610365" rel="noopener"> <em class="pa">分类变量和连续变量之间相关性度量的概述</em> </a></li><li id="e6ad" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq nf ng nh ni bi translated"><a class="ae le" href="https://www.statology.org/cramers-v-in-excel/" rel="noopener ugc nofollow" target="_blank"> <em class="pa">如何在Excel中计算克莱姆的V</em></a></li><li id="70ab" class="na nb iq lt b lu nj lx nk ma nl me nm mi nn lq nf ng nh ni bi translated"><a class="ae le" href="https://statistics.laerd.com/spss-tutorials/point-biserial-correlation-using-spss-statistics.php#:~:text=For%20example,%20you%20could%20use,%22%20and%20%22females%22)." rel="noopener ugc nofollow" target="_blank"> <em class="pa">点-双列相关采用SPSS统计</em> </a></li></ul><h1 id="5170" class="no np iq bd nq nr ns nt nu nv nw nx ny kf nz kg oa ki ob kj oc kl od km oe of bi translated">我们连线吧！</h1><p id="b616" class="pw-post-body-paragraph lr ls iq lt b lu og ka lw lx oh kd lz ma ow mc md me ox mg mh mi oy mk ml lq ij bi translated">如果你和我一样，对AI、数据科学或经济学充满热情，请随时在<a class="ae le" href="http://www.linkedin.com/in/indraneel-dutta-baruah-ds" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae le" href="https://github.com/IDB-FOR-DATASCIENCE" rel="noopener ugc nofollow" target="_blank"> Github </a>和<a class="ae le" href="https://medium.com/@indraneeldb1993ds" rel="noopener"> Medium </a>上添加/关注我。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/8732644597d38f7e1d1142c7cf8bb513.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*myKBxCQFdY64h3a4"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">皮特·佩德罗萨在<a class="ae le" href="https://unsplash.com/photos/VyC0YSFRDTU" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure></div></div>    
</body>
</html>