<html>
<head>
<title>3-Step Feature Selection Guide in Sklearn to Superchage Your Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Sklearn中的3步功能选择指南，让您的模型焕然一新</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/3-step-feature-selection-guide-in-sklearn-to-superchage-your-models-e994aa50c6d2#2022-10-04">https://towardsdatascience.com/3-step-feature-selection-guide-in-sklearn-to-superchage-your-models-e994aa50c6d2#2022-10-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0617" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">为任何受监督的问题开发一个健壮的特征选择工作流</h2></div><p id="4c21" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">了解如何使用最好的Sklearn功能选择器来面对机器学习的最大挑战之一。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/b10b49b0f3d8e813b426f2dd59cd6f9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fittSjUEJ0EbMK1Kz_whHg.jpeg"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">照片由<a class="ae lu" href="https://www.pexels.com/photo/person-s-index-finger-977246/" rel="noopener ugc nofollow" target="_blank">斯蒂夫·约翰森</a>拍摄</p></figure><h2 id="3679" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">介绍</h2><p id="fba9" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">如今，数据集拥有数百甚至数千个要素是很常见的。从表面上看，这似乎是一件好事——更多的特性提供了关于每个样本的更多信息。但通常情况下，这些额外的功能并不能提供太多的价值，反而会带来复杂性。</p><p id="df11" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">机器学习的最大挑战是通过使用尽可能少的特征来创建具有强大预测能力的模型。但是考虑到当今数据集的庞大规模，很容易忽略哪些特征重要，哪些不重要。</p><p id="21c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是为什么在ML领域需要学习一整套技能——特征选择。特征选择是选择最重要特征的子集，同时试图保留尽可能多的信息的过程(摘自本系列的<a class="ae lu" rel="noopener" target="_blank" href="/how-to-use-variance-thresholding-for-robust-feature-selection-a4503f2b5c3f">第一篇文章</a>)。</p><p id="720b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于功能<em class="mt">选择</em>是一个如此紧迫的问题，有无数的解决方案可供你选择<em class="mt">🤦‍♂️🤦‍♂️.为了减轻你的痛苦，我将教你3个特征选择技巧，当一起使用时，可以增强任何模型的性能。</em></p><p id="4fa9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本文将向您概述这些技术以及如何使用它们，而不需要过多了解其内部原理。为了更深入的理解，我为每一个都写了单独的帖子，并解释了细节。我们开始吧！</p><div class="mu mv gp gr mw mx"><a href="https://ibexorigin.medium.com/membership" rel="noopener follow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd iu gy z fp nc fr fs nd fu fw is bi translated">通过我的推荐链接加入Medium-BEXGBoost</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">获得独家访问我的所有⚡premium⚡内容和所有媒体没有限制。支持我的工作，给我买一个…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">ibexorigin.medium.com</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl lo mx"/></div></div></a></div><p id="ae20" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">获得由强大的AI-Alpha信号选择和总结的最佳和最新的ML和AI论文:</p><div class="mu mv gp gr mw mx"><a href="https://alphasignal.ai/?referrer=Bex" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd iu gy z fp nc fr fs nd fu fw is bi translated">阿尔法信号|机器学习的极品。艾总结的。</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">留在循环中，不用花无数时间浏览下一个突破；我们的算法识别…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">alphasignal.ai</p></div></div><div class="ng l"><div class="nm l ni nj nk ng nl lo mx"/></div></div></a></div></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h2 id="dc49" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">数据集介绍和问题陈述</h2><p id="0a64" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">我们将使用安苏尔男性数据集，该数据集包含100多种不同的美国陆军人员身体测量数据。在这个特征选择系列中，我一直在过度使用这个数据集，因为它包含98个数字特征，这是一个教授特征选择的完美数据集。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nu nv l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl nw"><img src="../Images/2b33c1ca8041f32f610458d5bfaf1059.png" data-original-src="https://miro.medium.com/v2/format:webp/1*nJCmTg0MtSw0j1dApICuKg.png"/></div></figure><p id="3742" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将尝试预测以磅为单位的重量，因此这是一个回归问题。让我们用简单的线性回归建立一个基本性能。LR是这个问题的一个很好的候选，因为我们可以预期身体测量是线性相关的:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="cd33" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于基本性能，我们得到了令人印象深刻的0.956的R平方。然而，这可能是因为在特性中也有以千克为单位的重量列，为算法提供了它所需要的一切(我们试图预测以磅为单位的重量)。所以，让我们试着不用它:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="c864" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们有0.945，但我们设法降低了模型的复杂性。</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h2 id="3e1a" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">第一步:方差阈值</h2><p id="d904" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">第一种技术将针对每个特征的单独属性。方差阈值化背后的思想是，具有低方差的特征对整体预测没有太大贡献。这些类型的要素的分布具有太少的唯一值或足够低的方差，因此无关紧要。VT使用Sklearn帮助我们移除它们。</p><p id="5987" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">应用VT之前的一个关注点是特征的规模。随着要素中的值变大，方差呈指数增长。这意味着不同分布的特征具有不同的尺度，因此我们不能安全地比较它们的方差。因此，我们必须应用某种形式的标准化，将所有特征置于相同的比例，然后应用VT。代码如下:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="71b3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">归一化后(这里，我们将每个样本除以特征的平均值)，您应该选择一个介于0和1之间的阈值。我们没有使用VT估算器的<code class="fe nx ny nz oa b">.transform()</code>方法，而是使用了<code class="fe nx ny nz oa b">get_support()</code>，它给出了一个布尔掩码(应该保留的特征的真值)。然后，可以用它来对数据进行子集划分，同时保留列名。</p><p id="6a1a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这可能是一个简单的技术，但它可以帮助消除无用的功能。要获得更深入的见解和对代码的更多解释，您可以阅读本文:</p><div class="mu mv gp gr mw mx"><a rel="noopener follow" target="_blank" href="/how-to-use-variance-thresholding-for-robust-feature-selection-a4503f2b5c3f"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd iu gy z fp nc fr fs nd fu fw is bi translated">如何使用方差阈值进行鲁棒特征选择</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">编辑描述</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">towardsdatascience.com</p></div></div><div class="ng l"><div class="ob l ni nj nk ng nl lo mx"/></div></div></a></div></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h2 id="2a6c" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">第二步:两两相关</h2><p id="b12e" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">我们将通过关注特征之间的<em class="mt">关系</em>来进一步修整我们的数据集。显示<em class="mt">线性</em>关系的最佳指标之一是皮尔逊相关系数(表示为<em class="mt"> r </em>)。使用<em class="mt"> r </em>进行特征选择的逻辑很简单。如果特征A和B之间的相关性为0.9，这意味着您可以在90%的情况下使用A的值来预测B的值。换句话说，在存在A的数据集中，您可以丢弃B，反之亦然。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl nw"><img src="../Images/8fa8a639601dcd11573ad82b0c30512c.png" data-original-src="https://miro.medium.com/v2/format:webp/1*vrkgxsisgHUJnyRQYOzCQg.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><strong class="bd oc">作者照片</strong></p></figure><p id="f3cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">没有一个Sklearn估算器实现基于相关性的特征选择。所以，我们要靠自己:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="7527" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个函数是一个简写，它返回根据自定义相关阈值应该删除的列的名称。通常，阈值将超过0.8才是安全的。</p><p id="a2d9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在函数中，我们首先使用<code class="fe nx ny nz oa b">.corr()</code>创建一个相关矩阵。接下来，我们创建一个布尔掩码，只包含相关矩阵对角线以下的相关性。我们用这个掩模来划分矩阵的子集。最后，在列表理解中，我们找到应该删除的特征的名称并返回它们。</p><p id="b8a9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">关于代码，我还有很多没有解释。即使这个函数工作得很好，我还是建议阅读我的另一篇关于基于相关系数的特征选择的文章。我充分解释了相关性的概念以及它与因果关系的不同之处。还有一个单独的部分是关于将完美的相关矩阵绘制成热图，当然还有对上述函数的解释。</p><div class="mu mv gp gr mw mx"><a rel="noopener follow" target="_blank" href="/how-to-use-pairwise-correlation-for-robust-feature-selection-20a60ef7d10"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd iu gy z fp nc fr fs nd fu fw is bi translated">如何使用成对相关性进行鲁棒的特征选择</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">将久经考验的方法添加到您的武器库中</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">towardsdatascience.com</p></div></div><div class="ng l"><div class="od l ni nj nk ng nl lo mx"/></div></div></a></div><p id="5bcb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于我们的数据集，我们将选择阈值0.9:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="08d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该函数告诉我们删除13个功能:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="10de" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，只剩下35个特征了。</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h2 id="664e" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">步骤三:交叉验证的递归特征消除(RFECV)</h2><p id="62be" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">最后，我们将根据特性对模型性能的影响来选择最终的特性集。大多数Sklearn模型都有<code class="fe nx ny nz oa b">.coef_</code>(线性模型)或<code class="fe nx ny nz oa b">.feature_importances_</code>(基于树和集合模型)属性，显示每个特征的重要性。例如，让我们将线性回归模型拟合到当前的要素集，并查看计算出的系数:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nu nv l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl nw"><img src="../Images/cec88842d54670eb2d4d1f02796a6faf.png" data-original-src="https://miro.medium.com/v2/format:webp/1*B6sseh6V5HCUhBhqiMtRoQ.png"/></div></figure><p id="6c44" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的数据框显示了系数最小的特征。特征的权重或系数越小，它对模型预测能力的贡献就越小。考虑到这一点，递归特征消除使用交叉验证逐个移除特征，直到剩下最佳的最小特征集。</p><p id="b146" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Sklearn在RFECV类下实现了这种技术，它采用一个任意的估计量和几个其他参数:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="b29c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在将估计量拟合到数据之后，我们可以得到一个布尔掩码，其中的真值对应该保留的特征进行编码。我们最终可以用它来最后一次对原始数据进行子集划分:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="b634" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在应用RFECV之后，我们成功地放弃了另外5个特性。让我们在此要素所选数据集上评估最终的GradientBoostingRegressor模型，并查看其性能:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="8ea9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管我们的性能略有下降，但我们设法删除了近70个特性，从而显著降低了模型的复杂性。</p><p id="75b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在另一篇文章中，我进一步讨论了<code class="fe nx ny nz oa b">.coef_</code>和<code class="fe nx ny nz oa b">.feature_importances_</code>属性，以及RFE每一轮淘汰赛中发生的额外细节:</p><div class="mu mv gp gr mw mx"><a rel="noopener follow" target="_blank" href="/powerful-feature-selection-with-recursive-feature-elimination-rfe-of-sklearn-23efb2cdb54e"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd iu gy z fp nc fr fs nd fu fw is bi translated">Sklearn的递归特征消除(RFE)功能强大的特征选择</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">编辑描述</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">towardsdatascience.com</p></div></div><div class="ng l"><div class="oe l ni nj nk ng nl lo mx"/></div></div></a></div></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h2 id="6683" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">摘要</h2><p id="b87e" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">功能选择不应该掉以轻心。在降低模型复杂性的同时，由于数据集中缺少分散注意力的特征，一些算法甚至可以看到性能的提高。依赖单一方法也是不明智的。相反，从不同的角度和使用不同的技巧来解决问题。</p><p id="1412" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">今天，我们了解了如何分三个阶段将要素选择应用于数据集:</p><ol class=""><li id="919a" class="of og it kk b kl km ko kp kr oh kv oi kz oj ld ok ol om on bi translated">基于使用方差阈值的每个特征的属性。</li><li id="0c6a" class="of og it kk b kl oo ko op kr oq kv or kz os ld ok ol om on bi translated">基于使用成对相关的特征之间的关系。</li><li id="dc69" class="of og it kk b kl oo ko op kr oq kv or kz os ld ok ol om on bi translated">基于特征如何影响模型的性能。</li></ol><p id="f3fb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在处理过程中使用这些技术应该可以为你所面临的任何监督问题提供可靠的结果。</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h2 id="efaf" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">关于特征选择的进一步阅读</h2><ul class=""><li id="94ef" class="of og it kk b kl mo ko mp kr ot kv ou kz ov ld ow ol om on bi translated">Sklearn官方<a class="ae lu" href="https://scikit-learn.org/stable/modules/feature_selection.html" rel="noopener ugc nofollow" target="_blank">功能选择指南</a></li><li id="3841" class="of og it kk b kl oo ko op kr oq kv or kz os ld ow ol om on bi translated"><a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html" rel="noopener ugc nofollow" target="_blank">差异阈值文档</a></li><li id="c00a" class="of og it kk b kl oo ko op kr oq kv or kz os ld ow ol om on bi translated"><a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html" rel="noopener ugc nofollow" target="_blank"> RFECV文件</a></li><li id="9750" class="of og it kk b kl oo ko op kr oq kv or kz os ld ow ol om on bi translated"><a class="ae lu" href="https://towardsdev.com/how-to-not-misunderstand-correlation-75ce9b0289e" rel="noopener ugc nofollow" target="_blank">相关系数深度指南</a></li><li id="6e11" class="of og it kk b kl oo ko op kr oq kv or kz os ld ow ol om on bi translated"><a class="ae lu" rel="noopener" target="_blank" href="/superior-feature-selection-by-combining-multiple-models-607002c1a324">通过组合多个模型进行高级特征选择</a></li></ul><div class="mu mv gp gr mw mx"><a href="https://ibexorigin.medium.com/membership" rel="noopener follow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd iu gy z fp nc fr fs nd fu fw is bi translated">通过我的推荐链接加入Medium。</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">获得独家访问我的所有⚡premium⚡内容和所有媒体没有限制。你可以给我买一个coffee☕，用你的…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">ibexorigin.medium.com</p></div></div><div class="ng l"><div class="ox l ni nj nk ng nl lo mx"/></div></div></a></div><div class="mu mv gp gr mw mx"><a href="https://ibexorigin.medium.com/subscribe" rel="noopener follow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd iu gy z fp nc fr fs nd fu fw is bi translated">每当Bex T .发布时收到电子邮件。</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">每当Bex T .发布时收到电子邮件。注册后，如果您还没有中型帐户，您将创建一个…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">ibexorigin.medium.com</p></div></div><div class="ng l"><div class="oy l ni nj nk ng nl lo mx"/></div></div></a></div></div></div>    
</body>
</html>