<html>
<head>
<title>Retrain, or not Retrain? Online Machine Learning with Gradient Boosting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">再培训，还是不再培训？具有梯度推进的在线机器学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/retrain-or-not-retrain-online-machine-learning-with-gradient-boosting-9ccb464415e7#2022-06-22">https://towardsdatascience.com/retrain-or-not-retrain-online-machine-learning-with-gradient-boosting-9ccb464415e7#2022-06-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b68a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Scikit-Learn中持续学习的改装策略比较</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/af86c0cedd01d86d4bc2e52650dcd548.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qOHe9FXD9TAGcriR"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@vdphotography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> VD摄影</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="2464" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练机器学习模型需要精力、时间和耐心。聪明的<strong class="lb iu">数据科学家组织实验，并对历史数据</strong>进行跟踪试验，以部署最佳解决方案。当我们将新的可用样本传递给我们的预构建机器学习管道时，可能会出现问题。在预测算法的情况下，<strong class="lb iu">记录的性能可能偏离预期的性能</strong>。</p><p id="dbc9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">差异背后的原因多种多样。除了技术错误之外，<strong class="lb iu">最常见也最令人担忧的原因是数据漂移</strong>。从标准分布转变到偷偷摸摸的多元和概念漂移，我们必须准备处理所有这些情况。</p><p id="3b6c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本帖中，<strong class="lb iu">我们不关注如何检测数据漂移。我们尝试概述如何应对数据漂移</strong>。最近已经引入了许多有趣的工具和新奇的技术来促进数据漂移检测。那很酷，但是之后我们能做什么呢？<strong class="lb iu"> <em class="lv">“改装就是你需要的一切”</em> </strong>是用来处理这种情况的最广为人知的口号。换句话说，当新的标记数据变得可用时，我们应该让我们的模型不断地从中学习新的见解。</p><p id="32cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于<strong class="lb iu">在线机器学习</strong>，我们指的是一个多步骤的训练过程，以允许我们的算法动态地适应新的模式。如果制作得当，它可能会比从头再训练提供更大的好处(在速度和性能方面)。这正是我们在这篇文章中想要测试的。</p><h1 id="bd51" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">实验设置</h1><p id="6a00" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu">我们设想在一个流环境中运行</strong>，在这个环境中，我们可以定期访问新的标记数据，计算感兴趣的指标，并重新训练我们的预测模型。</p><p id="cb2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们模拟了一个概念漂移的场景。我们有一些特征可以保持稳定的分布，并且随着时间的推移保持不变。我们的目标是这些特征的线性组合。每个单一特征对目标的贡献是动态的，并且不是随时间恒定的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/8f02f2c1f45ad8958edd5f1c5fdd700b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WGvmGuzZj90PRVSU.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">左:模拟特征。右图:模拟系数(图片由作者提供)</p></figure><p id="ffcd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，<strong class="lb iu">使用一段时间前训练的相同预测模型可能是无用的</strong>。让我们研究一下我们可以选择的方案。</p><h1 id="dc1e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">改装是你所需要的</h1><p id="751e" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">为了了解目标和特征<strong class="lb iu">之间的关系，我们需要经常更新我们的模型</strong>。从这个意义上来说，我们有不同的战略可以选择。</p><p id="25c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以采用<em class="lv">状态学习</em>，在这里，我们以一些预定义的时间间隔初始化，用我们处理的数据从头开始训练。我们要做的就是将新样本和历史样本合并。由于我们重新创建了先前拟合的模型，因此不需要存储它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/4f33f0ac5a2e2fd1c0bbe5272d011f74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EGcg8-7PXczK3PVGTJvTYA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">有状态学习(图片由作者提供)</p></figure><p id="4f5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">状态学习的变体</em>它是<em class="lv">加权状态学习。它在于给予最新的观察更高的权重。这可能有助于衡量更多的最新数据，并使新模型专注于最新的模式。进行加权训练很简单。许多最新的机器学习算法实现提供了内置的可能性，为每个样本赋予不同的权重。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/60037446cfd2dfd836da660cd30b865a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ewac8XB6-eeHNcX3anuSdQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">加权状态学习(图片由作者提供)</p></figure><p id="1d90" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，我们可以考虑<em class="lv">连续学习</em>，<em class="lv"> </em>又名<em class="lv">在线机器学习</em>。在<em class="lv">连续学习</em>中，我们使用先前的模型知识来初始化新的训练步骤。我们采用新的可用样本集，并使之前拟合的模型从中学习新的模式。通过更新(而不是重新初始化)模型知识，我们希望获得更好的性能，降低从头开始训练的成本。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/6a03aaae7829970703bf9ece30ede1a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WSQNCDvfv2cX9ewx-C1aCQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">持续学习(图片由作者提供)</p></figure><h1 id="57de" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">实践中的在线机器学习</h1><p id="0b5d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">所有基于神经网络的算法都支持在线机器学习。我们可以随时通过传递新数据来更新样本损失，从而继续训练过程。</p><p id="d3d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实事求是地说，在scikit-learn生态系统中，所有支持<code class="fe mx my mz na b"><strong class="lb iu"><em class="lv">partial_fit</em></strong></code>方法的算法都可以进行持续学习。在下面的代码片段中，我们介绍了如何用几行代码就能做到这一点。</p><pre class="kj kk kl km gt nb na nc nd aw ne bi"><span id="aaf3" class="nf lx it na b gy ng nh l ni nj">cv = TimeSeriesSplit(n_splits, test_size=test_size)</span><span id="4dea" class="nf lx it na b gy nk nh l ni nj">for i,(id_train,id_test) in enumerate(cv.split(X)):<br/>    if i&gt;0:<br/>        model = model.<strong class="na iu">partial_fit</strong>(<br/>            X[id_train[-test_size:]], y[id_train[-test_size:]]<br/>        )<br/>    else:<br/>        model = SGDRegressor(**fit_params).fit(<br/>            X[id_train], y[id_train]<br/>        )</span></pre><p id="4302" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回到我们的实验，我们使用一个<code class="fe mx my mz na b"><strong class="lb iu">SGDRegressor</strong></code>在我们的模拟数据上测试三个提到的训练策略(<em class="lv">状态学习、加权状态学习、</em>和<em class="lv">连续学习</em>)。我们不会只做一次，而是通过模拟不同的场景来做多次，以更好地处理模拟过程中的可变性。我们定期评估20个周期的模型，并存储所有模拟场景的预测误差(计算为SMAPE)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/ecf6aa4c6f276417683180a2192ea5f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/0*uw8gBExbqeu1Tp6D.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">SGDRegressor在线机器学习性能(图片由作者提供)</p></figure><p id="1be1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，与其他策略相比，<em class="lv">持续学习</em>可以实现最佳性能。考虑到最近的观察结果，<em class="lv">加权状态学习</em>也可以比标准<em class="lv">状态学习</em>做得更好。</p><p id="5dde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些结果听起来很有希望。<strong class="lb iu">有没有可能用其他算法做在线机器学习？</strong>我们知道基于树的梯度推进的巨大威力。由于它们在各种情况下的适应性，许多机器学习项目都使用它们。如果能和他们一起操作在线机器学习，那就太好了。</p><p id="9c00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望我们能做到！和前面的例子一样简单。我们报告了一个片段，其中我们介绍了如何使用<code class="fe mx my mz na b"><strong class="lb iu">LGBMRegressor</strong></code>来实现它。</p><pre class="kj kk kl km gt nb na nc nd aw ne bi"><span id="3878" class="nf lx it na b gy ng nh l ni nj">cv = TimeSeriesSplit(n_splits, test_size=test_size)</span><span id="dcdf" class="nf lx it na b gy nk nh l ni nj">for i,(id_train,id_test) in enumerate(cv.split(X)):<br/>    if i&gt;0:<br/>        model = LGBMRegressor(**fit_params).fit(<br/>            X[id_train[-test_size:]], y[id_train[-test_size:]],<br/>            <strong class="na iu">init_model = model.booster_</strong><br/>        )<br/>    else:<br/>        model = LGBMRegressor(**fit_params).fit(<br/>            X[id_train], y[id_train]<br/>        )</span></pre><p id="4346" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们在模拟场景中看看它的实际应用。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/3fdc619d578f946acd477dfc79921b8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*pSjFzHJs9H2Agh0-CtZhJw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">LGBMRegressor在线机器学习性能(图片由作者提供)</p></figure><p id="205f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们取得了和以前一样令人满意的结果。<strong class="lb iu">如果处理得当，在线机器学习听起来是有效的，并且可用于不同的算法</strong>。</p><h1 id="c305" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">摘要</h1><p id="4e32" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在这篇文章中，我们介绍了在线机器学习的概念。我们探索了不同的<em class="lv">有状态</em>改装策略，将它们与<em class="lv">持续学习</em>方法进行比较。在线机器学习在某些应用中被证明是一种很好的测试方法。<strong class="lb iu">做在线机器学习有点艺术</strong>。人们并不认为这可能会提高成绩。高使模型忘记它所学的东西是有风险的(<a class="ae ky" href="https://en.wikipedia.org/wiki/Catastrophic_interference" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">灾难性遗忘</strong> </a>)。从这个意义上来说，拥有一个可靠且充分的验证策略比以往任何时候都更加重要。</p><p id="7683" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">如果你对题目感兴趣，我建议:</strong></p><ul class=""><li id="6ebf" class="nm nn it lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated"><a class="ae ky" href="https://medium.com/towards-data-science/shap-for-drift-detection-effective-data-shift-monitoring-c7fb9590adb0" rel="noopener"> <strong class="lb iu">漂移检测SHAP:有效数据漂移监控</strong> </a></li><li id="4f00" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><a class="ae ky" href="https://medium.com/towards-data-science/data-drift-explainability-interpretable-shift-detection-with-nannyml-83421319d05f" rel="noopener"> <strong class="lb iu">数据漂移可解释性:用NannyML进行可解释的移位检测</strong> </a></li><li id="85c4" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/model-tree-handle-data-shifts-mixing-linear-model-and-decision-tree-facfd642e42b"> <strong class="lb iu">模型树:混合线性模型和决策树</strong> </a>处理数据移位</li></ul></div><div class="ab cl oa ob hx oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="im in io ip iq"><p id="8fee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/cerlymarco/MEDIUM_NoteBook" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">查看我的GITHUB回购</strong> </a></p><p id="e337" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">保持联系:<a class="ae ky" href="https://www.linkedin.com/in/marco-cerliani-b0bba714b/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a></p></div></div>    
</body>
</html>