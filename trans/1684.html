<html>
<head>
<title>Coding A Neural Network From Scratch in NumPy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在NumPy中从头开始编写神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/coding-a-neural-network-from-scratch-in-numpy-31f04e4d605#2022-04-21">https://towardsdatascience.com/coding-a-neural-network-from-scratch-in-numpy-31f04e4d605#2022-04-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f699" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过从零开始实现人工智能来加深您对人工智能的理解</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b8dcb921e91ccc9ec8c08ef106a4ea7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eOngPd01boBG_ISI"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@possessedphotography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">附身摄影</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><h1 id="22d6" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="4110" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在本文中，我将使用NumPy从头开始介绍人工神经网络的开发。这个模型的结构是所有人工神经网络中最基本的——一个简单的前馈网络。我还将展示这个模型的<a class="ae kv" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>等价物，因为我试图让我的实现‘Keras-esque’。虽然与变压器等其他神经网络相比，前馈体系结构是基本的，但这些核心概念可以推广到构建更复杂的神经网络。这些话题本质上是技术性的。关于人工智能更概念性的文章，请查看我的另一篇文章，<a class="ae kv" href="https://medium.com/geekculture/demystifying-artificial-intelligence-bdd9a117d4a6" rel="noopener">揭开人工智能的神秘面纱</a>。</p><h2 id="4b1f" class="mk kx iq bd ky ml mm dn lc mn mo dp lg lx mp mq li mb mr ms lk mf mt mu lm mv bi translated">目录</h2><ul class=""><li id="2b90" class="mw mx iq lq b lr ls lu lv lx my mb mz mf na mj nb nc nd ne bi translated"><strong class="lq ir">架构概述</strong> <br/> —正向传递<br/> —反向传递</li><li id="2961" class="mw mx iq lq b lr nf lu ng lx nh mb ni mf nj mj nb nc nd ne bi translated"><strong class="lq ir"> NumPy实现<br/> </strong> —数据<br/> —构建层<br/> —构建网络<br/> — <strong class="lq ir">网络</strong> : <em class="nk">正向传递</em> <br/> — <strong class="lq ir">层</strong> : <em class="nk">正向传递<br/> —执行正向传递/完整性检查<br/> — </em> <strong class="lq ir">网络</strong> : <em class="nk">反向传递<br/> — </em> <strong class="lq ir"/></li><li id="30c9" class="mw mx iq lq b lr nf lu ng lx nh mb ni mf nj mj nb nc nd ne bi translated"><strong class="lq ir">结论</strong></li></ul><h2 id="2450" class="mk kx iq bd ky ml mm dn lc mn mo dp lg lx mp mq li mb mr ms lk mf mt mu lm mv bi translated">术语词汇表</h2><ul class=""><li id="775f" class="mw mx iq lq b lr ls lu lv lx my mb mz mf na mj nb nc nd ne bi translated">X =输入</li><li id="fb57" class="mw mx iq lq b lr nf lu ng lx nh mb ni mf nj mj nb nc nd ne bi translated">y =标签</li><li id="73d1" class="mw mx iq lq b lr nf lu ng lx nh mb ni mf nj mj nb nc nd ne bi translated">W =重量</li><li id="a293" class="mw mx iq lq b lr nf lu ng lx nh mb ni mf nj mj nb nc nd ne bi translated">b =偏差</li><li id="ed78" class="mw mx iq lq b lr nf lu ng lx nh mb ni mf nj mj nb nc nd ne bi translated">Z =和W加上b的点积</li><li id="068e" class="mw mx iq lq b lr nf lu ng lx nh mb ni mf nj mj nb nc nd ne bi translated">A =激活(Z)</li><li id="47bb" class="mw mx iq lq b lr nf lu ng lx nh mb ni mf nj mj nb nc nd ne bi translated">k =类别数</li><li id="8a9e" class="mw mx iq lq b lr nf lu ng lx nh mb ni mf nj mj nb nc nd ne bi translated">小写字母表示向量，大写字母表示矩阵</li></ul></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="7eba" class="kw kx iq bd ky kz ns lb lc ld nt lf lg jw nu jx li jz nv ka lk kc nw kd lm ln bi translated">体系结构</h1><h2 id="eb24" class="mk kx iq bd ky ml mm dn lc mn mo dp lg lx mp mq li mb mr ms lk mf mt mu lm mv bi translated">前进传球</h2><blockquote class="nx ny nz"><p id="c71d" class="lo lp nk lq b lr oa jr lt lu ob ju lw oc od lz ma oe of md me og oh mh mi mj ij bi translated">点积</p></blockquote><p id="fab3" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">首先，我们计算输入和权重的点积，并添加一个偏差项。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/40319ac29c0f782b0fbd053394871590.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*KjWi732QUtD6d2fJZYk5eQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">点积+偏差。图片作者。</p></figure><p id="a6af" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">第二，我们将第一步中获得的加权和通过一个激活函数。</p><p id="8ad2" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">这两种操作都是基于元素的，并且简单明了。所以，我就不深入了。更多关于点积和激活函数的信息，请点击<a class="ae kv" href="https://www.mathsisfun.com/algebra/vectors-dot-product.html" rel="noopener ugc nofollow" target="_blank">点积</a>、<a class="ae kv" href="https://www.mygreatlearning.com/blog/activation-functions/" rel="noopener ugc nofollow" target="_blank">激活函数</a>。这些计算发生在每个隐藏层的每个神经元中。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/276ba6fa96fbe855a78676064aa6e14d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BawMGY2lwHBbcpI9EuCyWQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">神经元计算概述。图片作者。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/3c456a4a95bf1b0b6363f99efcef6509.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*kbU_WxdXApU8exvIgLghDg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">函数符号—模型预测。图片作者。</p></figure><blockquote class="nx ny nz"><p id="57da" class="lo lp nk lq b lr oa jr lt lu ob ju lw oc od lz ma oe of md me og oh mh mi mj ij bi translated">激活功能</p></blockquote><p id="062f" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">在我的实现中，我们在隐藏层使用ReLU激活，因为这很容易区分，而在输出层使用Softmax激活(下面将详细介绍)。在未来的版本中，我将把它构建得更加健壮，并启用这些激活功能中的任何一个。</p><p id="8de3" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated"><strong class="lq ir"> <em class="nk">常用激活功能:</em> </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/b7e895c9dd42dc39bbbfd5e605569eb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/format:webp/1*CIwyP7lkw9QAuJbQ4wLrFQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">乙状结肠激活功能。图片作者。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/b24507777f4512b86517935ac764d9b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/format:webp/1*oGPbwVE6zMapLDyzbbuptw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Tanh激活函数。图片作者。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/dabea7db91c489990ba73eb13b9c8770.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*E3Ev_QczwZmM7er-dSmLTA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">ReLU激活功能。图片作者。</p></figure><h2 id="8767" class="mk kx iq bd ky ml mm dn lc mn mo dp lg lx mp mq li mb mr ms lk mf mt mu lm mv bi translated">偶数道次</h2><blockquote class="nx ny nz"><p id="0929" class="lo lp nk lq b lr oa jr lt lu ob ju lw oc od lz ma oe of md me og oh mh mi mj ij bi translated">损失函数</p></blockquote><p id="dbda" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">我们首先计算损耗，也称为误差。这是衡量模型有多不正确的标准。</p><p id="d2de" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">损失是一个微分目标函数，我们将训练模型使其最小化。根据您尝试执行的任务，您可以选择不同的损失函数。在我的实现中，我们使用分类交叉熵损失，因为这是一个多分类任务，如下所示。对于二元分类任务，可以使用二元交叉熵损失，对于回归任务，可以使用均方误差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/b8d780bb64fe7ebaa6c86b99ea005c8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*wcTVsYh3d4IQnYcUGdu66Q@2x.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">分类交叉熵损失。图片作者。</p></figure><p id="4ff6" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">这给我造成了一些困惑，所以我想详细说明一下这里发生了什么。上面的公式暗示标签是<a class="ae kv" href="https://en.wikipedia.org/wiki/One-hot" rel="noopener ugc nofollow" target="_blank">一键编码</a>。Keras希望标签是一次性的，但是我的实现不希望如此。这里有一个计算交叉熵损失的例子，以及为什么<strong class="lq ir">没有必要对标签</strong>进行一次性编码的例子。</p><p id="ab33" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">给定来自单个样本的以下数据，独热编码标签(y)和我们的模型预测(yhat)，我们计算交叉熵损失。</p><p id="e9ee" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">y = [1，0，0]</p><p id="a258" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">ŷ = [3.01929735e-07，7.83961013e-09，9.9999690 e-01]</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/08cd0d0754f52cab1dcd7e7babb845db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VFB7qW75a_gOp-9jiLOI4w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">计算带有独热编码标签的交叉熵损失。图片作者。</p></figure><p id="9216" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">正如您所看到的，这个示例中正确的类是零，由y数组的零索引中的1表示。我们将输出概率的负对数乘以该类别的相应标签，然后对所有类别求和。</p><p id="890c" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">你可能已经注意到了，除了零索引，我们得到零，因为任何乘以零的东西都是零。<strong class="lq ir">这归结起来就是我们在正确类别的相应索引处的概率的负对数。这里正确的分类是零，所以我们取零索引处概率的负对数。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/b8b93d1b1333f690db4094accd0a6aac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ep2YOuj-jtEESQFS_gvURQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">零索引处概率的负对数。图片作者。</p></figure><p id="d64d" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">总损耗是所有样本的平均值，在等式中用<strong class="lq ir"> m </strong>表示。为了得到这个数字，我们对每个样本重复上述计算，计算总和并除以样本总数。</p><blockquote class="nx ny nz"><p id="8393" class="lo lp nk lq b lr oa jr lt lu ob ju lw oc od lz ma oe of md me og oh mh mi mj ij bi translated">随机梯度下降</p></blockquote><p id="5864" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">现在我们已经计算出了损失，是时候把它降到最低了。我们首先计算相对于输入参数的输出概率的<a class="ae kv" href="https://betterexplained.com/articles/vector-calculus-understanding-the-gradient/" rel="noopener ugc nofollow" target="_blank">梯度</a>，并将梯度反向传播到每一层的参数。</p><p id="4e59" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">在每一层，我们执行与正向传递类似的计算，除了不是只对Z和A进行计算，我们必须对每个参数(dZ，dW，db，d A)执行计算，如下所示。</p><p id="f878" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated"><em class="nk">隐藏层</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/28f17e4296cd38261806b7075066370a.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*cPcGXME75GZXRj44Pf4zYA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">隐藏层dZ。图片作者。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/7adb8ccd8fe97656474be8032e079573.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*rfhmhubmD52nuPjofOw5Jg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">当前层的权重渐变。图片作者。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/8225d9d347568c27b323bd688323316e.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*IvAGHhkkLonrNqKoirUJmA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">当前层的偏置梯度-样本之和。图片作者。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/5111e23f32c8bfef61a3d27d3ad741a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*cba69YG7qqrEh8LaGv674Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">激活梯度—这是下一层的dA[L]。图片作者。</p></figure><p id="9c3a" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">在输出层有一个dZ的特例，因为我们使用的是softmax激活。本文稍后将对此进行深入解释。</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="06b3" class="kw kx iq bd ky kz ns lb lc ld nt lf lg jw nu jx li jz nv ka lk kc nw kd lm ln bi translated">NumPy实现</h1><h2 id="40a8" class="mk kx iq bd ky ml mm dn lc mn mo dp lg lx mp mq li mb mr ms lk mf mt mu lm mv bi translated">数据</h2><p id="b184" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我将为这个模型使用简单的虹膜数据集。</p><pre class="kg kh ki kj gt ou ov ow ox aw oy bi"><span id="a8d8" class="mk kx iq ov b gy oz pa l pb pc">from sklearn.preprocessing import LabelEncoder</span><span id="fe1a" class="mk kx iq ov b gy pd pa l pb pc">def get_data(path):<br/>    data = pd.read_csv(path, index_col=0)</span><span id="ad02" class="mk kx iq ov b gy pd pa l pb pc">    cols = list(data.columns)<br/>    target = cols.pop()</span><span id="3c91" class="mk kx iq ov b gy pd pa l pb pc">    X = data[cols].copy()<br/>    y = data[target].copy()</span><span id="1fe8" class="mk kx iq ov b gy pd pa l pb pc">    y = LabelEncoder().fit_transform(y)</span><span id="b236" class="mk kx iq ov b gy pd pa l pb pc">    return np.array(X), np.array(y)</span><span id="7290" class="mk kx iq ov b gy pd pa l pb pc">X, y = get_data("&lt;path_to_iris_csv&gt;")</span></pre><h2 id="6388" class="mk kx iq bd ky ml mm dn lc mn mo dp lg lx mp mq li mb mr ms lk mf mt mu lm mv bi translated">初始化图层</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pe pf l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">密集层类</p></figure><h2 id="7a85" class="mk kx iq bd ky ml mm dn lc mn mo dp lg lx mp mq li mb mr ms lk mf mt mu lm mv bi translated">初始化网络</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pe pf l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">ANN类</p></figure></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="9d8b" class="kw kx iq bd ky kz ns lb lc ld nt lf lg jw nu jx li jz nv ka lk kc nw kd lm ln bi translated">网络——向前传递</h1><h2 id="518c" class="mk kx iq bd ky ml mm dn lc mn mo dp lg lx mp mq li mb mr ms lk mf mt mu lm mv bi translated">体系结构</h2><p id="2afa" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">让我们从动态初始化网络架构开始。这意味着我们可以为任意数量的层和神经元初始化我们的网络架构。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pe pf l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">人工神经网络架构</p></figure><p id="439c" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">我们首先创建一个矩阵，将我们的维度(特征数量)映射到输入层的神经元数量。从这一点来看，它非常简单——新层的输入维度是前一层中神经元的数量，输出维度是当前层中神经元的数量。</p><pre class="kg kh ki kj gt ou ov ow ox aw oy bi"><span id="19df" class="mk kx iq ov b gy oz pa l pb pc">model = Network()<br/>model.add(DenseLayer(6))<br/>model.add(DenseLayer(8))<br/>model.add(DenseLayer(10))<br/>model.add(DenseLayer(3))</span><span id="d98f" class="mk kx iq ov b gy pd pa l pb pc">model._compile(X)</span><span id="4e83" class="mk kx iq ov b gy pd pa l pb pc">print(model.architecture)</span><span id="325e" class="mk kx iq ov b gy pd pa l pb pc"><strong class="ov ir">Out --&gt;</strong></span><span id="b918" class="mk kx iq ov b gy pd pa l pb pc">[{'input_dim': 4, 'output_dim': 6, 'activation': 'relu'},<br/> {'input_dim': 6, 'output_dim': 8, 'activation': 'relu'},<br/> {'input_dim': 8, 'output_dim': 10, 'activation': 'relu'},<br/> {'input_dim': 10, 'output_dim': 3, 'activation': 'softmax'}]</span></pre><h2 id="4d7a" class="mk kx iq bd ky ml mm dn lc mn mo dp lg lx mp mq li mb mr ms lk mf mt mu lm mv bi translated">因素</h2><p id="3c39" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">现在我们已经创建了一个网络，我们需要再次动态初始化我们的可训练参数(W，b ),用于任意数量的层/神经元。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pe pf l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">初始化人工神经网络参数</p></figure><p id="423f" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">如您所见，我们在每一层都创建了一个权重矩阵。</p><p id="b6e0" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">该矩阵包含每个神经元的向量和每个输入特征的维度。</p><p id="f5c9" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">一层中的每个神经元都有一个具有维度的偏置向量。</p><p id="6937" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">还要注意，我们设置了一个np.random.seed()，以便每次都能获得一致的结果。试着注释掉这行代码，看看它对你的结果有什么影响。</p><pre class="kg kh ki kj gt ou ov ow ox aw oy bi"><span id="7adb" class="mk kx iq ov b gy oz pa l pb pc">model = Network()<br/>model.add(DenseLayer(6))<br/>model.add(DenseLayer(8))<br/>model.add(DenseLayer(10))<br/>model.add(DenseLayer(3))</span><span id="9bea" class="mk kx iq ov b gy pd pa l pb pc">model._init_weights(X)<br/>print(model.params[0]['W'].shape, model.params[0]['b'].shape)<br/>print(model.params[1]['W'].shape, model.params[1]['b'].shape)<br/>print(model.params[2]['W'].shape, model.params[2]['b'].shape)<br/>print(model.params[3]['W'].shape, model.params[3]['b'].shape)</span><span id="53f2" class="mk kx iq ov b gy pd pa l pb pc"><strong class="ov ir">Out --&gt;</strong></span><span id="f611" class="mk kx iq ov b gy pd pa l pb pc">(6, 4) (1, 6)<br/>(8, 6) (1, 8)<br/>(10, 8) (1, 10)<br/>(3, 10) (1, 3)</span></pre><h2 id="9e00" class="mk kx iq bd ky ml mm dn lc mn mo dp lg lx mp mq li mb mr ms lk mf mt mu lm mv bi translated">正向传播</h2><p id="92dd" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在网络中执行一次全转发的功能。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pe pf l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">网络正向传播</p></figure><p id="50b1" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">我们将前一层的输出作为输入传递给下一层，用<strong class="lq ir"> A_prev </strong>表示。</p><p id="6c83" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">我们将输入和加权和存储在模型存储器中。这是执行向后传递所需要的。</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="3042" class="kw kx iq bd ky kz ns lb lc ld nt lf lg jw nu jx li jz nv ka lk kc nw kd lm ln bi translated">层—向前传递</h1><h2 id="ad15" class="mk kx iq bd ky ml mm dn lc mn mo dp lg lx mp mq li mb mr ms lk mf mt mu lm mv bi translated">激活功能</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pe pf l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">正向传递层方法</p></figure><p id="8241" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated"><strong class="lq ir">记住这些是基于元素的函数。</strong></p><blockquote class="nx ny nz"><p id="e232" class="lo lp nk lq b lr oa jr lt lu ob ju lw oc od lz ma oe of md me og oh mh mi mj ij bi translated">热卢</p></blockquote><p id="5ff4" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">用于隐藏层。概述部分提到了函数和图形。下面是我们调用np.maximum()时发生的情况。</p><pre class="kg kh ki kj gt ou ov ow ox aw oy bi"><span id="2016" class="mk kx iq ov b gy oz pa l pb pc">if input &gt; 0:<br/> return input<br/>else:<br/> return 0</span></pre><blockquote class="nx ny nz"><p id="9291" class="lo lp nk lq b lr oa jr lt lu ob ju lw oc od lz ma oe of md me og oh mh mi mj ij bi translated">Softmax</p></blockquote><p id="54cf" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">用于最后一层。该函数获取k个实数值的输入向量，并将其转换为k个概率的向量，其总和为1。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/476ea5a7ae775363fb389665b9369af6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*hGz9VQ-qWaBOgmvMqFLUkA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">函数符号。图片作者。</p></figure><p id="ed1c" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated"><em class="nk">其中:</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/1b58ffcc30f2eb4c78a8d878d082be57.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*e81mbd7DzEdoFGyHKPgJ0Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">获取非标准化值。图片作者。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/efe294bf82a56a0af280f72c48c3b86c.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*0D9-LGF8JO0Of7xG0TpNbQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">对每个元素进行归一化，以获得概率分布。图片作者。</p></figure><p id="f919" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated"><em class="nk">单样本示例:</em></p><p id="4272" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">输入向量= [ 8.97399717，-4.76946857，-5.33537056]</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pj"><img src="../Images/98bc13a67b414848fedae37744577378.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jOEWIngFDMizeC3Xw623Vw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Softmax计算如下公式。图片作者。</p></figure><h2 id="310d" class="mk kx iq bd ky ml mm dn lc mn mo dp lg lx mp mq li mb mr ms lk mf mt mu lm mv bi translated">单层正向传播</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pe pf l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">层向前传播</p></figure><p id="2715" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated"><em class="nk">其中:</em></p><ul class=""><li id="5ec0" class="mw mx iq lq b lr oa lu ob lx pk mb pl mf pm mj nb nc nd ne bi translated">输入= A_prev</li><li id="1303" class="mw mx iq lq b lr nf lu ng lx nh mb ni mf nj mj nb nc nd ne bi translated">权重=当前层的权重矩阵</li><li id="272d" class="mw mx iq lq b lr nf lu ng lx nh mb ni mf nj mj nb nc nd ne bi translated">偏置=当前层的偏置向量</li><li id="284c" class="mw mx iq lq b lr nf lu ng lx nh mb ni mf nj mj nb nc nd ne bi translated">激活=当前层的激活功能</li></ul><p id="1e04" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">我们在网络的<strong class="lq ir"> _forwardprop </strong>方法中调用这个函数，并将网络的参数作为输入传递。</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="d3f3" class="kw kx iq bd ky kz ns lb lc ld nt lf lg jw nu jx li jz nv ka lk kc nw kd lm ln bi translated">执行向前传球</h1><pre class="kg kh ki kj gt ou ov ow ox aw oy bi"><span id="f65f" class="mk kx iq ov b gy oz pa l pb pc">model = Network()<br/>model.add(DenseLayer(6))<br/>model.add(DenseLayer(8))<br/>model.add(DenseLayer(10))<br/>model.add(DenseLayer(3))</span><span id="778e" class="mk kx iq ov b gy pd pa l pb pc">model._init_weights(X)<br/>out = model._forwardprop(X)</span><span id="0e3b" class="mk kx iq ov b gy pd pa l pb pc">print('SHAPE:', out.shape)<br/>print('Probabilties at idx 0:', out[0])<br/>print('SUM:', sum(out[0]))</span><span id="aefb" class="mk kx iq ov b gy pd pa l pb pc"><strong class="ov ir">Out --&gt;</strong><br/>SHAPE: (150, 3)</span><span id="0e01" class="mk kx iq ov b gy pd pa l pb pc">Probabilties at idx 0: [9.99998315e-01, 1.07470169e-06, 6.10266912e-07]</span><span id="04a3" class="mk kx iq ov b gy pd pa l pb pc">SUM: 1.0</span></pre><p id="29f6" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">完美。一切都在一起了！我们有150个实例映射到我们的3个类，每个实例的概率分布总和为1。</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="c68d" class="kw kx iq bd ky kz ns lb lc ld nt lf lg jw nu jx li jz nv ka lk kc nw kd lm ln bi translated">网络——后向通道</h1><h2 id="adb2" class="mk kx iq bd ky ml mm dn lc mn mo dp lg lx mp mq li mb mr ms lk mf mt mu lm mv bi translated">反向传播</h2><p id="090e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在网络中执行一次完整的反向传递的功能。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pe pf l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">网络反向传播</p></figure><p id="a5fb" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">我们从计算分数的梯度开始。用dscores表示。这是概述部分提到的dZ的特例。</p><p id="a8b5" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">根据<a class="ae kv" href="http://cs231n.stanford.edu/" rel="noopener ugc nofollow" target="_blank"> CS231n </a>:</p><blockquote class="pn"><p id="765a" class="po pp iq bd pq pr ps pt pu pv pw mj dk translated">我们现在希望了解z内部的计算值应该如何改变，以减少本示例对完全目标有贡献的损耗Li。换句话说，我们想得到梯度∂Li/∂zk.</p><p id="64cd" class="po pp iq bd pq pr ps pt pu pv pw mj dk translated">损耗Li是从p计算的，而p又取决于z。对于读者来说，使用链式法则来推导梯度是一个有趣的练习，但在很多东西相互抵消之后，最终证明它非常简单且易于理解:“</p></blockquote><figure class="py pz qa qb qc kk gh gi paragraph-image"><div class="gh gi px"><img src="../Images/24e60ce966e4a6a701ad66b27538d5b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*Bjs2PyfaFn0fnsbYsUntng.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">根据Stanfords CS231n推导dscores。来源:<a class="ae kv" href="http://cs231n.stanford.edu/" rel="noopener ugc nofollow" target="_blank">斯坦福CS231n </a></p></figure><p id="1c13" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated"><em class="nk">其中:</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/6d390943e8b712f83cb01ed752c2ffd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:296/format:webp/1*DcRNMi-R9Qmxv1Ysrp1ugw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">引入术语Li来表示损失。在概述部分对此进行了深入探讨。来源:<a class="ae kv" href="http://cs231n.stanford.edu/" rel="noopener ugc nofollow" target="_blank">斯坦福CS231n </a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/d25b7a2e40f6fe8bd335e9e6fccba920.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/format:webp/1*sKZAIY18HzF4MOX-UUrCCA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">引入术语pk来表示归一化概率(softmax输出)。来源:<a class="ae kv" href="http://cs231n.stanford.edu/" rel="noopener ugc nofollow" target="_blank">斯坦福CS231n </a></p></figure><p id="6762" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated"><em class="nk">单样本示例:</em></p><p id="2396" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">对于每个样本，我们找到正确类别的索引并减去1。相当简单！这是上面代码块中的第9行。由于dscores是一个矩阵，我们可以使用样本和相应的类标签进行双重索引。</p><p id="e871" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">输入向量= [9.99998315e-01，1.07470169e-06，6.10266912e-07]</p><p id="2445" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">输出向量= [-1.68496861e-06，1.07470169e-06，6.10266912e-07]</p><p id="5856" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">这里正确的索引是零，所以我们从零索引中减去1。</p><p id="0646" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated"><strong class="lq ir">注意，我们从输出层开始，然后转移到输入层。</strong></p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="dfa4" class="kw kx iq bd ky kz ns lb lc ld nt lf lg jw nu jx li jz nv ka lk kc nw kd lm ln bi translated">层—向后传递</h1><h2 id="a3ff" class="mk kx iq bd ky ml mm dn lc mn mo dp lg lx mp mq li mb mr ms lk mf mt mu lm mv bi translated">活化导数</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pe pf l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">ReLU导数函数</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/4a60b9cdaa35bbe060ebd85acdbf7b91.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*RjKGKkms3ZY38hx6QrOzZw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">ReLU导数的函数符号。图片作者。</p></figure><h2 id="19a4" class="mk kx iq bd ky ml mm dn lc mn mo dp lg lx mp mq li mb mr ms lk mf mt mu lm mv bi translated">单层反向传播</h2><p id="1866" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">该函数将梯度反向传播到层中的每个参数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pe pf l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">层后退法</p></figure><p id="b3f4" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">再次展示这些，因为它们非常重要。这些是反向传递计算。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/f2745c6f8e51b9cf4e3586d7a608116a.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*ZYv7ukdaYi6JZjL4TjxvHA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">dZ-soft max层的特例。图片作者。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/28f17e4296cd38261806b7075066370a.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*cPcGXME75GZXRj44Pf4zYA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">隐藏层dZ。图片作者。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/158e71ae1cf70474a23f728bed14f847.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*NVODnHY0DiY2SLSDU9-vXA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">dW —在所有层中保持一致。图片作者。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/86ffd3a683ff032ca0217cf8b6ad1892.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*p8XFwGL5OagSJ3wGcmRhcw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">db —在所有层中保持一致。图片作者。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/7141727acf0d9de666704544deb74444.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*uxbYJ5AcrTpNYN7sXq-IKA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">dA[L-1] —在所有层中保持一致。图片作者。</p></figure><p id="df71" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">大家可以看到，除了dZ的计算，每一层的步骤都是一样的。</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="eb4b" class="kw kx iq bd ky kz ns lb lc ld nt lf lg jw nu jx li jz nv ka lk kc nw kd lm ln bi translated">执行向后传递</h1><pre class="kg kh ki kj gt ou ov ow ox aw oy bi"><span id="a446" class="mk kx iq ov b gy oz pa l pb pc">model = Network()<br/>model.add(DenseLayer(6))<br/>model.add(DenseLayer(8))<br/>model.add(DenseLayer(10))<br/>model.add(DenseLayer(3))</span><span id="b3ed" class="mk kx iq ov b gy pd pa l pb pc">model._init_weights(X)<br/>out = model._forwardprop(X)<br/>model._backprop(predicted=out, actual=y)</span><span id="03be" class="mk kx iq ov b gy pd pa l pb pc">print(model.gradients[0]['dW'].shape, model.params[3]['W'].shape)<br/>print(model.gradients[1]['dW'].shape, model.params[2]['W'].shape)<br/>print(model.gradients[2]['dW'].shape, model.params[1]['W'].shape)<br/>print(model.gradients[3]['dW'].shape, model.params[0]['W'].shape)</span><span id="08c8" class="mk kx iq ov b gy pd pa l pb pc"><strong class="ov ir">Out --&gt;<br/></strong>(10, 3) (3, 10)<br/>(8, 10) (10, 8)<br/>(6, 8) (8, 6)<br/>(4, 6) (6, 4)</span></pre><p id="af1d" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">哇，太美了。请记住，渐变是从输出图层开始计算的，向后移动到输入图层。</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="a976" class="kw kx iq bd ky kz ns lb lc ld nt lf lg jw nu jx li jz nv ka lk kc nw kd lm ln bi translated">火车模型</h1><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pe pf l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">准确性/损失方法</p></figure><p id="4373" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">要理解这里发生了什么，请重新访问概述部分，在那里我深入讨论了损失的计算并给出了一个例子。</p><p id="9cfe" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">现在是在每次迭代(历元)之后执行参数更新的时候了。让我们实现<strong class="lq ir"> _update </strong>方法。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pe pf l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">更新参数</p></figure><p id="7f5b" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">终于到了把所有的东西放在一起并训练模型的时候了！</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pe pf l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">训练模型方法</p></figure><h2 id="3921" class="mk kx iq bd ky ml mm dn lc mn mo dp lg lx mp mq li mb mr ms lk mf mt mu lm mv bi translated">数字模型</h2><pre class="kg kh ki kj gt ou ov ow ox aw oy bi"><span id="db3b" class="mk kx iq ov b gy oz pa l pb pc">model = Network()<br/>model.add(DenseLayer(6))<br/>model.add(DenseLayer(8))<br/>model.add(DenseLayer(10))<br/>model.add(DenseLayer(3))</span><span id="7485" class="mk kx iq ov b gy pd pa l pb pc">model.train(X, y, 200)</span><span id="6978" class="mk kx iq ov b gy pd pa l pb pc"><strong class="ov ir">Out --&gt;<br/></strong>EPOCH: 0, ACCURACY: 0.3333333333333333, LOSS: 8.40744716505373<br/>EPOCH: 20, ACCURACY: 0.4, LOSS: 0.9217739285797661<br/>EPOCH: 40, ACCURACY: 0.43333333333333335, LOSS: 0.7513140371257646<br/>EPOCH: 60, ACCURACY: 0.42, LOSS: 0.6686109548451099<br/>EPOCH: 80, ACCURACY: 0.41333333333333333, LOSS: 0.6527102403575207<br/>EPOCH: 100, ACCURACY: 0.6666666666666666, LOSS: 0.5264810434939678<br/>EPOCH: 120, ACCURACY: 0.6666666666666666, LOSS: 0.4708499275871513<br/>EPOCH: 140, ACCURACY: 0.6666666666666666, LOSS: 0.5035542867669844<br/>EPOCH: 160, ACCURACY: 0.47333333333333333, LOSS: 1.0115020349485782<br/>EPOCH: 180, ACCURACY: 0.82, LOSS: 0.49134888468425214</span></pre><blockquote class="nx ny nz"><p id="104b" class="lo lp nk lq b lr oa jr lt lu ob ju lw oc od lz ma oe of md me og oh mh mi mj ij bi translated">检查结果</p></blockquote><div class="kg kh ki kj gt ab cb"><figure class="qg kk qh qi qj qk ql paragraph-image"><img src="../Images/db2fe9783855b28bc0bf18a2c102f284.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*6Nf9YP39Xu7o2PEcYDssbA.png"/></figure><figure class="qg kk qm qi qj qk ql paragraph-image"><img src="../Images/ba2309757ce9b65f737a241ac005ed92.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*HsCexC916zPUXABF7Xng2A.png"/><p class="kr ks gj gh gi kt ku bd b be z dk qn di qo qp translated">图片作者。</p></figure></div><blockquote class="nx ny nz"><p id="be40" class="lo lp nk lq b lr oa jr lt lu ob ju lw oc od lz ma oe of md me og oh mh mi mj ij bi translated">尝试不同的架构</p></blockquote><pre class="kg kh ki kj gt ou ov ow ox aw oy bi"><span id="ca72" class="mk kx iq ov b gy oz pa l pb pc">model = Network()<br/>model.add(DenseLayer(6))<br/>model.add(DenseLayer(8))<br/># model.add(DenseLayer(10))<br/>model.add(DenseLayer(3))</span><span id="ca91" class="mk kx iq ov b gy pd pa l pb pc">model.train(X, y, 200)</span></pre><div class="kg kh ki kj gt ab cb"><figure class="qg kk qq qi qj qk ql paragraph-image"><img src="../Images/ea42c6e82968a31f2a297d32b09a4760.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*u6CpSlWHhiLiX4e7PziSyw.png"/></figure><figure class="qg kk qq qi qj qk ql paragraph-image"><img src="../Images/8369bbfce1bdf2fddb303c5cda9c8375.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*uHj-qpQHlhL-TRfERZup4Q.png"/><p class="kr ks gj gh gi kt ku bd b be z dk qr di qs qp translated">图片作者。</p></figure></div><h2 id="7446" class="mk kx iq bd ky ml mm dn lc mn mo dp lg lx mp mq li mb mr ms lk mf mt mu lm mv bi translated">喀拉斯当量</h2><pre class="kg kh ki kj gt ou ov ow ox aw oy bi"><span id="0f83" class="mk kx iq ov b gy oz pa l pb pc">from keras.models import Sequential<br/>from keras.layers import Dense<br/>import tensorflow as tf<br/>from tensorflow.keras.optimizers import SGD</span><span id="7115" class="mk kx iq ov b gy pd pa l pb pc">ohy = tf.keras.utils.to_categorical(y, num_classes=3)</span><span id="3195" class="mk kx iq ov b gy pd pa l pb pc">model2 = Sequential()<br/>model2.add(Dense(6, activation='relu'))<br/>model2.add(Dense(10, activation='relu'))<br/>model2.add(Dense(8, activation='relu'))<br/>model2.add(Dense(3, activation='softmax'))</span><span id="8cfb" class="mk kx iq ov b gy pd pa l pb pc">model2.compile(SGD(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])</span><span id="ac7b" class="mk kx iq ov b gy pd pa l pb pc">model2.fit(x=X, y=ohy, epochs=30)</span></pre><blockquote class="nx ny nz"><p id="24e4" class="lo lp nk lq b lr oa jr lt lu ob ju lw oc od lz ma oe of md me og oh mh mi mj ij bi translated">检查结果</p></blockquote><div class="kg kh ki kj gt ab cb"><figure class="qg kk qt qi qj qk ql paragraph-image"><img src="../Images/3b706d40816439a9e985af473bb3ca02.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*DoMw-olZNT-V5wUeuqdG6w.png"/></figure><figure class="qg kk qu qi qj qk ql paragraph-image"><img src="../Images/fa5df22e1d14059a23783bc7df9d26b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*6-LPVEMES7tP9QYAEfIKUg.png"/><p class="kr ks gj gh gi kt ku bd b be z dk qv di qw qp translated">图片作者。</p></figure></div><p id="6614" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">Keras不设置random.seed()，所以每次运行之间会得到不同的结果。如果从<strong class="lq ir"> _init_weights </strong>方法中删除这一行，NumPy网络的行为是一样的。</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="99c2" class="kw kx iq bd ky kz ns lb lc ld nt lf lg jw nu jx li jz nv ka lk kc nw kd lm ln bi translated">结论</h1><p id="36fd" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">如果你已经走到这一步，那么恭喜你。这些都是令人费解的话题——这也是为什么我必须把这篇文章放在一起的原因。来验证我自己对神经网络的理解，并希望将这些知识传递给其他开发者！</p><p id="5f52" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">完整代码在此:<a class="ae kv" href="https://github.com/j0sephsasson/numpy-NN/blob/main/model/model.py" rel="noopener ugc nofollow" target="_blank"> NumPy-NN/GitHub </a></p><p id="be00" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">我的LinkedIn: <a class="ae kv" href="https://www.linkedin.com/in/joseph-sasson23/" rel="noopener ugc nofollow" target="_blank">约瑟夫·萨森| LinkedIn </a></p><p id="a3d6" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">我的邮箱:sassonjoe66@gmail.com</p><p id="1f90" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated">请不要犹豫，取得联系，并呼吁任何错误/错误，你可能会遇到的代码或数学！</p><p id="c1ff" class="pw-post-body-paragraph lo lp iq lq b lr oa jr lt lu ob ju lw lx od lz ma mb of md me mf oh mh mi mj ij bi translated"><strong class="lq ir"> <em class="nk">感谢阅读。</em>T13】</strong></p></div></div>    
</body>
</html>