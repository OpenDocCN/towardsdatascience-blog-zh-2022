<html>
<head>
<title>Words into Vectors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词转化成向量</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/words-into-vectors-a7ba23acaf3d#2022-07-26">https://towardsdatascience.com/words-into-vectors-a7ba23acaf3d#2022-07-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="5c18" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/word-embeddings-primer" rel="noopener" target="_blank">单词嵌入入门</a></h2><div class=""/><div class=""><h2 id="3563" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">单词嵌入的概念<strong class="ak"/></h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/45fa570198494825dc38b5ae4bcec061.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*09bevpwyqB0kXCbceY5tWg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@amadorloureiro" rel="noopener ugc nofollow" target="_blank">阿玛多·洛雷罗</a>、<a class="ae le" href="https://unsplash.com/@mukulwadhwa" rel="noopener ugc nofollow" target="_blank">穆库尔·瓦德瓦</a>、<a class="ae le" href="https://unsplash.com/@tamanna_rumee" rel="noopener ugc nofollow" target="_blank">塔曼娜·茹米</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="e190" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">本文是2ⁿᵈ系列<strong class="lh ja">中的一篇关于单词嵌入的入门文章:<br/> </strong> 1。<a class="ae le" href="https://medium.com/@jongim/a-primer-on-word-embeddings-95e3326a833a" rel="noopener">word 2 vec背后有什么</a> | 2。<strong class="lh ja">单词成向量</strong> | <br/> 3。<a class="ae le" href="https://medium.com/@jongim/statistical-learning-theory-26753bdee66e" rel="noopener">统计学习理论</a> | 4。<a class="ae le" href="https://medium.com/@jongim/the-word2vec-classifier-5656b04143da" rel="noopener">word 2 vec分类器</a> | <br/> 5。<a class="ae le" href="https://medium.com/@jongim/the-word2vec-hyperparameters-e7b3be0d0c74" rel="noopener">word 2 vec超参数</a> | 6。<a class="ae le" href="https://medium.com/@jongim/characteristics-of-word-embeddings-59d8978b5c02" rel="noopener">单词嵌入的特征</a></p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="b7a3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">第一篇文章，<a class="ae le" href="https://medium.com/@jongim/a-primer-on-word-embeddings-95e3326a833a" rel="noopener"><strong class="lh ja">Word 2 vec</strong></a><strong class="lh ja">，</strong>介绍了系列文章，为word嵌入做好铺垫。在本文中，我们将回顾导致现代单词嵌入的基本NLP概念。</p><h1 id="44f0" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">数据驱动的自然语言处理简介</h1><p id="253e" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi nf translated">虽然从最早的人类文字开始，人们就已经开始研究语言学，从最早的计算机开始，自然语言处理(NLP)就已经发展起来，但直到20世纪90年代，NLP中的“统计革命”才形成(Johnson，2009)。这个时代标志着NLP的重点从符号或基于规则的算法(如语法或句法)转向数据驱动的技术，该技术将更强大的计算机上的机器学习与互联网上的大型语料库相结合。</p><p id="6141" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">根据Nivre (2002)的说法，统计以三种主要方式参与NLP:</p><ol class=""><li id="41cd" class="no np iq lh b li lj ll lm lo nq ls nr lw ns ma nt nu nv nw bi translated"><strong class="lh ja">应用</strong>:应用于随机模型的算法是确定性的</li><li id="273d" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated"><strong class="lh ja">采集</strong>:模型使用来自经验数据的推断</li><li id="e6a1" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated"><strong class="lh ja">评估</strong>:包括描述性统计、估计和假设检验</li></ol><p id="a687" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">约翰逊(2009)讨论的“统计革命”主要是关于采集方法。</p><p id="c07b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">接下来，我们将回顾导致机器学习技术应用于NLP的统计获取方法。</p><p id="7cff" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在书面文件中，单词掌握着主要意思。虽然标点、大写、符号、拼写、布局、格式、字体/手写和插图也可以传达意义，甚至可以应用于矢量或集成到单词矢量中，但大部分价值在于单词及其排序。</p><h1 id="10cd" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">基于频率的矩阵</h1><p id="0929" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">在计算机语言处理中使用向量来表示单词的想法至少可以追溯到1975年，当时Salton等人(1975)发表了引入向量空间模型概念的基础性工作。最简单的向量空间模型是<em class="oc"> one-hot </em>向量，其中在1× <em class="oc"> v </em>向量的一个单元中使用数字1来显示词汇表<em class="oc"> v </em>中唯一单词的存在，所有其他单元都为0。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/2bf04eb0a13122a1ac6d0da7aa058a8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fO0TEa4DfcTedpqnjLZneA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">一个热词向量示例</strong>(图片由作者提供)</p></figure><p id="d2e2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们将介绍以下三个额外的例子，说明如何在计算机语言处理中使用向量来表示单词(NSS，2017年):</p><ol class=""><li id="ee06" class="no np iq lh b li lj ll lm lo nq ls nr lw ns ma nt nu nv nw bi translated">字数</li><li id="d8a5" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated">术语频率-逆文档频率(TF-IDF)</li><li id="427e" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated">共现</li></ol><p id="2171" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">使用这些向量空间模型的语言建模技术被称为<em class="oc">基于计数的</em>方法，因为它们使用全局单词出现值的总和(Almeida and Xexéo，2019；Levy等人，2015年)。</p><h2 id="f93b" class="of mj iq bd mk og oh dn mo oi oj dp ms lo ok ol mu ls om on mw lw oo op my iw bi translated">字数</h2><p id="6015" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">当收集分布式单词表示的单词数据时，可以从对一系列文档中的单词进行简单计数开始。每个单词在每个文档中出现的次数总和是一个<em class="oc">计数向量</em>。例如:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/604d21a3e8424121436c456dd990ef74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sg-GMfLZ__tWN9lAniXlkQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">字数示例</strong> <br/>(图片由作者提供，灵感来自Potts和MacCartney，2019)</p></figure><p id="d0bb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">计数向量是单词在文档中表示时的基本描述性统计，维度为<em class="oc"> v </em> × <em class="oc"> d </em>，其中<em class="oc"> v </em>是词汇表中的单词数，<em class="oc"> d </em>是文档数。这个矩阵的每一行可以被认为是一个长度为<em class="oc"> d </em>的字向量，它是一个稀疏矩阵。</p><h2 id="5043" class="of mj iq bd mk og oh dn mo oi oj dp ms lo ok ol mu ls om on mw lw oo op my iw bi translated">术语频率-逆文档频率(TF-IDF)</h2><p id="118a" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">对于文档来说，一个更重要的描述性统计数据是单词在文档中的相对频率，即一个单词在文档中出现的频率相对于它在整个语料库中出现的频率。例如，在确定某个单词在文档中对于基于文本的搜索引擎结果排名的重要性时，该值非常有用。</p><p id="3449" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">计算相对频率值的典型方法称为<em class="oc">项频率-逆文档频率(TF-IDF) </em>，并根据各种公式定义为乘以<em class="oc"> TF×IDF </em>，但以下两个公式最常用(Voita，2020):</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oq"><img src="../Images/62840a0bc23ad0620e66581e37a403b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MOOgoyieVUl355YAJc3Chw@2x.png"/></div></div></figure><p id="eba0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">其中，<em class="oc"> t </em>为术语或单词，<em class="oc"> d </em>为文档，<em class="oc"> n </em>为<em class="oc"> t </em>在<em class="oc"> d </em>中出现的次数，<em class="oc"> N </em>为<em class="oc"> t </em>在所有文档中出现的次数，<em class="oc"> D </em>为文档总数，| {<em class="oc">D</em>∈<em class="oc">D</em></p><p id="928d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于<em class="oc"> TF </em>和<em class="oc"> IDF </em>统计，这些公式还有其他变化。<em class="oc"> IDF </em>公式的一个值得注意的方面是在各种变化中一致使用log，这在信息论中有理论依据(Robertson，2004)，其细节是在TF <em class="oc"> - </em> IDF在互联网上广泛用于搜索后正式开发的。</p><p id="6804" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这里的一个要点是，TF-IDF值在很大程度上非常有用，因为它是原始统计数据的<em class="oc">重新加权</em>。</p><p id="58bc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">TF-IDF字向量的维数与计数向量的维数相同:<em class="oc"> v </em> × <em class="oc"> d </em>。矩阵的每一行都可以作为长度为<em class="oc"> d </em>的字向量。</p><h2 id="81c3" class="of mj iq bd mk og oh dn mo oi oj dp ms lo ok ol mu ls om on mw lw oo op my iw bi translated">共现</h2><p id="f594" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">现在，让我们看一个<em class="oc">同现</em> <em class="oc">矩阵</em>的例子。下图是一个表格，显示了两个单词在语料库的每个文档中出现的频率。这个表也可以被认为是一个大小为<em class="oc"> v </em> × <em class="oc"> v </em>的矩阵，它与文档的数量无关。因为矩阵量化了词的接近度，根据语言学中的分布假设(Firth，1957)，值指向词的意义。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/f5ff7b443f7fa3b3947b6e6106b272fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3RJkNwBm8LdWWAKXwEIThQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">同现矩阵示例</strong> <br/>(图片由作者提供，灵感来自Potts和MacCartney，2019)</p></figure><p id="3675" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这个矩阵的每一行都可以作为长度为<em class="oc"> v </em>的词向量，其中<em class="oc"> v </em>为词汇量。该矩阵是单词邻近度的模型，并且是比之前示出的单词计数矩阵更密集的矩阵。</p><p id="22ba" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">但是共现矩阵不需要仅用于每个文档的单词。人们可以查看段落、句子或一定数量的单词窗口中的成对单词。原来设置一个相关的<em class="oc">窗口大小</em>是相当重要的。我们将在下一节“实验设计考虑”中讨论窗口大小。</p><p id="73a0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">尽管如此，因为大多数值是重复的，所以同现矩阵并不像理想的那样密集，所以可以应用降维技术而不会丢失重要的信息，正如我们将在下面题为“降维”的部分中看到的。</p><p id="2c61" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有许多方法可以形成字数统计向量，这取决于所建模的内容。对于语境，再多几个计数向量分别是:单词×语篇语境、音系片段×特征值、单词×句法语境(Potts和MacCartney，2019)。</p><h1 id="282f" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">重新加权</h1><p id="0859" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">我们之前讨论过TF-IDF，其中的<em class="oc"> idf </em>为单词计数值增加了一个权重。TF-IDF值是<em class="oc">重新加权</em>的一个例子，可以用来将信息集中到一个字向量中。</p><p id="7507" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">向量空间模型中的重新加权通常涉及调整单词的频率，正如我们在TF-IDF中看到的那样。在自然语言中，原始单词计数在很大程度上受到词频倾向于遵循<a class="ae le" href="https://en.wikipedia.org/wiki/Zipf%27s_law" rel="noopener ugc nofollow" target="_blank"> <em class="oc"> Zipf定律</em> </a>的事实的影响，也就是说，当绘制log( <em class="oc"> rank </em>)与log( <em class="oc"> frequency </em>)时，它几乎是线性的。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi or"><img src="../Images/b58d1a4a2ea2ecce0c20ccdaca7093e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L0t2wJlGc9ErrUP73AddIw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">维基百科语料库</strong> <br/>词频中的齐夫定律(Sergio Jimenez via<a class="ae le" href="https://en.wikipedia.org/wiki/Zipf's_law#/media/File:Zipf_30wiki_en_labels.png" rel="noopener ugc nofollow" target="_blank">Wikipedia</a>，<a class="ae le" href="https://creativecommons.org/licenses/by-sa/4.0" rel="noopener ugc nofollow" target="_blank"> CC BY-SA 4.0 </a>)</p></figure><p id="d909" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">重新加权的一个挑战是确保不常用词的信息被充分表示，而不引入来自离群数据放大的异常。基于计数的向量的重新加权公式的示例包括(Potts和MacCartney，2019年):</p><ol class=""><li id="7cbc" class="no np iq lh b li lj ll lm lo nq ls nr lw ns ma nt nu nv nw bi translated"><strong class="lh ja">归一化</strong>:向量值的欧几里德或<em class="oc"> L </em> norming (L2范数)</li><li id="e47e" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated"><strong class="lh ja">概率</strong>:将向量值表示为概率<em class="oc">P</em>(<em class="oc">d</em>|<em class="oc">t</em>)，总和为1</li><li id="d388" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated"><strong class="lh ja">观察/预期</strong> : <em class="oc"> O </em> / <em class="oc"> E </em>以及相关的χ或<em class="oc">G</em>-测试<em class="oc"> </em>统计</li><li id="b080" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated"><strong class="lh ja"> TF-IDF </strong>:参见上一节，注意TF-IDF的变体考虑了单词的经验分布</li><li id="024f" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated"><strong class="lh ja"> PMI </strong>:逐点互信息</li><li id="ae9f" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated"><strong class="lh ja"> PPMI </strong>:正逐点互信息</li></ol><p id="9822" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">上述重新加权公式适用于字数统计向量，其中<em class="oc"> t </em>是单词，而<em class="oc"> d </em>是文档。</p><p id="c831" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">使用哪种重新加权公式取决于应用。在<em class="oc">自然语言理解</em>中，Potts将PMI称为“故事的英雄”(Potts和MacCartney，2019)，因为它倾向于揭示自然语言处理中的见解。PMI的公式是:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oq"><img src="../Images/922bc01d6b62932d52edf22e6f809ad6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S6p1Fcm8iyzJCLk7divMYA@2x.png"/></div></div></figure><p id="e468" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">其中<em class="oc"> P </em> ( <em class="oc"> t </em>，<em class="oc"> d </em>)是单词<em class="oc"> t </em>和文档<em class="oc"> d </em>一起出现的概率，<em class="oc"> P </em> ( <em class="oc"> t </em>)是单词出现的概率，<em class="oc"> P </em> ( <em class="oc"> d </em>)是文档出现的概率。PMI量化了两个事件发生的概率之间的差异，如果事件是独立的，则给出它们的联合分布和它们的单独分布。由于当个体概率变得非常小时，PMI很难定义，因此更经常使用PPMI(Jurafsky和Martin，2019)。PPMI用0代替负的PMI值。</p><h1 id="6720" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">实验设计考虑</h1><p id="d446" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated"><em class="oc">窗口缩放</em>是一个与数据收集方式有关的重新加权概念。在共现矩阵示例中，为每个文档测量并记录单词的共现。如果共现的范围被改变为部分或段落而不是整个文档，则得到的矩阵是不同的。事实上，上下文窗口通常被测量为一定数量的单词，例如10。当单词出现在单独的句子中或包含标点符号时，是否被计算在内是其他考虑因素。</p><p id="1ce2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">计算单词以创建单词向量的一个重要方面是用于<em class="oc">预处理</em>文档的标准。例如:</p><ul class=""><li id="fed1" class="no np iq lh b li lj ll lm lo nq ls nr lw ns ma os nu nv nw bi translated">标点符号应该包含还是删除？</li><li id="827c" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma os nu nv nw bi translated">是否应该去掉大写以使单词被认为是相同的，即使一些单词的大写会改变它们的意思？</li><li id="458f" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma os nu nv nw bi translated">是否应该运行一个<em class="oc">命名实体识别</em>脚本来将名称分类到预定义的类别中，比如人名、企业名称和街道名称？</li><li id="de7a" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma os nu nv nw bi translated">是否应该标记词类，以便区分单词的动词和名词形式，如“run ”?</li><li id="4fe5" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma os nu nv nw bi translated">是否应该修改动词，使动词的所有时态都相同(例如，<em class="oc">词条化</em>或<em class="oc">词干化</em>)？(动词的预处理通常通过去除英语中动词的‘ed’结尾来完成。)</li><li id="dc9d" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma os nu nv nw bi translated">拼写错误和生僻字应该怎么处理？</li><li id="8dfc" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma os nu nv nw bi translated">像“the”和“a”这样非常常见的词应该删除吗？</li><li id="2e83" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma os nu nv nw bi translated">表情符号应该被包括在内吗？🤔</li></ul><p id="607b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当决定是否以及如何预处理文档以准备单词嵌入训练时，需要回答这些类型的问题。</p><p id="4a14" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">像单词的开始和结束这样简单的事情也不总是很清楚。在英语中，空格通常分隔单词，但连字符有助于创建复合词。另一方面，在德语中，复合词是不发音的，而且会变得很长，例如在拼出数字123，456时:</p><blockquote class="ot ou ov"><p id="0f1d" class="lf lg oc lh b li lj ka lk ll lm kd ln ow lp lq lr ox lt lu lv oy lx ly lz ma ij bi translated">einhundertdreiundzwanzig tau sendvierhundertsechsundfünfzig</p></blockquote><p id="c7c8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">此外，甚至不考虑“单词”作为向量的基本度量单位，而是考虑字母组合(<em class="oc">字符n-grams </em>)来寻找意义模式，这值得吗？</p><p id="3d51" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对如何预处理文档做出适当的决定是实验设计的一个重要方面，这取决于所构建的应用程序和语言模型。</p><h1 id="2751" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">距离测量</h1><p id="ceb9" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">一旦从数据中设计和创建了单词向量，应该如何测量它们的相似性呢？也就是说，如果一个语言模型预测了一个具有特定向量估计的单词，那么在给定其在数据集中的向量的情况下，应该如何发现最近的单词呢？</p><p id="c4f0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="oc">欧几里德距离</em>是矢量的几何标准。它的方程式是:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oq"><img src="../Images/cb2dff15808985553821ecd500481c84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9iXtXsjnitXnv2n-AF6XsQ@2x.png"/></div></div></figure><p id="d872" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">其中<strong class="lh ja"> a </strong>和<strong class="lh ja"> b </strong>是两个大小为<em class="oc"> n </em>的向量。然而，在记录词的频率的向量空间模型中，更频繁的词往往具有更大的量级，这往往阻碍了词的比较(Jurafsky和Martin，2019)。几何上，当向量方向是唯一的度量时，可以使用<em class="oc">余弦相似度</em>，这是用于测量单词嵌入相似度的最常见度量(Jurafsky和Martin，2019；Turney和Pantel，2010年)。余弦相似性的公式为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oz"><img src="../Images/401aff9344e1c0db211578ac33f6244e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4KcsLWGzWVym9fyXw_fqow@2x.png"/></div></div></figure><p id="04ac" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这里，点符号表示向量上的点积(粗体)，双垂直线表示向量的<em class="oc"> L </em>范数，它在分母中用于归一化幅度。值的范围从1表示完全相反到1表示完全相同。</p><p id="b8b8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">下图显示了在确定最近邻时，欧几里德距离和余弦相似性如何相互矛盾。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pa"><img src="../Images/3d8fb0eccd15cb27f3854cc2d6e5311c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HFdd9WHmWXgatYYJ8KcU0A.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">欧几里德距离和余弦相似度</strong>(图片由作者提供)</p></figure><p id="5615" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">注意，如果余弦相似度通过减去平均值而被归一化，则它成为<em class="oc">皮尔逊相关系数</em>。像余弦相似性一样，皮尔逊相关系数的范围从1到1，但它是对线性关系量的度量，不受位置和比例的影响。</p><p id="4325" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">要将余弦相似性转换为“距离”度量，其中更远的距离由更大的数字表示，可以考虑使用:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oq"><img src="../Images/36a271ec47b81c42ec0a7637d4473fa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uHGpuMmgStS-j8X9_ALmuw@2x.png"/></div></div></figure><p id="773c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">但是请注意，这个度量不是一个“适当的”距离度量。根据Potts和MacCartney的说法，真实距离必须是对称的，对于相同的向量必须是0，并且必须满足三角形不等式(Potts和MacCartney，2019)。因此，余弦相似度应转换为<em class="oc">角距离</em>，范围从0到1:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pb"><img src="../Images/5206655fb8a1d834a3fe2c9857d1fe53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S0srFiSC-G-G1xIJlFF0CA@2x.png"/></div></div></figure><p id="e0e5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在文献中用于词向量的额外距离度量(无论合适与否)有(Potts和MacCartney，2019):</p><ul class=""><li id="41ec" class="no np iq lh b li lj ll lm lo nq ls nr lw ns ma os nu nv nw bi translated">与交集相关的基于匹配的方法，包括:匹配、Jaccard、骰子和重叠</li><li id="884c" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma os nu nv nw bi translated">与Fisher信息相关的Kullback-Leibler (KL)散度方法，用于推导概率，包括:KL散度、对称KL和Jensen-Shannon散度，以及具有偏斜的KL</li></ul><p id="d797" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">选择哪种距离度量取决于应用需要强调什么。在统计NLP中，测量结果的试错多于统计纯度。</p><h1 id="ffd9" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">降维</h1><p id="6f75" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">可视化通过将多维数据减少到两个或三个维度来直观地显示主导关系，从而帮助我们理解向量关系。在统计学中，人们经常使用主成分分析(PCA)，其中向量被线性映射到数据方差最大化的二维平面中。</p><p id="d984" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="oc">t</em>-分布式随机邻居嵌入(<em class="oc"> t </em> -SNE)是一种用于可视化的机器学习算法，由范德马滕和辛顿于2008年发表，其目标是<em class="oc"> " </em>很好地捕捉高维数据的大部分局部结构，同时也揭示全局结构，例如在几个尺度上存在的聚类”(范德马滕和辛顿，2008)。使用<em class="oc"> t </em> -SNE来可视化一组单词关系或者甚至获得一大组单词关系的高级视图是很常见的。</p><p id="ac83" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">PCA和<em class="oc"> t </em> -SNE是众所周知的用于在二维或三维中可视化向量的降维技术，但是它们也可以用于将单词向量的维数减少到任何更低的维数。</p><p id="522a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">向量空间模型的其他降维技术包括(Potts和MacCartney，2019年):</p><ol class=""><li id="e07e" class="no np iq lh b li lj ll lm lo nq ls nr lw ns ma nt nu nv nw bi translated">奇异值分解</li><li id="223c" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated">LSA(也称为LSI)，它使用截断SVD</li><li id="d458" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated">非负矩阵分解(NMF)</li><li id="d6ef" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated">概率LSA (PLSA)</li><li id="5777" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated">潜在狄利克雷分配</li><li id="992e" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated">使用神经网络的自动编码器</li></ol><p id="9b75" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">降维技术导致了基于机器学习预测的单词嵌入，最著名的早期方法是Word2vec，我们将在统计学习理论介绍之后的未来文章中详细讨论它。</p><h1 id="c692" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">摘要</h1><p id="d67a" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">在本文中，我们学习了如何使用向量来量化文本中的单词邻近关系，如何测量这些关系，以及如何使用降维和机器学习技术来最大化数据的可用性。</p><p id="20e0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在本系列的下一篇文章<a class="ae le" href="https://medium.com/@jongim/statistical-learning-theory-26753bdee66e" rel="noopener"> <strong class="lh ja">统计学习理论</strong> </a>中，我们将回顾一下理解一个浅层神经网络分类器的数学理论，比如Word2vec。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="8524" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这篇文章是2ⁿᵈ系列文章<strong class="lh ja">中关于单词嵌入的初级读本:<br/> </strong> 1。<a class="ae le" href="https://medium.com/@jongim/a-primer-on-word-embeddings-95e3326a833a" rel="noopener">word 2 vec背后有什么</a> | 2。字成矢| <br/> 3。<a class="ae le" href="https://medium.com/@jongim/statistical-learning-theory-26753bdee66e" rel="noopener">统计学习理论</a> | 4。<a class="ae le" href="https://medium.com/@jongim/the-word2vec-classifier-5656b04143da" rel="noopener">word 2 vec分类器</a> | <br/> 5。<a class="ae le" href="https://medium.com/@jongim/the-word2vec-hyperparameters-e7b3be0d0c74" rel="noopener">word 2 vec超参数</a> | 6。<a class="ae le" href="https://medium.com/@jongim/characteristics-of-word-embeddings-59d8978b5c02" rel="noopener">单词嵌入的特征</a></p><p id="0381" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">关于这个主题的更多信息:</strong>我推荐学习更多关于单词嵌入基础的资源是斯坦福大学的这个在线计算机科学课程:Potts，c .和MacCartney，B. (2019)。<a class="ae le" href="https://web.stanford.edu/class/cs224u/2019/" rel="noopener ugc nofollow" target="_blank"> <em class="oc"> CS224U自然语言理解</em> </a>。</p><h1 id="b437" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">参考</h1><p id="e13f" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">阿尔梅达，f .和Xexéo，G. (2019)。词汇嵌入:一个综述。可从<a class="ae le" href="https://arxiv.org/abs/1901.09069" rel="noopener ugc nofollow" target="_blank"> arXiv:1901.09069v1 </a>获得。</p><p id="3229" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">弗斯，J. R. (1957)。语言学理论概要，1930-1955。在弗斯(编辑)，<em class="oc">语言分析研究</em>，语言学会特刊，第1-32页。英国牛津:巴兹尔·布莱克威尔出版社。</p><p id="2b62" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">约翰逊博士(2009年)。统计革命如何改变(计算)语言学。计算语言学协会2009年欧洲分会关于语言学和计算语言学之间的相互作用研讨会的会议录:良性、恶性还是空洞？，第3–11页。<a class="ae le" href="https://www.aclweb.org/anthology/W09-0103/" rel="noopener ugc nofollow" target="_blank"> PDF </a>。</p><p id="8a8d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">jurafsky d .和Martin j .(2019年)。<em class="oc">语音和语言处理:自然语言处理、计算语言学和语音识别的介绍</em>。徒弟堂，<a class="ae le" href="https://web.stanford.edu/~jurafsky/slp3/" rel="noopener ugc nofollow" target="_blank">第三版，2019稿</a>。</p><p id="c4c3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Levy、y . Goldberg和I . Dagan(2015年)。利用从单词嵌入中获得的经验改进分布相似性。《计算语言学协会汇刊》，3:211–225。可在doi <a class="ae le" href="https://www.aclweb.org/anthology/W14-1618/" rel="noopener ugc nofollow" target="_blank"> 10.1162/tacl_a_00134 </a>处获得。</p><p id="3dcb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Nivre，J. (2002年)。自然语言处理中的统计方法。在布本科j .和旺格勒。<em class="oc">促进它。第二次促进瑞典新大学和大学学院IT研究的会议</em>，第684–694页。斯克夫德大学。</p><p id="7e13" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">NSS (2017年)。<a class="ae le" href="https://www.analyticsvidhya.com/blog/2017/06/ word-embeddings-count-word2veec/" rel="noopener ugc nofollow" target="_blank">对单词嵌入的直观理解:从计数向量到Word2Vec </a>。<em class="oc">分析Vidhya。</em></p><p id="43ba" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Potts c .和MacCartney b .(2019)。<a class="ae le" href="https://web.stanford.edu/class/cs224u/2019/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="oc"> CS224U自然语言理解</em> </a>，在线计算机科学课程。加州斯坦福:斯坦福大学。</p><p id="db1c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">罗伯逊(2004年)。理解逆文献频率:关于IDF的理论争论。<em class="oc">文献杂志</em>，60(5):503–520。可在doi<a class="ae le" href="https://www.researchgate.net/publication/238123710_Understanding_Inverse_Document_Frequency_On_Theoretical_Arguments_for_IDF" rel="noopener ugc nofollow" target="_blank">10.1108/00220410410560582</a>处获得。</p><p id="722d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">黄和杨(1975)。自动标引的向量空间模型。<em class="oc">ACM的通信</em>，18(11):613–620。可在doi<a class="ae le" href="https://doi.org/10.1145/361219.361220" rel="noopener ugc nofollow" target="_blank">10.1145/361219.361220</a>处获得。</p><p id="6591" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">特尼博士和潘特尔博士(2010年)。从频率到意义:语义学的向量空间模型。<em class="oc">人工智能研究杂志</em>，37:141–188。<a class="ae le" href="https://www.researchgate.net/publication/45904528_From_Frequency_to_Meaning_Vector_Space_Models_of_Semantics" rel="noopener ugc nofollow" target="_blank"> PDF </a>。</p><p id="004e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">范德马滕和辛顿(2008年)。使用t-SNE可视化数据。<em class="oc">机器学习研究杂志</em>，9:2579–2605。<a class="ae le" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwiHiZrerKbtAhVE6aQKHc04BxIQFjABegQIBBAC&amp;url=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume9%2Fvandermaaten08a%2Fvandermaaten08a.pdf&amp;usg=AOvVaw3ywBqd8s9cRemL_RqAWoa9" rel="noopener ugc nofollow" target="_blank"> PDF </a>。</p><p id="d690" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">沃伊塔湖(2020)。<a class="ae le" href="https://lena-voita.github.io/nlp_course/word_embeddings.html" rel="noopener ugc nofollow" target="_blank">自然语言处理课程:单词嵌入</a>。<em class="oc"> Github </em>。</p><p id="6ace" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">*除非另有说明，数字和图像均由作者提供。</p></div></div>    
</body>
</html>