<html>
<head>
<title>Forecasting Uncertainty with Linear Models like in Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用深度学习中的线性模型预测不确定性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/forecasting-uncertainty-with-linear-models-like-in-deep-learning-bc58f53938#2022-09-21">https://towardsdatascience.com/forecasting-uncertainty-with-linear-models-like-in-deep-learning-bc58f53938#2022-09-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bd96" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">将任意的和认知的不确定性纳入预测区间</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fe8771dc32c34f2ecc204eed51bb33b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cQ3GU0kyL_cCZ8Mm"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae ky" href="https://unsplash.com/@enginakyurt?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> engin akyurt </a>拍摄的照片</p></figure><p id="494f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常，机器学习行业的应用程序不会考虑如何产生不确定性估计。在许多现实世界的任务中，不仅需要做出准确的预测。<strong class="lb iu">提供模型结果的置信度对于做出最有效的决策可能至关重要</strong>。</p><p id="0f0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了一些深度学习技术或者其他特殊情况，<strong class="lb iu">产生信心估计并不是免费的午餐</strong>。我们可以把产生置信度得分的所有解决方案看作是标准训练和预测阶段的一个独立步骤。最简单和直观的方法在于<a class="ae ky" href="https://medium.com/towards-data-science/add-prediction-intervals-to-your-forecasting-model-531b7c2d386c" rel="noopener">使用残差自举作为估计观测值和预测值之间不确定性的方法</a>。<strong class="lb iu">基于Bootstrap的技术是接近不确定性领域的起点</strong>。</p><p id="2740" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用bootstrapping，我们可以用较少的努力和简单的假设为我们的预测建立一个置信度。另一方面，与盲目使用bootstrap规则相比，产生可靠和校准的估计可能需要更多的关注和验证。</p><p id="4dee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先要记住的是，我们不想优化预测模型来产生最佳的逐点预测。有准确的预测是可取的，但提供可靠的界限，其中我们有信心观察值下降，将是梦幻般的。换句话说，有好的预测是一个起点，但是<strong class="lb iu">建立信心评估在大多数时候是一项单独的任务</strong>，应该使用适当的评分方法进行验证。</p><p id="2f59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其次，我们应该记住存在两种不同的不确定性来源。我们分别指的是任意的和认知的不确定性。可以确定的是，所有的AI预测总是不确定的。由于这个原因，<strong class="lb iu">一个好的置信度应该包含任意的和认知的不确定性</strong>。</p><p id="52c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们介绍了一种在回归任务中为我们的机器学习模型提供置信度估计的方法。所提出的方法只是作为预测之后的一个附加步骤。它适合一个单独的线性模型，通过最小化一个“特别的”损失函数来专门预测样本的不确定性。为了对这个主题有一个更技术性的概述，我建议<a class="ae ky" href="https://arxiv.org/abs/2204.09308" rel="noopener ugc nofollow" target="_blank">这篇文章</a>也适用于神经网络生态系统之外。</p><h1 id="69eb" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">实验设置</h1><p id="b6ff" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">所提出的解决方案的先决条件可以概括为两个一般概念:</p><ul class=""><li id="590c" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><strong class="lb iu">适应性</strong>:作为流程中的一个附加步骤，我们的解决方案应该适用于每个预测解决方案的末尾。</li><li id="c61c" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated"><strong class="lb iu">完备性</strong>:该方法应该足够好，能够涵盖任意的和认知的不确定性场景。</li></ul><p id="048f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为起点，<strong class="lb iu">我们必须获得一个经验表示来对预测模型</strong>的方差进行数值估计。在这个意义上，高斯负对数似然(NLL)损失是一个完美的候选。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/396f9cd3fb81bf416aa87706631635e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iC72bI__X8ukhnBBk_4InQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">高斯负对数似然公式[图片由作者提供]</p></figure><p id="e1f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上述损失中，主要涉及3个条款:</p><ul class=""><li id="59ab" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><em class="nh"> yᵢ: </em>我们用来建立预测模型的目标值。</li><li id="9051" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated"><em class="nh"> μ </em> ( <em class="nh"> xᵢ </em>):预测模型做出的逐点预测。</li><li id="94b1" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated"><em class="nh">∑</em>(<em class="nh">xᵢ</em>):估计的不确定性，确切地代表了我们想要预测的东西。</li></ul><p id="302c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了提供不确定性的样本估计，我们认为给出了<em class="nh"> yᵢ </em>和<em class="nh"> μ </em> ( <em class="nh"> xᵢ </em>)。这意味着我们已经拟合了我们选择的预测模型。为了最小化NLL损失，我们拟合了一个线性模型，该模型试图预测<em class="nh"> σ </em> ( <em class="nh"> xᵢ </em>)的最佳值。在最小化结束时，<strong class="lb iu">我们有一个线性模型，它接收我们处理的特征作为输入，并返回每个观察值的不确定性预测</strong>。</p><h1 id="6e8a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">预测不确定性</h1><p id="785f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">让我们来看看所提议的方法在起作用。</p><p id="ab7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在固定域中生成正弦数据，在一个方向上增加高斯噪声。我们明确地选择了这些数据，以观察一个模型在其知识领域之外可以预测什么。尽管数据非常简单，但我们并不期望我们的模型能在列车边界之外很好地推广。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/99d96aee0690dde8540e632bb4152f5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pqpyYNfe8FFWVsAS.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">原始正弦曲线(左)。噪音增加(右侧)。[图片由作者提供]</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/9916c409ca2744fe28c1b525e0a44a51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*I4R6xMAAGk4AKqCk.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">原始正弦曲线加上不断增加的噪声。[图片由作者提供]</p></figure><p id="587e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，<strong class="lb iu">我们的不确定性估计应该概述我们的模型在其知识领域之外做出预测的困难</strong>。</p><p id="aeb8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下图中，我们描述了使用多层感知器回归器的预测，通过建议的方法获得的不确定性。生成的西格玛间隔的宽度随着数据中噪声的增加而增加。这是正确的，这意味着我们可以将数据中存在的噪声捕捉为一种不确定性(任意不确定性)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/c40214aa0047f3f9cf7d1fabbf8c0ac0.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/0*XzIaFHJJN0G_JGRP.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">包含任意不确定性的适马区间。[图片由作者提供]</p></figure><p id="d160" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一步，我们想看看我们是否能把模型预测的不准确性(认知不确定性)作为一个额外的不确定性来源。在这种意义上，我们期望在训练边界之外看到两个方向上的置信带增长，因为模型不适合预测该范围之外的数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/60ef9e53b6155e13bb6931dfea009438.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/0*F7IX5fgS7Aw12GLl.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">包含任意和认知不确定性的适马区间。[图片由作者提供]</p></figure><p id="e5ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上图中，我们观察到的正是我们的目标。<strong class="lb iu">我们的sigma区间同时封装了来自数据和模型结果的不确定性</strong>。我们通过在系综格式中应用我们的方法简单地获得了这个结果。换句话说，我们拟合多个多层感知器回归器，并将它们组合以构建高斯混合。混合物的平均值和标准偏差分别作为真实值和不确定性的预测返回。</p><p id="d9f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">仔细观察两种方法中的西格玛预测，有助于区分两种不确定性来源的考虑方式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/2a9c1af916d953d0767fc9292e2d601c.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/0*j4Mhp380arl47-qP.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">只有任意不确定性(红色)和任意加认知不确定性的适马区间宽度。(蓝色)[图片由作者提供]</p></figure><h1 id="4eb9" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">摘要</h1><p id="650e" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在这篇文章中，我们介绍了一种评估模型不确定性的方法。我们提供了一个奇特而巧妙的实现，它包括拟合一个线性回归来预测每个逐点预测的标准偏差。我们还发现了如何将任意的和认知的来源结合到我们的信心预测中。作为一个附加值，建议的实现似乎适用于所有具有内置外推能力的模型。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><p id="14d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/cerlymarco/MEDIUM_NoteBook" rel="noopener ugc nofollow" target="_blank">T3】查看我的GITHUB回购T5】</a></p><p id="a644" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">保持联系:<a class="ae ky" href="https://www.linkedin.com/in/marco-cerliani-b0bba714b/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a></p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="ce3c" class="nt lw it bd lx nu nv dn mb nw nx dp mf li ny nz mh lm oa ob mj lq oc od ml oe bi translated">参考</h2><p id="6953" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">马蒂亚斯·瓦尔登格罗-托罗；丹尼尔·萨罗莫·莫里。(<em class="nh"> arXiv </em> 2022) <a class="ae ky" href="https://doi.org/10.48550/arXiv.2204.09308" rel="noopener ugc nofollow" target="_blank">对任意性和认知不确定性的深入研究</a>。</p></div></div>    
</body>
</html>