<html>
<head>
<title>Decision Trees, Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树，解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-trees-explained-d7678c43a59e#2022-05-08">https://towardsdatascience.com/decision-trees-explained-d7678c43a59e#2022-05-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3b01" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何训练他们，他们如何工作，与工作代码的例子</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b676dd4b85927a277612f175834d4c7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5bVRjMrdPHAJZOspdvmmUg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">津巴布韦的一棵大决策树。图片作者。</p></figure><p id="9bf5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这篇文章中，我们将讨论一种常用的机器学习模型，称为<strong class="kx ir"> <em class="lr">决策树</em> </strong>。决策树是许多应用程序的首选，主要是因为它们的高度可解释性，但也因为它们的设置和训练相对简单，以及用决策树进行预测所需的时间较短。决策树对于表格数据来说是很自然的，事实上，在这种类型的数据上(相对于图像)，它们目前似乎优于神经网络。与神经网络不同，树不需要输入标准化，因为它们的训练不是基于梯度下降，并且它们只有很少的参数可以优化。他们甚至可以对有缺失值的数据进行训练，但现在这种做法不太推荐，缺失值通常是估算的。</p><p id="0820" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">决策树的众所周知的用例是推荐系统(根据你过去的选择和其他特征，如年龄、性别等，你预测的电影偏好是什么)。)和搜索引擎。</p><p id="618c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">树中的预测过程由样本属性(特征)与预先学习的阈值的一系列比较组成。从顶部(树根)开始向下(朝向树叶，是的，与真实的树相反)，在每一步中，比较的结果确定样本在树中是向左还是向右，并由此确定下一步的比较。当我们的样本到达一个叶子(一个末端节点)时，基于叶子中的多数类做出决策或预测。</p><p id="e2cb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">图1显示了根据花瓣和萼片的长度和宽度将鸢尾属样品分成3个不同种类(类)的问题。</p><p id="1b39" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的示例将基于著名的Iris数据集(Fisher，R.A .“分类问题中多重测量的使用”Annual Eugenics，7，Part II，179–188(1936))。我是用<a class="ae ls" href="https://scikit-learn.org/stable/faq.html" rel="noopener ugc nofollow" target="_blank"> sklearn </a>包下载的，这是一个<strong class="kx ir"> <em class="lr"> BSD </em> </strong> (Berkley源码发布)授权软件。我修改了其中一个类的特性，减小了训练集的大小，将这些类混合在一起，让它变得更有趣。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lt"><img src="../Images/01bbef322520f26a5ef8a05fe60a336a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oUzKoJLSobKLwxGwXXxtYg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1在Iris数据集的修正训练集上训练的决策树。图片作者。</p></figure><p id="1a1b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们稍后会解决这棵树的细节问题。现在，我们将检查根节点，并注意到我们的训练群体有45个样本，分为3类，如下所示:[13，19，13]。“class”属性告诉我们，如果这个样本是一个叶子，树将为它预测的标签——基于节点中的多数类。例如，如果不允许我们运行任何比较，我们将处于根节点，我们的最佳预测将是类Veriscolor，因为它在训练集中有19个样本，而其他两个类只有13个样本。如果我们的比较序列将我们引向左起第二片叶子，模型的预测将再次是Veriscolor，因为在训练集中有4个该类的样本到达这片叶子，而只有1个Virginica类的样本和0个Setosa类的样本。</p><p id="99c3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">决策树可用于分类或回归问题。让我们从讨论分类问题开始，解释树训练算法是如何工作的。</p><h1 id="4bb4" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">做法:</h1><p id="4c6b" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">让我们看看如何使用sklearn训练一棵树，然后讨论其机制。</p><p id="8af4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下载数据集:</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="6536" class="mw lv iq ms b gy mx my l mz na">import numpy as np<br/>from sklearn.datasets import load_iris<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import OneHotEncoder</span><span id="765e" class="mw lv iq ms b gy nb my l mz na">iris = load_iris()<br/>X = iris['data']<br/>y = iris['target']<br/>names = iris['target_names']<br/>feature_names = iris['feature_names']</span><span id="e2ba" class="mw lv iq ms b gy nb my l mz na"># One hot encoding<br/>enc = OneHotEncoder()<br/>Y = enc.fit_transform(y[:, np.newaxis]).toarray()</span><span id="46b4" class="mw lv iq ms b gy nb my l mz na"># Modifying the dataset<br/>X[y==1,2] = X[y==1,2] + 0.3</span><span id="4901" class="mw lv iq ms b gy nb my l mz na"># Split the data set into training and testing<br/>X_train, X_test, Y_train, Y_test = train_test_split(<br/>    X, Y, test_size=0.5, random_state=2)</span><span id="223a" class="mw lv iq ms b gy nb my l mz na"># Decreasing the train set to make things more interesting<br/>X_train = X_train[30:,:]<br/>Y_train = Y_train[30:,:]</span></pre><p id="0150" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们将数据集可视化。</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="989c" class="mw lv iq ms b gy mx my l mz na"># Visualize the data sets<br/>import matplotlib<br/>import matplotlib.pyplot as plt</span><span id="ef1b" class="mw lv iq ms b gy nb my l mz na">plt.figure(figsize=(16, 6))<br/>plt.subplot(1, 2, 1)<br/>for target, target_name in enumerate(names):<br/>    X_plot = X[y == target]<br/>    plt.plot(X_plot[:, 0], X_plot[:, 1], linestyle='none', marker='o', label=target_name)<br/>plt.xlabel(feature_names[0])<br/>plt.ylabel(feature_names[1])<br/>plt.axis('equal')<br/>plt.legend();</span><span id="c8e7" class="mw lv iq ms b gy nb my l mz na">plt.subplot(1, 2, 2)<br/>for target, target_name in enumerate(names):<br/>    X_plot = X[y == target]<br/>    plt.plot(X_plot[:, 2], X_plot[:, 3], linestyle='none', marker='o', label=target_name)<br/>plt.xlabel(feature_names[2])<br/>plt.ylabel(feature_names[3])<br/>plt.axis('equal')<br/>plt.legend();</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/073f7c71abae2810a56ca48f6c0dd74e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qP2YMXmHc6sOX5xaB1Xjqw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2改进的Iris数据集的可视化。图片作者。</p></figure><p id="c1a3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">仅仅是火车组:</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="acd0" class="mw lv iq ms b gy mx my l mz na">plt.figure(figsize=(16, 6))<br/>plt.subplot(1, 2, 1)<br/>for target, target_name in enumerate(names):<br/>    X_plot = X_train[Y_train[:,target] == 1]<br/>    plt.plot(X_plot[:, 0], X_plot[:, 1], linestyle='none', marker='o', label=target_name)<br/>plt.xlabel(feature_names[0])<br/>plt.ylabel(feature_names[1])<br/>plt.axis('equal')<br/>plt.legend();</span><span id="7f7f" class="mw lv iq ms b gy nb my l mz na">plt.subplot(1, 2, 2)<br/>for target, target_name in enumerate(names):<br/>    X_plot = X_train[Y_train[:,target] == 1]<br/>    plt.plot(X_plot[:, 2], X_plot[:, 3], linestyle='none', marker='o', label=target_name)<br/>plt.xlabel(feature_names[2])<br/>plt.ylabel(feature_names[3])<br/>plt.axis('equal')<br/>plt.legend();</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/5e8a0dcb7ce76833e801190dfabbb669.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eK8sT2xnvgCEPTa3t-WNJw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3——只有(缩小的)列车组。图片作者。</p></figure><p id="85b0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，我们准备训练一棵树，并将其可视化。结果就是我们在图1中看到的模型</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="e947" class="mw lv iq ms b gy mx my l mz na">from sklearn import tree<br/>import graphviz</span><span id="7fdf" class="mw lv iq ms b gy nb my l mz na">iristree = tree.DecisionTreeClassifier(max_depth=3, criterion='gini', random_state=0)<br/>iristree.fit(X_train, enc.inverse_transform(Y_train))</span><span id="aa05" class="mw lv iq ms b gy nb my l mz na">feature_names = ['sepal length', 'sepal width', 'petal length', 'petal width']</span><span id="9d7a" class="mw lv iq ms b gy nb my l mz na">dot_data = tree.export_graphviz(iristree, out_file=None, <br/>                      feature_names=feature_names,  <br/>                      class_names=names,<br/>                      filled=True, rounded=True,  <br/>                      special_characters=True)  <br/>graph = graphviz.Source(dot_data)</span><span id="96a4" class="mw lv iq ms b gy nb my l mz na">display(graph)</span></pre><p id="8ef0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以看到，该树使用花瓣宽度进行第一次和第二次分割——第一次分割清楚地将类Setosa与其他两个类分开。请注意，对于第一次分裂，花瓣长度可以同样好地工作。</p><p id="6d8a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们看看这个模型在训练集上的分类精度，然后是测试集:</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="cfe3" class="mw lv iq ms b gy mx my l mz na">from sklearn.metrics import precision_score<br/>from sklearn.metrics import recall_score</span><span id="8917" class="mw lv iq ms b gy nb my l mz na">iristrainpred = iristree.predict(X_train)<br/>iristestpred = iristree.predict(X_test)</span><span id="5c42" class="mw lv iq ms b gy nb my l mz na"># train precision:<br/>display(precision_score(enc.inverse_transform(Y_train), iristrainpred.reshape(-1,1), average='micro', labels=[0]))<br/>display(precision_score(enc.inverse_transform(Y_train), iristrainpred.reshape(-1,1), average='micro', labels=[1]))<br/>display(precision_score(enc.inverse_transform(Y_train), iristrainpred.reshape(-1,1), average='micro', labels=[2]))</span><span id="b9e8" class="mw lv iq ms b gy nb my l mz na">&gt;&gt;&gt; 1.0<br/>&gt;&gt;&gt; 0.9047619047619048<br/>&gt;&gt;&gt; 1.0</span><span id="5131" class="mw lv iq ms b gy nb my l mz na"># test precision:<br/>display(precision_score(enc.inverse_transform(Y_test), iristestpred.reshape(-1,1), average='micro', labels=[0]))<br/>display(precision_score(enc.inverse_transform(Y_test), iristestpred.reshape(-1,1), average='micro', labels=[1]))<br/>display(precision_score(enc.inverse_transform(Y_test), iristestpred.reshape(-1,1), average='micro', labels=[2]))</span><span id="b137" class="mw lv iq ms b gy nb my l mz na">&gt;&gt;&gt; 1.0<br/>&gt;&gt;&gt; 0.7586206896551724<br/>&gt;&gt;&gt; 0.9473684210526315</span></pre><p id="41fa" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如我们所看到的，训练集中数据的稀缺以及类1和类2混合的事实(因为我们修改了数据集)导致测试集中这些类的精度较低。0类保持完美的精度，因为它与其他两类高度分离。</p><h1 id="f018" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">现在谈谈理论——树是如何训练的？</h1><p id="403b" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">换句话说，它如何选择最佳的特征和阈值放入每个节点？</p><h2 id="c84d" class="mw lv iq bd lw ne nf dn ma ng nh dp me le ni nj mg li nk nl mi lm nm nn mk no bi translated">基尼杂质</h2><p id="c33e" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">如同在其他机器学习模型中一样，决策树训练机制试图最小化由训练集上的预测误差引起的一些损失。<strong class="kx ir">基尼杂质指数</strong>(源自意大利统计学家<strong class="kx ir"> <em class="lr">科拉多基尼</em> </strong>)是分类准确度的自然衡量标准。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/a5938ec9ed5ddd9d80c5abf89136b23b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WM7wOsRiiWnI1V1yEUcuhg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4-树木的两个常见训练目标:基尼系数和熵。P(ci)是从总体中随机选取一个ci类样本的概率。n是类的数量。</p></figure><p id="0d81" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">高基尼系数对应的是异质人口(每个阶层的样本量相似)，而低基尼系数表示同质人口(即主要由单一阶层组成)</p><p id="06bf" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最大可能的基尼值取决于类别的数量:在C个类别的分类问题中，最大可能的基尼值是1–1/C(当类别均匀分布时)。最低基尼系数为0，当整个人口由一个阶层组成时，基尼系数达到0。</p><p id="b596" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">基尼系数是随机分类时错误分类的期望值。</strong></p><p id="38b2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为什么？</p><p id="0004" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因为从ci类中随机选取一个样本的概率是<strong class="kx ir"> <em class="lr"> p(ci) </em> </strong>。挑了那个，预测错类的概率是<strong class="kx ir"> <em class="lr"> (1-p(ci)) </em> </strong>。如果我们对所有类别的<strong class="kx ir"><em class="lr">p(ci)*(1-p(ci))</em></strong>求和，我们得到图4中的基尼系数公式。</p><p id="859f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以基尼指数为目标，该树在每一步中选择特征和阈值，以最大限度地降低两个结果人口的加权平均基尼系数(或最大限度地增加其加权平均同质性)的方式分割人口。换句话说，训练逻辑是最小化两个结果群体中随机分类错误的概率，将更多的权重放在较大的子群体上。</p><p id="99ff" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">是的——该机制会检查所有样本值，并根据标准将其拆分，以检查所得的基尼系数。</p><p id="f219" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">根据这个定义，我们还可以理解为什么阈值总是在至少一个训练样本上找到的实际值——使用样本之间的间隙值没有好处，因为得到的分割是相同的。</p><p id="e773" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">另一个常用于树训练的度量是<strong class="kx ir"> <em class="lr">熵</em> </strong>(见图4中的公式)。</p><h2 id="41d4" class="mw lv iq bd lw ne nf dn ma ng nh dp me le ni nj mg li nk nl mi lm nm nn mk no bi translated">熵</h2><p id="0580" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">基尼策略的目标是最小化下一步的随机分类误差，而熵最小化策略的目标是最大化<strong class="kx ir"> <em class="lr">信息增益</em> </strong>。</p><h2 id="75d4" class="mw lv iq bd lw ne nf dn ma ng nh dp me le ni nj mg li nk nl mi lm nm nn mk no bi translated">信息增益</h2><p id="2d55" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">在缺乏关于一个群体如何被分成10类的先验知识的情况下，我们假设在它们之间平均分配。在这种情况下，我们需要平均3.3个是/否问题来确定样本的分类(您是1-5级吗？如果不是，你是6-8班的吗？等等。).这意味着总体的熵是3.3比特。</p><p id="3533" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是现在让我们假设我们做了一些事情(比如一个聪明的分裂)给了我们关于人口分布的信息，现在我们知道50%的样本在1类，25%在2类，25%在3类。在这种情况下，总体的熵将是1.5——我们只需要1.5比特来描述随机样本。(第一个问题——你是一班的吗？这个序列有50%的时间会在这里结束。第二个问题——你是二班的吗？并且不需要更多的问题—所以平均50%*1 + 50%*2 = 1.5个问题)。我们得到的信息值1.8比特。</p><p id="3d45" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">像基尼一样，最小化熵也与创建更同质的人口相关，因为同质人口具有更低的熵(单个阶层人口的极端熵为0——无需问任何是/否问题)。</p><h2 id="a9b4" class="mw lv iq bd lw ne nf dn ma ng nh dp me le ni nj mg li nk nl mi lm nm nn mk no bi translated">基尼还是熵？</h2><p id="7a11" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">大多数消息来源声称，这两种策略之间的差异并不显著(事实上，如果你试图在我们刚刚解决的问题上训练一棵熵树，你会得到完全相同的分裂)。很容易看出为什么:当基尼系数最大化一类概率的期望值时，熵最大化对数类概率的期望值。但是对数概率是概率的单调递增函数，所以它们的操作通常非常相似。然而，当人口高度不平衡时，<strong class="kx ir">熵最小化可能选择不同于基尼系数的配置</strong>。例如，我们可以考虑一个包含1000个训练样本的数据集，其中980个属于0类，20个属于1类。假设该树可以根据图5中的示例<strong class="kx ir"> <em class="lr"> a </em> </strong>或示例<strong class="kx ir"> <em class="lr"> b </em> </strong>选择一个阈值对其进行分割。我们注意到，这两个示例都创建了一个主要由多数类组成的人口众多的节点，以及一个主要由少数类组成的人口较少的第二节点。</p><p id="80a3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这种情况下，对数函数在小值处的陡度将比基尼准则更强烈地激励熵准则来净化具有大群体的节点。如果我们算一下，我们会发现基尼系数会选择分裂<strong class="kx ir"> <em class="lr"> a </em> </strong>，熵系数会选择分裂<strong class="kx ir"> <em class="lr"> b </em> </strong>。</p><p id="a932" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这可能导致特征/阈值的不同选择。不一定更好或更坏——这取决于我们的目标。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/c25a1862f60acbf9948b9f5b29e28aad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1QLa0O80uNUKv0_RYRtdcA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5高度不平衡数据的两种可能分裂。图片作者。</p></figure><p id="34a2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请注意，即使在初始群体平衡的问题中，分类树的较低节点通常也会具有高度不平衡的群体。</p><h1 id="41e2" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">培训结束</h1><p id="2b26" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">当树中的路径达到指定的深度值时，或者当它包含零基尼/熵群体时，它停止训练。当所有路径停止训练时，树就准备好了。</p><p id="d0e3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通常的做法是限制树的深度。另一个是限制一个叶子中的样本数量(不允许样本少于阈值)。这两种做法都是为了防止列车组过度装配。</p><h1 id="551f" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">回归树</h1><p id="df0d" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">既然我们已经研究出了训练分类树的细节，那么理解回归树就非常简单了:回归问题中的标签是连续的而不是离散的(例如，给定药物剂量的有效性，以%的情况来衡量)。在这类问题上训练，回归树也分类，但是标签是作为每个节点中样本的平均值动态计算的。这里，通常使用<strong class="kx ir"> <em class="lr">均方误差</em> </strong>或<strong class="kx ir"> <em class="lr">卡方度量</em> </strong>作为最小化的目标，而不是基尼和熵。</p><h1 id="3f12" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">结论</h1><p id="2bb5" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">在这篇文章中，我们了解到决策树基本上是比较序列，可以训练来执行分类和回归任务。我们运行python脚本来训练决策树分类器，使用我们的分类器来预测几个数据样本的类别，并计算训练集和测试集上预测的精度和召回指标。我们还学习了决策树训练背后的数学机制，其目的是在每次比较后最小化一些预测误差度量(基尼、熵、mse)。我希望你喜欢读这篇文章，在我的其他文章中再见！</p></div></div>    
</body>
</html>