<html>
<head>
<title>Monte Carlo Cross-Validation for Time Series</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">时间序列的蒙特卡罗交叉验证</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/monte-carlo-cross-validation-for-time-series-ed01c41e2995#2022-12-13">https://towardsdatascience.com/monte-carlo-cross-validation-for-time-series-ed01c41e2995#2022-12-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ceb4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何获得更好的预测性能估计值<strong class="ak">带有一点随机性</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c52a32916f9984048068330e5de1776c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DZ06-YmGhE0WP_nS"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@markusspiske?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马库斯·斯皮斯克</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="04b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://medium.com/towards-data-science/4-things-to-do-when-applying-cross-validation-with-time-series-c6a5674ebf3a" rel="noopener">对时间序列应用交叉验证需要注意几个关键方面</a>。这些要点对于防止泄漏和获得可靠的性能估计非常重要。</p><p id="b20b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，您将了解蒙特卡罗交叉验证。流行的时间序列分割法的替代方法。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="1bc7" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">时间序列交叉验证</h1><p id="e9e7" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">时间序列分割通常是使用时间序列数据进行交叉验证的首选方法。图 1 说明了这种方法是如何操作的。可用的时间序列被分成几个大小相等的折叠。然后，每个折叠首先用于测试一个模型，然后重新训练它。除了第一折，只用于训练。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/4fc64475d79378d894c85f0df437142e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*faizY1gXBIK2v9di5pKb8Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 1:时间序列分成 5 个部分。使用 plotnine Python 库创建的图。<a class="ae ky" href="https://github.com/vcerqueira/blog/blob/main/src/cv_plot.py" rel="noopener ugc nofollow" target="_blank">下面是源代码</a>。图片作者。</p></figure><p id="1475" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用 TimeSeriesSplit 进行交叉验证的主要好处如下:</p><ol class=""><li id="f87b" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated">它保留了观察的顺序。这个问题在有序数据集(如时间序列)中很重要。</li><li id="4ade" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">它会产生许多裂缝。几个分割导致更健壮的性能估计。如果数据集不大，这一点尤其重要。</li></ol><p id="e637" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TimeSeriesSplit 的主要缺点是跨折叠的训练样本大小不一致。</p><p id="db48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是什么意思？</p><p id="7aac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设该方法应用了 5 次折叠，如图 1 所示。在第一次迭代中，所有可用观测值的 20%用于训练。但是，这个数字在最后一次迭代中是 80%。因此，初始迭代可能不代表完整的时间序列。这个问题会影响性能估计。</p><p id="9663" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你如何解决这个问题？</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="a80b" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">蒙特卡罗交叉验证</h1><p id="df7f" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">蒙特卡洛交叉验证是一种可用于时间序列的方法。想法是在不同的<strong class="lb iu">随机</strong>起点重复典型的维持周期。</p><p id="76d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是对这种方法的直观描述:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/97a05eafa29b6b8eaf7c6b786ab98f5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iP1fYsD6nHxk-66nr6v73Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 2:蒙特卡洛交叉验证，5 倍。图片作者。</p></figure><p id="0902" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像时间序列一样，MonteCarloCV 也保留了观测的时间顺序。它还重复几次估计过程。</p><p id="feeb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MonteCarloCV 与 TimeSeriesSplit 的区别主要体现在两个方面:</p><ul class=""><li id="cc1e" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu np ng nh ni bi translated"><strong class="lb iu">训练和验证样本量</strong>。如上所述，使用 TimeSeriesSplit 时，训练集的大小会增加。在 MonteCarloCV 中，训练集大小在过程的每次迭代中都是固定的。这防止了不代表整个数据的训练大小；</li><li id="2f7a" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu np ng nh ni bi translated"><strong class="lb iu">随机分区</strong>。在 MonteCarloCV 中，验证起点是随机选择的。这个原点是标志训练集结束和验证开始的点。在时间序列分裂的情况下，这一点是决定性的。它是根据迭代次数预先定义的。</li></ul><p id="cbe5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">蒙特卡洛夫最早是由皮卡尔和库克使用的。详情可查阅参考文献[1]。</p><p id="786a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我在博士期间研究过蒙特卡洛夫的一些细节。这包括与其他方法的比较，如 TimeSeriesSplit。我的目标之一是了解哪种方法能提供最好的估计。结论指向蒙特卡洛夫，所以我一直用到了现在。您可以查看参考文献[2]中的完整研究。</p><p id="9b70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，<em class="nq"> scikit-learn </em>没有提供 MonteCarloCV 的实现。所以，我决定自己来做:</p><pre class="kj kk kl km gt nr ns nt bn nu nv bi"><span id="6477" class="nw md it ns b be nx ny l nz oa">from typing import List, Generator<br/><br/>import numpy as np<br/><br/>from sklearn.model_selection._split import _BaseKFold<br/>from sklearn.utils.validation import indexable, _num_samples<br/><br/><br/>class MonteCarloCV(_BaseKFold):<br/><br/>    def __init__(self,<br/>                 n_splits: int,<br/>                 train_size: float,<br/>                 test_size: float,<br/>                 gap: int = 0):<br/>        """<br/>        Monte Carlo Cross-Validation<br/><br/>        Holdout applied in multiple testing periods<br/>        Testing origin (time-step where testing begins) is randomly chosen according to a monte carlo simulation<br/><br/>        :param n_splits: (int) Number of monte carlo repetitions in the procedure<br/>        :param train_size: (float) Train size, in terms of ratio of the total length of the series<br/>        :param test_size: (float) Test size, in terms of ratio of the total length of the series<br/>        :param gap: (int) Number of samples to exclude from the end of each train set before the test set.<br/>        """<br/><br/>        self.n_splits = n_splits<br/>        self.n_samples = -1<br/>        self.gap = gap<br/>        self.train_size = train_size<br/>        self.test_size = test_size<br/>        self.train_n_samples = 0<br/>        self.test_n_samples = 0<br/><br/>        self.mc_origins = []<br/><br/>    def split(self, X, y=None, groups=None) -&gt; Generator:<br/>        """Generate indices to split data into training and test set.<br/>        Parameters<br/>        ----------<br/>        X : array-like of shape (n_samples, n_features)<br/>            Training data, where `n_samples` is the number of samples<br/>            and `n_features` is the number of features.<br/>        y : array-like of shape (n_samples,)<br/>            Always ignored, exists for compatibility.<br/>        groups : array-like of shape (n_samples,)<br/>            Always ignored, exists for compatibility.<br/>        Yields<br/>        ------<br/>        train : ndarray<br/>            The training set indices for that split.<br/>        test : ndarray<br/>            The testing set indices for that split.<br/>        """<br/><br/>        X, y, groups = indexable(X, y, groups)<br/>        self.n_samples = _num_samples(X)<br/><br/>        self.train_n_samples = int(self.n_samples * self.train_size) - 1<br/>        self.test_n_samples = int(self.n_samples * self.test_size) - 1<br/><br/>        # Make sure we have enough samples for the given split parameters<br/>        if self.n_splits &gt; self.n_samples:<br/>            raise ValueError(<br/>                f'Cannot have number of folds={self.n_splits} greater'<br/>                f' than the number of samples={self.n_samples}.'<br/>            )<br/>        if self.train_n_samples - self.gap &lt;= 0:<br/>            raise ValueError(<br/>                f'The gap={self.gap} is too big for number of training samples'<br/>                f'={self.train_n_samples} with testing samples={self.test_n_samples} and gap={self.gap}.'<br/>            )<br/><br/>        indices = np.arange(self.n_samples)<br/><br/>        selection_range = np.arange(self.train_n_samples + 1, self.n_samples - self.test_n_samples - 1)<br/><br/>        self.mc_origins = \<br/>            np.random.choice(a=selection_range,<br/>                             size=self.n_splits,<br/>                             replace=True)<br/><br/>        for origin in self.mc_origins:<br/>            if self.gap &gt; 0:<br/>                train_end = origin - self.gap + 1<br/>            else:<br/>                train_end = origin - self.gap<br/>            train_start = origin - self.train_n_samples - 1<br/><br/>            test_end = origin + self.test_n_samples<br/><br/>            yield (<br/>                indices[train_start:train_end],<br/>                indices[origin:test_end],<br/>            )<br/><br/>    def get_origins(self) -&gt; List[int]:<br/>        return self.mc_origins</span></pre><h2 id="09a9" class="ob md it bd me oc od dn mi oe of dp mm li og oh mo lm oi oj mq lq ok ol ms om bi translated">实际例子</h2><p id="9654" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">MonteCarloCV 有四个参数:</p><ul class=""><li id="8b07" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu np ng nh ni bi translated"><em class="nq"> n_splits </em>:折叠或迭代的次数。这个值趋向于高达 10；</li><li id="4110" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu np ng nh ni bi translated"><em class="nq"> training_size </em>:每次迭代训练集的大小，为时间序列大小的比值；</li><li id="15e2" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu np ng nh ni bi translated"><em class="nq"> test_size </em>:类似 training_size，但是针对验证集；</li><li id="c178" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu np ng nh ni bi translated"><em class="nq"> gap </em>:分离训练集和验证集的观察值的数量。与 TimeSeriesSplits 中一样，该参数的值默认为 0(无间隙)。</li></ul><p id="56ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每次迭代的训练和验证规模取决于输入数据。我发现一个 0.6/0.1 的分区工作得很好。也就是说，在每次迭代中，60%的数据用于训练。以下 10%的观察值用于验证。</p><p id="687d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是上述配置的一个示例:</p><pre class="kj kk kl km gt nr ns nt bn nu nv bi"><span id="53f6" class="nw md it ns b be nx ny l nz oa">from sklearn.datasets import make_regression<br/>from src.mccv import MonteCarloCV<br/><br/>X, y = make_regression(n_samples=120)<br/><br/>mccv = MonteCarloCV(n_splits=5, <br/>                    train_size=0.6, <br/>                    test_size=0.1, <br/>                    gap=0)<br/><br/>for train_index, test_index in mccv.split(X):<br/>    print("TRAIN:", train_index, "TEST:", test_index)<br/>    X_train, X_test = X[train_index], X[test_index]<br/>    y_train, y_test = y[train_index], y[test_index]</span></pre><p id="14ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该实现也与<em class="nq"> scikit-learn </em>兼容。下面介绍如何将其与<em class="nq"> GridSearchCV </em>结合起来:</p><pre class="kj kk kl km gt nr ns nt bn nu nv bi"><span id="9242" class="nw md it ns b be nx ny l nz oa">from sklearn.ensemble import RandomForestRegressor<br/>from sklearn.model_selection import GridSearchCV<br/><br/>model = RandomForestRegressor()<br/>param_search = {'n_estimators': [10, 100]}<br/><br/>gsearch = GridSearchCV(estimator=model, cv=mccv, param_grid=param_search)<br/>gsearch.fit(X, y)</span></pre><p id="2599" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望你觉得 MonteCarloCV 有用！</p><p id="97f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读，下一个故事再见。</p><h2 id="cb98" class="ob md it bd me oc od dn mi oe of dp mm li og oh mo lm oi oj mq lq ok ol ms om bi translated">参考</h2><p id="6cd1" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">[1]皮卡尔、理查德·库克和丹尼斯·库克。"回归模型的交叉验证."美国统计协会杂志 79.387(1984):575–583。</p><p id="cef5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]熊伟·塞尔奎拉、路易斯·托尔戈和伊戈尔·莫泽蒂奇。"评估时间序列预测模型:性能估计方法的实证研究."机器学习 109.11(2020):1997–2028。</p></div></div>    
</body>
</html>