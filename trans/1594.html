<html>
<head>
<title>Graph Attention Networks: Self-Attention Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图形注意网络:自我注意的解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/graph-attention-networks-in-python-975736ac5c0c#2022-04-17">https://towardsdatascience.com/graph-attention-networks-in-python-975736ac5c0c#2022-04-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f4e0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用PyTorch几何图形的带自我注意的GNNs指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c99e1fbd18f2c57c9e0de1e7a30e4b51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*_p3JftpWqL3pJbRTxqflgg.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者提供，文件图标由<a class="ae ky" href="https://openmoji.org/" rel="noopener ugc nofollow" target="_blank">open moji</a>(<a class="ae ky" href="https://creativecommons.org/licenses/by-sa/4.0/#" rel="noopener ugc nofollow" target="_blank">CC BY-SA 4.0</a>)提供</p></figure><p id="3a61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图形注意力网络是<strong class="lb iu">最流行的图形神经网络类型</strong>之一。理由很充分。</p><p id="9e46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在图<em class="lv">卷积</em>网络(GCN)中，每个邻居都有<strong class="lb iu">相同的重要性</strong>。显然，情况不应该是这样的:有些节点比其他节点更重要。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/adb83dc5f45451256b42a643bb34329a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h0KUV0wX5KckVNoiWy5P2A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">节点4比节点3重要，节点3比节点2重要(图片由作者提供)</p></figure><p id="4df4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图表<em class="lv">注意</em>网络为这个问题提供了一个解决方案。为了考虑每个邻居的重要性，注意机制为每个连接分配一个<strong class="lb iu">加权因子。</strong></p><p id="eadd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将看到如何<strong class="lb iu">计算</strong>这些注意力分数，以及如何<strong class="lb iu">在PyTorch Geometric (PyG)中实现</strong>一个高效的GAT。你可以用下面的<a class="ae ky" href="https://colab.research.google.com/drive/1B0vLpH_gSfrOLgsc2UZVyXrcofzA-t0L?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Google Colab笔记本</a>运行本教程的代码。</p><h1 id="ffe8" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">🌐一.图表数据</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mp"><img src="../Images/39d1b7f3ee0024328eec4572d1fe2072.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6DAVkX2UjMdH-ijK"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">CiteSeer数据集(图片由作者提供，由<a class="ae ky" href="https://www.yworks.com/yed-live/" rel="noopener ugc nofollow" target="_blank"> yEd Live </a>制作)</p></figure><p id="7405" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以在这项工作中使用三个经典的图形数据集(麻省理工学院许可)。它们代表了研究论文的网络，每个连接都是一个引用。</p><ul class=""><li id="d150" class="mq mr it lb b lc ld lf lg li ms lm mt lq mu lu mv mw mx my bi translated"><a class="ae ky" href="http://www.kamalnigam.com/papers/cora-jnl.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> Cora </strong> </a>:由2708篇属于7个类别之一的机器学习论文组成。<br/> ➡️节点特征表示一篇论文中1433个单词的存在(1)或不存在(0)(二进制<a class="ae ky" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank">包单词</a>)。</li><li id="1b4c" class="mq mr it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated"><a class="ae ky" href="https://clgiles.ist.psu.edu/papers/DL-1998-citeseer.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> CiteSeer </strong> </a>:这是一个更大但相似的数据集，包含3312篇科学论文，可归为6个类别之一。<br/> ➡️节点特征表示论文中3703个单词的存在(1)或不存在(0)。</li><li id="6629" class="mq mr it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated"><a class="ae ky" href="http://eliassi.org/papers/ai-mag-tr08.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> PubMed </strong> </a>:这是一个更大的数据集，有来自PubMed数据库的19717篇关于糖尿病的科学出版物，分为3类。<br/> ➡️节点特征是来自500个唯一单词的字典的<a class="ae ky" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> TF-IDF加权单词向量</a>。</li></ul><p id="454b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些数据集已经被科学界广泛使用。作为一项挑战，我们可以使用<strong class="lb iu">多层感知器</strong> (MLPs)、<strong class="lb iu"> GCNs </strong>和<strong class="lb iu"> GATs </strong>将我们的准确度分数与在<a class="ae ky" href="https://arxiv.org/pdf/1710.10903.pdf" rel="noopener ugc nofollow" target="_blank">文献</a>中获得的分数进行比较:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/a0ce91a93424b2d50c1d7de67bf9d83d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nnsT7G3fb_EfuNS9NVSK4w.png"/></div></div></figure><p id="c8e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PubMed非常大，所以处理它和在它上面训练一个GNN需要更长的时间。Cora是文献中研究最多的一个，所以让我们<strong class="lb iu">把CiteSeer </strong>作为中间地带。</p><p id="1502" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以用<a class="ae ky" href="https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.Planetoid" rel="noopener ugc nofollow" target="_blank">小行星类</a>将这些数据集直接导入PyTorch Geometric:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="6bd3" class="nm ly it ni b gy nn no l np nq">Number of graphs: 1<br/>Number of nodes: 3327<br/>Number of features: 3703<br/>Number of classes: 6<br/>Has isolated nodes: True</span></pre><p id="d0d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有趣的是，我们有3327个节点，而不是3312个。我发现PyG实际上使用了本文的<a class="ae ky" href="https://arxiv.org/pdf/1603.08861.pdf" rel="noopener ugc nofollow" target="_blank">CiteSeer的</a>实现，它也显示3327个节点。谜团暂时解开了。</p><p id="831f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，我们观察到<strong class="lb iu">有些节点是孤立的</strong>(准确的说是48)！正确分类这些孤立的节点将是一个挑战，因为我们不能依赖任何聚合。</p><p id="98f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用<code class="fe nr ns nt ni b">degree</code>画出每个节点的连接数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/33089f2c29396fe695c4b1441aae6650.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Md0aDjloaEGZCPssWvxkaQ.png"/></div></div></figure><p id="074e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大多数节点只有<strong class="lb iu"> 1或2个邻居</strong>。这可以解释为什么CiteSeer <strong class="lb iu"> </strong>获得的准确度分数比其他两个数据集低…</p><h1 id="a602" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">⚠️二世。自我关注</h1><p id="607e" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">由<a class="ae ky" href="https://arxiv.org/abs/1710.10903" rel="noopener ugc nofollow" target="_blank">veli kovi等人</a>在2017年提出，GNNs中的自我关注依赖于一个简单的想法:<strong class="lb iu">节点不应该都具有相同的重要性</strong>。</p><p id="a272" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们谈论<em class="lv">自我</em>——注意力(不仅仅是注意力)，因为输入是相互比较的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/4cfaf09387eccd48161585305378594d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AVQq-PJoU3GCffAuQJXXvA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="cb86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该机制为每个连接分配一个<strong class="lb iu">加权因子</strong>(关注分数)<strong class="lb iu"> </strong>。姑且称之为<strong class="lb iu"> <em class="lv"> α </em> ᵢⱼ </strong>节点<em class="lv"> i </em>和<em class="lv"> j </em>之间的关注度得分。</p><p id="38c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是如何计算节点1的嵌入，其中𝐖是共享权重矩阵:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/c282bd8df5b7b85aec4438338bb0150d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-7zYVLyhNdqj7yxlOJZa6A.png"/></div></div></figure><p id="0e2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是我们如何计算注意力分数呢？我们可以写一个静态公式，但是有一个更聪明的解决方案:我们可以用神经网络来<strong class="lb iu">学习</strong> <strong class="lb iu">他们的值。这个过程有三个步骤:</strong></p><ol class=""><li id="3d0e" class="mq mr it lb b lc ld lf lg li ms lm mt lq mu lu ob mw mx my bi translated"><strong class="lb iu">线性变换</strong>；</li><li id="baa0" class="mq mr it lb b lc mz lf na li nb lm nc lq nd lu ob mw mx my bi translated"><strong class="lb iu">激活功能</strong>；</li><li id="0714" class="mq mr it lb b lc mz lf na li nb lm nc lq nd lu ob mw mx my bi translated"><strong class="lb iu"> Softmax归一化。</strong></li></ol><h2 id="5370" class="nm ly it bd lz oc od dn md oe of dp mh li og oh mj lm oi oj ml lq ok ol mn om bi translated">1️⃣线性变换</h2><p id="813e" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">我们想要计算每个连接的<strong class="lb iu">重要性，所以我们需要成对的隐藏向量。创建这些对的一个简单方法是连接两个节点的向量。</strong></p><p id="6a0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">只有这样，我们才能应用一个新的<strong class="lb iu">线性变换</strong>和一个权重矩阵𝐖 <strong class="lb iu"> ₐₜₜ </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/89c25d16daa6d24b0453f33f2bd50860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2zojhcpsK92_SneI0zmWaw.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/7e347c2bc655d6ee095d56852844fe0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*chXBes9TjC6WMD637MYSIg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="1310" class="nm ly it bd lz oc od dn md oe of dp mh li og oh mj lm oi oj ml lq ok ol mn om bi translated">2️⃣激活函数</h2><p id="99dd" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">我们正在构建一个<strong class="lb iu"> </strong>神经网络，所以第二步是添加一个激活函数。在这种情况下，论文的作者选择了<em class="lv"> LeakyReLU </em>函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/718648a264910db9f3dfdb393301862c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*onA_k8ocjpon4UovKwZSog.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/84d166adc55da68b97cbf0a958d0d4e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-1RHa4aDJRhdbNLtVuNM2w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="21e5" class="nm ly it bd lz oc od dn md oe of dp mh li og oh mj lm oi oj ml lq ok ol mn om bi translated">3️⃣软件最大归一化</h2><p id="69da" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">我们的神经网络的输出是<strong class="lb iu">而不是归一化的</strong>，这是一个问题，因为我们想要比较这些分数。为了能够说节点2对于节点1是否比节点3更重要(<em class="lv">α</em>₁₂&gt;t24】α₁₃)，我们需要共享相同的尺度。</p><p id="968e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用神经网络的一个常见方法是使用<strong class="lb iu"> <em class="lv"> softmax </em> </strong>函数。这里，我们将其应用于每个相邻节点:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/b18f006fa6d91aecf440653e15967163.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wdXTVPRQhs561BuWAoE8aw.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/502350af721f729cdfeca8ef1a916e7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZY9HLOvwVXxLc-wMNH17fw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="b320" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在你知道了:我们可以计算每一个ᵢⱼ.唯一的问题是… <strong class="lb iu">自我关注不是很稳定</strong>。为了提高性能，<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> Vaswani等人</a>在变压器架构中引入了多头关注。</p><h2 id="b6b5" class="nm ly it bd lz oc od dn md oe of dp mh li og oh mj lm oi oj ml lq ok ol mn om bi translated">4️⃣奖金:多头关注</h2><p id="238d" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">这只是稍微令人惊讶，因为我们已经谈论了很多关于自我关注的话题，但是，事实上，<strong class="lb iu">变形金刚是伪装的GNNs</strong>。这就是为什么我们可以在这里重用自然语言处理中的一些思想。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/72faececcc274c615ce176d42a124473.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A3C7OdAe1DrVEumyYvUdhw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多头关注(图片由作者提供)</p></figure><p id="8d82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在GATs中，多头注意力包括<strong class="lb iu">多次重复相同的3个步骤</strong>，以便平均或连接结果。就是这样。代替单一的<em class="lv"> h₁ </em>，我们为每个注意力头得到一个隐藏向量<em class="lv"> h₁ᵏ </em>。然后可以应用以下两种方案之一:</p><ul class=""><li id="01b6" class="mq mr it lb b lc ld lf lg li ms lm mt lq mu lu mv mw mx my bi translated"><strong class="lb iu">平均值</strong>:我们将不同的<em class="lv"> hᵢᵏ </em>相加，并通过注意力头数<em class="lv"> n </em>将结果归一化；</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/3ee99d58d2bf533905998db457e6aaf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O77fo-OAXP5n5y6izG0lAw.png"/></div></div></figure><ul class=""><li id="8e77" class="mq mr it lb b lc ld lf lg li ms lm mt lq mu lu mv mw mx my bi translated"><strong class="lb iu">串联</strong>:我们串联不同的<em class="lv"> hᵢᵏ </em>。​</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/368dc94eeb8b1b2747502f3a7a261154.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RiEetyq7VOsTxHtOAPx3lQ.png"/></div></div></figure><p id="169b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实际上，当它是一个隐藏层时，我们使用<strong class="lb iu">连接方案</strong>，当它是网络的最后一层时，我们使用<strong class="lb iu">平均方案</strong>。</p><h1 id="3058" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">🧠三世。图形注意网络</h1><p id="2cfb" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">让我们在PyTorch几何中实现一个GAT。这个库有<strong class="lb iu">两个不同的图形关注层</strong> : <code class="fe nr ns nt ni b">GATConv</code>和<code class="fe nr ns nt ni b">GATv2Conv</code>。</p><p id="ecb8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，我们谈论的是<code class="fe nr ns nt ni b">GatConv</code>层，但在2021年<a class="ae ky" href="https://arxiv.org/abs/2105.14491" rel="noopener ugc nofollow" target="_blank"> Brody等人</a>通过修改操作顺序引入了一种改进。在串联后应用权重矩阵𝐖<strong class="lb iu">，在<em class="lv">泄漏</em>函数</strong>后使用<strong class="lb iu">注意力权重矩阵𝐖 <strong class="lb iu"> ₐₜₜ </strong>。总而言之:</strong></p><ul class=""><li id="0892" class="mq mr it lb b lc ld lf lg li ms lm mt lq mu lu mv mw mx my bi translated"><code class="fe nr ns nt ni b">GatConv</code>:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/ee8112f672d3e66c2e45fc910aac4ddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zzxjH_OF43Hy8GZeo95aPA.png"/></div></div></figure><ul class=""><li id="73bf" class="mq mr it lb b lc ld lf lg li ms lm mt lq mu lu mv mw mx my bi translated"><code class="fe nr ns nt ni b">Gatv2Conv</code>:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/d6d41a5cbf186cf083821f0b258c38c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XNA9GR4YDZdjPrqHBMcsJQ.png"/></div></div></figure><p id="3f93" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你应该用哪一个？根据Brody等人的说法，<code class="fe nr ns nt ni b"><strong class="lb iu">Gatv2Conv</strong></code> <strong class="lb iu">始终优于</strong> <code class="fe nr ns nt ni b"><strong class="lb iu">GatConv</strong></code> <strong class="lb iu"> </strong>，因此应该首选。</p><p id="5b37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们对CiteSeer的论文进行分类！我试着<strong class="lb iu">大致重现原作者的实验</strong>而不增加太多复杂性。你可以在GitHub 上找到GAT <a class="ae ky" href="https://github.com/PetarV-/GAT" rel="noopener ugc nofollow" target="_blank">的官方实现。</a></p><p id="16f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，我们在两种配置中使用图形关注层:</p><ul class=""><li id="78fc" class="mq mr it lb b lc ld lf lg li ms lm mt lq mu lu mv mw mx my bi translated"><strong class="lb iu">第一层</strong>串接8路输出(多头注意)；</li><li id="f030" class="mq mr it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated"><strong class="lb iu">第二层</strong>只有1个头，产生我们的最终嵌入。</li></ul><p id="5ebd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还要训练和测试一个GCN来比较准确率。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="907b" class="nm ly it ni b gy nn no l np nq">GCN(<br/>  (gcn1): GCNConv(3703, 16)<br/>  (gcn2): GCNConv(16, 6)<br/>)</span><span id="c8fe" class="nm ly it ni b gy oo no l np nq">Epoch   0 | Train Loss: 1.782 | Train Acc:  20.83% | Val Loss: 1.79 <br/>Epoch  20 | Train Loss: 0.165 | Train Acc:  95.00% | Val Loss: 1.30 <br/>Epoch  40 | Train Loss: 0.069 | Train Acc:  99.17% | Val Loss: 1.66 <br/>Epoch  60 | Train Loss: 0.053 | Train Acc:  99.17% | Val Loss: 1.50 <br/>Epoch  80 | Train Loss: 0.054 | Train Acc: 100.00% | Val Loss: 1.67 <br/>Epoch 100 | Train Loss: 0.062 | Train Acc:  99.17% | Val Loss: 1.62 <br/>Epoch 120 | Train Loss: 0.043 | Train Acc: 100.00% | Val Loss: 1.66 <br/>Epoch 140 | Train Loss: 0.058 | Train Acc:  98.33% | Val Loss: 1.68 <br/>Epoch 160 | Train Loss: 0.037 | Train Acc: 100.00% | Val Loss: 1.44 <br/>Epoch 180 | Train Loss: 0.036 | Train Acc:  99.17% | Val Loss: 1.65 <br/>Epoch 200 | Train Loss: 0.093 | Train Acc:  95.83% | Val Loss: 1.73 <br/><br/><strong class="ni iu">GCN test accuracy: 67.70%<br/></strong><br/>CPU times: user 25.1 s, sys: 847 ms, total: 25.9 s<br/>Wall time: <strong class="ni iu">32.4 s</strong></span></pre><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="bbdd" class="nm ly it ni b gy nn no l np nq">GAT(<br/>  (gat1): GATv2Conv(3703, 8, heads=8)<br/>  (gat2): GATv2Conv(64, 6, heads=1)<br/>)</span><span id="515d" class="nm ly it ni b gy oo no l np nq">Epoch   0 | Train Loss: 1.790 | Val Loss: 1.81 | Val Acc: 12.80%<br/>Epoch  20 | Train Loss: 0.040 | Val Loss: 1.21 | Val Acc: 64.80%<br/>Epoch  40 | Train Loss: 0.027 | Val Loss: 1.20 | Val Acc: 67.20%<br/>Epoch  60 | Train Loss: 0.009 | Val Loss: 1.11 | Val Acc: 67.00%<br/>Epoch  80 | Train Loss: 0.013 | Val Loss: 1.16 | Val Acc: 66.80%<br/>Epoch 100 | Train Loss: 0.013 | Val Loss: 1.07 | Val Acc: 67.20%<br/>Epoch 120 | Train Loss: 0.014 | Val Loss: 1.12 | Val Acc: 66.40%<br/>Epoch 140 | Train Loss: 0.007 | Val Loss: 1.19 | Val Acc: 65.40%<br/>Epoch 160 | Train Loss: 0.007 | Val Loss: 1.16 | Val Acc: 68.40%<br/>Epoch 180 | Train Loss: 0.006 | Val Loss: 1.13 | Val Acc: 68.60%<br/>Epoch 200 | Train Loss: 0.007 | Val Loss: 1.13 | Val Acc: 68.40%<br/><br/><strong class="ni iu">GAT test accuracy: 70.00%<br/></strong><br/>CPU times: user 53.4 s, sys: 2.68 s, total: 56.1 s<br/>Wall time: <strong class="ni iu">55.9 s</strong></span></pre><p id="8b5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个实验并不是超级严格的:我们需要<strong class="lb iu">重复<em class="lv"> n </em>次</strong>，取平均精度和标准偏差作为最终结果。</p><p id="c518" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个例子中，我们可以看到<strong class="lb iu"> GAT在精度方面优于GCN</strong>(70.00%对67.70%)，但训练时间更长(55.9秒对32.4秒)。在处理大型图形时，这种折衷会导致可伸缩性问题。</p><p id="e67a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者获得了GAT的72.5%和GCN的70.3%，这明显优于我们所做的。这种差异可以通过<strong class="lb iu">预处理</strong>、模型中的一些<strong class="lb iu">调整、</strong>和不同的<strong class="lb iu">训练设置</strong>来解释(例如，<em class="lv">耐心为100，而不是固定的周期数)。</em></p><p id="6892" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们想象一下猫学到了什么。我们将使用<a class="ae ky" href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" rel="noopener ugc nofollow" target="_blank"> t-SNE </a>，这是一种在2D或3D中绘制高维数据的强大方法。首先，让我们看看在任何训练之前嵌入是什么样子的:它应该是绝对<strong class="lb iu">随机的</strong>，因为它们是由随机初始化的权重矩阵产生的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/f6b6f89d25ab5222b27ad3842505e26d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0qB0PpkvaER4bYwXTQIysQ.png"/></div></div></figure><p id="b3de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">的确，这里没有<strong class="lb iu">没有明显的结构</strong>。但是由我们训练的模型产生的嵌入看起来更好吗？</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/14a49e8d438abe29f4537af0ee2abf4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RGEUniuYZllkCOgLlFDp1g.png"/></div></div></figure><p id="1940" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">区别是显而易见的:<strong class="lb iu">属于相同类的节点聚集在一起</strong>。我们可以看到6组，对应于6类论文。有异常值，但这是意料之中的:我们的准确度分数远非完美。</p><p id="ba88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">之前，我推测连接不良的节点<strong class="lb iu"> </strong>可能会对CiteSeer上的<strong class="lb iu">性能产生负面影响。让我们计算每一个度数的模型精度。</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/492319053fb01e6ebd8cc9664548c736.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q-CkQnJqXCPaohouE_OkcA.png"/></div></div></figure><p id="44d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些结果证实了我们的直觉:邻居少的节点确实<strong class="lb iu">更难分类</strong>。这是由GNNs的性质决定的:你拥有的相关连接越多，你能聚集的信息就越多。</p><h1 id="db3b" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">结论</h1><p id="802a" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">虽然GATs需要更长的训练时间，但在准确性方面比gcn有了很大的提高。自关注机制自动计算加权因子而不是静态系数，以产生更好的嵌入。在这篇文章中，</p><ul class=""><li id="b6c4" class="mq mr it lb b lc ld lf lg li ms lm mt lq mu lu mv mw mx my bi translated">我们了解了应用于GNNs的<strong class="lb iu">自我关注</strong>机制；</li><li id="8145" class="mq mr it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">我们在PyTorch Geometric中实现并<strong class="lb iu">比较了</strong>两个<strong class="lb iu"> </strong>架构(一个GCN和一个GAT)；</li><li id="b07e" class="mq mr it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">我们用一个t-SNE图和每一个学位的准确度分数形象化了GAT如何学习和学习什么；</li></ul><p id="93c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">《服务贸易总协定》是许多GNN申请中事实上的标准。然而，当应用于大规模图形数据集时，它们的<strong class="lb iu">缓慢的训练时间</strong>会成为一个问题。可扩展性是深度学习的一个重要因素:通常情况下，更多的数据可以带来更好的性能。</p><p id="f236" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下一篇文章中，我们将看到<strong class="lb iu">如何使用迷你批处理和一个名为GraphSAGE的新GNN架构来提高可伸缩性</strong>。</p><p id="8039" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你喜欢这个教程，请随时在Twitter上关注我，了解更多关于GNN的内容。谢谢大家，下一篇文章再见！📣</p><h1 id="15de" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">相关文章</h1><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/introduction-to-graphsage-in-python-a9e7f9ecf9d7"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">Python中的GraphSAGE简介</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">将图形神经网络扩展到数十亿个连接</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="pe l pf pg ph pd pi ks ou"/></div></div></a></div><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/how-to-design-the-most-powerful-graph-neural-network-3d18b07a6e66"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">如何设计最强大的图形神经网络</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">基于图同构网络的图分类</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="pj l pf pg ph pd pi ks ou"/></div></div></a></div></div></div>    
</body>
</html>