<html>
<head>
<title>Large Scale K-Means Clustering with Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降的大规模K均值聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/large-scale-k-means-clustering-with-gradient-descent-c4d6236acd7a#2022-06-23">https://towardsdatascience.com/large-scale-k-means-clustering-with-gradient-descent-c4d6236acd7a#2022-06-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fbce" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">利用顺序数据、小批量数据和批量数据进行学习</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/733dde86512a4ed97d7fed644ffed3de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6V0NU63uHP1MkqWxS2PLZg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">拉杜·切拉里乌在<a class="ae kv" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="9ebd" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">简介</strong></h1><p id="b5bc" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">聚类是一种无监督形式的机器学习算法。它发现数据中的子组或模式。K-Means算法是一种简单直观的数据聚类方法。当我们应用K-Means算法时，我们必须注意数据集的大小和维度。这些中的任何一个都可能导致算法收敛缓慢。在本文中，我们将探索梯度下降优化和降维技术，以扩大K-Means算法的大数据集。我还将分享我在流行图像数据集上的一些实验结果。</p><p id="9d0f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在梯度下降优化中，我们计算成本函数的导数，并在梯度的相反方向上迭代移动，直到我们达到最优值。这种优化可用于通过在小批量数据集上执行优化来扩展ML算法以在大数据集上训练。</p><p id="cf62" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们还可以对数据集应用降维技术，以找到更好的数据表示并加快聚类时间。在下一节中，我们将看到如何使用梯度下降学习来执行K-Means。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="849a" class="kw kx iq bd ky kz mw lb lc ld mx lf lg jw my jx li jz mz ka lk kc na kd lm ln bi translated">k均值</h1><p id="38f8" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">K-Means算法将数据集分成K个不同的聚类组。它使用一个代价函数，该函数使聚类中心和它们的指定样本之间的平方距离之和最小化。基于领域知识或通过一些其他策略来设置聚类的数量。K均值成本函数可以写成:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/a115ab0b33eaec8fadea27caa735588c.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*0jqddOsj6b3I-JSF2H_0mg.png"/></div></figure><p id="4bae" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">其中样本<strong class="lq ir"> <em class="nc"> x </em> </strong>具有维度d，N是数据集中样本的总数。聚类中心由<strong class="lq ir"> <em class="nc"> w </em> </strong>表示。标准K-Means算法在聚类中心估计和聚类中心与样本之间的距离计算之间交替进行固定次数的迭代。</p><p id="0bab" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">接下来，我们将了解如何使用梯度下降优化来执行K均值聚类。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="cd73" class="kw kx iq bd ky kz mw lb lc ld mx lf lg jw my jx li jz mz ka lk kc na kd lm ln bi translated"><strong class="ak">批量梯度下降学习</strong></h1><p id="c3dd" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">标准K-Means算法在大数据集上可能具有缓慢的收敛和存储密集型计算。我们可以用梯度下降优化来解决这个问题。对于K-均值，聚类中心更新等式被写成:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/a2c8fcdfbf8df6cb2bfed563e61bfbcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*mNq_JzmqnPS0Tjf14hQ0uA.png"/></div></figure><p id="a7aa" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">其中s(w)是欧氏空间中最接近<strong class="lq ir"> <em class="nc"> x </em> </strong>的原型。学习率被设置为样本聚类计数，即分配给特定聚类中心的样本数。在固定数量的迭代之后或者当K-均值成本的变化低于某个容限值时，我们停止这种梯度优化。在批量梯度下降中，我们用整个数据集计算梯度。对于大型数据集，计算梯度可能会很慢且效率低下。</p><p id="76cf" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">接下来，我们将研究梯度下降的两种变体来缩放K-均值聚类。</p><h1 id="892f" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">随机梯度下降</strong></h1><p id="80ea" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">使用一个样本来计算梯度的梯度下降的变体被称为随机梯度下降(SGD)。由于它只用一个样本进行聚类更新，因此可以应用于在线学习。在实践中，我们随机化数据集，一次处理一个样本。学习率在每次迭代期间针对每个聚类中心进行更新。这种方法在实践中效果很好。SGD的伪代码如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/5d451759ec896f228270692e8cb684ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a93tNmwhdYHCIPcvNX_sDw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">带SGD的K均值伪代码</p></figure><p id="e819" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了可视化K-Means SGD学习过程，我创建了一个玩具数据集，其中有两个可线性分离的圆形斑点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/11c994fb3c97643e1f4e18e78f769444.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*KXSfI1DQ6M8PC6a4-6dEwg.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">具有SGD学习的K-Means的图示</p></figure><p id="a54c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">从图中，我们看到聚类中心更新可能很慢，因为一次只处理一个样本。当有许多不同的聚类中心要学习时，这可能会变得更糟。</p><h1 id="26b7" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">小批量随机梯度下降</strong></h1><p id="c5c0" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">SGD可能找不到好的聚类中心，因为只有一个样本用于计算梯度。我们可以修改梯度更新步骤，使用M个样本的批量来计算梯度。这种变体被称为小批量SGD。这比SGD具有更快和更平滑的成本收敛。mini-batch背后的理由是，数据子集比单个样本包含更多的信息，同时也不太可能包含在整个数据集中找到的冗余示例。实际上，小批量是随机抽样的。小批量SGD的伪代码如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/ddb95ee83ab9e4ec6b87d37881a13ea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SpOs_HfyW5-za0pjA48w-A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">带小批量SGD的K均值伪代码</p></figure><p id="8b60" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">该算法类似于SGD，只是我们使用M个样本来计算每次迭代的梯度。最小批量是一个需要调整的超参数，其值会影响算法的性能。</p><p id="3517" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">下面是小批量SGD K-Means算法在同一个玩具数据集上的图解，其中小批量的大小为4。相比新币，成本收敛更快。SGD和mini-batch SGD都能找到好的聚类中心，并很好地对玩具数据集进行分组。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/e04970c724320e4eefb36efd78af6302.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*p3WrIfBbW3WDI-u8rEqyQg.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">批量大小为4的小批量SGD学习的K均值图</p></figure><p id="6449" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">接下来，我们将讨论两种流行的降维技术，以及它们如何改进和加速聚类工作流。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="2cf0" class="kw kx iq bd ky kz mw lb lc ld mx lf lg jw my jx li jz mz ka lk kc na kd lm ln bi translated">降维</h1><p id="b5dc" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">降维可以看作是一个特征选择和特征提取的步骤。原始数据被映射到低维空间，这有助于更快地学习clusters⁴.我试验了主成分分析(PCA)和稀疏随机投影(SRP)方法。</p><h2 id="1fd9" class="ng kx iq bd ky nh ni dn lc nj nk dp lg lx nl nm li mb nn no lk mf np nq lm nr bi translated">主成分分析</h2><p id="22dd" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">PCA学习最大化数据方差的嵌入矩阵。这是一种数据相关的投影，因为嵌入矩阵包含样本协方差矩阵的特征向量。Jonathan Hui写了一篇关于奇异值分解的主成分分析的综述</p><h2 id="958e" class="ng kx iq bd ky nh ni dn lc nj nk dp lg lx nl nm li mb nn no lk mf np nq lm nr bi translated">稀疏随机投影</h2><p id="22a6" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">SRP构造了一个与数据无关的嵌入矩阵。成对的距离在较低的维度中保留了一个因子。与高斯随机投影矩阵相比，SRP支持更快的数据投影，并且内存效率高。本文由<a class="ns nt ep" href="https://medium.com/u/a9bc11f7a61b?source=post_page-----c4d6236acd7a--------------------------------" rel="noopener" target="_blank"> CJ Sullivan </a> ⁶.对SRP进行了详细的解释和应用</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="c442" class="kw kx iq bd ky kz mw lb lc ld mx lf lg jw my jx li jz mz ka lk kc na kd lm ln bi translated">实验</h1><p id="f022" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">为了比较不同版本的K-Means，我对iris、USPS和Fashion-MNIST数据集进行了聚类分析。这些数据集包含不同维度的小型、中型和大型样本。对于所有三个数据集，样本分布在所有类中是一致的。下面是不同数据集的总结。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/b7fd8339e2ef550a113abc898ae8c31e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*JNHzpN9tsWhFpJn2r_8_1w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据集摘要</p></figure><p id="25b9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">随机选择的样本被初始化为所有三个K均值算法的聚类中心。对于批量梯度下降，我将iris数据集的学习率设置为0.03/t，将USPS和Fashion-MNIST数据集的学习率设置为0.1/t。对于小批量SGD，我使用32作为批量大小。对于SGD和小批量SGD，我使用集群分配计数作为学习率。所有数据集都使用最小-最大缩放器进行缩放，该缩放器在范围[0，1]内缩放每个样本。下面是用于缩放数据的公式。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/5ef45fbc0c6f0830fdc6322a6732b022.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*wEMNO-qEbpApzujA5eILzA.png"/></div></figure><p id="45b9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我还尝试了标准缩放器，它将数据转换成零均值和单位标准差。对于上述三个数据集，我没有看到收敛速度或更低的最终成本有很大的差异。</p><h2 id="2e3b" class="ng kx iq bd ky nh ni dn lc nj nk dp lg lx nl nm li mb nn no lk mf np nq lm nr bi translated">不同梯度下降算法的代价收敛性</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/37443e7ba838bb1e213fc8affd8e2a47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*efpVJoe5o5TeBbmyMxU1SQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">iris、USPS和Fashion-MNIST算法的标准化K均值成本。</p></figure><p id="6795" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">当我们在所有三个数据集上比较成本收敛时，mini-batch SGD具有更平滑和更快的收敛。对于较小的iris数据集，SGD比小批量SGD成本更低。这可能是因为32的批量可能太大了。为了了解小批量如何影响收敛，我用16、32、64和128个小批量进行了一个实验。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/0475a27bb3f906e68e2c71070752e32f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*gK9UAMb6O9njYB4rZSO7bw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">标准化不同小批量的成本趋同。</p></figure><p id="9441" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们看到小批量和大批量的最终成本差距很大。16的小批量为所有三个数据集产生最低的最终成本。</p><h2 id="f31a" class="ng kx iq bd ky nh ni dn lc nj nk dp lg lx nl nm li mb nn no lk mf np nq lm nr bi translated">基于维数的相对聚类时间缩减</h2><p id="5df8" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">时间缩减是在具有全维度的标准K-均值和具有缩减维度的小批量SGD K-均值之间的度量。我在USPS和Fashion-MNIST数据集上运行了小批量K-Means算法，这些数据集用不同维度的PCA和SRP绘制。在时间计算中不考虑降维计算步骤，而只考虑数据聚类时间。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/8f2eaea9b4c0dc1b20df09288e9c4d91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ckJUn0wta0sCdclmgmAGwA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">用小批量SGD可视化相对时间减少</p></figure><p id="c9c2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">正如所料，随着更多的组件，K-Means需要更多的时间来收敛到一个更低的成本。时间减少趋势随维度线性减少。</p><h2 id="12d7" class="ng kx iq bd ky nh ni dn lc nj nk dp lg lx nl nm li mb nn no lk mf np nq lm nr bi translated">K-均值代价与维数的相对误差</h2><p id="44e8" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">相对误差是标准和小批量SGD K均值之间的最终成本差异。标准K-Means在原始数据集上训练，小批量K-Means在不同维度上训练。以下是PCA和SRP后的相对误差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/c7d4178e6e3c182ca29c7f6fb4946278.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jkAYr75cW3_5Bfl0t9knQQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">可视化标准K均值和小批量SGD之间的相对成本差异</p></figure><p id="22f9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">当原始维度非常高时，随机投影会受益。与SRP相比，PCA的相对成本差异随着更多组件而降低(在一定程度上)。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h2 id="bd32" class="ng kx iq bd ky nh ni dn lc nj nk dp lg lx nl nm li mb nn no lk mf np nq lm nr bi translated">代码片段</h2><p id="213c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们可以使用scikit-learn API为集群工作流创建一个简单的管道。下面是一个带有PCA和小批量K-Means的样本管道。</p><pre class="kg kh ki kj gt nz oa ob oc aw od bi"><span id="206c" class="ng kx iq oa b gy oe of l og oh"><strong class="oa ir">from</strong> <strong class="oa ir">sklearn.pipeline</strong> <strong class="oa ir">import</strong> <a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" rel="noopener ugc nofollow" target="_blank">Pipeline</a><br/><strong class="oa ir">from sklearn.preprocessing import </strong><a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html" rel="noopener ugc nofollow" target="_blank">MinMaxScaler</a><br/><strong class="oa ir">from</strong> <strong class="oa ir">sklearn.decomposition</strong> <strong class="oa ir">import</strong> <a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" rel="noopener ugc nofollow" target="_blank">PCA</a><br/><strong class="oa ir">from sklearn.cluster import </strong><a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans" rel="noopener ugc nofollow" target="_blank">MiniBatchKMeans</a><br/><strong class="oa ir">from sklearn.datasets import </strong><a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html" rel="noopener ugc nofollow" target="_blank">fetch_openml</a><br/></span><span id="31df" class="ng kx iq oa b gy oi of l og oh"><strong class="oa ir"># load dataset</strong><br/>X, y = fetch_openml('USPS', return_X_y=True, as_frame=False)</span><span id="69f9" class="ng kx iq oa b gy oi of l og oh"><strong class="oa ir"># add methods to pipeline</strong><br/>steps = [<br/>("scale_step", MinMaxScaler()),<br/>("dimred_step", PCA(n_components=50)), <br/>("cluster_step", MiniBatchKMeans(n_clusters=10, batch_size=16))<br/>]</span><span id="d91c" class="ng kx iq oa b gy oi of l og oh"><strong class="oa ir"># create pipeline</strong><br/>clusterpipe = Pipeline(steps)</span><span id="3717" class="ng kx iq oa b gy oi of l og oh"><strong class="oa ir"># fit pipeline on data</strong><br/>clusterpipe.fit(X)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/da0f5a2568c278bf40a0b83d456a7637.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*tK-aSKAZZ8W0Pl7hUXX8UQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">具有scikit-learn API的简单集群管道</p></figure></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="cdb1" class="kw kx iq bd ky kz mw lb lc ld mx lf lg jw my jx li jz mz ka lk kc na kd lm ln bi translated">结论</h1><p id="0294" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">两个SGD版本都可以很好地处理大规模K-Means聚类。在中型和大型数据集上，与SGD相比，mini-batch SGD收敛速度更快，发现的聚类中心更好。小批量大小是一个基于数据集大小进行调整和设置的超参数。聚类受益于降维方法，降维方法是内存高效的并且具有更快的收敛。当我们设计机器学习管道时，速度和低成本的权衡曲线可视化是有帮助的。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="75a1" class="kw kx iq bd ky kz mw lb lc ld mx lf lg jw my jx li jz mz ka lk kc na kd lm ln bi translated"><strong class="ak">参考文献</strong></h1><p id="49d7" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">[1]斯卡利博士(2010年4月)。网络规模的k均值聚类。在<em class="nc">第19届万维网国际会议记录</em>(第1177-1178页)。</p><p id="c4d0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[2]博图和本吉奥(1994年)。k-means算法的收敛性质。<em class="nc">神经信息处理系统的进展</em>、<em class="nc"> 7 </em>。</p><p id="a26c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[3]博图，L. (2012年)。随机梯度下降技巧。在<em class="nc">神经网络:交易技巧</em>(第421–436页)。斯普林格，柏林，海德堡。</p><p id="262a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[4] Boutsidis，c .，Zouzias，a .，和Drineas，P. (2010年)。随机投影为<em class="nc">k</em>-均值聚类。<em class="nc">神经信息处理系统的进展</em>、<em class="nc"> 23 </em>。</p><p id="327f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[5]乔纳森·惠。(2019).<a class="ae kv" href="https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491" rel="noopener">机器学习—奇异值分解&amp;主成分分析</a>。<a class="ae kv" href="https://medium.com/towards-data-science" rel="noopener">走向数据科学</a></p><p id="1624" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[6]希杰·沙利文。(2021).<a class="ae kv" rel="noopener" target="_blank" href="/behind-the-scenes-on-the-fast-random-projection-algorithm-for-generating-graph-embeddings-efb1db0895">幕后讲述生成图嵌入的快速随机投影算法</a>。<a class="ae kv" href="https://medium.com/towards-data-science" rel="noopener">走向数据科学</a></p><p id="bae1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="nc">所有图片，除非特别注明，均为作者所有。</em></p></div></div>    
</body>
</html>