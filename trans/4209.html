<html>
<head>
<title>What is Interpretable Machine Learning?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是可解释的机器学习？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-interpretable-machine-learning-2d217b62185a#2022-09-19">https://towardsdatascience.com/what-is-interpretable-machine-learning-2d217b62185a#2022-09-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="34e7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">IML介绍——旨在使机器学习模型为人类所理解的领域</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a7734e4decec219499b39cbb37236cae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ihB0RZ-_2sslKD7G7KnX1w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(用<a class="ae ky" href="https://www.craiyon.com/" rel="noopener ugc nofollow" target="_blank"> DALLE Mini </a>创建)</p></figure><p id="b034" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们应该永远相信一个表现良好的模型吗？</p><p id="3413" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个模特可能会拒绝你的抵押贷款申请或者诊断出你患有癌症。这些决定是有后果的。后果严重。即使他们是正确的，我们也会<strong class="lb iu">期待一个解释</strong>。</p><p id="db9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">人类可以给一个。人类能够告诉你，你的收入太低，或者一群细胞是恶性的。为了从一个模型中得到类似的解释，我们把目光投向了<strong class="lb iu">的可解释机器学习领域</strong>。</p><p id="b097" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们探索这个领域，了解它的目标是什么。我们讨论提供解释的两种主要方法:</p><ul class=""><li id="8e24" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">本质上可解释的模型</li><li id="d2d0" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">模型不可知的方法</li></ul><p id="6a0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还讨论不太突出的方法:</p><ul class=""><li id="d512" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">因果模型</li><li id="7d61" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">反事实的解释</li><li id="6351" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">对立的例子</li><li id="496a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">非不可知方法</li></ul><p id="797a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们谈谈从技术解释到人性化解释需要做些什么。</p><h1 id="4c31" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">什么是IML？</h1><p id="8148" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">可解释的机器学习是一个研究领域。它旨在建立人类可以理解的机器学习模型。这包括开发:</p><ul class=""><li id="6514" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">解释黑盒模型的方法</li><li id="ee67" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">建模方法用于构建易于解释的模型</li></ul><p id="1c4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">向不太懂技术的观众解释机器学习的方法也属于IML的领域。</p><blockquote class="ng"><p id="cd39" class="nh ni it bd nj nk nl nm nn no np lu dk translated">IML的目标是建立人类能够理解的模型</p></blockquote><p id="b407" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">理解一个模型意味着理解个体预测是如何做出的。我们称这些为<strong class="lb iu">本地解释</strong>。有了这些，我们想知道每个单独的模型特征是如何对预测做出贡献的。</p><p id="66d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这也意味着理解一个模型作为一个整体是如何工作的。也就是理解模型用来做预测的趋势。我们称这些为<strong class="lb iu">全球诠释</strong>。</p><h1 id="4b7b" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">本质上可解释的模型</h1><p id="46ee" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">第一种方法是构建<strong class="lb iu">内在可解释的</strong>模型。这些是人类能够理解的简单模型。这不需要额外的方法。我们只需要查看模型的参数或模型摘要。这些将告诉我们预测是如何做出的，模型捕捉到了什么趋势。</p><blockquote class="ng"><p id="2dda" class="nh ni it bd nj nk nl nm nn no np lu dk translated">内在可解释=无需额外方法即可理解</p></blockquote><p id="299a" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">例如，我们在<strong class="lb iu">图1 </strong>中有一个决策树。它被训练来预测某人是否会在汽车贷款上违约(是)或不违约(否)。这是一个内在可解释的模型。</p><p id="b7d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了理解为什么会做出某种预测，我们可以沿着树往下看。观察不同的划分，我们可以看到模型捕捉到的趋势——较低的年龄、较低的收入和学生身份都与较高的违约风险相关。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/bc28a338001ddb4ba8af63194a7c8c88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pTz_6jtfBoUODVAlaF3rFA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:决策树的例子(来源:作者)</p></figure><p id="33b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其他例子有<strong class="lb iu">线性和逻辑回归</strong>。为了理解这些模型是如何工作的，我们可以看看赋予每个模型特征的参数值。参数*特征值给出了该特征对预测的贡献。参数的符号和大小告诉我们特征和目标变量之间的关系。</p><p id="3367" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用这些模型会让我们远离机器学习。我们转向一种更加统计的思维模式来构建模型。建立一个内在可解释的模型需要更多的思考。我们需要在功能工程上投入更多的时间，选择一小组<a class="ae ky" rel="noopener" target="_blank" href="/characteristics-of-a-good-feature-4f1ac7a90a42">不相关的功能</a>。好处是有一个易于解释的简单模型。</p><p id="e328" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有些问题不能用简单的模型来解决。对于像图像识别这样的任务，我们转向更难解释的或黑盒模型。我们可以在<strong class="lb iu">图2 </strong>中看到一些这样的例子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/26f3cb7e7d0e91bcd3300683ec877b8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gSwtFS4Qt3vHpFqbwNv_Wg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:本质上可解释的模型与黑盒模型(来源:作者)</p></figure><p id="02a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">黑盒模型太复杂，人类无法直接理解。为了理解随机森林，我们需要同时理解所有的决策树。类似地，神经网络会有太多的参数，无法一次理解。我们需要额外的方法来窥探黑盒。</p><h1 id="599f" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">模型不可知的方法</h1><p id="806d" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">这就把我们带到了<a class="ae ky" rel="noopener" target="_blank" href="/what-are-model-agnostic-methods-387b0e8441ef">模型不可知的</a>方法。这些方法包括PDP、ICE图、ALE图、SHAP、LIME和Friedman的h-statistic。这些方法可以解释任何模型。该算法实际上被视为一个黑盒，可以替换为任何其他模型。</p><blockquote class="ng"><p id="ca44" class="nh ni it bd nj nk nl nm nn no np lu dk translated">使用代理模型和置换，模型不可知的方法可以解释任何模型</p></blockquote><p id="2468" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">一种方法是使用代理模型。 这些方法从使用原始模型开始进行预测。然后，我们根据这些预测训练另一个模型(即代理模型)。也就是说，我们使用原始模型的预测，而不是目标变量。通过这种方式，代理模型了解了用于进行预测的原始模型的特征。</p><p id="f54b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">重要的是代理模型本质上是可解释的。比如我们上面讨论的模型之一。这允许我们通过直接查看代理模型来解释原始模型。</p><p id="094c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一种方法是使用<strong class="lb iu">排列</strong>。这涉及到更改/置换模型特征。我们使用该模型对这些置换特征进行预测。然后我们可以理解特征值的变化如何导致预测的变化。</p><p id="716b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">排列方法的一个好例子是<a class="ae ky" rel="noopener" target="_blank" href="/the-ultimate-guide-to-pdps-and-ice-plots-4182885662aa">PDP和ICE Plots </a>。你可以在<strong class="lb iu">图3 </strong>中看到其中一个。具体来说，<strong class="lb iu">冰图</strong>由所有单独的线给出。在我们的数据集中，每个观察值都有一行。为了创建每条线，我们置换一个特征的值并记录结果预测。我们这样做的同时保持其他特性的值不变。粗黄线是<strong class="lb iu"> PDP </strong>。这是所有单独线条的平均值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/abd358b7441ffc606308b87b0af4b8eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gykLuefqIRVQe9459IBRMQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3: PDP和ICE图示例(来源:作者)</p></figure><p id="98a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，平均而言，预测随着特征而降低。观察冰图，有些观察结果并不符合这一趋势。这表明我们的数据中存在潜在的相互作用。</p><p id="935a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PDP和ICE图是全球解释方法的一个例子。我们可以用它们来理解模型捕捉到的趋势。它们不能用来理解个人预测是如何做出的。</p><p id="32a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">沙普利值可以。如<strong class="lb iu">图4 </strong>所示，每个模型特征都有一个值。它们告诉我们，当与平均预测<strong class="lb iu">E[f(x)】</strong>相比时，每个特征如何对预测<strong class="lb iu"> f(x) </strong>做出贡献。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/947579dc4be69e258af4cf759847d309.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pjv1tf23gv7TJj5BPusPCQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图Shapley值的例子(来源:作者)</p></figure><p id="ea6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">过去，Shapley值是用排列来近似的。最近一种叫做<a class="ae ky" rel="noopener" target="_blank" href="/introduction-to-shap-with-python-d27edc23c454"> SHAP </a>的方法显著提高了这些近似的速度。它使用排列和代理模型的组合。单个观察值的特征值被置换。然后根据这些值训练线性回归模型。该模型的权重给出了近似的Shapley值。</p><p id="ae3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常，这种方法被称为<strong class="lb iu">本地代理模型</strong>。我们根据个别预测而不是所有预测的排列来训练代理模型。石灰是另一种使用这种方法的方法。</p><h1 id="0d10" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">其他IML方法</h1><p id="489e" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">内在可解释模型和模型不可知方法是IML的主要方法。其他一些方法包括<strong class="lb iu">因果模型</strong>、<strong class="lb iu">反事实</strong>、<strong class="lb iu">解释</strong>和<strong class="lb iu">对立例子</strong>。事实上，任何旨在理解模型如何做出预测的方法都属于IML的范畴。已经为特定模型开发了许多方法。我们称这些<strong class="lb iu">为非不可知方法</strong>。</p><blockquote class="ng"><p id="e5f2" class="nh ni it bd nj nk nl nm nn no np lu dk translated">IML包括任何用于理解模型如何做出预测的方法</p></blockquote><h2 id="97f9" class="oa mk it bd ml ob oc dn mp od oe dp mt li of og mv lm oh oi mx lq oj ok mz ol bi translated">因果模型</h2><p id="5a5a" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">机器学习只关心相关性。一个模型可以使用原产国来预测患皮肤癌的几率。然而，真正的原因是每个国家不同程度的日照。我们称原产地为日照量的代表。</p><p id="354d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在构建因果模型时，我们的目标是只使用因果关系。我们不想包含任何代表真正原因的模型特征。要做到这一点，我们需要依赖领域知识，并在特征选择上投入更多的精力。</p><p id="0cc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">建立因果模型并不意味着模型更容易解释。这意味着任何解释都将忠实于现实。特征对预测的贡献将接近事件的真正原因。你给出的任何解释也会更有说服力。</p><p id="b4f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你为什么诊断我得了皮肤癌？</p><p id="dd3b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“因为你来自南非”，并不是一个令人信服的理由。</p><h2 id="35d6" class="oa mk it bd ml ob om dn mp od on dp mt li oo og mv lm op oi mx lq oq ok mz ol bi translated"><strong class="ak">反事实解释</strong></h2><p id="759b" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">反事实的解释可以被认为是一种排列方法。它们依赖于特征值的排列。重要的是，他们专注于寻找改变预测的特征值。例如，我们想知道从阴性诊断到阳性诊断需要什么。</p><p id="a12c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更具体地说，反事实解释是我们为了改变预测而需要对特征值做出的最小改变。对于连续目标变量，变化将是预定义的百分比或数量。</p><p id="71e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">反事实解释对于回答相反的问题很有用。客户将他们的当前位置与潜在的未来位置进行比较的问题。例如，在贷款被拒绝后，他们可以问:</p><p id="8f3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“我怎么才能被接受？”</p><p id="80b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于反事实的解释，我们可以回答:</p><p id="4f16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“你需要增加200美元的月收入”或者“你需要减少10000美元的现有债务”。</p><h2 id="d6b8" class="oa mk it bd ml ob om dn mp od on dp mt li oo og mv lm op oi mx lq oq ok mz ol bi translated"><strong class="ak">对抗性例子</strong></h2><p id="73e6" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">对立的例子是导致非直觉预测的观察。如果人类看了这些数据，他们会做出不同的预测。</p><p id="4b24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">寻找对立的例子类似于反事实的解释。不同的是，我们想要改变特征值来有意欺骗模型。我们仍在试图理解这个模型是如何工作的，但不是提供解释。我们希望找到模型中的弱点，并避免<a class="ae ky" rel="noopener" target="_blank" href="/what-is-adversarial-machine-learning-dbe7110433d6">对抗性攻击</a>。</p><p id="5fbe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对立的例子在像图像识别这样的应用中很常见。有可能创建对人类来说看起来完全正常的图像，但是导致不正确的预测。</p><p id="a21a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，谷歌的研究人员展示了引入一层噪声如何改变图像识别模型的预测。看着<strong class="lb iu">图5 </strong>，你可以看到，对于人类来说，这层噪声甚至是不可察觉的。然而，这个模型现在预测熊猫是一只长臂猿。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/dbc6a5211cfcb32c596704f3a44351aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6Nv_GFr2D5_z8xFX.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5:对抗的例子(来源:<a class="ae ky" href="https://arxiv.org/pdf/1412.6572.pdf" rel="noopener ugc nofollow" target="_blank"> I. Goodfellow等。艾尔。</a>)</p></figure><h2 id="da4a" class="oa mk it bd ml ob om dn mp od on dp mt li oo og mv lm op oi mx lq oq ok mz ol bi translated"><strong class="ak">非不可知方法</strong></h2><p id="5a00" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">已经为特定的黑盒模型开发了许多方法。对于基于树的方法，我们可以计算每个特征的分裂次数。对于神经网络，我们有像像素分解和<a class="ae ky" href="https://github.com/kundajelab/deeplift" rel="noopener ugc nofollow" target="_blank">深度提升</a>这样的方法。</p><p id="0793" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管SHAP被认为是模型不可知的，但它也有非不可知的近似方法。例如，TreeSHAP只能用于基于树的方法，而DeepSHAP只能用于神经网络。</p><p id="623b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">明显的缺点是非不可知方法只能用于特定的模型。这就是为什么研究一直指向不可知论的方法。这些为我们选择算法提供了更大的灵活性。这也意味着我们的解释方法是经得起未来考验的。它们可以用来解释尚未开发的算法。</p><h1 id="d96e" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">从解释到说明</h1><p id="67cd" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我们讨论的方法都是技术性的。数据科学家使用它们向其他数据科学家解释模型。实际上，我们将被期望向非技术观众解释我们的模型。这包括同事、监管者或客户。为此，我们需要弥合技术解释和人性化解释之间的差距。</p><p id="dd47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您需要:</p><ul class=""><li id="0d1e" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">根据观众的专业知识调整级别</li><li id="315e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">仔细考虑要解释哪些特征</li></ul><p id="32ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个好的解释不一定需要解释所有特征的贡献。</p><p id="b094" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将在下面的文章中更深入地讨论这个过程。作为一个例子，我们走过如何使用SHAP特征贡献来给出一个令人信服的解释。</p><div class="os ot gp gr ou ov"><a rel="noopener follow" target="_blank" href="/the-art-of-explaining-predictions-22e3584ed7d8"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd iu gy z fp pa fr fs pb fu fw is bi translated">解释预言的艺术</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">如何以人性化的方式解释你的模型</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">towardsdatascience.com</p></div></div><div class="pe l"><div class="pf l pg ph pi pe pj ks ov"/></div></div></a></div><p id="5754" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">IML是一个令人兴奋的领域。如果您想了解更多，请查看下面的教程:</p><div class="os ot gp gr ou ov"><a rel="noopener follow" target="_blank" href="/introduction-to-shap-with-python-d27edc23c454"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd iu gy z fp pa fr fs pb fu fw is bi translated">Python SHAP简介</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">如何创造和解释SHAP情节:瀑布，力量，决定，SHAP和蜂群</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">towardsdatascience.com</p></div></div><div class="pe l"><div class="pk l pg ph pi pe pj ks ov"/></div></div></a></div><div class="os ot gp gr ou ov"><a rel="noopener follow" target="_blank" href="/the-ultimate-guide-to-pdps-and-ice-plots-4182885662aa"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd iu gy z fp pa fr fs pb fu fw is bi translated">PDP和ICE绘图的最终指南</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">部分依赖图和个人条件期望背后的直觉、数学和代码(R和Python)</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">towardsdatascience.com</p></div></div><div class="pe l"><div class="pl l pg ph pi pe pj ks ov"/></div></div></a></div></div><div class="ab cl pm pn hx po" role="separator"><span class="pp bw bk pq pr ps"/><span class="pp bw bk pq pr ps"/><span class="pp bw bk pq pr"/></div><div class="im in io ip iq"><p id="5f88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望这篇文章对你有帮助！如果你想看更多，你可以成为我的<a class="ae ky" href="https://conorosullyds.medium.com/membership" rel="noopener"> <strong class="lb iu">推荐会员</strong> </a> <strong class="lb iu">来支持我。你可以访问medium上的所有文章，我可以得到你的部分费用。</strong></p><div class="os ot gp gr ou ov"><a href="https://conorosullyds.medium.com/membership" rel="noopener follow" target="_blank"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd iu gy z fp pa fr fs pb fu fw is bi translated">通过我的推荐链接加入Medium康纳·奥沙利文</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">conorosullyds.medium.com</p></div></div><div class="pe l"><div class="pt l pg ph pi pe pj ks ov"/></div></div></a></div><p id="7356" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在|<a class="ae ky" href="https://twitter.com/conorosullyDS" rel="noopener ugc nofollow" target="_blank">Twitter</a>|<a class="ae ky" href="https://www.youtube.com/channel/UChsoWqJbEjBwrn00Zvghi4w" rel="noopener ugc nofollow" target="_blank">YouTube</a>|<a class="ae ky" href="https://mailchi.mp/aa82a5ce1dc0/signup" rel="noopener ugc nofollow" target="_blank">时事通讯</a>上找到我——注册免费参加<a class="ae ky" href="https://adataodyssey.com/courses/shap-with-python/" rel="noopener ugc nofollow" target="_blank"> Python SHAP课程</a></p><h2 id="1378" class="oa mk it bd ml ob om dn mp od on dp mt li oo og mv lm op oi mx lq oq ok mz ol bi translated">图像来源</h2><p id="4c46" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">所有图片都是我自己的或从<a class="ae ky" href="http://www.flaticon.com/" rel="noopener ugc nofollow" target="_blank">www.flaticon.com</a>获得的。在后者的情况下，我拥有他们的<a class="ae ky" href="https://support.flaticon.com/hc/en-us/articles/202798201-What-are-Flaticon-Premium-licenses-" rel="noopener ugc nofollow" target="_blank">保费计划</a>中定义的“完全许可”。</p><h2 id="b905" class="oa mk it bd ml ob om dn mp od on dp mt li oo og mv lm op oi mx lq oq ok mz ol bi translated">参考</h2><p id="4958" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">C.莫尔纳尔、<strong class="lb iu">可解释机器学习</strong> <em class="nx"> </em> (2021)、<a class="ae ky" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">https://christophm.github.io/interpretable-ml-book/</a></p><p id="b3a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">南Masís，<strong class="lb iu">用Python进行可解释的机器学习</strong> (2021)</p><p id="3af0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">莫拉法、卡拉米、郭、拉格林和刘，2020年。<strong class="lb iu">机器学习的因果可解释性——问题、方法和评估</strong>。<a class="ae ky" href="https://arxiv.org/abs/2003.03934" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2003.03934</a></p><p id="434b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">微软，<strong class="lb iu">因果关系与机器学习</strong>，<a class="ae ky" href="https://www.microsoft.com/en-us/research/group/causal-inference/" rel="noopener ugc nofollow" target="_blank">https://www . Microsoft . com/en-us/research/group/causal-inference/</a></p></div></div>    
</body>
</html>