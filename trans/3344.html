<html>
<head>
<title>How to Handle Large Datasets in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在Python中处理大型数据集</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-handle-large-datasets-in-python-1f077a7e7ecf#2022-07-26">https://towardsdatascience.com/how-to-handle-large-datasets-in-python-1f077a7e7ecf#2022-07-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9d0f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">CSV、Pickle、Parquet、Feather和HDF5的比较</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7c589c4361143d657da2f5c2de094976.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QiyUi2kF31rFooEoAT34Ow.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者提供。</p></figure><p id="3210" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">经过这么长时间，当Kaggle最终推出<a class="ae lu" href="https://www.kaggle.com/competitions/amex-default-prediction" rel="noopener ugc nofollow" target="_blank">一个新的表格数据竞赛</a>时，起初，每个人都很兴奋。直到他们不在了。当Kagglers发现数据集有50 GB大时，社区开始讨论如何处理这样大的数据集[4]。</p><blockquote class="lv"><p id="6242" class="lw lx it bd ly lz ma mb mc md me lt dk translated">CSV文件格式需要很长时间来读写大型数据集，并且除非明确告知，否则不会记住数据类型。</p></blockquote><p id="b830" class="pw-post-body-paragraph ky kz it la b lb mf ju ld le mg jx lg lh mh lj lk ll mi ln lo lp mj lr ls lt im bi translated">除了通过减少数据类型来减少所需的磁盘空间，问题是在工作会话之间以何种格式保存修改的数据集[4]。CSV文件格式需要很长时间来读写大型数据集，并且除非明确告知，否则不会记住列的数据类型。<strong class="la iu">本文探讨了处理大型数据集的CSV文件格式的四种替代方案:Pickle、Feather、Parquet和HDF5。此外，我们将看看这些压缩文件格式。</strong></p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="fe69" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本文探索了pandas库的可选文件格式。现在，你可能会想<strong class="la iu">“在处理大型数据集时，为什么还要使用熊猫呢？”这是一个合理的问题。虽然像Datatable这样的pandas的替代品在读取和写入CSV文件时会更快，但pandas的优势在于它为数据处理提供了很大的灵活性。此外，pandas支持开箱即用地读写许多文件格式。同样，像Datatables这样的替代方法只支持CSV、Jay、XLSX和纯文本格式[3]。</strong></p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="63ad" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以在我的<a class="ae lu" href="https://www.kaggle.com/iamleonie/how-to-handle-large-datasets-in-python" rel="noopener ugc nofollow" target="_blank"> Kaggle笔记本</a>里找到代码。</p><h1 id="33f0" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">基准设置</h1><p id="b58c" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">出于基准测试的目的，我们将创建一个虚构的数据集。这个虚构的数据集包含每种数据类型的一列，但有以下例外:本例中省略了数据类型为<code class="fe no np nq nr b">float16</code>和<code class="fe no np nq nr b">categorical</code>的列，因为parquet不支持<code class="fe no np nq nr b">float16</code>，而带有<code class="fe no np nq nr b">format = "table"</code>的HDF5不支持<code class="fe no np nq nr b">categorical</code>。为了减少时间噪声以提高可比性，这个虚构的数据集包含10，000，000行，并且<strong class="la iu">几乎有1GB </strong> <strong class="la iu">大</strong>如[8]所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/bf9a768e7dea46b0b4772dfda1ac05ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M75Wda7LLXV-5FOuoPCGIw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用于基准测试的虚构数据集的头(图片由作者通过<a class="ae lu" href="https://www.kaggle.com/iamleonie/how-to-handle-large-datasets-in-python" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>提供)</p></figure><p id="49fa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据的特征会影响读写时间，例如数据类型、数据帧的宽度(列数)与长度(行数)。但是，这超出了本文的范围。对于进一步的阅读，我推荐以下资源:</p><div class="nt nu gp gr nv nw"><a rel="noopener follow" target="_blank" href="/the-best-format-to-save-pandas-data-414dca023e0d"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd iu gy z fp ob fr fs oc fu fw is bi translated">保存熊猫数据的最佳格式</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">将pandas数据帧序列化到持久存储的各种方法的比较</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="og l oh oi oj of ok ks nw"/></div></div></a></div><div class="nt nu gp gr nv nw"><a href="https://www.architecture-performance.fr/ap_blog/loading-data-into-a-pandas-dataframe-a-performance-study/" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd iu gy z fp ob fr fs oc fu fw is bi translated">将数据加载到Pandas数据框架中——性能研究</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">因为做机器学习意味着用不同的参数尝试许多选项和算法，从数据清理…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">www . architect-performance . fr</p></div></div><div class="of l"><div class="ol l oh oi oj of ok ks nw"/></div></div></a></div><h1 id="a435" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">文件格式概述</h1><p id="e704" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">在这一节中，我们将简要介绍每种文件格式的一些关键特征:简短描述、文件扩展名、使用的压缩和pandas读写方法。</p><h2 id="b16d" class="om ms it bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated"><em class="oy">逗号分隔值</em> (CSV)</h2><p id="419d" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">使用逗号分隔值的文本文件。文件扩展名为<code class="fe no np nq nr b">.csv</code>。</p><p id="b4dc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将使用gzip压缩。</p><pre class="kj kk kl km gt oz nr pa pb aw pc bi"><span id="2c47" class="om ms it nr b gy pd pe l pf pg"># Reading<br/>df = pd.read_csv(file_name, <br/>                 dtype = {...})</span><span id="2c8f" class="om ms it nr b gy ph pe l pf pg"># Writing<br/>df.to_csv(file_name, <br/>          index = False,<br/>          compression = ...) # None or "gzip" </span></pre><h2 id="f928" class="om ms it bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated">泡菜</h2><blockquote class="pi pj pk"><p id="7322" class="ky kz pl la b lb lc ju ld le lf jx lg pm li lj lk pn lm ln lo po lq lr ls lt im bi translated"><code class="fe no np nq nr b"><a class="ae lu" href="https://docs.python.org/3/library/pickle.html#module-pickle" rel="noopener ugc nofollow" target="_blank">pickle</a></code>模块实现了用于序列化和反序列化Python对象结构的二进制协议。[7]</p></blockquote><p id="6e63" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">文件扩展名是<code class="fe no np nq nr b">.pkl</code>。</p><p id="c009" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将使用gzip压缩。</p><pre class="kj kk kl km gt oz nr pa pb aw pc bi"><span id="8303" class="om ms it nr b gy pd pe l pf pg"># Reading<br/>df = pd.<!-- -->read_pickle(<!-- -->file_name<!-- -->)</span><span id="efe5" class="om ms it nr b gy ph pe l pf pg"># Writing<br/>df.to_pickle(file_name, <br/>             compression = ...) # None or "gzip"</span></pre><h2 id="786f" class="om ms it bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated">镶木地板</h2><blockquote class="pi pj pk"><p id="f663" class="ky kz pl la b lb lc ju ld le lf jx lg pm li lj lk pn lm ln lo po lq lr ls lt im bi translated">Apache Parquet是一种列存储格式，适用于Hadoop生态系统中的任何项目，无论选择的是数据处理框架、数据模型还是编程语言。[2]</p></blockquote><p id="cba7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">文件扩展名为<code class="fe no np nq nr b">.parquet</code>。</p><p id="4dec" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将使用<a class="ae lu" href="https://arrow.apache.org/docs/python/" rel="noopener ugc nofollow" target="_blank"> pyarrow </a>引擎和gzip压缩。</p><pre class="kj kk kl km gt oz nr pa pb aw pc bi"><span id="0e20" class="om ms it nr b gy pd pe l pf pg"># Reading<br/>df = pd.<!-- -->read_parquet(<!-- -->file_name<!-- -->)</span><span id="39ee" class="om ms it nr b gy ph pe l pf pg"># Writing<br/>df.<!-- -->to_parquet(<!-- -->file_name<!-- -->, <br/>              <!-- -->engine = "pyarrow", <br/>              compression = ...) # None or "gzip"</span></pre><h2 id="60f0" class="om ms it bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated">羽毛</h2><blockquote class="pi pj pk"><p id="51f4" class="ky kz pl la b lb lc ju ld le lf jx lg pm li lj lk pn lm ln lo po lq lr ls lt im bi translated">Feather是一种用于存储箭头表或数据框(来自Python或R等语言)的可移植文件格式，它在内部利用了<a class="ae lu" href="https://arrow.apache.org/docs/python/ipc.html#ipc" rel="noopener ugc nofollow" target="_blank">箭头IPC格式</a>。Feather是在Arrow项目早期创建的，作为Python (pandas)和r[1]的快速、语言无关的数据帧存储的概念证明</p></blockquote><p id="5a07" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">文件扩展名是<code class="fe no np nq nr b">.feather</code>。</p><p id="d02b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于gzip压缩不适用于feather格式，我们将使用zstd压缩。</p><pre class="kj kk kl km gt oz nr pa pb aw pc bi"><span id="7bf6" class="om ms it nr b gy pd pe l pf pg"># Reading<br/>df = pd.read_feather(file_name)</span><span id="5673" class="om ms it nr b gy ph pe l pf pg"># Writing<br/>df.to_feather(file_name, <br/>              compression = ...) # None or "zstd"</span></pre><h2 id="681a" class="om ms it bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated">分层数据格式(HDF5)</h2><blockquote class="pi pj pk"><p id="fbd4" class="ky kz pl la b lb lc ju ld le lf jx lg pm li lj lk pn lm ln lo po lq lr ls lt im bi translated">HDF5是一种用于存储和管理数据的数据模型、库和文件格式。它支持无限多种数据类型，专为灵活高效的I/O以及大量复杂数据而设计。HDF5具有可移植性和可扩展性，允许应用在使用HDF5的过程中不断发展。[5]</p></blockquote><p id="fa7e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">文件扩展名为<code class="fe no np nq nr b">.h5</code>。</p><p id="022f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">HDF5有两种格式选项:</p><ul class=""><li id="3170" class="pp pq it la b lb lc le lf lh pr ll ps lp pt lt pu pv pw px bi translated"><code class="fe no np nq nr b">"fixed"</code>，哪个写字快【6】</li><li id="1956" class="pp pq it la b lb py le pz lh qa ll qb lp qc lt pu pv pw px bi translated"><code class="fe no np nq nr b">"table"</code>，速度较慢，但提供了“灵活的操作，如搜索/选择数据子集”[6]</li></ul><p id="79ed" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了读写HDF5文件，你需要安装<code class="fe no np nq nr b">tables</code>。</p><p id="b943" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于gzip压缩不适用于feather格式，我们将使用zlib压缩。</p><pre class="kj kk kl km gt oz nr pa pb aw pc bi"><span id="4f8b" class="om ms it nr b gy pd pe l pf pg"># Reading<br/>df = pd.<!-- -->read_hdf(<!-- -->file_name<!-- -->)</span><span id="945b" class="om ms it nr b gy ph pe l pf pg"># Writing<br/>df.to_hdf(file_name, <br/>          key = "df", <br/>          format = ..., # "fixed" or "table"<br/>          complib = ..., # None or "zlib"<br/>          complevel = 9)</span></pre><h1 id="6076" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">比较</h1><p id="d8b9" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">本节根据写入时间、文件大小、读取时间和各种特征(如可读性、一致性、预期存储时间和可移植性以及在小型数据集上的性能)来比较这五种文件格式。</p><h2 id="c0f3" class="om ms it bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated">写作时代</h2><p id="6b10" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">下面，您可以看到为每种文件格式编写文件所需的时间。实心黑条表示未压缩文件的写入时间，而散列条表示压缩文件的写入时间。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qd"><img src="../Images/33ab8f9f799252dd0aed50e726c7dee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*h6eQTkUQWfcATx1i.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">不同文件格式的写入时间比较(图片由作者通过<a class="ae lu" href="https://www.kaggle.com/iamleonie/how-to-handle-large-datasets-in-python" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>提供)</p></figure><p id="5ac4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，让我们解决房间里的大象:<strong class="la iu">压缩增加了任何文件格式的写入时间</strong>。但这并不奇怪，因为数据压缩是写入过程中的一项额外任务。</p><p id="fa09" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，我们可以进行以下观察:</p><ul class=""><li id="25d0" class="pp pq it la b lb lc le lf lh pr ll ps lp pt lt pu pv pw px bi translated">正如所料，对于<strong class="la iu"> CSV，写入时间最长</strong></li><li id="cea2" class="pp pq it la b lb py le pz lh qa ll qb lp qc lt pu pv pw px bi translated"><strong class="la iu">羽毛和拼花有最快的</strong> <strong class="la iu">未压缩的</strong>写入时间</li><li id="02b0" class="pp pq it la b lb py le pz lh qa ll qb lp qc lt pu pv pw px bi translated"><strong class="la iu"> Feather对于未压缩和压缩的</strong>文件都有最快的写入时间</li><li id="286f" class="pp pq it la b lb py le pz lh qa ll qb lp qc lt pu pv pw px bi translated">不出所料，<strong class="la iu"> HDF5搭配</strong> <code class="fe no np nq nr b"><strong class="la iu">format = "fixed"</strong></code> <strong class="la iu">比</strong> <code class="fe no np nq nr b"><strong class="la iu">format = "table"</strong></code>快，但搭配压缩HDF5搭配<code class="fe no np nq nr b">format = "fixed"</code>与<code class="fe no np nq nr b">format = "table"</code>差不多</li></ul><h2 id="4792" class="om ms it bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated">文件大小</h2><p id="f87b" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">下面，您可以看到每种文件格式的结果文件大小。实心黑条表示未压缩文件的文件大小，而散列条表示压缩文件的文件大小。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qe"><img src="../Images/13863e58f0c7c8065b848fbe15d7db1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vZdxzCjar11iAxev.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">不同文件格式的文件大小比较(图片由作者通过<a class="ae lu" href="https://www.kaggle.com/iamleonie/how-to-handle-large-datasets-in-python" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>提供)</p></figure><p id="733a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以做出如下观察:</p><ul class=""><li id="4a70" class="pp pq it la b lb lc le lf lh pr ll ps lp pt lt pu pv pw px bi translated">不出所料，<strong class="la iu">压缩文件比</strong>未压缩文件小</li><li id="c627" class="pp pq it la b lb py le pz lh qa ll qb lp qc lt pu pv pw px bi translated"><strong class="la iu"> CSV是最大的</strong>文件</li><li id="15c5" class="pp pq it la b lb py le pz lh qa ll qb lp qc lt pu pv pw px bi translated"><strong class="la iu">拼花是最小的未压缩</strong>文件</li><li id="32ba" class="pp pq it la b lb py le pz lh qa ll qb lp qc lt pu pv pw px bi translated"><strong class="la iu">带</strong> <code class="fe no np nq nr b"><strong class="la iu">format = "table"</strong></code> <strong class="la iu">的拼花和HDF5是最小的压缩</strong>文件</li></ul><h2 id="979e" class="om ms it bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated">阅读时间</h2><p id="5c5f" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">下面，您可以看到每种文件格式读取文件所需的时间。实心黑条表示未压缩文件的读取时间，而散列条表示压缩文件的读取时间。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qf"><img src="../Images/f95fce1907cb9bb904d520c5154162d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QWrdixksMBe8TH3A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">不同文件格式的阅读时间对比(图片由作者通过<a class="ae lu" href="https://www.kaggle.com/iamleonie/how-to-handle-large-datasets-in-python" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>提供)</p></figure><p id="f9a9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以做出如下观察:</p><ul class=""><li id="c368" class="pp pq it la b lb lc le lf lh pr ll ps lp pt lt pu pv pw px bi translated">正如所料，对于<strong class="la iu"> CSV，读数需要较长的时间</strong></li><li id="46dc" class="pp pq it la b lb py le pz lh qa ll qb lp qc lt pu pv pw px bi translated"><strong class="la iu"> Pickle和HDF5带</strong> <code class="fe no np nq nr b"><strong class="la iu">format = "fixed"</strong></code> <strong class="la iu">对<strong class="la iu">未压缩和压缩的</strong>文件都有最快的</strong>读取时间</li><li id="99bb" class="pp pq it la b lb py le pz lh qa ll qb lp qc lt pu pv pw px bi translated">不出所料，<strong class="la iu"> HDF5与</strong> <code class="fe no np nq nr b"><strong class="la iu">format = "fixed"</strong></code> <strong class="la iu">比</strong> <code class="fe no np nq nr b"><strong class="la iu">format = "table"</strong></code>更快，同样具有压缩性</li></ul><h2 id="28d4" class="om ms it bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated">多方面的</h2><p id="4029" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">除了读写时间和文件大小之外，我们还应该关注更多的特性:可读性、一致性、预期存储时间和可移植性，以及小数据集的性能。</p><p id="513c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">人类可读性</strong></p><p id="d4ce" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">CSV是人类可读的唯一文件格式。所有可选的文件格式都是二进制格式，因此人类无法阅读。</p><p id="1ba0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">一致性</strong></p><p id="d065" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">除非你明确告诉<code class="fe no np nq nr b">.read_csv()</code>方法用哪种数据类型来读取每一列，否则CSV文件格式不会记住数据类型。这需要预先了解数据类型，也是一项额外的工作。</p><p id="719e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果读取方法中没有明确列出数据类型，这会导致所需存储空间增加，因为所有整数都被读取为<code class="fe no np nq nr b">int64</code>，所有浮点都被读取为<code class="fe no np nq nr b">float64</code>，而<code class="fe no np nq nr b">datetime64[ns]</code>和<code class="fe no np nq nr b">categorical</code>被读取为<code class="fe no np nq nr b">object</code>。在下面，您可以看到原始数据帧和读写CSV文件后的数据帧:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qg"><img src="../Images/c73f0f856b52124778c670685deff709.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OuBi_7FiuDmYP7DOqjjzGg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">CSV文件读写前后的列数据类型比较(图片由作者通过<a class="ae lu" href="https://www.kaggle.com/iamleonie/how-to-handle-large-datasets-in-python" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>提供)</p></figure><p id="3b00" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如您所看到的，由于将较小的数据类型转换为较大的数据类型，所需的内存使用增加了。</p><p id="38cb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">预期储存时间和便携性</strong></p><p id="440b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">泡菜和羽毛旨在短期储存[7，8，9]。Pickle用于在工作会话之间保存Python对象，因此仅受Python支持。Feather旨在Python和R [9]之间交换数据。Pickle和Feather也不能保证版本之间的稳定性[7，9]。</p><p id="337a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">小数据集上的性能</strong></p><p id="ca7b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">尽管本文关注的是大型数据集，但值得一提的是<strong class="la iu">对于小型数据集，HDF5格式的读写时间很短。</strong>如下图所示，如果数据集小于2 MB，读取HDF5文件所需的时间甚至比CSV文件还要长。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qh"><img src="../Images/9f094a7d27bdc5cce59196faf51054b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Y4NUvDW83hTHYW_H.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">不同文件格式在不同文件大小下的读写时间比较(图片由作者通过<a class="ae lu" href="https://www.kaggle.com/iamleonie/how-to-handle-large-datasets-in-python" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>提供)</p></figure></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="be1b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下表总结了这一部分的比较:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qi"><img src="../Images/d7dd51987bc1364cd6895593a99a460a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zInrzoBRbUL-auTiKX_QLQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">CSV、Pickle、Parquet、Feather和HDF5的对比(图片由作者提供)</p></figure><h1 id="9b4f" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">结论</h1><p id="d5ce" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">正如所料，CSV文件格式在读取和写入文件时都是最慢的。除非您正在优化文件大小，否则所有替代方案大约只有CSV文件格式的一半。</p><p id="d860" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不幸的是，这个价值100万美元的问题有一个不令人满意的答案:</p><blockquote class="lv"><p id="a1a7" class="lw lx it bd ly lz ma mb mc md me lt dk translated">"哪种格式是处理大型数据集的最佳格式？"</p><p id="a200" class="lw lx it bd ly lz ma mb mc md me lt dk translated">—“这取决于您的使用情形，但可能不是CSV”</p></blockquote><p id="2cf7" class="pw-post-body-paragraph ky kz it la b lb mf ju ld le mg jx lg lh mh lj lk ll mi ln lo lp mj lr ls lt im bi translated">这里是我试图为这个答案的“视情况而定”部分提供更多的细节。我很想听听你在这方面的意见。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qj"><img src="../Images/aa4015fa5f51d1eb27a8e1a47fb7e3e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p-uhZoGtzTZBZHmYKjE2sQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">试图提供一个选择最佳文件格式的粗略指南(图片由作者提供)</p></figure><p id="193e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">尽管有缺点，CSV还是被广泛使用。看起来，它是人类可读的，而不是二进制文件格式，这一事实使它成为一种直观的文件格式，您可以快速打开并查看它，而不会有任何麻烦。因此，除非您正在处理数千兆字节的数据，否则CSV仍然是一个可接受的选项。</p><blockquote class="lv"><p id="1ce3" class="lw lx it bd ly lz ma mb mc md me lt dk translated">“CSV毕竟没那么差。别管了！”</p></blockquote></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="3d72" class="mr ms it bd mt mu qk mw mx my ql na nb jz qm ka nd kc qn kd nf kf qo kg nh ni bi translated">喜欢这个故事吗？</h1><p id="3ba6" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">要阅读更多来自我和其他作家的故事，请在Medium上注册。报名时可以用我的 <a class="ae lu" href="https://medium.com/@iamleonie/membership" rel="noopener"> <em class="pl">推荐链接</em> </a> <em class="pl">支持我。我将收取佣金，不需要你额外付费。</em></p><div class="nt nu gp gr nv nw"><a href="https://medium.com/@iamleonie/membership" rel="noopener follow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd iu gy z fp ob fr fs oc fu fw is bi translated">通过我的推荐链接加入Medium—Leonie Monigatti</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">阅读Leonie Monigatti(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">medium.com</p></div></div><div class="of l"><div class="qp l oh oi oj of ok ks nw"/></div></div></a></div><p id="ae10" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="pl">在</em> <a class="ae lu" href="https://www.linkedin.com/in/804250ab/" rel="noopener ugc nofollow" target="_blank"> <em class="pl"> LinkedIn </em> </a> <em class="pl">和</em> <a class="ae lu" href="https://www.kaggle.com/iamleonie" rel="noopener ugc nofollow" target="_blank"> <em class="pl">上找我Kaggle </em> </a> <em class="pl">！</em></p><h1 id="48f7" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">参考</h1><p id="d646" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">[1] Apache Arrow，“羽毛文件格式”。apache.org。<a class="ae lu" href="https://arrow.apache.org/docs/python/feather.html" rel="noopener ugc nofollow" target="_blank">https://arrow.apache.org/docs/python/feather.html</a>(2022年7月25日访问)</p><p id="71ff" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2]阿帕奇拼花地板，“概述”。apache.org。<a class="ae lu" href="https://parquet.apache.org/docs/overview/" rel="noopener ugc nofollow" target="_blank">https://parquet.apache.org/docs/overview/</a>(2022年7月25日访问)</p><p id="0138" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[3]数据表，“datatable.fread()”。datatable . readthe docs . io .<a class="ae lu" href="https://datatable.readthedocs.io/en/latest/api/dt/fread.html" rel="noopener ugc nofollow" target="_blank">https://datatable . readthe docs . io/en/latest/API/dt/fread . html</a>(2022年7月25日访问)</p><p id="4033" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[4] C. Deotte，“如何减少数据量”。kaggle.com。<a class="ae lu" href="https://www.kaggle.com/competitions/amex-default-prediction/discussion/328054" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/competitions/amex-default-prediction/discussion/328054</a>(2022年7月25日访问)</p><p id="8608" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[5]HDF小组，“HDF5”。hdfgroup.org。https://portal.hdfgroup.org/display/HDF5/HDF5(2022年7月25日访问)</p><p id="c881" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[6]“熊猫”，“熊猫。DataFrame.to_hdf "。pydata.org。<a class="ae lu" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_hdf.html#pandas.DataFrame.to_hdf" rel="noopener ugc nofollow" target="_blank">https://pandas . pydata . org/pandas-docs/stable/reference/API/pandas。data frame . to _ hdf . html</a>(2022年7月25日访问)</p><p id="a893" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[7]“Python”，“pickle — Python对象序列化。”python.org。https://docs.python.org/3/library/pickle.html(2022年7月25日访问)</p><p id="4658" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[8]“stack overflow”，“羽毛和拼花有什么区别？”。stackoverflow.com。https://stack overflow . com/questions/48083405/feather-and-parquet之间的区别是什么</p><p id="31fc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[9] H. Wickham，“Feather:一种用于R和Python的快速磁盘上数据帧格式，由Apache Arrow提供支持”。rstudio.com。https://www.rstudio.com/blog/feather/(2022年7月25日访问)</p></div></div>    
</body>
</html>