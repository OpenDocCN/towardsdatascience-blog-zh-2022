<html>
<head>
<title>Out-of-distribution generalization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">非分布概括</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/out-of-distribution-generalization-66b6f8980ef3#2022-03-09">https://towardsdatascience.com/out-of-distribution-generalization-66b6f8980ef3#2022-03-09</a></blockquote><div><div class="fc ig ih ii ij ik"/><div class="il im in io ip"><h2 id="571c" class="iq ir is bd b dl it iu iv iw ix iy dk iz translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/tds-podcast" rel="noopener" target="_blank">播客</a></h2><div class=""/><div class=""><h2 id="0740" class="pw-subtitle-paragraph jy jb is bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">Irina Rish 谈论人工智能中最棘手的问题之一</h2></div><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="kv kw l"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated"><a class="ae lb" href="https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2" rel="noopener ugc nofollow" target="_blank">苹果</a> | <a class="ae lb" href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz" rel="noopener ugc nofollow" target="_blank">谷歌</a> | <a class="ae lb" href="https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU" rel="noopener ugc nofollow" target="_blank"> SPOTIFY </a> | <a class="ae lb" href="https://anchor.fm/towardsdatascience" rel="noopener ugc nofollow" target="_blank">其他</a></p></figure><p id="a1ba" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated"><em class="ly">编者按:TDS 播客由杰雷米·哈里斯主持，他是人工智能安全初创公司墨丘利的联合创始人。每周，Jeremie 都会与该领域前沿的研究人员和商业领袖聊天，以解开围绕数据科学、机器学习和人工智能的最紧迫问题。</em></p><p id="7060" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">在训练过程中，人工智能往往会学习根据容易学习但具有欺骗性的特征进行预测。</p><p id="e1d5" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">例如，想象一下，一个经过训练的人工智能可以在图像中识别奶牛。理想情况下，我们希望它能够根据奶牛的形状和颜色来识别它们。但是如果我们放在训练数据集中的奶牛图片总是显示奶牛站在草地上呢？</p><p id="77d7" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">在这种情况下，我们在草和牛之间有一个<em class="ly">虚假关联</em>，如果我们不小心，我们的人工智能可能会学习成为草检测器而不是牛检测器。更糟糕的是，只有当我们在现实世界中部署它，并且它第一次撞上一头没有站在草地上的牛时，我们才能意识到这种情况的发生。</p><p id="5dd1" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">那么，你如何构建能够学习健壮的、通用的概念的人工智能系统，这些概念在其训练数据的上下文之外仍然有效？</p><p id="3230" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">这就是非分布概括的问题，也是 Mila 魁北克人工智能研究所的核心成员、加拿大自主人工智能卓越研究主席 Irina Rish 研究议程的核心部分。Irina 的研究探索了许多不同的策略，旨在克服分布不足的问题，从经验性的人工智能扩展工作到更多的理论工作，她和我一起在本期播客中讨论了这一点。</p><p id="c794" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">以下是我在对话中最喜欢的一些观点:</p><ul class=""><li id="fc70" class="lz ma is le b lf lg li lj ll mb lp mc lt md lx me mf mg mh bi translated">对伊琳娜来说，GPT-3 是人工智能比对和人工智能安全研究的“AlexNet 时刻”。我们第一次在没有真正理解人工智能的行为，或者不知道如何驾驭它的情况下，构建了高能力的人工智能。因此，Irina 认为这是进入人工智能比对研究的绝佳时机。</li><li id="bd3b" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">Irina 认为，分布外泛化是人工智能能力研究开始与人工智能对齐和人工智能安全研究融合的一个领域。让系统学习健壮的概念不仅有助于确保它们对世界有丰富的表示(这有助于能力)，还有助于通过解决虚假相关性问题来确保事故不会发生。</li><li id="e159" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">综合区域信息网研究了几项旨在解决分发范围以外的抽样问题的战略。其中一个涉及到使用不变性原则:无论我们的数据来自什么环境，我们希望我们的人工智能模型学习的特征都将是一致的(不变的)。以我之前提到的奶牛检测为例:我们希望我们的人工智能锁定的特征(例如，奶牛的形状和颜色)在不同的环境中保持一致。不管是在牧场、室内还是在沙漠中，牛还是牛。Irina 正在探索一些技术，让人工智能能够区分不变的和令人满意的特征(如奶牛的形状和颜色)和可变的和不可靠的预测特征(如地面上是否有草)。</li><li id="077c" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">Irina 认为另一个有前途的方法是扩大规模。我们之前讨论过播客的规模——但简而言之，这是一种想法，即当前的深度学习技术或多或少可以让我们按原样进入 AGI，只要它们用于训练足够大的神经网络，拥有足够大的数据集和同样巨大的计算能力。原则上，扩展某些类型的神经网络可以让人工智能了解太多关于它们的训练数据，以至于它们的性能只受到数据本身不可减少的噪声的限制。</li><li id="8d8c" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">这种可能性提出了另一个问题:对于只接受语言训练的人工智能来说，语言数据(用于训练 GPT-3 和第一代大规模基础模型)是否有太多噪音，以达到人类水平的全面能力？Irina 认为这是可能的——这就是为什么她对多模态学习的趋势感到兴奋:同时对多种数据类型(例如，图像、文本和音频数据)训练人工智能的实践。希望通过组合这些输入数据类型，人工智能可以学习超越可能单独存在于任何一种数据类型中的噪声限制。</li></ul><p id="9e7e" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">你可以<a class="ae lb" href="https://twitter.com/irinarish" rel="noopener ugc nofollow" target="_blank">在 Twitter 上关注伊琳娜</a>，或者<a class="ae lb" href="https://twitter.com/jeremiecharris" rel="noopener ugc nofollow" target="_blank">我在这里</a>。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mn"><img src="../Images/74e59ec970aff1f44c6d127b3681af29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xf-Wno65qk-l0AYiqCfB8A.png"/></div></div></figure><h2 id="fd8b" class="mu mv is bd mw mx my dn mz na nb dp nc ll nd ne nf lp ng nh ni lt nj nk nl iy bi translated">章节:</h2><ul class=""><li id="de3f" class="lz ma is le b lf nm li nn ll no lp np lt nq lx me mf mg mh bi translated">0:00 介绍</li><li id="9ba1" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">2:00 研究、安全和推广</li><li id="f0dc" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">8:20 不变风险最小化</li><li id="5965" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">15:00 缩放的重要性</li><li id="a21d" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">语言的作用</li><li id="7d5c" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">AGI 和缩放</li><li id="3b1e" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">32:30 GPT 对 ResNet 50</li><li id="45a3" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">37:00 建筑领域的潜在革命</li><li id="0347" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">42:30 电感偏置方面</li><li id="71ee" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">46:00 新风险</li><li id="b124" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">49:30 总结</li></ul></div></div>    
</body>
</html>