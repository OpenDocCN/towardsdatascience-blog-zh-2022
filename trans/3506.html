<html>
<head>
<title>Loss Functions and Their Use In Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">损失函数及其在神经网络中的应用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/loss-functions-and-their-use-in-neural-networks-a470e703f1e9#2022-08-04">https://towardsdatascience.com/loss-functions-and-their-use-in-neural-networks-a470e703f1e9#2022-08-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4845" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">损失函数及其实现概述</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a75da75f4fbb2102963f880b090aabee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zMFuVqqKFLayW0U7"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">克里斯里德在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="3b3c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">损失函数是神经网络最重要的方面之一，因为它们(以及优化函数)直接负责将模型拟合到给定的训练数据。</p><p id="e415" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文将深入探讨如何在神经网络中使用损失函数，不同类型的损失函数，在TensorFlow中编写自定义损失函数，以及处理图像和视频训练数据的损失函数的实际实现——计算机视觉中使用的主要数据类型，这是我感兴趣和关注的主题。</p><h1 id="e135" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">背景资料</h1><p id="561e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">首先，快速回顾神经网络的基本原理及其工作原理。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/6816edad21b19bcced6df08284a79de4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2k2pwxplR6fNdJNi.jpg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:维基共享资源</p></figure><p id="77d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">神经网络</strong>是一套算法，旨在识别一组给定训练数据中的趋势/关系。这些算法基于人类神经元处理信息的方式。</p><p id="5e29" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该等式表示神经网络如何处理每一层的输入数据，并最终产生预测的输出值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/a4cdaaa28b2f95d3a9bfe206ee481e27.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/1*_RlmGhVg6-KGL3RT8eXuWw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:作者</p></figure><p id="bf64" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了<strong class="ky ir">训练</strong>——模型映射训练数据和输出之间的关系的过程——神经网络更新其超参数、权重、<em class="mr"> wT </em>和偏差、<em class="mr"> b、</em>以满足上面的等式。</p><p id="4388" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个训练输入在一个称为<strong class="ky ir">正向传播</strong>的过程中被加载到神经网络中。一旦模型产生了一个输出，这个预测输出在一个称为<strong class="ky ir">反向传播</strong>的过程中与给定的目标输出进行比较——然后调整模型的超参数，使其现在输出一个更接近目标输出的结果。</p><p id="fee2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是损失函数出现的地方。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/8d54d8fe5218269f055e58ae14f7faea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*XWWbqWIJttTSfmwF.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:维基共享资源</p></figure><h1 id="b805" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">损失函数概述</h1><p id="fcb4" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated"><strong class="ky ir">损失函数</strong>是<strong class="ky ir">比较</strong>目标和预测输出值的函数；衡量神经网络对训练数据的建模程度。训练时，我们的目标是尽量减少预测输出和目标输出之间的这种损失。</p><p id="bf6d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">调整<strong class="ky ir">超参数</strong>以最小化平均损失——我们找到最小化<em class="mr"> J </em>值(平均损失)的权重<em class="mr"> wT </em>和偏差<em class="mr"> b </em>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/aed930da815daf5d9946109f25738e75.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*EBJivCIFmbfIhNue5gug4A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:作者</p></figure><p id="2901" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以认为这类似于统计学中的残差，它测量实际的<em class="mr"> y </em>值与回归线(预测值)的距离，目标是最小化净距离。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/b35fca52713135a3cfdc1d2b2aff240f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xjbUMyJa5Rl4VLYs.jpg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:维基共享资源</p></figure><h1 id="98ec" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">如何在TensorFlow中实现损失函数</h1><p id="9b0a" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">对于本文，我们将使用谷歌的<strong class="ky ir"> TensorFlow </strong>库来实现不同的损失函数——很容易演示损失函数是如何在模型中使用的。</p><p id="9e34" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在TensorFlow中，神经网络使用的损失函数被指定为model.compile()中的参数，model . compile()是训练神经网络的最终方法。</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="c21d" class="na lt iq mw b gy nb nc l nd ne">model.compile(loss='mse', optimizer='sgd')</span></pre><p id="3a2d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">损失函数可以作为字符串(如上所示)或函数对象(从TensorFlow导入或编写为自定义损失函数)输入，我们将在后面讨论。</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="1d06" class="na lt iq mw b gy nb nc l nd ne">from tensorflow.keras.losses import mean_squared_error<br/>model.compiile(loss=mean_squared_error, optimizer='sgd')</span></pre><p id="33d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">张量流中的所有损失函数具有相似的结构:</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="43f5" class="na lt iq mw b gy nb nc l nd ne">def loss_function (y_true, y_pred): <br/>    return losses</span></pre><p id="ba76" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它必须以这种方式格式化，因为model.compile()方法只需要loss属性的两个输入参数。</p><h1 id="2abe" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">损失函数的类型</h1><p id="3f4e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在监督学习中，有两种主要类型的损失函数，它们与两种主要类型的神经网络相关:回归和分类损失函数</p><ol class=""><li id="354d" class="nf ng iq ky b kz la lc ld lf nh lj ni ln nj lr nk nl nm nn bi translated">回归损失函数——用于回归神经网络；给定一个输入值，模型预测相应的输出值(而不是预先选择的标签)；《出埃及记》均方误差，平均绝对误差</li><li id="9fcc" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated">分类损失函数——用于分类神经网络；给定一个输入，神经网络产生一个属于各种预设类别的输入概率向量，然后可以选择具有最高归属概率的类别；《出埃及记》二元交叉熵，分类交叉熵</li></ol><h2 id="08e9" class="na lt iq bd lu nt nu dn ly nv nw dp mc lf nx ny me lj nz oa mg ln ob oc mi od bi translated">均方误差</h2><p id="91c7" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">MSE是最受欢迎的损失函数之一，它可以找到目标输出和预测输出之间的平均方差</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/08c0db35113e927d829926489ee6dc52.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*kjfms6RCnHVMLRSq75AD0Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:作者</p></figure><p id="bd8d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个函数有许多特性，使它特别适合于计算损失。差值是平方的，这意味着预测值是高于还是低于目标值并不重要；但是，误差较大的值会受到惩罚。MSE也是一个凸函数(如上图所示),具有明确定义的全局最小值——这允许我们更容易地利用<strong class="ky ir">梯度下降优化</strong>来设置权重值。</p><p id="38d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是TensorFlow中的一个标准实现——也内置在TensorFlow库中。</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="9084" class="na lt iq mw b gy nb nc l nd ne">def mse (y_true, y_pred): <br/>    return tf.square (y_true - y_pred)</span></pre><p id="5bcc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，这种损失函数的一个缺点是它对异常值非常敏感；如果预测值明显大于或小于其目标值，这将大大增加损失。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/b6f6644888155b5afb5f1b62a363325a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/0*dutW2qhu_nrSv93s.jpg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:维基共享资源</p></figure><h2 id="687d" class="na lt iq bd lu nt nu dn ly nv nw dp mc lf nx ny me lj nz oa mg ln ob oc mi od bi translated">平均绝对误差</h2><p id="ce48" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">MAE找出目标和预测输出之间的绝对差值的平均值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/b2afa94bced8b6a2a0a110eb6afda3c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*78B4XwuVtBbacdTFIkjOXQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:作者</p></figure><p id="0e2b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在某些情况下，这个损失函数被用作MSE的替代函数。如前所述，MSE对异常值非常敏感，这会显著影响损失，因为距离是平方的。MAE用于训练数据有大量异常值的情况，以减轻这种情况。</p><p id="8a5e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是TensorFlow中的一个标准实现——也内置在TensorFlow库中。</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="4d56" class="na lt iq mw b gy nb nc l nd ne">def mae (y_true, y_pred): <br/>    return tf.abs(y_true - y_pred)</span></pre><p id="be8b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它也有一些缺点；当平均距离接近0时，梯度下降优化将不起作用，因为函数在0处的导数是未定义的(这将导致错误，因为不可能除以0)。</p><p id="5adf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，开发了一种称为<strong class="ky ir"> Huber损失</strong>的损失函数，它具有MSE和MAE的优点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/1309eb698b61c3d65eab2e51120a251a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_cBTm_xd7ixdvcby7ZRcAw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:作者</p></figure><p id="831d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果实际值和预测值之间的绝对差值小于或等于阈值𝛿，则应用MSE。否则，如果误差足够大，则应用MAE。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/d569b92aab4422ac3a8878ff1352dc63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*_S2wqIIqrNNDY7aZM4Ef7w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:维基共享资源</p></figure><p id="68d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是TensorFlow的实现——这涉及到使用一个包装器函数来利用阈值变量，我们稍后会对此进行讨论。</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="7e4a" class="na lt iq mw b gy nb nc l nd ne">def huber_loss_with_threshold (t = 𝛿): <br/>    def huber_loss (y_true, y_pred): <br/>        error = y_true - y_pred <br/>        within_threshold = tf.abs(error) &lt;= t<br/>        small_error = tf.square(error)<br/>        large_error = t * (tf.abs(error) - (0.5*t))<br/>        if within_threshold: <br/>                return small_error<br/>        else: <br/>                return large_error<br/>    return huber_loss</span></pre><h2 id="39ac" class="na lt iq bd lu nt nu dn ly nv nw dp mc lf nx ny me lj nz oa mg ln ob oc mi od bi translated">二元交叉熵/对数损失</h2><p id="3ea6" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这是二元分类模型中使用的损失函数，其中模型接受一个输入，并将其分类到两个预设类别中的一个。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/fdbc7bb72514a020272fcaff231e6785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gbe2JiIk6Vsf32ulhhahCA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:作者</p></figure><p id="d7bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">分类神经网络的工作原理是输出一个概率向量——给定输入符合每个预设类别的概率；然后选择概率最高的类别作为最终输出。</p><p id="3702" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在二进制分类中，y只有两个可能的实际值— 0或1。因此，为了准确地确定实际值和预测值之间的损失，需要将实际值(0或1)与输入符合该类别的概率进行比较(<em class="mr"> p(i) </em> =该类别为1的概率；1 — <em class="mr"> p(i) </em> =类别为0的概率)</p><p id="4a4c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是TensorFlow实现。</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="aaef" class="na lt iq mw b gy nb nc l nd ne">def <strong class="mw ir">log_loss</strong> (y_true, y_pred): <br/>    y_pred = tf.clip_by_value(y_pred, le-7, 1 - le-7)<br/>    error <!-- -->= y_true * tf.log(y_pred + 1e-7) (1-y_true) * tf.log(1-y_pred + 1e-7)<br/>    return -error</span></pre><h2 id="9e0e" class="na lt iq bd lu nt nu dn ly nv nw dp mc lf nx ny me lj nz oa mg ln ob oc mi od bi translated">分类交叉熵损失</h2><p id="cddf" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在类的数量大于两个的情况下，我们利用分类交叉熵——这遵循与二进制交叉熵非常相似的过程。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/d1699c26d2d5a66dd47b57a32db50aec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*HOCJtpCyQzWX8Xp3H8Ez2w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:作者</p></figure><p id="5082" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">二元交叉熵是分类交叉熵的一个特例，其中<em class="mr">M</em>= 2——类别的数量是2。</p><h1 id="5b28" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">自定义损失函数</h1><p id="8f14" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如前所述，在编写神经网络时，可以从the tf.keras.losses模块中导入损失函数作为函数对象。该模块包含以下内置损失函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/1ec7a4d3f7eabe947781e4c341baba8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ayR4LJx7MabyIjVVJc3c7A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:作者</p></figure><p id="15ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，可能存在这些传统/主要损失函数可能不足够的情况。例如，如果您的训练数据中有太多噪音(异常值、错误的属性值等)。)——这不能用数据预处理来补偿——或者在无监督学习中使用(我们将在后面讨论)。在这些情况下，您可以编写自定义损失函数来满足您的特定条件。</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="915e" class="na lt iq mw b gy nb nc l nd ne">def <strong class="mw ir">custom_loss_function</strong> (y_true, y_pred): <br/>    return losses</span></pre><p id="9fda" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">编写自定义损失函数非常简单；唯一的要求是损失函数必须只接受两个参数:y_pred(预测输出)和y_true(实际输出)。</p><p id="8275" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些的一些例子是3个定制损失函数，在变分自动编码器(VAE)模型的情况下，来自Soon Yau Cheong的TensorFlow 的<em class="mr">手动图像生成。</em></p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="6e8e" class="na lt iq mw b gy nb nc l nd ne">def vae_kl_loss(y_true, y_pred):<br/>    kl_loss =  - 0.5 * tf.reduce_mean(1 + vae.logvar -    tf.square(vae.mean) - tf.exp(vae.logvar))<br/>    return kl_loss</span><span id="c9fe" class="na lt iq mw b gy ok nc l nd ne">def vae_rc_loss(y_true, y_pred):<br/>    #rc_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)<br/>    rc_loss = tf.keras.losses.MSE(y_true, y_pred)<br/>    return rc_loss</span><span id="b0b8" class="na lt iq mw b gy ok nc l nd ne">def vae_loss(y_true, y_pred):<br/>    kl_loss = vae_kl_loss(y_true, y_pred)<br/>    rc_loss = vae_rc_loss(y_true, y_pred)<br/>    kl_weight_const = 0.01<br/>    return kl_weight_const*kl_loss + rc_loss</span></pre><p id="a631" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据损失函数的数学计算，您可能需要添加额外的参数，例如休伯损失中的阈值𝛿(如上)；为此，您必须包含一个包装函数，因为TF不允许在您的损失函数中有超过2个参数。</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="1f73" class="na lt iq mw b gy nb nc l nd ne">def custom_loss_with_threshold (threshold = 1): <br/>    def custom_loss (y_true, y_pred): <br/>        pass #Implement loss function - can call the threshold variable<br/>    return custom_loss</span></pre><h1 id="f087" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">图像处理中的实现</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/685743461d4542239448b785c26e7756.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PeKaM1C9sE8LaOcl"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@cgower?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">克里斯多夫·高尔</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="6393" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看损失函数的一些实际实现。具体来说，我们将研究在各种用例中如何使用损失函数来处理图像数据。</p><h2 id="955b" class="na lt iq bd lu nt nu dn ly nv nw dp mc lf nx ny me lj nz oa mg ln ob oc mi od bi translated">图像分类</h2><p id="e8f6" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">计算机视觉最基本的方面之一是图像分类——能够将图像分配到两个或更多预选标签中的一个；这允许用户识别物体、书写、人等。在图像内(在图像分类中，图像通常只有一个主题)。</p><p id="340a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图像分类中最常用的损失函数是交叉熵损失/对数损失(二进制用于2个类别之间的分类，稀疏分类用于3个或更多类别)，其中模型输出输入图像属于每个预设类别的概率向量。然后将该输出与实际输出进行比较，实际输出由大小相等的向量表示，其中正确类别的概率为1，所有其他类别的概率为0。</p><p id="cc67" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它的基本实现可以直接从TensorFlow库中导入，不需要任何进一步的定制或修改。下面是IBM的一个开源深度卷积神经网络(CNN)的摘录，它对文档图像(身份证、申请表等)进行分类。).</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="3dc1" class="na lt iq mw b gy nb nc l nd ne">model<strong class="mw ir">.</strong>add(Dense(5, activation<strong class="mw ir">=</strong>'sigmoid'))<br/>model<strong class="mw ir">.</strong>summary()</span><span id="f213" class="na lt iq mw b gy ok nc l nd ne">model<strong class="mw ir">.</strong>compile(<!-- -->optimizer='adam'<!-- -->, loss<strong class="mw ir">=</strong>'categorical_crossentropy',<br/>    metrics<strong class="mw ir">=</strong>['accuracy'])</span></pre><p id="2140" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">目前正在研究开发新的(自定义)损失函数来优化多类分类。下面是杜克大学研究人员提出的损失函数的摘录，该函数通过查看错误结果中的模式来扩展分类交叉熵损失，以加快学习过程。</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="a548" class="na lt iq mw b gy nb nc l nd ne">def matrix_based_crossentropy (output, target, matrixA, from_logits = False):<br/>    Loss = 0<br/>    ColumnVector = np.matul(matrixA, target)<br/>    for i, y in enumerate (output):<br/>        Loss -= (target[i]*math.log(output[i],2))<br/>        Loss += ColumnVector[i]*exponential(output[i])<br/>        Loss -= (target[i]*exponential(output[i]))<br/>    newMatrix = updateMatrix(matrixA, target, output, 4)<br/>    return [Loss, newMatrix]</span></pre><h2 id="ab62" class="na lt iq bd lu nt nu dn ly nv nw dp mc lf nx ny me lj nz oa mg ln ob oc mi od bi translated">图象生成</h2><p id="d169" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">图像生成是一个过程，通过该过程，神经网络根据用户的指定创建图像(从现有库中)。</p><p id="de6f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在整篇文章中，我们主要讨论了监督学习中损失函数的使用——我们已经清楚地标记了输入、<em class="mr"> x </em>和输出、<em class="mr"> y </em>，并且该模型应该确定这两个变量之间的关系。</p><p id="075b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图像生成是无监督学习的一种应用，其中需要模型来分析和查找未标记输入数据集中的模式。损失函数的基本原理仍然成立；无监督学习中损失函数的目标是确定输入示例和假设(输入示例本身的模型近似值)之间的差异。</p><p id="dfa7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，这个方程模拟MSE将如何被实现用于无监督学习，其中<em class="mr"> h(x) </em>是假设函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/6c5462a1d24edd4bcba114a3323ef6aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W8njBwH2_9tJ0-KEbM-zyg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:作者</p></figure><p id="2cff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是一个对比语言-图像预训练(CLIP)扩散模型的摘录——它通过文本描述生成艺术(图像)——以及一些图像样本。</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="483e" class="na lt iq mw b gy nb nc l nd ne">if args.init_weight:<br/>    result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)</span><span id="ef7a" class="na lt iq mw b gy ok nc l nd ne">lossAll,img = ascend_txt()</span><span id="72ad" class="na lt iq mw b gy ok nc l nd ne">if i % args.display_freq == 0:<br/>    checkin(i, lossAll)</span><span id="ea1c" class="na lt iq mw b gy ok nc l nd ne">loss = sum(lossAll)<br/>loss.backward()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/abcb3d7dc3b158876eda6c0d3a4aadbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0xP1PMkDhB93oltZegJWHA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:作者</p></figure><p id="cedf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在变分自动编码器(VAE)模型的情况下，在我们的<strong class="ky ir">自定义损失函数</strong>部分中显示了在图像生成中使用损失函数的另一个例子。</p><h1 id="216a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">包扎</h1><p id="3130" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在本文中，我们讨论了1)损失函数如何工作，2)它们如何在神经网络中使用，3)适合特定神经网络的不同类型的损失函数，4) 2个特定的损失函数及其使用案例，5)编写自定义损失函数，以及6)图像处理损失函数的实际实现。</p></div><div class="ab cl oo op hu oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="ij ik il im in"><p id="75b6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">谢谢你看我的文章！你可以通过我的LinkedIn联系我，或者给我发邮件到vishal.yathish@gmail.com</p></div></div>    
</body>
</html>