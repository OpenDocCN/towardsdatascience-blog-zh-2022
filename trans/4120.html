<html>
<head>
<title>Transformers for Tabular Data: TabTransformer Deep Dive</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于表格数据的转换器:TabTransformer深入分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transformers-for-tabular-data-tabtransformer-deep-dive-5fb2438da820#2022-09-13">https://towardsdatascience.com/transformers-for-tabular-data-tabtransformer-deep-dive-5fb2438da820#2022-09-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0f0d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解我们的TabTransformer并学习应用它</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2111e6425b04828bf92f4e2fec650a73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iPDX0cNVz95Ww3cO"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@samule?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Samule孙</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="bff0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="cf1b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">今天，变压器是大多数最先进的自然语言处理(NLP)和计算机视觉(CV)架构中的关键模块。尽管如此，表格领域仍然主要由梯度推进决策树(GBDT)主导，所以有人试图弥合这一差距是合乎逻辑的。第一个基于transformer的模型是由黄等人(2020)在他们的<a class="ae ky" href="https://arxiv.org/abs/2012.06678" rel="noopener ugc nofollow" target="_blank"> TabTransformer:使用上下文嵌入的表格数据建模</a>论文中介绍的。</p><p id="e076" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这篇文章旨在提供论文的概述，深入模型细节，并向您展示如何使用TabTransformer处理您的数据。</p><h1 id="f34a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">论文概述</h1><p id="f100" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">本文的主要思想是，如果我们使用转换器将正则范畴嵌入转换为上下文嵌入，正则多层感知器(MLP)的性能可以得到显著提高。让我们稍微消化一下这种说法。</p><h2 id="06c0" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">范畴嵌入</h2><p id="a237" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在深度学习模型中使用分类特征的经典方法是训练它们的嵌入。这意味着每个分类值都有一个唯一的密集矢量表示，可以传递给下一层。例如，下面你可以看到每个分类特征都用一个4维数组表示。这些嵌入然后与数字特征连接，并用作MLP的输入。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/3c8ab4f73941cb40598ae45e782c89e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lFxBmnKwpVfkAywXhErkRQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">带有范畴嵌入的MLP。图片作者。</p></figure><h2 id="9579" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">语境嵌入</h2><p id="4e81" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">该论文的作者认为，范畴嵌入缺乏上下文含义，即它们不编码范畴变量之间的任何交互和关系。为了将嵌入上下文化，建议使用当前在NLP中用于完全相同目的的转换器。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/a0fc09ce0fa8124a39b55c637078b50a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M5xJfs2_fDjsW-hsVWqE-g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TabTransformer中的上下文嵌入。图片作者。</p></figure><p id="f119" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了形象化动机，考虑下面经过训练的上下文嵌入的图像。突出显示了两个分类特征— <strong class="lt iu">关系(黑色)</strong>和<strong class="lt iu">婚姻状况(蓝色)</strong>。这些特征是相关的，因此“已婚”、“丈夫”和“妻子”的值在向量空间中应该彼此接近，即使它们来自不同的变量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/d612fc3bea096ed75be5e03a7cb0b98f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XlnM5z6tlRLq2NMO"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">经过训练的TabTransformer嵌入示例。图片作者。</p></figure><p id="f12f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">通过训练的上下文嵌入，我们确实可以看到“已婚”的婚姻状态更接近于“丈夫”和“妻子”的关系水平，而“未婚”的分类值来自右侧的单独聚类。这种类型的上下文使得这些嵌入更加有用，而使用简单的分类嵌入是不可能的。</p><h2 id="6dfc" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">TabTransformer架构</h2><p id="8511" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">考虑到上述动机，作者提出了以下架构:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/66c6fd9fe611e5498ee6fd3ad433ef77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZfFJ4gfa4p5PpHmGNClj5A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TabTransformer架构。改编自<a class="ae ky" href="https://arxiv.org/abs/2012.06678" rel="noopener ugc nofollow" target="_blank">黄等(2020) </a></p></figure><p id="e154" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们可以将该架构分为5个步骤:</p><ol class=""><li id="88ac" class="ni nj it lt b lu mn lx mo ma nk me nl mi nm mm nn no np nq bi translated">数字特征被标准化并向前传递</li><li id="7e9d" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm nn no np nq bi translated">嵌入了分类特征</li><li id="233f" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm nn no np nq bi translated">嵌入通过变换器块传递N次，以获得上下文嵌入</li><li id="ab84" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm nn no np nq bi translated">上下文范畴嵌入与数字特征连接在一起</li><li id="70ee" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm nn no np nq bi translated">连接通过MLP得到所需的预测</li></ol><p id="1c8b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">虽然模型架构非常简单，但作者表明，增加变压器层可以显著提高性能。当然，神奇之处就发生在这些变压器模块内部，所以让我们更详细地探索一下。</p><h2 id="3b36" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">变形金刚(电影名)</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/7d950aa396cacb005ba64b95e92ca058.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ytdr4Naey6YGF2kPpYIzZw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">变压器架构。改编自<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">瓦斯瓦尼等人(2017) </a></p></figure><p id="d2fd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">你可能以前见过变形金刚的架构(如果你没有，我强烈推荐<a class="ae ky" href="https://blog.varunajayasiri.com/ml/transformer.html" rel="noopener ugc nofollow" target="_blank">这款带注释的笔记本</a>)但是快速回顾一下，记住它由编码器和解码器部分组成(见上文)。对于TabTransformer，我们只关心将输入嵌入上下文化的编码器部分(解码器部分将这些嵌入转换为最终输出)。但是它到底是怎么做到的呢？答案是——多头注意力机制。</p><h2 id="2517" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">多头注意力</h2><p id="1cc1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">引用我最喜欢的关于注意力机制的文章:</p><blockquote class="nx ny nz"><p id="eb7e" class="lr ls oa lt b lu mn ju lw lx mo jx lz ob mp mc md oc mq mg mh od mr mk ml mm im bi translated">自我关注背后的关键概念是，它允许网络学习如何最好地在输入序列的片段之间传递信息。</p></blockquote><p id="7d54" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">换句话说，自我关注有助于模型找出在代表某个单词/类别时，输入的哪些部分更重要，哪些部分不太重要。我强烈推荐阅读上面引用的文章，以获得关于它为什么如此有效的良好直觉。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/8bab38a9d5c4ee56fb4be73c4deaf328.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1h1lk2cezVLX-Ete"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多头关注。改编自<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">瓦斯瓦尼等人(2017) </a></p></figure><p id="c25b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">注意力是通过三个学习矩阵来计算的——Q、K和V，分别代表查询、关键字和值。首先，我们将Q和K相乘得到注意力矩阵。该矩阵被缩放并通过softmax层。然后，我们将它乘以V矩阵得到最终值。为了更直观的理解，考虑下图，它显示了我们如何使用矩阵Q、K和v从输入嵌入到上下文嵌入。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/98a8c0a5ce45de718606fb1f5c334847.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gjJBI_ERoncASFDQjq-Pmg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自我关注流可视化。图片作者。</p></figure><p id="9883" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">通过重复这个过程<em class="oa"> h </em>次(用不同的Q、K、V矩阵),我们得到了多个上下文嵌入，形成了我们最终的多头注意力。</p><p id="ff5c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">简短回顾</strong></p><p id="1a58" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我知道这是很多，所以让我们总结一下上面所说的一切。</p><ul class=""><li id="5f1e" class="ni nj it lt b lu mn lx mo ma nk me nl mi nm mm og no np nq bi translated">简单的分类嵌入不包括上下文信息</li><li id="0a20" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm og no np nq bi translated">通过将分类嵌入传递给Transformer Encoder，我们能够将嵌入上下文化</li><li id="903a" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm og no np nq bi translated">Transformer架构可以将嵌入上下文化，因为它使用多头关注机制</li><li id="f19c" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm og no np nq bi translated">多头注意力使用矩阵Q、K和V来寻找有用的相互作用和相关性，同时对变量进行编码</li><li id="5b16" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm og no np nq bi translated">在TabTransformer中，上下文嵌入与数字输入连接，并通过简单的MLP输出预测</li></ul><p id="09c1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">虽然TabTransformer背后的想法非常简单，但注意力的机制可能需要一些时间来掌握，所以我强烈建议你重新阅读上面的解释，如果你感到迷失，请按照所有建议的链接进行操作。会变得容易的，我保证！</p><h2 id="2fef" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">结果</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/e8cd693ef22418c058ab6d01f2fe1029.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1rfO2h-3yWcGa1ljk8gOUw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">结果部分。改编自<a class="ae ky" href="https://arxiv.org/abs/2012.06678" rel="noopener ugc nofollow" target="_blank">黄等(2020) </a></p></figure><p id="7e07" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">根据报告的结果，TabTransformer优于所有其他深度学习表格模型(尤其是我在这里<a class="ae ky" href="https://syslog.ravelin.com/classification-with-tabnet-deep-dive-49a0dcc8f7e8" rel="noopener ugc nofollow" target="_blank">提到的TabNet</a>)。此外，它接近GBDTs的性能水平，这非常令人鼓舞。该模型对缺失和噪声数据也相对稳健，并在半监督设置中优于其他模型。然而，这些数据集显然不是详尽的，正如进一步的论文所证明的(例如<a class="ae ky" href="https://arxiv.org/abs/2106.03253" rel="noopener ugc nofollow" target="_blank"> this </a>)，仍然有很大的改进空间。</p><h1 id="205c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">演示</h1><p id="128e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在，让我们最终了解如何将模型应用到您自己的数据中。示例数据取自<a class="ae ky" href="https://www.kaggle.com/competitions/tabular-playground-series-aug-2022" rel="noopener ugc nofollow" target="_blank">表格形式的操场追逐赛</a>。为了方便地使用TabTransformer，我创建了一个<code class="fe oi oj ok ol b"><a class="ae ky" href="https://github.com/aruberts/TabTransformerTF" rel="noopener ugc nofollow" target="_blank">tabtransformertf</a></code>包。它可以使用<code class="fe oi oj ok ol b">pip install tabtransformertf</code>进行安装，并允许我们在没有大量预处理的情况下使用该模型。下面您可以看到训练模型所需的主要步骤，但请务必查看<a class="ae ky" href="https://www.kaggle.com/antonsruberts/tabtransformer" rel="noopener ugc nofollow" target="_blank">补充笔记本</a>了解更多细节。</p><h2 id="40fb" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">数据预处理</h2><p id="845f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">第一步是设置适当的数据类型，并将我们的训练和验证数据转换成TF数据集。先前安装的软件包有一个很好的实用工具来做到这一点。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="870c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下一步是为我们将传递给主模型的分类数据准备预处理层。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="12e9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">预处理到此为止！现在，我们可以开始构建模型了。</p><h2 id="abc2" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">TabTransformer模型</h2><p id="948e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">初始化模型非常容易。有几个参数需要指定，但最重要的是— <code class="fe oi oj ok ol b">embbeding_dim</code>、<code class="fe oi oj ok ol b">depth</code>和<code class="fe oi oj ok ol b">heads</code>。所有的参数都是在超参数调整后选择的，所以检查<a class="ae ky" href="https://www.kaggle.com/antonsruberts/tabtransformer" rel="noopener ugc nofollow" target="_blank">笔记本</a>来查看程序。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="b7ca" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">模型初始化后，我们可以像其他Keras模型一样拟合它。训练参数也可以调整，所以可以随意调整学习速度和提前停止。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><h2 id="52e9" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">估价</h2><p id="7a55" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">竞争指标是ROC AUC，所以让我们一起使用它和PR AUC来评估模型的性能。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="cb2f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">也可以自己给测试集打分，提交给<a class="ae ky" href="https://www.kaggle.com/competitions/ieee-fraud-detection/overview" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>。这个解决方案把我放在了前35%的位置，这还不错，但也不算太好。为什么TabTransfromer表现不佳？可能有几个原因:</p><ul class=""><li id="c538" class="ni nj it lt b lu mn lx mo ma nk me nl mi nm mm og no np nq bi translated">数据集太小，深度学习模型是出了名的数据饥渴</li><li id="ae09" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm og no np nq bi translated">TabTransformer很容易在像tabular playground这样的玩具例子上过度拟合</li><li id="7696" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm og no np nq bi translated">没有足够的分类特征使模型有用</li></ul><h1 id="7508" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="23b9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">本文探讨了TabTransformer背后的主要思想，并展示了如何使用<code class="fe oi oj ok ol b"><a class="ae ky" href="https://github.com/aruberts/TabTransformerTF" rel="noopener ugc nofollow" target="_blank">tabtransformertf</a></code>包来应用它。</p><p id="301a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">TabTransformer是一个有趣的架构，在当时胜过了许多/大部分深度表格模型。它的主要优点是它将范畴嵌入情境化，这增加了它们的表达能力。它使用分类特征上的多头注意力机制来实现这一点，这是Transformers对表格数据的第一个应用之一。</p><p id="267f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">该架构的一个明显缺点是数字特征被简单地传递到最终的MLP层。因此，它们没有被语境化，它们的值也没有在分类嵌入中被考虑。在下一篇的<a class="ae ky" rel="noopener" target="_blank" href="/improving-tabtransformer-part-1-linear-numerical-embeddings-dbc3be3b5bb5">文章中，我将探索如何修复这个缺陷并进一步提高性能。所以，一定要看完，给我你的想法！</a></p><h1 id="2e16" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">接下来读什么…</h1><ul class=""><li id="8965" class="ni nj it lt b lu lv lx ly ma oo me op mi oq mm og no np nq bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/improving-tabtransformer-part-1-linear-numerical-embeddings-dbc3be3b5bb5">表格数据的转换器(第二部分):线性数字嵌入</a></li><li id="44d3" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm og no np nq bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/transformers-for-tabular-data-part-3-piecewise-linear-periodic-encodings-1fc49c4bd7bc">表格数据转换器(三):分段线性&amp;周期编码</a></li></ul></div></div>    
</body>
</html>