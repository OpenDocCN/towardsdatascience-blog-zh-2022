<html>
<head>
<title>Clustering Contextual Embeddings for Topic Modelling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于主题建模的聚类上下文嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/clustering-contextual-embeddings-for-topic-model-1fb15c45b1bd#2022-05-20">https://towardsdatascience.com/clustering-contextual-embeddings-for-topic-model-1fb15c45b1bd#2022-05-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="11a0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过聚类上下文句子嵌入提取主题</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/9bcdefecb756522a24f01597caef5287.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*HYsUH3WI406oGU8fkuIuVQ.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">在<a class="ae kr" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae kr" href="https://unsplash.com/@luisadenu?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Luisa Denu </a>拍摄的照片</p></figure><h1 id="a276" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">TL；速度三角形定位法(dead reckoning)</h1><p id="137d" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">我们进行了大量的实验来比较基于聚类的主题模型和传统的神经主题模型(NTMs ),展示了一种从文档中提取主题的替代方法。</p><p id="4458" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">查看我们的<strong class="lm ir">T5 NAACL 2022T7】论文📄"<a class="ae kr" href="https://arxiv.org/abs/2204.09874" rel="noopener ugc nofollow" target="_blank"> <strong class="lm ir"> <em class="ml">神经话题建模比聚类好吗？关于主题</em></strong></a><em class="ml"/>和官方<a class="ae kr" href="https://github.com/hyintell/topicx" rel="noopener ugc nofollow" target="_blank"> <strong class="lm ir"> Github </strong> </a>的语境嵌入聚类的实证研究。</strong></p></div><div class="ab cl mm mn hu mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ij ik il im in"><h1 id="208c" class="ks kt iq bd ku kv mt kx ky kz mu lb lc jw mv jx le jz mw ka lg kc mx kd li lj bi translated">范例1:传统的主题建模</h1><p id="65cc" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi my translated">opic建模是一种无监督的方法，用于提取文档中的语义主题。从传统的潜在狄利克雷分配(LDA)模型到神经网络增强的神经主题模型(NTMs ),主题建模取得了显著的进步。然而，这些主题模型通常采用词袋(BoW)作为文档表示，这限制了模型的性能。</p><p id="82cd" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">后来，由各种预训练语言模型(如BERT)产生的上下文化单词和句子嵌入出现，并在多种自然语言处理(NLP)任务中显示出最先进的结果。最近的作品如<a class="ae kr" href="https://aclanthology.org/2021.acl-short.96.pdf" rel="noopener ugc nofollow" target="_blank"> CombinedTM </a>和<a class="ae kr" href="https://aclanthology.org/2021.eacl-main.143.pdf" rel="noopener ugc nofollow" target="_blank"> ZeroShotTM </a> ⁴将这些情境化的嵌入整合到NTMs中，显示出比传统NTMs更好的建模性能。</p><p id="171d" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">尽管结果很有希望，但是这种NTM遭受计算开销，并且当前将预训练的嵌入集成到NTM架构中是幼稚的。</p><blockquote class="nh"><p id="b75b" class="ni nj iq bd nk nl nm nn no np nq mf dk translated">有了高质量的上下文化文档表示，我们真的需要复杂的ntm来获得连贯和可解释的主题吗？</p></blockquote><h1 id="caef" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw nr jx le jz ns ka lg kc nt kd li lj bi translated">范例2:基于聚类的主题建模</h1><p id="54ed" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">相反，我们探索了另一种从文档中建模主题的方法。我们使用一个简单的集群框架和上下文嵌入来进行主题建模，如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/5a7d7164190ec8bf263a5d6fac851337.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*2WR--arYO8MEoQ1kgJwFWA.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">我们方法的架构。图片作者。</p></figure><p id="b233" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">我们首先通过预先训练的语言模型对预处理的文档进行编码，以获得情境化的嵌入。之后，在应用聚类方法(例如，K-Means)来分组相似的文档之前，我们降低嵌入(例如，UMAP)的维度。每个群组将被视为一个主题。最后，我们采用加权的方法选择代表词作为主题。降低嵌入维度是可选的，但可以节省运行时间(更多细节见下文)。</p><p id="2039" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">这种基于聚类的主题建模范例并不新鲜。<a class="ae kr" href="https://arxiv.org/abs/2008.09470" rel="noopener ugc nofollow" target="_blank"> Top2vec </a> ⁵联合提取单词和文档向量，将每个密集区域的质心作为主题向量，n个最近的单词向量作为主题词；<a class="ae kr" href="https://arxiv.org/abs/2203.05794" rel="noopener ugc nofollow" target="_blank"> BERTopic </a> ⁶采用类似的方法，但是它使用提出的c-TF-IDF来选择每个聚类内的主题词；<a class="ae kr" href="https://aclanthology.org/2020.emnlp-main.135/" rel="noopener ugc nofollow" target="_blank"> Sia等人(2020) </a> ⁷聚类词汇级的单词嵌入，并使用加权和重新排序从每个聚类中获得顶部单词。</p><p id="8ad1" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">然而，上述方法忽略了最近提出的有前途的NTMs。基于聚类的主题模型的性能还没有与传统的主题模型进行比较。</p><h1 id="ff65" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">我们提议的工作</h1><p id="53bc" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">在这里，我们介绍一下我们的<strong class="lm ir"> <em class="ml"> NAACL 2022 </em> </strong>论文📄"<a class="ae kr" href="https://arxiv.org/abs/2204.09874" rel="noopener ugc nofollow" target="_blank"> <strong class="lm ir"> <em class="ml">神经话题建模比聚类好吗？主题</em></strong></a><em class="ml"/>和官方<a class="ae kr" href="https://github.com/hyintell/topicx" rel="noopener ugc nofollow" target="_blank"> Github </a>的语境嵌入聚类实证研究。据我们所知，我们是第一个使用各种基于transformer的模型产生的上下文嵌入来与ntm进行比较的。此外，我们提出了新的单词选择方法，该方法将全局单词重要性与每个聚类中的本地词频率相结合。</p><p id="c75f" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">我们以<code class="fe nv nw nx ny b"><a class="ae kr" href="https://archive.ics.uci.edu/ml/datasets/Health+News+in+Twitter" rel="noopener ugc nofollow" target="_blank">Health News in Twitter</a></code> ⁰数据集(从<a class="ae kr" href="https://archive.ics.uci.edu/ml/index.php" rel="noopener ugc nofollow" target="_blank"> UCI机器学习库</a>中检索)为例，说明<strong class="lm ir"> <em class="ml">范式2:基于聚类的主题建模</em> </strong>也能抽取高质量的主题。</p><h2 id="2916" class="nz kt iq bd ku oa ob dn ky oc od dp lc lt oe of le lx og oh lg mb oi oj li ok bi translated">1.数据准备</h2><p id="34e4" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">我们首先使用<a class="ae kr" href="https://github.com/MIND-Lab/OCTIS#datasets-and-preprocessing" rel="noopener ugc nofollow" target="_blank"> OCTIS </a>加载预处理过的数据集:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="1557" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">文档被标记化，给出了3929个预处理的句子:</p><pre class="kg kh ki kj gt on ny oo op aw oq bi"><span id="7259" class="nz kt iq ny b gy or os l ot ou">length of documents: 3929<br/>preprocessed documents: [<br/>  'breast cancer risk test devised',<br/>  'workload harming care bma poll',<br/>  'short people heart risk greater',<br/>  'new approach hiv promising',<br/>  'coalition undermined nhs doctors',<br/>  'review case nhs manager',<br/>  'day empty going',<br/>  'overhaul needed end life care',<br/>  'care dying needs overhaul',<br/>  'nhs labour tory key policies',<br/>  ...<br/>]</span></pre><h2 id="7a3c" class="nz kt iq bd ku oa ob dn ky oc od dp lc lt oe of le lx og oh lg mb oi oj li ok bi translated">2.把...嵌入</h2><p id="7943" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">其次，我们需要将这些文档转换成矢量表示。我们可以使用任何嵌入，这里，我们使用来自<a class="ae kr" href="https://github.com/princeton-nlp/SimCSE" rel="noopener ugc nofollow" target="_blank"> SimCSE </a>的预训练<code class="fe nv nw nx ny b">princeton-nlp/unsup-simcse-bert-base-uncased</code>嵌入。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ol om l"/></div></figure><h2 id="583f" class="nz kt iq bd ku oa ob dn ky oc od dp lc lt oe of le lx og oh lg mb oi oj li ok bi translated">3.减少和集群嵌入</h2><p id="ea83" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">现在，我们有了文档的嵌入表示，我们可以通过应用聚类方法将相似的文档分组在一起。减小嵌入大小可以有效地节省运行时间。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ol om l"/></div></figure><h2 id="d8fb" class="nz kt iq bd ku oa ob dn ky oc od dp lc lt oe of le lx og oh lg mb oi oj li ok bi translated">4.从聚类中选择主题词</h2><p id="5e92" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">最后，我们可以应用加权方法从每个聚类中选择主题词:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="80e9" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">推特中<em class="ml">健康新闻设置5个话题时的评价是:</em></p><pre class="kg kh ki kj gt on ny oo op aw oq bi"><span id="d8d6" class="nz kt iq ny b gy or os l ot ou">num_topics: 5 td: 1.0 npmi: 0.106 cv: 0.812</span></pre><p id="111a" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">示例主题包括:</p><pre class="kg kh ki kj gt on ny oo op aw oq bi"><span id="2c63" class="nz kt iq ny b gy or os l ot ou">Topic:<br/>0: ['nhs', 'amp', 'care', 'hospital', 'health', 'mental']<br/>1: ['ebola', 'vaccine', 'flu', 'liberia', 'leone', 'virus']<br/>2: ['dementia', 'obesity', 'alzheimer', 'diabetes', 'brain', 'disabled']<br/>3: ['cancer', 'heart', 'breast', 'surgery', 'transplant', 'lung']<br/>4: ['cigarette', 'cigarettes', 'pollution', 'smoking', 'sugar', 'drug']</span></pre><p id="8dba" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">我们可以看到<strong class="lm ir"> <em class="ml">范式2:基于聚类的主题建模</em> </strong>发现的主题也可以是高度连贯和多样的，虽然文档的长度相对较短。尽管所有话题都是关于健康的，但我们可以区分话题1是关于传染性病毒和疾病的，话题2是关于疾病症状的，等等。</p></div><div class="ab cl mm mn hu mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ij ik il im in"><p id="8e08" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">为了进行比较，我们还使用<a class="ae kr" href="https://github.com/MilaNLProc/contextualized-topic-models" rel="noopener ugc nofollow" target="_blank"> CombinedTM </a>，一个<strong class="lm ir"> <em class="ml">范例1:基于常规主题建模</em> </strong>的模型来提取Twitter 数据集中相同的<em class="ml">健康新闻。我们使用相同的<code class="fe nv nw nx ny b">princeton-nlp/unsup-simcse-bert-base-uncased</code>嵌入。</em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="0be3" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">对推特中<em class="ml">健康新闻的评价在设置5个话题时分别是:</em></p><pre class="kg kh ki kj gt on ny oo op aw oq bi"><span id="3d8c" class="nz kt iq ny b gy or os l ot ou">num_topics: 5 td: 1.0 npmi: -0.267 cv: 0.401</span></pre><p id="ef69" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">示例主题包括:</p><pre class="kg kh ki kj gt on ny oo op aw oq bi"><span id="874b" class="nz kt iq ny b gy or os l ot ou">Topic:<br/>0: ['cancer', 'amp', 'drug', 'death', 'audio', 'test']<br/>1: ['food', 'obesity', 'cigarette', 'smokers', 'link', 'smoking']<br/>2: ['ebola', 'mers', 'liberia', 'vaccine', 'malaria', 'virus']<br/>3: ['babies', 'brain', 'sperm', 'man', 'human', 'cell']<br/>4: ['nhs', 'health', 'mental', 'care', 'staff', 'hospital']</span></pre><p id="420e" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">我们可以看到，一些主题是不连贯的，例如，主题0和3。</p></div><div class="ab cl mm mn hu mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ij ik il im in"><p id="eedc" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">我们总结了基于聚类的主题建模的发现。</p><h2 id="8aa2" class="nz kt iq bd ku oa ob dn ky oc od dp lc lt oe of le lx og oh lg mb oi oj li ok bi translated">直接聚类高质量的嵌入可以产生好的主题。</h2><p id="fb67" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">实验表明，高质量的嵌入对于基于聚类的主题建模至关重要。我们实验了不同的嵌入，包括伯特、<a class="ae kr" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank">罗伯塔</a> ⁸、<a class="ae kr" href="https://arxiv.org/abs/2104.08821" rel="noopener ugc nofollow" target="_blank">辛姆塞</a> ⁹等。，基于三个不同长度的数据集。对RoBERTa进行聚类得到的结果与上下文化的ntm相似或更差，这表明嵌入质量很重要。</p><blockquote class="ov ow ox"><p id="18cc" class="lk ll ml lm b ln mg jr lp lq mh ju ls oy mi lv lw oz mj lz ma pa mk md me mf ij bi translated">最近的<a class="ae kr" href="https://github.com/voidism/diffcse" rel="noopener ugc nofollow" target="_blank"> <strong class="lm ir"> DiffCSE </strong> </a>在某些数据集上可以达到略高的性能！</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="pc pd di pe bf pf"><div class="gh gi pb"><img src="../Images/f19640b047f48768dc2737a030531fe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*woMaKC4ybhMyDezwGKt-fg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">前10个词的主题连贯性(NPMI和CV)和主题多样性(TU)的不同嵌入。图片由<a class="ae kr" href="https://arxiv.org/abs/2204.09874" rel="noopener ugc nofollow" target="_blank">纸</a></p></figure><h2 id="8add" class="nz kt iq bd ku oa ob dn ky oc od dp lc lt oe of le lx og oh lg mb oi oj li ok bi translated">选词方法至关重要。</h2><p id="bafb" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">一旦我们有了一组聚类的文档，选择有代表性的主题词对于识别主题的语义是至关重要的。与以前提出的方法不同，我们捕获每个聚类中的全局词重要性和本地词频率，并比较4种不同的方法:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/be0daac2df7565abc7da43ab91b4586c.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*bA3npJGYK-vQmTEtAReIYQ.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图片作者。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/5029150b5db6ad086460b43b729c36e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*0Yy4gJTClbvdpA1bbu4mFA.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图片作者。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/80ae707dbdcfb136bc40b75dcbd5b68e.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*U7DpHP8gy28TFiSQS4T19g.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图片作者。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/4da8c457564802600bb1277b1dedcdf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*at1JF6ac98pPZXarQTPQYg.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图片作者。</p></figure><p id="461c" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">我们发现，在所有方法中，TFTDF × IDFᵢ取得了明显更好的结果。这表明TFIDF标记出整个语料库中每个文档的重要单词，而IDFᵢ惩罚多个簇中的常见单词。相反，其他三种方法忽略了一个聚类中的频繁词也可能在其他聚类中流行，因此选择这样的词导致低主题多样性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="pc pd di pe bf pf"><div class="gh gi pk"><img src="../Images/40a2de4a8f038caca1a27420e66fa855.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FrI3CYQ4dOWsXOcaO3C5UQ.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">不同主题词选择方法的比较。图片由<a class="ae kr" href="https://arxiv.org/abs/2204.09874" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h2 id="99f4" class="nz kt iq bd ku oa ob dn ky oc od dp lc lt oe of le lx og oh lg mb oi oj li ok bi translated">嵌入维度对主题质量的影响可以忽略不计。</h2><p id="a67d" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">在聚类之前，我们应用UMAP来降低句子嵌入的维数。我们发现，在聚类之前降低维数对性能的影响可以忽略不计，但可以节省运行时间。</p><h1 id="0795" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">用法示例</h1><p id="7a60" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">如<a class="ae kr" href="https://github.com/hyintell/topicx" rel="noopener ugc nofollow" target="_blank"> Github </a>中所述，你可以从<code class="fe nv nw nx ny b">[tfidf_idfi, tfidf_tfi, tfidfi, tfi]</code>中选择一种选词方法。如果您不想使用UMAP减少嵌入维数，只需设置<code class="fe nv nw nx ny b">dim_size=-1</code>。您可以训练模型，并获得评估结果和主题:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="3d9d" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">预期的输出应该类似于:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ol om l"/></div></figure><h1 id="42fc" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">结论</h1><p id="3c5f" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">在这篇文章中，我们介绍了一种基于聚类的方法，只要使用高质量的上下文嵌入，就可以产生值得称赞的主题，以及一种合适的主题词选择方法。与神经主题模型相比,<br/>基于聚类的模型更加简单、高效，并且对各种文档长度和主题数量更加鲁棒，这可以作为一种替代方案<br/>应用于某些情况。</p></div><div class="ab cl mm mn hu mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ij ik il im in"><p id="a0a0" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">[1]:大卫·M·布雷，安德鲁·吴和迈克尔一世·乔丹。2003.潜在狄利克雷分配。机器学习研究杂志，3:993–1022。</p><p id="1244" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">[2]: Devlin，j .，Chang，M.W .，Lee，k .和Toutanova，k .，2018。Bert:用于语言理解的深度双向转换器的预训练。<em class="ml"> arXiv预印本arXiv:1810.04805 </em>。</p><p id="55a9" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">[3]:比安奇，女，特拉尼，s和霍维，d，2020。预训练是一个热门话题:语境化的文档嵌入提高了主题的连贯性。arXiv预印本arXiv:2004.03974 。</p><p id="29e3" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">[4]:比安奇，f .，特拉尼，s .，霍维，d .，诺扎，d .，费尔西尼，e .，2020。零镜头学习的跨语言语境化主题模型。arXiv预印本arXiv:2004.07737 。</p><p id="83ce" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">[5]:安杰洛夫特区，2020年。Top2vec:主题的分布式表示。<em class="ml"> arXiv预印本arXiv:2008.09470 </em>。</p><p id="d3aa" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">[6]:格罗腾多斯特，m，2022。BERTopic:使用基于类的TF-IDF过程的神经主题建模。<em class="ml"> arXiv预印本arXiv:2203.05794 </em>。</p><p id="baa5" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">[7]: Sia，s .，Dalmia，a .和Mielke，S.J .，2020。厌倦了话题模型？预先训练的单词嵌入集群也是快速和良好的主题！。arXiv预印本arXiv:2004.14914 。</p><p id="18db" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">[8]:刘，y .，奥特，m .，戈亚尔，n .，杜，j .，乔希，m .，陈，d .，列维，o .，刘易斯，m .，泽特勒莫耶，l .，斯托扬诺夫，v .，2019。Roberta:稳健优化的bert预训练方法。<em class="ml"> arXiv预印本arXiv:1907.11692 </em>。</p><p id="371a" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">[9]:高，汤，姚，陈，2021。Simcse:句子嵌入的简单对比学习。<em class="ml"> arXiv预印本arXiv:2104.08821 </em>。</p><p id="640e" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">[10]:卡拉米，a .，甘戈帕迪亚，a .，周，b .，&amp;哈拉齐，H. (2017)。健康和医学语料库中的模糊方法主题发现。国际模糊系统杂志，1–12。</p><p id="573d" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">[11]: Dua，d .和Graff，C. (2019)。UCI机器学习知识库[http://archive . ics . UCI . edu/ml]。加州欧文:加州大学信息与计算机科学学院。</p><p id="0b71" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">[12]:张，z，方，m，陈，l .，&amp;纳马齐-拉德，M. R. (2022)。神经主题建模比聚类好吗？基于主题上下文嵌入的聚类实证研究。<em class="ml"> arXiv预印本arXiv:2204.09874 </em>。</p></div></div>    
</body>
</html>