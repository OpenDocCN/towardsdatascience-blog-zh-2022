<html>
<head>
<title>Target-encoding Categorical Variables</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">目标编码分类变量</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dealing-with-categorical-variables-by-using-target-encoder-a0f1733a4c69#2022-03-17">https://towardsdatascience.com/dealing-with-categorical-variables-by-using-target-encoder-a0f1733a4c69#2022-03-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3b9b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一个很好的替代方法是一键编码你的类别</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/34cf4ed2971831d3907fe9931e20d394.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fu9aFquMfd5jrYtIsIz5ww.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@tengyart" rel="noopener ugc nofollow" target="_blank">腾雅特</a>在 Unsplash 上原创</p></figure><p id="c930" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">分类变量对机器学习算法来说是一个挑战。因为它们中的大多数(如果不是全部)只接受数值作为输入，所以我们需要将类别转换成数字，以便在模型中使用它们。</p><p id="70f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过一次性编码，我们创建了一个真正稀疏的矩阵，并增加了模型需要处理的维数，我们可能会成为可怕的维数灾难的受害者。当特征具有太多类别时，这种情况会被放大，其中大部分类别对于预测是无用的。</p><p id="bdd2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">处理这个问题的一个聪明的方法是目标编码器。</p><p id="accc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文中使用的代码和示例也可以在我的 GitHub 存储库中找到:</p><div class="ls lt gp gr lu lv"><a href="https://github.com/vinyluis/Articles/tree/main/Target%20Encoder" rel="noopener  ugc nofollow" target="_blank"><div class="lw ab fo"><div class="lx ab ly cl cj lz"><h2 class="bd ir gy z fp ma fr fs mb fu fw ip bi translated">物品/主要乙烯基/物品上的目标编码器</h2><div class="mc l"><h3 class="bd b gy z fp ma fr fs mb fu fw dk translated">文章:了解目标编码器[EN]介绍目标编码器背后的思想、代码示例和潜在的…</h3></div><div class="md l"><p class="bd b dl z fp ma fr fs mb fu fw dk translated">github.com</p></div></div><div class="me l"><div class="mf l mg mh mi me mj kp lv"/></div></div></a></div><h1 id="4478" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">目标编码器</h1><p id="0d69" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">目标编码器背后的主要思想是通过替换类别来对类别进行编码，以测量它们可能对目标产生的影响。</p><p id="b14d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在二元分类器上，最简单的方法是计算概率<strong class="ky ir"> <em class="nh"> p(t = 1 | x = ci) </em> </strong>，其中<strong class="ky ir"> <em class="nh"> t </em> </strong>表示目标，<strong class="ky ir"> <em class="nh"> x </em> </strong>是输入，<strong class="ky ir"> <em class="nh"> ci </em> </strong>是第 I 个类别。在贝叶斯统计中，假定输入是类别<strong class="ky ir"> <em class="nh"> ci </em> </strong>，这被认为是<strong class="ky ir"> <em class="nh"> t=1 </em> </strong>的后验概率。</p><p id="7f9c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这意味着我们将用类别<strong class="ky ir"> <em class="nh"> ci </em> </strong>替换目标的后验概率值为 1 的类别。</p><p id="0379" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">考虑下面的数据集:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/6654c5c4f027bb8af231e3d7ea265489.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/0*LCAi6UPOBxcPZph0"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="8e8c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于每一个可能的类别(非虚构、浪漫、戏剧、科幻和幻想)，我们需要计算目标 0 和目标 1 出现的次数。然后我们计算:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/903dd9fac7fa1925d63a5b676083890d.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*FIrSVBx4It6tJOYXcuL49g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="e60f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这可以通过下面的代码来完成:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="06a2" class="np ml iq nl b gy nq nr l ns nt">categories = df['genre'].unique()<br/>targets = df['target'].unique()<br/>cat_list = []<br/>for cat in categories:<br/>    aux_dict = {}<br/>    aux_dict['category'] = cat<br/>    aux_df = df[df['genre'] == cat]<br/>    counts = aux_df['target'].value_counts()<br/>    aux_dict['count'] = sum(counts)<br/>    for t in targets:<br/>        aux_dict['target_' + str(t)] = counts[t]<br/>    cat_list.append(aux_dict)</span><span id="a8dc" class="np ml iq nl b gy nu nr l ns nt">cat_list = pd.DataFrame(cat_list)<br/>cat_list['genre_encoded_dumb'] = cat_list['target_1'] / cat_list['count']</span></pre><p id="5f26" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以最后，<code class="fe nv nw nx nl b">cat_list</code>数据帧看起来会像这样:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/42237f8d00f0009bcb1658f3e45f571b.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/0*tHcC8UaIOFC5Y5Am"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="d76f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">既然感兴趣的目标是值“1”，那么这个概率实际上就是目标的<strong class="ky ir"> <em class="nh">均值</em> </strong>，给定一个类别。这就是为什么这种目标编码方法也被称为“平均”编码的原因。</p><p id="1703" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以用一个简单集合来计算这个平均值，然后:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="7357" class="np ml iq nl b gy nq nr l ns nt">stats = df['target'].groupby(df['genre']).agg(['count', 'mean'])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/94088d2724a1c0d8cead21ca486c0fd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/0*yiHF7-9KbCs5YoUV"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="a442" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就对了。变量及其各自的编码用一行代码表示。</p><p id="f1f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以用它们的编码值替换类别，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/4780321aa3533fd71c79d9c2cffb0f59.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/0*l-pluhZ4ZgirpPrQ"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="b6df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于我们使用每个类别的目标平均值，这种方法也很容易适用于回归模型。</p><h1 id="58f1" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">这种方法的问题</h1><p id="d2a7" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">这种编码方式真的很简单也很强大。然而，在使用它的时候，有一些重要的问题你需要记住。</p><p id="e400" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个真正重要的影响是<em class="nh">目标泄漏</em>。通过使用目标的概率对特征进行编码，我们向它们提供了我们试图建模的变量的信息。这就像“欺骗”,因为模型将从一个自身包含目标的变量中学习。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/5c10a28bd23e8e1756e2916fda488359.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*D2yvk2fftZ5N-oAa"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@zhenhappy" rel="noopener ugc nofollow" target="_blank">潘晓珍</a>在 Unsplash 上原创</p></figure><p id="8fe5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以认为这可能不是一个问题，如果这种编码反映了目标的真实概率，给定一个类别。但是如果是这样的话，我们甚至不需要一个模型。相反，我们可以使用这个变量作为这个目标的一个强大的预测器。</p><p id="8ea0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，使用平均值作为整个分布的预测值是好的，但并不完美。如果所有不同的数据分布和组合都可以用一个平均值来表示，我们的生活会轻松得多。</p><p id="0bf8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">即使平均值是一个很好的总结，我们也要在一小部分数据中训练模型。这个分数的平均值可能不是全部人口的平均值(还记得中心极限定理吗？)，所以编码可能不正确。如果样本与总体差异足够大，模型甚至可能会过度拟合训练数据。</p><h1 id="d71e" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">具有先验平滑的目标编码器</h1><p id="316d" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">我们可以使用先前的平滑来减少那些不想要的影响。</p><p id="64c0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个想法很简单。假设我们有一个模型来预测在线商店中一本书的质量。我们可能有一本书有 5 个评价，得到 9.8 分(满分 10 分)，但其他书的平均分数是 7 分。这种影响是因为我们使用的是小样本的平均值，与我上面提到的问题类似。</p><p id="e418" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过考虑所有书籍的平均值，我们可以用更少的评价来“平滑”这本书的分数。</p><p id="89f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">回到我们的例子，我们有 5 个类别要编码:非小说，浪漫，戏剧，科幻和幻想，我们已经知道如何使用每个类别的平均编码。现在我们可以使用目标<em class="nh">在所有类别</em>上的平均值来平滑每个类别的编码。</p><p id="c3bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们称目标的平均值为先验概率 p(t = 1)的 T6，编码可以使用从 0 到 1 的参数 T12、T13、α、T14、T15 来平衡这种平滑。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/c61267373cd195194ecf0edf0debd655.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*9Fxy5HYYG5wj8YgkgZ4pLQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="775c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通常，我们从下面的表达式中得到这个α:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/88f6d6846c5ba3f5029485aa639ca0cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:232/format:webp/1*hhukjbpKGANAnkudHBzckw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="ffbd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用先前平滑来执行编码的代码是:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="e262" class="np ml iq nl b gy nq nr l ns nt">smoothing_factor = 1.0 # The f of the smoothing factor equation <br/>min_samples_leaf = 1 # The k of the smoothing factor equation</span><span id="f01e" class="np ml iq nl b gy nu nr l ns nt">prior = df['target'].mean()<br/>smoove = 1 / (1 + np.exp(-(stats['count'] - min_samples_leaf) / smoothing_factor))<br/>smoothing = prior * (1 - smoove) + stats['mean'] * smoove<br/>encoded = pd.Series(smoothing, name = 'genre_encoded_complete')</span></pre><p id="8f76" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这改编自基于 sklearn 的类别编码器库<a class="ae kv" href="https://contrib.scikit-learn.org/category_encoders/targetencoder.html" rel="noopener ugc nofollow" target="_blank"/>。我们还可以使用该库进行编码，而无需手动操作:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="6c6d" class="np ml iq nl b gy nq nr l ns nt">from category_encoders import TargetEncoder<br/>encoder = TargetEncoder()<br/>df['genre_encoded_sklearn'] = encoder.fit_transform(df['genre'], df['target'])</span></pre><p id="abb8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有方法的结果如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/a2b129107111b0dc959f1a93a8be5fb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IPdkxtb7vc3OrM6w"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="4635" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以看到，虽然有平滑和没有平滑的方法之间存在一些差异，但它们仍然非常接近。</p><h1 id="ccaa" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">多类方法</h1><p id="1d16" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">到目前为止，我解释了二进制分类器的目标编码器，很容易理解我们如何使它适应回归。但是多类分类呢？</p><p id="60a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下面的数据集上，如果我们简单地取平均值，就会认为目标 2 是目标 1 的两倍大。此外，如果目标也是一个类别，我们将如何取平均值？</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/d7610c299deaa6b285ff08c0a49e5915.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/0*Coz1tkOiLoxLvS8O"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="a49c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">编码的结果很奇怪:非小说类的编码条件概率是 1.00，这显然是错误的，因为它可能有所有三个目标。</p><p id="0f67" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了使目标编码器适用于多类分类，我们需要对每个目标的特征进行独立编码。因此，让我们计算给定每个类别的每个目标的后验概率。</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="5ec4" class="np ml iq nl b gy nq nr l ns nt">categories = df['genre'].unique()<br/>targets = df['target'].unique()<br/>cat_list = []<br/>for cat in categories:<br/>    aux_dict = {}<br/>    aux_dict['category'] = cat<br/>    aux_df = df[df['genre'] == cat]<br/>    counts = aux_df['target'].value_counts()<br/>    aux_dict['count'] = sum(counts)<br/>    for t in targets:<br/>        aux_dict['target_' + str(t)] = counts[t] if t in counts.keys() else 0<br/>    cat_list.append(aux_dict)</span><span id="09cb" class="np ml iq nl b gy nu nr l ns nt">cat_list = pd.DataFrame(cat_list)</span><span id="cde8" class="np ml iq nl b gy nu nr l ns nt">for t in targets:<br/>    cat_list['genre_encoded_target_' + str(t)] = cat_list['target_' + str(t)] / cat_list['count']</span></pre><p id="2f64" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/2b859b70110e8d8ad7f20dcc2e63ffff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XERDQtgfHSRzxfNZ"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="b27a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如所料，所有的编码现在都正确地反映了后验性。甚至“浪漫”对于目标“1”也将被编码为“0”，因为它从未出现在该类别中。</p><p id="db22" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的代码也适用于分类目标。如果我们使用以下代码行更改目标:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="44eb" class="np ml iq nl b gy nq nr l ns nt">df['target'] = df['target'].replace({0: 'apple', 1: "banana", 2: 'orange'})</span></pre><p id="5f4e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果不会不同:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/a57e3fb2ed2070a19555e3c8ade4cdd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ITepXQsj12qXt0OM"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="f746" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">既然我们已经理解了在多类问题中使用目标编码需要做些什么，那么就很容易创建一个简单的代码来在这个场景中使用<code class="fe nv nw nx nl b">category_encoders.TargetEncoder</code>对象:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="a9e0" class="np ml iq nl b gy nq nr l ns nt">from category_encoders import TargetEncoder</span><span id="a194" class="np ml iq nl b gy nu nr l ns nt">targets = df['target'].unique()<br/>for t in targets:<br/>    target_aux = df['target'].apply(lambda x: 1 if x == t else 0)<br/>    encoder = TargetEncoder()<br/>    df['genre_encoded_sklearn_target_' + str(t)] = encoder.fit_transform(df['genre'], target_aux)</span></pre><p id="2a34" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">轻松搞定！两种方法的结果都足够好:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/bd4b77275dfee232fbde8abeb7f8ddd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_7r55YOGfD_9PY0xE-m9pw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="e3d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您熟悉 One-Hot 编码，您会知道现在可以删除任何编码列以避免多重共线性。</p><h1 id="6917" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">结论</h1><p id="5439" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">目标编码分类变量解决了我们通过使用一键编码得到的维数问题，但是这种方法需要小心使用以避免目标泄漏。</p><p id="f32e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您应该在您的模型上使用它，并将其与其他编码进行比较，以选择更适合您的情况的编码。</p><h1 id="a24c" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">如果你喜欢这个帖子…</h1><p id="823e" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">支持我一杯咖啡！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><a href="https://www.buymeacoffee.com/vinitrevisan"><div class="gh gi oj"><img src="../Images/acf4154cfebdc13859934db49fd502cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*h_y4o6IwDDFFWIyKQE7Rww.png"/></div></a><p class="kr ks gj gh gi kt ku bd b be z dk translated">给我买杯咖啡！</p></figure><p id="cf7c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看看这个很棒的帖子</p><div class="ls lt gp gr lu lv"><a rel="noopener follow" target="_blank" href="/evaluating-classification-models-with-kolmogorov-smirnov-ks-test-e211025f5573"><div class="lw ab fo"><div class="lx ab ly cl cj lz"><h2 class="bd ir gy z fp ma fr fs mb fu fw ip bi translated">用 Kolmogorov-Smirnov (KS)检验评估分类模型</h2><div class="mc l"><h3 class="bd b gy z fp ma fr fs mb fu fw dk translated">使用 KS 检验评估类分布之间的分离</h3></div><div class="md l"><p class="bd b dl z fp ma fr fs mb fu fw dk translated">towardsdatascience.com</p></div></div><div class="me l"><div class="ok l mg mh mi me mj kp lv"/></div></div></a></div><h1 id="f91c" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">参考</h1><p id="0eea" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated"><a class="ae kv" href="https://contrib.scikit-learn.org/category_encoders/targetencoder.html" rel="noopener ugc nofollow" target="_blank">https://contrib . sci kit-learn . org/category _ encoders/target encoder . html</a></p><p id="d7d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://maxhalford.github.io/blog/target-encoding/" rel="noopener ugc nofollow" target="_blank">https://maxhalford.github.io/blog/target-encoding/</a></p><p id="857e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://dl.acm.org/doi/10.1145/507533.507538" rel="noopener ugc nofollow" target="_blank">https://dl.acm.org/doi/10.1145/507533.507538</a></p></div></div>    
</body>
</html>