<html>
<head>
<title>How to Build a Neural Network for NLP Tasks with PyTorch and GPU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用 PyTorch 和 GPU 构建用于 NLP 任务的神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-a-neural-network-for-nlp-tasks-with-pytorch-and-gpu-da542591dbe1#2022-11-22">https://towardsdatascience.com/how-to-build-a-neural-network-for-nlp-tasks-with-pytorch-and-gpu-da542591dbe1#2022-11-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bdb9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 Google Colab 对文本数据建模的框架</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2af4ca8b591276b2d1f1e89de35fc948.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pyUdXC1KPVvLtMoiNdCwdQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">斯蒂夫·约翰森摄:<a class="ae ky" href="https://www.pexels.com/photo/blue-red-and-black-abstract-painting-2130475/" rel="noopener ugc nofollow" target="_blank">https://www . pexels . com/photo/blue-red-and-black-abstract-painting-2130475/</a></p></figure><p id="4c57" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> T </span>他的文章旨在作为一个框架，使用 Python、神经网络和 GPU 来解决自然语言处理(NLP)问题。虽然理解这里的一切需要很高的数学水平，但我写这篇文章的目的是让一个完全的初学者或经验丰富的老手从中有所收获。我希望一个初学者带走解决 NLP 问题的一般框架，而一个老手可能使用它作为在 PyTorch 中将模型推送到 GPU 的参考。</p><p id="3cca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们开始吧！</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h2 id="7a52" class="ml mm it bd mn mo mp dn mq mr ms dp mt li mu mv mw lm mx my mz lq na nb nc nd bi translated">NLP:数据和预处理</h2><p id="2b2e" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">尽管数据清理不是建模过程中最令人兴奋的方面，但它无疑是最重要的。数据清理和预处理指的是将原始文本数据结构化和格式化成我们可以建模的东西。对这一过程的关注可以成就或破坏后续模型的统计推断。我们将使用来自 NLTK(用于 NLP 的 Python 库)语料库模块的数据，该模块包括电影评论及其<em class="nj">真相</em>分类(无论评论是正面还是负面)。让我们首先导入我们将需要的模块并下载必要的数据…</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">该项目的相关进口报表</p></figure><p id="e5f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 NLP 中有很多被抛弃的术语——让我们介绍一些更相关的术语…</p><ul class=""><li id="6840" class="nm nn it lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated"><strong class="lb iu">文集</strong>—<em class="nj">文档的集合</em></li><li id="4f34" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><strong class="lb iu">文档</strong> —来自<em class="nj">语料库</em>的文本主体</li><li id="12fe" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><strong class="lb iu">令牌</strong>—<em class="nj">文档中的单词列表</em></li><li id="92f4" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><strong class="lb iu">词汇</strong> —词汇</li></ul><p id="3263" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用粗笔画，所有包含#ElonMusk 的 tweets 就是一个<em class="nj">语料库</em>的例子。包含#ElonMusk 的所有 tweet 集合中的每个 tweet 都是一个<em class="nj">文档</em>。因此，每个文档中的每个单词都是一个<em class="nj">令牌</em>。这个环境中的词汇属于社交词汇；也就是使用#ElonMusk 时在社交语境中使用的词汇。</p><p id="e6a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了开始 Python 中的预处理，我们将创建两个列表。第一个列表包含电影评论语料库中的每个文档。第二个列表包含基本事实，一个对应于评论是正面还是负面的值(“正面”或“负面”)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">创建文档及其各自类别的列表</p></figure><p id="3748" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看一下每个列表中的单个元素，这样我们就可以更好地了解我们正在处理的数据…</p><p id="6b9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">文件[0]: </strong></p><pre class="kj kk kl km gt oa ob oc bn od oe bi"><span id="066f" class="of mm it ob b be og oh l oi oj">"plot : two teen couples go to a church party..."</span></pre><p id="4e11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是<em class="nj">文档</em>列表中的众多电影评论之一。</p><p id="ce19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">猫[0]: </strong></p><pre class="kj kk kl km gt oa ob oc bn od oe bi"><span id="62a6" class="of mm it ob b be og oh l ok oj">0</span></pre><p id="3a7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是与上述电影评论相对应的类别(负面)。请注意，在代码中，我们是如何将负数(' neg ')转换为 0，将正数(' pos ')转换为 1 的——这对于我们的神经网络模型是必要的。</p><p id="688c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的目标是建立一个神经网络，可以学习电影评论中常用的词，这些词可以是积极的，也可以是消极的。在此之前，我们需要清理一下电影评论文本。</p><p id="a4e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们构建一个函数来清理和预处理每个文档。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">预处理和清洗改编自<a class="ae ky" href="https://www.datacamp.com/tutorial/discovering-hidden-topics-python" rel="noopener ugc nofollow" target="_blank">数据营</a></p></figure><p id="2be4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">再一次，让我们分解一些语言学术语…</p><ul class=""><li id="7275" class="nm nn it lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated"><strong class="lb iu">令牌化</strong> —将<em class="nj">文档</em>转换为<em class="nj">令牌列表</em></li><li id="ccd6" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><strong class="lb iu">词干化</strong>——孤立词根(<em class="nj">示教</em> = <em class="nj">示教</em>)</li><li id="38e8" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><strong class="lb iu">终止词</strong>——无信息贡献的词(<em class="nj"> the </em>、<em class="nj">和</em>、<em class="nj">同为</em>)</li></ul><p id="8346" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nj">预处理</em>功能为每个文档建立一个<em class="nj">标记器</em>和一个<em class="nj">词干器</em>。我们通过遍历列表中的每一个文档，并从删除所有标点符号和减少所有单词开始。在某些情况下，我们需要考虑不同的情况，但为了简单起见，我们将'<em class="nj"> GREAT' </em>和'<em class="nj"> great </em>视为同一事物。</p><p id="2505" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将标记器应用于文档，创建一个单词列表，在其中应用<em class="nj">停止单词</em>移除和词干。如果这个列表中有一个单词存在于 NLTK 预定义的停止单词列表中，我们会将其从单词列表中删除。这里的目标是开发尽可能小的单词列表，同时保持尽可能多的关于原始文档的信息。换句话说，如果我们有一长串不相关的信息，我们会给我们的模型带来不必要的噪音，让它更难学习。例如，“<em class="nj">”</em>很可能出现在正面和负面的电影评论中。因此，在清理后的文本中包含“<em class="nj">”和“</em>”不会帮助我们的模型学习。</p><p id="e540" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们将把剩下的单词“T42”转换成“教学”。<em class="nj">“-ing”</em>结尾是为了语法目的而存在的，但是这个词的词根意义没有改变，所以我们打算以类似的方式在所有文档中对其进行建模。再一次，我们想最小化我们的模型面临的噪音，所以它有最好的学习机会。</p><p id="6c03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然我们已经预处理了文档，我们需要开发一个文档术语矩阵，即所谓的<em class="nj">文档术语矩阵</em>。这听起来可能很复杂，但它实际上是一个文档矩阵和每个文档中的术语。也许更合适的名称是<em class="nj">文档-令牌矩阵</em>，因为每行对应一个文档，每列对应一个令牌以及它在文档中出现的次数。这意味着对于所有文档中的每一个标记，都有一个用于<em class="nj">的列，其中矩阵中的每个值都对应于它在该文档中出现的次数。</em></p><p id="b227" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们为预处理的电影评论数据建立一个文档术语矩阵。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">创建文档术语矩阵并将其转换为数据框架</p></figure><p id="0a1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们准备开始构建我们的语言模型。我想强调的是，我们仍然可以做大量的事情来进一步预处理这些数据。在继续构建模型之前，我们可以应用降维技术、聚类算法等等，从我们的文档(文档术语矩阵)中进一步提取有意义的信息。虽然深入探讨降维技术超出了本文的范围，但我想留下一些我创建的资源来进行降维主题的教育…</p><ul class=""><li id="2b02" class="nm nn it lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated"><a class="ae ky" href="https://github.com/Quant-Guild/PCA_Tutorial" rel="noopener ugc nofollow" target="_blank">主成分分析</a></li><li id="ba86" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/singular-value-decomposition-158469b433ad">奇异值分解</a></li></ul><p id="534f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事不宜迟，让我们进入有趣的部分，构建模型。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h2 id="1b84" class="ml mm it bd mn mo mp dn mq mr ms dp mt li mu mv mw lm mx my mz lq na nb nc nd bi translated">NLP:使用 GPU 在 PyTorch 中构建神经网络模型</h2><p id="24dc" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">在本节中，我们将构建神经网络来学习如何对电影评论进行分类。我们还将学习如何使用 PyTorch 和 GPU，通过并行化来提高训练速度。我推荐使用 Google Colab 和他们的免费 GPU 层来开始。如果您对直观理解并行化及其如何加速计算感兴趣，请查看下一篇文章的直觉部分。</p><ul class=""><li id="e437" class="nm nn it lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated"><a class="ae ky" href="https://medium.com/quant-guild/parallelizing-randomized-singular-value-decomposition-using-gpus-62dff9e0c945" rel="noopener">随机化奇异值分解</a></li></ul><p id="8f2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练任何机器学习或人工智能的第一步是将数据分割成<em class="nj">训练</em>和<em class="nj">测试集</em>。我不会在模型开发的哲学上花太多时间，我们将一起构建每个组件。如果你有兴趣更深入地理解我通常所说的建模艺术，请随意阅读以下文章…</p><ul class=""><li id="9761" class="nm nn it lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated"><a class="ae ky" href="https://medium.com/swlh/artificial-intelligence-bootcamp-8745d61a9d25" rel="noopener">人工智能训练营</a></li><li id="35f7" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/how-do-machines-learn-e3b023bfa28d">机器如何学习？</a></li><li id="ebe2" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/linear-regression-v-s-neural-networks-cd03b29386d4">线性回归与神经网络</a></li></ul><p id="2c27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们将数据分成训练集和测试集。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">为我们的分类模型创建电影评论的训练和测试集</p></figure><p id="7314" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们已经准备好建立我们的分类模型了。我非常支持边做边学(在这里是编码)。然而，理解构成人工智能模型的基础数学是有好处的。让我们从代码开始，提供一些关于我们所做的数学选择的直觉。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PyTorch GPU 二进制分类模型</p></figure><p id="5972" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，神经网络最重要的功能选择是输出层的激活功能。摆在我们面前的任务是一个二进制分类任务，因此我们需要将一个输入分类为两个类别之一(<em class="nj"> 1 或 0，正或负</em>)。我们选择一个<em class="nj"> sigmoid </em>激活函数来做这件事。该激活函数将其输入压缩到对应于电影评论是否正面的概率中。如果概率大于 50%,我们会说电影评论是正面的，否则我们会说它是负面的。我们的神经网络将<em class="nj">通过调整其权重来学习</em>输入到概率的正确映射，以改进其对训练集中的基本事实的预测。一旦我们训练了我们的模型，我们将用测试集来测试它；我们这样做是为了确保我们的模型不会<em class="nj">记忆</em> ( <em class="nj">过度拟合</em>)训练集中的观察值。</p><p id="854e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，在上面的代码块中，我们包含了一个语句<em class="nj">。到(' cuda')' </em>。这会将模型推送到 GPU，因此当开始训练过程时，它会自动并行化。这将显著提高培训过程的绩效。</p><p id="032b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nj">注意:使用 GPU 时最常见的错误之一是在使用 GPU 时试图访问存储在 CPU 中的内存，反之亦然。永远记住变量在内存中的存储位置。</em></p><p id="a2c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在准备使用训练数据来训练模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PyTorch GPU 模型培训</p></figure><p id="dfc9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在模型被充分训练后，我们需要测试它的表现如何<em class="nj">样本外</em>。我们将使用我们的模型<em class="nj">尚未见过的测试数据</em>来确定我们的模型对新数据的分类能力。这将确保我们的神经网络模型实际上是在<em class="nj">学习</em>一些东西，而不仅仅是<em class="nj">过度拟合</em>训练数据。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">创建一个二元测试集来与基本事实进行比较</p></figure><pre class="kj kk kl km gt oa ob oc bn od oe bi"><span id="6f27" class="of mm it ob b be og oh l ok oj">print(classification_report(y_hat_int, y_test))</span></pre><pre class="ol oa ob oc bn od oe bi"><span id="5942" class="of mm it ob b be og oh l ok oj">               precision    recall  f1-score   support<br/><br/>           0       0.87      0.88      0.88       258<br/>           1       0.87      0.86      0.87       242<br/><br/>    accuracy                           0.87       500<br/>   macro avg       0.87      0.87      0.87       500<br/>weighted avg       0.87      0.87      0.87       500</span></pre><p id="0f35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用分类报告功能，我们可以比较模型的预测和实际情况，以确定模型的准确性。</p><p id="3e63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的打印输出告诉我们，我们的模型在确定哪些电影评论是正面的，哪些是负面的方面做得非常好！</p><p id="dd13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nj">一些最后的想法… </em></p><p id="946c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们观察到相对平衡的支持，测试集中正面和负面评论的数量。需要注意的是，在我们的训练和测试集中，我们通常希望有一个像这样平衡的类表示。如果类别不平衡，我们的模型可以<em class="nj">学习</em>猜测出现最多的类别，这意味着它真的没有学到任何东西。</p><p id="55b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这在异常检测问题(如洗钱和医疗保险欺诈)中很常见，因为异常的发生通常只占数据的一小部分(比如 0.01%)。因此，一个模型仅仅通过猜测出现最多的类就可以获得大约 99.99%的准确率。</p><p id="e71b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，这是一个有趣的问题，我们可以在另一篇文章中探讨！</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h2 id="bc72" class="ml mm it bd mn mo mp dn mq mr ms dp mt li mu mv mw lm mx my mz lq na nb nc nd bi translated"><strong class="ak">参考文献</strong></h2><div class="om on gp gr oo op"><a href="https://www.kaggle.com/datasets/nltkdata/movie-review" rel="noopener  ugc nofollow" target="_blank"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd iu gy z fp ou fr fs ov fu fw is bi translated">电影评论</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">情感极性数据集版本 2.0</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">www.kaggle.com</p></div></div><div class="oy l"><div class="oz l pa pb pc oy pd ks op"/></div></div></a></div><pre class="kj kk kl km gt oa ob oc bn od oe bi"><span id="670c" class="of mm it ob b be og oh l oi oj">Bo Pang and Lillian Lee. 2004. A Sentimental Education: Sentiment Analysis <br/>Using Subjectivity Summarization Based on Minimum Cuts. In ACL.</span></pre></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="4a47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非常感谢您的阅读！我希望你喜欢这篇文章——如果你有任何问题，请随时发表评论或联系我们:<em class="nj">roman@quantguild.com</em>。</p><p id="a51c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你有兴趣了解更多，请查看<a class="ae ky" href="https://quantguild.com/" rel="noopener ugc nofollow" target="_blank"> Quant Guild </a>！</p><p id="7976" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Quant Guild 是我创办的一所在线学校，你可以在那里学习包括数学、统计学、计算机科学和数据科学在内的主题。</p><p id="b4ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你对更多<strong class="lb iu">免费课程和资源</strong>感兴趣，或者想在 Quant Guild 支持我们，你可以看看我们的…</p><ul class=""><li id="5711" class="nm nn it lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated"><a class="ae ky" href="https://github.com/Quant-Guild" rel="noopener ugc nofollow" target="_blank"> GitHub </a></li><li id="39e5" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><a class="ae ky" href="https://www.youtube.com/quantguild" rel="noopener ugc nofollow" target="_blank"> YouTube </a></li><li id="ebd2" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><a class="ae ky" href="https://medium.com/quant-guild" rel="noopener">中等</a></li><li id="5140" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><a class="ae ky" href="https://www.patreon.com/quantguild" rel="noopener ugc nofollow" target="_blank">帕特里翁</a></li><li id="6eca" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><a class="ae ky" href="https://quantguild.com/" rel="noopener ugc nofollow" target="_blank">网站</a></li><li id="82e4" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><a class="ae ky" href="https://www.linkedin.com/company/76565075/admin/" rel="noopener ugc nofollow" target="_blank">领英</a></li></ul><p id="4fea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除此之外，下期再见！</p><p id="a9a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> RMP </strong></p></div></div>    
</body>
</html>