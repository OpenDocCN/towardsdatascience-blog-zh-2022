<html>
<head>
<title>Feature Importance in Machine Learning, Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的特征重要性，已解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-importance-in-machine-learning-explained-443e35b1b284#2022-11-03">https://towardsdatascience.com/feature-importance-in-machine-learning-explained-443e35b1b284#2022-11-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="eb40" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用SHAP和Sci-Kit Learn识别与Python中的模型相关的重要特性</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0868e3936f7eeac6619c6b06db17351a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5o5gEbUDXAaY75EV"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://unsplash.com/@joshgmit" rel="noopener ugc nofollow" target="_blank">约书亚·戈德</a>从<a class="ae ky" href="https://unsplash.com/photos/qIu77BsFdds" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="0e12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文将重点介绍与各种方法相关的直觉和Python实现，以识别与机器学习模型相关的重要特性。以下是文章的结构。</p><p id="e118" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">目录</strong></p><ul class=""><li id="0c6a" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">机器学习中的特征重要性是什么？</li><li id="9f8d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">基于系数的特征重要性</li><li id="7107" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">置换特征重要性</li><li id="99c6" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">树特征重要性</li><li id="ef96" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">SHAP特征重要性</li><li id="7317" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">实现<br/> -需求<br/> -合成数据&amp;生成模型<br/> -系数<br/> -排列<br/> -树<br/> - SHAP</li><li id="a6b2" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">结束语</li><li id="5429" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">资源</li></ul><h1 id="9d51" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">机器学习中的特征重要性是什么？</h1><p id="37cb" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">特征重要性是模型开发中不可或缺的组成部分。它突出显示了传递到模型中的哪些要素对生成预测的影响程度高于其他要素。识别重要特征的结果可以直接反馈到模型测试和模型可解释性中。有多种方法可以计算特征的重要性，例如:</p><ol class=""><li id="07c8" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ng mb mc md bi translated">基于系数的特征重要性</li><li id="9baa" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ng mb mc md bi translated">基于排列的特征重要性</li><li id="1b6b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ng mb mc md bi translated">树特征重要性</li><li id="e36b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ng mb mc md bi translated">SHAP</li></ol><p id="c015" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，并非所有计算特征重要性的方法都适用于所有类型的模型。这些方法主要适用于监督经典机器学习问题(如分类和回归)中的大多数模型。</p><h1 id="1cac" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">基于系数的特征重要性</h1><p id="f9c8" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">基于系数的特征重要性可能是最容易理解的。直观上，基于系数的特征重要性指的是将预测生成为输入值的加权和的模型。</p><p id="4401" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并非所有模型都可以计算特征的重要性。这适用于线性模型，如线性回归、逻辑回归、岭回归、支持向量机(仅当核是线性时)。这些类型的模型的主要共同点是它们识别与一组系数相关联的权重，我们可以将其解释为特征重要性。您可以从高到低对与这些系数相关的特征进行排序，最高的是最重要的特征，最低的是最不重要的特征。</p><h1 id="037b" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">置换特征重要性</h1><blockquote class="nh ni nj"><p id="0695" class="kz la nk lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated">基于置换的特征重要性被定义为当单个特征值被随机打乱时模型得分的减少。该过程打破了特征和目标之间的关系，因此模型分数的下降指示了模型对特征的依赖程度。[2]</p><p id="5783" class="kz la nk lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated"><a class="ae ky" href="http://scikit-learn.org/stable/modules/permutation_importance.html" rel="noopener ugc nofollow" target="_blank">-http://sci kit-learn . org/stable/modules/permutation _ importance . html</a></p></blockquote><p id="7842" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">置换特征重要性的结果可以是负的和正的。当置换分数对于某些特征为负时，它表明从混洗数据生成的预测碰巧比真实数据更准确。当模型认为特性应该具有重要性，但却没有时，就会发生这种情况。发生这种情况是因为随机机会导致从混洗数据生成的预测更加准确。这意味着这些特征有噪声。</p><h1 id="4fdf" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">树特征重要性</h1><p id="625d" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">sci-kit中基于树的模型学习，如决策树、随机森林、梯度增强、ada增强等。嵌入了它们自己的特性重要性。他们根据用于选择分裂点的标准(如基尼系数或熵)的减少来计算他们的重要性分数[1]。可以通过训练模型后可用的<code class="fe no np nq nr b">feature_importances_</code>属性引用这些分数。</p><h1 id="0c45" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">SHAP特征重要性</h1><p id="cd87" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">SHAP是一个流行的研究论文，重点是模型的可解释性。SHAP特征重要性是置换特征重要性的替代方法[3]。排列法和SHAP的区别在于，SHAP着眼于特征属性的大小，而排列法着眼于模型性能的下降[3]。SHAP图书馆内置了一系列的<code class="fe no np nq nr b">explainer</code>类。它支持线性模型、内核、树、深度学习模型等的可解释性。</p><h1 id="9069" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">履行</h1><h2 id="6d3d" class="ns mk it bd ml nt nu dn mp nv nw dp mt li nx ny mv lm nz oa mx lq ob oc mz od bi translated">要求</h2><pre class="kj kk kl km gt oe nr of og aw oh bi"><span id="304a" class="ns mk it nr b gy oi oj l ok ol">Python=3.9.12<br/>pandas&gt;=1.4.3<br/>numpy&gt;=1.23.2<br/>shap&gt;=0.41.0<br/>matplotlib&gt;=3.5.1<br/>sklearn&gt;=1.1.2</span></pre><h2 id="0718" class="ns mk it bd ml nt nu dn mp nv nw dp mt li nx ny mv lm nz oa mx lq ob oc mz od bi translated">综合数据并生成模型</h2><p id="24b1" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">请注意，因为我们使用的是随机生成的数据，所以特性重要性的结果几乎没有意义。这只是为了展示如何通过代码为您正在处理的问题实现这些不同类型的特性重要性度量。下面的脚本将合成一个具有5个特征的随机数据集，并从中训练2个基于回归的模型。即梯度推进回归器和支持向量回归器。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><h2 id="0487" class="ns mk it bd ml nt nu dn mp nv nw dp mt li nx ny mv lm nz oa mx lq ob oc mz od bi translated">系数</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/8d69fea4bbdcfc628a700f9e1cc194f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CXHczviZX2IwqF0sdjrTFg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">系数特征重要性。图片由作者提供。</p></figure><h2 id="f48b" class="ns mk it bd ml nt nu dn mp nv nw dp mt li nx ny mv lm nz oa mx lq ob oc mz od bi translated">排列</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/a81ebe2c56c2fa2c0068de5183eec7b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1qJerJUR95OBZfWPk_WB6w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">置换特征重要性。图片由作者提供。</p></figure><h2 id="a5f5" class="ns mk it bd ml nt nu dn mp nv nw dp mt li nx ny mv lm nz oa mx lq ob oc mz od bi translated">树</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/f1a02a298f37f3300e8e89590728d183.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wfZR3-5qsp1bY5qC_TOJmA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基于树的特征重要性。图片由作者提供。</p></figure><h2 id="5a41" class="ns mk it bd ml nt nu dn mp nv nw dp mt li nx ny mv lm nz oa mx lq ob oc mz od bi translated">SHAP</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/58cc2e232177e2f4b776398f4cc3427b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uqcM6NUDwjfeYPtoghoxpQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线性核SVM回归模型的SHAP特征重要性。图片由作者提供。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/12ad2501177676d96dfa04f2263ffc30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5WjVHtlo5FI5rdgysqLzJA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">SHAP特征对梯度推进回归器的重要性。图片由作者提供。</p></figure><p id="2902" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SHAP库中的汇总绘图功能允许您根据SHAP值直观地查看模型的最重要功能。第一个图显示了每个要素的影响分布，而第二个图是由SHAP值的MAE(平均绝对值)生成的条形图。</p><h1 id="4978" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">结束语</h1><p id="5043" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">基于对识别特征重要性的各种方法如何与模型的训练数据相关联的直观理解，以及所提供的相关代码，您应该能够为您正在处理的模型实现这一点，以查看哪些特征具有最高的影响。这对于模型测试和可解释性非常有用。</p><p id="96e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请在我的GitHub页面<a class="ae ky" href="https://github.com/vatsal220/medium_articles/blob/main/feature_importance/feature_importance.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>查看与本教程相关的资源库。</p><p id="b18c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想转型进入数据行业，并希望得到经验丰富的导师的指导和指引，那么你可能想看看最敏锐的头脑。Sharpest Minds是一个导师平台，导师(他们是经验丰富的实践数据科学家、机器学习工程师、研究科学家、首席技术官等。)将有助于你的发展和学习在数据领域找到一份工作。在这里查看<a class="ae ky" href="https://www.sharpestminds.com/?r=vatsal-patal" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="5ae4" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">资源</h1><ul class=""><li id="af97" class="lv lw it lb b lc nb lf nc li os lm ot lq ou lu ma mb mc md bi translated">[1]<a class="ae ky" href="https://machinelearningmastery.com/calculate-feature-importance-with-python/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/calculate-feature-importance-with-python/</a></li><li id="5bf3" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">[2]<a class="ae ky" href="http://scikit-learn.org/stable/modules/permutation_importance.html" rel="noopener ugc nofollow" target="_blank">http://sci kit-learn . org/stable/modules/permutation _ importance . html</a></li><li id="5de1" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">[3]<a class="ae ky" href="https://christophm.github.io/interpretable-ml-book/shap.html#:~:text=SHAP%20feature%20importance%20is%20an,on%20magnitude%20of%20feature%20attributions" rel="noopener ugc nofollow" target="_blank">https://christophm . github . io/interpretable-ml-book/shap . html</a></li></ul></div><div class="ab cl ov ow hx ox" role="separator"><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa"/></div><div class="im in io ip iq"><p id="3761" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你觉得这篇文章有用，这里有一些我写的其他文章，你可能也会觉得有用。</p><div class="pc pd gp gr pe pf"><a rel="noopener follow" target="_blank" href="/word2vec-explained-49c52b4ccb71"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd iu gy z fp pk fr fs pl fu fw is bi translated">Word2Vec解释道</h2><div class="pm l"><h3 class="bd b gy z fp pk fr fs pl fu fw dk translated">解释Word2Vec的直观性&amp;用Python实现它</h3></div><div class="pn l"><p class="bd b dl z fp pk fr fs pl fu fw dk translated">towardsdatascience.com</p></div></div><div class="po l"><div class="pp l pq pr ps po pt ks pf"/></div></div></a></div><div class="pc pd gp gr pe pf"><a rel="noopener follow" target="_blank" href="/link-prediction-recommendation-engines-with-node2vec-c97c429351a8"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd iu gy z fp pk fr fs pl fu fw is bi translated">使用Node2Vec的链接预测推荐引擎</h2><div class="pm l"><h3 class="bd b gy z fp pk fr fs pl fu fw dk translated">在Python中使用节点嵌入进行链接预测</h3></div><div class="pn l"><p class="bd b dl z fp pk fr fs pl fu fw dk translated">towardsdatascience.com</p></div></div><div class="po l"><div class="pu l pq pr ps po pt ks pf"/></div></div></a></div><div class="pc pd gp gr pe pf"><a rel="noopener follow" target="_blank" href="/recommendation-systems-explained-a42fc60591ed"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd iu gy z fp pk fr fs pl fu fw is bi translated">推荐系统解释</h2><div class="pm l"><h3 class="bd b gy z fp pk fr fs pl fu fw dk translated">用Python解释和实现基于内容的协同过滤和混合推荐系统</h3></div><div class="pn l"><p class="bd b dl z fp pk fr fs pl fu fw dk translated">towardsdatascience.com</p></div></div><div class="po l"><div class="pv l pq pr ps po pt ks pf"/></div></div></a></div><div class="pc pd gp gr pe pf"><a rel="noopener follow" target="_blank" href="/text-summarization-in-python-with-jaro-winkler-and-pagerank-72d693da94e8"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd iu gy z fp pk fr fs pl fu fw is bi translated">用Jaro-Winkler和PageRank实现Python中的文本摘要</h2><div class="pm l"><h3 class="bd b gy z fp pk fr fs pl fu fw dk translated">用Jaro-Winkler和PageRank构建一个文本摘要器</h3></div><div class="pn l"><p class="bd b dl z fp pk fr fs pl fu fw dk translated">towardsdatascience.com</p></div></div><div class="po l"><div class="pw l pq pr ps po pt ks pf"/></div></div></a></div></div></div>    
</body>
</html>