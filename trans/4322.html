<html>
<head>
<title>Language Modeling with LSTMs in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch中LSTMs的语言建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/language-modeling-with-lstms-in-pytorch-381a26badcbf#2022-09-25">https://towardsdatascience.com/language-modeling-with-lstms-in-pytorch-381a26badcbf#2022-09-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7ec6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">“学习如何在这种类型中保护他们的感情，这本身就是一个自信的例子，”这位模特说。</h2></div><p id="2ac9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在前三个故事中，我们从理论角度讨论了很多关于RNNs和LSTMs的内容。在这个故事中，我们将通过在PyTorch中使用LSTMs实现一个英语语言模型来弥合实践之间的差距。</p><h2 id="9a75" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">什么是语言模型？</h2><p id="4e2a" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">语言模型是已经学会估计记号序列的概率的模型。它可以使用这些事实来执行序列生成。也就是说，<strong class="kh ir">给定任何初始令牌集</strong>，它可以预测在所有可能的令牌中，<strong class="kh ir">最有可能跟随</strong>的是哪个<strong class="kh ir">令牌</strong>。这意味着，我们可以给模型一个单词，并让它生成一个完整的序列，方法是让它重复生成下一个单词，给出目前为止的前一个单词。</p><p id="75e9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">给定“这是”的初始表征，模型将以如下方式进行推理:</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi lz"><img src="../Images/64f3f6f3aecab25b748a12208e802aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2mL1DWifrBig6mTepKxIDQ.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">这张图片和所有其他图片都是作者的</p></figure><p id="05da" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，一旦模型产生一个特殊的句子结束标记<eos>，我们就停止给它输入前一个标记。这假设模型是在所有以这个标记结尾的序列上训练的，所以它知道什么时候输出它。另一种方法是仅在模型生成大量令牌后停止模型。</eos></p><p id="86e9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可能已经猜到，语言模型的一个应用是自动完成。他们可以学习帮你写电子邮件、代码、莎士比亚的书，只要你训练他们。</p><h2 id="7155" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">我们如何建立一个语言模型？</h2><p id="34e6" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">任何语言模型都与一个<strong class="kh ir">词汇表</strong>相关联，它从这个词汇表中抽取标记。对于英语单词级语言建模，如果这样的词汇表有100K个单词，那么给定任何初始单词集，模型必须选择哪一个(在100K个中)来预测为下一个单词。</p><p id="39bb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们可以通过使用具有分类头的LSTM网络来建立语言模型。也就是说，输出层应该是一个Softmax，它为词汇表中的每个单词分配一个概率。然后，可以通过选择与最高概率相关联的预测来为下一个字选择最佳预测，或者更经常地，只是随机采样Softmax输出分布。</p><p id="2bc3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了训练模型，我们所需要的是一大堆有许多序列的文本作为我们的数据集。假设“这个闻起来真香。”是其中的一个序列，那么模型可以按照下面的方式进行训练</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi lz"><img src="../Images/507240a99c9c08814259d6bc609bf0c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SLneGwpTkOVSpVZn63hFrA.png"/></div></div></figure><p id="d2de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在每个时间步中，我们查看预测的令牌是否确实是下一个令牌，并相应地计算损失。注意训练标签也是如何从语料库中获得的；对于语料库中任何长度为T的序列，前T-1个单词构成输入序列，后T-1个单词构成目标序列(标签)。在这个意义上，训练一个语言模型可以说是自我监督的。</p><p id="9c35" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在您已经理解了什么是语言模型，让我们开始构建一个吧！我们将使用WikiText-2数据集，该数据集来自维基百科上的许多精彩文章。</p><h2 id="d2b0" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">我们来编码吧！</h2><p id="f469" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">这将是我们的管道。我们将使用笔记本代码快照来回顾和解释其中的每个步骤。也可以笔记本格式<a class="ae mp" href="https://colab.research.google.com/drive/15g7-HyNIT35GUXUZ-am3285XmaQB-SSE?usp=sharing" rel="noopener ugc nofollow" target="_blank">在这里</a>查看。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mq"><img src="../Images/bfd5ce8816ec380956e7bb3239dbc941.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GQTsstXeg3-RIoExJ7JNzA.png"/></div></div></figure><p id="d9a5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将使用HuggingFace中的<a class="ae mp" href="https://huggingface.co/docs/datasets/index" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">数据集库</strong> </a>来加载和映射数据集，<a class="ae mp" href="https://pytorch.org/text/stable/index.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> Torchtext </strong> </a>来标记化数据集并构造词汇，<a class="ae mp" href="https://pytorch.org/tutorials/" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> PyTorch </strong> </a>来定义、训练和评估模型。<a class="ae mp" href="https://tqdm.github.io/" rel="noopener ugc nofollow" target="_blank"> tqdm </a>的目的只是在培训和评估过程中显示进度条。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mr ms l"/></div></figure><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="65c5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了导入之外，我们还设置了一个设备变量，我们将在后面的函数中使用它来确保计算在GPU上进行(如果可能的话),我们还设置了一个种子值，这样我们就可以在需要的时候重现结果。</p><p id="11bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">加载数据集</strong></p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mr ms l"/></div></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mt"><img src="../Images/9c19c885d14a2a4ba8677e9b8cd73fe1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RLYn_uw-P4JdnPZaMOBFgQ.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">电池输出</p></figure><p id="9d83" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了加载数据集，我们使用来自<strong class="kh ir">数据集的<em class="mu"> load_dataset() </em>函数。</strong>有两个<a class="ae mp" href="https://huggingface.co/datasets/wikitext" rel="noopener ugc nofollow" target="_blank"> WikiText数据集</a>，一个旧版本:WikiText-103，一个新版本:WikiText-2。每一个都有一个原始版本和一个稍微预处理的版本。我们选择了新数据集的原始版本，因为稍后我们将自己处理预处理。</p><p id="d096" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">来自<em class="mu"> load_datasets </em>的输出已经为我们拆分了训练、测试和验证集。要打印一个示例，我们首先选择三个集合中的一个，然后选择对应于该示例的行，然后选择我们要打印的特征(列)的名称。这里，数据集总是有一个“文本”列，对应于维基中的一段/一段文本。如果你试图改变索引，你可能会注意到有时没有段落，而是一个空字符串，所以我们将不得不在以后处理这个问题。</p><p id="b14a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">对数据集进行标记</strong></p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="fa8f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下一步是标记数据集中的每个序列。为此，我们从第1行的torchtext中获取一个标记化器，然后定义一个函数，给出一个带有特征“text”的示例，返回一个带有特征“tokens”的示例，其中包含文本的标记化。</p><p id="23ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以如果文本是“有意义的”然后函数会返回["it "，" makes "，" sense "，]. "]</p><p id="30de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们没有实现自己的记号赋予器，因为像这样的内置记号赋予器通常是为处理特殊情况而精心设计的。</p><p id="ab88" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在第三行中，我们使用datasets库中的map函数对每个示例应用tokenize_data函数。map需要将示例和标记器一起传递给tokenize_data，因此我们也在<em class="mu"> fn_kwargs </em>中传递标记器。此时，我们不再需要文本列，所以我们删除了它。</p><p id="97c6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这一步非常重要，因为LSTM/RNN逐个符号地考虑序列。所以它必须被分解成我们可以迭代的记号。</p><p id="474e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">构建词汇</strong></p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mr ms l"/></div></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mt"><img src="../Images/446d73ae529bb2e11714bbc363e2671e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mAzj14qrwpz_xrUs0jpFzA.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">电池输出</p></figure><p id="a580" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在第一行中，我们告诉torchtext将任何在数据集中出现至少三次的单词添加到词汇表中，因为否则它会太大(记住它的长度将是输出分类层中神经元的数量),并且有些单词很少出现。然后，我们手动添加一个<unk>标记，并将is设置为默认索引，这样每当我们从词汇表中请求一个单词的索引时，就会得到<unk>。</unk></unk></p><p id="de42" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在第4行，我们还插入了一个<eos>令牌。我们稍后会将它插入到每个序列的末尾，这样当它生成的序列应该结束时，模型会学习产生它。</eos></p><p id="baf2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如单元格输出所示，词汇表长度约为30K，我们注意到，通过打印词汇表中的前10个元素，词汇表中确实有<unk>和<eos>。“itos”是指“字符串的索引”。</eos></unk></p><p id="a064" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">实现数据加载器</strong></p><p id="25aa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">PyTorch中的dataloader是一个给定数据集的函数，它为您提供了一种迭代批量数据集的方法。在批处理中，所有示例都是并行处理的。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="a394" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我们定义一个做4件事的函数:</p><p id="59f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1-它给每个标记化文本序列附加一个<eos>标记来标记它的结束。(第5行)</eos></p><p id="8ac2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2-它将每个标记编码成一个数值，该数值等于它在词汇表中的索引。请注意，在数据集中出现少于三次的标记将映射到未知标记。(第6行)</p><p id="5252" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3-它将所有的数字序列组合成一个列表(1D张量)。(第2行，第8行)</p><p id="d007" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">4-它将其重新成形为维度为[批量大小，数量批次]的2D张量(第10行，第11行)</p><p id="257c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了进一步澄清，假设数据集只包含来自Wiki的三段文本，它们是</p><blockquote class="mv mw mx"><p id="4c8f" class="kf kg mu kh b ki kj jr kk kl km ju kn my kp kq kr mz kt ku kv na kx ky kz la ij bi translated">你读得越多，你知道和理解的东西就越多</p><p id="dd52" class="kf kg mu kh b ki kj jr kk kl km ju kn my kp kq kr mz kt ku kv na kx ky kz la ij bi translated">“好奇心是学习之烛的灯芯”</p><p id="029c" class="kf kg mu kh b ki kj jr kk kl km ju kn my kp kq kr mz kt ku kv na kx ky kz la ij bi translated">“最终事情开始变得有意义”</p></blockquote><p id="1e44" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，该函数给定批量为5，返回2D张量数据<strong class="kh ir">的形式为</strong></p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nb"><img src="../Images/12a0031a6857d0f17d3f7797721228f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U58AcTHXhFi8usPFceS5dg.png"/></div></div></figure><p id="e7be" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是有更多的列和数字而不是单词。我们一会儿会看到如何用这个来训练我们的模型。现在，让我们在训练集、测试集和验证集上应用这个函数。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="ff38" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">定义模型</strong></p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nb"><img src="../Images/580a0ed50d397e65a5c8d57f1418eff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jR41vj92z59LBq4pATaE-Q.png"/></div></div></figure><p id="0249" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将要建立的模型将对应于上图。三个关键组件是<a class="ae mp" href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html" rel="noopener ugc nofollow" target="_blank">嵌入层</a>、<a class="ae mp" href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html" rel="noopener ugc nofollow" target="_blank"> LSTM层</a>和分类层。我们已经知道了LSTM和分类层的用途。嵌入层的目的是将每个单词(作为索引给出)映射到一个E维的向量中，其他层可以从中学习。不确定向量或等效的单热点向量被认为是差的表示，因为它们假设单词彼此之间没有关系。这种映射也是在训练期间学习的。</p><p id="ef6b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为正则化的一种形式，我们将在嵌入层、LSTM层和输出层之前使用丢弃层。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="e603" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关于上述模型的实现，有几点需要强调:</p><ol class=""><li id="7c49" class="nc nd iq kh b ki kj kl km ko ne ks nf kw ng la nh ni nj nk bi translated"><em class="mu">平局_砝码</em>的争论。这样做的目的是使嵌入层与输出层共享权重。这有助于减少参数的数量，因为<a class="ae mp" href="https://paperswithcode.com/method/weight-tying" rel="noopener ugc nofollow" target="_blank">已经表明</a>输出权重在某种意义上也学习单词嵌入。请注意，要做到这一点，隐藏层和嵌入层的大小必须相同。</li><li id="d35a" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nh ni nj nk bi translated"><em class="mu"> self.init_weights() </em>调用。我们将初始化权重，如本文中的<a class="ae mp" href="https://arxiv.org/abs/1708.02182" rel="noopener ugc nofollow" target="_blank">所示。它们声明在范围[-0.1，0.1]内统一初始化嵌入权重，并且在范围[-1/sqrt(H)，1/sqrt(H)]内统一初始化所有其他层。要将此应用于LSTM，我们必须遍历其每一层，以初始化其隐藏到隐藏和隐藏到下一层的权重。</a></li></ol><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="9655" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还实现了一个函数，将LSTM的隐藏和单元格状态设置为零。</p><p id="4b86" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们将在LSTM类下实现的最后一个函数是detach_hidden</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="3145" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们在训练时将需要这个函数来明确地告诉PyTorch，由于不同的序列导致的隐藏状态是独立的。现在不要担心它。</p><p id="1b03" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">超参数调整&amp;模型初始化</strong></p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="8a5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，我们将嵌入和隐藏维度设置为相同的值，因为我们将使用权重绑定。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="b5f6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我们初始化模型、优化器和损失标准。我们还计算出参数数量为47M。</p><p id="f742" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们准备开始训练。但在此之前，让我们回过头来，记住我们的数据结构。</p><p id="acd8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">回想一下，我们有一个[batch _ size =<em class="mu">128</em>，num _ batches =<em class="mu">16214</em>张量，它看起来类似于</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nb"><img src="../Images/5ea24d4b2eeb1960f74b10d49e2f8f38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VTKORo5OMOunSKtO4_VXFQ.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">这里[批次大小=4，批次数量=24]</p></figure><p id="3fb9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还记得LSTM将形状为[N，L，E]的张量作为输入，其中N是批量大小，L是序列长度，E是序列中每个元素的长度(嵌入长度)。因此，我们需要决定一个序列长度(L ),我们需要将数据集分成该序列的多个块，然后一个接一个地输入它们。如果我们决定对上表取L=4，那么模型在6次迭代中对所有数据进行训练，因为每种颜色对应于“一批序列”,这是到模型的一次前馈传递。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nq"><img src="../Images/bdd1580f072488e72cb4b99bfbb7332e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HAeYEE_178XFizInUb1nJw.png"/></div></div></figure><p id="c4a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">是的，这意味着将输入模型的一些序列可能涉及原始数据集中不同序列的部分，或者是一个序列的子集(取决于序列长度L)。由于这个原因，我们以后将只在每个时期重置隐藏状态，这就像假设下一批序列可能总是原始数据集中前一批序列的后续。</p><p id="5acb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们所做的被称为“通过时间窗口使用固定反向传播”,这只是处理我们不能有一批长度不等的序列的问题的方法之一。</p><p id="e7a2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，因为我们还没有执行将数据集分成“L序列批次”的步骤，所以我们将定义一个函数，该函数在给定批次中第一批标记的索引的情况下返回相应的序列批次。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="34da" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该函数采用[batch_size，num_batches]格式的数据集、序列长度和索引，并返回与LSTM的输入和目标相对应的序列批次。</p><p id="cd46" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">培训&amp;评估模型</strong></p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="0cbb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，第4行将模型设置为training mode to train mode(因此没有禁用dropout)。第6行到第8行确保数据集可以分解成长度为seq_len (L)的批。</p><p id="6541" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在for循环中，我们考虑了在非索引[0，seq_len，2*seq_len，..]其中的每一个都将被提供给<strong class="kh ir"> get_batch </strong>函数，因此它返回输入(src)和标签(trg)的相应批次序列，如第16行所示。两者都有维度<em class="mu">【批量大小，序列长度】。</em></p><p id="0658" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在for循环的前两行中，我们将前一批的梯度归零，并分离其隐藏状态。</p><p id="446c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们在第20行中得到的预测具有维度<em class="mu">[批处理大小，序列长度，vocab大小]，</em>我们将其整形为<em class="mu">[批处理大小*序列长度，vocab] </em>，并将目标平坦化为<em class="mu">[批处理大小*序列长度]。</em>在这种情况下，损失函数需要目标和预测。</p><p id="24fa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在第25行中，我们计算网络中的梯度，然后剪切所有超过“剪切”的梯度，以避开爆炸梯度。在第27行我们更新了权重，在第28行我们计算了损失。Loss.item()将总损失除以batch_size和sequence_length，再乘以seq_len，这样我们最终可以计算每个序列(而不是每个令牌)的平均损失。</p><p id="6f35" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你在Google Colab上得到一个Nvidia T4，在上面的超参数设置下，训练需要大约2个半小时。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="dc98" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">评估循环类似于训练循环，只是我们不再需要反向投影或跟踪梯度。</p><p id="16dc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们需要调用这两个函数</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="9066" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我们使用ReduceLROnPlateu，在没有改进的每个时期之后，将学习率降低2倍。在这种情况下，我们很可能在最小值附近振荡，所以是时候减速了。</p><p id="56aa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还保存具有最高验证损失的模型，并返回困惑度，该困惑度是损失的增函数，用于度量模型的置信度。</p><p id="ec8d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">推论</strong></p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="3c8f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是我们管道的最后一步！</p><p id="6723" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我们将提示、标记化、编码并输入到模型中以获得预测(这是逻辑，记住Softmax应用于损失函数)。因此，我们随后应用Softmax，同时指定我们希望得到序列中最后一个单词的输出，该输出表示对下一个单词的预测。</p><p id="9f76" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将对数除以温度值，通过调整Softmax概率分布来改变模型的置信度。我建议<a class="ae mp" href="https://lukesalamone.github.io/posts/what-is-temperature/" rel="noopener ugc nofollow" target="_blank">检查一下这个</a>，了解更多关于效果的信息。</p><p id="cd5d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我们有了Softmax分布，我们就对它进行随机采样，以预测下一个单词。如果我们得到了<unk>，那么我们会再试一次。</unk></p><p id="b9a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我们得到<eos>，我们就停止预测。</eos></p><p id="c46b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将预测解码回第24行和第25行的字符串。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mr ms l"/></div></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mt"><img src="../Images/e4ea60c274db13c9e54bcbf18fd2d240.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YTpGh3cwuKeecErJ0MlZWw.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">输出</p></figure><p id="2da7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就是这样！恭喜你完成了故事的结尾。在你离开之前，让我们回顾一下这个故事的<strong class="kh ir">要点</strong></p><ol class=""><li id="1187" class="nc nd iq kh b ki kj kl km ko ne ks nf kw ng la nh ni nj nk bi translated">语言模型只是一个带有分类头的RNN，它被训练来预测大型文本语料库中的下一个标记(通常是标记)。</li><li id="8c54" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nh ni nj nk bi translated">一旦经过训练，该模型就能感知不同序列的概率。因此，给定初始记号集，模型可以生成后续记号，以便最终的序列是有意义的。</li><li id="9c56" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nh ni nj nk bi translated">给定一个大的文本语料库，在我们开始训练之前，我们需要对它进行标记、编码并将其置于正确的形状。</li><li id="8fb1" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nh ni nj nk bi translated">剩下的部分来自于你在PyTorch上实现深度学习模型的普通经验</li></ol><p id="46b4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这将使我们的故事结束。如果你发现这很有用，那么记住你可以通过留下一些掌声来帮助它到达其他人那里。下次见，再见。</p></div><div class="ab cl nr ns hu nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="ij ik il im in"><p id="a6ee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">参考书目:</strong><a class="ae mp" href="https://github.com/bentrevett/pytorch-language-modeling" rel="noopener ugc nofollow" target="_blank">https://github.com/bentrevett/pytorch-language-modeling</a></p><p id="9a4e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">数据集</strong>:“wiki text 2.0，指针哨兵混合模型，作者:Stephen Merity和Xiong以及James Bradbury和Richard Socher，根据<a class="ae mp" href="https://creativecommons.org/licenses/by-sa/4.0/" rel="noopener ugc nofollow" target="_blank">知识共享署名-共享许可协议(CC BY-SA 4.0)提供</a>”</p></div></div>    
</body>
</html>