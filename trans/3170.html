<html>
<head>
<title>How to learn imbalanced data arising from multiple domains</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何学习来自多个领域的不平衡数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-learn-imbalanced-data-arising-from-multiple-domains-7d0c0d6e3c17#2022-07-12">https://towardsdatascience.com/how-to-learn-imbalanced-data-arising-from-multiple-domains-7d0c0d6e3c17#2022-07-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e4bd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">多领域长尾识别、非平衡领域推广及超越(ECCV 2022)</h2></div><p id="21ed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">给大家介绍一下我们的新工作，已经被ECCV 2022接受:<a class="ae lb" href="https://arxiv.org/abs/2203.09513" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">关于多领域长尾识别、不平衡领域泛化和超越</strong> </a>。顾名思义，这项工作的问题是，当存在来自多个域的数据，并且每个域都可能存在(潜在不同的)数据不平衡时，如何学习一个健壮的模型。现有的处理不平衡数据/长尾分布的方法只针对<strong class="kh ir">单域</strong>，即数据来源于同一个域；然而，自然数据可能来自<strong class="kh ir"> <em class="lc">不同的</em>域</strong>，其中一个域中的少数类可能有来自其他域的大量实例。有效地利用来自不同领域的数据可能会提高所有领域的长尾学习的性能。本文推广了传统不平衡分类问题的范式，将其从<strong class="kh ir">单</strong>域推广到<strong class="kh ir">多域</strong>。</p><p id="c85a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们首先提出<strong class="kh ir"> <em class="lc">领域类可迁移性图</em> </strong>，量化了数据不平衡情况下不同领域类对之间的可迁移性。在这个图中，每个节点指的是一个域类对，每个边指的是嵌入空间中两个域类对之间的距离。我们证明了可迁移性图决定了跨领域不平衡学习的性能。受此启发，我们设计了<strong class="kh ir"> BoDA </strong>，一个理论上跟踪可转移性统计量上限的损失函数，以提高模型性能。我们构建了五个新的多域不平衡数据集，并对大约20个算法进行了比较。代码、数据和模型已经在GitHub上开源:<a class="ae lb" href="https://github.com/YyzHarry/multi-domain-imbalance" rel="noopener ugc nofollow" target="_blank">https://github.com/YyzHarry/multi-domain-imbalance</a>。</p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><h1 id="b2cc" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">背景和动机</h1><p id="51ee" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">现实世界中的数据经常表现出标签不平衡，而不是在类之间均匀的标签分布，事实上，数据本质上是不平衡的:少数类包含大量实例，而许多其他类只有少量实例。为了处理这种现象，已经提出了许多解决数据不平衡的方法。更详细的回顾可以在我的<a class="ae lb" rel="noopener" target="_blank" href="/struggling-with-data-imbalance-semi-supervised-self-supervised-learning-help-4de8b8f23490">上一篇文章</a>中找到。</p><p id="8b5a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是，现有的不平衡数据学习的解决方案主要考虑的是<em class="lc">单域</em>的情况，即<em class="lc">所有样本来自同一数据分布</em>。然而，在现实中，同一任务的<strong class="kh ir">数据可能来自不同的域</strong>。例如，下图显示了Terra Incognita，这是一个为野生动物识别&amp;分类收集的真实数据集。左图是在不同地点设立的相机陷阱，以及捕捉到的野生动物样本；右图显示了(部分)不同摄像机位置获得的具体数据分布及其拍摄效果。我们可以清楚地看到，即使是同样的野生动物分类任务，不同相机的参数、拍摄背景、光照强度等。<strong class="kh ir"> <em class="lc">与</em> </strong>完全不同；即不同相机陷阱之间存在一个<strong class="kh ir">畴隙</strong>。而且由于有些动物只出现在特定的位置，这进一步导致了对于一个摄像机(域)的<strong class="kh ir">数据不平衡</strong>，甚至存在对于某些类别的<strong class="kh ir"> <em class="lc">无数据</em> </strong>(比如位置100对于类别0和1几乎没有数据)。然而，由于不同摄像机捕获的标签分布通常差异很大，这也意味着其他属性域可能具有这些类别中的许多样本-例如，位置46具有更多类别1数据。这表明我们可以利用<strong class="kh ir"><em class="lc"/></strong>多域数据来解决<strong class="kh ir"> <em class="lc">各域</em> </strong>内部固有的数据不平衡。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mh"><img src="../Images/71090115a2d0d5c02811e4c23742f118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bb-gnjZz83yHx059as9R7w.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">未知领域数据集。即使是同样的野生动物分类任务，参数、拍摄背景、光照强度等。不同相机的拍摄效果可能完全不同。每个相同的摄像机获得的数据也是极不平衡的；再者，不同相机拍摄到的标签分布也大相径庭，不匹配。但它也说明了我们可以利用多域数据来解决每个域中固有的数据不平衡。(图片由作者提供)</p></figure><p id="64ad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样，在其他实际应用中也会出现类似的情况。例如，在视觉识别问题中，来自“<em class="lc">照片</em>图像的少数类可以用来自“<em class="lc">草图</em>图像的潜在丰富样本来补充。同样，在自动驾驶中，“<em class="lc">真实</em>生活中的少数事故类别可以通过“<em class="lc">模拟</em>中产生的事故来丰富。此外，在医学诊断中，来自不同人群的数据可以相互增强，来自一个机构的少数群体样本可以用来自其他机构的实例来丰富。在上面的例子中，不同的数据类型充当<strong class="kh ir">不同的域</strong>，这样的多域数据可以用来解决每个域中固有的数据不平衡。</p><p id="522a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，在本工作中，我们将<strong class="kh ir">多域长尾识别(MDLT) </strong>问题公式化为从多域不平衡数据中学习，每个域都有其自己的不平衡标签分布，并推广到在<strong class="kh ir"> <em class="lc">所有域类对</em> </strong>上平衡的测试集。MDLT的目标是从多个不同领域的不平衡数据中学习，解决<em class="lc">标签不平衡</em>、<em class="lc">域转移</em>和<em class="lc">跨域的不同标签分布</em>，并推广到所有域的整个类别集。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mx"><img src="../Images/068af6e8a3e62d00a5e95d3aaf4d5323.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*yK2bECMi4LUArrGbvUXsYw.gif"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">多域长尾识别(MDLT)从多域不平衡数据中学习，解决跨域的标签不平衡、域移位和发散标签分布，并推广到所有域类对。(图片由作者提供)</p></figure><h1 id="31f3" class="lk ll iq bd lm ln my lp lq lr mz lt lu jw na jx lw jz nb ka ly kc nc kd ma mb bi translated">多领域不平衡学习的挑战</h1><p id="7692" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">然而，我们注意到，MDLT带来了不同于其单一领域对手的新挑战。</p><p id="e981" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> (I) </strong>首先，每个域的<strong class="kh ir"> <em class="lc">标签分布很可能与其他域的</em> </strong>不同。例如，在上面的gif图中，“<em class="lc">照片</em>”和“<em class="lc">漫画</em>”域名呈现出不平衡的标签分布；然而，“<em class="lc">漫画</em>”中的“马”类比“<em class="lc">照片</em>”中的样本要多得多。除了域内数据不平衡之外，这还带来了跨域的<strong class="kh ir">不同标签分布的挑战</strong>。</p><p id="9355" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">(二)</strong>二、多域数据内在涉及<strong class="kh ir"><em class="lc"/></strong>。简单地将不同的领域作为一个整体来对待，并应用传统的数据不平衡方法不太可能产生最好的结果，因为领域差距可以任意大。</p><p id="b5a0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> (III) </strong>第三，MDLT自然地激发了 内的<strong class="kh ir"/><strong class="kh ir"><em class="lc">和</em> </strong>域间的<strong class="kh ir"><em class="lc">——即概括为域内缺失类(gif图右部)，以及没有训练数据的<strong class="kh ir"> <em class="lc">新域</em> </strong>，其中后一种情况通常表示为<strong class="kh ir">域概括(DG) </strong>。</em></strong></p><p id="f4bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">综上所述，与传统的单领域不平衡分类相比，MDLT面临着新的困难和挑战。那么，应该怎么做多领域不平衡学习呢？在接下来的两节中，我们将从整体建模、激励示例、观察到的现象、理论推导，到最终损失函数的设计，逐步分析这个问题，并最终在MDLT任务上提高模型性能。</p><h1 id="ac8c" class="lk ll iq bd lm ln my lp lq lr mz lt lu jw na jx lw jz nb ka ly kc nc kd ma mb bi translated">领域类可转移图</h1><p id="c94d" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">这里我们首先提出一组定义来模拟MDLT问题。我们认为，与单领域不平衡学习相反，在单领域不平衡学习中，人们关心的基本单元是T21，在MDLT，基本单元自然地转化为领域类对。</p><p id="2c38" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么当我们从“领域类对”入手，就可以衡量它们之间的可转移性(相似性)，可转移性定义为嵌入空间中不同领域类对之间的距离:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nd"><img src="../Images/a97884de4b5c76f7ae5ab013fba50a12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b3pCyPi54DM3ZmgQNoBjWA.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">(图片由作者提供)</p></figure><p id="4623" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">直观上，两个领域类对之间的<strong class="kh ir"> <em class="lc">可转移性</em> </strong>是它们的学习表示之间的<strong class="kh ir">平均距离，表征它们在特征空间中的接近程度。默认选择<em class="lc"> d </em>作为欧氏距离，但也可以表示(<em class="lc"> d，c </em>)的高阶统计量。例如，马氏距离使用协方差。那么，基于可转移性，我们可以进一步定义<strong class="kh ir"> <em class="lc">可转移性图:</em> </strong></strong></p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ne"><img src="../Images/95316c6226444f717a5803ded9a5953d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_GGblnj-rw16udqtYxj0BQ.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">(图片由作者提供)</p></figure><p id="47fc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在可转移性图中，每个<em class="lc">节点</em>表示一个领域类对，每个<em class="lc">边</em>表示嵌入空间中两个领域类对之间的距离(即可转移性)。此外，我们可以使用多维标度(MDS)在2D空间中可视化该图。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nf"><img src="../Images/c157a982a9aed6227116e395ec868847.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9YqrtFb8BcIgB_IeSgLMVw.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">可转移性图的总体框架。(a)为所有域类对计算分布统计，由此我们生成完整的可转移性矩阵。(b) MDS用于将图形投影到2D空间中进行可视化。(c)我们定义了(α，β，γ)可转移性统计量来进一步描述整个可转移性图。(图片由作者提供)</p></figure><p id="a870" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">具体来说，如上面的图(a)(b)所示，对于每个域类对，我们可以计算特征统计(均值、协方差等)。)属于这个域类对的所有数据。然后对于不同的域类对，我们进一步计算对之间的可转移性，从中生成一个完整的可转移性图，用矩阵表示(图a)。然后，我们可以使用多维标度(MDS)在2D平面上可视化这种相似性。在图b中，我们可以看到不同的域用不同的颜色标记，每个点代表一个域类对，它的大小代表包含的数据量，数字代表具体的类别；而它们之间的距离可以看作是可转移性。很明显，我们希望同号(也就是同一个类别)的域类对靠得更近，而不同类别的域类对离得更远。这种关系可以进一步抽象为三种可转移性统计量:<em class="lc">异域同类</em> ( <strong class="kh ir"> <em class="lc"> α </em> </strong>)、<em class="lc">异类同域</em> ( <strong class="kh ir"> <em class="lc"> β </em> </strong>)、以及<em class="lc">异类异域</em> ( <strong class="kh ir"> <em class="lc"> γ </em> </strong>):</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ng"><img src="../Images/830a262ebf05df0e025234520dec4aae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L1-KeG2127goGTWAhyR9mw.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">(图片由作者提供)</p></figure><p id="ee56" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">到目前为止，我们已经对MDLT进行了建模和数学定义。接下来，我们将进一步探讨可转移性(统计数据)和最终MDLT性能之间的关系。</p><h1 id="278b" class="lk ll iq bd lm ln my lp lq lr mz lt lu jw na jx lw jz nb ka ly kc nc kd ma mb bi translated">在MDLT，什么是好的代表？</h1><h2 id="3792" class="nh ll iq bd lm ni nj dn lq nk nl dp lu ko nm nn lw ks no np ly kw nq nr ma ns bi translated">分散的标签分布妨碍了可转移的特征</h2><p id="20e8" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">MDLT必须处理跨域标签分布之间的差异。为了理解这个问题的含义，我们从一个例子开始。</p><p id="ee80" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">激励范例</strong>。我们构建了<em class="lc">数字-MLT </em>，一个双域玩具MDLT数据集，它结合了两个数字数据集:MNIST-M和SVHN。任务是10类数字分类。我们手动改变每个域类对的样本数量，以模拟不同的标签分布，并针对每种情况使用经验风险最小化(ERM)来训练普通的ResNet-18。我们保持所有测试集的平衡和一致。结果揭示了有趣的观察。当跨域的每个域的标签分布是<strong class="kh ir">平衡的</strong>和<strong class="kh ir">相同的</strong>时，尽管存在域间隙，但这并不禁止模型学习高准确度(90.5%)的区别特征，如图a所示。如果标签分布是<strong class="kh ir">不平衡的</strong>但<strong class="kh ir">相同的</strong>，如图b所示，ERM仍然能够在两个域中对齐相似的类，其中多数类(例如类9)在可转移性方面比少数类(例如类9)更接近相比之下，当标签都是跨域的<strong class="kh ir">不平衡</strong>和<strong class="kh ir">不匹配</strong>时，如图c所示，学习到的特征是<strong class="kh ir"> <em class="lc">不再可转移</em> </strong>，导致跨域的<strong class="kh ir">间隙</strong>和最差的准确度。这是因为<em class="lc">跨域的发散标签分布</em>产生了不期望的捷径；该模型可以通过简单地分离两个域来最小化分类损失。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nt"><img src="../Images/0954bcd4b377e1925968794ea0ca50e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jgtprfu7OdZPvrGUt52mSQ.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">当数字的标号比例变化时，可转移性图形的演变模式——MLT。(a)两个域的标签分布是平衡且相同的。(b)两个域的标签分布不平衡但相同。两个域的标签分布是不平衡和发散的。(图片由作者提供)</p></figure><p id="8d0a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可转移的特征是可取的。结果表明，需要<em class="lc">可转移特征</em>跨(<em class="lc"> d，c </em>)对，尤其是出现不平衡时。特别地，跨域的同一类之间的可转移性链接应该大于域内或跨域的不同类之间的可转移性链接。这可以通过(<em class="lc"> α，β，γ </em>)可转移性统计来捕获，如下所示。</p><h2 id="6762" class="nh ll iq bd lm ni nj dn lq nk nl dp lu ko nm nn lw ks no np ly kw nq nr ma ns bi translated">可迁移性统计特征概括</h2><p id="a8a3" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated"><strong class="kh ir">激励的例子。</strong>同样，我们使用具有不同标签分布的<em class="lc">数字-MLT </em>。我们考虑<strong class="kh ir">三种不平衡类型</strong>来组成不同的标签配置:(1) <strong class="kh ir">统一</strong>(即平衡标签)，(2) <strong class="kh ir">前向-LT </strong>，其中标签在类id上呈现长尾，以及(3) <strong class="kh ir">后向-LT </strong>，其中标签相对于类id是反向长尾。对于每种配置，我们用不同的超参数训练20个ERM模型。然后，我们计算每个模型的(<em class="lc"> α，β，γ </em>)统计量，并绘制其相对于<em class="lc">(β+γ)——α</em>的分类精度。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nu"><img src="../Images/7e833eeb5d9be9958e671e1860d04bf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gw_goyLw8rOnS-NhmzlEfA.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">(β+γ)α数量与不同数字标签配置测试准确度之间的对应关系-MLT。每个图表示两个域的特定标记分布(例如,( a)对域1采用“统一”,对域2采用“统一”)。每个点对应一个使用不同超参数的ERM训练模型。(图片由作者提供)</p></figure><p id="4c23" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它揭示了多个有趣的发现:<strong class="kh ir">(1)</strong><strong class="kh ir"><em class="lc"/></strong><em class="lc">α，β，γ </em> <strong class="kh ir"> <em class="lc">统计数据表征了一个模特在MDLT </em> </strong>的表现。特别是,(β+γ)α量在整个范围和每种标签配置中与测试性能表现出非常强的相关性。<strong class="kh ir"> (2) </strong> <strong class="kh ir"> <em class="lc">数据不平衡增加了学习较少可转移特征的风险。</em>然而，随着标签变得不平衡(图b、c)和进一步发散(图d、e)，模型学习不可转移特征(即较低的<em class="lc">(β+γ)——α</em>)的机会增加，导致性能大幅下降。</strong></p><h2 id="735d" class="nh ll iq bd lm ni nj dn lq nk nl dp lu ko nm nn lw ks no np ly kw nq nr ma ns bi translated">限制可转移性统计的损失</h2><p id="7f28" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">我们利用上述发现来设计一个特别适合MDLT的新损失函数。我们将首先引入损失函数，然后证明它最小化(<em class="lc"> α，β，γ </em>)统计量的上界。我们从一个简单的损失开始，这个损失是由公制学习目标激发的。我们将这种损失称为L_{DA}，因为它旨在实现域类分布对齐，即跨域对齐同一类的特征:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nv"><img src="../Images/e6556c6d0150604adb84c99db75cf815.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jv0OTIocIiu1FeOfy2RzDA.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">(图片由作者提供)</p></figure><p id="d3ff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">直观上，这种损失解决了<strong class="kh ir"> <em class="lc">标签发散</em> </strong>，因为共享同一类的(<em class="lc"> d，c </em>)对将被<strong class="kh ir">拉近</strong>，反之亦然。也与(<em class="lc"> α，β，γ </em>)统计有关，因为分子代表<strong class="kh ir"> <em class="lc">正</em> </strong> <em class="lc">跨域对</em> ( <em class="lc"> α </em>)，分母代表<strong class="kh ir"> <em class="lc">负</em> </strong> <em class="lc">跨类对</em> ( <em class="lc"> β，γ </em>)。</p><p id="1e66" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">但是</strong>，并没有解决标签不平衡的问题。注意(<em class="lc"> α，β，γ </em>)是以<strong class="kh ir"><em class="lc"/></strong>方式定义的，与每个(<em class="lc"> d，c </em>)的样本数无关。然而，给定一个<strong class="kh ir">不平衡的</strong>数据集，大多数样本将来自<em class="lc">多数</em>域类对，这将主导损失并导致<em class="lc">少数</em>对被忽略。</p><p id="f3a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">平衡域级分布比对</strong>。为了解决(<em class="lc"> d，c </em>)对之间的数据不平衡，我们将损耗修改为BoDA损耗:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nw"><img src="../Images/adf366d3ab22ebae77c6e78ca8da2ba0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3qKEBmf3LgllsBP2thVp7A.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">(图片由作者提供)</p></figure><p id="a384" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">BoDA通过因子<em class="lc"> 1/N_{d，c} </em>缩放原始的<em class="lc"> d </em>，该因子是域类对中的样本数(<em class="lc"> d，c </em>)。因此，它通过引入一个<strong class="kh ir">平衡的</strong> <em class="lc">距离度量</em>来抵消<strong class="kh ir">不平衡的</strong>域类对的影响。对于博达，我们有以下定理:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nx"><img src="../Images/76d2dee47ce31ac8f695f1ae49b3621c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x2-f2BQDnB_fOHX4C8cf8g.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">(图片由作者提供)</p></figure><p id="501e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请参考<a class="ae lb" href="https://arxiv.org/pdf/2203.09513.pdf" rel="noopener ugc nofollow" target="_blank">我们的论文</a>了解证明细节。定理1具有以下有趣的含义:</p><ol class=""><li id="2eae" class="ny nz iq kh b ki kj kl km ko oa ks ob kw oc la od oe of og bi translated"><strong class="kh ir"> <em class="lc"> BoDA上界(</em> </strong> <em class="lc"> α，β，γ </em> <strong class="kh ir"> <em class="lc">)以期望的形式统计，自然转化为更好的性能</em> </strong>。通过最小化BoDA，我们保证一个<strong class="kh ir"> <em class="lc">低</em> </strong> <em class="lc"> α </em>(吸引相同阶层)和<strong class="kh ir"> <em class="lc">高</em></strong><em class="lc"/>、γ (分开不同阶层)，这是在MDLT推广的必要条件。</li><li id="6ae3" class="ny nz iq kh b ki oh kl oi ko oj ks ok kw ol la od oe of og bi translated"><strong class="kh ir"> <em class="lc">常数因子对应于各成分对可转移性图形</em> </strong>的贡献大小。放大<em class="lc"> exp( ) </em>的论点，我们观察到目标与<em class="lc">α(β* 1/| D |+γ*(| D | 1)/| D |)</em>成正比。根据定义3，我们注意到<em class="lc"> α </em>总结了<strong class="kh ir">相同类别</strong>的数据相似性，而后一个表达式使用<em class="lc"> β </em>和<em class="lc"> γ </em>的<strong class="kh ir">加权平均值</strong>总结了不同类别的数据相似性<strong class="kh ir">，其中它们的权重与关联域的数量成比例。</strong></li></ol><h2 id="8ce9" class="nh ll iq bd lm ni nj dn lq nk nl dp lu ko nm nn lw ks no np ly kw nq nr ma ns bi translated">针对数据不平衡的校准导致更好的传输</h2><p id="cfae" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">BoDA的工作原理是鼓励<strong class="kh ir"> <em class="lc">特征跨</em> </strong>域转移相似的类，也就是说，如果(<em class="lc"> d，c </em>和(<em class="lc">d′，c </em>)在不同的域中引用同一个类，那么我们希望将它们的特征互相转移。<strong class="kh ir">但是</strong>、<em class="lc">少数</em>领域类对自然会有<strong class="kh ir">更差的</strong>特征估计，这是由于<em class="lc">数据稀缺、</em>以及强迫其他对转移给它们会损害学习。因此，当在嵌入空间中拉近两个域类对时，我们希望少数的<em class="lc"/>(<em class="lc">d，c </em>)转移到多数的<em class="lc">中，而不是相反。</em></p><p id="4388" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里有很多细节，我就直接跳过了。在<a class="ae lb" href="https://arxiv.org/pdf/2203.09513.pdf" rel="noopener ugc nofollow" target="_blank">我们的论文</a>中给出了详细的激励示例和解释。结论是通过给BoDA增加一个<em class="lc">校准项</em>，可以通过两个域类对的<strong class="kh ir">相对样本大小来控制<em class="lc">转移度</em>:</strong></p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi om"><img src="../Images/3cd44cdd4ab3c0ee1174dec6c86de8b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-kpbEPpbC82D-l-CjuXO7Q.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">(图片由作者提供)</p></figure><h1 id="b3b4" class="lk ll iq bd lm ln my lp lq lr mz lt lu jw na jx lw jz nb ka ly kc nc kd ma mb bi translated">基准MDLT数据集和实验</h1><p id="5055" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">为了支持多领域不平衡学习方法的实际评估，并促进未来的研究，我们从现有的多领域数据集筛选了五个MDLT基准:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nt"><img src="../Images/29772423d0ee9896d2fb9937fae47d1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aVxRUEIT-e9sSD15spnnNA.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">(图片由作者提供)</p></figure><p id="a415" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，我们选择了<strong class="kh ir"> ~20种算法</strong>，涵盖了多领域学习、分布式鲁棒优化、不变特征学习、元学习、不平衡学习等多个类别。作为基线比较，以及<em class="lc">每个算法的优化超参数</em>。这样的过程确保了比较是最佳对最佳的，并且超参数对于所有算法都是优化的。</p><p id="d343" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在评估期间，我们报告跨域的平均<strong class="kh ir"><em class="lc"/></strong><em class="lc">准确度</em>；我们还报告了域上最差的<strong class="kh ir"><em class="lc"/></strong><em class="lc">精度</em>，并将所有域类对进一步划分为<em class="lc">多射</em>(超过100个训练样本的对)、<em class="lc">中射</em>(20∾100个训练样本的对)、<em class="lc">少射</em>(20个训练样本以下的对)和<em class="lc">零射</em>(没有训练数据的对)，并报告结果</p><p id="b96d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">实验</strong>:由于实验很多，这里只展示有代表性的结果(所有结果请参考论文)。首先，BoDA在所有数据集上始终保持<strong class="kh ir">最佳</strong>平均准确度<strong class="kh ir">。它还在大多数情况下实现了<strong class="kh ir">最佳</strong> <strong class="kh ir">最坏</strong>精度。此外，在某些数据集上(例如，OfficeHome-MLT)，MDL方法表现更好(例如，CORAL)，而在其他数据集上(例如，TerraInc-MLT)，不平衡方法实现更高的增益(例如，CRT)；然而，不考虑数据集，BoDA <em class="lc">优于所有方法</em>，突出了其对于MDLT任务的有效性。最后，与ERM相比，BoDA略微提高了平均和多次拍摄的性能，而<em class="lc">大大提高了</em>中镜头、少镜头和零镜头对的性能。</strong></p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi on"><img src="../Images/a8398244246855f5a3a379b544417722.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h7aIc5k83Zg2gi91bil8jg.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">(图片由作者提供)</p></figure><p id="ec8c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">理解博达的分析</strong>:我们对博达进行进一步分析。我们绘制了通过BoDA学习的可迁移性图，并在不同的跨域标签分布下与ERM进行了比较。从下图可以看出，博达学习了一个更平衡的特征空间来区分不同的类别。当标签分布<strong class="kh ir">平衡</strong>和<strong class="kh ir">相同</strong>时，ERM和博达都能学习到好的特性；当标签开始<strong class="kh ir">不平衡</strong> (b，c)，甚至<strong class="kh ir">不匹配</strong>跨域(d，e)时，ERM学习到的可迁移性图中有一个<em class="lc">明显的域缺口</em>；相比之下，博达总能学习到一个<strong class="kh ir">平衡的</strong>和<strong class="kh ir">对齐的</strong>特征空间。因此，更好的学习特征转化为更好的精度(9.5%的绝对精度增益)。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oo"><img src="../Images/fa63b7aa7b172138be698630c5003bbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P0mOtcX5rBRyvuGUMEJ4Yw.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">(图片由作者提供)</p></figure><h2 id="4812" class="nh ll iq bd lm ni nj dn lq nk nl dp lu ko nm nn lw ks no np ly kw nq nr ma ns bi translated">超越MDLT:不平衡的领域泛化</h2><p id="aba6" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">领域泛化(DG)是指从多个领域学习，并泛化到看不见的领域。由于学习域的标签分布自然不同，甚至可能在每个域中存在类别不平衡，我们研究BoDA是否可以提高DG的性能。请注意，我们为MDLT调整的所有数据集都是DG的标准基准，这证实了数据不平衡是DG中的固有问题，但被过去的工作忽略了。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi op"><img src="../Images/555649b51ff16616b865e05b7fc9b9d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ryJLWARg1-qDr7ByH6jFgQ.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">(图片由作者提供)</p></figure><p id="30f0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们按照标准的DG评估测试BoDA。上表揭示了以下发现:首先，<strong class="kh ir"> BoDA单独使用</strong>可以在五个数据集中的四个上改善当前的SOTA，并实现<strong class="kh ir">显著的</strong>平均性能增益。此外，<strong class="kh ir">将</strong>与当前的SOTA相结合，博达在所有数据集上进一步显著提高了结果，表明<strong class="kh ir">标签不平衡与现有的DG特定算法</strong>正交。最后，与MDLT相似，增益取决于数据集内不平衡的严重程度TerraInc显示出跨域的最严重的标注不平衡，而BoDA在这方面的增益最高。这些有趣的结果揭示了<strong class="kh ir">标签不平衡如何影响分布外泛化</strong>，并强调了<strong class="kh ir">整合标签不平衡对于实际DG算法设计的重要性</strong>。</p><h1 id="b2b7" class="lk ll iq bd lm ln my lp lq lr mz lt lu jw na jx lw jz nb ka ly kc nc kd ma mb bi translated">结束语</h1><p id="9566" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">作为本文的总结，我们提出了(1)一个称为多域长尾识别(MDLT)的<strong class="kh ir">新任务</strong>,( 2)一个新的<strong class="kh ir">理论上保证损失</strong>函数BoDA来建模和改进MDLT，以及(3)五个<strong class="kh ir">新基准</strong>来促进未来对多域不平衡数据的研究。此外，我们发现标签不平衡影响分布外泛化，实际和稳健的DG算法设计也需要纳入标签不平衡的重要性。最后，我附上了几个与本文相关的链接；感谢阅读！</p><p id="a646" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">代号</strong>:<a class="ae lb" href="https://github.com/YyzHarry/multi-domain-imbalance" rel="noopener ugc nofollow" target="_blank">https://github.com/YyzHarry/multi-domain-imbalance</a></p><p id="017b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">项目页面</strong>:<a class="ae lb" href="http://mdlt.csail.mit.edu/" rel="noopener ugc nofollow" target="_blank">http://mdlt.csail.mit.edu/</a></p></div></div>    
</body>
</html>