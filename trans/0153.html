<html>
<head>
<title>Implementing Bayesian Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实施贝叶斯线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-bayesian-linear-regression-9375a9994f98#2022-02-07">https://towardsdatascience.com/implementing-bayesian-linear-regression-9375a9994f98#2022-02-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c951" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">本文介绍了贝叶斯线性回归的基础，包括一个供感兴趣的程序员使用的Python实践示例。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d21fb1ea0c97b185559b3f35f01daa5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TMO3Lm_wa00zz20G"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://www.pexels.com/@goumbik" rel="noopener ugc nofollow" target="_blank">卢卡斯</a>在<a class="ae ky" href="https://www.pexels.com/photo/chart-close-up-data-desk-590022/" rel="noopener ugc nofollow" target="_blank">像素</a>上拍摄</p></figure><p id="3880" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归试图通过将线性方程拟合到观察到的数据来模拟两个变量之间的关系。<em class="lv">人口简单线性回归</em>可表述如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/d3e24a9635c4ef880e5a79bdb786c1db.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*Jrl1Cr32aqare0J8F1hfzg.png"/></div></figure><p id="fae3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中:</p><ul class=""><li id="aa3d" class="lx ly it lb b lc ld lf lg li lz lm ma lq mb lu mc md me mf bi translated">y是因变量，X是自变量</li><li id="faf4" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">β0是总体截距，β1是总体斜率。这些是我们要估计的回归参数。</li></ul><p id="f7d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，使用最小二乘回归原理，我们可以计算最小化均方误差成本函数的权重和偏差的值。如果你已经知道了这些概念，那么你就熟悉了<a class="ae ky" href="https://javiferfer.medium.com/introduction-to-linear-regression-b9387e7e34b4" rel="noopener"> <em class="lv"> frequentist </em>线性回归</a>方法。</p><p id="f595" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，除了<em class="lv">频率主义者</em>方法，还有另一个统计推断学派，叫做<em class="lv">贝叶斯</em>。一方面，<em class="lv">frequency ist</em>方法将概率理解为由重复实验确定的事件的可测量频率。另一方面，<em class="lv">贝叶斯</em>方法将概率描述为对事件可信度的测量，使用从观察数据中获得的先验知识。</p><p id="2660" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文重点关注<em class="lv">贝叶斯</em>线性回归，并通过以下方式进行介绍:(1)介绍一些基本概念，如贝叶斯定理，(2)描述贝叶斯线性回归背后的理论，以及(3)展示Python中的实际操作示例。</p><h1 id="6af1" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">1.基本概念</h1><p id="2e16" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">贝叶斯定理在数学上表述为以下等式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/e60cf49f827c116e4cb84be6b746e834.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NVKTNYMQHs3g5l6vG7_deQ.png"/></div></div></figure><p id="681e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中:</p><ul class=""><li id="183a" class="lx ly it lb b lc ld lf lg li lz lm ma lq mb lu mc md me mf bi translated"><strong class="lb iu"> P(A|B) </strong>被称为<strong class="lb iu">后路</strong>；这就是我们试图估计的。</li><li id="159e" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated"><strong class="lb iu"> P(B|A) </strong>称为<strong class="lb iu">可能性</strong>；这是根据我们最初的假设，观察到新证据的概率。</li><li id="0755" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated"><strong class="lb iu"> P(A) </strong>称为<strong class="lb iu">先于</strong>；这是我们假设的概率，没有任何额外的先验信息。</li><li id="19df" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated"><strong class="lb iu"> P(B) </strong>称为<strong class="lb iu">边际可能性</strong>；这是观察到证据的总概率。</li></ul><p id="e2e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在机器学习中，变量<strong class="lb iu"> <em class="lv"> B </em> </strong>可以指示观察到的数据，这意味着利用生成模型<em class="lv"> 𝑃 </em> ( <em class="lv"> 𝐵 </em> | <em class="lv"> 𝐴 </em>)和一个可能的模型参数P(A)的先验分布，我们可以得到给定观察到的数据的模型参数的后验。我们将在下一节描述<em class="lv">贝叶斯线性回归</em>时看到更多细节。</p><h1 id="ecd3" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">2.贝叶斯线性回归</h1><p id="2952" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">在贝叶斯分析中，目标是通过结合来自观察数据的关于参数的信息来更新参数的概率分布。使用贝叶斯公式，这可以表述为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/8c93542119ea8b2e9ceaeb7d117360da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0o0kImE11ZIeihbSv9W-Dw.png"/></div></div></figure><p id="f875" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中:</p><ul class=""><li id="1626" class="lx ly it lb b lc ld lf lg li lz lm ma lq mb lu mc md me mf bi translated"><strong class="lb iu"> P(β|x，y) </strong>描述给定观测数据、目标和先验分布的参数的概率。</li><li id="e577" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated"><strong class="lb iu"> P(y|x，β) </strong>描述给定数据和参数的目标值的概率</li><li id="d18f" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated"><strong class="lb iu"> P(β) </strong>描述了关于参数的初始知识。如果我们没有关于分布的任何估计，我们可以使用参数的非信息先验，如正态分布。</li><li id="00e3" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated"><strong class="lb iu"> P(x，y) </strong>描述了数据和目标的联合概率。</li></ul><p id="47de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总之，为了计算后验概率，我们将可能性<strong class="lb iu"> P(y|x，β) </strong>与先验概率<strong class="lb iu"> P(β) </strong>相乘，前者表明我们认为数据是如何分布的，后者表明我们认为<strong class="lb iu"> β </strong>是如何分布的。随着数据数量的增加，与先前的相比，可能性变得更加显著，并且解决方案越来越接近从最小二乘法获得的值。</p><p id="670f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，还有归一化项<strong class="lb iu"> P(x，y) </strong>，其通过对所有可能的参数值进行积分来估计:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/46bb785fef8aa6568428fc466e761ef7.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*HXOL_vyPA3ubUPit_SPm6g.png"/></div></figure><p id="5c26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管这个公式看起来很简单，但即使是简单的模型，也很难用封闭形式来计算。这就是所谓的马尔可夫链蒙特卡罗(MCMC)方法发挥作用的时候了。简单地说，这种方法包括一类从概率分布中取样的算法。因为详细解释这种方法本身就需要一整篇文章，所以我在这里为感兴趣的人留下了一个很好的参考资料。</p><p id="d79e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总结整个逻辑，要计算贝叶斯线性回归，必须遵循以下步骤:</p><ol class=""><li id="4a5d" class="lx ly it lb b lc ld lf lg li lz lm ma lq mb lu nl md me mf bi translated">设定关于参数P(β)分布的初始信念。</li><li id="b96b" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu nl md me mf bi translated">收集一些数据。</li><li id="f198" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu nl md me mf bi translated">使用贝叶斯公式计算参数β的后验概率分布。</li><li id="ce81" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu nl md me mf bi translated">对未知数据进行推断，计算输出的后验分布参数，而不是点估计。</li></ol><h1 id="1e1d" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">3.编码</h1><p id="4727" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">最后，这里有一个Python中的<em class="lv">贝叶斯线性回归</em>的实际例子。本节分为两部分。第一部分(<em class="lv">评估贝叶斯回归模型</em>)包括frequentist和贝叶斯回归模型之间的比较，以及用不同数量的数据进行训练时参数分布的分析。第二部分(<em class="lv">更新先验贝叶斯回归模型</em>)展示了在获得更多数据时如何重新训练模型，并展示了每次训练迭代的输出分布的演变。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><h2 id="a92f" class="no mm it bd mn np nq dn mr nr ns dp mv li nt nu mx lm nv nw mz lq nx ny nb nz bi translated">评估贝叶斯回归模型</h2><p id="53aa" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">对于第一部分，我们创建了一个包含1000个数据点的数据集，其斜率为2，截距为1。然后，我们也给数据点添加了噪声以增加随机性。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="6f21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们计算了普通最小二乘法，以查看使用<em class="lv"> frequentist线性回归</em>时的输出。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/6cccebe5d91076e435b5179e16a3d60f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*EZS4e4TQMFU2soaQcQLJkg.png"/></div></figure><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="2114" class="no mm it oc b gy og oh l oi oj">Intercept: 0.986230345022046<br/>Slope: 2.071679208492629</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/432490bfcf2743ae509913d18e351835.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*0_XMGt4XFsHQEQQaNqVcfA.png"/></div></figure><p id="9431" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上所述，斜率和截距近似于生成数据时设定的参数。</p><p id="54fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，让我们建立贝叶斯模型</p><ul class=""><li id="6984" class="lx ly it lb b lc ld lf lg li lz lm ma lq mb lu mc md me mf bi translated">截距~ N( μ=0，σ=20)</li><li id="4e28" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">斜率~半柯西(β=10)</li><li id="226f" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">μ =截距+斜率* x</li><li id="f7b7" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">σ ~半柯西(β=10)</li><li id="14dc" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">μ =截距+斜率* x</li><li id="7a55" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">y ~ N(μ=μ，σ=σ)</li></ul><p id="0335" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们使用Hamiltonian MC No U-Turn采样器(NUTS)从10000个随机MCMC样本中创建模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/ee35ca005b3012a37f67660c1029d0bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*l8RfVhxaiDCEwZLAeW_Zkg.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/09122c7e4dfdf7a7177acbba75184176.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*M7dbzAsHqX05exxScABXWQ.png"/></div></figure><p id="a4cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似于<em class="lv"> frequentist线性回归</em>模型，我们可以看到模型得出了相同的结果:斜率在2左右，截距在1左右。然而，参数的分布表明模型仍然是不确定的，例如94%的</p><p id="d053" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，斜率的94%的最高密度区间在1.9和2.3之间，这仍然是一个相当宽的区间。拦截也是如此。贝叶斯推理的伟大之处在于，我们提供的数据越多，模型就变得越确定。这可以在下一个示例中观察到，在该示例中，用500个数据点(总数据的一半)对模型进行了训练。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/6bbf441248f78100d1b06c667b62a569.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*VwZeRdWJeKFfYsZnBiI78w.png"/></div></figure><p id="1b6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，与上述模型相比，斜率的区间在[1.7，2.3]之间，而不是[1.9，2.3]。</p><p id="d11d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦回归模型被训练，就有可能进行预测。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/1950f64001058cc7805974ff19e782ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*VmKjjcinCYYnY735UjG2BA.png"/></div></figure><p id="37e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，当增加数据数量时，也可以观察到模型的确定性，因为右边的分布(1000个数据点)比左边的分布(500个数据点)窄。</p><h2 id="cb66" class="no mm it bd mn np nq dn mr nr ns dp mv li nt nu mx lm nv nw mz lq nx ny nb nz bi translated"><em class="op">更新先验贝叶斯回归模型</em></h2><p id="19a9" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">最后，这是一个如何用新数据重新训练模型的例子。为此，我们创建了一个包含两个独立变量的新数据集。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="b1d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是上面为前100个数据点创建的相同模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="b430" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了更新我们关于参数的信念，我们使用后验分布，它将在后验地用作下一个推断的先验分布。</p><p id="5774" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用于每次推理迭代的数据必须独立于之前的迭代。否则，相同的(可能是错误的)信念会被一遍又一遍地注入系统，放大错误并误导推论。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/dbde1f5cc6195ad047638755dfa2db9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oahoKOiQa6S6w2Rt-eeayw.png"/></div></div></figure><p id="b45a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该图显示了每次迭代的回归模型的分布。可以观察到，我们用新数据训练模型越多，参数就越接近设定值。</p></div><div class="ab cl or os hx ot" role="separator"><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow"/></div><div class="im in io ip iq"><p id="0669" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">如果你喜欢这个帖子，请考虑</em> </strong> <a class="ae ky" href="https://javiferfer.medium.com/membership" rel="noopener"> <strong class="lb iu"> <em class="lv">订阅</em> </strong> </a> <strong class="lb iu"> <em class="lv">。你将获得我所有的内容+所有其他来自牛逼创作者的文章！</em>T13】</strong></p></div><div class="ab cl or os hx ot" role="separator"><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow"/></div><div class="im in io ip iq"><h1 id="ff1f" class="ml mm it bd mn mo oy mq mr ms oz mu mv jz pa ka mx kc pb kd mz kf pc kg nb nc bi translated">参考</h1><p id="724e" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">[1] <a class="ae ky" href="https://alpopkes.com/posts/machine_learning/bayesian_linear_regression/" rel="noopener ugc nofollow" target="_blank">贝叶斯线性回归</a></p><p id="08f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]中等，<a class="ae ky" rel="noopener" target="_blank" href="/bayesian-linear-regression-in-python-using-machine-learning-to-predict-student-grades-part-2-b72059a8ac7e">贝叶斯线性回归中的Python </a></p><p id="d82f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] PyMC3，<a class="ae ky" href="https://docs.pymc.io/en/v3/pymc-examples/examples/pymc3_howto/updating_priors.html" rel="noopener ugc nofollow" target="_blank">更新先验知识</a></p><p id="77d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4] Kaggle，<a class="ae ky" href="https://www.kaggle.com/billbasener/bayesian-linear-regression-in-pymc3" rel="noopener ugc nofollow" target="_blank">pymc 3中的贝叶斯线性回归</a></p><p id="7d56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[5] GitHub，<a class="ae ky" href="https://github.com/krasserm/bayesian-machine-learning/blob/dev/bayesian-linear-regression/bayesian_linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">贝叶斯线性回归</a></p><p id="5cc7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[6]中，<a class="ae ky" rel="noopener" target="_blank" href="/bayesian-inference-and-markov-chain-monte-carlo-sampling-in-python-bada1beabca7">贝叶斯推理和马尔可夫链蒙特卡罗抽样中的Python </a></p></div></div>    
</body>
</html>