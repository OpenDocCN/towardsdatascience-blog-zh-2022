<html>
<head>
<title>Adversarial Robustness For Embedded Vision Systems</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">嵌入式视觉系统的对抗鲁棒性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adversarial-robustness-for-embedded-vision-systems-773ee518fbf0#2022-06-11">https://towardsdatascience.com/adversarial-robustness-for-embedded-vision-systems-773ee518fbf0#2022-06-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a7b3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">什么是恶意攻击，如何保护您的嵌入式设备免受这些攻击</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3710dd196ff700511f3e2002b8de4876.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kQte8y5-UZyD5V7JK6VveA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h2 id="3e56" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">快速介绍</h2><p id="8b6d" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">由于嵌入式系统的<em class="mk">相对健壮的</em>深度神经网络(DNN)的选项非常有限，本文试图提供该领域的入门知识，并探索一些现成的框架。</p><h2 id="6911" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">什么是<em class="ml">对抗强大的</em> DNN？</h2><p id="8816" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">深度神经网络已经使机器学习和推理大众化。这有两个主要原因。首先，我们不需要从目标数据集中查找和设计要素，DNN会自动完成这项工作。其次，预训练模型的可用性有助于我们快速创建特定于任务的、经过微调/转移的学习模型。</p><p id="86f0" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">尽管dnn广受欢迎，但它有一个严重的弱点，这使得这些网络无法用于现实生活中的安全关键系统。尽管DNN模型被证明对随机噪声相当稳健，但它无法抵抗精心设计的对输入的<em class="mk">对抗性扰动</em>。</p><p id="a12d" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">在计算机视觉中，对抗性攻击通过像素的微小变化来改变输入图像，使得这些变化对于人类来说仍然是视觉上不可察觉的，但是DNN不能正确地推断图像。后果可能非常严重。自动驾驶车辆的交通标志识别模块可能会将左转解释为右转路标，并掉入沟渠中！光学字符识别器(OCR)可能会错误地读取数字，从而导致财务欺诈。</p><p id="29b0" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">幸运的是，许多专心致志的研究人员正在努力创造出对抗稳健的DNN模型，这种模型不会轻易被对抗扰动所愚弄。</p><h2 id="2cba" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">对抗鲁棒性对嵌入式视觉重要吗？</h2><p id="97e6" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated"><em class="mk">绝对。</em>思科预测，到2023年，机器对机器(M2M)连接数量将达到147亿[8]；来自Linux基金会的这份报告[9]预计到2028年<em class="mk">边缘计算</em>的功率足迹将达到102千兆瓦(MW)，Statista预测到2030年将有77.02亿个边缘支持的物联网设备[10]。许多<em class="mk">边缘计算</em>应用都是嵌入式视觉相关应用，通常部署在安全关键系统中。然而，对于嵌入式系统来说，对抗鲁棒性的选择并不多。我们正在积极研究这一领域，我将在这里提到我们最近的一项研究。</p><p id="7a11" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">但是在深入研究嵌入式系统的对抗性健壮性之前，我将在下一节给出这个主题的简单背景。已经熟悉这个题目的读者可以跳到下一部分。</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h2 id="cc22" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">对抗性鲁棒性引物</h2><p id="bedd" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">对抗性鲁棒性是一个非常流行和快速发展的研究领域。在这个博客[1]中，你可以看到致力于这一领域的大量研究文章。</p><p id="b6ce" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">对敌对图像最大的担忧是几乎不可能发现图像被篡改。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/c2d42f2e3081b54fcba30cd0fc3f061a.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*EYdSGz0csTC2sAgbxa9a1A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">列1:原始图像，标准CNN推理，列3:被攻击的图像，标准CNN推理列2:原始图像，鲁棒CNN推理，列4:被攻击的图像，鲁棒CNN推理(Sym)手写数字8取自<a class="ae mz" href="http://yann.lecun.com/exdb/publis/index.html#lecun-98" rel="noopener ugc nofollow" target="_blank">MNIST</a>【15】在<a class="ae mz" href="https://creativecommons.org/licenses/by-sa/3.0/" rel="noopener ugc nofollow" target="_blank">知识共享署名-共享3.0许可</a>下，由作者使用类可视化的图像集合</p></figure><p id="6864" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">例如，上图中来自<a class="ae mz" href="http://yann.lecun.com/exdb/publis/index.html#lecun-98" rel="noopener ugc nofollow" target="_blank"> MNIST </a>数据集【15】的<em class="mk"> a手写数字8 </em>被标准的卷积神经网络(CNN)推理误归类为<em class="mk">数字2 </em>。你能看出第一栏和第三栏中的图像有什么显著的不同吗？我不能。</p><p id="785f" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">CNN做了。对手做了一些微小的改变，这些改变对人眼来说并不重要，但它对CNN的决策过程造成了严重破坏。然而，当我们使用健壮的推理机制(由Sym表示)时，我们看到被攻击的图像也被正确分类(列4)。</p><p id="7b29" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">让我们从另一个角度想一想。第3栏的图片从任何角度看都是<em class="mk">数字2 </em>吗？不幸的是，CNN是这么认为的。这是一个更大的问题。对手可以调整任何图像，哄出一个有利于它的决定，以实现其恶意的目标。</p><p id="f63a" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">事实上，这些对立的例子就是这样被发明出来的。正如Ian Goodfellow在本次演讲中所述(大约7分钟)，他们在探索CNN运作如此之好的原因。在这个过程中，他们想对不同类别的图像进行小的改变，这样这些图像就变成了<em class="mk">飞机</em>类别的图像。虽然他们期望某种飞机特有的特征(比如说，机翼)能够在飞船图像中显现出来，但这种事情并没有发生。在对图像做了一些改变后，他们的CNN开始自信地推断出一张完全看起来像一艘<em class="mk">船</em>的图像，作为一架<em class="mk">飞机。</em></p><p id="71f8" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">现在，有了基本的概述，让我快速总结一下开始对抗鲁棒性需要知道的要点:</p><ol class=""><li id="6f3d" class="na nb iq lt b lu mm lx mn le nc li nd lm ne mj nf ng nh ni bi translated"><strong class="lt ir"> <em class="mk">对抗性攻击</em> </strong>是<em class="mk">优化</em>算法，用于找到DNN做出错误决策的示例输入。有趣的是，这样的例子一点也不难找到，相反，它们几乎存在于输入流形的任何地方。尽管标准的、基于经验风险最小化的训练方法在训练DNN模型时可能不包括这样的例子。</li><li id="322d" class="na nb iq lt b lu nj lx nk le nl li nm lm nn mj nf ng nh ni bi translated">一个好的<strong class="lt ir"> <em class="mk">对抗性攻击</em> </strong> <em class="mk">快速</em>找出最有效的对抗性例子。<em class="mk">有效</em>对抗图像在视觉上与原始图像相似(就距离度量而言)，但迫使模型将其映射到不同于原始图像的决策边界。距离度量是曼哈顿距离(L1范数)、欧几里德距离(L2范数)或切比雪夫距离(L∞范数)。</li><li id="462b" class="na nb iq lt b lu nj lx nk le nl li nm lm nn mj nf ng nh ni bi translated">如果对手的目标是迫使DNN将决策映射到<em class="mk">任何错误的</em>类，则称为<strong class="lt ir"> <em class="mk">无目标</em> </strong>攻击。相反，在<strong class="lt ir"> <em class="mk">有针对性的</em> </strong>攻击中，对手迫使DNN映射到对手期望的特定类别<em class="mk">。</em></li><li id="7135" class="na nb iq lt b lu nj lx nk le nl li nm lm nn mj nf ng nh ni bi translated"><strong class="lt ir"> <em class="mk">扰动</em> </strong>以每像素的变化来度量，通常用ϵ.来表示例如，0.3的ϵ将意味着干净示例(原始图像)的每个像素在相应的相反示例中经历0.3的最大变化<em class="mk">。</em></li><li id="b6cf" class="na nb iq lt b lu nj lx nk le nl li nm lm nn mj nf ng nh ni bi translated">用<em class="mk">清晰</em>图像和<em class="mk">对抗</em>图像测量的DNN的精度通常分别称为<strong class="lt ir"> <em class="mk">清晰精度</em> </strong>和<strong class="lt ir"> <em class="mk">稳健精度</em> </strong>。</li><li id="b66f" class="na nb iq lt b lu nj lx nk le nl li nm lm nn mj nf ng nh ni bi translated">对手可以攻击DNN推论，而不需要访问模型、训练细节和对测试数据分布的有限估计。这个<strong class="lt ir"> <em class="mk">威胁模型</em> </strong>叫做一个完整的<strong class="lt ir"> <em class="mk">黑盒</em> </strong>设置【2】。这可以像在一个真实的物体上贴一个小的物理标签一样简单，DNN正试图推断出这一点。在另一个极端，完整的<strong class="lt ir"> <em class="mk">白盒</em> </strong>设置指的是一种攻击模型，其中的模型、训练细节，甚至防御机制都是对手已知的。这被认为是对抗性辩护的考验。在上述两种设定的<em class="mk">中级</em>下评估不同的攻击和防御。</li><li id="f09c" class="na nb iq lt b lu nj lx nk le nl li nm lm nn mj nf ng nh ni bi translated"><strong class="lt ir"> <em class="mk">对抗性防御</em> </strong>旨在挫败对抗性攻击。对抗鲁棒性可以通过<em class="mk">经验</em>防御(例如，使用对抗示例的对抗训练)或<em class="mk">启发式</em>防御(例如，预处理被攻击的图像以消除干扰)来实现。然而，这些方法经常通过实验结果得到验证，使用<em class="mk">然后是</em>攻击。没有理由为什么一个新的更强的对手不能在以后打破这些防御。这正是对抗性鲁棒性研究中正在发生的事情，通常被称为<em class="mk">猫捉老鼠游戏</em>。</li></ol><p id="996a" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">对抗性研究社区旨在解决最后一点中强调的这种追赶游戏。该领域目前的研究重点是开发<strong class="lt ir"> <em class="mk">认证和可证明的防御</em> </strong>。这些防护修改模型操作，使得这些操作对于输入值的范围是可证明正确的，或者使用高斯噪声执行分类器的随机平滑，等等。</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h2 id="c99b" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">嵌入式视觉系统的对抗性鲁棒性？</h2><p id="3d85" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">到目前为止，对抗性研究团体并没有关注嵌入式系统的对抗性健壮性。这是完全正常的，因为这个领域本身仍在形成之中。</p><p id="2ca8" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">我们在卷积神经网络中看到了同样的趋势。AlexNet [12]在2012年赢得ILSVRC挑战赛[14]后，DeepCompression [11]花了四年时间才率先生成小型修剪模型。然而，在那个时期，没有办法完全利用这些修剪过的模型，因为没有既定的硬件支持来<em class="mk">在CNN推断期间利用稀疏性</em>。最近，这种支持正在发展，研究人员甚至在微控制器上运行CNN。</p><p id="a375" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">嵌入式系统的对抗性防御必须在<em class="mk">大小</em>、运行时间<em class="mk">内存</em>、<em class="mk"> CPU周期</em>和<em class="mk">能量</em>消耗方面<em class="mk">非常低的开销</em>。这立即取消了最佳可用选项，即对抗性训练模型。这些型号<em class="mk">巨大</em>，<a class="ae mz" href="http://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR-10 </a>约500 MB(此处<a class="ae mz" href="https://robustbench.github.io/" rel="noopener ugc nofollow" target="_blank">见</a>)。正如本文[3]中恰当解释的那样，与标准模型相比，对抗性训练的模型倾向于学习更复杂的决策区域。这种可证明的强大防御还没有扩展到现实生活中的网络。使用生成式对抗网络(GAN)或自动编码器来净化受攻击图像的启发式防御对于嵌入式设备来说太耗费资源了。</p><p id="a626" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">这使得基于简单<em class="mk">启发式输入转换的</em>防御成为资源有限设备的唯一选择。然而，正如开创性的著作[4–5]所示，所有这些防御都依赖于某种形式的<em class="mk">梯度混淆</em>，并且最终可以被强大的<em class="mk">适应性</em>攻击打破。这种防御可以带来一些好处的方式是使转变变得非常强大。这不能抵御强大的适应性攻击，但是破坏转换的时间将取决于对手可获得的计算资源。</p><p id="5ef4" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">其中一个成功的作品[6]使用了一系列简单的变换<em class="mk"/>,例如降低颜色的位分辨率、JPEG压缩、噪声注入等。它们一起防御[4]中介绍的<em class="mk">反向传递可微逼近</em> (BPDA)攻击。遗憾的是，本文没有公开的实现可供尝试。</p><p id="2c27" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">另一个最近的工作[7]使用输入<em class="mk">离散化</em>在DNN推理之前净化对立的图像。在<em class="mk">净化</em>过程中，位于不受限制的实数域中的图像像素值被转换成一组受限的离散值。由于这些离散值是从大型干净的图像数据集中学习的，因此转换<em class="mk">难以打破</em>，并且替换移除了一些不利的扰动。</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><p id="552e" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated"><strong class="lt ir">有人动手… </strong></p><p id="5247" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">首先，让我们看一些示例攻击，以创建一些敌对的图像。</p><p id="9ff1" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">我们可以使用任何现有的库，例如<a class="ae mz" href="https://github.com/bethgelab/foolbox" rel="noopener ugc nofollow" target="_blank"> Foolbox </a>、<a class="ae mz" href="https://github.com/Harry24k/adversarial-attacks-pytorch" rel="noopener ugc nofollow" target="_blank">对抗性攻击pytorch </a>、<a class="ae mz" href="https://github.com/Trusted-AI/adversarial-robustness-toolbox" rel="noopener ugc nofollow" target="_blank"> ART </a> …,等等。</p><p id="1fed" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">下面是一个用<a class="ae mz" href="https://github.com/Harry24k/adversarial-attacks-pytorch" rel="noopener ugc nofollow" target="_blank">对抗性攻击-pytorch </a>使用AutoAttack的实现来攻击和创建对抗性例子的例子[13]:</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="ab6f" class="kv kw iq np b gy nt nu l nv nw">from torchattacks import *</span><span id="c54f" class="kv kw iq np b gy nx nu l nv nw">attack = AutoAttack(your_dnn_model, eps=8/255, n_classes=10, version='standard')</span><span id="bed2" class="kv kw iq np b gy nx nu l nv nw">for images, labels in your_testloader:<br/>    attacked_images = attack(images, labels)</span></pre><p id="35c6" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">这里是另一个用<a class="ae mz" href="https://github.com/bethgelab/foolbox" rel="noopener ugc nofollow" target="_blank"> Foolbox </a>使用投射梯度下降攻击[3]攻击和创造对抗性例子的例子:</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="9c9c" class="kv kw iq np b gy nt nu l nv nw">import foolbox</span><span id="aa19" class="kv kw iq np b gy nx nu l nv nw">attack = foolbox.attacks.PGD(your_dnn_model)</span><span id="94c8" class="kv kw iq np b gy nx nu l nv nw">for images, labels in your_testloader:<br/>    attacked_images = attack(images, labels)</span></pre><p id="8d02" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">这些<em class="mk">被攻击的图像</em>可用于通过DNN进行推断。</p><p id="3470" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">接下来，让我们尝试用SymDNN来防御攻击。使用这个库的第一步是从<a class="ae mz" href="https://github.com/swadeykgp/SymDNN" rel="noopener ugc nofollow" target="_blank">这个</a>链接克隆或下载这个库。</p><p id="2362" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">下一步是安装SymDNN的依赖项。存储库的自述文件中描述了这一步骤和python可选虚拟环境设置。</p><p id="07e5" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">接下来，我们需要为<em class="mk">离散化</em>设置一些<em class="mk">默认参数</em>。所提供的缺省值对于一些数据集和攻击来说工作良好。我们可以稍后调整这些，以进行推断<em class="mk">健壮性-延迟-内存</em>的权衡。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="b6a3" class="kv kw iq np b gy nt nu l nv nw"># For similarity search<br/>import faiss<br/>import sys<br/>sys.path.insert(1, './core')</span><span id="bfc4" class="kv kw iq np b gy nx nu l nv nw"># function for purification of adversarial perturbation<br/>from patchutils import symdnn_purify</span><span id="b9a4" class="kv kw iq np b gy nx nu l nv nw"># Setup robustness-latency-memory tradeoff parameters, defaults are good enough, then call the purification function on attacked image</span><span id="c777" class="kv kw iq np b gy nx nu l nv nw">purified_image = symdnn_purify(attacked_image, n_clusters, index, centroid_lut, patch_size, stride, channel_count)</span></pre><p id="6b09" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">这些<em class="mk">净化后的</em>图像可用于使用DNN的进一步推断。这个净化功能在<em class="mk">处理</em>和<em class="mk">内存</em>方面极其<em class="mk">低开销</em>，适合嵌入式系统。</p><p id="40ea" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">SymDNN存储库包含在几种攻击和不同威胁模型下对CIFAR-10、ImageNet和MNIST的许多攻击示例。</p><p id="8cfa" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">这是一个在SymDNN中使用<em class="mk"> BPDA攻击</em>【4】实现攻击和创建对抗示例的示例，摘自<a class="ae mz" href="https://github.com/anishathalye/obfuscated-gradients" rel="noopener ugc nofollow" target="_blank">此处</a>:</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="17d5" class="kv kw iq np b gy nt nu l nv nw">bpda_adversary16 = BPDAattack(your_dnn_model, None, None, epsilon=16/255, learning_rate=0.5, max_iterations=100)</span><span id="6dab" class="kv kw iq np b gy nx nu l nv nw">for images, labels in your_testloader:<br/>    attacked_images = bpda_adversary16.generate(images, labels)</span></pre><p id="a917" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated"><em class="mk">攻击强度</em>，<em class="mk">迭代</em>等。<em class="mk">超参数</em>在原始论文【4】中有解释，通常取决于应用场景。</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h2 id="3f57" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">结论</h2><p id="ac63" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">尽管对抗性鲁棒性是一个经过充分研究的领域，但我们认为，在它能够成功部署到现场之前，有几个点需要连接，特别是在<em class="mk">边缘计算</em>场景中。我们提出的SymDNN防御，不能够<em class="mk">撤销</em>所有攻击。然而，它可以很容易地用于使嵌入式视觉系统对一些敌对攻击具有鲁棒性。随着嵌入式人工智能(AI)在未来迅速发展，手头有<em class="mk">东西</em>来保护它们总比什么都没有好。</p><h2 id="202c" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">参考</h2><p id="ca41" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">[1] Nicholas Carlini，<em class="mk">所有(arXiv)对抗性示例论文完整列表</em> (2019)，<a class="ae mz" href="https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html" rel="noopener ugc nofollow" target="_blank">https://Nicholas . Carlini . com/writing/2019/All-Adversarial-Example-Papers . html</a></p><p id="16d0" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">[2] Nicolas Papernot，Patrick McDaniel，Ian Goodfellow，Somesh Jha，Z. Berkay切利克和Ananthram Swami，<em class="mk">针对机器学习的实际黑盒攻击</em> (2017)，在2017年亚洲计算机与通信安全会议上</p><p id="2d45" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">[3] Aleksander Madry，Aleksandar Makelov，Ludwig Schmidt，Dimitris Tsipras，和Adrian Vladu，T <em class="mk"> owards深度学习模型抵抗对抗性攻击</em> (2018)，在2018年ICLR</p><p id="eec6" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">[4] Anish Athalye，Nicholas Carlini和David A. Wagner，<em class="mk">模糊的梯度给人一种错误的安全感:绕过对抗例子的防御</em> (2018)。在2018年的ICML</p><p id="4fd2" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">[5] Florian Tramèr，Nicholas Carlini，维兰德·布伦德尔和亚历山大·马德瑞，<em class="mk">关于对抗性示例防御的适应性攻击</em> (2020)。在NeurIPS 2020中</p><p id="4ac0" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">[6] Edward Raff，Jared Sylvester，Steven Forsyth和Mark McLean，<em class="mk">随机变换的弹幕，用于对抗性的稳健防御</em> (2019)，2019年在CVPR举行</p><p id="8b72" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">[7] Swarnava Dey、Pallab Dasgupta和Partha P Chakrabarti，<em class="mk"> SymDNN:简单&amp;嵌入式系统的有效对抗鲁棒性</em> (2022)，CVPR嵌入式视觉研讨会2022</p><p id="3ffa" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">[8]思科、<em class="mk">思科年度互联网报告(2018–2023)</em>、<a class="ae mz" href="https://www.cisco.com/c/en/us/solutions/collateral/326" rel="noopener ugc nofollow" target="_blank">https://www.cisco.com/c/en/us/solutions/collateral/</a>高管视角/年度互联网报告/白皮书-c11–741490 . html</p><p id="92b1" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">Linux基金会，<em class="mk">State of The edge 2021</em>(2021)<a class="ae mz" href="https://stateoftheedge.com/reports/state-of-the-edge-report-2021/" rel="noopener ugc nofollow" target="_blank">https://stateofthedge . com/reports/State-of-The-edge-report-2021/</a></p><p id="590a" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">[10] Statista，<em class="mk">2020年至2030年全球物联网(iot)设备的数量，按市场划分</em> (2022年)，<a class="ae mz" href="https://www.statista.com/statistics/1259878/edge-enabled-iot-device-market-worldwide/," rel="noopener ugc nofollow" target="_blank">https://www . Statista . com/statistics/1259878/edge-enabled-IOT-device-market-world wide/，</a> 2022年</p><p id="239b" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">[11]宋涵，，毛，威廉·戴利，<em class="mk">深度压缩:通过剪枝、训练量化和霍夫曼编码压缩深度神经网络</em> (2016)，2016</p><p id="3853" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">[12] A. Krizhevsky、I. Sutskever和G. E. Hinton，<em class="mk">使用深度卷积神经网络的Imagenet分类</em> (2012年)，载于NeurIPS 2012年</p><p id="70b0" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">[13] Francesco Croce和Matthias Hein，<em class="mk">对各种无参数攻击集合的对抗性鲁棒性的可靠评估</em> (2020)，ICML 2020</p><p id="ae6c" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">[14] Olga Russakovsky*、Jia Deng*、苏浩、Jonathan Krause、Sanjeev Satheesh、Sean Ma、、Andrej Karpathy、Aditya Khosla、Michael Bernstein、Alexander C. Berg和李菲菲。(* =同等贡献)<em class="mk"> ImageNet大规模视觉识别挑战赛。</em> <em class="mk"> IJCV，</em> 2015</p><p id="5b7c" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">[15] Y. LeCun、L. Bottou、Y. Bengio和P. Haffner。<em class="mk">基于梯度的学习应用于文档识别。美国电气和电子工程师学会会议录，86(11):2278–2324，1998年11月</em></p></div></div>    
</body>
</html>