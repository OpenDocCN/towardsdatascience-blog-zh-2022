<html>
<head>
<title>Regularization Techniques for Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络的正则化技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/regularization-techniques-for-neural-networks-379f5b4c9ac3#2022-08-24">https://towardsdatascience.com/regularization-techniques-for-neural-networks-379f5b4c9ac3#2022-08-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="02ae" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">当训练深度神经网络时，在训练集和验证集上实现相同的性能通常很麻烦。验证集上相当高的误差是过度拟合的明显标志:网络在训练数据中变得过于专门化。在本文中，我提供了一个关于如何绕过这个问题的综合指南。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/482a7a24825752f05913d4ecca1fdb04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bq80v10pNnyvplKPHyU-yg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:pixabay.com</p></figure><h1 id="b873" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">神经网络中的过拟合</h1><p id="cc6e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在处理任何机器学习应用程序时，对模型的偏差和方差有一个清晰的理解是很重要的。在传统的机器学习算法中，我们谈到了<a class="ae kv" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#:~:text=In%20statistics%20and%20machine%20learning,bias%20in%20the%20estimated%20parameters." rel="noopener ugc nofollow" target="_blank">偏差与方差权衡</a>，这包括试图最小化模型的方差和偏差时的斗争。为了减少模型的偏差，即减少错误假设的误差，我们需要一个更复杂的模型。相反，减少模型的方差，即模型在捕捉训练数据的变化时的灵敏度，意味着更简单的模型。很明显，在传统的机器学习中，偏差与方差的权衡来自于同时需要更复杂和更简单的模型的冲突。</p><p id="f93e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在深度学习时代，有一些工具可以只减少模型的方差而不损害模型的偏差，或者相反，减少偏差而不增加方差。在探索用于防止神经网络过度拟合的不同技术之前，澄清高方差或高偏差的含义是很重要的。</p><p id="8a39" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">考虑一个常见的神经网络任务，如图像识别，并考虑一个识别图片中熊猫存在的神经网络。我们可以自信地估计，人类可以以接近0%的误差完成这项任务。因此，这是图像识别网络准确性的合理基准。在训练集上训练神经网络并评估其在训练集和验证集上的性能后，我们可能会得出这些不同的结果:</p><ol class=""><li id="6fd3" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj mu mv mw mx bi translated">训练误差= 20%，验证误差= 22%</li><li id="8e82" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated">训练误差= 1%，验证误差= 15%</li><li id="ad29" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated">训练误差= 0.5%，验证误差= 1%</li><li id="92f7" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated">训练误差= 20%，验证误差= 30%</li></ol><p id="9b78" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">第一个例子是高偏差的典型实例:训练集和验证集的误差都很大。相反，第二个例子的方差很大，在处理模型没有从中学习的数据时，准确性要低得多。第三个结果代表低方差和偏差，模型可以被认为是有效的。最后，第四个示例显示了高偏差和方差的情况:与基准相比，不仅训练误差大，而且验证误差也更高。</p><p id="de5e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">从现在开始，我将介绍几种正则化技术，用于减少模型对训练数据的过度拟合。它们对于情况2是有益的。第四。前一个例子。</p><h1 id="022c" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">神经网络的L1和L2正则化</h1><p id="5390" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">类似于经典回归算法(线性、逻辑、多项式等。)，L1和L2正则化也为防止高方差神经网络中的过拟合找到了位置。为了保持这篇文章简短切题，我不会回忆L1和L2正则化是如何在回归算法上工作的，但是你可以查看<a class="ae kv" href="https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization" rel="noopener ugc nofollow" target="_blank">这篇文章</a>以获得更多信息。</p><p id="2424" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">L1和L2正则化技术背后的思想是将模型的权重限制得更小，或者将其中一些缩小到0。</p><p id="5522" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">考虑经典深度神经网络的成本函数J:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/74851e40e220fbd23aa00087890900a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*-N86OtfsYownsXoe8yl-ww.png"/></div></figure><p id="ba40" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">当然，成本函数j是每一层的权重和偏差的函数，L. m是训练样本的数量，ℒ是损失函数。</p><h2 id="ecf6" class="ne kx iq bd ky nf ng dn lc nh ni dp lg lx nj nk li mb nl nm lk mf nn no lm np bi translated">L1正则化</h2><p id="d56a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在L1正则化中，我们将以下项添加到成本函数J中:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/6708fd5d805a36ef04965644cd8919aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:220/format:webp/1*ugLBtbX1DKIFWy1YZ22qmg.png"/></div></figure><p id="3d9e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">其中矩阵范数是网络的每一层1，…，L的权重的绝对值之和:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/e0840263ec9997187da46ac6795bb596.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/format:webp/1*_4QKzvI_rs17YiYwcnQBlQ.png"/></div></figure><p id="5964" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">λ是正则项。这是一个必须仔细调整的超参数。λ直接控制正则化的影响:随着λ增加，对权重收缩的影响更加严重。</p><p id="5f8b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">L1正则化下的完整成本函数变成:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/8696d7d55275f3fef2b4ca9d1585bfb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*UNHir4t0uAw-FuLQWsd6GA.png"/></div></figure><p id="7bb6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对于λ=0，L1正则化的效果为零。相反，选择太大的λ值会过度简化模型，很可能导致网络不匹配。</p><p id="d2da" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">L1正则化可以被视为一种神经元选择过程，因为它会使一些隐藏神经元的权重为零。</p><h2 id="6c1a" class="ne kx iq bd ky nf ng dn lc nh ni dp lg lx nj nk li mb nl nm lk mf nn no lm np bi translated">L2正则化</h2><p id="bdd2" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在L2正则化中，我们添加到成本函数的项如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/b08ca79a0cf0042384850dd21cca9cd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:290/format:webp/1*i9WoOD-by_9iwRQEDHxl9g.png"/></div></figure><p id="4b66" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在这种情况下，正则项是每个网络图层权重的平方范数。这个矩阵范数称为Frobenius范数，具体计算如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/771b8017371792e5895c8f3b36983455.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*pAA2Cter2B3-qfkFDXshug.png"/></div></figure><p id="a21c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">请注意，相对于层l的权重矩阵具有n^{[l]}行和n^{[l-1]}列。</p><p id="21c5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，L2正则化下的完整成本函数变成:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/20df25a8f8630e67332916e96f232a1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*oxp-gleLyXbOFbpNmqf5zQ.png"/></div></figure><p id="09f4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">同样，λ是正则项，对于λ=0，L2正则化的效果为零。</p><p id="3f32" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">L2正则化使权重值趋于零，从而产生更简单的模型。</p><h2 id="4d62" class="ne kx iq bd ky nf ng dn lc nh ni dp lg lx nj nk li mb nl nm lk mf nn no lm np bi translated">L1和L2正则化如何减少过度拟合？</h2><p id="5978" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">L1和L2正则化技术对训练数据的过拟合有积极影响，原因有两个:</p><ul class=""><li id="0467" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj nw mv mw mx bi translated">一些隐藏单元的权重变得更接近(或等于)0。结果，它们的作用被削弱了，最终的网络更简单了，因为它更接近于一个更小的网络。如介绍中所述，较简单的网络不容易过度拟合。</li><li id="2d9c" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj nw mv mw mx bi translated">对于较小的权重，隐藏神经元的激活函数的输入z也变得较小。对于接近0的值，许多激活函数表现为线性。</li></ul><p id="c4cb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">第二个原因不是微不足道的，值得扩展。考虑双曲正切(tanh)激活函数，其图形如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/c870348ac160bf803458abf429616288.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/1*FixIQ1pKLkKoFEbXE3prbA.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">双曲正切值</p></figure><p id="c658" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">从函数图中我们可以看出，如果输入值x很小，函数tanh(x)几乎呈线性。当tanh用作神经网络隐藏层的激活函数时，输入值为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/fef52ba778f11544a14bbc260189c706.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/format:webp/1*6swg9QB3Vzou2MrhtF8qtw.png"/></div></figure><p id="124d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这对于小重量w也接近于零。</p><p id="4b7e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果神经网络的每一层都是线性的，我们可以证明整个网络的行为是线性的。因此，约束一些隐藏单元来模拟线性函数，导致更简单的网络，并且因此有助于防止过度拟合。</p><p id="78c9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">一个更简单的模型通常是<a class="ae kv" rel="noopener" target="_blank" href="/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036">不能捕捉训练数据中的噪声</a>，因此过拟合不太频繁。</p><h1 id="d95c" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">拒绝传统社会的人</h1><p id="a815" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">退出正则化的思想是随机删除网络中的一些节点。在训练过程之前，我们为网络的每个节点设置一个概率(假设p = 50%)。在训练阶段，每个节点被关闭的概率为p。退出过程是随机的，并且针对每个训练示例单独执行。因此，每个训练示例可能在不同的网络上训练。</p><p id="11f9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">至于L2正则化，丢弃正则化的结果是更简单的网络，更简单的网络导致更不复杂的模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/a1949c771af16c058a1b49153f38e052.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qvbJTLsbhD7t4_SZ7ToJeA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">简单网络中的掉线效应。来源:作者</p></figure><h2 id="a612" class="ne kx iq bd ky nf ng dn lc nh ni dp lg lx nj nk li mb nl nm lk mf nn no lm np bi translated">实际辍学</h2><p id="7dc5" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在这个简短的部分中，我将展示如何在实践中实现辍学正规化。我将通过几行简单的代码(python)。如果你只对正则化的一般理论感兴趣，你可以很容易地跳过这一节。</p><p id="c0f6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">假设我们已经在numpy数组<code class="fe oa ob oc od b">a4</code>中存储了网络第4层的激活值。首先，我们创建辅助向量<code class="fe oa ob oc od b">d4</code>:</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="fce0" class="ne kx iq od b gy oi oj l ok ol"># Set the keeping probability<br/>keep_prob = 0.7</span><span id="af5b" class="ne kx iq od b gy om oj l ok ol"># Create auxiliary vector for layer 4<br/>d4 = np.random.rand(a4.shape[0], a3.shape[1]) &lt; keep_prob</span></pre><p id="1874" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">向量<code class="fe oa ob oc od b">d4</code>具有与<code class="fe oa ob oc od b">a4</code>相同的维数，并且包含基于概率<code class="fe oa ob oc od b">keep_prob</code>的值<code class="fe oa ob oc od b">True</code>或<code class="fe oa ob oc od b">False</code>。如果我们设置70%的保持概率，这是给定的隐藏单元被保持的概率，因此，在给定的<code class="fe oa ob oc od b">d4</code>元素上具有<code class="fe oa ob oc od b">True</code>值的概率。</p><p id="0675" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们将辅助向量<code class="fe oa ob oc od b">d4</code>应用于激活<code class="fe oa ob oc od b">a4</code>:</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="325c" class="ne kx iq od b gy oi oj l ok ol"># Apply the auxiliary vector d4 to the activation a4<br/>a4 = np.multiply(a4,d4)</span></pre><p id="1b54" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，我们需要用<code class="fe oa ob oc od b">keep_prob</code>值来缩放修改后的向量<code class="fe oa ob oc od b">a4</code>:</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="539f" class="ne kx iq od b gy oi oj l ok ol"># Scale the modified vector a4<br/>a4 /= keep_prob</span></pre><p id="4dfd" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">需要这最后一个操作来补偿层中单元的减少。在训练过程中执行该操作允许我们在测试阶段不应用退出。</p><h2 id="8f46" class="ne kx iq bd ky nf ng dn lc nh ni dp lg lx nj nk li mb nl nm lk mf nn no lm np bi translated">辍学如何减少过度拟合？</h2><p id="8e48" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">退出的效果是暂时将网络转换成一个更小的网络，我们知道更小的网络不太复杂，也不容易过度拟合。</p><p id="ae69" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">考虑上面所示的网络，重点关注第二层的第一个单元。因为它的一些输入可能会由于退出而暂时关闭，所以该单元在训练阶段不能总是依赖它们。因此，隐藏单元被鼓励将它的权重分散到它的输入上。扩展权重具有降低权重矩阵的平方范数的效果，导致一种L2正则化。</p><p id="d8dd" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">设置保持概率是有效的丢失正则化的基本步骤。通常，为神经网络的每一层分别设置保持概率。对于具有大权重矩阵的层，我们通常设置较小的保持概率，因为在每一步，我们希望相对于较小的层保持成比例的较少权重。</p><h1 id="71fa" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">其他正则化技术</h1><p id="71b1" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">除了L1/L2正则化和退出，还有其他正则化技术。其中两个是数据扩充和提前停止。</p><p id="d4a1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">从理论上，我们知道在更多的数据上训练网络对减少高方差有积极的作用。由于获取更多数据通常是一项艰巨的任务，对于某些应用程序来说，数据增强是一种允许机器学习实践者几乎免费获取更多数据的技术。在计算机视觉中，数据增强通过翻转、缩放和平移原始图像来提供更大的训练集。在数字识别的情况下，我们也可以对图像施加失真。您可以检查<a class="ae kv" rel="noopener" target="_blank" href="/generating-synthetic-data-to-train-an-ocr-learning-algorithm-4889f443fe92">手写数字识别数据增强的应用</a>。</p><p id="5983" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">早期停止，顾名思义，包括在最初定义的迭代次数之前停止训练阶段。如果我们将训练集和验证集上的成本函数绘制为迭代的函数，我们可以体验到，对于过拟合模型，训练误差总是保持减小，但是验证误差可能在一定次数的迭代之后开始增大。当验证误差停止减小时，正是停止训练过程的时候。通过更早地停止训练过程，我们迫使模型更简单，从而减少过度拟合。</p></div></div>    
</body>
</html>