<html>
<head>
<title>Principal Component Analysis: Everything You Need To Know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析:你需要知道的一切</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-analysis-everything-you-need-to-know-5f834c9eaa83#2022-09-22">https://towardsdatascience.com/principal-component-analysis-everything-you-need-to-know-5f834c9eaa83#2022-09-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="5948" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">👨‍🏫<a class="ae ep" href="https://equipintelligence.medium.com/list/mathematics-demystified-7e4d1c18041f" rel="noopener">数学</a></h2><div class=""/><div class=""><h2 id="56de" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">协方差，特征值，方差和一切</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/8cd85d39e2808a34b3e400bf78e460e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UnNO5izdqhAJjR66FResfQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="fd6d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">主成分分析(PCA)是一种减少数据维度的流行技术，包含在大多数ML/DS课程的“无监督学习”部分。有很多博客和YT视频一起解释PCA，那么为什么会有这个博客呢？</p><h2 id="48d5" class="ma mb iq bd mc md me dn mf mg mh dp mi ln mj mk ml lr mm mn mo lv mp mq mr iw bi translated">又一个关于PCA的博客？</h2><p id="9287" class="pw-post-body-paragraph le lf iq lg b lh ms ka lj lk mt kd lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">作为一个更热爱数学的ML学习者，我发现PCA上的每个博客都不完整。总结我阅读的每一篇博客，我每次只能得出一个结论，<em class="mx"> PCA最大化投影数据点的方差</em>，我没有任何地方是错误的。但由于我发现到处都只有这种观点，我无法深入这个话题，以了解更多的数学知识以及特征值是如何突然出现的。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者的故事</p></figure><p id="4f00" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我决定认真对待这件事，我收集了书籍、论坛和其他博客来制作我写的这个博客。它包含了不那么简单的数学，但是我保证没有一个概念会让你挠头超过10分钟。这个故事是为那些，</p><ul class=""><li id="c3c5" class="na nb iq lg b lh li lk ll ln nc lr nd lv ne lz nf ng nh ni bi translated"><em class="mx">好奇想知道PCA背后的数学</em></li><li id="ec38" class="na nb iq lg b lh nj lk nk ln nl lr nm lv nn lz nf ng nh ni bi translated"><em class="mx">了解线性代数概念，如基、矩阵的迹、特征值和特征分解</em></li><li id="ef79" class="na nb iq lg b lh nj lk nk ln nl lr nm lv nn lz nf ng nh ni bi translated"><em class="mx">了解统计概念，如随机变量、随机向量、方差和协方差</em></li></ul><p id="2eab" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">让我知道你过得怎么样！</p><h2 id="9c8f" class="ma mb iq bd mc md me dn mf mg mh dp mi ln mj mk ml lr mm mn mo lv mp mq mr iw bi translated">小小免责声明</h2><p id="0fe7" class="pw-post-body-paragraph le lf iq lg b lh ms ka lj lk mt kd lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">这里涉及的数学将使用线性代数和统计学中的大量术语。故事中引入的每一个新术语都将被标记上一个资源，可以参考该术语的更多知识。</p><h1 id="981f" class="no mb iq bd mc np nq nr mf ns nt nu mi kf nv kg ml ki nw kj mo kl nx km mr ny bi translated">问题是</h1><p id="ce42" class="pw-post-body-paragraph le lf iq lg b lh ms ka lj lk mt kd lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">PCA的主要目的是将给定的观察变量(数据)解释成一组潜在变量，使得<a class="ae nz" href="https://en.wikipedia.org/wiki/Latent_variable" rel="noopener ugc nofollow" target="_blank">潜在变量</a>保留观察变量的大部分信息。</p><p id="0b2f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">观察变量是那些真实值被测量的变量。考虑一个问题，其中我们被要求对特定股票的价格进行建模，观察变量可以是公司进行的投资、销售、回报和其他可以直接影响股票价格的变量。</p><p id="284a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">“潜伏”这个词的意思是隐藏的或隐蔽的。在我们的例子中，变量不是直接观察到的，而是在观察到的变量上使用的模型的帮助下推断出来的。在相同的股票价格预测示例中，潜在变量可以是某人购买该股票的机会、该股票的未来价格等。</p><p id="24e3" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">就认证后活动而言，目标是，</p><blockquote class="oa"><p id="54a8" class="ob oc iq bd od oe of og oh oi oj lz dk translated">确定潜在变量，使其包含观察变量中包含的大部分信息。潜在变量的数量应该少于观察变量的数量。</p></blockquote><h2 id="6e23" class="ma mb iq bd mc md ok dn mf mg ol dp mi ln om mk ml lr on mn mo lv oo mq mr iw bi translated">为什么我们不能从数据集中去掉一些特征来降低维度呢？</h2><p id="1d73" class="pw-post-body-paragraph le lf iq lg b lh ms ka lj lk mt kd lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">在大多数真实世界的数据集中，要素或观测变量在某种程度上相互依赖。例如，在气象数据集中，降雨量可能取决于也可能不取决于特定地区的温度。如果我们直接删除温度变量，我们可能会丢失特征中包含的一些有用信息。因此，我们需要以这样一种方式转换观察到的变量，即我们可以很容易地从数据集中删除不太突出的特征，同时保留信息。</p><h2 id="cdc1" class="ma mb iq bd mc md me dn mf mg mh dp mi ln mj mk ml lr mm mn mo lv mp mq mr iw bi translated">1.寻找数据点的替代表示</h2><p id="e79d" class="pw-post-body-paragraph le lf iq lg b lh ms ka lj lk mt kd lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">首先，我们考虑包含<strong class="lg ja"> <em class="mx"> N </em> </strong>个样本的数据集，其中每个样本具有<strong class="lg ja"> D </strong>个特征。我们可以用矩阵<strong class="lg ja"> <em class="mx"> X </em> </strong>的形式表示我们的数据集，其中每行包含一个样本，每列对应一个特定特性的值。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi op"><img src="../Images/1c417b57fecab80b9831f439916b93f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/0*_0Q9qgSex3Wifhiq"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(1)定义我们的数据集和数据矩阵<strong class="bd oq"> X </strong></p></figure><p id="d5e6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在故事的后面部分，我们还会将<strong class="lg ja"> <em class="mx"> X </em> </strong>视为一个随机向量，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi or"><img src="../Images/3544c8ffcbb6c773ba9241a54df2f782.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/0*Gcm0JYVv5eUNFEam"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">随机向量X，它的每一次实现都可以被视为一个数据点。</p></figure><p id="a252" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们认为每个特征都是随机变量。你不需要担心这个，当我们考虑把<strong class="lg ja"> <em class="mx"> X </em> </strong>作为随机向量时，我会清楚地提到。</p><blockquote class="oa"><p id="4873" class="ob oc iq bd od oe of og oh oi oj lz dk translated">另外，我们做的一个重要假设是数据点以平均值为中心。我们可以通过从每个样本中减去均值向量来做到这一点，这样得到的数据点就以均值为中心。</p></blockquote><p id="5544" class="pw-post-body-paragraph le lf iq lg b lh os ka lj lk ot kd lm ln ou lp lq lr ov lt lu lv ow lx ly lz ij bi translated">我们希望用新的基向量来表示这些数据点。一个<a class="ae nz" href="https://en.wikipedia.org/wiki/Basis_(linear_algebra)" rel="noopener ugc nofollow" target="_blank">基</a>是一组跨越它们周围向量空间的独立向量。更简单地说，基(或基向量)中所有向量的线性组合产生了向量空间中存在的每个向量。新的基础将为特征提供另一种表示，通过这种表示，我们可以很容易地去掉不太重要的特征。通过选择新的基，我们还需要新的坐标来表示数据点。</p><p id="604d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这些基向量很特别，因为我们可以选择需要多少基向量，来得到想要的维数。但是，我们需要仔细选择这些基准，以便它们能使<em class="mx">几乎与</em>完全相同的数据点，而没有任何<em class="mx">相当大的损失</em>。为了建立这样一个基础，我们需要，</p><ul class=""><li id="8447" class="na nb iq lg b lh li lk ll ln nc lr nd lv ne lz nf ng nh ni bi translated"><a class="ae nz" href="https://en.wikipedia.org/wiki/Orthonormal_basis" rel="noopener ugc nofollow" target="_blank">标准正交基</a>以替代方式表示数据点(如上所述)。基向量的数量等于向量空间的维数，所以在我们的例子中，我们需要<strong class="lg ja"><em class="mx"/></strong>个基向量。我们的新基将跨越我们的数据点所在的相同向量空间。所以，在新的基中，我们会有<strong class="lg ja"> D </strong>向量。</li><li id="7a5b" class="na nb iq lg b lh nj lk nk ln nl lr nm lv nn lz nf ng nh ni bi translated"><a class="ae nz" href="https://en.wikipedia.org/wiki/Coordinate_vector" rel="noopener ugc nofollow" target="_blank">坐标</a>用新的标准正交基唯一地表示每个数据点<strong class="lg ja"> <em class="mx"> x_i </em> </strong>。</li></ul><p id="0e51" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们可以在D维空间中定义新的标准正交基，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/4f41fed551f6c4cf7fe899595df52675.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/0*gDdsNLIGVNfCcS08"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(2)定义一个新的标准正交基。标准正交基意味着基向量彼此正交，并且具有单位幅度。</p></figure><p id="b768" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">将所有这些正交向量收集到一个矩阵中，其中每列包含一个向量，我们剩下一个<a class="ae nz" href="https://en.wikipedia.org/wiki/Semi-orthogonal_matrix" rel="noopener ugc nofollow" target="_blank">正交矩阵</a>(它有一些有用的属性，我们将在后面的部分中探讨)，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/bc65cac3da944bf3f25697d0b76807f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*-AdR4cSL5YXhLXI5"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(3)将新的标准正交基装入矩阵<strong class="bd oq"> W </strong>中。该矩阵将是正交矩阵，其性质将被进一步利用。</p></figure><p id="3fd8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">接下来，我们需要一些坐标或权重，它们可以乘以新的基来表示数据点。坐标和基向量的加权组合不会精确地表示数据点，但是我们希望得到良好的近似，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/3646ecb43142e25cdc35fbf359efecfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/0*z6IJdZA04vSUe8ss"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(4)将数据点表示为新基向量的加权组合。</p></figure><p id="e2af" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">对于所有的<strong class="lg ja"> <em class="mx"> N </em> </strong>数据点，坐标将是不同的，因此在矩阵向量符号中移位是有益的，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/7d2f2856bea985c3090065e619ecec8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:262/0*MI4Sp3jNWZNmd6hH"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/4d4eb2979a42ab0a8d9f81c6cbe5532d.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/0*E2pqCqWOx5W5QkIN"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(5)对于矩阵向量符号中的所有数据点，写出表达式(4)</p></figure><h2 id="1b70" class="ma mb iq bd mc md me dn mf mg mh dp mi ln mj mk ml lr mm mn mo lv mp mq mr iw bi translated">2.寻找最佳参数</h2><p id="dfaa" class="pw-post-body-paragraph le lf iq lg b lh ms ka lj lk mt kd lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">我们希望使用新的基和坐标来获得我们的数据点的近似。为了测量数据点<strong class="lg ja"> <em class="mx"> x </em> </strong>及其近似<strong class="lg ja"> <em class="mx"> Wz </em> </strong>的“<em class="mx">接近度</em>，我们计算所有<strong class="lg ja"> <em class="mx"> x-Wz、</em> </strong>的平方<a class="ae nz" href="https://mathworld.wolfram.com/L2-Norm.html" rel="noopener ugc nofollow" target="_blank"> L2范数(欧几里德范数)</a>的平均值</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/8c39b56db1058245822ab463f607fa51.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/0*c9U275Uyf3l42AIp"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(6)定义一个目标函数，最小化它，我们使用<strong class="bd oq"> z </strong>和w获得数据点<strong class="bd oq"> x </strong>的“最佳”近似。</p></figure><p id="5d77" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">函数<strong class="lg ja"> <em class="mx"> L </em> </strong>的值越低(函数<strong class="lg ja"> <em class="mx"> L </em> </strong>的值是一个标量)，我们得到的近似值就越好。所以，现在我们的问题进入最优化理论，其中我们需要确定为<strong class="lg ja"><em class="mx">【L】</em></strong>给出最低值的<strong class="lg ja"> <em class="mx"> W </em> </strong>和<em class="mx"/>的值。我们可以计算<strong class="lg ja"><em class="mx">L</em></strong>w . r . t .<strong class="lg ja"><em class="mx">z _ n</em></strong>的导数，并使其等于零。</p><blockquote class="pd pe pf"><p id="3a7e" class="le lf mx lg b lh li ka lj lk ll kd lm pg lo lp lq ph ls lt lu pi lw lx ly lz ij bi translated">下一步，我们要计算目标函数w.r.t .对向量的偏导数。如果你对矩阵演算不太熟悉，你可以参考这些<a class="ae nz" href="https://atmos.washington.edu/~dennis/MatrixCalculus.pdf" rel="noopener ugc nofollow" target="_blank">笔记</a>，其中包含一些有用的结果及其推导。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pj"><img src="../Images/b49eca1551b4e24b4f29e3d095236fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*h6gMIwYGCU7KRJE3"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(7)根据矩阵/向量乘积重写L2范数。简单来说，考虑到欧几里得空间，向量的L2范数就是它的大小。</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/ca98d0794917a6e9d29159b6915e9142.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/0*7Qas9TLAX0L7iq-g"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(8)计算函数<strong class="bd oq"> L </strong> w.r.t <strong class="bd oq"> z_i. </strong>的偏导数这里，<strong class="bd oq"> z_i </strong>保存数据集中某个数据点<strong class="bd oq"> x_i </strong>的坐标。</p></figure><p id="7d90" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">正如所承诺的，在(7)的最后一步中，我们使用了正交矩阵的一个优良特性。接下来，我们将(8)中得到的偏导数等于零，从而根据<strong class="lg ja"> <em class="mx"> W </em> </strong>和<strong class="lg ja"> <em class="mx"> x </em> </strong>得到<strong class="lg ja"> <em class="mx"> z </em> </strong>的最优值。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/2003a0c9a0b099d1e92011a3ab7f04fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/0*ZcZiI1jOw0UhZdqB"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(9)使偏导数等于0，从而得到在函数<strong class="bd oq"> L </strong>的全局最小值处<strong class="bd oq"> z_i </strong>的值。</p></figure><p id="d24e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">正如你可能观察到的，当<strong class="lg ja"> <em class="mx"> z_i </em> </strong>等于<strong class="lg ja"> </strong>数据点<strong class="lg ja"> <em class="mx"> x_i </em> </strong>乘以<strong class="lg ja"> <em class="mx"> W </em> </strong>矩阵的转置时，得到目标<strong class="lg ja"> <em class="mx"> L </em> </strong>的最小值。作为我们目标函数表达式中的第一项，σ<strong class="lg ja"><em class="mx">x_n^t x _ n，</em> </strong>是一个常数，我们可以把它去掉，继续展开项σ<strong class="lg ja"><em class="mx">z_n^t z _ n，</em> </strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/bdc65fa10c6cd241d0ee92f84f66eeba.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/0*h8UfSiyAPxLXO3BG"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(10)去掉常数项，代入(9)的结果，这样我们得到一个仅用<strong class="bd oq"> z </strong>表示的表达式。</p></figure><p id="4a2c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">z和W之间的关系从(9)中是显而易见的，但是我们需要熟练地使用它来获得想要的结果，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/2f80e317258ca4b50e252b1413231011.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/0*K1z6XIVic0i1ICbL"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(11)使用跟踪算子的性质，我们能够根据数据点的协方差矩阵来减少我们的目标函数</p></figure><p id="fd66" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在上面的评估中，我们有跟踪操作符的一些<a class="ae nz" href="https://en.wikipedia.org/wiki/Trace_(linear_algebra)#Basic_properties" rel="noopener ugc nofollow" target="_blank">属性，</a></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi po"><img src="../Images/ed3f2084c99c67c2040bf44357b0b7f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/0*BuEzZuiZ-PtlQSc1"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(12)我们在(11)中使用的跟踪算子的一些性质</p></figure><p id="3aed" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这里有必要使用trace运算符，因为它将帮助我们得出一个重要的结果，通过这个结果可以计算出<strong class="lg ja"> <em class="mx"> W </em> </strong>的最佳值。此外，我们还利用了统计学的一个重要结果，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/444643de6ffe32b4f1c5eeacfbe82289.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/0*9iJFV37Ec-_7_hoO"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(13)表示为数据点的外积总和的经验协方差矩阵。</p></figure><p id="23e6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><a class="ae nz" href="https://www.cuemath.com/algebra/covariance-matrix/" rel="noopener ugc nofollow" target="_blank">经验协方差矩阵</a> <strong class="lg ja"> <em class="mx"> K_XX </em> </strong>是保存随机向量中包含的不同随机变量对之间的协方差的矩阵。乍一看，该表达式可能看起来不可信，但您可以通过考虑将产生2 * 2协方差矩阵的2D数据点来验证它。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/7ff983527f4ac351a03d6658d290f5d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/0*_HDNhUpNP3qhIRB4"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(14)随机向量x的协方差矩阵。在(13)中，我们用数据点数值计算它。</p></figure><p id="b2eb" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">协方差矩阵也是对称的，我们可以很容易地从(14)中计算出来。<a class="ae nz" href="https://www.doc.ic.ac.uk/~ae/papers/lecture05.pdf" rel="noopener ugc nofollow" target="_blank">对称矩阵还拥有一些漂亮的特性</a>比如，</p><ul class=""><li id="4a18" class="na nb iq lg b lh li lk ll ln nc lr nd lv ne lz nf ng nh ni bi translated">对称矩阵有实特征值，所以没有复杂的东西！</li><li id="7066" class="na nb iq lg b lh nj lk nk ln nl lr nm lv nn lz nf ng nh ni bi translated">相应的特征向量彼此正交。它们可以作为向量空间的优秀基，称为特征基(希望你得到了提示！</li></ul><p id="01ed" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了确定W的最佳值，我们可以建立一个拉格朗日函数，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/d7778354852b80b6ba7b79b0dd5057e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/0*-ZlyUDncL7L3d7FB"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/6497310e345b42696973caeb1ec9db0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/0*l2gZ_Ywyuwh0JMUm"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(15)定义拉格朗日目标，以最大化函数<strong class="bd oq">L’</strong>的值。</p></figure><p id="3429" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在(11)中，观察到最大化轨迹将导致最小化<strong class="lg ja"><em class="mx">L’</em></strong>。在我们的优化问题中，我们对<strong class="lg ja"> <em class="mx"> W </em> </strong>只有一个约束，那就是它的正交性。为了在拉格朗日乘数的上下文中保持定义良好，我们在上面的表达式(14)中定义了矩阵内积，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/7b40e4d734e0986664e06bb960193729.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/0*LS2g_n4H5of4DgxE"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(16)矩阵内积(<a class="ae nz" href="https://en.wikipedia.org/wiki/Frobenius_inner_product" rel="noopener ugc nofollow" target="_blank"> Frobenius内积</a>)</p></figure><p id="6fdc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">它被称为<a class="ae nz" href="http://Frobenius_inner_product" rel="noopener ugc nofollow" target="_blank"> Frobenius内积</a>，它采用两个矩阵并返回一个标量(所有内积的典型属性)。矩阵λ将是包含乘数的方阵，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/ae2fd5e3f9d9de03ae04459fc2698f3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/0*BKN_LTbyq6VUi98Q"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(17)矩阵<strong class="bd oq"> <em class="pv"> λ </em> </strong> <em class="pv">包含</em> <strong class="bd oq"> <em class="pv"> W </em> </strong> <em class="pv">中每个向量的拉格朗日乘数。</em></p></figure><p id="d56c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">继续，我们计算目标<strong class="lg ja"><em class="mx">P</em></strong>W . r . t .<strong class="lg ja"><em class="mx">W</em></strong>的偏导数，并使其等于零，以获得<strong class="lg ja"> <em class="mx"> λ </em> </strong>的值。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/bb755bcb45c5a2601c8d1126412b1069.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/0*Z8THH02RidP9FU-m"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi px"><img src="../Images/7bbd7e0e051e04853224b285ee4c81d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/0*lwETlO2yPQNQzius"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(18)使P的偏导数等于零。通过这样做，我们观察到<strong class="bd oq"> <em class="pv"> λ </em> </strong> <em class="pv">是具有经验协方差矩阵</em>的特征值的对角矩阵</p></figure><p id="8f00" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">上面的表达式为我们提供了迄今为止最美丽的结果，我们将在这里等待欣赏它的美丽。</p><p id="76b9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">线性代数中的<a class="ae nz" href="https://brilliant.org/wiki/spectral-theorem" rel="noopener ugc nofollow" target="_blank">谱定理</a>说对称矩阵是可对角化的。它仅仅意味着一个对称矩阵可以通过使用一些其他的特殊矩阵转换成一个对角矩阵(其中元素只位于矩阵的主对角线上)。可对角化是一个很好的性质，因为它大大减少了各种其他有用的性质，如行列式，迹等的计算。一个对称矩阵<strong class="lg ja"> <em class="mx">一个</em> </strong>是可对角化的，比如，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi py"><img src="../Images/f237fcab2cf099bb20eb1225a2b14eaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/0*6anQTnugcegmLZA-"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(19) <a class="ae nz" href="https://en.wikipedia.org/wiki/Diagonalizable_matrix#Diagonalization" rel="noopener ugc nofollow" target="_blank">对称实矩阵<strong class="bd oq"> A </strong>的对角化</a></p></figure><p id="0ce3" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">其中<strong class="lg ja"> <em class="mx"> D </em> </strong>是从<strong class="lg ja"> <em class="mx"> A </em> </strong>导出的对角矩阵。<strong class="lg ja"> <em class="mx"> D </em> </strong>的元素是矩阵<strong class="lg ja"> <em class="mx"> A </em> </strong>的实特征值。矩阵<strong class="lg ja"> <em class="mx"> P </em> </strong>的列包含矩阵<strong class="lg ja"> <em class="mx"> A </em> </strong>的特征向量。由于矩阵<strong class="lg ja"> <em class="mx"> A </em> </strong>是对称的，其特征向量相互正交，因此矩阵<strong class="lg ja"> <em class="mx"> P </em> </strong>也是正交的。</p><p id="6545" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">回到表达式(18)，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/fe330ca830fed8c9bba82c5716b801c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/0*ptRGycjdoWlg0mRN"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(20)因为我们的数据点的协方差矩阵也是实对称矩阵，所以它是可对角化的。这里的<strong class="bd oq"> <em class="pv"> λ </em> </strong> <em class="pv">是一个对角矩阵。</em></p></figure><p id="967e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">协方差矩阵<strong class="lg ja"> <em class="mx"> K_XX </em> </strong>是对称的，所以对角矩阵<strong class="lg ja"> <em class="mx"> λ </em> </strong>包含协方差矩阵的特征值<strong class="lg ja"> <em class="mx"> W </em> </strong>包含归一化的特征向量。所以，最后我们得到了标准正交基<strong class="lg ja"> <em class="mx"> W </em> </strong>和坐标<strong class="lg ja"> <em class="mx"> z </em> </strong> as，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/0037344af9db5bf844133daf1e243fa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/0*_7zUCSQAI8ugqtLi"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(21)我们得到的关于<strong class="bd oq"> W </strong>和<strong class="bd oq"> z </strong>的最终结果。</p></figure><p id="f639" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们的目标是为数据点找到一种新的表示方法，以便可以轻松地删除这些特征。此外，我们还讨论了为什么这些特性不能轻易删除，因为它们是相互依赖的。我们在步骤(2)和(3)中所做的是，我们试图使这些特性相互独立，这样我们就可以轻松地选择它们，而不用担心它们之间的依赖性。这不仅仅是一个想法，而是我们用步骤(2)和(3)从数学上证明了它。</p><p id="752b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">让我们回到过去，我们会意识到两个随机变量<em class="mx">之间的协方差大约</em>捕获了变量之间的相关性。如果两个变量是独立的，协方差为零。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qb"><img src="../Images/79fad3334c8b5f78f149a4bf71fb826d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ijn5HIXFbhlmZj8-"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(22)协方差矩阵<strong class="bd oq"> X </strong>，其中<strong class="bd oq"> X </strong>为随机向量。<strong class="bd oq"> X_1 </strong>、<strong class="bd oq"> X_2 </strong>、…、<strong class="bd oq"> X_D </strong>是表示为随机变量的特征。注意，在(18)中，该矩阵被转换成对角矩阵，从而将非对角元素设置为0，即特征之间的协方差。</p></figure><p id="b908" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在表达式(21)中，我们已经将协方差矩阵<strong class="lg ja"> <em class="mx"> K_XX </em> </strong>表示为对角矩阵。在协方差矩阵中，所有非对角元素表示随机变量或特征对之间的协方差。通过将协方差矩阵转换为对角矩阵，我们已经破坏了特征/随机变量之间存在的所有协方差。我们已经尝试让每个特性独立，这样我们就可以使用合适的标准轻松地对它们进行排序，而不用太担心内部的依赖性。</p><blockquote class="oa"><p id="3822" class="ob oc iq bd od oe of og oh oi oj lz dk translated">通过最小化函数<strong class="ak"> <em class="pv"> L </em> </strong>，我们已经认识到数据点<strong class="ak"> <em class="pv"> x_i </em> </strong>的最佳近似可以通过将数据点的基变换为新的基，即<em class="pv">特征基</em>来获得！</p></blockquote><h2 id="2b9f" class="ma mb iq bd mc md ok dn mf mg ol dp mi ln om mk ml lr on mn mo lv oo mq mr iw bi translated">3.缩小尺寸(去除特征)</h2><p id="25da" class="pw-post-body-paragraph le lf iq lg b lh ms ka lj lk mt kd lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">到目前为止，我们已经找到了一个新的基础来表示数据点，这样我们就可以很容易地分离特征，而不用考虑一个特征对另一个特征的依赖性。但问题是，我们应该放弃哪些功能？我们需要一些标准来从数据集中保留最好的<strong class="lg ja"> <em class="mx"> K </em> </strong>特征。</p><p id="f2c1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">对于PCA，我们将选择具有较高方差的特征。简单的逻辑是，具有低方差的特征与常数一样好，并且对响应变量几乎没有影响。这里有一个<a class="ae nz" href="https://stats.stackexchange.com/a/489031/323037" rel="noopener ugc nofollow" target="_blank">关于同一主题的交叉验证</a>的好讨论。这也是为什么您可能会注意到<code class="fe qc qd qe qf b">sklearn.feature_selection</code>模块中的<code class="fe qc qd qe qf b">VarianceThreshold</code>选择特性的方差大于给定阈值的原因。正如我们前面讨论的，我们的数据的协方差矩阵<strong class="lg ja"> <em class="mx"> K_XX </em> </strong>在新基中看起来像一个对角矩阵，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi qg"><img src="../Images/f1fb644e7563ed5490b18aea732ceaf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/0*xP_BsZXq7JuVIXeX"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(23)变换后的数据点的协方差矩阵是一个对角矩阵，因为我们已经破坏了</p></figure><p id="4647" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">对于上面的表达式，我们将<strong class="lg ja"> <em class="mx"> z </em> </strong>(转换后的数据点)视为一个随机向量，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/7624cbabd9d19f019180be67c233192c.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/0*VzMAJ_xUXwrs3vs7"/></div></figure><p id="590c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">协方差矩阵的对角线包含特征的单个方差。因此，特征值对应于变换数据点的方差，我们希望选择顶部的<strong class="lg ja"> <em class="mx"> K </em> </strong>特征值。同样，从矩阵<strong class="lg ja"> <em class="mx"> W </em> </strong>中，我们将选择相应的<strong class="lg ja"> <em class="mx"> K </em> </strong>特征向量，并最终使用它们来变换数据集。</p><p id="6980" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">因此，我们将选择<strong class="lg ja"> <em class="mx"> K </em> </strong>特征向量，其中每个特征向量的维数为<strong class="lg ja"> <em class="mx"> D </em> </strong>，并将它们打包成矩阵<strong class="lg ja"> <em class="mx"> W_K，</em> </strong>转置它，并与每个数据点<strong class="lg ja"> <em class="mx"> x_i </em> </strong>相乘，以获得更低维的表示<strong class="lg ja"> <em class="mx"> z_i </em> </strong>，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/2cf5715698fae319f944c1c4e36547b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/0*cMna7or-uAHSoEKx"/></div></figure><p id="16cd" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">当我们选择<strong class="lg ja"> <em class="mx"> K &lt; D </em> </strong>(数据的实际维度)时，我们将得到数据点的近似值，而不是它在新基中的精确表示，这只是因为我们抛弃了来自<strong class="lg ja"> <em class="mx"> W </em> </strong>的基向量。这就是我们如何通过以一种聪明的方式剥离特征来减少数据的维度。</p><p id="90df" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这都是关于主成分分析及其在降维中的作用。</p><h1 id="98e3" class="no mb iq bd mc np nq nr mf ns nt nu mi kf nv kg ml ki nw kj mo kl nx km mr ny bi translated">怒发冲冠！那是一大堆数学！</h1><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者的其他故事</p></figure><p id="8d2c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我希望这个故事提供了主成分分析的全貌，以及它与协方差、特征向量和维数约简的关系。在那之前，继续学习数学，祝你有美好的一天(如果你坚持到这里)！</p></div></div>    
</body>
</html>