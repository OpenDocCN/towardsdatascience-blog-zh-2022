<html>
<head>
<title>How to Reduce the Training Time of Your Neural Network from Hours to Minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何将你的神经网络的训练时间从几小时减少到几分钟</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-reduce-the-training-time-of-your-neural-network-from-hours-to-minutes-fe7533a3eec5#2022-05-06">https://towardsdatascience.com/how-to-reduce-the-training-time-of-your-neural-network-from-hours-to-minutes-fe7533a3eec5#2022-05-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d2b2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关于使用HPC的人工智能的文章的第2部分:使用Horovod和GPU并行CNN以获得75-150倍的加速。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7673e225f946ddc87669ad63094789d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fSWbNI1zIkE25lUl"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">罗伯特·卡茨基在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="b91c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本系列的<a class="ae kv" rel="noopener" target="_blank" href="/machine-learning-on-supercomputers-data-processing-aa403e51516b">第1部分</a>中，我们研究了如何通过使用多处理模块的几行Python代码在IO操作中获得<strong class="ky ir">到1500倍的加速。<strong class="ky ir">在本文中，我们将研究并行化深度学习代码，并将训练时间从大约13小时减少到13分钟！</strong></strong></p><h1 id="1dfe" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">背景</h1><p id="103b" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">作为一名数据科学家，你最终会面临以下问题(如果你还没有面对过的话)。</p><p id="fcde" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="mp">“我有一个要训练的神经网络，但输入数据不适合内存！”</em> </strong>或<strong class="ky ir"> <em class="mp">“我的神经网络用这么大的数据量来训练，永远花不完！”</em> </strong></p><p id="8f3e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果将你的一大部分数据排除在训练之外，或者等待几个小时(甚至几天)让你的神经网络完成训练，那肯定是很可惜的。必须有一个更好的方法，这就是所谓的<strong class="ky ir">数据并行</strong>。</p><h1 id="c239" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">问题是</h1><p id="7eef" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在这篇文章中，我们将使用在<a class="ae kv" href="https://www.kaggle.com/datasets/concaption/pepsico-lab-potato-quality-control" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上可获得的百事公司实验室薯片控制数据集的修改版本。原始图像为2000 x 2000像素格式，有3个通道。我修改图像，将它们缩放到500 x 500，并应用5次变换(一次垂直翻转、一次水平翻转和三次90度旋转)来创建比原始数据集中多6倍的图像。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/f74d32a0a59c469efe5026a45a9a145f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O57Y6_w4SYSVSa4Km5hkVA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">这两种类型的芯片区分在这个职位。</p></figure><p id="53dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我编写了一个简单的卷积神经网络(CNN)代码，用于基于从原始数据集创建的输入数据进行分类。所有的材料都可以在这里找到。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/3ac01df1277d490d4153eb9b2a457b1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sIyp5jTkVm0gsQhCq3tQiw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">这篇文章中使用的CNN是使用<a class="ae kv" href="https://github.com/paulgavrikov/visualkeras" rel="noopener ugc nofollow" target="_blank"> visualkeras </a>可视化的。请注意，为了美观，第一层没有按比例绘制，但它给出了我为这个问题创建的微型CNN的概念。</p></figure><h2 id="277a" class="ms lt iq bd lu mt mu dn ly mv mw dp mc lf mx my me lj mz na mg ln nb nc mi nd bi translated">资源和存储库</h2><p id="70b3" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在本文中，我们将使用:</p><ul class=""><li id="43e5" class="ne nf iq ky b kz la lc ld lf ng lj nh ln ni lr nj nk nl nm bi translated">张量流2和Keras</li><li id="cc1e" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">霍罗沃德</li><li id="ee9c" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">Python 3.8+版本</li><li id="b539" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">至少有1个GPU的计算机</li><li id="93e7" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">所有的代码和数据都可以在这个<a class="ae kv" href="https://github.com/codiusmaximus/chips_cnn" rel="noopener ugc nofollow" target="_blank">库</a>中找到。</li></ul><h1 id="32c3" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">数据并行性</h1><p id="eedb" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在数据并行中，我们部署相同的模型，在不同的处理单元(比如GPU)上进行训练，但是使用输入数据的唯一子集。在每个时期之后，模型参数被收集、聚集并传送回处理单元，以继续训练它们各自的模型。通过这种方式，每个处理单元使用其自己唯一的数据子集(即数据的一部分)来训练相同的模型，但是模型参数会随着对整个输入数据集的了解而更新。</p><h2 id="7e41" class="ms lt iq bd lu mt mu dn ly mv mw dp mc lf mx my me lj mz na mg ln nb nc mi nd bi translated">我如何将这种方法用于神经网络？</h2><p id="849d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如果你使用TensorFlow、PyTorch或任何流行的公共深度学习框架，你可以使用<a class="ae kv" href="https://github.com/horovod/horovod" rel="noopener ugc nofollow" target="_blank"> Horovod </a>进行分布式训练。Horovod是由优步开发的，目的是将HPC的思想引入深度学习。安装相当简单，我就不赘述了，因为他们的<a class="ae kv" href="https://github.com/horovod/horovod" rel="noopener ugc nofollow" target="_blank"> GitHub页面</a>很好地解释了这一点。在这篇文章的剩余部分，我们将讨论如何使用Horovod数据并行化他们的TensorFlow代码。</p><p id="355b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">把Horovod想象成你现有代码的一个插件。在<em class="mp">Horovod-ing</em>你的代码中涉及的步骤遵循数据并行的思想，所以在做任何事情之前，你必须完全理解你的代码将如何被horo VOD执行。然后，您必须决定分发数据的方式。一旦完成，就可以开始在代码中实现Horovod了。</p><p id="c931" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对我来说，用下面的步骤来处理这个问题更容易。</p><ol class=""><li id="4b10" class="ne nf iq ky b kz la lc ld lf ng lj nh ln ni lr ns nk nl nm bi translated">了解你的代码将如何用Horovod执行，即<em class="mp"> horovodrun </em></li><li id="0111" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr ns nk nl nm bi translated">确保您有一个好的数据分发策略</li><li id="2961" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr ns nk nl nm bi translated">在代码中初始化Horovod</li><li id="d2a8" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr ns nk nl nm bi translated">确保您的优化器是分布式的</li><li id="b69c" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr ns nk nl nm bi translated">从协调器广播您的变量并修复您的回调</li></ol><p id="58a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在将详细讨论每一点。</p><h1 id="be47" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">1.Horovod如何运作</h1><p id="2973" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">Horovodrun将一些基本的消息传递接口(MPI)命令和概念封装到一个更高级别的API中。因此，如果熟悉MPI的话，您可以发现两者之间有很多相似之处。在一个有<em class="mp"> n </em>个GPU的系统上，人们会执行一个CNN代码，这里已经实现了Horovod，如下所示</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="de48" class="ms lt iq nu b gy ny nz l oa ob">horovodrun -np <strong class="nu ir">n</strong> python cnn_parallel.py</span></pre><p id="e6ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用Horovod修改过的代码需要用<em class="mp"> horovodrun </em>或<em class="mp"> mpirun </em>来执行。对于数据并行框架，使用<em class="mp"> n </em>个GPU可以创建n个任务。每个任务都使用python运行修改过的CNN代码，但是只针对数据的一个唯一子集。在下图中，我也做了同样的解释。我们正在一台有2个GPU的机器上工作。输入数据集D被分成两个子集a和b，使得它们之间没有共同的元素。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/18fa227f5a3a896d34d7782b2c1ca3ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*0XFnT76gBBqK7T1kCfSUQA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">理解Horovod如何并行执行代码。</p></figure><p id="281b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里要记住的重要一点是，您的整个python代码将运行<em class="mp"> n </em>次，其中n是GPU的数量。因此，每个打印命令，每个IO操作，每个绘图部分等。将被执行n次。起初这可能是一个令人困惑的概念，但好的一面是每个屏幕上的消息(例如print(…))都有自己的GPU标识符。</p><p id="d6c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Horovod附带了一些有用的调用(类似于MPI标准定义的调用),帮助定义我们将在其上运行代码的系统的架构。“hvd.size()”命令显示实际可用的GPU数量，“hvd.local_rank()”告诉您GPU的本地等级，“hvd.rank()”告诉您GPU的全局等级。等级可以被认为是GPU id。下图更好地解释了这个想法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/9edbed4a2b30a8450f9bb87e4b4813c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lmhZiB-ILyUTo2-jG21sRg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">了解单节点与多节点设置中GPU的大小和等级。</p></figure><h1 id="d3dc" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">2.数据分发策略</h1><p id="5da0" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">那么，为什么必须分发数据呢？为了充分利用数据并行性，每个GPU必须看到唯一的数据子集。然而，如果您传递相同的数据集进行训练，您将在n个GPU上使用相同的样本对模型进行n次训练，或者换句话说，您将使用相同的数据以完全相同的方式运行代码n次。这不是我们想要通过数据并行实现的。</p><p id="2ccb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了正确地分配数据，可以采用许多策略。Pytorch、Tensorflow等都有内置的数据分发方法。，但实际的策略将取决于数据的结构和数据量。</p><p id="59db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们用一个简单的例子来创建一些数据分布策略。假设我们的数据由从1到100的所有整数组成，按升序排列，没有重复。我们希望将这些数据分布在2个GPU上。</p><h2 id="8bb1" class="ms lt iq bd lu mt mu dn ly mv mw dp mc lf mx my me lj mz na mg ln nb nc mi nd bi translated"><strong class="ak">选项1:张量流数据生成器</strong></h2><p id="82f8" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们将在训练开始时采用在GPU上随机分配批次的想法。我们将欺骗TensorFlow中的fitting调用，使用“steps_per_epoch”关键字将1/n个批次分配给每个GPU。这种方法适用于数据生成器，在我们的例子中是图像数据生成器。我们保持与我们在没有任何GPU的情况下训练时相同的批量大小。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/7bf0ddb9ada6c84643c84185f4ff1331.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ooLXWb5tDKFeOtwtDTHeZg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">用张量流数据发生器进行数据分配。</p></figure><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="8e44" class="ms lt iq nu b gy ny nz l oa ob">img_gen = tf.keras.preprocessing.image.ImageDataGenerator(<br/>                                           validation_split=split)</span><span id="bb89" class="ms lt iq nu b gy of nz l oa ob">train_dataset = img_gen.flow_from_directory(<br/>                           directory= train_data_path,<br/>                           target_size=(img_height,img_width),              <br/>                           color_mode=<strong class="nu ir">'rgb'</strong>,<br/>                           batch_size=<strong class="nu ir">batch_size</strong>,<br/>                           class_mode=<strong class="nu ir">'sparse'</strong>,<br/>                           seed=123, <br/>                           shuffle=True, <br/>                           subset=<strong class="nu ir">'training'</strong>)</span><span id="881b" class="ms lt iq nu b gy of nz l oa ob">val_dataset=img_gen.flow_from_directory(directory=...  <br/>                           subset=<strong class="nu ir">'validation'</strong>)</span><span id="21ca" class="ms lt iq nu b gy of nz l oa ob"><strong class="nu ir">total_train_batches</strong> = train_dataset.samples//batch_size</span><span id="79ef" class="ms lt iq nu b gy of nz l oa ob"><strong class="nu ir">total_val_batches</strong> = val_dataset.samples//batch_size</span></pre><p id="0c8c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，当我们执行拟合时，我们指示TensorFlow仅使用总批次的1/n进行训练和验证。这是因为训练将在每个GPU上进行，并且通过将“steps_per_epoch”指定为total_batches/n，我们指示TensorFlow在两个GPU上拆分批次，以便每个GPU从我们的数据生成器获得批次的不相交子集。</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="f56f" class="ms lt iq nu b gy ny nz l oa ob">history = model.fit(<br/>    train_dataset,<br/>    epochs=epochs,<br/>    batch_size=batch_size,<br/><strong class="nu ir">    steps_per_epoch=total_train_batches//hvd.size(), </strong><em class="mp"><br/>    </em>validation_data=val_dataset,<br/><strong class="nu ir">    validation_steps=total_val_batches//hvd.size(), </strong><em class="mp"><br/>    ...</em><br/>)</span></pre><h2 id="e663" class="ms lt iq bd lu mt mu dn ly mv mw dp mc lf mx my me lj mz na mg ln nb nc mi nd bi translated"><strong class="ak">方案二:张量流数据。数据集对象</strong></h2><p id="a398" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们将使用“分片”的概念，这相当于获取数据集中的第q个元素。下面的伪代码应该能更好的解释。</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="28b2" class="ms lt iq nu b gy ny nz l oa ob">D = [1,2,...,99,100]</span><span id="5dcd" class="ms lt iq nu b gy of nz l oa ob"># split into 2 shards<br/>shard_1 = [1,3,5,...]<br/>shard_2 = [2,4,6,...]</span><span id="bbe0" class="ms lt iq nu b gy of nz l oa ob"># split into 3 shards<br/>shard_1 = [1,4,7,...]<br/>shard_2 = [2,5,8,...]<br/>shard_3 = [3,6,9,...]</span></pre><p id="d7f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面我描述了两种不同的方法，每批4个。请注意，这些不是分发数据可以采用的唯一策略，您可以自由设计自己的策略。</p><ul class=""><li id="7fdd" class="ne nf iq ky b kz la lc ld lf ng lj nh ln ni lr nj nk nl nm bi translated">方法A:将数据集分片2次→批量化</li><li id="3673" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">方法B:混洗数据集→分片2次数据集→批量处理</li></ul><p id="60ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从机器学习的角度来看，你能解释为什么只有一种方法是正确的吗？</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="c575" class="ms lt iq nu b gy ny nz l oa ob">D = [1,2,...,99,100]</span><span id="c4ef" class="ms lt iq nu b gy of nz l oa ob"><strong class="nu ir"># method A</strong></span><span id="48ab" class="ms lt iq nu b gy of nz l oa ob">shard_1 = [1,3,5...]<br/>shard_2 = [2,4,6...]</span><span id="8575" class="ms lt iq nu b gy of nz l oa ob">shard_1_batch = [[1,3,5,7],[9,11,13,15]...] # split 1<br/>shard_2_batch = [[2,4,6,8],[10,12,14,16]...] # split 2</span><span id="d506" class="ms lt iq nu b gy of nz l oa ob"><strong class="nu ir"># method B</strong></span><span id="1884" class="ms lt iq nu b gy of nz l oa ob">D_shuffle = [63,74,22,26,36,14,34,94,60,42,56,17,65,1,12,51...]</span><span id="4ae3" class="ms lt iq nu b gy of nz l oa ob">shuffle_shard_1 = [63,22,36,34,60,56,65,12]<br/>shuffle_shard_2 = [74,26,14,94,42,17,1,51]</span><span id="d1a3" class="ms lt iq nu b gy of nz l oa ob">shuffle_shard_1_batch = [[63,22,36,34],[60,56,65,12]...] # split 1<br/>shuffle_shard_2_batch = [[74,26,14,94],[42,17,1,51]...] # split 2</span></pre><p id="1e9f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用方法A，所有奇数都在分割1中结束，偶数在分割2中结束。很明显，方法B是最好的，因为产生的分割公平地表示(和分布)了偶数和奇数。</p><p id="bcad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在想想CNN的输入。理想情况下，输入文件夹将包含数千(如果不是数百万)您需要训练的图像，通常分组到不同的类别(子文件夹)。当您从图像文件夹中创建TensorFlow数据集时，它会从目录结构中推断出类。下面的片段应该能更好地阐明这一点。</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="687b" class="ms lt iq nu b gy ny nz l oa ob"><strong class="nu ir">input_folder<br/>|-train<br/>| |-class_1</strong><br/>| | |-image_1.jpg<br/>| | |-...<br/>| | |-image_100.jpg<br/><strong class="nu ir">| |-class_2</strong><br/>| | |-image_1.jpg<br/>| | |-...<br/>| | |-image_50.jpg<br/><strong class="nu ir">| |-class_3</strong><br/>| | |-image_1.jpg<br/>| | |-...<br/>| | |-image_500.jpg</span></pre><p id="f951" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，一旦这样的文件夹结构被传递给TensorFlow，它会自动将文件名映射到它们各自的类，准备好进行处理并传递给CNN进行训练。<strong class="ky ir">注意，默认情况下，</strong><em class="mp">image _ dataset _ from _ directory</em><strong class="ky ir">会打乱文件名，或者换句话说，如果您使用它摄取图像，您的数据集已经被打乱了。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/43739a7a3539897784a6fac502a49347.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lcxZEEcPimCv3Tam-8BxHQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用分片的数据分发。</p></figure><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="d174" class="ms lt iq nu b gy ny nz l oa ob">import tensorflow.keras.preprocessing as tkp</span><span id="d35c" class="ms lt iq nu b gy of nz l oa ob">local_bs = .. # will be scaled with the hvd.size() <br/>              # we will discuss this later</span><span id="0ee9" class="ms lt iq nu b gy of nz l oa ob">X_train = tkp.image_dataset_from_directory(..., batch_size = 1) </span><span id="4645" class="ms lt iq nu b gy of nz l oa ob">train_dataset = X_train.<br/>                        <strong class="nu ir">unbatch()</strong>.<br/>                        <strong class="nu ir">shard(hvd.size(), hvd.rank()).<br/>                        batch(local_bs).<br/>                        cache()</strong></span></pre><p id="bc02" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mp"> hvd.shard() </em>调用接受GPU的数量(创建多少个碎片)和GPU的<em class="mp"> id </em>来创建数据的唯一碎片。使用hvd.size()和hvd.rank()作为该调用的输入，我们可以轻松地管理将由每个GPU处理的数据分割。至此，<em class="mp"> train_dataset </em>尚未创建，因此不存在于内存中。它将在最终使用下面的命令训练模型时创建。</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="bbe2" class="ms lt iq nu b gy ny nz l oa ob">model.fit(train_dataset,...)</span></pre><p id="4e44" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，这里我们使用了<em class="mp"> local_bs，</em>来代替原来的批量大小。我将很快解释原因。</p><h2 id="5668" class="ms lt iq bd lu mt mu dn ly mv mw dp mc lf mx my me lj mz na mg ln nb nc mi nd bi translated"><strong class="ak">数据分发后训练模型</strong></h2><p id="d6a4" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">一旦我们正确地决定并实现了数据分发策略，我们就准备好训练CNN了。此时，使用2个GPU，数据训练策略将被执行，如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/716634119dd3d18ad60a0f5babf07888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t8o1-3zj2td-ZD9D3LyXqA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">训练CNN时执行的数据分发策略。</p></figure><h2 id="d0b5" class="ms lt iq bd lu mt mu dn ly mv mw dp mc lf mx my me lj mz na mg ln nb nc mi nd bi translated">批量大小和学习率(数据分布策略中的选项2)</h2><p id="9199" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如果你使用过神经网络，你必须熟悉学习率和批量的概念。在数据分布式框架中，人们可能需要正确地调整这些，以补偿我们的训练策略的变化。由于每个GPU现在都在用唯一的数据子集训练模型，NN now <em class="mp">在每个时期有效处理的样本数量与批量大小不同。请注意，根据您决定如何分发数据以及如何编写代码，您可能不需要执行这种缩放。根据我的经验，如果相应地调整批量大小和/或学习率，上面的数据分布选项2通常会更好。基本上，如果您在实现数据并行后发现奇怪的损失和准确性曲线，那么这可能是要记住的事情。</em></p><p id="1c68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们回到选项2的数据分发。我们有2个GPU，数据集由100个整数组成，批量大小为4。每个GPU获得50个唯一的整数，分组为大小为4的批次。每个GPU通过处理其自己的13个批次(最后一个批次将只有2个元素)来执行训练，并且在每个时期之后，模型参数被传递到Horovod用于聚合。Horovod从批量大小为4的GPU 0和批量大小为4的GPU 1聚合模型参数，因此<em class="mp">有效地</em>批量大小为8！换句话说，当Horovod在每个时期结束时更新模型参数时，模型在技术上“看到”了一个批次，其中每个批次具有8个独特的样本。下图应该能更好地解释这一点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/d5f0e0ce9e8b3ba815c9f67cbe18ce0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tFA0IVTE9X7DUCWtaNoABQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">基于两个GPU的CNN数据分布式训练。在每个时期结束时，Horovod(橙色)聚集来自每个GPU(蓝绿色和紫红色)的模型参数，并更新CNN模型，现在为下一个时期的训练做好准备。</p></figure><p id="d491" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们不改变批量大小的情况下，即保持其固定为与代码的非数据分布版本相同的值，我们必须随着GPU的总数线性*调整学习速率。这是因为在我们的<em class="mp">有效批次</em>中有更多的样本，可以增加算法在寻找损失函数最小值的过程中允许采取的步长。换句话说，我们有更少的噪声(更多的样本)，因此我们可以在参数空间中进行更大的跳跃，以便找到最小值。</p><p id="65cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，如果我们希望保持固定的学习速率，即与代码的非数据分布版本相同，我们必须与GPU的数量成反比地调整批量大小。让我们以整数为例。对于2个GPU，如果我以2的批处理大小对我的每个数据分片进行批处理，那么Horovod在每个epoch结束时看到的<em class="mp">有效批处理大小</em>是4。因此，我可以保持与数据分发前相同的学习速度。</p><p id="fe44" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总结一下:</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="c718" class="ms lt iq nu b gy ny nz l oa ob">############################################ THEORY<br/># original batch size: batch_size<br/># local batch size: local_bs (batch size on each gpu)</span><span id="e93b" class="ms lt iq nu b gy of nz l oa ob">#original learning rate: lr<br/>#scaled learning rate: scaled_Llr</span><span id="f1fc" class="ms lt iq nu b gy of nz l oa ob">## option 1: <br/># bs= local_bs : therefore, the effective batch size is larger<br/># therefore scaled_lr = lr*n_gpus</span><span id="5c61" class="ms lt iq nu b gy of nz l oa ob">## option 2:<br/># scaled_lr = lr<br/># therefore local_bs = batch_size/n_gpus <br/># now the effective batch size will be the same as before<br/>############################################</span><span id="6ad9" class="ms lt iq nu b gy of nz l oa ob">############################################ IMPLEMENTATION<br/>#<strong class="nu ir">pick one</strong> below:<br/><strong class="nu ir"># Option 1:</strong><br/># local_bs = batch_size<br/># scaled_lr = base_lr * hvd.size()<br/># <strong class="nu ir">OR<br/># Option 2:<br/></strong>local_bs = int(batch_size/hvd.size())<br/>scaled_lr = base_lr</span></pre><p id="d5ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">* <em class="mp">有时，学习率的平方根缩放或2/3幂缩放比严格的线性缩放效果更好</em></p><h1 id="b370" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">3.在代码中初始化Horovod</h1><p id="0acd" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">第一步是告诉你的代码你将使用Horovod。</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="0ca1" class="ms lt iq nu b gy ny nz l oa ob"><strong class="nu ir">import horovod.tensorflow as hvd</strong></span><span id="ac95" class="ms lt iq nu b gy of nz l oa ob"><strong class="nu ir">hvd.init()</strong> # this initiliases horovod</span><span id="fe14" class="ms lt iq nu b gy of nz l oa ob">print('Total GPUs available:', hvd.size())<br/>print('Hi I am a GPU and my rank is:', hvd.rank())</span></pre><p id="36fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下一步是使用GPU的本地等级作为标识符，将每个可用的GPU固定到一个“进程”。</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="914b" class="ms lt iq nu b gy ny nz l oa ob"><em class="mp"># Pin GPU to be used to process local rank (one GPU per process)</em><br/>gpus = tf.config.experimental.list_physical_devices('GPU')<br/>for gpu in gpus:<br/>    tf.config.experimental.set_memory_growth(gpu, True)<br/>if gpus:<br/>  tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')</span></pre><h1 id="6126" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">4.分发优化器</h1><p id="4ab3" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这是优化器进行聚合的地方。Horovod使这变得非常容易，因为我们只需要在代码中添加一行来分发优化器。</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="dd5b" class="ms lt iq nu b gy ny nz l oa ob">opt = keras.optimizers.Adam(learning_rate=learning_rate)</span><span id="53ee" class="ms lt iq nu b gy of nz l oa ob"># Horovod: add Horovod Distributed Optimizer.<br/><strong class="nu ir">opt = hvd.DistributedOptimizer(opt)</strong></span></pre><h1 id="4f4f" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">5.修复回电</h1><p id="ac09" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">下一步是确保我们不会通过允许所有GPU更新模型参数来破坏我们的训练。因此，我们将我们的一个GPU声明为“协调器”。这个GPU将负责收集模型参数，聚合它们，并在每个训练时期后将它们广播回其他GPU。请记住，它也将使用自己的数据子集来训练CNN。通常我们声明排名为0的GPU为我们的协调器。</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="8161" class="ms lt iq nu b gy ny nz l oa ob">callbacks = [<br/>    # Horovod: broadcast initial variable states from rank 0 to<br/>    # all other processes. This is necessary to ensure consistent<br/>    # initialization of all workers when training is started with<br/>    # random weights or restored from a checkpoint.<br/><strong class="nu ir">    hvd.callbacks.BroadcastGlobalVariablesCallback(0),</strong><br/><br/>    # Horovod: average metrics among workers at the end of every<br/>    # epoch.<br/><strong class="nu ir">    hvd.callbacks.MetricAverageCallback(),</strong><br/>    ]</span><span id="a268" class="ms lt iq nu b gy of nz l oa ob"># Horovod: save checkpoints only on worker 0 to prevent other<br/># workers from corrupting them.<br/><strong class="nu ir">if hvd.rank() == 0:<br/>    callbacks.append(keras.callbacks.ModelCheckpoint('...h5'))</strong></span></pre><h1 id="0bc4" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">运行代码</h1><p id="fcc9" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们现在准备运行代码。如果您使用的计算机只有1个GPU，您可以从终端执行该命令，如下所示</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="a544" class="ms lt iq nu b gy ny nz l oa ob">horovodrun -np 1 python python_code/horovod_dist_gen.py</span></pre><p id="fb84" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">相反，如果我们要在一台超级计算机上运行它，我们需要编写一个脚本来提交我们的“作业”，其中作业是一段要用给定的资源组合运行的代码。下面是一个脚本的例子，我在一台有4个GPU的机器上运行代码。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/b0ba7ae5b73523b0d7cd8a0ed62e46b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LbjKLZl_7PwyJB-tHTmsRQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用带有<em class="oj"> Horovod </em>的GPU运行CNN代码的slurm脚本。</p></figure><h1 id="106e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结果如何呢？</h1><p id="f03f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated"><strong class="ky ir">无GPU时间:14小时</strong></p><p id="b702" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">使用1个GPU的时间:12分钟</strong></p><p id="3bd3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于使用碎片的数据分发方法，<strong class="ky ir">我们使用1个GPU </strong>获得了75倍的速度提升，使用2个或更多GPU获得了<strong class="ky ir">150倍的速度提升。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/9b2e5220503d41d870f7f7575c3919f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2oJtAW3Bcu03HbZRfrfLTA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在我们的CNN代码中实现Horovod并使用1、2和4个GPU运行后获得的加速。</p></figure><p id="f6d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与生成器方法相比，以碎片形式分布的数据性能更好。从1个GPU到2个GPU的加速几乎是线性的，即75倍到150倍，但之后就不再加速了。这是为什么呢？这是因为当我们并行处理一个代码时，我们的想法是最大限度地利用可用资源。有了1个GPU，即使代码运行速度比没有GPU时快75倍，也仍然有从更多计算能力中受益的空间。当我们添加第二个GPU时，运行时间减半，从而导致理想的线性加速。此时，我们处于这样一个阶段，即提供给代码的资源对于计算量来说是最优的。增加更多的资源对我们没有好处。因此，我们看不到4个GPU有更多的速度提升。我们可以使用命令<em class="mp"> nvidia-smi来监控资源使用情况(在我们的例子中是GPU使用情况)。</em>我不会深入探讨这个话题，但如果你有兴趣，请随时联系我。</p><h1 id="bf2d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结束语</h1><ul class=""><li id="7838" class="ne nf iq ky b kz mk lc ml lf ol lj om ln on lr nj nk nl nm bi translated">请随意设计您自己的数据分发策略。这通常取决于你的数据是如何组织的，以及数据的大小。</li><li id="0153" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">使用GPU时，初始设置总会有开销，即根据您使用的系统，可能需要一些时间来设置GPU-CPU通信、环境变量等。这是由引擎盖下的系统完成的，所以如果在开始时代码似乎停顿了几秒钟，它很可能只是在设置GPU。</li><li id="7d05" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">为了简单起见，我只对训练部分计时。原则上，人们可能需要对整个代码的运行时进行基准测试。这取决于你想优化什么。</li><li id="d8e8" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">为了正确测试性能，需要多次运行相同的代码，以获得更好的运行时统计数据。</li><li id="81a8" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">还必须检查模型的收敛性，即使用GPU的并行化训练应该收敛到与在没有GPU的运行中看到的相同的损失和精度值。此外，无论使用多少个GPU，精度和损耗曲线都应该收敛到相同的值。</li></ul></div></div>    
</body>
</html>