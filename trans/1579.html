<html>
<head>
<title>Enhancing the Performance in Training Tiny Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">提高微型神经网络的训练性能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/enhancing-the-performance-in-training-tiny-neural-networks-601b0321716b#2022-04-15">https://towardsdatascience.com/enhancing-the-performance-in-training-tiny-neural-networks-601b0321716b#2022-04-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="08da" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">注意训练大型和小型神经网络的区别</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/89eca7e8742f7865672d067a0c016fd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zpol7w4NA0j1JRU9"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">克雷格·麦克戈尼格尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="922d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练深度神经网络(NN)是困难的，有时甚至对于资深从业者来说也是棘手的。为了在给定特定数据集的情况下达到模型的最高潜在性能，我们需要考虑许多方面:超参数调整、正则化、验证度量、归一化等等。<strong class="lb iu">然而，我们应该考虑的问题并不是对所有不结盟国家都适用的。这取决于模型的大小。</strong></p><p id="02bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于大模型容易过拟合，使用各种正则化方法消除过拟合是训练大模型时最重要的课题之一。然而，对于微型模型来说，情况并非如此，因为微型模型容易出现拟合不足的情况。如果在训练微小模型时使用正则化方法，性能可能会变差。</p><p id="6c8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为什么会这样？模型容量和正则化有什么关系？这篇文章将讨论它们。</p><h2 id="5ecd" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">模型拟合应考虑模型容量</h2><p id="8c1d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">深度学习模型通常很大，可扩展，这意味着模型规模越大，性能越好，尤其是对变形金刚而言。然而，有时我们需要在资源受限的AI芯片上部署微小的模型。工业上既需要大型号，也需要小型号。</p><p id="eecd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是问题所在。大多数书籍，教程，博客等。关于如何训练一个模特实际上是关于如何训练大模特。这将留下一种错觉，即所有规模的模型都可以以类似的方式训练，以获得良好的性能。</p><p id="155e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实上，如果你用训练大模型的方式训练一个小模型，你可能会错过要点。</p><h2 id="9857" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">大模型往往会过度拟合</h2><p id="8fc8" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">由于大模型有如此多的参数，它们容易学习数据集的所有噪声，这被称为过拟合。<strong class="lb iu">所有取自真实物理世界的数据集都包含大量噪声，在训练过程中消除这些噪声对于模型泛化至关重要</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/aa98a9ed86c02dec3c6d2766133576ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-5Up5xFNUc19eJuwk3TD9A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">欠拟合与过拟合(图片由作者提供)</p></figure><p id="398d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">过度拟合也意味着该模型不是抗噪声的。这就是为什么该模型在训练数据集上表现良好，但在验证数据集上表现不佳，因为包含在验证数据集中的噪声没有被该模型学习到。正则化是使模型对噪声鲁棒所必需的，因此该模型也可以在验证数据集上表现良好，即使它包含与训练数据集不同的噪声。</p><h2 id="34dd" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">为什么正则化对小模型不起作用</h2><p id="0d01" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu">正则化通过向数据集或模型添加噪声来工作</strong>。数据扩充等正则化方法会向数据集添加噪声，而丢弃等方法会向模型添加噪声。因为标签是固定的，所以模型可以被训练来推断具有这种噪声的相同标签，因此是抗噪声的。</p><p id="f53e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，如果模型很小，容易出现欠拟合怎么办？如上图，<strong class="lb iu">容量低的微小模型都无法很好的学习数据集，更不用说数据集中包含的噪音了。</strong>在这种情况下，我们应该扩大模型容量，使其先过拟合，然后再正则化。</p><h2 id="dd13" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">增强模型，而不是数据集</strong></h2><p id="5996" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">如何放大微小的模型？模型尺寸根本放大不了怎么办？</p><p id="12bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，麻省理工学院最近在ICLR2022上发表的一篇论文提出了一个名为NetAug的解决方案。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/f2ef51ab41f2b853d9b662338c30a9b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AM5ib4p-9uNQe4FixIOA6g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/pdf/2110.08890.pdf" rel="noopener ugc nofollow" target="_blank"> NetAug </a></p></figure><p id="4808" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者提出了一种简单的方法，在训练过程中扩大CNN模型的宽度(通道),在推理过程中保持原有的结构。如上图，结果很惊人。它还表明，正则化对小模型有负面影响。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/f6e2cf30c319062be35fef645aee5ffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eaFZ9avSp-8XNqzkaVut0A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/pdf/2110.08890.pdf" rel="noopener ugc nofollow" target="_blank"> NetAug损失函数</a></p></figure><p id="e7e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">NetAug的损耗函数是原始和扩充架构损耗的组合。由于每一层的增强的可能性，例如如何扩展信道，不是唯一的，所以我们可以有多个增强的架构。为了计算这些损失，将向前和向后通过的总次数相乘。然而，训练时间开销不会增加太多，因为模型很小，训练时间主要由数据加载和通信决定。</p><h2 id="3867" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">参考</h2><p id="53de" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/pdf/2110.08890.pdf" rel="noopener ugc nofollow" target="_blank">面向微小深度学习的网络增强，2021 </a></p><div class="mw mx gp gr my mz"><a href="https://dushuchen.medium.com/membership" rel="noopener follow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd iu gy z fp ne fr fs nf fu fw is bi translated">加入我的介绍链接-陈数杜媒体</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">阅读陈数·杜(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接支持…</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">dushuchen.medium.com</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn ks mz"/></div></div></a></div></div></div>    
</body>
</html>