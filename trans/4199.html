<html>
<head>
<title>Topic Modeling with LSA, pLSA, LDA, NMF, BERTopic, Top2Vec: a Comparison</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主题建模与LSA、pLSA、LDA、NMF、BERTopic、Top2Vec的比较</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/topic-modeling-with-lsa-plsa-lda-nmf-bertopic-top2vec-a-comparison-5e6ce4b1e4a5#2022-09-19">https://towardsdatascience.com/topic-modeling-with-lsa-plsa-lda-nmf-bertopic-top2vec-a-comparison-5e6ce4b1e4a5#2022-09-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="519a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">不同主题建模策略的比较，包括实际的Python例子</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/afb524f858770e8002a84b117fb43574.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gZdKeE6uuzUrstJYuOTpYw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><h1 id="8f17" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">目录</h1><ol class=""><li id="c09a" class="lq lr it ls b lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated"><a class="ae mi" href="#b6e0" rel="noopener ugc nofollow">简介</a></li><li id="6e13" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md me mf mg mh bi translated"><a class="ae mi" href="#ff29" rel="noopener ugc nofollow">话题建模策略</a> <br/> 2.1 <a class="ae mi" href="#80de" rel="noopener ugc nofollow">简介</a> <br/> 2.2 <a class="ae mi" href="#78f7" rel="noopener ugc nofollow">潜在语义分析(LSA) </a> <br/> 2.3 <a class="ae mi" href="#5a51" rel="noopener ugc nofollow">概率潜在语义分析(pLSA) </a> <br/> 2.4 <a class="ae mi" href="#1825" rel="noopener ugc nofollow">潜在狄利克雷分配(LDA) </a> <br/> 2.5 <a class="ae mi" href="#aee1" rel="noopener ugc nofollow">非负矩阵分解(NMF</a><br/>2.6<a class="ae mi" href="#5763" rel="noopener ugc nofollow">BERTopic和Top2Vec </a></li><li id="1824" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md me mf mg mh bi translated"><a class="ae mi" href="#e768" rel="noopener ugc nofollow">对比</a></li><li id="35fe" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md me mf mg mh bi translated"><a class="ae mi" href="#5868" rel="noopener ugc nofollow">附加备注</a> <br/> 4.1 <a class="ae mi" href="#70a3" rel="noopener ugc nofollow">一个题目不一定是我们所想的</a> <br/> 4.2 <a class="ae mi" href="#1ef5" rel="noopener ugc nofollow">题目不容易评价</a></li><li id="04a7" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md me mf mg mh bi translated"><a class="ae mi" href="#f916" rel="noopener ugc nofollow">结论</a></li><li id="ad16" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md me mf mg mh bi translated"><a class="ae mi" href="#675c" rel="noopener ugc nofollow">参考文献</a></li></ol><h1 id="47b6" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">1.介绍</h1><p id="029a" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated">在自然语言处理(NLP)中，术语主题建模包含一系列统计和深度学习技术，以在文档集中找到隐藏的语义结构。</p><p id="bedf" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">主题建模是一个<em class="ng">无监督的</em>机器学习问题。<em class="ng">无监督</em>意味着算法在没有标签或标记的情况下学习模式。</p><p id="4366" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">我们人类产生和交换的大多数信息都具有文本的性质。文档、对话、电话、消息、电子邮件、笔记、社交媒体帖子。在缺乏<em class="ng">先验</em>知识的情况下，从这些来源中自动提取价值的能力是数据科学中一个永恒且普遍存在的问题。</p><p id="740d" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">在这篇文章中，我们讨论了主题建模的流行方法，从传统算法到基于深度学习的最新技术。我们旨在分享一个<strong class="ls iu">对这些模型的友好介绍</strong>，以及<strong class="ls iu">比较它们在实际应用中的优缺点</strong>。</p><p id="89a7" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">我们还为最主要的方法提供了端到端的Python示例。</p><p id="d52d" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">最后，我们分享了关于无监督文本数据分析的两个最具挑战性的方面的一些考虑:人类对“主题”的定义与其统计对应物之间的差异，以及与主题模型性能的定量评估相关的困难。</p><h1 id="ff29" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">2.主题建模策略</h1><h2 id="80de" class="nh kz it bd la ni nj dn le nk nl dp li lx nm nn lk lz no np lm mb nq nr lo ns bi translated">2.1简介</h2><p id="de53" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated"><strong class="ls iu">潜在语义分析(LSA) </strong> (Deerwester等，1990)<strong class="ls iu">概率潜在语义分析(pLSA) </strong> (Hofmann，1999)<strong class="ls iu">潜在狄利克雷分配(LDA) </strong> (Blei等，2003)<strong class="ls iu">非负矩阵分解</strong> (Lee等，1999)是传统的和众所周知的主题建模方法。</p><p id="2abf" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">他们将文档表示为一个单词包，并假设每个文档都是潜在主题的混合物。</p><p id="8a9e" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">它们都是从将文本语料库转换成<strong class="ls iu">文档术语矩阵(DTM) </strong>开始的，这是一个表格，其中每行是一个文档，每列是一个不同的单词:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/283b9d5680417c331d061bb3488a365c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vLnFZ7Pz1WsKyxUpaCk78g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自一组样本文档的文档术语矩阵。图片作者。</p></figure><p id="3ba0" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><em class="ng">注</em>:实施/研究论文也可能使用/参考术语文档矩阵(TDM)，即DTM的转置。</p><p id="69b8" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">每个单元格<code class="fe nu nv nw nx b">&lt;i, j&gt;</code>包含一个计数，即单词<code class="fe nu nv nw nx b">j</code>在文档<code class="fe nu nv nw nx b">i</code>中出现的次数。字数统计的一个常见替代方法是<strong class="ls iu"> TF-IDF </strong>评分。它考虑了<strong class="ls iu">词频(TF) </strong>和<strong class="ls iu">逆文档频率(IDF) </strong>来惩罚在语料库中出现非常频繁的词的权重，并增加更罕见的词的权重:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/a97d1a40ee7ae0c89971a73d2a0f938f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I4uCELHdOEEimOyWm6Fmpw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TF-IDF。图片作者。</p></figure><p id="4d0c" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">潜在主题搜索的基本原理是将DTM分解为文档-主题和主题-术语矩阵。以下方法在定义和实现这一目标的方式上有所不同。</p><h2 id="78f7" class="nh kz it bd la ni nj dn le nk nl dp li lx nm nn lk lz no np lm mb nq nr lo ns bi translated"><strong class="ak"> 2.2潜在语义分析(LSA) </strong></h2><p id="cb58" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated">为了分解DTM和提取主题，<strong class="ls iu">潜在语义分析(LSA) </strong>应用称为<strong class="ls iu">单值分解(SVD) </strong>的矩阵分解技术。</p><p id="6c56" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">SVD将DTM分解为三个不同矩阵的乘积:<code class="fe nu nv nw nx b">DTM = U ∙ Σ ∙ Vᵗ</code>，其中</p><ul class=""><li id="310a" class="lq lr it ls b lt nb lv nc lx nz lz oa mb ob md oc mf mg mh bi translated"><code class="fe nu nv nw nx b">U</code>和<code class="fe nu nv nw nx b">V</code>的大小分别为<code class="fe nu nv nw nx b">m x m</code>和<code class="fe nu nv nw nx b">n x n</code>，分别是文档的数量<code class="fe nu nv nw nx b">m</code>和语料库中单词的数量<code class="fe nu nv nw nx b">n</code>。</li><li id="c0d6" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated"><code class="fe nu nv nw nx b">Σ</code>是<code class="fe nu nv nw nx b">m x n</code>，并且只有它的主对角线被填充:它包含DTM的奇异值。</li></ul><p id="b57e" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">LSA选择DTM的前<code class="fe nu nv nw nx b">t &lt;= min(m, n)</code>个最大奇异值，从而分别丢弃<code class="fe nu nv nw nx b">U</code>和<code class="fe nu nv nw nx b">V</code>的最后<code class="fe nu nv nw nx b">m - t</code>和<code class="fe nu nv nw nx b">n - t</code>列。这个过程被称为<strong class="ls iu">截断奇异值分解</strong>。得到的DTM近似值的等级为<code class="fe nu nv nw nx b">t</code>，如下图所示。</p><p id="a87b" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">就L₂范数而言，DTM的秩<code class="fe nu nv nw nx b">t</code>近似是最接近DTM的秩<code class="fe nu nv nw nx b">t</code>矩阵，从这个意义上说，DTM的秩<code class="fe nu nv nw nx b">t</code>近似是最佳的。剩下的列<code class="fe nu nv nw nx b">U</code>和<code class="fe nu nv nw nx b">V</code>可以解释为文档-主题和单词-主题矩阵，<code class="fe nu nv nw nx b">t</code>表示主题的数量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/9b750644f84ac449982bd0a38d361a71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HR8de7uYPKtp3b6t3hiAIw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在文档术语矩阵(DTM)上截断SVD以提取潜在变量(主题)。图片作者。</p></figure><p id="328f" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu">优点</strong>:</p><ul class=""><li id="c646" class="lq lr it ls b lt nb lv nc lx nz lz oa mb ob md oc mf mg mh bi translated">直觉。</li><li id="3742" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">它既适用于短文档，也适用于长文档。</li><li id="d36e" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">主题通过<code class="fe nu nv nw nx b">V</code>矩阵向人类解释开放。</li></ul><p id="c709" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu">缺点</strong>:</p><ul class=""><li id="2d8b" class="lq lr it ls b lt nb lv nc lx nz lz oa mb ob md oc mf mg mh bi translated">DTM不考虑语料库中单词的语义表示。相似的概念被视为不同的矩阵元素。预处理技术可能会有所帮助，但只是在一定程度上。例如，词干可能有助于将"<em class="ng">意大利语</em>"和"<em class="ng">意大利语</em>"视为相似的术语(它们应该是相似的)，但是具有不同词干的相近单词，如"<em class="ng"> money </em>"和"<em class="ng"> cash </em>"仍然会被视为不同的。此外，词干化也可能导致更难解释的主题。</li><li id="fe47" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">LSA需要大量的预处理阶段来从文本输入数据中获得有意义的表示。</li><li id="9f48" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">截断SVD <strong class="ls iu">中要维护的奇异值<code class="fe nu nv nw nx b">t</code>(题目)个数必须是已知的</strong> <em class="ng">先验的</em>。</li><li id="3078" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated"><code class="fe nu nv nw nx b">U</code>和<code class="fe nu nv nw nx b">V</code>可能包含负值。这就造成了可解释性的问题(详见第2.5段)。</li></ul><h2 id="5a51" class="nh kz it bd la ni nj dn le nk nl dp li lx nm nn lk lz no np lm mb nq nr lo ns bi translated"><strong class="ak"> 2.3概率潜在语义分析(pLSA) </strong></h2><p id="af7b" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated">Hofmann (1999)提出了一种LSA的变体，其中使用概率模型而不是SVD来估计主题。因此得名，<strong class="ls iu">概率潜在语义分析(pLSA) </strong>。</p><p id="027e" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">特别地，pLSA将看到单词<code class="fe nu nv nw nx b">w</code>和文档<code class="fe nu nv nw nx b">d</code>的联合概率<code class="fe nu nv nw nx b">P(d, w)</code>建模为条件独立多项式分布的混合:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/dfd84f8c2349a9c1bc0dd6190d2e62da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*Gyk4uh5mFbRB9-1u2Jp5_g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">摘自霍夫曼(1999)。</p></figure><p id="f2b3" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">其中:</p><ul class=""><li id="7026" class="lq lr it ls b lt nb lv nc lx nz lz oa mb ob md oc mf mg mh bi translated"><code class="fe nu nv nw nx b">w</code>表示一个词。</li><li id="eeca" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated"><code class="fe nu nv nw nx b">d</code>表示文档。</li><li id="cf87" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated"><code class="fe nu nv nw nx b">z</code>表示一个话题。</li><li id="fcd0" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated"><code class="fe nu nv nw nx b">P(z|d)</code>是主题<code class="fe nu nv nw nx b">z</code>出现在文档<code class="fe nu nv nw nx b">d</code>中的概率。</li><li id="7efa" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated"><code class="fe nu nv nw nx b">P(w|z)</code>是单词<code class="fe nu nv nw nx b">w</code>出现在主题<code class="fe nu nv nw nx b">z</code>中的概率。</li><li id="8f33" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">我们假设<code class="fe nu nv nw nx b">P(w|z, d) = P(w|z)</code>。</li></ul><p id="ca64" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">前面的表达式可以重写为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/593f03680d33947e56c1e82c138d0564.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y9_uJI0EJf7umNgnP80XIw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">摘自霍夫曼(1999)。</p></figure><p id="ed3a" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">我们可以在这个表达式和以前的DTM分解公式之间进行类比，其中:</p><ul class=""><li id="8f2b" class="lq lr it ls b lt nb lv nc lx nz lz oa mb ob md oc mf mg mh bi translated"><code class="fe nu nv nw nx b">P(d, w)</code>对应于DTM。</li><li id="a758" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated"><code class="fe nu nv nw nx b">P(z)</code>类似于<code class="fe nu nv nw nx b">Σ</code>的主对角线。</li><li id="b05d" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated"><code class="fe nu nv nw nx b">P(d|z)</code>和<code class="fe nu nv nw nx b">P(w|z)</code>分别对应<code class="fe nu nv nw nx b">U</code>和<code class="fe nu nv nw nx b">V</code>。</li></ul><p id="96b9" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">该模型可以使用<strong class="ls iu">期望最大化算法(EM) </strong>进行拟合。简而言之，EM在潜在变量(在本例中是主题)存在的情况下执行最大似然估计。</p><p id="5fed" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">值得注意的是，DTM的分解依赖于不同的目标函数。对于LSA，它是L₂范数，而对于pLSA，它是似然函数。后者旨在明确最大化模型的预测能力。</p><p id="9e79" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">pLSA与LSA模型具有相同的优点和缺点，但也有一些特殊的差异:</p><p id="a233" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu">优点:</strong></p><ul class=""><li id="1ecb" class="lq lr it ls b lt nb lv nc lx nz lz oa mb ob md oc mf mg mh bi translated">与LSA相比，pLSA表现出更好的性能(Hofmann，1999)。</li></ul><p id="ffc6" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu">缺点:</strong></p><p id="31d9" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">pLSA没有提供文档级别的概率模型。这意味着:</p><ul class=""><li id="7d67" class="lq lr it ls b lt nb lv nc lx nz lz oa mb ob md oc mf mg mh bi translated">参数的数量随着文档的数量线性增长，导致可伸缩性和过度拟合的问题。</li><li id="3831" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">它不能给新文档分配概率。</li></ul><h2 id="1825" class="nh kz it bd la ni nj dn le nk nl dp li lx nm nn lk lz no np lm mb nq nr lo ns bi translated"><strong class="ak"> 2.4潜在狄利克雷分配(LDA) </strong></h2><p id="c396" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated"><strong class="ls iu">潜在狄利克雷分配(LDA) </strong> (Blei等人，2003)通过使用狄利克雷先验以贝叶斯方法估计文档-主题和术语-主题分布来改进pLSA。</p><p id="6d9b" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">狄利克雷分布<code class="fe nu nv nw nx b">Dir(α)</code>是一族由正实数向量<code class="fe nu nv nw nx b">α</code>参数化的连续多元概率分布。</p><p id="db72" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">让我们想象一份报纸有三个部分:政治、体育和艺术，每个部分也代表一个主题。报纸版面中主题混合的假设分布是狄利克雷分布的一个例子:</p><p id="923b" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">第一节(政治):</p><ul class=""><li id="5111" class="lq lr it ls b lt nb lv nc lx nz lz oa mb ob md oc mf mg mh bi translated">混合主题:政治0.99，体育0.005，艺术0.005。</li></ul><p id="fe74" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">第二节(体育):</p><ul class=""><li id="e236" class="lq lr it ls b lt nb lv nc lx nz lz oa mb ob md oc mf mg mh bi translated">主题混合:政治0.005，体育0.99，艺术0.005。</li></ul><p id="3f4f" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">第3节(艺术):</p><ul class=""><li id="9423" class="lq lr it ls b lt nb lv nc lx nz lz oa mb ob md oc mf mg mh bi translated">混合主题:政治0.005，体育0.005，艺术0.99。</li></ul><p id="857b" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">让我们观察LDA的平板符号(在图形模型中表示变量的传统方法)来解释Dirichlet先验的使用:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/6097e7b4cd86f8da724ee1bdd13c5349.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*OfiLT56R8JrtNp7y7wN7DQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">LDA的平面符号。来自Barbieri⁵ (2013)。灰色圆圈表示观察变量(语料库中的单词)，而白色圆圈表示潜在变量。</p></figure><p id="2bf1" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">m表示文档的数量，N表示文档中的字数。从顶部开始，我们观察到<code class="fe nu nv nw nx b">α</code>，它是每个文档主题分布的Dirichlet先验的参数。从狄利克雷分布<code class="fe nu nv nw nx b">Dir(α)</code>中，我们抽取一个随机样本来代表文档的主题分布<code class="fe nu nv nw nx b">θ</code>。就好像，在我们的报纸例子中，我们用一个混合物(<em class="ng"> 0.99政治，0.05体育，0.05艺术</em>)来描述一篇文章的主题分布。</p><p id="d917" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">从选择的混合物<code class="fe nu nv nw nx b">θ</code>中，我们根据分布绘制一个主题<code class="fe nu nv nw nx b">z</code>(在我们的例子中，<em class="ng">政治</em>)。从底部，我们观察到<code class="fe nu nv nw nx b">β</code>，狄利克雷先验关于每主题单词分布的参数。从狄利克雷分布<code class="fe nu nv nw nx b">Dir(𝛽)</code>中，我们选择一个代表给定主题<code class="fe nu nv nw nx b">z</code>的词分布<code class="fe nu nv nw nx b">φ</code>的样本。并且，从<code class="fe nu nv nw nx b">φ</code>我们画出一个字<code class="fe nu nv nw nx b">w</code>。</p><p id="e4da" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">最后，我们感兴趣的是在给定文档<code class="fe nu nv nw nx b">d</code>和参数<code class="fe nu nv nw nx b">α</code>和<code class="fe nu nv nw nx b">𝛽</code>的情况下，估计主题<code class="fe nu nv nw nx b">z</code>的概率，即<code class="fe nu nv nw nx b">P(z|d, α, 𝛽)</code>。该问题被公式化为给定文档的隐藏变量的后验分布的计算:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/fe315a4e50416c8193d3598b715ae920.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*59XHJE4vpCUSM_hKVX6Tmg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="3e55" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">由于这种分布难以计算，Blei等人(2013年)建议使用近似推理算法(变分近似)。通过最小化近似分布和真实后验分布之间的Kullback-Leibler散度找到优化值<code class="fe nu nv nw nx b">P(θ, z|d, α, 𝛽)</code>。一旦我们获得了数据的最佳参数，我们可以再次计算<code class="fe nu nv nw nx b">P(z|d, α, 𝛽)</code>，在某种意义上，它对应于文档-主题矩阵<code class="fe nu nv nw nx b">U</code>。<code class="fe nu nv nw nx b">𝛽₁, 𝛽₂, ..., 𝛽ₜ</code>的每一个条目都是<code class="fe nu nv nw nx b">p(w|z)</code>，与术语-主题矩阵<code class="fe nu nv nw nx b">V</code>相对应。主要区别在于，很像pLSA，矩阵系数有一个统计解释。</p><p id="d182" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu">优点</strong>:</p><ul class=""><li id="c818" class="lq lr it ls b lt nb lv nc lx nz lz oa mb ob md oc mf mg mh bi translated">它提供了比LSA和pLSA更好的性能。</li><li id="37f9" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">与pLSA不同，由于文档主题狄利克雷分布，LDA可以为新文档分配概率。</li><li id="39f8" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">它既适用于短文档，也适用于长文档。</li><li id="302e" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">主题可以由人来解释。</li><li id="4f5d" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">作为一个概率模块，LDA可以嵌入到更复杂的模型中，也可以扩展。Blei等人(2013)的原始工作之后的研究扩展了LDA，并解决了一些原始限制。</li></ul><p id="f28e" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu">缺点:</strong></p><ul class=""><li id="3855" class="lq lr it ls b lt nb lv nc lx nz lz oa mb ob md oc mf mg mh bi translated">题目的数量必须事先知道。</li><li id="77bb" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">与LSA和pLSA类似，词袋方法不考虑语料库中词的语义表示。</li><li id="6001" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">贝叶斯参数<code class="fe nu nv nw nx b">α</code>和<code class="fe nu nv nw nx b">β</code>的估计基于文档可交换性的假设。</li><li id="f3e0" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">它需要大量的预处理阶段来从文本输入数据中获得有意义的表示。</li><li id="59ed" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">研究报告LDA可能会产生<em class="ng">过于笼统的</em> (Rizvi⁶等人，2019年)或<em class="ng">不相关的</em> (Alnusyan⁷等人，2020年)主题。不同的执行结果也可能不一致(Egger⁸等人，2021)。</li></ul><p id="1af5" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu">LDA的实际例子</strong></p><p id="2307" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">流行的LDA实现在<a class="ae mi" href="https://radimrehurek.com/gensim/models/ldamodel.html" rel="noopener ugc nofollow" target="_blank"> Gensim </a>和<a class="ae mi" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html" rel="noopener ugc nofollow" target="_blank"> sklearn </a>包(Python)和<a class="ae mi" href="https://mimno.github.io/Mallet/" rel="noopener ugc nofollow" target="_blank"> Mallet </a> (Java)中。</p><p id="435e" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">在下面的例子中，我们使用Gensim库和<a class="ae mi" href="https://pypi.org/project/pyLDAvis/" rel="noopener ugc nofollow" target="_blank"> pyLDAvis </a>进行可视化主题探索。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/afb524f858770e8002a84b117fb43574.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gZdKeE6uuzUrstJYuOTpYw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由前面的代码片段生成的pyLDAvis主题探索的交互式图表。图片作者。</p></figure><h2 id="aee1" class="nh kz it bd la ni nj dn le nk nl dp li lx nm nn lk lz no np lm mb nq nr lo ns bi translated">2.5非负矩阵分解(NMF)</h2><p id="9dd5" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated"><strong class="ls iu">lee⁴等人(1999)提出的非负矩阵分解(NMF) </strong>是LSA的一种变体。</p><p id="455e" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">LSA利用奇异值分解分解文档术语矩阵，提取潜在信息(主题)。SVD的一个特性是基向量彼此正交，迫使基中的一些元素为负。</p><p id="5d63" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">简而言之，矩阵系数为负的因式分解(如SVD)带来了可解释性的问题。减法组合不允许理解一个成分如何对整体作出贡献。NMF将文档-术语矩阵分解成主题-文档矩阵<code class="fe nu nv nw nx b">U</code>和主题-术语矩阵<code class="fe nu nv nw nx b">Vᵗ</code>，很像奇异值分解，但是有一个额外的约束，即<code class="fe nu nv nw nx b">U</code>和<code class="fe nu nv nw nx b">Vᵗ</code> <strong class="ls iu">只能包含非负元素</strong>。</p><p id="5eb9" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">此外，虽然我们利用了形式<code class="fe nu nv nw nx b">U ∙ Σ ∙ Vᵗ</code>的分解，但在非负矩阵分解的情况下，这变成了:<code class="fe nu nv nw nx b">U ∙ Vᵗ</code>。</p><p id="0cdb" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">DTM的分解可以被视为一个优化问题，其目标是最小化DTM与其近似值之间的差异。经常采用的距离度量是Frobenius范数和Kullback-Leibler散度。</p><p id="c9b1" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">NMF与其他经典模型具有相同的主要优点和缺点(单词袋方法、需要预处理等)，但也有一些独特的特征:</p><p id="fe7c" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu">优点:</strong></p><ul class=""><li id="4907" class="lq lr it ls b lt nb lv nc lx nz lz oa mb ob md oc mf mg mh bi translated">文献论证了NMF比SVD(因此LSA)在产生更多可解释的和连贯的主题方面的优势(Lee⁴等1999，Xu⁹等2003；卡萨利诺·⁰等人，2016年)。</li></ul><p id="cf63" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu">缺点:</strong></p><ul class=""><li id="e563" class="lq lr it ls b lt nb lv nc lx nz lz oa mb ob md oc mf mg mh bi translated">非负约束使得分解更加困难，并可能导致不准确的主题。</li><li id="60a4" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">NMF问题是非凸问题。不同的<code class="fe nu nv nw nx b">U</code>和<code class="fe nu nv nw nx b">Vᵗ</code>可能近似DTM，导致不同运行的结果可能不一致。</li></ul><p id="399e" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu">NMF的实际例子</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h2 id="5763" class="nh kz it bd la ni nj dn le nk nl dp li lx nm nn lk lz no np lm mb nq nr lo ns bi translated">2.6 BERTopic和Top2Vec</h2><p id="7eb0" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated">Grootendorst (2022)和安杰洛夫(2020)提出了主题建模的新方法，分别是<strong class="ls iu"> BERTopic </strong>和<strong class="ls iu"> Top2Vec </strong>。这些模型解决了迄今为止讨论的传统策略的局限性。我们将在下面的段落中一起探讨它们。</p><p id="1135" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu"> 2.6.1文件嵌入</strong></p><p id="070f" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">BERTopic和Top2Vec从输入文档中制造语义嵌入。</p><p id="0cc6" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">在最初的论文中，BERTopic利用BERT句子转换器(SBERT)来制造高质量的上下文单词和句子向量表示。相反，Top2Vec使用Doc2Vec创建联合嵌入的单词、文档和主题向量。</p><p id="ca1d" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">在撰写本文时，这两种算法都支持各种嵌入策略，尽管BERTopic对嵌入模型的覆盖范围更广:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">BERTopic和Top2Vec目前支持的嵌入模型。表中的参考资料提供了更详细的信息。</p></figure><p id="dbdf" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu">2 . 6 . 2 UMAP降维</strong></p><p id="28e3" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">可以将聚类算法直接应用于嵌入，但是这将增加计算开销并导致较差的聚类性能(由于“<em class="ng">维数灾难</em>”)。</p><p id="40a9" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">因此，在聚类之前应用降维技术。<strong class="ls iu"> UMAP(均匀流形近似和投影)</strong>(麦金尼斯等人，2018)提供了几个好处:</p><ul class=""><li id="242e" class="lq lr it ls b lt nb lv nc lx nz lz oa mb ob md oc mf mg mh bi translated">它在较低的投影维度中保留了更多的高维数据的<em class="ng">局部</em>和<em class="ng">全局</em>特征(麦金尼斯等人，2018)。</li><li id="ef1e" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">UMAP对嵌入维数没有计算限制(麦金尼斯等人，2018)。因此，它可以有效地用于不同的文档嵌入策略。</li><li id="1d48" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">使用UMAP降低嵌入维数提高了K-Means和HDBSCAN在准确性和时间方面的聚类性能(阿拉维⁴等人，2020)。</li><li id="a705" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">UMAP可以轻松扩展到大型数据集(安杰洛夫，2020)。</li></ul><p id="9b4d" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu"> 2.6.3聚类</strong></p><p id="6102" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">BERTopic和Top2Vec最初都利用HDBSCAN(麦金尼斯·⁵等人，2017年)作为聚类算法。</p><p id="2b4c" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu">优点:</strong></p><ul class=""><li id="dbd4" class="lq lr it ls b lt nb lv nc lx nz lz oa mb ob md oc mf mg mh bi translated">HDBSCAN继承了DBSCAN的优点，并对其进行了改进(麦金尼斯·⁵等人，2017)。</li><li id="7b1d" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">HDBSCAN(作为DBSCAN)不会将观察结果强制到集群中。它将不相关的观察值建模为异常值。这提高了主题的代表性和连贯性。</li></ul><p id="67ef" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu">缺点:</strong></p><ul class=""><li id="b15e" class="lq lr it ls b lt nb lv nc lx nz lz oa mb ob md oc mf mg mh bi translated">将不相关的文档建模为离群值可能会导致信息丢失。离群值可能成为噪声数据集中原始语料库的相关部分。</li></ul><p id="b5bb" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">BERTopic目前还支持<strong class="ls iu"> K均值</strong>和<strong class="ls iu">凝聚聚类</strong>算法，提供了选择的灵活性。K-Means允许选择所需数量的簇，并强制每个文档成为一个簇。这避免了离群值的产生，但也可能导致较差的主题表示和连贯性。</p><p id="1d5d" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu"> 2.6.4。主题表示</strong></p><p id="1201" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">BERTopic和Top2Vec在如何制造主题的表示方面互不相同。</p><p id="c8fe" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">BERTopic 将同一个集群(主题)内的所有文档连接起来，并应用修改后的TF-IDF。简而言之，它用原始TF-IDF公式中的集群替换文档。然后，它使用每个聚类的第一个最重要的词作为主题表示。</p><p id="12ea" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">这个分数被称为<strong class="ls iu">基于类别的TF-IDF (c TF-IDF) </strong>，因为它估计的是词在聚类中的重要性，而不是文档。</p><p id="f51c" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu"> Top2Vec </strong>相反，使用最接近集群的<strong class="ls iu">质心</strong>的单词来生成表示。特别地，对于通过HDBSCAN获得的每个密集区域，计算原始维度中文档向量的质心，然后选择最接近的单词向量。</p><p id="19b0" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu">BERTopic和Top2Vec的优劣:</strong></p><ul class=""><li id="c152" class="lq lr it ls b lt nb lv nc lx nz lz oa mb ob md oc mf mg mh bi translated">题目数量不一定事先给定。BERTopic和Top2Vec都支持分层主题缩减，以优化主题数量。</li><li id="3e01" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">与单词袋方法不同，高质量的嵌入考虑了语料库中单词之间的语义关系。这就引出了更好、更丰富的话题。</li><li id="1b5e" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">由于嵌入的语义本质，在大多数情况下不需要文本预处理(词干化、词元化、停用词移除等)。</li><li id="df59" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">BERTopic支持动态主题建模。</li><li id="5858" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">模块化。每个步骤(文档嵌入、降维、聚类)实际上都是自洽的，并且可以根据该领域的进步、特定项目的特性或技术限制而改变或发展。例如，可以使用具有Doc2Vec嵌入的BERTopic来代替SBERT，或者应用K-Means聚类来代替HDBSCAN。</li><li id="6bee" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">与传统方法相比，它们在更大的语料库中具有更好的扩展性(安杰洛夫，2020)。</li><li id="e523" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">BERTopic和Top2Vec都提供了高级的内置搜索和可视化功能。它们使调查主题质量和推动进一步优化变得更加简单，并为演示制作高质量的图表。</li></ul><p id="e303" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu">BERTopic和Top2Vec的对比:</strong></p><ul class=""><li id="af9e" class="lq lr it ls b lt nb lv nc lx nz lz oa mb ob md oc mf mg mh bi translated">他们更擅长处理较短的文本，如社交媒体帖子或新闻标题。大多数基于transformers的嵌入在构建语义表示时，对可以考虑的标记数量有限制。T2有可能对较长的文档使用这些算法。例如，可以在嵌入步骤之前将文档分成句子或段落。然而，这不一定有利于为较长的文档生成有意义和有代表性的主题。</li><li id="36be" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">每个文档只分配给一个主题。相反，像LDA这样的传统方法是建立在每个文档都包含混合主题的假设之上的。</li><li id="9cc1" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">与传统模型相比，它们的速度较慢(Grootendorst，2022)。此外，更快的训练和推理可能需要更昂贵的硬件加速器(GPU)。</li><li id="2948" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">尽管BERTopic利用基于transformers的大型语言模型来制造文档嵌入，但主题表示仍然使用单词包方法(c TF-IDF)。</li><li id="02de" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">对于小数据集，它们可能不太有效(&lt;1000 docs) (Egger¹⁶ et al., 2022).</li></ul><p id="a791" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu">BERTopic</strong>的实际例子)</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/48faa391aebb4f48f6e7e1004dc06e8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*6gGefdPQHDID1hedn_phNg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由前面的代码片段生成的主题间距离图。该可视化类似于pyLDAvis获得的可视化。图片作者。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/c4477fdb595b00f1e24b08f86bc19f24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K251QZFnzF-cb0OG1e6igA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">记录由前面的代码片段生成的投影(细节)。图片作者。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/17375d67f26339f4846a96f87b26140f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WSfhBMOhII7QLY_PNosvMg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由前面的代码片段生成的主题层次结构(树状图)。图片作者。</p></figure><p id="8ba2" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu">top 2 vec的实际例子</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/4eaf2a588eabe772557191b8901a4388.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gJKBF6Fpx9TFJaEzgfYuzw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由前面的代码片段生成的与输入查询“faith”最接近的五个主题的词云。图片作者。</p></figure><h1 id="e768" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">3.比较</h1><p id="3b88" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated">下表总结了考虑到实际应用场景的不同主题建模策略的显著特征:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">不同主题建模技术的比较。<strong class="ak">注</strong> : LSA和pLSA未包括在内，因为LDA克服了它们的局限性，被认为是三种方法中的最佳方法。按作者分类的表格。</p></figure><p id="c8d9" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">该汇总表为给定用例提供了<strong class="ls iu">高级选择标准</strong>。</p><p id="ed65" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">下面分享一些例子。</p><p id="f551" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">想象一下，只需很少的预处理工作，就能在推文中找到热门话题。在这种情况下，可以选择使用Top2Vec和BERTopic。它们在较短的文本资源上表现出色，并且不需要太多的预处理。</p><p id="5241" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">相反，设想一个场景，其中客户有兴趣发现一个给定的文档如何包含多个主题的混合。在这种情况下，像LDA和NMF这样的方法会更好。BERTopic和Top2Vec只为一个主题分配一个文档。虽然HDBSCAN的概率分布可以作为主题分布的代理，但BERTopic和Top2Vec并不是混合成员模型<em class="ng">的设计</em>。</p><h1 id="5868" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">4.附加备注</h1><p id="c827" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated">在讨论主题建模时，有两个主要的注意点值得在我们的旅程结束时提及。</p><h2 id="70a3" class="nh kz it bd la ni nj dn le nk nl dp li lx nm nn lk lz no np lm mb nq nr lo ns bi translated"><strong class="ak"> 4.1一个话题(不一定)是我们想象的那样</strong></h2><p id="0161" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated">当我们在候诊室看到一本杂志时，我们一眼就知道它属于哪种类型。当我们进入一段对话时，几句话就足以让我们猜出讨论的对象。这是一个从人类角度出发的<em class="ng">话题</em>。</p><p id="a3b5" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">不幸的是，术语“<em class="ng">主题</em>”在到目前为止讨论的模型中具有完全不同的含义。</p><p id="c7c5" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">让我们记住文档术语矩阵。在高层次上，我们希望将其分解为文档-主题和主题-术语矩阵的产物，并提取该过程中的潜在维度-主题。这些策略(如LSA)的目标是最小化分解误差。</p><p id="2c16" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">概率生成模型(如LDA)添加了一个额外的统计形式层，采用了一种健壮而优雅的贝叶斯方法，但它们真正试图做的是以最小的误差重建原始的文档词分布。</p><p id="a5b2" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">这些模型<em class="ng">都不能确保</em>从<em class="ng">人类</em>的角度来看，所获得的主题是有信息的或有用的。</p><p id="28b3" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">用Blei等人(2013)的漂亮话来说:</p><blockquote class="oo op oq"><p id="73a9" class="mo mp ng ls b lt nb ju mq lv nc jx mr or nd mt mu os ne mw mx ot nf mz na md im bi translated"><em class="it">“我们将LDA模型中的潜在多项式变量称为主题，以便利用面向文本的直觉，但是</em> <strong class="ls iu"> <em class="it">除了这些潜在变量在表示单词集</em> </strong> <em class="it">上的概率分布的效用之外，我们不对这些潜在变量做出认识论上的声明。”</em></p></blockquote><p id="5f9a" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">另一方面，BERTopic和Top2Vec利用了语义嵌入。因此，从“人”的角度来看，用来表示文档的向量代表了它们的“意义”(这是目前为止我们所知道的最接近的)。这些令人惊叹的模型假设，将这些嵌入的投影聚集起来可能会导致更有意义和更具体的主题。</p><p id="43cc" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">研究(举几个例子:Grootendorst 2022，Angelov 2020，Egger ⁶等人2022)表明，利用语义嵌入获得的主题在几个领域中更具信息性和连贯性。</p><p id="cd0f" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">但即使在这种情况下，人类和这类模型的主题定义之间仍然存在潜在的不匹配。这些算法产生的是<strong class="ls iu">“语义相似的文档的可解释的、良好表示的和连贯的组”</strong>。</p><p id="5783" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">毫无疑问:这是一个杰出和独特的结果，在该领域开辟了一个全新的前沿，取得了前所未有的成绩。</p><p id="3e4a" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">但是我们仍然会争论这如何接近人类对主题的定义，以及在什么情况下。</p><p id="6a02" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">而如果你认为这是一个微不足道的微妙之处，你有没有尝试过向商业利益相关者解释类似“<em class="ng"> mail_post_email_posting </em>”这样的话题？是的，它是连贯的，可解释的，但它是他们想象一个“<em class="ng">题目</em>”时想到的吗？</p><p id="907f" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">这就引出了我们要注意的第二点。</p><h2 id="1ef5" class="nh kz it bd la ni nj dn le nk nl dp li lx nm nn lk lz no np lm mb nq nr lo ns bi translated">4.2题目不好评价</h2><p id="4e6e" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated">主题建模是一种无监督的技术。评估期间没有标签可依赖。</p><p id="6d76" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu">连贯性测量</strong>已被提议用于评估主题在可解释性方面的质量。例如，<strong class="ls iu">归一化逐点互信息(NPMI) </strong>(布马·⁷，2009)估计两个单词<code class="fe nu nv nw nx b">x</code>和<code class="fe nu nv nw nx b">y</code>同现的可能性比我们随机预期的要大:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/bdfe9d114fd7f90ef8ad714e8f6d69c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*B5HqP3YuFfYdgzrS3A8zJQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="1b2c" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">NPMI可以从-1(无共现)到+1(完全共现)不等。<code class="fe nu nv nw nx b">x</code>和<code class="fe nu nv nw nx b">y</code>发生之间的独立性导致NPMI=0。</p><p id="a797" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">刘⁸等人(2014)认为，这一指标在合理的程度上模仿了人类的判断。</p><p id="d79f" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">还存在其他一致性措施。例如，Cv(罗德⁹等人，2015年)和UMass(明诺⁰等人，2011年)。</p><p id="edfd" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">这些一致性度量存在一系列缺点:</p><ul class=""><li id="2078" class="lq lr it ls b lt nb lv nc lx nz lz oa mb ob md oc mf mg mh bi translated">对于定性绩效应使用何种指标，目前尚无统一的约定(左等人，2016年；布莱尔等人，2020年；杜根等人，2021年)。</li><li id="06d9" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">Blair等人(2020)报告了不同一致性测量之间的不一致结果。</li><li id="0d98" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">Doogan等人(2021年)指出了一致性测量在评估特定领域(Twitter数据)的主题模型时的不可靠性。</li><li id="7ea9" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">霍伊尔·⁴等人2021年提出，NPMI等指标可能无法评估神经主题模型的可解释性。</li><li id="02b1" class="lq lr it ls b lt mj lv mk lx ml lz mm mb mn md oc mf mg mh bi translated">由于报道的不一致，Cv的作者不鼓励⁵使用cv。</li></ul><p id="b61c" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">正如Grootendorst (2022年)所写:</p><blockquote class="oo op oq"><p id="a1d2" class="mo mp ng ls b lt nb ju mq lv nc jx mr or nd mt mu os ne mw mx ot nf mz na md im bi translated">主题一致性和主题多样性等验证措施是本质上主观评估的代理。一个用户对主题的连贯性和多样性的判断可能与另一个用户不同。因此， <strong class="ls iu"> <em class="it">虽然这些衡量标准可以用来得到一个模特表现的指标，但它们仅仅是一个指标</em> </strong> <em class="it">。</em></p></blockquote><p id="8bfc" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">总之，验证方法无法清晰地评估主题模型的性能。它们不像分类问题的<em class="ng">准确度</em>或<em class="ng"> F1分数</em>那样提供明确的解释。因此，对所获得主题的“<em class="ng">品质度量</em>的量化仍然需要领域知识和人工评估。商业价值的评估(<em class="ng">“这些主题会使项目受益吗？”</em>)不是一件小事，可能需要综合指标和整体方法。</p><h1 id="f916" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">5.结论</h1><p id="6bc7" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated">在这篇文章中，我们分享了流行主题建模算法的友好概述，从生成统计模型到基于变压器的方法。</p><p id="6451" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">我们还提供了一个表格，突出了每种技术的优点和缺点。这可用于比较，并有助于不同场景下的初步模型选择。</p><p id="e904" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">最后，我们分享了无监督文本数据分析的两个最具挑战性的方面。</p><p id="498c" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">首先，人类对“主题”的定义和它的统计对应物之间的差异经常被忽视，这是“主题建模”算法的结果。理解这种差异对于满足项目目标和指导NLP工作中业务涉众的期望是至关重要的。</p><p id="b7bc" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">然后，我们讨论了定量评估主题模型性能的困难，介绍了流行的度量标准及其缺点。</p><h1 id="675c" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">6.参考</h1><p id="eff5" class="pw-post-body-paragraph mo mp it ls b lt lu ju mq lv lw jx mr lx ms mt mu lz mv mw mx mb my mz na md im bi translated">[1] Deerwester等人，<em class="ng">潜在语义分析索引</em>，《美国信息科学学会杂志》第41卷第6期第391–407页，1990年(<a class="ae mi" href="http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="dd3f" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[2] Hofmann，<em class="ng">概率潜在语义分析</em>，第十五届人工智能不确定性会议论文集(UAI1999)，1999 ( <a class="ae mi" href="https://arxiv.org/abs/1301.6705" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="10d4" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[3] Blei等人，<em class="ng">潜在狄利克雷分配</em>，《机器学习研究杂志》第3卷第993–1022页，2003 ( <a class="ae mi" href="https://dl.acm.org/doi/10.5555/944919.944937" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="dc61" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[4] Lee等，<em class="ng">用非负矩阵分解</em>学习物体的部件，《自然》，第401卷，第788–791页，1999 ( <a class="ae mi" href="https://www.nature.com/articles/44565" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="e7ea" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[5]巴比耶里等，<em class="ng">序列数据的概率主题模型</em>，《机器学习》第93卷第5–29页，2013 ( <a class="ae mi" href="https://link.springer.com/article/10.1007/s10994-013-5391-2" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="2cbe" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[6] Rizvi等人，<em class="ng">分析社交媒体数据以了解消费者对膳食补充剂的信息需求</em>，Stud。健康技术。告知。，第264卷，第323–327页，2019年(<a class="ae mi" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6792048/" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="4448" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[7] Alnusyan等，<em class="ng">一种用于用户评论主题建模和分类的半监督方法</em>，国际计算与信息技术会议，2020年1–5期(<a class="ae mi" href="https://ieeexplore.ieee.org/document/9213721" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="adf3" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[8] Egger和Yu，<em class="ng">识别Instagram数据中隐藏的语义结构:一个主题建模比较</em>，Tour。版本2021:244，2021 ( <a class="ae mi" href="https://www.researchgate.net/publication/355425735_Identifying_hidden_semantic_structures_in_Instagram_data_A_topic_modelling_comparison" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="78df" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[9]徐等，<em class="ng">基于非负矩阵分解的文档聚类</em>，第26届国际ACM信息检索研究与发展会议论文集，2003年第267–273页(<a class="ae mi" href="https://dl.acm.org/doi/10.1145/860435.860485" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="c123" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[10] Casalino等人，<em class="ng">智能数据分析的非负矩阵分解</em>，非负矩阵分解技术。Springer，第49–74页，2016 ( <a class="ae mi" href="https://www.researchgate.net/publication/282942631_Non_Negative_Matrix_Factorizations_for_Intelligent_Data_Analysis" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="f6b6" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[11] Grootendorst，<em class="ng"> BERTopic:基于类的TF-IDF过程的神经主题建模</em>，2022 ( <a class="ae mi" href="https://arxiv.org/abs/2203.05794" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="ee5a" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[12]安杰洛夫，<em class="ng"> Top2Vec:主题的分布式表示</em>，2020 ( <a class="ae mi" href="https://arxiv.org/abs/2008.09470" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="d10b" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[13]麦金尼斯等，<em class="ng"> UMAP:降维的一致流形逼近与投影</em>，2018 ( <a class="ae mi" href="https://arxiv.org/abs/1802.03426" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="0a48" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[14] Allaoui等人，<em class="ng">使用umap降维技术显著改进聚类算法:比较研究</em>，图像和信号处理国际会议，斯普林格，第317–325页，2020 ( <a class="ae mi" href="https://link.springer.com/chapter/10.1007/978-3-030-51935-3_34" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="aea5" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[15]麦金尼斯等，<em class="ng"> hdbscan:基于层次密度的聚类</em>，开源软件杂志，2(11):205，2017 ( <a class="ae mi" href="https://www.researchgate.net/publication/315508524_hdbscan_Hierarchical_density_based_clustering" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="fdab" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[16] Egger等人，<em class="ng">LDA、NMF、Top2Vec和BERTopic之间的一个主题建模比较，以揭开Twitter帖子的神秘面纱</em>，Frontiers in Sociology，Volume 7，Article 886498，2022 ( <a class="ae mi" href="https://www.frontiersin.org/articles/10.3389/fsoc.2022.886498/full" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="8549" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[17] Bouma，<em class="ng">词语搭配抽取中的归一化(逐点)互信息</em>，GSCL学报，2009年30:31–40(<a class="ae mi" href="https://www.researchgate.net/publication/267306132_Normalized_Pointwise_Mutual_Information_in_Collocation_Extraction" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="f7ad" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[18] Lau等，<em class="ng">机器阅读茶叶:自动评估话题连贯性和话题模型质量</em>，计算语言学协会欧洲分会第14届会议论文集，第530–539页，2014 ( <a class="ae mi" href="https://aclanthology.org/E14-1056/" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="3b6c" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[19]roder等人，<em class="ng">探索主题一致性测量的空间</em>，第八届ACM网络搜索和数据挖掘国际会议论文集，第399-408页。ACM，2015 ( <a class="ae mi" href="https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="5e18" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[20] Mimno等人，<em class="ng">优化主题模型中的语义一致性</em>，Proc .Conf的。论自然语言处理中的经验方法，第262–272页，2011 ( <a class="ae mi" href="http://dirichlet.net/pdf/mimno11optimizing.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="13a7" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[21] Y .左等，<em class="ng">词网主题模型:一种简单但通用的解决短小不平衡文本的方法</em>，《知识与信息系统》，48(2)，第379–398页(<a class="ae mi" href="https://link.springer.com/article/10.1007/s10115-015-0882-z#Sec10" rel="noopener ugc nofollow" target="_blank">链接</a>)</p><p id="3c4d" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[22] Blair等人，<em class="ng">增加社交媒体话题连贯性的聚合话题模型</em>，Applied Intelligence，50(1)，第138–156页，2020 ( <a class="ae mi" href="https://link.springer.com/article/10.1007/s10489-019-01438-z#Sec15" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="d223" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[23]杜根等人，<em class="ng">话题模型还是话题废话？重新评估语义可解释性测量</em>，计算语言学协会北美分会2021年会议记录:人类语言技术，第3824–3848页，2021年(<a class="ae mi" href="https://aclanthology.org/2021.naacl-main.300/" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="9541" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated">[24] Hoyle等，<em class="ng">自动化话题模型评估是否被打破？相干性的非相干性</em>，神经信息处理系统进展，34，2021 ( <a class="ae mi" href="https://arxiv.org/abs/2107.02173" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="e913" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><a class="ae mi" href="https://github.com/dice-group/Palmetto/issues/13" rel="noopener ugc nofollow" target="_blank">https://github.com/dice-group/Palmetto/issues/13</a></p><p id="cda1" class="pw-post-body-paragraph mo mp it ls b lt nb ju mq lv nc jx mr lx nd mt mu lz ne mw mx mb nf mz na md im bi translated"><strong class="ls iu">数据集许可</strong>:本文中的Python示例使用了<em class="ng"> 20个新闻组数据集</em>，这些新闻组数据集是由<a class="ae mi" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>包在<a class="ae mi" href="https://opensource.org/licenses/BSD-3-Clause" rel="noopener ugc nofollow" target="_blank"> BSD 3条款</a>许可下提供的。</p></div></div>    
</body>
</html>