<html>
<head>
<title>Using AI to Generate 3D Models, Fast!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用AI生成3D模型，快！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-ai-to-generate-3d-models-2634398c0799#2022-07-27">https://towardsdatascience.com/using-ai-to-generate-3d-models-2634398c0799#2022-07-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="7d50" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">AR/VR/3D</h2><div class=""/><div class=""><h2 id="aee8" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">如何使用NVIDIA的NeRF代码创建3D模型</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/b9a17a6059e5c320e955527889e1eabb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JG9ZJlqifvrgxS52"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@deepmind?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> DeepMind </a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="5f74" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">生成3D模型可能非常耗时，或者需要大量的参考图像。解决这个问题的一种方法是神经辐射场(NeRF)，这是一种生成图像的人工智能方法。NeRFs利用你对一个物体或场景拍摄的一小组2D图像，并使用它们(有效地)构建一个3D表示。这是通过学习在你已经拥有的图像之间转换来完成的。这种跳跃(或插值，这是一个稍微有点花哨的术语)让你产生物体的新的视角的图像！</p><p id="c807" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">听起来很棒，对吧？从一小组图像中，您可以制作一个3D模型！这比标准摄影测量有好处，标准摄影测量需要一个巨大的图像库来生成一些东西(你需要每个角度的镜头)。然而，我们在一开始就承诺NeRFs很快，直到最近，情况并非如此。以前，NeRFs花了很长时间来学习如何将你的一组图像转换成3D图像。</p><p id="57fc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在情况不再是这样了。Nvidia发布了其instant <a class="ae le" href="https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.pdf" rel="noopener ugc nofollow" target="_blank"> NeRF软件</a>，该软件利用GPU硬件来运行必要的复杂计算。这将创建模型所需的时间从几天减少到了几秒钟！NVIDIA对<code class="fe mb mc md me b">instant-ngp</code>软件的可用性和速度做了很多令人兴奋的声明。他们提供的结果和例子令人印象深刻:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/29723439d95531dba843cb73ed503e27.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/1*km445ArR7TbG7qv1q_xv7w.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">NVIDIA建造了一个很酷的机器人实验室。Gif由<a class="ae le" href="https://github.com/NVlabs/instant-ngp" rel="noopener ugc nofollow" target="_blank">https://github.com/NVlabs/instant-ngp</a></p></figure><p id="6bf5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我认为很难不对它印象深刻——它看起来棒极了！我想看看把它转移到我自己的图像上并生成我自己的NeRF模型有多容易。所以我决定自己安装使用这个软件。在这篇文章中，我将通过我的尝试，并详细介绍我所做的模型！我们走吧！</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="f0a6" class="mn mo iq bd mp mq mr ms mt mu mv mw mx kf my kg mz ki na kj nb kl nc km nd ne bi translated">管道</h1><p id="fc0c" class="pw-post-body-paragraph lf lg iq lh b li nf ka lk ll ng kd ln lo nh lq lr ls ni lu lv lw nj ly lz ma ij bi translated">那我们该怎么办？</p><ul class=""><li id="2977" class="nk nl iq lh b li lj ll lm lo nm ls nn lw no ma np nq nr ns bi translated">我们需要参考录像。我们去录些我们想3D化的东西吧！</li><li id="9407" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma np nq nr ns bi translated">我们将把它转换成静态图像。这个过程也试图理解我们记录的角度。</li><li id="8d69" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma np nq nr ns bi translated">我们将它传递给<code class="fe mb mc md me b">instant-ngp</code>。然后，这将训练人工智能理解我们生成的图像之间的空间。这实际上与制作3D模型是一样的。</li><li id="55a9" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma np nq nr ns bi translated">最后，我们要制作一个我们创作的视频！在NVIDIA制作的软件内部，我们将为摄像机绘制一条路径，以便在模型周围拍摄，然后渲染视频。</li></ul><p id="76d0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我不会深入了解这一切是如何工作的(尽管可以随意提问！)，但我会放上很多我觉得有用的资源的链接。相反，我会专注于我制作的视频，以及旅途中偶然发现的小部分。</p><h1 id="2bc8" class="mn mo iq bd mp mq ny ms mt mu nz mw mx kf oa kg mz ki ob kj nb kl oc km nd ne bi translated">我的尝试(又名邋遢的Nerf Herding)</h1><p id="332b" class="pw-post-body-paragraph lf lg iq lh b li nf ka lk ll ng kd ln lo nh lq lr ls ni lu lv lw nj ly lz ma ij bi translated">我不会说谎，我发现这很难安装。虽然说明很清楚，但我觉得当涉及到您需要的特定软件版本时，没有回购协议的要求部分所暗示的那么大的回旋余地。使用<code class="fe mb mc md me b">CUDA 11.7</code>或VS2022对我来说似乎是不可能的，但我认为是切换回<code class="fe mb mc md me b">11.6</code>和VS2019帮助安装过程向前推进。我有很多错误，比如:<code class="fe mb mc md me b">CUDA_ARCHITECTURES is empty for target.</code>这是由于CUDA不想很好地使用Visual Studio造成的。我真的推荐<a class="ae le" href="https://github.com/bycloudai/instant-ngp-Windows" rel="noopener ugc nofollow" target="_blank">这个</a> <a class="ae le" href="https://www.youtube.com/watch?v=kq9xlvz73Rg" rel="noopener ugc nofollow" target="_blank">视频</a>和这个<a class="ae le" href="https://github.com/bycloudai/instant-ngp-Windows" rel="noopener ugc nofollow" target="_blank">回购</a>进一步帮助设置一切！</p><p id="f453" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">除此之外，这个过程很顺利。Python脚本有助于指导将您拍摄的视频转换为图像，以及随后将其转换为模型和视频的步骤。</p><h2 id="97e8" class="od mo iq bd mp oe of dn mt og oh dp mx lo oi oj mz ls ok ol nb lw om on nd iw bi translated"><strong class="ak">尝试1 </strong>:乐高汽车</h2><p id="15c0" class="pw-post-body-paragraph lf lg iq lh b li nf ka lk ll ng kd ln lo nh lq lr ls ni lu lv lw nj ly lz ma ij bi translated">最初，我试图在办公室里改装一辆小型乐高汽车。我认为我的摄影技术有所欠缺，因为我根本无法创作出任何有意义的东西。只是一个奇怪的3D污迹。为什么呢？好吧，让我们看看英伟达给我们提供的一个例子。注意摄像机的位置:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oo"><img src="../Images/98fa9c040b96ff01dd9c256b44068cec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_hNjCnWojDnE8gpHKGCdow.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">NVIDIA提供的默认NeRF的“摄像机”位置，挖掘机。照片由<a class="ae le" href="https://github.com/NVlabs/instant-ngp" rel="noopener ugc nofollow" target="_blank">https://github.com/NVlabs/instant-ngp</a>拍摄</p></figure><p id="67b5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">训练良好的设置的一个特点是像上面那样放置“摄像机”。这些摄像头是软件认为你拍摄视频时面对的角度。它应该是一个漂亮的圆圈。我的第一辆乐高汽车完全不是这样的，而是一个压扁的半圆。</p><h2 id="8af1" class="od mo iq bd mp oe of dn mt og oh dp mx lo oi oj mz ls ok ol nb lw om on nd iw bi translated"><strong class="ak">尝试二:乐高车(不过这次更大！)</strong></h2><p id="8d80" class="pw-post-body-paragraph lf lg iq lh b li nf ka lk ll ng kd ln lo nh lq lr ls ni lu lv lw nj ly lz ma ij bi translated">试图从我的第一次尝试中吸取教训，我找到了一张可以完全走动的桌子，并找到了一辆更大的乐高汽车。我试着确保我比以前拍得更久。最后，我从各个角度拍摄了1分钟的流畅镜头。我训练模型不到30秒。经过4个小时720p的渲染，这是我做的视频:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi op"><img src="../Images/9ca674ebf61e416496404f21d83f2394.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*-BGzgX3gecFGBeCaYdsVYQ.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">我的第二辆NeRF——乐高技术车！</p></figure><h2 id="9b6a" class="od mo iq bd mp oe of dn mt og oh dp mx lo oi oj mz ls ok ol nb lw om on nd iw bi translated"><strong class="ak">尝试三:植物</strong></h2><p id="4814" class="pw-post-body-paragraph lf lg iq lh b li nf ka lk ll ng kd ln lo nh lq lr ls ni lu lv lw nj ly lz ma ij bi translated">好吧，所以第二次尝试更好，至少技术上可行。然而，有一个奇怪的雾，它不是超级尖锐的。对于我的下一次尝试，我也试图从更远的后面拍摄(我假设雾是由AI对那里的东西“困惑”引起的)。我试着更多地控制<code class="fe mb mc md me b">aabc_scale</code>(一种测量场景大小的方法)，然后训练了几分钟。渲染后，视频看起来是这样的:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi op"><img src="../Images/cae7c0f6d2f29c6078a92eb4a8da9795.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*rD7x-hzeaMLen2Sx2pGWag.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">我在客厅的桌子上用一株植物做了一个NeRF。</p></figure><p id="bd78" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">好多了！这里令人难以置信的是，它是如何设法精确地呈现钩针花盆和木材上的凹槽，以及错综复杂的树叶。看看相机在树叶间的小俯冲！</p><p id="cd60" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">尝试四</strong>:所以，我们越来越好了！不过，我想要一个外面的——让我们试试吧。我在公寓外拍摄了不到两分钟的镜头，并开始处理。这是特别厚的渲染/训练。我的猜测是，我的值<code class="fe mb mc md me b">aabc_scale</code>相当高(8)，因此渲染“光线”必须走得相当远(也就是说，我想要渲染的东西更多)。我不得不切换到480p，并将渲染fps从30降低到10。这表明你选择的设置确实会影响你的渲染时间。经过8个小时的渲染，我做出了这个:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi op"><img src="../Images/68f7420f2c63d0c72e98e24b8b507893.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*f43nyKHd1nKM8FQBYniakQ.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">我在公寓外面做的一个NeRF。</p></figure><p id="1344" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我认为尝试3仍然是我的最爱。我想我可以把第四次尝试做得更好。然而，当渲染时间如此之长时，迭代版本和尝试不同的渲染和训练设置是很困难的。现在甚至为渲染设置摄像机角度都很困难，导致程序对我来说变得迟钝。</p><p id="eb9d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">尽管如此，多么惊人的输出，从短短一两分钟的视频，我有一个详细的，栩栩如生的3D模型！</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="7cf8" class="mn mo iq bd mp mq mr ms mt mu mv mw mx kf my kg mz ki na kj nb kl nc km nd ne bi translated">利弊</h1><p id="a75e" class="pw-post-body-paragraph lf lg iq lh b li nf ka lk ll ng kd ln lo nh lq lr ls ni lu lv lw nj ly lz ma ij bi translated">我认为最令人印象深刻的是，给定1-2分钟的镜头，一个完全没有受过摄影测量(me)训练的人可以创建一个可行的3D模型。这需要一些技术知识，但是一旦你安装好了所有的东西，它就会变得非常光滑和简单。将视频转换成图像效果很好，有Python脚本可以做到这一点。一旦这些都做好了，输入到人工智能就顺利了。</p><p id="dfbf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然而，这很难指责NVIDIA，但我觉得我应该提出来:这需要一个相当强大的GPU。我的笔记本电脑里有一台T500，它将它推到了绝对的极限。训练时间确实比宣传的5秒长，尝试在1080渲染会导致程序崩溃(我是在135*74附近动态渲染下来的)。现在，这仍然是一个巨大的进步，以前的NeRF实现需要几天时间。我不认为每个人都有3090用于这样的项目，所以有必要简单提一下。低性能使它很难使用程序，特别是当我试图“飞”着相机来设置我的渲染视频时。尽管如此，还是很难不对这个过程的输出留下深刻的印象。</p><p id="04a9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我面临的另一个问题是寻找<code class="fe mb mc md me b">render.py</code>(正如你可能猜到的，这对于渲染视频至关重要)。非常奇怪的是，它不在回购协议中，尽管在大多数广告文章和其他文件中被大量提及。我不得不把它从<a class="ae le" href="https://github.com/bycloudai/instant-ngp-Windows" rel="noopener ugc nofollow" target="_blank">这里</a>挖出来。</p><p id="2a66" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最后，我也希望能够提取这些作为一个<code class="fe mb mc md me b">.obj</code>——也许这已经是可能的了？</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/54b90a25a1f2622f229701c038f610ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/1*Jl4LyfVgQTFU1YIVoG106A.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">一张狐狸的gif图——这不是我做的，是英伟达做的。干得好，嗯？https://github.com/NVlabs/instant-ngp<a class="ae le" href="https://github.com/NVlabs/instant-ngp" rel="noopener ugc nofollow" target="_blank">gif</a></p></figure></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h2 id="2640" class="od mo iq bd mp oe of dn mt og oh dp mx lo oi oj mz ls ok ol nb lw om on nd iw bi translated">最后的想法和下一步</h2><p id="d15d" class="pw-post-body-paragraph lf lg iq lh b li nf ka lk ll ng kd ln lo nh lq lr ls ni lu lv lw nj ly lz ma ij bi translated">这让我想到的是<a class="ae le" href="https://huggingface.co/spaces/dalle-mini/dalle-mini" rel="noopener ugc nofollow" target="_blank"> DALL-E </a>，生成图像的AI。这已经变得难以置信的流行，部分是因为它是如此容易获得。它给了人们一个非常酷的例子，展示了人工智能模型可以做什么，以及它们的局限性。它已经进入了流行文化(或者至少在我的Twitter上占据了重要位置)，人们制作自己的怪异DALL-E图像并分享它们。我可以想象这种技术也会发生类似的事情。一个可以让任何人上传视频并创建一个你可以与朋友分享的3D模型的网站的病毒潜力是巨大的。最终有人做出这个几乎是必然的。</p><p id="efaa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">就我个人而言，我期待着更多的尝试。我希望能够生成超级逼真的模型，然后将其转储到AR/VR中。从那里你可以在那个空间主持会议——那不是很有趣吗？因为你只需要手机上的摄像头，所以大部分需要的硬件已经在用户手中了。</p><p id="58a7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">总的来说，我真的很感动。能够在手机上拍摄1分钟的视频，并将其转化为一个你可以一步一步走过的模型，这太棒了。是的，渲染需要一段时间，安装有点困难，但是效果很好。经过几次尝试，我已经得到了非常酷的输出！我期待更多的尝试！</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="2400" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">安德鲁·布朗斯</p><p id="7f05" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">请在Medium上订阅我，以跟踪任何令人兴奋的未来文章！也可以在这里联系:<a class="ae le" href="https://www.linkedin.com/in/andrew-blance/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a> | <a class="ae le" href="https://twitter.com/andrewblance" rel="noopener ugc nofollow" target="_blank"> Twitter </a></p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="699b" class="mn mo iq bd mp mq mr ms mt mu mv mw mx kf my kg mz ki na kj nb kl nc km nd ne bi translated">参考</h1><ul class=""><li id="4e2e" class="nk nl iq lh b li nf ll ng lo os ls ot lw ou ma np nq nr ns bi translated">英伟达Git:<a class="ae le" href="https://github.com/NVlabs/instant-ngp" rel="noopener ugc nofollow" target="_blank">https://github.com/NVlabs/instant-ngp</a></li><li id="4d91" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma np nq nr ns bi translated">NVIDIA博客:<a class="ae le" href="https://developer.nvidia.com/blog/getting-started-with-nvidia-instant-nerfs/" rel="noopener ugc nofollow" target="_blank">https://developer . NVIDIA . com/blog/getting-started-with-NVIDIA-instant-nerfs/</a></li><li id="6c23" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma np nq nr ns bi translated">补充Git:<a class="ae le" href="https://github.com/bycloudai/instant-ngp-Windows" rel="noopener ugc nofollow" target="_blank">https://github.com/bycloudai/instant-ngp-Windows</a></li></ul></div></div>    
</body>
</html>