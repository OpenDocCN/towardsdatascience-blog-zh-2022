<html>
<head>
<title>Feature Selection for Data Science: Simple Approaches</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学的特征选择:简单方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-for-data-science-simple-approaches-e1f2527cb363#2022-06-08">https://towardsdatascience.com/feature-selection-for-data-science-simple-approaches-e1f2527cb363#2022-06-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7921" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过去除与数据集无关的特征，我们可以创建更好的预测模型。</h2></div><p id="bbeb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每一个数据科学家都曾经或者将要遇到这个问题；一个庞大的数据集，有如此多的功能，他们甚至不知道从哪里开始。虽然有许多高级方法可用于为数据集选择最佳要素集合，但有时简单的方法可以为您的分析提供很好的基线，甚至是选择数据集最佳要素所需的唯一必要方法。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/f832dee6d0e79820fbd9f4b8f73ae22f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2_g4Wg7oUBlWLmqS"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">照片由<a class="ae lu" href="https://unsplash.com/@uxindo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">UX</a>在<a class="ae lu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="1f05" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">特征选择:为什么重要？</h1><p id="bd97" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">要素选择非常重要，因为包含太多要素的数据集可能会导致高计算成本以及使用冗余或不重要的属性进行模型预测。通过去除与我们的数据集无关的特征，我们可以创建一个建立在强大基础上的更好的预测模型。虽然有使用机器学习算法进行特征选择的高级方法，但今天我想探索两种简单的方法，它们可以帮助引导任何分析的方向。</p><h1 id="29a6" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">资料组</h1><p id="f9b3" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">我们今天将使用的数据集是来自Kaggle.com的<a class="ae lu" href="https://www.kaggle.com/datasets/devansodariya/student-performance-data?resource=download" rel="noopener ugc nofollow" target="_blank">学生表现数据集。该数据集中有33个要素，它们是:</a></p><p id="6d1e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="ms">学校，性别，年龄，地址，家庭大小，p状态，Medu，Fedu，Mjob，Fjob，原因，监护人，旅行时间，学习时间，失败，schoolsup，famsup，付费，活动，托儿所，高等，互联网，浪漫，famrel，自由时间，goout，Dalc，Walc，健康，缺勤，G1，G2，G3 </em></p><p id="7cef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你还没数过，这个数据集有33个不同的特征！让我们采取一些简单的方法来挑选出最适合我们的！</p><h2 id="b55a" class="mt lw it bd lx mu mv dn mb mw mx dp mf kr my mz mh kv na nb mj kz nc nd ml ne bi translated">预处理</h2><p id="bc80" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">在开始之前，我们需要对数据集做一些预处理。首先，让我们看一下数据集。</p><pre class="lf lg lh li gt nf ng nh ni aw nj bi"><span id="7617" class="mt lw it ng b gy nk nl l nm nn">import pandas as pd <br/>df = pd.read_csv('student_data.csv')<br/>df.head()</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi no"><img src="../Images/ccf3a482a0040f88fc3900519e5b3b00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iGzBU1JoXAP1rVkKaXKRgQ.png"/></div></div></figure><p id="9c7e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从视觉上看，似乎有一些列在本质上是分类的。在转换这些列之前，我们还想检查是否有任何丢失的值。</p><pre class="lf lg lh li gt nf ng nh ni aw nj bi"><span id="9873" class="mt lw it ng b gy nk nl l nm nn">#Checkign for null values -- False means no null values<br/>df.isnull().values.any()</span></pre><p id="73ef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">运行此代码返回“False ”,因此不需要填写空值。</p><pre class="lf lg lh li gt nf ng nh ni aw nj bi"><span id="20c2" class="mt lw it ng b gy nk nl l nm nn">df.dtypes</span></pre><p id="9f62" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">键入上面的代码显示了每一列的所有类型，这表明实际上有分类列需要转换成数字。我创建了一个简单的映射函数，可以很快地帮你做到这一点。</p><pre class="lf lg lh li gt nf ng nh ni aw nj bi"><span id="bfbd" class="mt lw it ng b gy nk nl l nm nn">#Function that converts the categorical to numerical through a dictionary. <br/>def mapper(df_column):<br/>    map_dict = {}<br/>    <br/>    for i, x in enumerate(df_column.unique()):<br/>        map_dict[x] = i <br/>    <br/>    print(map_dict) #Print this out to see <br/>    df_column.replace(map_dict, inplace = True)<br/>    <br/>    return df_column</span></pre><p id="d89d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用这个函数，我们可以将所有的对象列都改为数字。</p><pre class="lf lg lh li gt nf ng nh ni aw nj bi"><span id="ac5e" class="mt lw it ng b gy nk nl l nm nn">def categorical_converter(df):<br/>    for i in df.select_dtypes(include=['object']):<br/>        mapper(df[i])<br/>    <br/>    return df</span><span id="b66e" class="mt lw it ng b gy np nl l nm nn">categorical_converter(df)</span></pre><h1 id="493b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">方法1:评估特性的相关性</h1><p id="62c0" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">如果两个或多个特征高度相关，这可能意味着它们都以相同的方式解释因变量。这引出了从模型中移除这些特征之一的理由。如果您不确定要删除哪个特性，您可以考虑构建两个模型，每个模型包含一个特性。要获得相关矩阵，只需调用<em class="ms">。数据框上的corr </em></p><pre class="lf lg lh li gt nf ng nh ni aw nj bi"><span id="0dbe" class="mt lw it ng b gy nk nl l nm nn">df.corr()</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nq"><img src="../Images/6b418de34b8b14cbff3f1fd2adba1655.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c_TKlSZxyX0-0V1yLJFwGQ.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">Img:相关矩阵的一部分</p></figure><p id="7435" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">显然，有了这么多的特征，相关矩阵将是巨大的。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/e473f3ffcd4d318ed0041e8ad6a8fa5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*x1oND7lZLSSyIOSvppOvtw.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">G1、G2和G3的相关性</p></figure><p id="3aec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要素<em class="ms"> G1、G2和G3 </em>具有高度相关性，因此我们希望从数据集中移除其中两个要素。我去掉了<em class="ms"> G2 </em>和<em class="ms"> G3。</em>此外，我将我的<em class="ms"> X </em>数据设置为等于除了<em class="ms"> G1 </em>之外的所有列，这些列成为我的Y数据。</p><pre class="lf lg lh li gt nf ng nh ni aw nj bi"><span id="b000" class="mt lw it ng b gy nk nl l nm nn">df.drop(['G2','G3'], axis=1, inplace=True)<br/>X = df.drop(['G1'], axis=1)<br/>Y = df['G1']</span></pre><h1 id="bb19" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">方法2:移除低方差特征</strong></h1><p id="ebfb" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">任何具有低方差的要素都应被视为从数据集中移除。为什么？首先，我们来思考一下。如果我试图比较谁是更好的学生，并且两个学生每个月一起上完全相同的课，那么学生之间的“班级”特征几乎没有差异，这将归因于为什么一个学生比另一个学生表现得更好。当一个特征变得接近恒定时，我们可以保持该特征恒定并将其从数据集中移除(一如既往，这取决于！).为此，我们将使用Scikit学习特征选择库中的VarianceThreshold()。</p><pre class="lf lg lh li gt nf ng nh ni aw nj bi"><span id="1063" class="mt lw it ng b gy nk nl l nm nn">from sklearn.feature_selection import VarianceThreshold</span></pre><p id="92ba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面的代码将创建一个方差阈值对象，该对象将转换我们的X数据并返回给定的最佳特性。</p><pre class="lf lg lh li gt nf ng nh ni aw nj bi"><span id="fab7" class="mt lw it ng b gy nk nl l nm nn">vthresh = VarianceThreshold(threshold=.05)</span><span id="29b1" class="mt lw it ng b gy np nl l nm nn">selector = vthresh.fit(X)</span><span id="c1a4" class="mt lw it ng b gy np nl l nm nn">selected = vthresh.get_support()</span><span id="00c2" class="mt lw it ng b gy np nl l nm nn">X = X.loc[:, selected]</span></pre><p id="d6cb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">运行此代码将返回一个包含9个要素的X数据框！</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ns"><img src="../Images/74d40ba9b9a3d2da609efa44231ac52f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tW8ULK40EB9piFHWVo-uAg.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">具有选定特征的x数据</p></figure><p id="9242" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">选定的功能包括:</p><p id="f13d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="ms">年龄，Medu(母亲教育)，Fedu(父亲教育)，Mjob(母亲工作)，reason，goout，Walc，heath，</em>和<em class="ms">缺勤。</em></p><p id="2242" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然这些可能不是我们最终得出的结论，但这让我们对影响学生成绩的因素有了初步的了解。我们仅仅通过相关性和方差就能做到这一点！这两个简单的初步方法允许将数据集从33个特征减少到10个特征(不能忘记Y！)现在我们可以开始用较小的数据集构建模型了！</p><p id="a551" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你喜欢今天的阅读，请关注我，让我知道你是否还有其他话题想让我探讨(这对我的帮助超乎你的想象)！另外，在<a class="ae lu" href="https://www.linkedin.com/in/benjamin-mccloskey-169975a8/" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">LinkedIn</strong></a><strong class="kk iu">上加我，或者随时联系！感谢阅读！</strong></p><h1 id="e20a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">引文</h1><p id="4b9c" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated"><a class="ae lu" href="https://www.kaggle.com/datasets/devansodariya/student-performance-data?resource=download" rel="noopener ugc nofollow" target="_blank">学生成绩数据集</a> - <a class="ae lu" href="https://creativecommons.org/publicdomain/zero/1.0/" rel="noopener ugc nofollow" target="_blank"> CC0:公共领域</a>，经批准公开使用，作者放弃所有权利，无版权限制。</p></div></div>    
</body>
</html>