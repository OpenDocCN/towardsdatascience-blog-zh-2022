<html>
<head>
<title>Systematic Way to Extract Features From Image Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从图像数据中提取特征的系统方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-engineering-for-machine-learning-with-picture-data-d7ff8554920#2022-06-06">https://towardsdatascience.com/feature-engineering-for-machine-learning-with-picture-data-d7ff8554920#2022-06-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1d32" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对于机器学习算法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8b522004e15b66df587011241fb4ca1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DvDXlLAMH0YIjoGcH56_Ow.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">解释维数灾难的插图(作者图片)</p></figure><p id="c12e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">特征工程是获取原始数据并提取对建模有用的特征的过程。对于图像，这通常意味着提取颜色、纹理和形状等信息。进行特征工程有多种方法，您采用的方法将取决于您正在处理的数据类型和您试图解决的问题。</p><p id="6ed6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是我们为什么需要它的图片呢？</p><p id="a123" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">图像封装了大量信息，但这是有代价的:高维数。例如，具有3个通道(红、绿、蓝)的尺寸为200×100像素的小图片已经表示了60000维的特征向量。</p><p id="ffc7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这需要处理很多事情，而且我们正面临着维数灾难。我们稍后会对此进行更多的讨论。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/87aa113f7629de9c0b96c41ced4f6877.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QWXlFZulVMIWVxHQ3we3OA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片的维数示例(图片由作者提供)</p></figure><p id="bdf9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇文章中，我们将看看如何降低图片的维度来对抗维度的诅咒。</p><h1 id="b5dc" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">维度的诅咒</h1><p id="4097" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">“维数灾难”是一个术语，用来描述在处理具有大量维度的数据集时出现的问题。</p><p id="9813" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一个原因是，准确了解数据的基本分布所需的数据点的数量随着维度的数量呈指数增长。换句话说，如果我们有一个包含100个特征的数据集，我们可能需要大约1000甚至10000个数据点才能有很好的机会准确学习。</p><p id="c3b5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了更好地理解维数灾难，我喜欢举一个二维k近邻图的例子，如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/5c6a6b0778cb3330a4a55001725ac8d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5terv0nKmguIBxM78R6jLg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">k近邻图可视化(图片由作者提供)</p></figure><p id="8e38" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是如果所有的点都是等距的，会发生什么呢？这是一个非常不可能的场景，但它(在某种程度上)显示了当我们试图计算具有许多维度的特征之间的欧几里德距离时通常会发生什么。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/649473e18ae8243bedc33287b8453601.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sfMuuRnPGY-xdFptz6Tpqg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等距点的最近邻问题(图片由作者提供)</p></figure><p id="a7bd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了理解维度的数量如何影响我们的距离度量，我们可以看一下内接在超立方体中的n维球(n球)内部和外部的体积之比。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/cf8085f72e42dcf315cf2210b7d8028f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1oP4nu7SViAclL2vXXfl8A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">超立方体中n球内外的体积(图片由作者提供)</p></figure><p id="edc9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以在此图中观察到，n球外部的体积将占据大部分空间。这是非常违反直觉的，不像我们习惯掌握的二维或三维空间。</p><blockquote class="mv"><p id="b0b8" class="mw mx it bd my mz na nb nc nd ne lt dk translated">如果我们在超立方体中均匀分布点，它们中的任何一个都不太可能落入内接的n球中。</p></blockquote><p id="d11d" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">这将给人这样的印象，即超立方体中的所有点将离n球的中心非常远，并且看起来是等距离的，并且集中在超立方体的长尖角上。这将导致我们的基于距离的算法容易受到输入特征上的噪声的影响。</p><p id="00fb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那么，如何才能解决呢？</p><h1 id="f48e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">缩小图像尺寸</h1><p id="b89b" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">降低特征向量维数的一个简单方法是通过降低图像的分辨率，利用抽取(下采样)来减小图像的大小。</p><p id="3b06" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果颜色分量不相关，我们也可以将图片转换为灰度，以将数字维度除以三。但是还有其他方法可以减少图片的尺寸，并潜在地提取特征。例如，我们可以使用小波分解。</p><p id="bb45" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">小波分解是一种在空间和频率上分解信号的方法。就图片而言，这意味着将图像分解成水平、垂直和对角线分量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/c16f371f52f92576d77c95e7efc4024c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5VUJnWrxQrLW1m5m8q0lWw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">小波分解示例(图片由作者提供)</p></figure><h1 id="a98e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">方向梯度直方图</h1><p id="90c0" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">HOG特征描述符是在计算机视觉和图像处理中用于检测数字图像中的对象的流行技术。在Dalal和Triggs于2005年展示了这种描述符的效率之后，HOG描述符变得流行起来，该描述符专注于静态图像中的行人检测。</p><p id="3572" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">HOG描述符是一种特征描述符，它通过计算图像中亮度梯度的分布来编码对象的形状和外观。</p><p id="ce5b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们的情况下，最重要的参数是每个单元的像素数，因为它将为我们提供一种方法来找到维度数量和图片中捕捉的细节数量之间的最佳平衡。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/905807446bf08e5fd3988731e9402a01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eudOCcZcr5bzr9eN8ReGSA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">8×8像素单元的方向渐变直方图示例(图片由作者提供)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/6254ee92c19286892aa9c1b9d1b10f0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qp2tBjLzMaMtevgrNI_khw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">16乘16像素单元的方向渐变直方图示例(图片由作者提供)</p></figure><p id="4f4f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于上面的示例，输入图像具有20，000个维度(100×200像素), HOG特征对于8×8像素单元具有2，400个维度，对于16×16像素单元具有576个维度。分别减少了88%和97%。</p><h1 id="77ce" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">主成分分析</h1><p id="88d0" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">我们还可以使用主成分分析(PCA)来降低特征向量的维数。PCA是一种统计技术，可用于寻找最大化方差和最小化数据集中投影误差的方向(分量)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/200926c03d258b7b88099f5b5757b9f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OeBKuDvl6FXRCkVXWiqjoA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">方差最大的轴(绿色)和投影误差较小的轴(红色)(图片由作者提供)</p></figure><p id="0825" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">换句话说，主成分分析可用于寻找代表数据中最多信息的方向。</p><p id="25c4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用PCA时，需要记住以下几点:</p><ul class=""><li id="da4b" class="no np it la b lb lc le lf lh nq ll nr lp ns lt nt nu nv nw bi translated">PCA最好是作为降维的工具，而不是用于特征选择。</li><li id="df97" class="no np it la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">使用PCA进行降维时，首先对数据进行归一化是很重要的。</li><li id="1aa7" class="no np it la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">PCA是线性变换，因此它不能捕捉数据中的非线性关系。</li><li id="5b04" class="no np it la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">为了减少到N维，你至少需要N-1次观察</li></ul><p id="8dad" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我建议阅读以下文章，以便更好地理解PCA及其局限性:</p><div class="oc od gp gr oe of"><a rel="noopener follow" target="_blank" href="/an-intuitive-guide-to-pca-1174055fc800"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">PCA直观指南</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">主成分分析背后的思想</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">towardsdatascience.com</p></div></div><div class="oo l"><div class="op l oq or os oo ot ks of"/></div></div></a></div><h1 id="aace" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">流形学习</h1><p id="110f" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">流形学习在某些方面是线性方法(如PCA)的扩展，用于降低维数，但用于数据中的非线性结构。</p><p id="fa44" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">流形是局部欧几里得的拓扑空间，这意味着在每个点附近它都类似于欧几里得空间。流形自然地出现在数学和物理的许多领域，流形的研究是微分几何的一个中心课题。</p><p id="a3ed" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用多方面学习时，有几件事要记住:</p><ul class=""><li id="e1f2" class="no np it la b lb lc le lf lh nq ll nr lp ns lt nt nu nv nw bi translated">流形学习是降维的有力工具。</li><li id="0e52" class="no np it la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">它可以用来发现数据中隐藏的模式。</li><li id="0ce0" class="no np it la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">流形学习通常是一项计算密集型任务，因此在使用这些算法之前对它们有一个很好的理解是很重要的。</li></ul><p id="0c71" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现实生活中的流程很少使用其所有维度来描述其底层结构。例如下面的图片，只需要几个维度来描述杯子的位置和旋转。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/e7edd2fd3cbdd6138712c28d44854dbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Yu_1yrtBR6DWNkx-.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有不同位置和旋转的杯子数据集(图片由作者提供)</p></figure><p id="f187" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种情况下，一旦使用流形学习算法(如t-分布式随机邻居嵌入(t-SNE ))进行投影，只有两个维度能够编码杯子的位置和旋转。</p><p id="ddfe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">编码位置:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/d24c4d3a77333e49542213989c97def7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QD3wyiVZD_j6JnNi.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">30-cup数据集的最近邻图(图片由作者提供)</p></figure><p id="4a1d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">编码旋转:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/957a8eebf3c343eba5cab7c96f2166ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GbW50fifqs-4geM8.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第一个位置的编码旋转(图片由作者提供)</p></figure><h1 id="3c49" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="2e53" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">在这篇文章中，我们看到了如何使用图片的特征工程来对抗维数灾难。我们已经了解了如何通过抽取、小波分解、HOG描述符和PCA来降低图片的维度。我们还看到了如何使用流形学习来发现数据中隐藏的模式。</p><p id="84ea" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">还有许多其他方法可以减少图片的尺寸，您采用的方法将取决于您正在处理的数据类型和您试图解决的问题。</p><p id="129b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">特征工程是一个迭代的过程，它有助于对不同的可能性和可用的方法有一个总体的了解。</p><p id="20c5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望这篇文章对你有帮助！如果您对如何构建k-NN图有任何疑问，我建议您阅读以下文章:</p><div class="oc od gp gr oe of"><a rel="noopener follow" target="_blank" href="/how-to-explore-a-dataset-of-images-with-graph-theory-fd339c696d99"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">如何用图论探索图像数据集</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">组合特征提取、相似性度量和最近邻图</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">towardsdatascience.com</p></div></div><div class="oo l"><div class="ov l oq or os oo ot ks of"/></div></div></a></div></div><div class="ab cl ow ox hx oy" role="separator"><span class="oz bw bk pa pb pc"/><span class="oz bw bk pa pb pc"/><span class="oz bw bk pa pb"/></div><div class="im in io ip iq"><p id="a0b6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">好奇想了解更多关于Anthony的工作和项目吗？在<a class="ae pd" href="https://medium.com/@anthonycvn" rel="noopener">媒体</a>、<a class="ae pd" href="https://www.linkedin.com/in/anthonycavin/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae pd" href="https://twitter.com/Anthony66333223" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上关注他。</p><p id="acaf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="pe">需要技术作家？将您的请求发送到</em><a class="ae pd" href="https://amigocci.io/blog/mlops-at-medium-scale/" rel="noopener ugc nofollow" target="_blank"><em class="pe">https://amigo CCI . io</em></a><em class="pe">。</em></p></div></div>    
</body>
</html>