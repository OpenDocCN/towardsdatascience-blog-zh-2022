<html>
<head>
<title>Semantic Segmentation of Aerial Imagery Using U-Net in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中基于 U-Net 的航空影像语义分割</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/semantic-segmentation-of-aerial-imagery-using-u-net-in-python-552705238514#2022-03-31">https://towardsdatascience.com/semantic-segmentation-of-aerial-imagery-using-u-net-in-python-552705238514#2022-03-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3331" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 Python 中的 TensorFlow U-Net 模型对迪拜 MBRSC 航空影像进行语义分割</h2></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h2 id="b152" class="kp kq it bd kr ks kt dn ku kv kw dp kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">介绍</h2><p id="9c89" class="pw-post-body-paragraph ll lm it ln b lo lp ju lq lr ls jx lt ky lu lv lw lc lx ly lz lg ma mb mc md im bi translated"><strong class="ln iu"> <em class="me">图像分割</em> </strong> <em class="me">是在像素级对一幅图像进行分类的任务。</em></p><p id="4b26" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">每张数字图片都由<strong class="ln iu">个像素值</strong>组成，语义分割包括给每个像素加标签。</p><p id="c6e5" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">本文旨在演示如何使用<em class="me"> TensorFlow </em>中定义的<strong class="ln iu"> U-Net </strong>模型对<strong class="ln iu">航空影像进行语义分段。</strong></p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mk"><img src="../Images/e67104e035ec07084bece9a3cd7a49ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bJrZ0qcA8HeXC3CD"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">照片由<a class="ae na" href="https://unsplash.com/@zqlee?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">ZQ·李</a>在<a class="ae na" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h2 id="3c74" class="kp kq it bd kr ks kt dn ku kv kw dp kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">资料组</h2><p id="586c" class="pw-post-body-paragraph ll lm it ln b lo lp ju lq lr ls jx lt ky lu lv lw lc lx ly lz lg ma mb mc md im bi translated"><strong class="ln iu"> MBRSC </strong>数据集存在于 CC0 许可下，可供<a class="ae na" href="https://www.kaggle.com/humansintheloop/semantic-segmentation-of-aerial-imagery" rel="noopener ugc nofollow" target="_blank">下载</a>。它由<em class="me">迪拜</em>的航空影像组成，由<strong class="ln iu"> MBRSC 卫星</strong>获得，并在<em class="me"> 6 类</em>中标注了逐像素语义分割。有三个<em class="me"/><em class="me">主要挑战</em>与数据集相关联:</p><ol class=""><li id="d029" class="nb nc it ln b lo mf lr mg ky nd lc ne lg nf md ng nh ni nj bi translated"><em class="me">类颜色</em>为十六进制<strong class="ln iu">，而掩模图像为 RGB<strong class="ln iu"/>。</strong></li><li id="2ab2" class="nb nc it ln b lo nk lr nl ky nm lc nn lg no md ng nh ni nj bi translated">数据集的总容量为<em class="me"> 72 幅图像，分成六个较大的图块</em>。七十二幅图像是用于训练神经网络的相对<strong class="ln iu">小数据集</strong>。</li><li id="92d9" class="nb nc it ln b lo nk lr nl ky nm lc nn lg no md ng nh ni nj bi translated">每个图块都有不同高度和宽度的<strong class="ln iu">图像</strong>，并且同一图块中的一些图片大小可变。神经网络模型期望输入具有<em class="me">相等的空间维度</em>。</li></ol><p id="2de6" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">图 1 描绘了一个训练集<em class="me">输入图像</em>及其对应的<em class="me">掩码</em>，带有叠加的类注释。</p><div class="ml mm mn mo gt ab cb"><figure class="np mp nq nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><img src="../Images/b62dd374cae20514e151fec252035536.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*kUrAUjDQ0zr1cegMvUrUqg.jpeg"/></div></figure><figure class="np mp nv nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><img src="../Images/472c7ee9c881679d8744f60f62d69f2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*oypCvGATxpr4MJh-BM2TWw.png"/></div><p class="mw mx gj gh gi my mz bd b be z dk nw di nx ny translated">图 1-样本训练数据输入图像(左)和输出掩码(右)(图片由作者提供)</p></figure></div><p id="ba5e" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">表 1 给出了每个<em class="me">类名</em>，对应的<em class="me">十六进制颜色</em>代码，以及转换后的<em class="me"> RBG </em>值。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nz"><img src="../Images/3078b04bc740d3d10cfa4330088bef0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X0if3zq8fBrgr5YvpluaRw.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">表 1-类别名称和遮罩颜色(图片由作者提供)</p></figure></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h2 id="eed1" class="kp kq it bd kr ks kt dn ku kv kw dp kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">预处理</h2><p id="8ef9" class="pw-post-body-paragraph ll lm it ln b lo lp ju lq lr ls jx lt ky lu lv lw lc lx ly lz lg ma mb mc md im bi translated"><em class="me">图像在输入神经网络的输入层时，尺寸必须相同</em>。因此，在模型训练之前，图像被分解成<em class="me">个面片</em>。</p><p id="ac69" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">选择的<strong class="ln iu"> patch_size </strong>为<em class="me"> 160 px </em>。没有理想的补丁大小；它作为一个超参数，可以进行性能优化试验。</p><p id="f95a" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">从具有<em class="me"> 1817 </em>像素的<em class="me">宽度</em>和<em class="me"> 2061 </em>像素的<em class="me">高度</em>的<em class="me">拼贴 7 </em>中获取图像，表达式 1 说明了如何计算创建的拼贴元的数量。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi oa"><img src="../Images/a41ed9ef41050f7cc7a2e60fec1a79c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-IZ4UfwN9PPlLOREkouGXQ.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">表达式 1——碎片尺寸= 160 px 的平铺 7 图像的碎片数量的计算</p></figure><p id="49b4" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">接下来，图像被裁剪至最接近的尺寸，可被 patch_size 整除，以避免 patch 与区域重叠。表达式 2 确定瓦片 7 图像的新的修整宽度和高度。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi ob"><img src="../Images/12a3b4ae656d6d2eb2ea1df98942fa40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7l_rGIw7uyMRhINTFz_17A.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">表达式 2-计算图像的裁剪宽度和高度，并除以补丁大小</p></figure><p id="6f36" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">图 2 阐明了单幅图像的<strong class="ln iu">裁剪</strong>和<strong class="ln iu">修补</strong>过程。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi oc"><img src="../Images/5fdab4567b3f42aa0e2aedd9ed573ed6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2GE8U-Aqp6q4xCLZmdiAVw.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">图 2-修补和裁剪的说明(图片由作者提供)</p></figure><p id="75ba" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">表 2 给出了<em class="me">瓦片</em>；它们的<em class="me">尺寸</em>是使用尺寸<strong class="ln iu"> 160 px </strong>创建的<em class="me">补丁总数</em>。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nz"><img src="../Images/1f5507d09536d3b3039f27921c78b194.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VyazSVR5MtXOzkhvuywmRg.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">表 2 —瓷砖高度、宽度和补丁数量(图片由作者提供)</p></figure><p id="9094" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">在<code class="fe od oe of og b">cropping</code>和<code class="fe od oe of og b">patchifying</code>之后，<em class="me"> 3483 幅图像和掩模</em>组成了<em class="me">输入数据集。</em>图 3 给出了<em class="me">六个</em>随机选择的图像补片及其可比较的掩模。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi oh"><img src="../Images/1db3fc71cfae8fda697ddd12edfa6ae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i1y4pnW_7V6F6yf6PFoEag.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">图 3 —修补的图像(作者提供的图像)</p></figure><p id="68e0" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">使用从 Gist 1 到<em class="me">的 Python 代码从一个目录加载图像文件</em>并执行数据<strong class="ln iu">预处理</strong>，如上所述。</p><figure class="ml mm mn mo gt mp"><div class="bz fp l di"><div class="oi oj l"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">Gist 1 —从目录加载图像并返回裁剪的补丁的 Python 代码</p></figure><p id="5a2c" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">掩码是形状<code class="fe od oe of og b">(160, 160, 3).</code>轴 3 的<em class="me">张量</em>，或第三维，可解释为 8 位无符号整数<em class="me">的<em class="me"> NumPy </em>数组。</em>这些值的范围从 0 到 255，对应于表 2 中列出的<em class="me"> RGB </em>颜色。</p><p id="62bb" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated"><em class="me">多类分类</em>问题需要将<strong class="ln iu">输出编码为整数</strong>。即对于具有 RGB 值<code class="fe od oe of og b">(60, 16, 152)</code>的<code class="fe od oe of og b">Building</code>类，合适的标签是<code class="fe od oe of og b">0</code>。</p><p id="e35f" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">要将遮罩的第三维度转换为表示适当类标签的独热编码向量，请执行 Gist 2 中的 Python 代码。</p><figure class="ml mm mn mo gt mp"><div class="bz fp l di"><div class="oi oj l"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">要点 2 —对遮罩的轴 3 进行分类编码</p></figure><p id="82df" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">目前，<strong class="ln iu">输入图像</strong>是张量，可解释为 8 位无符号整数(十进制 0 到 255)的 NumPy 数组。因此，r <strong class="ln iu"> escaling </strong>值从 0–255 到 0–1<strong class="ln iu">T57】提高了性能和训练稳定性。</strong></p><p id="b9b4" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">Keras 提供了一个<code class="fe od oe of og b"><a class="ae na" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Rescaling" rel="noopener ugc nofollow" target="_blank">Rescaling</a></code> <strong class="ln iu">预处理层</strong>，它将输入值修改为一个新的范围。使用下面的 Python 代码定义该图层。</p><pre class="ml mm mn mo gt ok og ol om aw on bi"><span id="ab98" class="kp kq it og b gy oo op l oq or">rescaling_layer = layers.experimental.preprocessing.<strong class="og iu">Rescaling</strong>(<br/>    <strong class="og iu">scale</strong>=1. / 255,<br/>    <strong class="og iu">input_shape</strong>=(img_height, img_width, 3)<br/>)</span></pre><p id="be7e" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">每个输入图像值乘以<code class="fe od oe of og b">scale</code>，如图 4 所示。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi os"><img src="../Images/0f5ead14b457fc3df117fbff6b8234a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Mj6WNoUvZgjKVZr9kt9sg.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">图 4-缩放 RBG 通道(图片由作者提供)</p></figure><p id="afeb" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">一般分为三个部分，<em class="me">培训、验证和测试</em>。然而，与其他机器学习计算机视觉数据集相比，该数据集相对较小。因此，只需要一次分割，并且<strong class="ln iu">推理</strong>发生在<code class="fe od oe of og b">X_test</code>数据集上。</p><ul class=""><li id="ad8e" class="nb nc it ln b lo mf lr mg ky nd lc ne lg nf md ot nh ni nj bi translated"><code class="fe od oe of og b">X_train</code>和<code class="fe od oe of og b">Y_train</code>由训练集的<em class="me"> 90% </em>组成</li><li id="ae7f" class="nb nc it ln b lo nk lr nl ky nm lc nn lg no md ot nh ni nj bi translated"><code class="fe od oe of og b">X_test</code>和<code class="fe od oe of og b">Y_test</code>构成了剩余的<em class="me">数据的 10% </em></li></ul></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h2 id="51fe" class="kp kq it bd kr ks kt dn ku kv kw dp kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">模型架构</h2><p id="62de" class="pw-post-body-paragraph ll lm it ln b lo lp ju lq lr ls jx lt ky lu lv lw lc lx ly lz lg ma mb mc md im bi translated">许多预先训练或预先定义的<em class="me">卷积神经网络</em> (CNN)架构存在于图像分割任务中，例如<strong class="ln iu"> U-Net </strong>，其中<strong class="ln iu"> </strong>在这方面表现出了优越的能力。</p><p id="1022" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">U-Net 由<em class="me">两条关键路径</em>组成:</p><ol class=""><li id="3e95" class="nb nc it ln b lo mf lr mg ky nd lc ne lg nf md ng nh ni nj bi translated"><strong class="ln iu">收缩</strong>对应一般卷积<code class="fe od oe of og b">Conv2D</code>操作，其中<em class="me">滤波器</em>滑过输入图像提取特征。<em class="me"><br/></em><code class="fe od oe of og b">Conv2D</code><em class="me">，</em><code class="fe od oe of og b">MaxPooling2D</code>层对像素组<em class="me"> </em>进行操作，并通过<strong class="ln iu">选择最大值</strong>来过滤值。汇集<em class="me">向下采样</em>输入图像尺寸，同时保持特征信息。</li><li id="3749" class="nb nc it ln b lo nk lr nl ky nm lc nn lg no md ng nh ni nj bi translated"><strong class="ln iu">扩展:</strong>使用转置卷积或<em class="me">去卷积</em>、<code class="fe od oe of og b">Conv2DTranspose</code>扩展下采样图像，以恢复输入图像空间信息。在上采样时，<strong class="ln iu">跳过连接</strong> <em class="me">连接</em>对称<em class="me">收缩</em>和<em class="me">扩展</em>层之间的特征。</li></ol><p id="c0a1" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">图 5 描述了用于<em class="me">航空影像分割</em>的简单 U-Net 设置，并突出显示了网络和图层形状的所有重要方面。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi ou"><img src="../Images/2e7bb791c11ed392707919b7af76a3ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n1PFaorpCSvxiIaA2Ub1bA.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">图 5 —带有输出形状的 U-Net 架构图(图片由作者提供)</p></figure><p id="ee38" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated"><em class="me">两种类型的</em>信息允许 U-Net 在<strong class="ln iu">语义分割</strong>问题上发挥最佳功能:</p><ol class=""><li id="3108" class="nb nc it ln b lo mf lr mg ky nd lc ne lg nf md ng nh ni nj bi translated">扩展路径中的过滤器包含<em class="me">高级空间</em>和<em class="me">上下文</em>特征信息</li><li id="a0d2" class="nb nc it ln b lo nk lr nl ky nm lc nn lg no md ng nh ni nj bi translated">收缩路径中包含的详细的<em class="me">细粒度的</em>结构信息</li></ol><p id="6438" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">这两个元件<em class="me">通过跳线连接熔断</em>。因此，<strong class="ln iu">串联</strong>允许神经网络使用伴随低级特征信息的高分辨率数据来进行预测。</p><p id="5561" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">Gist 3 提供了使用<code class="fe od oe of og b">Keras</code>构建简单 U-Net 模型的示例代码。</p><figure class="ml mm mn mo gt mp"><div class="bz fp l di"><div class="oi oj l"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">要点 3 —在 Keras 中定义 U-Net 模型</p></figure><p id="c5a1" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">图 6 提供了使用<code class="fe od oe of og b">model.summary</code>获得的控制台日志。将<code class="fe od oe of og b">Keras</code>输出与图 5 中的<em class="me">架构图</em>进行比较，可以看到相应的<strong class="ln iu">区块</strong>和<strong class="ln iu">图层形状</strong>。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi ov"><img src="../Images/27437403797b80063d616d2ffe9d6e7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5eTe4UlaVXH74tfxyqiLrA.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">图 6 —卷积神经网络模型总结(图片由作者提供)</p></figure></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h2 id="58ae" class="kp kq it bd kr ks kt dn ku kv kw dp kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">培训和评估</h2><p id="3ef9" class="pw-post-body-paragraph ll lm it ln b lo lp ju lq lr ls jx lt ky lu lv lw lc lx ly lz lg ma mb mc md im bi translated">在拟合训练数据之前，必须定义性能指标。由于图像分割涉及为图片中的每个像素分配一个类别，标准的成功指标是<strong class="ln iu">交集/并集</strong> ( <em class="me"> IOU </em>)系数。</p><p id="566a" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated"><strong class="ln iu"> IOU </strong>或<em class="me"> Jaccard Index </em>测量由实际和预测掩膜<em class="me">包围的<em class="me">重叠像素</em>的数量除以</em>横跨<em class="me">和</em>两个掩膜的<em class="me">总像素</em>。表达式 3 计算 IOU。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi ow"><img src="../Images/1c975730c5374614dad2c39faea4e48e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0pqheSZTYnyEwb4cwtQ2wQ.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">表达式 3 — Jaccard 相似性指数(作者图片)</p></figure><p id="eb48" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">下面在要点 4 中定义了定制的 Jaccard 相似性函数。</p><figure class="ml mm mn mo gt mp"><div class="bz fp l di"><div class="oi oj l"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">要点 4 — Jaccard 指数评估指标</p></figure><p id="9b12" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">特定的<strong class="ln iu">回调</strong>在训练神经网络时很有帮助，因为它们在训练期间提供了模型内部状态的<em class="me">视图。Gist 5 中定义的三个回调是:</em></p><ol class=""><li id="be1e" class="nb nc it ln b lo mf lr mg ky nd lc ne lg nf md ng nh ni nj bi translated"><code class="fe od oe of og b"><strong class="ln iu">ModelCheckpoint</strong></code>:保存所有时期中验证精度最高的最佳模型</li><li id="f0c7" class="nb nc it ln b lo nk lr nl ky nm lc nn lg no md ng nh ni nj bi translated"><code class="fe od oe of og b"><strong class="ln iu">EarlyStopping</strong></code>:如果 validation_loss 在两个时期内没有继续减少，则停止拟合</li><li id="64c3" class="nb nc it ln b lo nk lr nl ky nm lc nn lg no md ng nh ni nj bi translated"><code class="fe od oe of og b"><strong class="ln iu">CSVLogger</strong></code>:将每个时期的模型状态值保存到 CSV 文件中</li></ol><p id="b7e7" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">一旦定义了回调，模型就用<code class="fe od oe of og b">Adam</code> <em class="me">优化</em>、<code class="fe od oe of og b">categorical_crossentropy</code> <em class="me">损失</em>进行编译，并为<em class="me"> 20 个时期</em>启动训练，如要点 5 所示。</p><figure class="ml mm mn mo gt mp"><div class="bz fp l di"><div class="oi oj l"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">要点 5 — Keras 回调、模型编译和训练数据拟合</p></figure><p id="146f" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">图 7 是培训期间返回的 Python <em class="me">控制台日志</em>的副本。它显示了在每次迭代中<code class="fe od oe of og b">categorical_crossentropy</code> <strong class="ln iu">损失减少</strong>，而<strong class="ln iu"> Jaccard 相似性评估度量增加</strong>。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi ox"><img src="../Images/ac8e8520c1d1e98344c4962527509f17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u1lGIiPKzd1xVS7lGEN5iA.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">图 7 —时期 1–10 的 Python 控制台日志(图片由作者提供)</p></figure><p id="57fb" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">训练完成后，模型保存到硬盘上，达到最终性能指标值:</p><ul class=""><li id="2641" class="nb nc it ln b lo mf lr mg ky nd lc ne lg nf md ot nh ni nj bi translated"><em class="me">损失</em> ≈ <strong class="ln iu"> 0.4170 </strong></li><li id="0ac4" class="nb nc it ln b lo nk lr nl ky nm lc nn lg no md ot nh ni nj bi translated"><em class="me">精度</em> ≈ <strong class="ln iu"> 0.8616 </strong></li><li id="3a0b" class="nb nc it ln b lo nk lr nl ky nm lc nn lg no md ot nh ni nj bi translated"><em class="me"> Jaccard 指数</em> ≈ <strong class="ln iu"> 0.6599 </strong></li></ul><p id="e95b" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">最后，使用 Gist 6 中的代码来加载带有必要的定制评估指标(定义为依赖项)的模型。</p><figure class="ml mm mn mo gt mp"><div class="bz fp l di"><div class="oi oj l"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">要点 6 —从磁盘加载 Keras 模型</p></figure></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h2 id="6106" class="kp kq it bd kr ks kt dn ku kv kw dp kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">预言；预测；预告</h2><p id="3358" class="pw-post-body-paragraph ll lm it ln b lo lp ju lq lr ls jx lt ky lu lv lw lc lx ly lz lg ma mb mc md im bi translated">由于<em class="me">数据集大小限制</em>，预测发生在从测试集中随机采样的图像上。</p><p id="d150" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">使用经过训练的 U-Net 模型进行预测的详细注释代码在下面的要点 7 中提供。</p><figure class="ml mm mn mo gt mp"><div class="bz fp l di"><div class="oi oj l"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">要点 7——进行预测并可视化结果</p></figure><p id="4a80" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">图 8 是十个输出<strong class="ln iu"> </strong>的<em class="me">可视化</em>，描绘了迪拜的修补<em class="me">航拍图像</em>、地面真相<em class="me">蒙版</em>和 U 网分割<em class="me">预测</em>。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi oy"><img src="../Images/7101265641a1779190edccafef1c76f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7LVlX1lMoDLR_nSXGkUM-Q.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">图 8-U-Net 航空影像分割预测(图片由作者提供)</p></figure></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h2 id="c206" class="kp kq it bd kr ks kt dn ku kv kw dp kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">结论</h2><p id="7778" class="pw-post-body-paragraph ll lm it ln b lo lp ju lq lr ls jx lt ky lu lv lw lc lx ly lz lg ma mb mc md im bi translated">由于<strong class="ln iu"> <em class="me">计算能力</em> </strong>受限，神经网络<strong class="ln iu"> <em class="me">规模受限</em> </strong>，训练迭代不超过 20 个历元。目前大约有<em class="me">140 万个可训练参数</em>，如图 6 所示。因此，模型性能无疑可以通过<em class="me">超参数调整</em>和采用更加<em class="me">复杂的</em>网络来提高。</p><p id="cb38" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">图 9 描述了一些<strong class="ln iu">最不成功的分割</strong>。一些<em class="me">相似之处</em>在航拍图像的最左边一栏很明显。每张图像都非常亮，包含许多像素，接近 RGB 光谱的白色端。相比之下，查看图 8，最<em class="me">可接受的分割</em>在<strong class="ln iu">高对比度图像</strong>上。因此，合成额外的更亮的训练图像或者<em class="me">在子集上微调</em>模型以提高性能可能是合理的。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi oy"><img src="../Images/e459687b19dabb58887616964c4ad30b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9O3DIEhA4fA1aZYo3DgcxQ.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">图 9——最佳预测检验(作者图片)</p></figure><p id="b111" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">然而，正如预测所显示的，考虑到小的数据集大小和有限的计算能力，该模型通常<strong class="ln iu">表现相对良好</strong>。</p><p id="95f4" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated"><em class="me">本文展示了如何使用 U-Net TensorFlow 模型对迪拜数据集的 MBRSC 航空影像进行语义分割。</em></p><p id="5397" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">类似的过程适用于其他机器学习图像分割任务。U-Net 最初是为生物医学领域的工作而设计的，自问世以来，已在其他各种领域得到应用。</p><p id="794c" class="pw-post-body-paragraph ll lm it ln b lo mf ju lq lr mg jx lt ky mh lv lw lc mi ly lz lg mj mb mc md im bi translated">所有代码都存储在这个 GitHub <a class="ae na" href="https://github.com/ad-1/u-net-aerial-imagery-segmentation" rel="noopener ugc nofollow" target="_blank">库</a>中:</p><div class="oz pa gp gr pb pc"><a href="https://github.com/ad-1/u-net-aerial-imagery-segmentation" rel="noopener  ugc nofollow" target="_blank"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd iu gy z fp ph fr fs pi fu fw is bi translated">GitHub-ad-1/u-net-航空影像-分段:MBRSC 航空影像的语义分段…</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">这个资源库伴随着这个中型项目的文章 https://medium.com/@andrewdaviesul/membership 旨在…</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">github.com</p></div></div><div class="pl l"><div class="pm l pn po pp pl pq mu pc"/></div></div></a></div><div class="oz pa gp gr pb pc"><a href="https://medium.com/@andrewdaviesul/membership" rel="noopener follow" target="_blank"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd iu gy z fp ph fr fs pi fu fw is bi translated">通过我的推荐链接加入媒体-安德鲁·约瑟夫·戴维斯</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">medium.com</p></div></div><div class="pl l"><div class="pr l pn po pp pl pq mu pc"/></div></div></a></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h2 id="a1e8" class="kp kq it bd kr ks kt dn ku kv kw dp kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">参考</h2><p id="f3fa" class="pw-post-body-paragraph ll lm it ln b lo lp ju lq lr ls jx lt ky lu lv lw lc lx ly lz lg ma mb mc md im bi translated">[1] <a class="ae na" href="https://www.kaggle.com/humansintheloop/semantic-segmentation-of-aerial-imagery" rel="noopener ugc nofollow" target="_blank">航空影像语义分割</a> ( <a class="ae na" href="https://creativecommons.org/publicdomain/zero/1.0/" rel="noopener ugc nofollow" target="_blank"> CC0:公共领域</a>)—ka ggle<br/>【2】<a class="ae na" href="https://www.coursera.org/learn/convolutional-neural-networks" rel="noopener ugc nofollow" target="_blank">卷积神经网络</a> —深度学习。艾，<br/>【3】<a class="ae na" href="https://www.youtube.com/watch?v=jvZm8REF2KY&amp;t=2109s" rel="noopener ugc nofollow" target="_blank">使用 U-net 对航空(卫星)影像进行语义分割</a>—digitals reeni<br/>【4】<a class="ae na" href="https://www.jeremyjordan.me/evaluating-image-segmentation-models/#:~:text=Intersection%20over%20Union&amp;text=This%20metric%20is%20closely%20related,pixels%20present%20across%20both%20masks." rel="noopener ugc nofollow" target="_blank">评估分割模型</a> —杰瑞米·乔登(2018 年 5 月 30 日)<br/>【5】Keras 三分钟讲解—作者<a class="ae na" href="https://www.linkedin.com/in/andreduong/" rel="noopener ugc nofollow" target="_blank"> Andre Duong </a>，UT Dallas</p></div></div>    
</body>
</html>