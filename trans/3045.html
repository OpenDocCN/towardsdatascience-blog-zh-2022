<html>
<head>
<title>Dropout in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的辍学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dropout-in-neural-networks-47a162d621d9#2022-07-05">https://towardsdatascience.com/dropout-in-neural-networks-47a162d621d9#2022-07-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="af8c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">丢弃层是减少神经网络过拟合的常用方法。它是现代深度学习时代的地下规则之王。</h2></div><p id="12b0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个深度学习的时代，几乎每个数据科学家都必须在他们构建神经网络的职业生涯中的某个时刻使用过dropout层。但是，为什么辍学如此普遍？辍学层内部是如何运作的？它解决了什么问题？除了退学还有别的选择吗？</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/da8cacd1dca02f72c82bfbf3c06a3878.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pX2hg-ss8ZZ4_KEhZrpwJA.jpeg"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图0:印度的Jharokhe，去掉一些光(图片由作者提供)</p></figure><p id="5cd9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您有关于辍学层的类似问题，那么您就来对地方了。在这篇博客中，你会发现著名的辍学层背后的错综复杂。完成这篇博客后，你会很舒服地回答与辍学有关的不同问题，如果你是一个更具创新精神的人，你可能会提出一个更高级版本的辍学层。</p><p id="ca97" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们开始吧…:)</p><h1 id="57b0" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated"><strong class="ak">概述</strong></h1><p id="76e9" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">本博客分为以下几个部分:</p><ol class=""><li id="fcb0" class="mo mp iq kh b ki kj kl km ko mq ks mr kw ms la mt mu mv mw bi translated">简介:它试图解决的问题</li><li id="1dc7" class="mo mp iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated">什么是辍学？</li><li id="62c5" class="mo mp iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated">它是如何解决问题的？</li><li id="84c2" class="mo mp iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated">辍学实施</li><li id="eba2" class="mo mp iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated">推理过程中的遗漏</li><li id="16ef" class="mo mp iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated">它是如何构思的</li><li id="47e9" class="mo mp iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated">Tensorflow实现</li><li id="508f" class="mo mp iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated">结论</li></ol><h1 id="2d23" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">介绍</h1><p id="d546" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">因此，在深入了解它的世界之前，让我们先来解决第一个问题。我们试图解决的问题是什么？</p><p id="c337" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">深度神经网络具有不同的架构，有时很浅，有时很深，试图在给定的数据集上进行概括。但是，在努力从数据集中学习不同特征的过程中，他们有时会学习数据集中的<strong class="kh ir">统计噪声</strong>。这无疑提高了模型在训练数据集上的性能，但在新的数据点(测试数据集)上却失败了。这就是<strong class="kh ir">过拟合的问题。</strong>为了解决这个问题，我们有各种惩罚网络权重的正则化技术，但这还不够。</p><p id="00d9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">减少过度拟合的最佳方式或调整固定大小模型的最佳方式是从所有可能的参数设置中获取平均预测值，并汇总最终输出。但是，这在计算上变得过于昂贵，并且对于实时推断/预测是不可行的。</p><p id="a54d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一种方法是受集成技术(如AdaBoost、XGBoost和Random Forest)的启发，其中我们使用不同架构的多个神经网络。但这需要训练和存储多个模型，随着时间的推移，随着网络越来越深，这将成为一个巨大的挑战。</p><p id="43e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们有一个很好的解决方案，称为脱落层。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/3bcdcea73b912332e2255a1eb285c88c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*wDGvx0z0-nEB8zQHykvwPw.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图1:应用于标准神经网络的下降(图片由<a class="ae nd" href="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank"> Nitish </a>提供)</p></figure><h1 id="8231" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">什么是辍学？</h1><blockquote class="ne nf ng"><p id="b406" class="kf kg nh kh b ki kj jr kk kl km ju kn ni kp kq kr nj kt ku kv nk kx ky kz la ij bi translated">术语“丢失”是指在神经网络中丢失节点(输入和隐藏层)(如图1所示)。与被丢弃节点的所有前向和后向连接都被临时移除，从而在父网络之外创建新的网络架构。节点以p的丢弃概率被丢弃。</p></blockquote><p id="9ed8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们用给定的输入x: {1，2，3，4，5}来理解全连接层。我们有一个掉线层，概率p = 0.2(或者保持概率= 0.8)。在从输入x向前传播(训练)期间，20%的节点将被丢弃，即x可能变成{1，0，3，4，5}或{1，2，0，4，5}等等。同样，它适用于隐藏层。</p><p id="cf2a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，如果隐藏层有1000个神经元(节点),并且以丢弃概率= 0.5应用丢弃，则在每次迭代(批次)中将随机丢弃500个神经元。</p><p id="9f94" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通常，对于输入层，保持概率，即1-丢弃概率，更接近于1，0.8是作者建议的最佳值。对于隐藏层，丢弃概率越大，模型越稀疏，其中0.5是最佳保持概率，表示丢弃50%的节点。</p><p id="cc85" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nh">那么辍学如何解决过度拟合的问题呢？</em></p><h1 id="a951" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">它是如何解决过拟合问题的？</h1><p id="2efa" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">在过拟合问题中，模型学习统计噪声。准确地说，在给定所有单元(神经元)的情况下，训练的主要动机是减少损失函数。因此，在过度拟合中，一个单元可能以修正其他单元的错误的方式改变。这导致了复杂的协同适应，这又导致了过拟合问题，因为这种复杂的协同适应不能在看不见的数据集上推广。</p><p id="b93f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，如果我们使用dropout，它会阻止这些单元修复其他单元的错误，从而阻止共同适应，因为在每次迭代中，一个单元的存在是非常不可靠的。因此，通过随机删除一些单元(节点)，它迫使各层采取概率方法对输入承担或多或少的责任。</p><p id="bd27" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这确保了模型变得一般化，从而减少了过度拟合问题。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/d6068ed49b360e8d24d4f0420ca78b9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*9Jp5erzZdf-4CwJKyvx0yA.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图2: (a)无脱落的隐藏层特征；(b)隐藏的图层特征与丢失(图片由<a class="ae nd" href="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank"> Nitish </a>提供)</p></figure><p id="d9e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从图2中，我们可以很容易地看出，与没有丢失的层中的共同适应相比，具有丢失的隐藏层正在学习更多的一般化特征。很明显，辍学打破了这种单位间的关系，更注重一般化。</p></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><h1 id="ba72" class="lr ls iq bd lt lu nt lw lx ly nu ma mb jw nv jx md jz nw ka mf kc nx kd mh mi bi translated">辍学实施</h1><p id="8c5f" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">说够了！让我们来看看辍学的数学解释。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/34f25757585a69d888c95b8a443cdced.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*DNewrtjKh8F2uK0TwE8C4A.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图3: (a)训练期间的单元(神经元)以概率p存在，并以权重'<strong class="bd nz"> w </strong>'连接到下一层；(b)推理/预测期间的单元总是存在的，并且通过权重连接到下一层，'<strong class="bd nz"> pw </strong>'(图片由<a class="ae nd" href="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank"> Nitish </a>提供)</p></figure><p id="021b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在丢弃层的原始实现中，在训练期间，以保持概率(1丢弃概率)选择层中的单元(节点/神经元)。这就在给定的训练批次中创建了一个更薄的架构，而且每次这个架构都不一样。</p><p id="03f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在标准神经网络中，在前向传播期间，我们有以下等式:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/e2735d1962a1dc04872a37d734452d5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*IhGc0KbapTZUKa52b2bQSw.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图4:标准神经网络的前向传播(图片由<a class="ae nd" href="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank"> Nitish </a>提供)</p></figure><p id="7122" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中:<br/> z:表示激活前层(l + 1)的输出向量<br/> y:表示层l的输出向量<br/> w:层l的权重<br/> b:层l的偏差</p><p id="0305" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，利用激活函数，z被转换成层(l+1)的输出。</p><p id="58da" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，如果有一个压差，正向传播方程会发生如下变化:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ob"><img src="../Images/974582963e740008e8c52629a19f59b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*6zbHNtrOopvWm3a_yBlbvw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图5:有缺失的层的向前传播(图片由<a class="ae nd" href="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank"> Nitish </a>提供)</p></figure><p id="ece9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，在我们计算<strong class="kh ir"> z之前，</strong>对该层的输入进行采样，并按元素乘以独立的伯努利变量。<strong class="kh ir"> r </strong>表示伯努利随机变量，每个伯努利随机变量具有为1的概率p。基本上，<strong class="kh ir"> r </strong>充当输入变量的掩码，确保根据丢失的保留概率只保留少数单元。这确保了我们具有细化的输出“<strong class="kh ir">y(bar)”</strong>，其在前馈传播期间作为输入提供给该层。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/14148f174d6b091ae2c365abd8ba220b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*-YVIa18tCe6GKf4qRHEA0A.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图6:前向传播期间给定层的压差网络与标准网络的比较(图片由<a class="ae nd" href="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank"> Nitish </a>提供)</p></figure><h1 id="ee86" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">推理过程中的遗漏</h1><p id="cfe0" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">现在，我们知道辍学在数学上行得通，但是在推理/预测过程中会发生什么呢？我们是使用带有丢失的网络，还是在推理过程中移除丢失？</p><p id="f40f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是辍学最重要的概念之一，但很少有数据科学家知道。</p><blockquote class="od"><p id="8f92" class="oe of iq bd og oh oi oj ok ol om la dk translated">根据推理过程中的原始实现(图3b ),我们不使用漏失层。这意味着在预测步骤中会考虑所有单元。但是，由于从一个层中取出所有的单元/神经元，最终的权重将比预期的要大，为了处理这个问题，权重首先按照所选择的辍学率进行缩放。有了这个，网络将能够做出准确的预测。</p></blockquote><p id="ab6a" class="pw-post-body-paragraph kf kg iq kh b ki on jr kk kl oo ju kn ko op kq kr ks oq ku kv kw or ky kz la ij bi translated">更准确地说，如果在训练期间以概率p保留一个单元，则在预测阶段该单元的输出权重乘以p。</p></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><h1 id="0346" class="lr ls iq bd lt lu nt lw lx ly nu ma mb jw nv jx md jz nw ka mf kc nx kd mh mi bi translated">辍学是如何形成的</h1><p id="7bd5" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">《辍学:防止神经网络过度适应的简单方法》的作者之一杰弗里·辛顿(Geoffrey Hinton)表示，有一系列事件激发了根本性的辍学。</p><ol class=""><li id="73a9" class="mo mp iq kh b ki kj kl km ko mq ks mr kw ms la mt mu mv mw bi translated">与谷歌大脑的类比是，它应该很大，因为它学习了大量的模型。在神经网络中，这不是一个非常有效的硬件使用，因为相同的功能需要由不同的模型分别发明。这是使用相同神经元子集的想法被发现的时候。</li><li id="3002" class="mo mp iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated">银行出纳员:在那个年代，出纳员会定期更换，这一定是因为需要员工之间的合作才能成功地欺骗银行。这植入了随机选择不同神经元的想法，使得每次迭代都使用不同组的神经元。这将确保神经元无法学习共同适应，并防止过度适应，类似于防止银行中的阴谋。</li><li id="24bd" class="mo mp iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated">有性生殖:它包括谈论父母一方的一半基因和另一方的一半基因，添加非常少量的随机突变，以产生后代。这创造了基因的混合能力，使它们更加强壮。这可能与用于破坏共同适应的辍学有关(就像基因突变一样增加了随机性)。</li></ol><p id="5678" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">整个旅程难道不令人着迷吗？</p><p id="ebc4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">介绍辍学概念的主要动机是激励读者探索他们周围的世界，并将其与其他几个神经网络的工作原理联系起来。它肯定会产生许多这样的创新。</p></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><h1 id="97f8" class="lr ls iq bd lt lu nt lw lx ly nu ma mb jw nv jx md jz nw ka mf kc nx kd mh mi bi translated"><strong class="ak"> Tensorflow实现</strong></h1><p id="0b0b" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">如果我们遵循最初的实现，我们需要在预测阶段将权重乘以丢弃概率。只是为了去除这个阶段中的任何处理，我们有一个被称为“<strong class="kh ir">反向丢弃”的实现。</strong></p><p id="2223" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将权重与丢失概率相乘的目的是确保最终的权重具有相同的尺度，因此预测是正确的。在反向辍学，这一步是在训练本身。在训练时，在丢弃操作之后剩余的所有权重乘以保持概率的倒数，即w * (1/p)。</p><p id="26c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了获得为什么两种操作在层权重上相似的数学证明，我推荐浏览一下<a class="ae nd" href="https://leimao.github.io/blog/Dropout-Explained/#:~:text=During%20inference%20time%2C%20dropout%20does,were%20multiplied%20by%20pkeep%20." rel="noopener ugc nofollow" target="_blank">毛蕾</a>的博客。</p><h1 id="28c0" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">结论</h1><p id="52a9" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">终于！！我们已经深入分析了几乎所有神经网络使用的漏失层。</p><p id="4e44" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">辍学可以用于大多数类型的神经网络。这是减少模型过度拟合的一个很好的工具。它比可用的正则化方法好得多，并且还可以与最大范数归一化相结合，这比仅使用辍学提供了显著的提升。</p><p id="3039" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在接下来的博客中，我们将会学到更多关于这些基本层的知识，这些基本层在几乎所有的网络中都有使用。仅举几个例子，批标准化、层标准化和关注层。</p><h1 id="ad34" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">参考</h1><p id="e197" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">[1] Nitish Srivastava，Dropout:防止神经网络过拟合的简单方法，<a class="ae nd" href="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank">https://jmlr . org/papers/volume 15/Srivastava 14 A/Srivastava 14 A . pdf</a></p><p id="6eb2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]杰森·布朗利(Jason Brownlee)，对正则化深度神经网络的辍学现象的温和介绍，<a class="ae nd" href="https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/Dropout-for-regulating-Deep-Neural-Networks/</a></p><p id="eced" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3]毛蕾，Dropout Explained，<a class="ae nd" href="https://leimao.github.io/blog/Dropout-Explained/#:~:text=During%20inference%20time%2C%20dropout%20does,were%20multiplied%20by%20pkeep%20" rel="noopener ugc nofollow" target="_blank">https://lei Mao . github . io/blog/Dropout-Explained/#:~:text =期间%20推断% 20时间%2C%20dropout%20does，被% 20乘%20pkeep%20 </a>。</p><p id="797f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4] Juan Miguel，Tensorflow中的Dropout explained and implementation，<a class="ae nd" href="http://laid.delanover.com/dropout-explained-and-implementation-in-tensorflow/" rel="noopener ugc nofollow" target="_blank">http://layed . delanover . com/Dropout-explained-and-implementation-in-tensor flow/</a></p></div></div>    
</body>
</html>