<html>
<head>
<title>Natural Language Process for Judicial Sentences with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 实现司法判决的自然语言处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/natural-language-process-for-judicial-sentences-with-python-ed21d9be9ca9#2022-12-13">https://towardsdatascience.com/natural-language-process-for-judicial-sentences-with-python-ed21d9be9ca9#2022-12-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="ir is gp gr it iu gh gi paragraph-image"><div class="ab gu cl iv"><img src="../Images/f07ee96bb38bd710e1e7005c8e7cfdb3.png" data-original-src="https://miro.medium.com/v2/format:webp/0*9g0o_RccyFazT8qI.jpg"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated"><a class="ae jc" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/</a></p></figure><div class=""/><div class=""><h2 id="1ee5" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">第 8 部分:命名实体和网络表示</h2></div><p id="2366" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如我们在以前的文章中看到的，NLP 处理包含大量信息的大量文本数据。我们看到了许多降低复杂性和从数据中提取相关信息的技术，从最简单的<a class="ae jc" rel="noopener" target="_blank" href="/natural-language-process-for-judicial-sentences-with-python-102064f24372">标记化/词汇化/词干化</a>到<a class="ae jc" rel="noopener" target="_blank" href="/natural-language-process-for-judicial-sentences-with-python-part-1-bdc01a4d7f04"> TF-IDF 分析</a>和<a class="ae jc" rel="noopener" target="_blank" href="/natural-language-process-for-judicial-sentences-with-python-bb60a6d3cc0b">单词/文档嵌入</a>。</p><p id="269f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">分析这些文本数据的另一个重要方面是识别命名实体。</p><p id="ec74" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">命名实体可以定义为具有适当标识的对象。例如，一个人是一个命名的实体，还有一个国家、一个组织等等。</p><p id="6fd5" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了从原始文本中提取命名实体，需要命名实体识别(NER)算法。NER 算法可以属于不同的类别，从监督学习模型(之前已经标记的实体之间的多类分类)到基于深度学习的方法再到统计模型。后者是我们将要使用的，因为它是 Python 库的 paCy 所使用的。</p><p id="d5dd" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">spaCy 的 NER 提供了 18 个命名实体，我们可以通过导入所需的库并列出这些实体来初步了解它们:</p><pre class="lq lr ls lt gt lu lv lw bn lx ly bi"><span id="f748" class="lz ma jf lv b be mb mc l md me">!pip install spacy<br/>import spacy<br/>from spacy import displacy<br/><br/>NER = spacy.load("en_core_web_sm")<br/><br/>ner_lst = NER.pipe_labels['ner']<br/><br/>print(len(ner_lst))<br/>print(ner_lst)</span></pre><figure class="lq lr ls lt gt iu gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi ca"><img src="../Images/6186d5766539b1c2ccff461ccda7ea3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CyiLArMqBRkmkzWJXZ5UpA.png"/></div></div></figure><p id="0df7" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们看一个带有原始文本的 NER 应用程序的例子(我使用了比尔盖茨的<a class="ae jc" href="https://en.wikipedia.org/wiki/Bill_Gates" rel="noopener ugc nofollow" target="_blank">维基百科</a>页面)。</p><pre class="lq lr ls lt gt lu lv lw bn lx ly bi"><span id="ba61" class="lz ma jf lv b be mb mc l md me">raw_text="William Henry Gates III (born October 28, 1955) is an American business magnate and philanthropist. He is a co-founder of Microsoft, along with his late childhood friend Paul Allen. During his career at Microsoft, Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect, while also being the largest individual shareholder until May 2014. He was a major entrepreneur of the microcomputer revolution of the 1970s and 1980s."<br/><br/>text1= NER(raw_text)<br/>for word in text1.ents:<br/>    print(word.text,word.label_)</span></pre><figure class="lq lr ls lt gt iu gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/73d9ff8b15fa8438fb1d5050b1e62811.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*UXS3e0KYtX-r7h_Ji2lBfg.png"/></div></figure><p id="57e3" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于每个命名实体，我们还可以看到 spaCy 提供的定义。也就是说，让我们看看“组织”的定义。</p><pre class="lq lr ls lt gt lu lv lw bn lx ly bi"><span id="bc5f" class="lz ma jf lv b be mb mc l md me">spacy.explain("ORG")</span></pre><figure class="lq lr ls lt gt iu gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/d1d091532b389acbfe53365038bddae1.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*xpYV3aDMULwF0xzIbzI1xw.png"/></div></figure><p id="95b8" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，我们还可以使用 spaCy 特性中的一个有趣的视觉效果:</p><pre class="lq lr ls lt gt lu lv lw bn lx ly bi"><span id="7d0b" class="lz ma jf lv b be mb mc l md me">displacy.render(text1,style="ent",jupyter=True)</span></pre><figure class="lq lr ls lt gt iu gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi ml"><img src="../Images/4f13957947e35b756757de3cbad5200a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FC_I5-de-DhLtxJmucr0XA.png"/></div></div></figure><p id="98d7" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在让我们跳到 NER 对我们的司法判决的分析。</p><h2 id="893d" class="mm ma jf bd mn mo mp dn mq mr ms dp mt ld mu mv mw lh mx my mz ll na nb nc nd bi translated">命名实体</h2><p id="a7e1" class="pw-post-body-paragraph ku kv jf kw b kx ne kg kz la nf kj lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">想看文章的词汇构成。由于“tax”类别是最常见的类别(正如我们在本系列的第 2 部分中发现的那样)，我将对该类别进行分析，但是同样的推理可以扩展到所有其余的类别。</p><pre class="lq lr ls lt gt lu lv lw bn lx ly bi"><span id="4746" class="lz ma jf lv b be mb mc l md me">#It might take a while, so skip this cell and the following one <br/>m = ['Tax' in df['category'][i] for i in range(len(df))]<br/>df_tax = df[m]<br/>df_tax = df_tax.reset_index(drop=True)<br/>#df_tax.head()<br/><br/>tax_entities=[entity.label_ for i in range(len(df_tax)) for entity in nlp(df['Tokens'][i]).ents]<br/>#saving results<br/>with open('data/tax_entities.pkl', 'wb') as f:<br/>    pickle.dump(tax_entities, f)</span></pre><p id="25be" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们初始化一个包含与命名实体相关的 NER 标签的字典。</p><pre class="lq lr ls lt gt lu lv lw bn lx ly bi"><span id="0878" class="lz ma jf lv b be mb mc l md me">#initializing a dictionary<br/>counts = dict()<br/>for i in tax_entities:<br/>    counts[i] = counts.get(i, 0) + 1<br/>    <br/>d = dict()<br/>for category in categories:<br/>    m = [category in df['category'][i] for i in range(len(df))]<br/>    tmp = df[m]<br/>    tmp = tmp.reset_index(drop=True)<br/>    entities=[entity.label_ for i in range(len(tmp)) for entity in nlp(df['Tokens'][i]).ents]<br/>    counts = dict()<br/>    for i in entities:<br/>        counts[i] = counts.get(i, 0) + 1<br/>    d[category]=counts<br/><br/>#saving results<br/>import pickle<br/>with open('data/named_entities_dict.pkl', 'wb') as f:<br/>    pickle.dump(d, f)</span></pre><p id="0a62" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，让我们将字典转换成熊猫数据框架并绘制结果:</p><pre class="lq lr ls lt gt lu lv lw bn lx ly bi"><span id="846f" class="lz ma jf lv b be mb mc l md me">#run this cell to download the result<br/>import pickle<br/><br/>with open('data/tax_entities.pkl', 'rb') as f:<br/>    tax_entities = pickle.load(f)<br/>with open('data/named_entities_dict.pkl', 'rb') as f:<br/>    d = pickle.load(f)<br/>#plotting results<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>df_dict = pd.DataFrame(d)<br/>df_dict.T.plot(kind="bar", stacked=True, figsize=(20,10))<br/>plt.title('Named entities composition by category')<br/>plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')<br/>plt.show()<br/></span></pre><figure class="lq lr ls lt gt iu gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nj"><img src="../Images/7bb0d37ad73c222af2cbff0d1f4d012c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AEzQJVY21mvPOuSfRHCKaQ.png"/></div></div></figure><p id="9a6b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">就命名实体而言，似乎所有类别都有类似的组成，最常见的是(每个类别)个人、组织和地缘政治实体。</p><p id="b8b5" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">多亏了 NER，</p><h2 id="8096" class="mm ma jf bd mn mo mp dn mq mr ms dp mt ld mu mv mw lh mx my mz ll na nb nc nd bi translated">网络表示</h2><p id="c0bf" class="pw-post-body-paragraph ku kv jf kw b kx ne kg kz la nf kj lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">我想在这部分做的最后一个视觉分析是用一个网络框架来表现我们的文本。</p><p id="63c9" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">网络是表示跨单元动态关系的强大数学模型。在这种情况下，单元(或节点)将是文章，我想检查它们之间的相似性。为此，我将根据链接的节点(即文章)之间的相似性来加权每条边。为了有一个可解释的可视化，我将只使用整个数据集的前 10 篇文章。</p><pre class="lq lr ls lt gt lu lv lw bn lx ly bi"><span id="c2fe" class="lz ma jf lv b be mb mc l md me">tmp = df.head(10)<br/>tokens = []<br/>lemma = []<br/>pos = []<br/>parsed_doc = [] <br/>col_to_parse = 'text'<br/>for doc in nlp.pipe(tmp[col_to_parse].astype('unicode').values, batch_size=50,<br/>                        n_threads=3):<br/>    if doc.is_parsed:<br/>        parsed_doc.append(doc)<br/>        tokens.append([n.text for n in doc])<br/>        lemma.append([n.lemma_ for n in doc])<br/>        pos.append([n.pos_ for n in doc])<br/>    else:<br/>        # We want to make sure that the lists of parsed results have the<br/>        # same number of entries of the original Dataframe, so add some blanks in case the parse fails<br/>        tokens.append(None)<br/>        lemma.append(None)<br/>        pos.append(None)<br/><br/>tmp['parsed_doc'] = parsed_doc<br/>tmp['comment_tokens'] = tokens<br/>tmp['comment_lemma'] = lemma<br/>tmp['pos_pos'] = pos</span></pre><p id="cbdb" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在让我们导入这个分析的主库，<a class="ae jc" href="https://networkx.org/" rel="noopener ugc nofollow" target="_blank"> NetworkX </a> —一个 Python 包，用于创建和分析网络的结构和交互。</p><p id="e637" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">使用 NetworkX 提供的工具，我将为每个节点分配一个与其标题对应的标签:</p><pre class="lq lr ls lt gt lu lv lw bn lx ly bi"><span id="2228" class="lz ma jf lv b be mb mc l md me">import warnings<br/>warnings.filterwarnings("ignore")<br/>import networkx as nx<br/>G = nx.Graph()<br/>for i in range(10):<br/>    G.add_node(i, text = tmp['parsed_doc'][i], title=tmp['titles'][i])<br/>for i in range(len(tmp['parsed_doc'])):        # sure, it's inefficient, but it will do<br/>    for j in range(len(tmp['parsed_doc'])):<br/>        if i != j:<br/>            if not (G.has_edge(i,j)):<br/>                sim = tmp['parsed_doc'][i].similarity(tmp['parsed_doc'][j])<br/>                G.add_edge(i, j, weight = sim)<br/>labels = {}<br/>for i in G.nodes():<br/>    labels[i]=G.node[i]['title']<br/>    <br/>labels</span></pre><figure class="lq lr ls lt gt iu gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nk"><img src="../Images/22fcad1dd07e689fadfd7ba9f7eef1d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sZIi52_ieqYOfs5dazjgww.png"/></div></div></figure><p id="9f7e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在让我们来看看结果:</p><pre class="lq lr ls lt gt lu lv lw bn lx ly bi"><span id="bd57" class="lz ma jf lv b be mb mc l md me">plt.rcParams['figure.figsize'] = [16, 9]  # a better aspect ratio for labelled nodes<br/>from math import sqrt<br/>count = G.number_of_nodes()<br/>equilibrium = 10 / sqrt(count)    # default for this is 1/sqrt(n), but this will 'blow out' the layout for better visibility<br/>pos = nx.fruchterman_reingold_layout(G, k=equilibrium, iterations=300)<br/>edgewidth = [d['weight'] for (u,v,d) in G.edges(data=True)] #weighting edges by the similarity of the nodes they link<br/><br/>nx.draw(G, pos, font_size=3, node_size=50, edge_color='gray', with_labels=False)<br/>for p in pos:  # raise positions of the labels, relative to the nodes<br/>    pos[p][1] -= 0.03<br/>nx.draw_networkx_edges(G, pos, width=edgewidth)<br/>nx.draw_networkx_labels(G,pos, font_size=10, font_color='k', labels = labels)<br/><br/>plt.show()</span></pre><figure class="lq lr ls lt gt iu gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nl"><img src="../Images/8bc6e6eb6195c055f509f06c17893b66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AK6mslQl34rKvJhnhhIYMA.png"/></div></div></figure><p id="6a91" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果你想更深入地研究网络科学和背后的数学，我建议阅读我以前的文章<a class="ae jc" href="https://medium.com/p/c306fc3860e0/edit" rel="noopener">这里</a>。</p><p id="ed95" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">通过命名实体和网络分析，我们丰富了可以从原始文本中检索的信息。在下一篇文章中，我们将继续使用无监督学习技术，通过无监督情感分析，不仅检测文本的含义，还检测与每个句子相关的情绪或情感。</p><p id="5445" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以请继续关注第 9 部分！</p><h2 id="5567" class="mm ma jf bd mn mo mp dn mq mr ms dp mt ld mu mv mw lh mx my mz ll na nb nc nd bi translated">参考</h2><ul class=""><li id="cb7d" class="nm nn jf kw b kx ne la nf ld no lh np ll nq lp nr ns nt nu bi translated"><a class="ae jc" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> NLTK::自然语言工具包</a></li><li id="aea8" class="nm nn jf kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated">Python 中的 spaCy 工业级自然语言处理</li><li id="8579" class="nm nn jf kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated"><a class="ae jc" href="https://www.justice.gov/news" rel="noopener ugc nofollow" target="_blank">司法新闻| DOJ |司法部</a></li><li id="1ac4" class="nm nn jf kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated"><a class="ae jc" href="https://www.kaggle.com/datasets/jbencina/department-of-justice-20092018-press-releases" rel="noopener ugc nofollow" target="_blank">司法部 2009-2018 年新闻发布| Kaggle </a></li><li id="4d3e" class="nm nn jf kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated"><a class="ae jc" href="https://spacy.io/usage/linguistic-features#named-entities" rel="noopener ugc nofollow" target="_blank">https://spacy.io/usage/linguistic-features#named-entities</a></li><li id="c2a5" class="nm nn jf kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated"><a class="ae jc" href="https://networkx.org/" rel="noopener ugc nofollow" target="_blank">https://networkx.org/</a></li></ul></div></div>    
</body>
</html>