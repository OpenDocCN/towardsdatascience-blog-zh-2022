<html>
<head>
<title>Feature Selection for the Lazy Data Scientist</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">懒惰的数据科学家的特征选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-for-the-lazy-data-scientist-c31ba9b4ee66#2022-04-26">https://towardsdatascience.com/feature-selection-for-the-lazy-data-scientist-c31ba9b4ee66#2022-04-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8812" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关于基于过滤器的特征选择方法的综合文献综述和代码</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ddd6a6207f173e1b43a11f2842f5b1c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UCoPSfdKxvHpd9tqc49DAQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">内森·杜姆劳在<a class="ae kv" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="3f6b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你是一名数据科学家，并且维数灾难袭击了你，这篇文章是为你准备的。这是一个关于特征选择算法的综合调查(有例子)。我们通过整合和评估不同特征选择器的集合来结束讨论，以得到一个全面的结论。</p><p id="e4fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，让我们从定义什么是特征选择开始。如果你面临高维数的问题，你可能听说过“降维(或PCA/自动编码器)”和“特征选择”在我们深入研究特性选择之前，这里有一个降维的简短描述，它将帮助您决定是否应该采用这种方法。</p><h2 id="c73e" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">降维</h2><p id="4037" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">降维通过将数据从特征空间转换到潜在空间来帮助我们减少特征的数量。这个更小的空间应该以更压缩的方式来表示学习到的特征。如果我们正确地选择了潜在空间的维数，那么我们甚至可以降低噪声，去除不相关的信息。这种方法的最大缺点是它的可解释性。从特征空间转移到潜在空间，我们不再直接看特征，只看它们的表示，它们通常不像特征那样有直观的解释或逻辑。在处理图像或信号等非结构化数据时，这种方法变得非常方便。关于潜在空间含义的进一步讨论，见<a class="ae kv" rel="noopener" target="_blank" href="/understanding-latent-space-in-machine-learning-de5a7c687d8d">此处</a>和<a class="ae kv" href="https://hackernoon.com/latent-space-visualization-deep-learning-bits-2-bd09a46920df" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/b394f9d091a2e453b5ca5ce2b14e5301.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*iY_3oRXSFtaku8IpYXqaPA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">一个自动编码器架构中潜在空间的例子，作者图片</p></figure><p id="fb90" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总之，如果可解释性对你的项目是必不可少的，或者出于某种原因，你可能想保持原来的表示，降维不适合你，你在正确的地方。</p><h2 id="9202" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">特征选择</h2><p id="84fc" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">要素选择方法允许您根据某些标准选择数据集中的最佳要素。听起来很明显，对吧？尽管如此，还是有很多不同的方法，每种方法都以稍微不同的方式定义了一个特性的优点，为了正确地选择最佳特性，您可能需要不止一种方法。</p><p id="06ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，一些术语:特征选择方法可以分为三类:</p><p id="c65b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(1) <strong class="ky ir">过滤方法</strong>:特征选择是预处理的一部分，即我们训练一个模型之前的<em class="mr">。我们根据一些标准过滤掉表现不佳的特性。简单的标准是简单的相关性。这种方法在我们拥有大量数据集时大放异彩，尤其是当我们拥有大量要素时。<a class="ae kv" href="https://arxiv.org/abs/2111.12140" rel="noopener ugc nofollow" target="_blank"> Hoph和Reifenrath (2021) </a>在他们的工作中发现(这将在下一节深入讨论)</em></p><blockquote class="ms mt mu"><p id="9d31" class="kw kx mr ky b kz la jr lb lc ld ju le mv lg lh li mw lk ll lm mx lo lp lq lr ij bi translated">..三个属性使得[filter]方法看起来特别适合某些数据集场景。这些是对噪声的鲁棒性、抵消类别不平衡的成本敏感性以及考虑用于检测冗余特征的特征组合(多变量方法)。</p></blockquote><p id="107f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(2) <strong class="ky ir">包装器方法:</strong>迭代选择一个特征子集，训练你的模型，选择最佳组合。正如您已经想到的，这种方法非常昂贵，而且几乎不切实际。</p><p id="ea0f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(3) <strong class="ky ir">嵌入方法:</strong>不要与<a class="ae kv" href="https://datascience.stackexchange.com/questions/53995/what-does-embedding-mean-in-machine-learning" rel="noopener ugc nofollow" target="_blank">嵌入</a>相混淆，矢量表示单词。嵌入式方法利用了算法中<em class="mr">嵌入</em>的特征重要性估计。例如，随机森林选择不同的特征子集，以避免过度拟合。模型定型后，我们可以查看有多少特征使预测性能受益。然而，就像在分类任务中一样，仅依赖于嵌入式方法，我们可能会使特征选择过度适应给定的算法，限制我们使用不同的算法和不同的数据集，特别是在使用像决策树这样的高方差方法时。</p><p id="cdeb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">方法(2)和(3)成本更高。他们需要一个训练有素的模型，并隐含地假设我们可以使用全部数据来训练一个模型，但情况并非总是如此。下图显示了不同特征选择方法之间的区别。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/213d066a899c86c1590118b5c326e356.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*y8pzeagjsmYYAXEW.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4:监督特征选择方法和技术的扩展分类法，<a class="ae kv" href="https://medium.com/@dannybutvinik" rel="noopener"> Danny Butvinik </a>，此处<a class="ae kv" href="https://medium.com/analytics-vidhya/feature-selection-extended-overview-b58f1d524c1c" rel="noopener">为</a>。</p></figure><p id="53be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的介绍到此结束，可以开始处理今天让我们来到这里的紧迫问题了。</p><h1 id="4641" class="mz lt iq bd lu na nb nc lx nd ne nf ma jw ng jx md jz nh ka mg kc ni kd mj nj bi translated">过滤方法</h1><p id="ab47" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">过滤方法基本上是根据特定标准来衡量功能性能。有些功能在某些设置下会很出色，而在其他设置下则表现不佳。因此，使用多种标准并整合特征分数对于全面掌握我们的数据至关重要。我尽量做到兼容并蓄，从文献中收集了不同的过滤方法([ <a class="ae kv" href="https://arxiv.org/abs/2111.12140" rel="noopener ugc nofollow" target="_blank"> 1 </a>，[ <a class="ae kv" href="https://www.sciencedirect.com/science/article/pii/S016794731930194X?via%3Dihub#sec2.4" rel="noopener ugc nofollow" target="_blank"> 2 </a>，[ <a class="ae kv" href="https://www.aaai.org/Papers/ICML/2003/ICML03-111.pdf" rel="noopener ugc nofollow" target="_blank"> 3 </a>，[ <a class="ae kv" href="https://www.mdpi.com/1999-5903/12/3/54" rel="noopener ugc nofollow" target="_blank"> 4 </a>)。最终，我以这篇长文章结束，由于我想添加一些代码和示例，所以我必须过滤掉一些过滤方法(见我在那里做了什么？).换句话说，如果您想对不同的技术有更深刻的理解，我建议您浏览一下本文中的适度的文献综述。</p><h2 id="1677" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">过滤方法—方法:)</h2><p id="be93" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">Bommert等人[ <a class="ae kv" href="https://www.sciencedirect.com/science/article/pii/S016794731930194X?via%3Dihub#sec2.4" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]将基于滤波器的特征选择器分为单变量、多变量、基于MI的方法和基于Relief的方法。我在这里也会这样做。</p><p id="1387" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所涉及的大多数方法都可以通过FSM来使用，这可以在我的Github <a class="ae kv" href="https://github.com/DavidHarar/FSM" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="2178" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">单变量方法</strong></p><p id="ef7d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据特征与结果变量的关系对特征进行单独排序，不考虑其他变量。单变量方法可以使用统计检验，如ANOVA，或使用预测性能，如AUC，通过一次仅使用一个特征来预测结果。</p><p id="d888" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mr">方差分析</em><strong class="ky ir">【ANOVA】</strong>:ANOVA特征选择器使用F统计量作为每个特征的得分。一般来说，F统计量询问给定特征的均值在不同类别的结果变量之间有多远。更详细的解释可以在<a class="ae kv" href="https://datascience.stackexchange.com/users/8878/kasra-manshaei" rel="noopener ugc nofollow" target="_blank"> Kasra Manshaei </a>的回答<a class="ae kv" href="https://datascience.stackexchange.com/questions/74465/how-to-understand-anova-f-for-feature-selection-in-python-sklearn-selectkbest-w" rel="noopener ugc nofollow" target="_blank">这里</a>，或者在<a class="ae kv" rel="noopener" target="_blank" href="/anova-for-feature-selection-in-machine-learning-d9305e228476">这篇</a>专门的博文中找到。简而言之，在区分不同类别时，一个特性越好，这个特性的F值也就越大。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/35fd76094cfa7787aaeec71c581144cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6B289CWGxrU83VsY.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">每个特征的类之间的间隔，X轴上的X和Y轴上的Y，<a class="ae kv" href="https://datascience.stackexchange.com/questions/74465/how-to-understand-anova-f-for-feature-selection-in-python-sklearn-selectkbest-w" rel="noopener ugc nofollow" target="_blank">源</a></p></figure><p id="5bc4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，使用sklearn实现非常简单。</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="6775" class="ls lt iq nm b gy nq nr l ns nt"><strong class="nm ir">from</strong> <strong class="nm ir">sklearn.feature_selection</strong> <strong class="nm ir">import</strong> <a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest" rel="noopener ugc nofollow" target="_blank">SelectKBest</a>, <a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif" rel="noopener ugc nofollow" target="_blank">f_classif</a><br/>anova_filter = <a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest" rel="noopener ugc nofollow" target="_blank">SelectKBest</a>(<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif" rel="noopener ugc nofollow" target="_blank">f_classif</a>, k=3)</span></pre><p id="514e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以及一个使用有限状态机的示例:</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="52b4" class="ls lt iq nm b gy nq nr l ns nt">selectors = FSM.FSM(k=20, filler = -1)<br/>selectors.anova_inference(X_,y_)</span><span id="9625" class="ls lt iq nm b gy nu nr l ns nt"># Results</span><span id="e46b" class="ls lt iq nm b gy nu nr l ns nt"># array(['member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv',<br/>#        'int_rate', 'annual_inc', 'dti', 'inq_last_6mths',<br/>#        'mths_since_last_record', 'pub_rec', 'revol_util', 'out_prncp',<br/>#        'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv',<br/>#        'total_rec_prncp', 'total_rec_late_fee', 'recoveries',<br/>#        'collection_recovery_fee', 'last_pymnt_amnt'], dtype=object)</span></pre><p id="6e04" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"><em class="mr">Kruskal</em></strong><em class="mr">:</em>对每个特征应用Kruskal-Wallis秩和检验[ <a class="ae kv" href="https://www.sciencedirect.com/topics/mathematics/rank-sum-test" rel="noopener ugc nofollow" target="_blank"> 5 </a> ]，这是方差分析的非参数等价(即，它不假设我们的特征是正态分布的)。在我们计算Kruskal统计量之前，我们首先必须从最小到最大排列我们的观察值(不考虑它们相应的结果)。然后，我们对每个I类中的等级求和。然后，克鲁斯卡尔统计量由下式给出</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/0836e83ce3b639ee1eeec3ab89adaf6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*2J8-3cHURWyGh731bnHO9w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">克鲁斯卡尔试验</p></figure><p id="448d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中T是类(I)内秩的总和，n_i是属于类(I)的观测值的数目，n是观测值的总数。</p><p id="0bee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，该统计值越高，意味着不同类别的对应特征值越不同。我们可以使用scipy实现Kruskal Wallis测试。</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="ef66" class="ls lt iq nm b gy nq nr l ns nt"><strong class="nm ir">from</strong> scipy <strong class="nm ir">import</strong> stats<br/>stats<strong class="nm ir">.</strong>kruskal<strong class="nm ir">(</strong>x_j<strong class="nm ir">,</strong> y<strong class="nm ir">)</strong></span></pre><p id="da86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在有限状态机中:</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="3fcb" class="ls lt iq nm b gy nq nr l ns nt">selectors = FSM.FSM(k=20, filler = -1)<br/>selectors.kruskal_inference(X_,y_)</span><span id="5b12" class="ls lt iq nm b gy nu nr l ns nt"># Results:</span><span id="92f6" class="ls lt iq nm b gy nu nr l ns nt"># Index(['recoveries', 'collection_recovery_fee', 'out_prncp_inv', 'out_prncp',<br/>#        'delinq_2yrs', 'pub_rec', 'total_rec_late_fee', 'acc_now_delinq',<br/>#        'collections_12_mths_ex_med', 'inq_last_6mths', 'policy_code',<br/>#        'revol_bal', 'dti', 'last_pymnt_amnt', 'total_rec_int',<br/>#        'total_rec_prncp', 'total_pymnt', 'total_pymnt_inv', 'member_id',<br/>#        'installment'],<br/>#       dtype='object')</span></pre><p id="f81a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="mr">卡方</em> </strong>:以χ2统计值作为得分。</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="122b" class="ls lt iq nm b gy nq nr l ns nt"><strong class="nm ir">from</strong> <strong class="nm ir">sklearn.feature_selection</strong> <strong class="nm ir">import</strong> <a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest" rel="noopener ugc nofollow" target="_blank">SelectKBest</a>, chi2<br/>chi2_filter = SelectKBest(chi2, k=2)</span></pre><p id="9314" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在有限状态机中:</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="34ca" class="ls lt iq nm b gy nq nr l ns nt">selectors = FSM.FSM(k=20, filler = -1)<br/>selectors.chi2(X_,y_)</span><span id="fde3" class="ls lt iq nm b gy nu nr l ns nt"># Results:</span><span id="5ac5" class="ls lt iq nm b gy nu nr l ns nt"># array(['member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv',<br/>#        'int_rate', 'installment', 'annual_inc', 'mths_since_last_record',<br/>#        'revol_bal', 'revol_util', 'out_prncp', 'out_prncp_inv',<br/>#        'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp',<br/>#        'total_rec_int', 'total_rec_late_fee', 'recoveries',<br/>#        'collection_recovery_fee', 'last_pymnt_amnt'], dtype=object)</span></pre><p id="2061" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">多元方法</strong></p><p id="9a53" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mr">最大关联最小冗余(</em><strong class="ky ir"><em class="mr">MRMR</em></strong><em class="mr">):</em>2005年推出，最近重新流行起来[ <a class="ae kv" rel="noopener" target="_blank" href="/mrmr-explained-exactly-how-you-wished-someone-explained-to-you-9cf4ed27458b"> 6 </a> ]。MRMR算法是一种贪婪的迭代算法。根据下面的规则，每次迭代选择最佳的特性，并将其添加到先前选择的特性中。它被称为最大相关性-最小冗余，因为在每次迭代中，我们都希望选择与目标变量最大相关的特征，以及与先前迭代中所选特征最小冗余的特征。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/a934fa3c0586803017223fd513d5a269.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*y31Pxf3hSRctaNYX.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由MRMR vs博鲁塔选择的特写，由<a class="ae kv" href="https://medium.com/@mazzanti.sam?source=post_page-----9cf4ed27458b-----------------------------------" rel="noopener">塞缪尔·马赞蒂</a>制作的人物，取自此处<a class="ae kv" rel="noopener" target="_blank" href="/mrmr-explained-exactly-how-you-wished-someone-explained-to-you-9cf4ed27458b"/>。在这个例子中，年龄(整数)机械地依赖于年龄；智商和父母的智商有很大的相关性，还有伴侣的身高和自己的身高。考虑到特征之间的高度相关性，通过分母归一化，MRMR将导致上述三个特征，这三个特征用红线圈出。</p></figure><p id="67d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然有许多方法来计算MRMR，我只介绍FCQ方法。访问[ <a class="ae kv" href="https://arxiv.org/pdf/1908.05376.pdf" rel="noopener ugc nofollow" target="_blank"> 7 </a> ]了解更多策略。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/26bf3ca8711035c1e536684a212193fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*424PdeNzXPmcah4q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图由<a class="ae kv" href="https://medium.com/@mazzanti.sam?source=post_page-----9cf4ed27458b-----------------------------------" rel="noopener">塞缪尔·马赞蒂</a>所作，取自<a class="ae kv" rel="noopener" target="_blank" href="/mrmr-explained-exactly-how-you-wished-someone-explained-to-you-9cf4ed27458b">此处</a>。</p></figure><p id="17c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在第<em class="mr"> i </em>次迭代中特征<em class="mr"> f </em>的相关性(分子)被计算为特征和目标变量之间的<a class="ae kv" href="https://en.wikipedia.org/wiki/F-test" rel="noopener ugc nofollow" target="_blank"> F统计量</a>。冗余(分母)计算为该特征和在先前迭代中选择的所有特征之间的皮尔逊相关的绝对值的平均值。</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="5404" class="ls lt iq nm b gy nq nr l ns nt">from mrmr import mrmr_classif<br/>data = pd.read_csv('/Users/davidharar/Downloads/data_loan.csv')<br/>X = data._get_numeric_data().drop(['default_ind'], axis = 1)<br/>y = data['default_ind']</span><span id="410a" class="ls lt iq nm b gy nu nr l ns nt">selected_features = mrmr_classif(X, y, K = 14)<br/># K is the size of the desired subset.</span><span id="a02c" class="ls lt iq nm b gy nu nr l ns nt">selected_features</span></pre><p id="9d96" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">预测性能</strong></p><p id="2687" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一种介于过滤方法和嵌入方法之间的中间地带。在这种方法下，我们迭代地运行一个模型，比方说一个随机森林，一次只有一个特征。然后，我们根据熟悉的KPI(如AUC和准确性)对我们的功能进行排名。AUC通常是首选的，因为它对不平衡的数据集更有弹性，并且通过多种阈值选择进行评估。AUC的缺点是它仅适用于二元预测。</p><p id="e7d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">相互信息</strong></p><p id="3ce5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">互信息是两个特征之间相互依赖的度量。与皮尔逊相关不同，它不限于单调关系。<em class="mr">互信息决定了联合分布与两个边际分布的乘积有多大的不同</em>。回想一下，如果两个变量是独立的，它们的联合分布等于它们的边际分布的乘积。</p><p id="6f08" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">熵是另一个重要的结构，它来源于互信息。<a class="ae kv" href="https://www.sciencedirect.com/topics/computer-science/entropy-measure" rel="noopener ugc nofollow" target="_blank">熵测量</a>变量的不确定性。当所有可能的值以大约相同的概率出现时，熵是高的，如果出现的概率非常不同，熵是低的。它与互信息相关—高互信息值表明不确定性大大降低，因为当我们认识到一个变量的值时，我们对另一个变量的期望值有相对较高的确定性。如果互信息为零，那么两个随机变量是<em class="mr">独立的，我们无法推断出一个给定另一个的值的合理性。</em></p><p id="ba1c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们能够计算互信息或熵之前，连续特征必须被离散化。T11】</p><p id="851e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">设X和Y是两个离散变量，具有(经验的)<a class="ae kv" href="https://www.sciencedirect.com/topics/computer-science/probability-mass-function" rel="noopener ugc nofollow" target="_blank">概率质量函数</a>s。Y的(无条件的)熵由下式给出</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/70dd517ca9ff1fe7fd710fe2391e9b89.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*VbBBheUBbT-ntQuupUF-bA.png"/></div></figure><p id="9b2f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">给定X，Y的条件熵由下式给出</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/3bb16c5b64ac1de6f79d492a93b6885b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HT8bZEmokWVeY-shlppePA.png"/></div></div></figure><p id="00c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">交互信息可以由下面的等式表示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/6afc7c7c6d88c74c2d174f1760847122.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*B7azcVZHnblc2-GX3Fi5oA.png"/></div></figure><p id="1c61" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上述等式的证明可以在<a class="ae kv" href="https://en.wikipedia.org/wiki/Mutual_information" rel="noopener ugc nofollow" target="_blank">这里</a>找到，这在上述等式的上下文中给出了交互信息的极好直觉:</p><blockquote class="ms mt mu"><p id="49f8" class="kw kx mr ky b kz la jr lb lc ld ju le mv lg lh li mw lk ll lm mx lo lp lq lr ij bi translated">..如果熵H(Y)被视为关于随机变量的不确定性的度量，那么H(Y|X)是X <em class="iq">没有说关于Y的</em>的度量。这是“在X已知之后关于Y剩余的不确定性的量”，因此这些等式中的第二个等式的右侧可以被解读为“Y中的不确定性的量减去X已知之后Y中剩余的不确定性的量”。</p></blockquote><p id="2c1a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">几种方法使用上述理论结构并探索信息的概念，以及不同特征之间的冗余。在这一部分，我们将讨论其中的一些。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/ccc558f20d2b4a0dbd852c5cba18d9a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*H_njlmhkXf97Kcxm.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Venn_diagram" rel="noopener ugc nofollow" target="_blank">维恩图</a>显示与相关变量X和Y相关的各种信息测量的加法和减法关系，来源:<a class="ae kv" href="https://en.wikipedia.org/wiki/Mutual_information" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="8e0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">基于互信息得分的贪婪方法</strong></p><p id="2648" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="mr">在我们开始之前，请注意以下大多数算法的计算复杂度取决于(有时是二次)所需的特征数量。大多数实现包括用于期望数量的特征的指定参数。探索它，为自己节省大量时间。</em>T11】</strong></p><p id="ab30" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mr">联合互信息</em>:尽管它不是一种贪婪的或迭代的方法，我还是把它加在这里，因为你会不时地看到一些文章使用JMI作为特征选择的方法。JMI有些天真，他提醒人们使用简单的相关性进行过滤。我们只计算目标和我们的每个特征之间的联合互信息，然后我们保留k个最高的。<strong class="ky ir">在JMIM下实施</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/561c0aff4128f32df44a6f8183f35e7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*89Ok8Y1o3puXooE8_M1e5Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">JMI功能排名，<a class="ae kv" href="https://www.nature.com/articles/s41598-021-00854-x" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="864b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mr">联合互信息最大化(</em><strong class="ky ir"><em class="mr">JMIM</em></strong><em class="mr">)和归一化联合互信息最大化(</em><strong class="ky ir"><em class="mr">NJ mim</em></strong><em class="mr">)</em>[<a class="ae kv" href="https://www.sciencedirect.com/science/article/pii/S0957417415004674" rel="noopener ugc nofollow" target="_blank">8</a>】<em class="mr">:</em>作为MRMR和CMIM，这两种算法对与目标互信息最大化的特征做贪婪的搜索。JMIM和CMIM的主要区别在于，后者最大化候选特征<em class="mr">对预选特征</em>的信息量，而JMIM选择最大化新特征+预选特征与目标特征的联合互信息的特征。这里的主要观点是JMIM也考虑了特征的冗余，而CMIM只是试图最大化加入的互信息。</p><p id="904d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="mr">关于实现的注意事项</em> </strong> <em class="mr"> : </em>找到一个详细的实现并不容易，但我很幸运地在这里找到了这个伟大的<a class="ae kv" href="https://danielhomola.com/feature%20selection/phd/mifs-parallelized-mutual-information-based-feature-selection-module/#going-parallel" rel="noopener ugc nofollow" target="_blank"/>。<code class="fe oc od oe nm b">mifs</code>包允许<code class="fe oc od oe nm b">JMI, JMIM</code>和<code class="fe oc od oe nm b">MRMR</code>。在我的实验中，相应子部分的<code class="fe oc od oe nm b">MRMR</code>实现更快，总体来说<code class="fe oc od oe nm b">JMI</code>比<code class="fe oc od oe nm b">JMIM</code>更快。最后，当你设置软件包时，记得把<code class="fe oc od oe nm b">from sklearn.feature_selection.base import SelectorMixin</code>改成<code class="fe oc od oe nm b">from sklearn.feature_selection._base import SelectorMixin</code>。除此之外，<code class="fe oc od oe nm b">mifs</code>灵活清晰。</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="4b86" class="ls lt iq nm b gy nq nr l ns nt">import mifs</span><span id="00b9" class="ls lt iq nm b gy nu nr l ns nt"># define MI_FS feature selection method<br/>feat_selector = mifs.MutualInformationFeatureSelector(method = "JMIM", n_jobs=8)</span><span id="a41b" class="ls lt iq nm b gy nu nr l ns nt"># Few important params for MutualInformationFeatureSelector:<br/># 1. method, self explenatory, could be "JMI", "JMIM" and "MRMR"<br/># 2. n_features: desired number of features after filtering.<br/># 3. n_jobs: How many CPUs should run parallelly.<br/># <br/># Important attributs:<br/># 1. feat_selector.get_support(): Boolean vector of length p that<br/>#    indicates whether a feature has been selected.<br/># 2. feat_selector.ranking_: Vector of length p that is ordered by<br/>#    importance, for example [52,1,53,...] indicates that feature 52 <br/>#    was the most important one.</span><span id="2a42" class="ls lt iq nm b gy nu nr l ns nt"># Both JMI and JMIM arn't compatible with NaNs<br/>X_filled = X.fillna(-1)<br/>feat_selector.fit(X_filled, y)</span><span id="2642" class="ls lt iq nm b gy nu nr l ns nt"># call transform() on X to filter it down to selected features<br/>X_filtered = feat_selector.transform(X_filled)<br/>X_filtered = pd.DataFrame(X_filtered, columns = X_filled.columns[feat_selector.ranking_])</span></pre><p id="3257" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mr">基于快速相关性的过滤(</em><strong class="ky ir"><em class="mr">【FCBF】)</em></strong>:一种基于相关性的过滤方法，考虑了特征之间的冗余。如果某个特征和目标之间的相关性足够高，使其与预测相关，则该特征适合于分类任务。它和任何其他相关特征之间的相关性没有达到任何其他相关特征可以预测它的水平。由于线性相关性有一些主要的局限性，FCBF的相关性是基于熵，特别是对称不确定性，苏。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/58e90f89ef97291d614ae031331733c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*c2x8Oa7gNHCZQft7-Us9Yw.png"/></div></figure><p id="aeca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该算法的工作原理如下:</p><ol class=""><li id="0bce" class="of og iq ky b kz la lc ld lf oh lj oi ln oj lr ok ol om on bi translated"><em class="mr">相关特征</em>:首先，我们为我们的每个p特征计算SU。我们选择安苏门槛。让我们假设安苏分数高于阈值的特征集合。让我们按降序排列这些功能。</li><li id="35eb" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated"><em class="mr">冗余特征</em>:从第一个排序后的特征开始，我们为s中的l计算SU(j，l)，如果SU(j，l) ≥SU(target，j)，我们从s中移除l，被移除的特征称为“特征j的冗余对等体”</li></ol><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="c0bf" class="ls lt iq nm b gy nq nr l ns nt">selectors = FSM.FSM(k=20, filler = -1)<br/>selectors.fcbf(X_,y_)</span><span id="9613" class="ls lt iq nm b gy nu nr l ns nt"># Results: (fcbf has a lower bound on SU rather than k features. The default is 0).</span><span id="ffa3" class="ls lt iq nm b gy nu nr l ns nt"># selectors = FSM.FSM(k=20, filler = -1)<br/># selectors.fcbf(X_,y_)</span></pre><p id="5970" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mr">条件互信息最大化</em><strong class="ky ir"><em class="mr">【CMIM】</em></strong>:基于特征与目标变量的联合互信息选择特征的贪婪算法。给定预先选择的特征，每次迭代选择与目标<em class="mr">共享最大互信息的特征。也就是说，我们选择与目标变量尚未描述的部分互信息最大化的特征，而不考虑冗余(即忽略当前特征与预选特征之间的高相关性)。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/0ccfc12debcbfe7d5815cb44208b1411.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*wvlOX4HAjFDQP9Li4JdHwg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">CMIM功能排名，<a class="ae kv" href="https://www.nature.com/articles/s41598-021-00854-x" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="7d05" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mr">关于实现的说明</em> : <a class="ae kv" href="https://github.com/jundongl/scikit-feature" rel="noopener ugc nofollow" target="_blank"> scikit-feature </a>是用Python 2编写的。在<a class="ae kv" href="https://github.com/CharlesGaydon/Fast-CMIM" rel="noopener ugc nofollow" target="_blank">这里</a>的作者将大部分的utils函数转换成Python 3，所以我最终使用了它的utils以及稍微修改过的<a class="ae kv" href="https://github.com/jundongl/scikit-feature" rel="noopener ugc nofollow" target="_blank"> scikit-feature </a>原始函数。代码和工作示例在附带的笔记本中。</p><p id="2339" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用有限状态机实现:</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="d545" class="ls lt iq nm b gy nq nr l ns nt">selectors = FSM.FSM(k=20, filler = -1)<br/>selectors.cmim(X_,y_)</span><span id="43f0" class="ls lt iq nm b gy nu nr l ns nt"># Results:</span><span id="858b" class="ls lt iq nm b gy nu nr l ns nt"># Index(['member_id', 'funded_amnt_inv', 'installment', 'open_acc', 'revol_util',<br/>#        'total_pymnt', 'delinq_2yrs', 'annual_inc', 'dti', 'revol_bal',<br/>#        'total_acc', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int',<br/>#        'total_rec_late_fee', 'recoveries', 'inq_last_6mths',<br/>#        'mths_since_last_record', 'collections_12_mths_ex_med',<br/>#        'mths_since_last_major_derog'],<br/>#       dtype='object')</span></pre><p id="af06" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mr">双输入对称相关(</em><strong class="ky ir"><em class="mr">DISR</em></strong><em class="mr">)</em>[<a class="ae kv" href="https://medium.com/r?url=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FGianluca-Bontempi%2Fpublication%2F220867410_On_the_Use_of_Variable_Complementarity_for_Feature_Selection_in_Cancer_Classification%2Flinks%2F09e4150588ffc5cd32000000%2FOn-the-Use-of-Variable-Complementarity-for-Feature-Selection-in-Cancer-Classification.pdf" rel="noopener">9</a>]:DISR依赖于这样的性质，即变量的组合可以返回比每个单独变量产生的信息总和更多的输出类信息。在DISR下，每个要素的JMI根据给定要素与其他要素的互补程度进行归一化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/db58d677905e40cc8b5555dbab1937ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*CLe0eC50Gn57hQJkBO5s6A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">DISR专题排名，<a class="ae kv" href="https://www.nature.com/articles/s41598-021-00854-x" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="1631" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用有限状态机实现:</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="ebeb" class="ls lt iq nm b gy nq nr l ns nt">selectors = FSM.FSM(k=20, filler = -1)<br/>selectors.disr(X_,y_)</span><span id="8c43" class="ls lt iq nm b gy nu nr l ns nt"># Results:</span><span id="28b9" class="ls lt iq nm b gy nu nr l ns nt"># Index(['member_id', 'dti', 'recoveries', 'collection_recovery_fee',<br/>#        'collections_12_mths_ex_med', 'mths_since_last_major_derog',<br/>#        'policy_code', 'annual_inc_joint', 'total_rec_late_fee',<br/>#        'total_rec_prncp', 'dti_joint', 'acc_now_delinq', 'tot_coll_amt',<br/>#        'tot_cur_bal', 'total_pymnt', 'total_pymnt_inv', 'revol_bal',<br/>#        'open_acc_6m', 'open_il_6m', 'open_il_12m'],<br/>#       dtype='object')</span></pre><p id="bdc7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mr">基于互信息的特征选择(</em><strong class="ky ir"><em class="mr"/></strong><em class="mr">)</em><a class="ae kv" href="https://www.researchgate.net/publication/3301850_Using_Mutual_Information_for_Selecting_Features_in_Supervised_Neural_Net_Learning" rel="noopener ugc nofollow" target="_blank">10</a>】<em class="mr">:</em>使用最佳特征w.r.t. I(i，target)的贪婪选择和对预选特征冗余的惩罚。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/2a07882c4c18662f1083fab82963ba31.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*ro2OZyMEshUeWwyZU9Usqg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">MIFS功能排名，<a class="ae kv" href="https://www.nature.com/articles/s41598-021-00854-x" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="1d19" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这一系列基于信息的特征选择方法的其他发展包括自适应MIFS (AMIFS) [ <a class="ae kv" href="https://ieeexplore.ieee.org/document/1379918" rel="noopener ugc nofollow" target="_blank"> 11 </a> ]，条件MIFS (CMIFS) [ <a class="ae kv" href="https://www.semanticscholar.org/paper/Conditional-Mutual-Information%E2%80%90Based-Feature-for-Cheng-Qin/ed090e62f2df266ddef21d1b9fb9156c4a3f62ed" rel="noopener ugc nofollow" target="_blank"> 12 </a> ]，以及更多方法，由于篇幅限制，这些方法留给读者作为练习。</p><p id="6566" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用有限状态机实现MIFS:</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="385e" class="ls lt iq nm b gy nq nr l ns nt">selectors = FSM.FSM(k=20, filler = -1)<br/>selectors.mifs(X_,y_)</span><span id="4421" class="ls lt iq nm b gy nu nr l ns nt"># Results:</span><span id="7656" class="ls lt iq nm b gy nu nr l ns nt"># Index(['member_id', 'collections_12_mths_ex_med',<br/>#        'mths_since_last_major_derog', 'policy_code', 'annual_inc_joint',<br/>#        'dti_joint', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal',<br/>#        'open_acc_6m', 'open_il_6m', 'open_il_12m', 'open_il_24m',<br/>#        'mths_since_rcnt_il', 'total_bal_il', 'il_util', 'open_rv_12m',<br/>#        'open_rv_24m', 'max_bal_bc', 'all_util'],<br/>#       dtype='object')</span></pre><p id="33af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">基于救济的方法</strong></p><p id="e5eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Relief [ <a class="ae kv" href="https://www.sciencedirect.com/science/article/pii/B9781558602472500371" rel="noopener ugc nofollow" target="_blank"> 13 </a>于1992年引入，用于二进制目标的特征选择，自从ReliefF将其推广到多类目标后，它开始流行。RReliefF是一种基于Relief的方法，用于回归[ <a class="ae kv" href="https://link.springer.com/content/pdf/10.1023/A:1025667309714.pdf" rel="noopener ugc nofollow" target="_blank"> 14 </a> ]、I-RELIEF、Tuned ReliefF、VLSReliefF、SURF和ReliefSeq。这里有一个很好的关于RBMs的调查。原始算法如下。假设我们有p个不同的特征和一个二元目标。然后，我们开始初始化一个权重向量w，长度为p，全为0。然后，对于B次，我们</p><ul class=""><li id="8f62" class="of og iq ky b kz la lc ld lf oh lj oi ln oj lr ow ol om on bi translated">从我们的数据中随机抽取一个例子，比如从0班。</li><li id="d462" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ow ol om on bi translated">找出欧氏距离最接近的例子，属于同一类。我们称这个例子为“近击”类似地，我们取最接近的点，它属于对立的类，称之为“接近失误”</li><li id="9fc3" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ow ol om on bi translated">更新w:对于m次迭代和p个特征，我们按下式更新w_j。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/14507a7ae638d29d4ae98f347898ff6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*sJ4tO4jly9Gp0Uu26itzlA.png"/></div></figure><p id="27b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，我们通过每个示例与同类的最近示例的(平方)距离来惩罚特征j(由w_j表示)的重要性。也就是说，Relief更喜欢稀疏度较低的要素，并且同一类的示例之间距离较近的要素受到的惩罚会更少。相反，Relief赋予来自一个类的示例离属于另一个类的示例更远的特征更高的重要性。</p><p id="a44b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与通常的KNN估计器一样，只考虑几个邻居可能会导致有噪声的预测和过度拟合。ReliefF算法除了将ReliefF推广到多类目标之外，还在估计重要性时纳入了来自k个邻居的信息。一般来说(对其他RBA也是如此)，在某种程度上，更多的邻居导致更准确的分数，但是需要更长的时间来收敛。</p><p id="d673" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与以前的方法相比，这种方法完全是非参数化的，并且与以前依赖于信息论的方法相比，Relief的重要性来自于距离的概念。</p><p id="99ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">I-RELIEF建议对整个命中和未命中实例集使用实例加权函数，而不是只对每个类的关闭实例使用加权函数。SURF采用距离阈值<em class="mr"> T </em>来将实例定义为邻居(其中<em class="mr"> T </em>等于数据中所有实例对之间的平均距离)。然而，与迭代救济相反，SURF对定义为邻居的所有实例使用相同的实例权重<a class="ae kv" href="https://www.sciencedirect.com/science/article/pii/S1532046418301400#b0195" rel="noopener ugc nofollow" target="_blank">【15】</a>。还有其他变体，如VLSReliefF和ReliefSeq，这里不做介绍。这篇中篇文章对基于救济的方法做了更深入的介绍。</p><p id="fe24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然它们不相同，但是基于起伏的方法与用于特征选择的拉普拉斯方法有相似之处。拉普拉斯方法也假设相同类别的观测值倾向于彼此更接近。在拉普拉斯特征选择中，我们通过测量任意距离和计算权重矩阵来将数据嵌入到最近邻居的图中。然后，我们计算每个特征的拉普拉斯准则，并获得这样一个性质，即最小值对应于最重要的维度。然而，在实践中，当选择特征子集时，通常使用不同的聚类算法(k-means方法)，在该算法的帮助下，选择最有效的组[ <a class="ae kv" href="https://raevskymichail.medium.com/feature-selection-overview-of-everything-you-need-to-know-598c53c01d46" rel="noopener">源</a> ]。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/3be1f7f58665882652ada7f09c888cfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*LfW12Zec7N26I6k0yTUfXA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">三个版本的浮雕，由作者绘制。上图展示了在不同的地貌算法下，哪些邻域被考虑在内。左上面板显示了我们在上面介绍过的基本版本。在这个版本中，当更新权重时，我们仅采用最接近的接近命中和接近未命中情况。右上面板显示了ReliefF，它考虑了k个不同的邻居，在本例中为4个。底部面板显示了在SURF/SURF*算法下更新权重时考虑的邻居。在这些算法中，我们不是考虑特定数量的邻居，而是定义一个边界，通过该边界将比它更近的邻居考虑在内。</p></figure><p id="09dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于起伏算法的两点注意事项:</p><p id="4bd6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">python中有几个Relief(及其变体)的实现(例如，<a class="ae kv" href="https://gitlab.com/moongoal/sklearn-relief" rel="noopener ugc nofollow" target="_blank"> sklearn-relief </a>，<a class="ae kv" href="https://medium.com/@yashdagli98/feature-selection-using-relief-algorithms-with-python-example-3c2006e18f83" rel="noopener"> 16 </a>)，但这个似乎是最详细的<a class="ae kv" href="https://epistasislab.github.io/scikit-rebate/using/" rel="noopener ugc nofollow" target="_blank"> scikit-rebate </a>。或许RBA如此受欢迎的原因之一是其灵活性。RBA兼容分类和连续特征的混合，以及多类分类和回归。然而，由于他们依赖于距离(和KNN)，他们深受维数灾难之苦。用户指南说:</p><blockquote class="ms mt mu"><p id="4a4e" class="kw kx mr ky b kz la jr lb lc ld ju le mv lg lh li mw lk ll lm mx lo lp lq lr ij bi translated">在非常大的特征空间中，用户可以预期基于核心起伏的算法分数在自己运行时变得不太可靠。这是因为随着特征空间变得非常大，最近邻居的确定变得更加随机。因此，在非常大的特征空间(例如&gt; 10，000个特征)中，用户应考虑将基于核心起伏的算法与迭代方法(如TuRF(此处实现)或VLSRelieF或迭代起伏)相结合。</p></blockquote><p id="bff2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">和</p><blockquote class="ms mt mu"><p id="54da" class="kw kx mr ky b kz la jr lb lc ld ju le mv lg lh li mw lk ll lm mx lo lp lq lr ij bi translated">当扩展到大数据问题时，请记住，使折扣方法速度最慢的数据方面是训练实例的数量，因为救济算法与功能的数量成线性比例，但与训练实例的数量成二次比例。这是基于起伏的方法需要计算距离数组(即训练数据集中实例之间的所有成对距离)的结果。如果您有大量可用的训练实例，那么在运行任何折扣方法时，可以考虑利用该数据集的类平衡随机采样来节省内存和计算时间。</p></blockquote><p id="92c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">而对于基于Relief的算法的具体时间复杂度，参见[ <a class="ae kv" href="https://arxiv.org/pdf/1711.08421.pdf" rel="noopener ugc nofollow" target="_blank"> 15 </a>中的表5。</p><p id="ac08" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用有限状态机实现:</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="6340" class="ls lt iq nm b gy nq nr l ns nt">selectors = FSM.FSM(k=20, filler = -1)<br/>selectors.ReliefF(X_,y_)<br/>selectors.SURF(X_,y_)<br/>selectors.SURFstar(X_,y_)<br/>selectors.MultiSURF(X_,y_)</span><span id="f9f1" class="ls lt iq nm b gy nu nr l ns nt"># Note: X_ and y_ have N=10,000, which may lead to a long computation time. </span></pre><h2 id="dc55" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">其他方法</h2><p id="7f8d" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">虽然我们已经讨论了大量的算法，但我们只能触及皮毛。我们还没有介绍用于特征选择的无监督算法[ <a class="ae kv" href="https://ieeexplore.ieee.org/document/8315427" rel="noopener ugc nofollow" target="_blank"> 17 </a>和[ <a class="ae kv" href="https://ieeexplore.ieee.org/abstract/document/7592899" rel="noopener ugc nofollow" target="_blank"> 18 </a> ]，用于特征选择的谱方法[ <a class="ae kv" href="https://www.routledge.com/Spectral-Feature-Selection-for-Data-Mining/Zhao-Liu/p/book/9781138112629" rel="noopener ugc nofollow" target="_blank"> 19 </a> ]，以及基于图的方法[ <a class="ae kv" href="https://ieeexplore.ieee.org/document/9357067" rel="noopener ugc nofollow" target="_blank"> 20 </a>，<a class="ae kv" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.422.9135&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank"> 21 </a>和[ <a class="ae kv" href="https://drive.google.com/file/d/1Jyzcc8fCQDBEyq6q2jWRnSLgQuufvXFx/view" rel="noopener ugc nofollow" target="_blank"> 22 </a> ]。其他过滤方法包括Burrata、信息片段(IF)、SOA等。正如标题所说，这篇文章是为懒惰的数据科学家写的。如果你不懒，我希望你会对这篇文章的参考书目感兴趣。</p><h1 id="b23c" class="mz lt iq bd lu na nb nc lx nd ne nf ma jw ng jx md jz nh ka mg kc ni kd mj nj bi translated">综合</h1><p id="397f" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">好的，如果你已经到这里了，你可能有一个问题一直困扰着你——你尝试了不同的方法，得到了不同的分数，这些分数不一定彼此一致。你该怎么办？</p><p id="ee7f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据[ <a class="ae kv" href="https://www.sciencedirect.com/science/article/pii/S1566253518303440" rel="noopener ugc nofollow" target="_blank"> 23 </a> ]，当集合不同的选择方法时，我们必须考虑以下几点:</p><ul class=""><li id="4269" class="of og iq ky b kz la lc ld lf oh lj oi ln oj lr ow ol om on bi translated">使用哪种方法和多少种方法？他们发现越多越好。</li><li id="84fa" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ow ol om on bi translated">要使用的训练集的数量和大小—我们应该对数据进行子集划分吗？自举？正如我们在上面看到的，我们的一些算法在观察次数上是二次的。总体来说，答案是<em class="mr">是的；我们应该重新采样并选择特征。最终，我们可以将这些信息与下面的评分方法结合起来。</em></li><li id="7314" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ow ol om on bi translated">我们最终想要多少功能？这个数字可以定义为我们拥有的或之前被决定的所有特征的百分比。</li><li id="f2f5" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ow ol om on bi translated">如何评估聚合方法？我将用这一部分来回答这个问题。</li></ul><p id="7624" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设您有不同的选定功能列表，您可以执行以下一项或多项操作。</p><ol class=""><li id="8ed2" class="of og iq ky b kz la lc ld lf oh lj oi ln oj lr ok ol om on bi translated">根据选择规则将你的电子邮件列表合并成一个列表(如下例)。然后，使用组合列表训练一个模型。</li><li id="c343" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">使用特征选择器集成—在不同的“瘦”模型中训练E，稍后，当新的数据集到达时，将通过使用E个不同的模型预测结果来进行预测。</li><li id="7ff5" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">介于1和2之间的中间点——根据E '不同的规则，将E个特征列表转换为E '组合特征列表，然后训练E' ( &lt;<e models="" as="" an="" ensemble.=""/></li></ol><p id="db7b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">对不同列表中的特征进行排序</strong>)可通过以下方式完成</p><ul class=""><li id="6938" class="of og iq ky b kz la lc ld lf oh lj oi ln oj lr ow ol om on bi translated">Min:给每个元素分配它所达到的最小(最好)等级。</li><li id="2426" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ow ol om on bi translated">中位数:为每个元素分配它所达到的中位数等级。</li><li id="3326" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ow ol om on bi translated">算术平均数:给每个元素分配它所达到的等级的平均数。</li><li id="785c" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ow ol om on bi translated">几何平均值:给每个元素分配它所达到的等级的几何平均值。</li></ul><p id="b711" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以使用投票[<a class="ae kv" href="https://medium.com/@vatvenger/combining-feature-selection-methods-cdb4ae5be979" rel="noopener">24</a>]:</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="edcb" class="ls lt iq nm b gy nq nr l ns nt">counter = Counter(features_selected_lists) # list of lists<br/>features_voted = [k for k, v in counter.items() if v &gt;= min_votes]</span></pre><p id="73cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然FSM没有告诉您如何对您的特征进行排序，但它附带了一个方便的推理函数，该函数使用bootstrap并返回一个特征列表，该列表根据每个分类器和每个采样的重要性进行排序。所有的预处理(离散化、规范化(例如，回想一下chi2不能处理负值)，以及编码)都在内部完成。在下面的例子中，我只使用了too抽样，每个样本有100个观察值。</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="bb32" class="ls lt iq nm b gy nq nr l ns nt">selectors = FSM.FSM(k=20, filler = -1)<br/>results = selectors.Bootstrapper(X_, y_, B=2,Sample_size=100)</span><span id="fc8c" class="ls lt iq nm b gy nu nr l ns nt"># Results:</span><span id="7a29" class="ls lt iq nm b gy nu nr l ns nt"># {'ANOVA': [array(['loan_amnt', 'int_rate', 'annual_inc', 'delinq_2yrs',<br/>#          'inq_last_6mths', 'mths_since_last_delinq',<br/>#          'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal',<br/>#          'total_acc', 'out_prncp', 'out_prncp_inv', 'total_pymnt',<br/>#          'total_pymnt_inv', 'total_rec_prncp', 'total_rec_late_fee',<br/>#          'recoveries', 'collection_recovery_fee', 'last_pymnt_amnt'],<br/>#         dtype=object),<br/>#   array(['member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv',<br/>#          'int_rate', 'annual_inc', 'inq_last_6mths',<br/>#          'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal',<br/>#          'revol_util', 'out_prncp', 'out_prncp_inv', 'total_pymnt',<br/>#          'total_pymnt_inv', 'total_rec_prncp', 'recoveries',<br/>#          'collection_recovery_fee', 'last_pymnt_amnt'], dtype=object)],<br/>#  'chi2': [array(['member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv',<br/>#          'annual_inc', 'mths_since_last_delinq', 'mths_since_last_record',<br/>#          'open_acc', 'revol_bal', 'total_acc', 'out_prncp', 'out_prncp_inv',<br/>#          'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp',<br/>#           ...</span></pre><p id="3cec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">如何评估一组特征选择器</strong></p><p id="a5aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通常，我们会查看三个KPI:<em class="mr">多样性</em> —例如在集成决策树中，我们不需要K个不同的树来给出总是相同的结果，我们不需要K个不同的选择器来选择相同的特征，而不管数据如何。另一个KPI是稳定性——我们必须确保集成将为相似的数据集返回相似的排名。最后是您的KPI(可能是准确性、AUC或其他)。使用冗余特征可能会导致更差的性能，我们应该期待使用所选子集和更快的训练会有更好的结果。</p><p id="f818" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">多样性</strong> Kuncheva和Whitaker [ <a class="ae kv" href="https://link.springer.com/article/10.1023/A:1022859003006" rel="noopener ugc nofollow" target="_blank"> 25 </a> ]提出了测量特征选择器集合多样性的附加测量的综合调查。设置是我们有两个或更多的训练模型，其中每一个都是在另一个特征子集上训练的。我们也有他们的预测，有些是正确的，有些是不正确的，有些是相同的，有些是不同的。他们提出并比较了10种多样性度量:Q统计量、相关系数、不一致度量、双故障度量、Kohavi-Wolpart方差、Interrator一致性、熵度量、难度度量、广义多样性和重合故障多样性。然而，由于这篇文章变得太长了，我将检查一下Q统计，并将其余的留给好奇的读者。</p><p id="8922" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Q统计是成对检验。给定两个分类器，我们有两个Di预测，长度为N(等于观察的数量)。我们交叉标记了他们的预测，并使用了以下符号:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oz"><img src="../Images/aca5457a3aef350fdbc6d19040d5ba79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nCGkjU1jCOdwYCf76Fq8RA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">两个不同分类器预测的交叉表。表作者。</p></figure><p id="d4c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那么一对分类器的Q统计量将是</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/0b03ef8aa82e47403209f23382d3e073.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*eDlQxEp4uFYqW6fDzadx7g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">一对分类器的q统计量，由作者提供。</p></figure><p id="19d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们有L个不同的分类器，那么我们将返回平均Q统计量:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/2f08a87ccac7ab8035a2b0d1b7705971.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*jiAC5GXFYLoVUHwtSFkyJg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">L个不同分类器的平均Q统计量，由作者提供。</p></figure><p id="6f1d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于统计上独立的分类器，Q(i，j)的期望值是0。q在1和1之间变化。倾向于正确识别相同对象的分类器将具有正的Q值，而那些在不同对象上出错的分类器将使Q为负。</p><p id="1fb0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">稳定性</strong>确保集成将为相似的数据集返回相似的排名。</p><p id="8716" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">计算稳定性有几种方法(例如[ <a class="ae kv" href="https://dl.acm.org/doi/10.5555/1295303.1295370" rel="noopener ugc nofollow" target="_blank"> 26 </a> ]，和[ <a class="ae kv" href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.816.8821&amp;rep=rep1&amp;type=pdf#page=22" rel="noopener ugc nofollow" target="_blank"> 27 </a> ])。我将从[ <a class="ae kv" href="https://jmlr.org/papers/volume18/17-514/17-514.pdf" rel="noopener ugc nofollow" target="_blank"> 28 </a>开始呈现一个更新的。使用[ <a class="ae kv" href="https://arxiv.org/pdf/2111.12140.pdf" rel="noopener ugc nofollow" target="_blank"> 29 </a> ]演示:假设我们有p个原始特征，我们想从中选择总共k个特征。我们将数据分成m个不同的子集(或从中重新取样)。将h_j定义为特征j被选为整个m个子集中最重要的k个特征之一的次数。将q定义为p个特征的总和(即，它计算选择特征1+选择特征2+…+选择特征p的次数)，然后如下</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pc"><img src="../Images/831ba8ea3b0060ab72799876036eeba6.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*PPcsvQee-zzkNIkHHQttlw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Nogueira等人(2018年)，来自Hopf和Reifenrath 2021年的公式</p></figure><p id="70db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">是完全定义的、严格单调的、有上下限的特征选择集合的稳定性度量，以及当所有特征集相等时它的最大值。</p><p id="d172" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">性能</strong>是我们这里的主要目标，使用冗余功能或不重要的功能会降低我们模型的性能。我们可以通过查看选择下的KPI，并与输入所有变量的基本场景进行比较，来衡量选择方法的成功。[ <a class="ae kv" href="https://medium.com/@vatvenger/combining-feature-selection-methods-cdb4ae5be979" rel="noopener"> 24 </a> ]进行t检验，比较选择后模型和没有选择后模型的KPI。</p><h2 id="2893" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">文献学</h2><ol class=""><li id="1da8" class="of og iq ky b kz ml lc mm lf pd lj pe ln pf lr ok ol om on bi translated">霍普夫，康斯坦丁和萨沙·莱芬拉斯。"监督机器学习应用中特征选择的过滤方法——综述和基准测试."arXiv预印本arXiv:2111.12140  (2021)。</li><li id="204e" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">《高维分类数据中特征选择的过滤方法的基准》<em class="mr">计算统计&amp;数据分析</em> 143 (2020): 106839。</li><li id="666c" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">余、雷、。“高维数据的特征选择:一种快速的基于相关性的过滤器解决方案。”<em class="mr">第20届机器学习国际会议论文集(ICML-03) </em>。2003.</li><li id="a220" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">皮尔宁斯基尼基塔和伊万·斯梅坦尼科夫。"作为python数据分析工具之一的特征选择算法."<em class="mr">未来互联网</em> 12.3 (2020): 54。</li><li id="80ba" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">谢尔登·m·罗斯<em class="mr">统计学导论</em>。学术出版社，2017。</li><li id="252c" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">“MRMR”准确地解释了你希望别人如何向你解释。</li><li id="a190" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">赵，陈振宇，拉迪卡·阿南德和马洛里·王。"营销机器学习平台的最大相关性和最小冗余特征选择方法."<em class="mr"> 2019 IEEE数据科学与高级分析国际会议(DSAA) </em>。IEEE，2019。</li><li id="e142" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">Bennasar，Mohamed，Yulia Hicks和Rossitza Setchi。"使用联合互信息最大化的特征选择."<em class="mr">专家系统与应用</em>42.22(2015):8520–8532。</li><li id="031b" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">迈耶、帕特里克·e和吉安卢卡·邦坦皮。"在癌症分类中使用可变互补性进行特征选择."<em class="mr">进化计算应用研讨会</em>。施普林格，柏林，海德堡，2006。</li><li id="6858" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">罗伯托·巴蒂蒂。"使用互信息在监督神经网络学习中选择特征."IEEE神经网络汇刊5.4(1994):537–550。</li><li id="f802" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">特斯默、米歇尔和巴勃罗·埃斯特韦斯。" AMIFS:利用互信息的自适应特征选择."2004年IEEE国际神经网络联合会议。№04CH37541) 。第一卷。IEEE，2004年。</li><li id="66d0" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">基于条件互信息的协同和冗余特征选择分析。Etri期刊33.2(2011):210–218。</li><li id="5c5f" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">基拉，肯吉和拉里·a·伦德尔。"一种实用的特征选择方法."《机器学习学报》1992年。摩根·考夫曼，1992年。249–256.</li><li id="5605" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">rob Nik-iko nja、Marko和Igor Kononenko。"减免的理论和实证分析."机器学习53.1(2003):23–69。</li><li id="2042" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">基于地形的特征选择:介绍和评论。<em class="mr">生物医学信息学杂志</em>85(2018):189–203。</li><li id="7455" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">Yash Dagli，使用起伏算法的要素选择与python示例。，中等。</li><li id="b365" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">陈，，等，〈基于分层特征加权的监督特征选择方法〉。<em class="mr"> IEEE访问</em>6(2018):15087–15098。</li><li id="56a7" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">王、石平、朱威廉。"稀疏图嵌入无监督特征选择."<em class="mr"> IEEE系统、人和控制论汇刊:系统</em>48.3(2016):329–341。</li><li id="61cb" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">赵，郑阿兰，。<em class="mr">用于数据挖掘的光谱特征选择</em>。泰勒弗朗西斯，2012年。</li><li id="b195" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">一种新的图形特征选择方法。<em class="mr"> 2020年第六届IEEE信息科学与技术大会(CiSt) </em>。IEEE，2021。</li><li id="827a" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">张志宏和埃德温·r·汉考克。“基于图形的特征选择方法。”<em class="mr">模式识别中基于图形表示的国际研讨会</em>。施普林格，柏林，海德堡，2011。</li><li id="f018" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">胡，荣耀，等。“无监督特征选择的图自表示方法”<em class="mr">神经计算</em>220(2017):130–137。</li><li id="2530" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">博隆-卡内多、韦尔尼卡和安帕罗·阿朗索-贝当佐斯。"特征选择的集成:回顾和未来趋势."<em class="mr">信息融合</em>52(2019):1–12。</li><li id="dfe9" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">Cohen，Cohen和Atlas，结合特征选择方法，<a class="ae kv" href="https://medium.com/@vatvenger?source=post_page-----cdb4ae5be979--------------------------------" rel="noopener"> vatvengers </a>，Medium。</li><li id="0698" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">昆切娃，柳德米拉和克里斯托弗·惠特克。"分类器集成中多样性的测量及其与集成精度的关系."<em class="mr">机器学习</em>51.2(2003):181–207。</li><li id="cc53" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">《特征选择的稳定性指数》<em class="mr">人工智能及应用</em>。2007.</li><li id="69e3" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">排名列表中的堪培拉距离。<em class="mr">NIPS 09研讨会进展会议录</em>。Citeseer，2009年。</li><li id="8daa" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">诺盖拉、萨拉、康斯坦丁诺斯·塞希迪斯和加文·布朗。"特征选择算法的稳定性."j .马赫。学习。第18.1号决议(2017年):6345至6398。</li><li id="c8f5" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">霍普夫，康斯坦丁和萨沙·莱芬拉斯。"监督机器学习应用中特征选择的过滤方法——综述和基准测试."<em class="mr"> arXiv预印本arXiv:2111.12140 </em> (2021)。</li></ol></div></div>    
</body>
</html>