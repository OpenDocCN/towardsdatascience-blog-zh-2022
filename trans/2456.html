<html>
<head>
<title>Understanding The Hyperplane Of scikit-learn’s SVC Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解scikit-learn的SVC模型的超平面</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-the-hyperplane-of-scikit-learns-svc-model-f8515a109222#2022-05-27">https://towardsdatascience.com/understanding-the-hyperplane-of-scikit-learns-svc-model-f8515a109222#2022-05-27</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="df43" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">如何解释scikit-learn中线性SVC的coef_ attribute以解决二元分类问题</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/7c82342f2f53fe526d1099a0d485c798.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*upX73PtTYpJ4BDtg"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">由<a class="ae kz" href="https://unsplash.com/@lisawentwandering?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Lisa vanthornout</a>在<a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="7cfd" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这篇文章将教你如何解释scikit-learn的SVC的<code class="fe lw lx ly lz b">coef_</code>和<code class="fe lw lx ly lz b">intercept_</code>属性，以及如何使用它们来预测新的数据点。</p><p id="f514" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我最近完成了一个项目，其中我必须用c语言部署一个SVC。我用Python训练了一个SVC，以便用高级语言完成寻找超平面的繁重工作，然后我从那个模型中提取必要的值。</p><p id="f97a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在这个过程中，我发现要准确理解如何解释<code class="fe lw lx ly lz b"><strong class="lc iv">coef_</strong></code>和<code class="fe lw lx ly lz b"><strong class="lc iv">intercept_</strong></code>属性中的值有点困难，所以这正是我将在这篇文章中向您展示的。</p><blockquote class="ma mb mc"><p id="b025" class="la lb md lc b ld le jv lf lg lh jy li me lk ll lm mf lo lp lq mg ls lt lu lv in bi translated"><strong class="lc iv">注意:</strong>这篇文章将<strong class="lc iv">而不是</strong>包括SVC背后的所有数学细节，相反，它旨在让您在使用scikit-learn的模型时，对正在发生的事情有一个直观和实际的理解。</p></blockquote></div><div class="ab cl mh mi hy mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="in io ip iq ir"><h2 id="6f09" class="mo mp iu bd mq mr ms dn mt mu mv dp mw lj mx my mz ln na nb nc lr nd ne nf ng bi translated">预测函数</h2><p id="e6ae" class="pw-post-body-paragraph la lb iu lc b ld nh jv lf lg ni jy li lj nj ll lm ln nk lp lq lr nl lt lu lv in bi translated">安装SVC意味着我们正在解决一个优化问题。换句话说，我们试图最大化超平面和不同标签的支持向量之间的间隔。一旦找到最佳边缘和超平面<strong class="lc iv">，我们可以使用以下等式来预测新数据点的标签:</strong></p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nm"><img src="../Images/cd11bc5de350d4a1d83642038ab461d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/0*mKridzq3XjQEMhLY.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">预测标签为1的等式</p></figure><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nn"><img src="../Images/2b88abe690026d656483bdfb30b1ebdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/0*vrx77EGQFbJoE78o.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">预测标签as -1的方程式</p></figure><p id="951c" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">其中<code class="fe lw lx ly lz b"><strong class="lc iv">w</strong></code>是来自拟合模型的<code class="fe lw lx ly lz b">coef_</code>属性的系数。<code class="fe lw lx ly lz b"><strong class="lc iv">x</strong></code> <strong class="lc iv"> </strong>是我们要分类的新数据点的向量。<code class="fe lw lx ly lz b"><strong class="lc iv">b</strong></code>是我们从模型的<code class="fe lw lx ly lz b">intercept_</code>属性中得到的一个偏差项。</p><p id="6371" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">请记住，一个单一的超平面本质上只是一条线，因此它只能分类两类，一边一个。数学上我们可以用1和-1来表示，(或者1和0，这并不重要)，如上面的等式所示。</p><p id="0c11" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">该等式的工作方式如下:我们取系数和新点的点积，然后加上偏差。如果结果大于或等于0，那么我们将新点分类为标签1。否则，如果结果低于0，那么我们将新点分类为标签-1。</p></div><div class="ab cl mh mi hy mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="in io ip iq ir"><h2 id="c688" class="mo mp iu bd mq mr ms dn mt mu mv dp mw lj mx my mz ln na nb nc lr nd ne nf ng bi translated">示例:二元问题的SVC</h2><p id="37dd" class="pw-post-body-paragraph la lb iu lc b ld nh jv lf lg ni jy li lj nj ll lm ln nk lp lq lr nl lt lu lv in bi translated">为了演示我们刚刚看到的数学，并初步了解如何从拟合模型中提取系数，我们来看一个例子:</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="no np l"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">使用scikit-learn的SVC的二进制分类scnario的代码示例—由作者创建</p></figure><p id="c77f" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">上面的代码片段创建了一些虚拟数据点，这些数据点显然是线性可分的，并被分成两个不同的类。在将SVC拟合到变量<code class="fe lw lx ly lz b">clf</code>中的数据后，还绘制了数据点和带有支持向量的超平面。这是结果图:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nq"><img src="../Images/54b1a01a7341a7588b82fce87ecad460.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*C31yqs_5_fl6jN2waBsSMg.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">二元分类问题的数据点和超平面图——由作者创建</p></figure><blockquote class="ma mb mc"><p id="3725" class="la lb md lc b ld le jv lf lg lh jy li me lk ll lm mf lo lp lq mg ls lt lu lv in bi translated"><strong class="lc iv">注意</strong> : <code class="fe lw lx ly lz b">sklearn.inspection.DecisionBoundaryDisplay</code>非常酷，可以用来绘制二分类问题(两个标签)的超平面和支持向量。</p></blockquote><p id="2c9e" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">现在让我们来看看之前装配好的<code class="fe lw lx ly lz b">clf</code>模型的<code class="fe lw lx ly lz b">coef_</code>和<code class="fe lw lx ly lz b">intercept_</code>属性。</p><pre class="kk kl km kn gu nr lz ns nt aw nu bi"><span id="0223" class="mo mp iu lz b gz nv nw l nx ny">print(clf.coef_)<br/>print(clf.intercept_)</span><span id="92eb" class="mo mp iu lz b gz nz nw l nx ny">&gt;&gt; [[ 0.39344262 -0.32786885]] #This is the <strong class="lz iv">w</strong> from the equation<br/>&gt;&gt; [-0.1147541]                #This is the <strong class="lz iv">b </strong>from the equation</span></pre><p id="21f6" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我们将很快回来，但首先让我们介绍两个新的数据点，我们将分类。</p><pre class="kk kl km kn gu nr lz ns nt aw nu bi"><span id="313c" class="mo mp iu lz b gz nv nw l nx ny">new_point_1 = np.array([[-1.0, 2.5]])<br/>new_point_2 = np.array([[2, -2.5]])</span><span id="85d3" class="mo mp iu lz b gz nz nw l nx ny">plt.scatter(new_point_1[:, 0], new_point_1[:, 1], c='blue', s=20)<br/>plt.scatter(new_point_2[:, 0], new_point_2[:, 1], c='red', s=20)</span><span id="bc7d" class="mo mp iu lz b gz nz nw l nx ny">plt.show()</span></pre><p id="cd3a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">如果我们继续执行帖子中显示的第一个代码要点，那么我们会得到下面的图，其中包括两个新的蓝色数据点(<code class="fe lw lx ly lz b">new_point_1</code>，左上角)和红色数据点(<code class="fe lw lx ly lz b">new_point_2</code>，右下角)。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nq"><img src="../Images/642c1a2f6b0dfbe9d8a12130d6ce16f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*nbCeXtlUqIsWdGxMabbFOQ.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">二元分类问题的数据点、超平面和两个新点的绘图—由作者创建</p></figure><p id="68c9" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">使用拟合的模型，我们可以通过调用<code class="fe lw lx ly lz b">predict</code>函数对这些点进行分类。</p><pre class="kk kl km kn gu nr lz ns nt aw nu bi"><span id="f5b7" class="mo mp iu lz b gz nv nw l nx ny">print(clf.predict(new_point_1))<br/>print(clf.predict(new_point_2))</span><span id="74fb" class="mo mp iu lz b gz nz nw l nx ny">&gt;&gt; [0] #Purple (result is less than 0)<br/>&gt;&gt; [1] #Yellow (result is greater than or equal to 0)</span></pre></div><div class="ab cl mh mi hy mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="in io ip iq ir"><h2 id="3c01" class="mo mp iu bd mq mr ms dn mt mu mv dp mw lj mx my mz ln na nb nc lr nd ne nf ng bi translated">模拟预测功能的手动计算</h2><p id="7b28" class="pw-post-body-paragraph la lb iu lc b ld nh jv lf lg ni jy li lj nj ll lm ln nk lp lq lr nl lt lu lv in bi translated">为了进行分类，该模型使用了我们之前看到的等式。我们可以“手动”计算一下，看看是否得到相同的结果。</p><p id="4388" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">提醒:<br/> <code class="fe lw lx ly lz b">coef_</code>被<code class="fe lw lx ly lz b">[[ 0.39344262 -0.32786885]]<br/>intercept_</code>被<code class="fe lw lx ly lz b">[-0.1147541]<br/>new_point_1</code>被<code class="fe lw lx ly lz b">[[-1.0, 2.5]]<br/>new_point_2</code>被<code class="fe lw lx ly lz b">[[2.0, -2.5]]</code></p><p id="c2e1" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">计算点积并加上偏差可以这样完成:</p><pre class="kk kl km kn gu nr lz ns nt aw nu bi"><span id="912c" class="mo mp iu lz b gz nv nw l nx ny">print(np.dot(clf.coef_[0], new_point_1[0]) + clf.intercept_)<br/>print(np.dot(clf.coef_[0], new_point_2[0]) + clf.intercept_)</span><span id="479f" class="mo mp iu lz b gz nz nw l nx ny">&gt;&gt; [-1.32786885] #Purple (result is less than 0) <br/>&gt;&gt; [1.49180328]  #Yellow (result is greater than or equal to 0)</span></pre><p id="9508" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">瞧啊。我们进行与预测函数相同的分类。</p></div><div class="ab cl mh mi hy mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="in io ip iq ir"><p id="08eb" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我希望这一点很清楚，容易理解。这并不是对SVC模型如何工作的深入研究，但足以在进行分类时获得对正在发生的事情的基本理解。</p><p id="4f49" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">当分类问题不是二元而是多类时，事情变得更加复杂。我会写一篇后续文章，解释如何解释这种模型的系数。</p></div><div class="ab cl mh mi hy mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="in io ip iq ir"><p id="134d" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">如果您有任何反馈或问题，请随时联系我。</p><p id="d01e" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">感谢阅读！</p></div></div>    
</body>
</html>