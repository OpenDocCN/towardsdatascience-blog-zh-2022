<html>
<head>
<title>Eigen Intuitions: Understanding Eigenvectors and Eigenvalues</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征直觉:理解特征向量和特征值</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/eigen-intuitions-understanding-eigenvectors-and-eigenvalues-630e9ef1f719#2022-06-27">https://towardsdatascience.com/eigen-intuitions-understanding-eigenvectors-and-eigenvalues-630e9ef1f719#2022-06-27</a></blockquote><div><div class="fc if ig ih ii ij"/><div class="ik il im in io"><div class=""/><div class=""><h2 id="e923" class="pw-subtitle-paragraph jo iq ir bd b jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dk translated">理解所有事物“本征”的直观基础</h2></div><h1 id="3f2b" class="kg kh ir bd ki kj kk kl km kn ko kp kq jx kr jy ks ka kt kb ku kd kv ke kw kx bi translated">动机</h1><p id="2eab" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">我们通常希望转换数据以减少特征的数量，同时尽可能多地保留方差(即样本之间的差异)。通常，你会听到人们提到主成分分析(PCA)和奇异值分解(SVD)，但如果不首先了解什么是特征向量和特征值，我们就无法理解这些方法是如何工作的。</p><h1 id="91a3" class="kg kh ir bd ki kj kk kl km kn ko kp kq jx kr jy ks ka kt kb ku kd kv ke kw kx bi translated">语源</h1><p id="3610" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">“特征向量”是一个非常奇怪的词。就像很多怪异的词(想想幼儿园)一样，我们可以责怪德国人。我听过的最有用的翻译来自Coursera课程“<a class="ae lu" href="https://www.coursera.org/specializations/mathematics-machine-learning?utm_source=gg&amp;utm_medium=sem&amp;utm_campaign=01-CourseraCatalog-DSA-US&amp;utm_content=B2C&amp;campaignid=9918777773&amp;adgroupid=128025796489&amp;device=c&amp;keyword=&amp;matchtype=&amp;network=g&amp;devicemodel=&amp;adpostion=&amp;creativeid=536102998481&amp;hide_mobile_promo&amp;gclid=Cj0KCQjwqPGUBhDwARIsANNwjV6fBy-ygiEp6GI17yQhyi_JW8NYJuJEvVrYf3KJcrhLsAg0GZZhYsUaAnUTEALw_wcB" rel="noopener ugc nofollow" target="_blank">机器学习专业化的数学</a>”:“eigen”的意思是“特性”。</p><p id="6e89" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">“特征向量”是“表征”线性变换的向量。</p><h1 id="14a8" class="kg kh ir bd ki kj kk kl km kn ko kp kq jx kr jy ks ka kt kb ku kd kv ke kw kx bi translated">视觉直觉</h1><p id="0bfc" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">让我们看看任意线性变换下的两个向量，如平移、缩放、旋转和剪切。我们的大部分矢量都被移动了。不过，有些在转换前后指向相同的方向。</p><p id="08b2" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">让我们来看一个简单的水平缩放！我们可以用线性变换矩阵[[2，0]，[0，1]]来实现这一点。</p><figure class="mb mc md me gu mf gi gj paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gi gj ma"><img src="../Images/c778a90feea1847e80710f69bc22cd95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*BZskyls6Yw3zO8WclAVn6A.gif"/></div></div><p class="mm mn gk gi gj mo mp bd b be z dk translated"><strong class="bd mq">图1 </strong>:通过水平缩放可视化三个矢量。图片作者。</p></figure><p id="afab" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">如果我们绘制三个单位长度向量——一个在0 °,一个在45 °,一个在90°—并想象应用变换矩阵后会发生什么，我们会看到一些向量仍然指向与之前相同的方向(0°和90 °),而其他向量则不指向相同的方向(45°)。有趣的是，这两个向量前后都指向同一个方向。在线性变换下，这些向量仅由一个标量项进行缩放。我们在90°处的单位矢量没有改变，即缩放1，而我们在0°处的单位矢量加倍。这些向量就是我们的特征向量！</p><blockquote class="mr"><p id="0529" class="ms mt ir bd mu mv mw mx my mz na lt dk translated">线性变换的特征向量是那些保持指向相同方向的向量。对于这些向量，变换矩阵的作用只是标量乘法。对于每个特征向量，特征值是向量在变换下缩放的标量。</p></blockquote><h1 id="efd2" class="kg kh ir bd ki kj kk kl km kn ko kp kq jx nb jy ks ka nc kb ku kd nd ke kw kx bi translated">数学</h1><p id="3ef8" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">绘制一堆矢量并等待动画渲染并不是一个非常有效的方法。幸运的是，我们需要做的只是将我们已经建立的直觉形式化。有些方程式看起来有点吓人，但是一旦我们理解了它们的来源，它们就没那么糟糕了。</p><p id="e0f3" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">虽然数学可以扩展到任意维数的矩阵，但我们将坚持使用2x2矩阵进行演示。</p><p id="ee27" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">让我们考虑一个线性变换矩阵<em class="ne"> A. </em>如我们所见，矩阵的特征向量是刚刚缩放的向量。这些标量是特征值，我们称它们为<em class="ne"> λ </em>。我们称我们的特征向量为<em class="ne"> x </em>。</p><p id="cc97" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">现在一起…特征向量<em class="ne"> x </em>被我们的特征值<em class="ne"> λ </em>缩放，被我们的矩阵<em class="ne"> A </em>缩放。</p><p id="d00a" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">我们可以用图2 中的顶部等式来形式化。</p><figure class="mb mc md me gu mf gi gj paragraph-image"><div class="gi gj nf"><img src="../Images/64bd4d196bb6617ac1dad8501b83cda8.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*VQO2HyH5ITTvhWUArS7uyw.png"/></div><p class="mm mn gk gi gj mo mp bd b be z dk translated"><strong class="bd mq">图2 </strong>:特征向量和特征值的基本形式化。图片作者。</p></figure><p id="db9f" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">让我们移动一些术语！减去<em class="ne"> λx </em>并分解出<em class="ne"> x </em>给出了一个很好的零值等式。为了从矩阵<em class="ne"> A </em>中减去一个常数(<em class="ne"> λ </em>，我们需要将它乘以一个与<em class="ne"> A </em>维数相同的单位矩阵。</p><p id="e4a8" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">让我们求解我们的特征向量和特征值！我们对这些方程的“琐碎”解不感兴趣，在这些解中,<em class="ne"> x </em>向量的值为零。相反，我们想知道什么时候项<em class="ne"> (A-λI </em>)等于零。我们可以通过检查行列式(<em class="ne">图3 </em>)的值为零来实现这一点！</p><figure class="mb mc md me gu mf gi gj paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gi gj ng"><img src="../Images/db69a882dac7739057d1d810b006841c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F1BcgnLKJB8QL-m246lB4A.png"/></div></div><p class="mm mn gk gi gj mo mp bd b be z dk translated"><strong class="bd mq">图3</strong>:2 x2矩阵行列式的定义。图片作者。</p></figure><p id="ba5b" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">现在让我们展开我们的矩阵<em class="ne"> A </em>，这样我们就可以看到矩阵中的每个值(<em class="ne">图4 </em>)。</p><figure class="mb mc md me gu mf gi gj paragraph-image"><div class="gi gj nh"><img src="../Images/ecf7efb6a068b33e861cbe7cb54d5103.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*lbo0iB_vnnS0_XEumHSKFg.png"/></div><p class="mm mn gk gi gj mo mp bd b be z dk translated"><strong class="bd mq">图4 </strong>:展开矩阵a .作者图片。</p></figure><p id="ed97" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">将这些东西拼凑在一起，我们得到了图5 所示的等式。</p><figure class="mb mc md me gu mf gi gj paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gi gj ni"><img src="../Images/21251c8377df51104fb7f459e8039b12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZKHC_Ew34VI4-_N4jrpdmA.png"/></div></div><p class="mm mn gk gi gj mo mp bd b be z dk translated"><strong class="bd mq">图5 </strong>:检查det( <em class="nj"> A-λI)=0。图片作者。</em></p></figure><p id="7734" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">使用我们从<em class="ne">图3 </em>中定义的行列式，我们可以代入我们的值。</p><figure class="mb mc md me gu mf gi gj paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gi gj ni"><img src="../Images/14aef03fbb109df31c2fb13e55c2a32d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AklopQxYaMUVZ8Ed67ZVig.png"/></div></div><p class="mm mn gk gi gj mo mp bd b be z dk translated"><strong class="bd mq"> <em class="nj">图6 </em> </strong>:将矩阵中的值代入行列式的定义中。图片作者。</p></figure><p id="cf65" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">最后，乘以各项，我们恢复出一种叫做“<a class="ae lu" href="https://en.wikipedia.org/wiki/Characteristic_polynomial" rel="noopener ugc nofollow" target="_blank">特征多项式</a>的形式</p><figure class="mb mc md me gu mf gi gj paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gi gj nk"><img src="../Images/4224e8b20dc7c57da5de1199297c3339.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NmYAC2sb6tLvmJ_TXAZBdw.png"/></div></div><p class="mm mn gk gi gj mo mp bd b be z dk translated"><strong class="bd mq">图7 </strong>:“特征多项式”的定义图片作者。</p></figure><p id="9b80" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">理论上我们只能做到这一步了！现在，让我们应用这个“特征多项式”，求解我们的特征值(<em class="ne"> λ </em>)。</p><figure class="mb mc md me gu mf gi gj paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gi gj nl"><img src="../Images/bab388bf37771426197b62d8288dedcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GOmANUgcNcr2ti93KKNxEQ.png"/></div></div><p class="mm mn gk gi gj mo mp bd b be z dk translated"><strong class="bd mq">图8 </strong>:样本矩阵a .作者图片。</p></figure><p id="e9fc" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">将矩阵中的值代入我们的特征多项式…</p><figure class="mb mc md me gu mf gi gj paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gi gj nm"><img src="../Images/60063ae9e22f85f89a44c8ef909b4401.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4rGNaLSJyFS-61Q8p9m18w.png"/></div></div><p class="mm mn gk gi gj mo mp bd b be z dk translated"><strong class="bd mq">图9 </strong>:求解矩阵a的特征多项式，作者图片。</p></figure><p id="1934" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">我们得到我们的特征值(<em class="ne"> λ </em>)。现在，我们可以代入我们的特征值，来求解我们的特征向量。</p><figure class="mb mc md me gu mf gi gj paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gi gj nn"><img src="../Images/12fba5459e1169d589a2e590047fb1dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5N5O1WdVQxL1Ykqkt1QqZA.png"/></div></div><p class="mm mn gk gi gj mo mp bd b be z dk translated"><strong class="bd mq">图10 </strong>:根据特征值确定我们的特征向量。图片作者。</p></figure><p id="240c" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">这是一个古怪的结果……@<em class="ne">λ</em>= 2<em class="ne">，</em> [0，-x₂] = 0。我们的x₁似乎已经消失了。这是什么意思？</p><p id="41db" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">这意味着对于<em class="ne"> λ </em> = 2，只要x₂为零，x₁可以等于任何值。</p><ul class=""><li id="0e10" class="no np ir la b lb lv le lw lh nq ll nr lp ns lt nt nu nv nw bi translated">例如[5，0]、[1，0]和[-3，0]</li></ul><p id="380b" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">同样，对于<em class="ne"> λ </em> = 1，只要x₁为零，x₂就可以等于任何东西。</p><ul class=""><li id="6dd6" class="no np ir la b lb lv le lw lh nq ll nr lp ns lt nt nu nv nw bi translated">例如，[0，2]，[0，-1]，[0，8]</li></ul><p id="de98" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">我们通过用占位符变量<em class="ne"> t </em>代替可以取任何值的项来表达这种不变性。</p><figure class="mb mc md me gu mf gi gj paragraph-image"><div class="gi gj nx"><img src="../Images/addb8dcf3ee71955aea099ce25ab9b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*a5kyNrDB1mL7SUOr29FdFw.png"/></div><p class="mm mn gk gi gj mo mp bd b be z dk translated"><strong class="bd mq">图11 </strong>:定义我们空间的特征向量。图片作者。</p></figure><p id="dc8a" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">这正是我们的视觉直觉告诉我们的！我们空间的所有水平向量都是本征向量，它们由本征值2缩放。我们空间的所有垂直向量都是本征向量，它们由本征值1缩放。</p><h1 id="3da9" class="kg kh ir bd ki kj kk kl km kn ko kp kq jx kr jy ks ka kt kb ku kd kv ke kw kx bi translated">自己图谋试试！</h1><p id="adf2" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">我发现为自己策划事情很有价值。如果你想试试，看看下面的来源！</p><figure class="mb mc md me gu mf"><div class="bz fq l di"><div class="ny nz l"/></div></figure><h1 id="0c68" class="kg kh ir bd ki kj kk kl km kn ko kp kq jx kr jy ks ka kt kb ku kd kv ke kw kx bi translated">承认</h1><p id="3670" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">重要的是在最明确的地方给予信任！虽然文章中的代码是我的，但是用于可视化的包(<a class="ae lu" href="https://docs.manim.community/" rel="noopener ugc nofollow" target="_blank"> manim </a>)肯定不是！可视化库和解释的方法都是无耻的从<a class="ae lu" href="https://www.youtube.com/c/3blue1brown" rel="noopener ugc nofollow" target="_blank"> 3blue1brown </a>偷来的。</p><p id="5305" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">文章前面提到过，但是我爱这个Coursera系列:<a class="ae lu" href="https://www.coursera.org/specializations/mathematics-machine-learning?utm_source=gg&amp;utm_medium=sem&amp;utm_campaign=01-CourseraCatalog-DSA-US&amp;utm_content=B2C&amp;campaignid=9918777773&amp;adgroupid=128025796489&amp;device=c&amp;keyword=&amp;matchtype=&amp;network=g&amp;devicemodel=&amp;adpostion=&amp;creativeid=536102998481&amp;hide_mobile_promo&amp;gclid=Cj0KCQjwqPGUBhDwARIsANNwjV6fBy-ygiEp6GI17yQhyi_JW8NYJuJEvVrYf3KJcrhLsAg0GZZhYsUaAnUTEALw_wcB" rel="noopener ugc nofollow" target="_blank">机器学习专精数学</a>！</p></div><div class="ab cl oa ob hv oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="ik il im in io"><p id="0c56" class="pw-post-body-paragraph ky kz ir la b lb lv js ld le lw jv lg lh lx lj lk ll ly ln lo lp lz lr ls lt ik bi translated">我很欣赏你的阅读！我希望这有助于解释一个复杂的概念。如果我犯了什么错误或者有什么不清楚的地方，请告诉我！</p></div></div>    
</body>
</html>