<html>
<head>
<title>Are you interpreting your logistic regression correctly?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你对逻辑回归的解释正确吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/are-you-interpreting-your-logistic-regression-correctly-d041f7acf8c7#2022-07-05">https://towardsdatascience.com/are-you-interpreting-your-logistic-regression-correctly-d041f7acf8c7#2022-07-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b173" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">回归系数本身并不能告诉你你需要知道什么</h2></div><blockquote class="kf kg kh"><p id="2d67" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">逻辑回归总是因其可解释性而受到称赞，但在实践中却经常被误解。在这些情况下，通常假设逻辑模型中的系数与多元线性回归模型中的系数工作方式相同。但事实并非如此。下面，我们将详细了解逻辑回归的正确解释以及影响特征效果的机制。</p></blockquote><p id="4dc0" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">大卫·罗克斯比·考克斯在【1958年的论文中引入了逻辑回归来模拟二元结果。今天，它被广泛应用于医学研究、社会科学，当然还有数据科学。当数据科学家开始一个新的分类项目时，逻辑回归往往是我们尝试的第一个模型。我们用它来感受最重要的特征和依赖的方向。之后，如果我们想要获得性能，我们可能会切换到一个不太容易解释的分类器，如梯度增强树或随机森林，或者如果对我们的利益相关者来说能够对模型进行推理是重要的，我们可能会坚持使用逻辑回归。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lj"><img src="../Images/e79af5da56e803a06da50c6e2af2ae1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*txpPE7sUsw9HI-75Gl3fiQ.jpeg"/></div></div><p class="lv lw gj gh gi lx ly bd b be z dk translated"><strong class="bd lz">大卫·罗克斯比·科克斯发明了用于生存分析的逻辑回归和比例风险模型(以他的名字命名为Cox回归)。来源:</strong> <a class="ae li" href="https://commons.wikimedia.org/wiki/File:Nci-vol-8182-300_david_cox.jpg" rel="noopener ugc nofollow" target="_blank"> <strong class="bd lz">维基共享资源</strong> </a> <strong class="bd lz">。</strong></p></figure><p id="0f2d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在线性回归模型中，系数<strong class="kl ir"> β </strong>告诉您当特征改变一个单位时，因变量改变多少个单位。这种变化不依赖于其他特征或其他系数的值，并且系数本身的水平直接描述了变化的幅度。相比之下，在逻辑模型中，因变量对特征变化的响应是一个函数，它取决于特征本身的值、其他特征的值以及模型中的所有其他系数！</p><p id="8f23" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">让我们考虑一个具有两个特征的简单模型的例子。在线性模型中，预测<strong class="kl ir"> ŷᵢ </strong>只是特征值和系数的加权和。为了简化符号，我们将这个加权和缩写为<strong class="kl ir"> μᵢ = β₀ + β₁ x₁ᵢ + β₂ x₂ᵢ </strong>。逻辑回归使用相同的加权和<strong class="kl ir"> μᵢ </strong>，但是将逻辑函数<strong class="kl ir">λ(x)= exp(x)/【1+exp(x)】</strong>包裹在它周围，因此所有预测都在0和1之间，并且可以解释为概率。</p><p id="f4bb" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><strong class="kl ir">线性回归</strong> : <strong class="kl ir"> ŷᵢ= μᵢ </strong> <br/> <strong class="kl ir">逻辑回归</strong>:<strong class="kl ir">ŷᵢ=λ(μᵢ)</strong><br/><br/>一般来说，系数被解释为因变量的变化，这种变化发生在特征的值有小的变化而所有其他特征保持不变的时候。数学上这意味着我们要考虑偏导数。那么下面是两个模型相对于第一个系数的偏导数<strong class="kl ir"> β₁: </strong> <br/> <br/> <strong class="kl ir">线性回归</strong>:<strong class="kl ir">∂ŷᵢ/∂x₁ᵢ=β₁</strong><br/><strong class="kl ir">logistic回归</strong>:<strong class="kl ir">∂ŷᵢ/∂x₁ᵢ=λ(μᵢ)[1-λ(μᵢ】</strong><br/><br/>可以看出，<strong class="kl ir">β\88857;</strong>本身就是线性回归模型中的偏导数。这里没有并发症。与此相反，与系数值<strong class="kl ir"> β₁ </strong>相关的逻辑回归中的边际效应取决于<strong class="kl ir"> μᵢ </strong>并因此取决于<strong class="kl ir"> x₁ᵢ </strong>、<strong class="kl ir"> β₂ </strong>和<strong class="kl ir"> x₂ᵢ </strong>。</p><p id="dba3" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">由于<strong class="kl ir">λ(μᵢ)</strong>和<strong class="kl ir">【1-λ(μᵢ】</strong>都是非负的，因此<strong class="kl ir"> β₁ </strong>的符号决定了边际效应的符号。因此，我们可以从多元线性回归中解释logistic模型中特征的影响方向。只有<strong class="kl ir"> β₁ </strong>的大小不能直接解读。</p><p id="7e2f" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">要了解<strong class="kl ir"> x₁ᵢ </strong>的变化对预测结果的影响有多大，取决于<strong class="kl ir"> x₁ᵢ </strong>本身的水平，<strong class="kl ir">t51】看一下<strong class="kl ir">图1 </strong>，下面显示的是<strong class="kl ir"> x₁ᵢ </strong>对<strong class="kl ir"> β₁ </strong>保持<strong class="kl ir"> β₀、</strong>和<strong class="kl ir"> x₂ᵢ </strong>恒定在<strong class="kl ir"> 1 </strong>的不同值的边际影响在多元线性回归模型中，边际效应与水平无关，因此所有三条线在相应系数的水平上只是一条水平线。相反，我们看到影响的大小要小得多。例如，蓝色曲线的最大值在<strong class="kl ir"> 0.25处，</strong>即使系数<strong class="kl ir"> β₁=1 </strong>。此外，系数越大，非线性越明显。</strong></p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/a7c535344d1ec757daf8a4d06ec978d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*98Ct0hT4MKgml3m7A9bi1w.jpeg"/></div><p class="lv lw gj gh gi lx ly bd b be z dk translated"><strong class="bd lz">图1:x₁的边际效应取决于特征值。图片由作者提供。</strong></p></figure><p id="f3e4" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">下面的图2说明了边际效应对其他特征的系数和值的依赖性。这一次，我们保持<strong class="kl ir"> x₁ᵢ=1 </strong>，但是改变总和<strong class="kl ir"> β₀ + β₂ x₂ᵢ.的值</strong>如前所述，我们可以观察到明显的非线性。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/9cba55e2c4afc57750c1aeb1607873b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*CwQhOBc7vnPyEYmxR8rvEQ.jpeg"/></div><p class="lv lw gj gh gi lx ly bd b be z dk translated"><strong class="bd lz">图2:x₁的边际效应取决于系数和其他特征的总和。图片由作者提供。</strong></p></figure><p id="8baa" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">为了更深入地了解逻辑回归的解释，请记住<strong class="kl ir"> ŷᵢ </strong>是对<strong class="kl ir"> yᵢ </strong>的预测，因此<strong class="kl ir"> </strong>意味着<strong class="kl ir"> ŷᵢ </strong>给出了<strong class="kl ir"> yᵢ=1 </strong>的概率。方程式<strong class="kl ir">ŷᵢ=λ(μᵢ)</strong>可以反过来看</p><p id="1e5e" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">μᵢ=λ⁻(ŷᵢ)= ln(ŷᵢ/(1-ŷᵢ)).</p><p id="49a4" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">这意味着逻辑回归暗示了在<strong class="kl ir"> μᵢ </strong>的特征和<strong class="kl ir">ŷᵢ/(1-ŷᵢ</strong>的比值比的对数之间的线性关系。</p><p id="1449" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">因此，<strong class="kl ir"> μᵢ </strong>中的<strong class="kl ir"> x₁ᵢ </strong>和<strong class="kl ir"> x₂ᵢ </strong>对对数概率(也称logits)的影响直接由系数<strong class="kl ir"> β₁ </strong>和<strong class="kl ir"> β₂.给出不幸的是，对数几率对人类来说有点不直观——所以这并没有提供一个很好的解释基础。</strong></p><h1 id="4ad3" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">如何做好:平均边际效应和平均边际效应</h1><p id="d389" class="pw-post-body-paragraph ki kj iq kl b km mt jr ko kp mu ju kr lf mv ku kv lg mw ky kz lh mx lc ld le ij bi translated">基于<strong class="kl ir">∂ŷᵢ/∂x₁ᵢ=λ(μᵢ)[1-λ(μᵢ)】β₁</strong>与<strong class="kl ir"> μᵢ = β₀ + β₁ x₁ᵢ + β₂ x₂ᵢ </strong>有两种方法可以量化<strong class="kl ir">x≯1</strong>的边际效应。第一个是在<strong class="kl ir"> x₂ᵢ.的平均值上评估导数</strong>这是样本均值的边际效应。这个想法可以随意扩展到任何一组被认为有代表性的值的边际效应——例如中位数或聚类平均值。或者，可以计算样本中每个观察值的偏导数，并报告以这种方式获得的平均值。这是平均边际效应。这两种方法都是有效的——在足够大的样本中，它们应该给出非常相似的结果。</p><p id="fba6" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">确定边际效应的方法很容易获得——例如R中的<a class="ae li" href="https://cran.r-project.org/web/packages/margins/vignettes/Introduction.html" rel="noopener ugc nofollow" target="_blank"> margins </a>包或Python中的<a class="ae li" href="https://www.statsmodels.org/devel/generated/statsmodels.discrete.discrete_model.LogitResults.get_margeff.html" rel="noopener ugc nofollow" target="_blank"> statsmodels </a>和<a class="ae li" href="https://scikit-learn.org/stable/modules/generated/sklearn.inspection.partial_dependence.html#sklearn.inspection.partial_dependence" rel="noopener ugc nofollow" target="_blank"> sklearn </a>。不幸的是，我无法在pyspark.ml中找到实现。</p><p id="2f8b" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">值得一提的一个特例是当一个或多个特征是虚拟变量(或一位热编码)时。在这种情况下，关于特征的导数<strong class="kl ir"> ∂ ŷᵢ / ∂ x₁ᵢ </strong>没有被很好地定义。在这种情况下，我们想要使用的是特征取值1时的预测结果与取值0时的预测结果之间的差异。如前所述，这可以在保持其他特征处于其平均值的同时进行计算，或者可以在取平均值之前对每个观察值进行计算。</p><p id="e24d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在实践中，我们经常使用部分相关图、累积局部效应、LIME或Shapley值等工具来更好地理解和解释我们的模型。这些都是模型不可知的，也适用于逻辑回归——尽管有人可能会认为其中一些对于可以通过边际效应直接解释的东西来说是多余的。然而，这些工具中的每一个都给出了对模型的不同理解，因此它们都可以增强我们的理解。如果不同的模型解释方法对某些特征的作用给出了不同的指示，这通常是特别有启发性的。</p><p id="6ad4" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我个人真的很喜欢边际效应，因为它们给你一个很好的总结，你的每个功能的效果，即使你使用更复杂的转换，如平方项和与其他功能的相互作用。在这些情况下，只有我们同时考虑多个特征，才能解释原始变量的影响。在这种情况下，边际效应延伸得很好。例如，如果我们包含一个平方项，那么<strong class="kl ir"> μᵢ = β₀ + β₁ x₁ᵢ + β₂ x₁ᵢ </strong>我们就有<strong class="kl ir">∂ŷᵢ/∂x₁ᵢ=λ(μᵢ)[1-λ(μᵢ)】(β₁+2β₂x₁ᵢ)</strong>。等式可能有点大，但在实践中，我们仍然可以看到x₁ᵢ对ŷᵢ的影响的一个数字总结。</p><p id="1a20" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">这就是我们关于多元线性回归和逻辑回归中系数解释之间关系的完整循环。简单地将逻辑回归模型的系数解释为边际效应是不正确的，就像我们在多元线性回归模型中所做的一样。但是在实践中，事情很快变得更加复杂，因为你的模型很可能包含多项式项和相互作用项。在这种情况下，多元线性回归和逻辑回归的处理方式是相同的——我们必须计算边际效应来解释它们。</p><p id="18d7" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><strong class="kl ir">参考文献:</strong></p><p id="2dce" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">W.H. Greene (2012年):计量经济学分析，第7版，皮尔森教育。</p><p id="a5b5" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">T.Hastie，R. Tibshirani和J. Friedman (2009):统计学习的要素，第二版，Springer Science+Business Media。</p><div class="my mz gp gr na nb"><a href="https://pub.towardsai.net/how-to-choose-your-loss-function-where-i-disagree-with-cassie-kozyrkov-2038d19b5e0a" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd ir gy z fp ng fr fs nh fu fw ip bi translated">如何选择你的损失函数——我不同意凯西·科兹尔科夫的观点</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">选择正确的损失函数和评估指标对于数据科学项目的成功至关重要。但是…</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">pub.towardsai.net</p></div></div><div class="nk l"><div class="nl l nm nn no nk np lt nb"/></div></div></a></div><div class="my mz gp gr na nb"><a href="https://medium.com/@christian_leschinski/why-you-should-not-try-to-predict-stock-markets-3f7d291b1a29" rel="noopener follow" target="_blank"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd ir gy z fp ng fr fs nh fu fw ip bi translated">为什么你不应该试图预测股票市场</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">我遇到过数量惊人的人，他们试图在某个时候建立一个交易算法。这就是为什么…</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">中等。</p></div></div><div class="nk l"><div class="nq l nm nn no nk np lt nb"/></div></div></a></div></div></div>    
</body>
</html>