<html>
<head>
<title>Apache Spark for Data Science — Hands-On Introduction to Spark SQL</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark for Data Science—Spark SQL实践介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/apache-spark-for-data-science-hands-on-introduction-to-spark-sql-43177fcf741c#2022-04-21">https://towardsdatascience.com/apache-spark-for-data-science-hands-on-introduction-to-spark-sql-43177fcf741c#2022-04-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cc46" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">spark SQL—10分钟内从基础到正则表达式和用户定义函数(UDF)</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3690f54165433fdbb579aaabd99c473b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AIzUBvCCHdE2teXp"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@jonasfrey?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">乔纳斯·弗雷</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="8b5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Spark中的数据帧是<a class="ae ky" href="https://betterdatascience.com/apache-spark-rdd-basics/" rel="noopener ugc nofollow" target="_blank"> RDDs </a>的自然扩展。它们非常类似于您通常在Pandas中看到的数据结构，但是有一个额外的功能——您可以用SQL直接操作它们。没错，您可以使用数据语言在本地和集群上处理大型数据集。</p><p id="843b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天，我们将保持简单，探索Spark SQL的基础知识。您将首先学习如何通过使用函数调用来完全避免SQL。我喜欢尽可能地使用SQL，因此本文的大部分内容将围绕聚合函数、条件逻辑、正则表达式和用户定义的函数展开——所有这些都在SQL中。</p><p id="7aa6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不想看书？请观看我的视频:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lv lw l"/></div></figure></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="3fa7" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">使用的数据集</h1><p id="1f80" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">为了使事情更加简单，我们将使用泰坦尼克号数据集。从<a class="ae ky" href="https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv" rel="noopener ugc nofollow" target="_blank">这个网址</a>下载并保存在你记得的地方:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/13395491bc59dbb2e41a9403690ad74e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XxmoHA1qyexunWHox3AmFA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片1 —泰坦尼克号数据集(图片由作者提供)</p></figure><p id="0029" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该数据集包含的功能比虹膜数据集多得多。对于用户定义的函数和正则表达式来说，这些将会派上用场。但首先，让我们看看如何加载它的火花。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="da51" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">如何用Spark DataFrame API读取CSV文件</h1><p id="9528" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">如果您在笔记本环境中使用PySpark，请始终使用此代码片段以获得更好的输出格式。否则，如果一次在屏幕上看到的列太多，数据帧可能会溢出:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="5cec" class="nh mf it nd b gy ni nj l nk nl">from IPython.core.display import HTML<br/>display(HTML("&lt;style&gt;pre { white-space: pre !important; }&lt;/style&gt;"))</span></pre><p id="371c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解决了这个问题后，我们可以初始化一个新的Spark会话:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="395d" class="nh mf it nd b gy ni nj l nk nl">from pyspark.sql import SparkSession<br/></span><span id="84ef" class="nh mf it nd b gy nm nj l nk nl">spark = SparkSession.builder.appName("spark-sql").getOrCreate()</span></pre><p id="dd3e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要读取CSV文件，只需指定<code class="fe nn no np nd b">read</code>模块的<code class="fe nn no np nd b">csv()</code>函数的路径。每当读取CSV文件时，<code class="fe nn no np nd b">inferSchema</code>和<code class="fe nn no np nd b">header</code>参数是强制性的。如果没有它们，Spark会将所有数据类型转换为string，并将标题行视为实际数据:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="32e3" class="nh mf it nd b gy ni nj l nk nl">titanic = spark.read.csv(<br/>    path="file://&lt;dataset-path&gt;", <br/>    inferSchema=True, <br/>    header=True<br/>)<br/>titanic.printSchema()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/252df5cf83f9d58da5e463c81033d9e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5sqxtmCAapbjvE1BPrJRPw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片2-泰坦尼克号数据集的推断模式(图片由作者提供)</p></figure><p id="0383" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据类型看起来是正确的，所以让我们打印前10行，以验证一切正常:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="faa6" class="nh mf it nd b gy ni nj l nk nl">titanic.show(10)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/64ec439610dae09af274f9f5e97def9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_n2kdaZOX1IlS-S1dDmUkw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3——泰坦尼克号数据集的前10行(图片由作者提供)</p></figure><p id="a0a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是我们开始分析泰坦尼克号数据集所需要的。</p><h2 id="b7e4" class="nh mf it bd mg ns nt dn mk nu nv dp mo li nw nx mq lm ny nz ms lq oa ob mu oc bi translated">使用Spark函数调用的数据聚合</h2><p id="09fa" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">您不需要使用SQL来处理Spark数据帧。每样东西都有专用的功能。例如，您可以使用<code class="fe nn no np nd b">select()</code>函数来隔离任何感兴趣的列:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="0b6d" class="nh mf it nd b gy ni nj l nk nl">titanic.select("Sex", "Survived").show(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/c3e74adee7e71f90ca6603b7d2587aa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*H1WiMujvkLUItdMVN26STQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4 —使用Spark的DataFrame API选择列(图片由作者提供)</p></figure><p id="3d17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在那里，您还可以通过对分类变量进行分组并传递聚合函数来聚合数据集。以下示例显示了每个性别的幸存乘客人数:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="f838" class="nh mf it nd b gy ni nj l nk nl">titanic.select("Sex", "Survived").groupBy("Sex").sum().show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/e0bd0636902deb444d8a5dd2689e5b9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*u1uQq5otN9hScwhH11hL9Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图Spark中的聚合计算(图片由作者提供)</p></figure><p id="4a99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">向前看，你有两个选择。您可以继续链接函数，或者多次重新声明一个变量以提高可读性。如果这两个听起来都不好玩，你绝对应该试试SQL语法。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="6148" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">如何使用Spark SQL查询Spark数据帧</h1><p id="dc77" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">在Spark中使用SQL之前，首先需要创建一个临时视图。这是通过在DataFrame上调用<code class="fe nn no np nd b">createOrReplaceTempView()</code>函数，并传入一个视图名(任意一个)来完成的。</p><p id="b02c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">创建后，在对<code class="fe nn no np nd b">spark.sql()</code>的调用中传递任何SQL语句:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="2122" class="nh mf it nd b gy ni nj l nk nl">titanic.createOrReplaceTempView("titanic")</span><span id="3617" class="nh mf it nd b gy nm nj l nk nl">spark.sql("SELECT * FROM titanic").show(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/005c3305ef5e598e48bfc0e7ffd71d6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8MZRRj32Wssp-2H46Yh3eA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片6——泰坦尼克号数据集的前5行(图片由作者提供)</p></figure><p id="acf2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您知道<code class="fe nn no np nd b">GROUP BY</code>子句是如何工作的，那么在SQL中重新创建上一节中的数据帧要简单得多:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="a1dd" class="nh mf it nd b gy ni nj l nk nl">spark.sql("""<br/>    SELECT <br/>        Sex,<br/>        SUM(Survived)<br/>    FROM titanic<br/>    GROUP BY Sex<br/>""").show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/0f0588c758a5027c85d123200e14d73c.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*dEJD8F4K1Z9FUy1z83uMqg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7 —使用Spark SQL进行聚合计算(图片由作者提供)</p></figure><p id="1215" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很简单，对吧？我们可以用类似的方法计算每个性别的平均年龄。<code class="fe nn no np nd b">ROUND()</code>函数将结果四舍五入到两位小数，使结果更具可读性:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="8a1e" class="nh mf it nd b gy ni nj l nk nl">spark.sql("""<br/>    SELECT<br/>        Sex,<br/>        ROUND(AVG(Age), 2) AS AvgAge<br/>    FROM titanic<br/>    GROUP BY Sex<br/>""").show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/b9259df5314e923e21736a5cc9cb7d99.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*V62V-XpCYg1INNi6jH5vpg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图8 —使用Spark SQL (2)进行聚合计算(图片由作者提供)</p></figure><h2 id="db74" class="nh mf it bd mg ns nt dn mk nu nv dp mo li nw nx mq lm ny nz ms lq oa ob mu oc bi translated">带有CASE运算符的条件逻辑</h2><p id="3483" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated"><code class="fe nn no np nd b">Cabin</code>栏目问题比较大。它最常见的值是<code class="fe nn no np nd b">NULL</code>，表示乘客没有客舱，或者不知道给定乘客的客舱。</p><p id="8df2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用SQL的<code class="fe nn no np nd b">CASE</code>操作符让这个列变得有用。下面的示例创建了一个列<code class="fe nn no np nd b">HasCabin</code>，如果<code class="fe nn no np nd b">Cabin</code>不是<code class="fe nn no np nd b">NULL</code>，则该列的值为1，否则为0:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="864a" class="nh mf it nd b gy ni nj l nk nl">spark.sql("""<br/>    SELECT<br/>        Cabin,<br/>        CASE <br/>            WHEN Cabin IS NOT NULL THEN 1<br/>            ELSE 0 <br/>        END AS HasCabin<br/>    FROM titanic<br/>""").show(10)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/43941346ed0c3eeea1525da54a56505a.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*cEbF-UDh-RjPTJ0i5-0x3g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图SQL查询中的条件逻辑(作者图片)</p></figure><h2 id="59a0" class="nh mf it bd mg ns nt dn mk nu nv dp mo li nw nx mq lm ny nz ms lq oa ob mu oc bi translated">使用正则表达式的特征工程</h2><p id="f942" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">正则表达式有时感觉像外语，但它们在数据科学和数据分析中有许多用例。例如，<code class="fe nn no np nd b">Name</code>列在第一个逗号符号后附加了一个标题给每个乘客。我们可以在Spark SQL中使用正则表达式来提取它:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="fcfc" class="nh mf it nd b gy ni nj l nk nl">spark.sql("""<br/>    SELECT<br/>        name,<br/>        REGEXP_EXTRACT(name, ' ([A-Za-z]+)\.') AS Title<br/>    FROM titanic<br/>""").show(10)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/cb4840515e188691ac41a2b4c0684c55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*T3tVYLbIwt1zXr2YJQKEZQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图10 —使用正则表达式提取乘客标题(作者图片)</p></figure><p id="ab9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里最困难的部分是提出合适的正则表达式。对于不同的用例，比如电话号码和电子邮件地址，网上有很多这样的信息，所以请记住这一点。没有必要重新发明轮子。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="9cf3" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">如何在Spark SQL中使用用户定义函数(UDF)</h1><p id="eb24" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">Spark SQL没有附带您需要的函数？这就是用户定义函数发挥作用的地方。在PySpark中，您可以创建一个Python函数，并用PySpark SQL <code class="fe nn no np nd b">udf()</code>包装它，将其注册为用户定义的函数。</p><p id="bd87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天我们保持简单，声明一个反转字符串的Python函数。稍后将使用该函数来反转名称。不是特别有用的功能，但这不是重点。</p><p id="45a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是该函数的声明和用法示例:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="1c7b" class="nh mf it nd b gy ni nj l nk nl">def reverse_name(name):<br/>    return name[::-1]<br/>    <br/>   <br/>reverse_name("Braund, Mr. Owen Harris")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/3b734783f3345ca0353c6f1468850c86.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*JulahNC4PD2ZoedUi8FkLA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图11-使用Python反转字符串(图片由作者提供)</p></figure><p id="f6ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要在PySpark中使用它，首先必须将Python函数注册为用户定义的函数。第一个参数指定您将在SQL中使用的名称，第二个参数是Python函数本身。</p><p id="8c91" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注册后，您可以在Spark SQL中自由使用它:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="18d6" class="nh mf it nd b gy ni nj l nk nl">spark.udf.register("reverse_name", reverse_name)</span><span id="6808" class="nh mf it nd b gy nm nj l nk nl">spark.sql("""<br/>    SELECT <br/>        Name,<br/>        reverse_name(Name) AS ReversedName<br/>    FROM titanic<br/>""").show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/0d165fc17e3846a31dccd868b2b283cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EhWmaLixuWsIG3WL2FinKQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图12 —在Spark SQL中使用UDF(图片由作者提供)</p></figure><p id="ca76" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">没有比这更简单的了。用户定义的函数是一个值得在另一篇文章中讨论的主题，所以今天我就不深入讨论了。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="6523" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">Python中Spark SQL概述</h1><p id="4f81" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">现在，您已经拥有了Spark SQL的基础知识，只需几分钟。您已经学到了很多—从阅读CSV文件到使用正则表达式和用户定义函数进行特征提取。一下子要处理的东西太多了，所以你可以随意浏览这些材料，直到他们完全理解为止。</p><p id="9660" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Spark中，用户自定义函数让一切皆有可能。即使您不知道如何在SQL中做一些事情，您也可以轻松地将逻辑包装在Python函数中，并在SQL语句中使用UDF。这肯定是我们将在以后的文章中探讨的一个领域。</p><p id="7fda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将把<a class="ae ky" href="https://betterdatascience.com/apache-spark-word-count/" rel="noopener ugc nofollow" target="_blank">字数</a>的解决方案从RDDs改写成DataFrames，敬请关注。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="edf6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="om">喜欢这篇文章吗？成为</em> <a class="ae ky" href="https://medium.com/@radecicdario/membership" rel="noopener"> <em class="om">中等会员</em> </a> <em class="om">继续无限制学习。如果你使用下面的链接，我会收到你的一部分会员费，不需要你额外付费。</em></p><div class="on oo gp gr op oq"><a href="https://medium.com/@radecicdario/membership" rel="noopener follow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">通过我的推荐链接加入Medium-Dario rade ci</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">medium.com</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe ks oq"/></div></div></a></div></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h2 id="ca5b" class="nh mf it bd mg ns nt dn mk nu nv dp mo li nw nx mq lm ny nz ms lq oa ob mu oc bi translated">推荐阅读</h2><ul class=""><li id="cd35" class="pf pg it lb b lc mw lf mx li ph lm pi lq pj lu pk pl pm pn bi translated"><a class="ae ky" href="https://betterdatascience.com/best-data-science-prerequisite-books/" rel="noopener ugc nofollow" target="_blank">学习数据科学先决条件(数学、统计和编程)的5本最佳书籍</a></li><li id="0ecd" class="pf pg it lb b lc po lf pp li pq lm pr lq ps lu pk pl pm pn bi translated"><a class="ae ky" href="https://betterdatascience.com/top-books-to-learn-data-science/" rel="noopener ugc nofollow" target="_blank">2022年学习数据科学的前5本书</a></li><li id="ba54" class="pf pg it lb b lc po lf pp li pq lm pr lq ps lu pk pl pm pn bi translated"><a class="ae ky" href="https://betterdatascience.com/python-list-print/" rel="noopener ugc nofollow" target="_blank">用Python打印列表的7种方法</a></li></ul><h2 id="8dee" class="nh mf it bd mg ns nt dn mk nu nv dp mo li nw nx mq lm ny nz ms lq oa ob mu oc bi translated">保持联系</h2><ul class=""><li id="61a6" class="pf pg it lb b lc mw lf mx li ph lm pi lq pj lu pk pl pm pn bi translated">雇用我作为一名<a class="ae ky" href="https://betterdatascience.com/contact/" rel="noopener ugc nofollow" target="_blank">技术作家</a></li><li id="b0bb" class="pf pg it lb b lc po lf pp li pq lm pr lq ps lu pk pl pm pn bi translated">在YouTube<a class="ae ky" href="https://www.youtube.com/c/BetterDataScience" rel="noopener ugc nofollow" target="_blank">上订阅</a></li><li id="99ac" class="pf pg it lb b lc po lf pp li pq lm pr lq ps lu pk pl pm pn bi translated">在<a class="ae ky" href="https://www.linkedin.com/in/darioradecic/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上连接</li></ul></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="3333" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="om">原载于2022年4月21日https://betterdatascience.com</em><a class="ae ky" href="https://betterdatascience.com/apache-spark-sql/" rel="noopener ugc nofollow" target="_blank"><em class="om"/></a><em class="om">。</em></p></div></div>    
</body>
</html>