<html>
<head>
<title>Inside Epistatic Nets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">内部上位网</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/inside-epistatic-nets-d9afeaf80689#2022-08-04">https://towardsdatascience.com/inside-epistatic-nets-d9afeaf80689#2022-08-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="dbdb" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">正则化神经网络以更好地预测生物适应度景观</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f038f7c8867f2212e69f6ab18de01879.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*c0McQO2JwTTCxWqA"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">艾玛·戈塞特在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="7d71" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">将你的归纳偏见与你的问题空间相匹配</strong></p><p id="695b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有机器学习(ML)算法都有一个固有的<a class="ae kv" href="https://en.wikipedia.org/wiki/Inductive_bias" rel="noopener ugc nofollow" target="_blank">归纳偏差</a>。归纳偏差是我们对遇到某种解决方案的可能性所做的假设。归纳偏差的经典例子是<a class="ae kv" href="https://en.wikipedia.org/wiki/Occam%27s_razor" rel="noopener ugc nofollow" target="_blank">奥卡姆剃刀</a>，我们假设简单的例子比复杂的更有可能。在ML中，我们经常通过施加稀疏性约束来将归纳偏向简单性转化为实践。<a class="ae kv" href="https://en.wikipedia.org/wiki/Lasso_(statistics)" rel="noopener ugc nofollow" target="_blank"> LASSO回归</a>例如，惩罚学习解中非零系数幅度的总和——LASSO偏向系数更小、更少的解。</p><p id="6898" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">并非所有的归纳偏差都适用于所有的问题空间(没有免费的午餐)。通过根据问题空间定制学习算法的归纳偏差，对于给定的努力，您将获得更好的性能——也就是说，您将需要更少的训练数据、更少的训练迭代，并且您的模型将更好地推广到看不见的示例。</p><p id="c7ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">生物适应度景观稀疏</strong></p><p id="15fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">生物学中一个常见的挑战是预测突变对健康的影响。生物有基因组，基因组由一系列基因组成，其中一些基因相互作用；更少的是一式三份；更高阶的更少；而且大部分基因根本不交互作用。在这种情况下，我指的是<a class="ae kv" href="https://en.wikipedia.org/wiki/Epistasis" rel="noopener ugc nofollow" target="_blank">“上位性”</a>方面的相互作用——也就是说，同时删除基因A和B可能会产生比你预期的A和B单独产生的影响更大(或更小)的影响。</p><p id="a3b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个生物体的整体“适应度”(<a class="ae kv" href="https://www.biorxiv.org/content/10.1101/2022.01.26.477860v2.full" rel="noopener ugc nofollow" target="_blank">其适应度“景观”</a>)不仅取决于基因本身的作用，还取决于基因之间的相互作用网络。适用于基因组的相同原理也适用于单个蛋白质；使得整个蛋白质的适合度不仅取决于其氨基酸序列，还取决于氨基酸之间的相互作用。例如，新型冠状病毒的新变种可能或多或少具有传染性(或多或少“适合”)，这取决于它们的蛋白质中包含的一系列突变，以及这些突变在蛋白质内和基因间如何相互作用。</p><p id="86f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<a class="ae kv" href="https://www.pnas.org/doi/10.1073/pnas.2109649118" rel="noopener ugc nofollow" target="_blank">最近的一篇PNAS论文</a>【1】中，加州大学伯克利分校的Listgarten小组表明，就位点之间的上位性相互作用而言，上述生物景观——对于生物体或单个蛋白质——往往非常稀疏。大多数基因或蛋白质中的大多数位点不相互作用。更少的是一式三份或四份。并且高阶相互作用不能从低阶相互作用中预测。</p><p id="7a3b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">规则化可以加强特定的归纳偏差</strong></p><p id="f373" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如上面的LASSO回归例子所展示的，正则化ML模型是加强特定归纳偏差的一种方式。深度神经网络(DNN)模型能够表示比套索模型复杂得多的功能，能够表示生物适应度景观中潜在存在的更高阶相互作用。然而，对于DNNs，大多数默认的正则化方法适用于单个层。例如，<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L1" rel="noopener ugc nofollow" target="_blank"> L1正则化可以应用于DNN </a>的单层，使得该特定层的权重偏向更稀疏的解决方案。</p><p id="91c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，在Berkeley的Listgarten和Ramchandran小组最近发表的一篇自然通讯论文<a class="ae kv" href="https://www.nature.com/articles/s41467-021-25371-3" rel="noopener ugc nofollow" target="_blank">中，他们指出了一个关键的细节:当使用DNN预测生物序列的适合度时，简单地将DNN层T10限制为稀疏不会导致整个DNN水平的稀疏解。引用他们2021年的论文:</a></p><blockquote class="lt lu lv"><p id="1dfd" class="kw kx ls ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">尽管有其好处，促进上位相互作用的稀疏性并没有在DNNs中作为一种归纳偏差进行研究。障碍在于找到一种方法来提高DNNs中的上位稀疏性。不幸的是，直接惩罚具有促进稀疏先验的DNNs的所有或一些参数(权重)不可能导致稀疏上位正则化，因为上位系数是DNNs中权重的复杂非线性函数。</p></blockquote><p id="d8f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">换句话说，当输入要素通过DNN传播时，即使具有稀疏权重的图层也可以(并且确实)重新组合成非稀疏解决方案。</p><p id="6fc8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总结一下:</p><ul class=""><li id="5f21" class="lz ma iq ky b kz la lc ld lf mb lj mc ln md lr me mf mg mh bi translated">我们喜欢dnn，因为它们可以表示任意程度的相互作用。</li><li id="9d30" class="lz ma iq ky b kz mi lc mj lf mk lj ml ln mm lr me mf mg mh bi translated">但是，dnn是如此灵活，它们很容易过度拟合，损害了通用性。</li><li id="4695" class="lz ma iq ky b kz mi lc mj lf mk lj ml ln mm lr me mf mg mh bi translated">就特征之间的上位性交互而言，生物景观往往是稀疏的。</li><li id="9815" class="lz ma iq ky b kz mi lc mj lf mk lj ml ln mm lr me mf mg mh bi translated">我们需要一种方法来加强整个DNN的上位稀疏性，以便更好地推广到新数据(即，施加更合适的归纳偏差)。</li></ul><p id="56b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们到了有趣的部分。Listgarten和Ramchandran小组通过在DNN训练期间向损失函数添加上位稀疏度的测量，解决了上述问题。</p><p id="74dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">使用沃尔什-哈达玛变换测量上位稀疏度</strong></p><p id="80f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">测量变量间相互作用稀疏性的一种方法是通过<a class="ae kv" href="https://en.wikipedia.org/wiki/Hadamard_transform" rel="noopener ugc nofollow" target="_blank">沃尔什-哈达玛(WD)变换</a>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/65b17dcfd769f97fcfdda0f1b1d08c12.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*eRFTIZolBmj_jQpsk90IVQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">排序后的<a class="ae kv" href="https://en.wikipedia.org/wiki/Walsh_function" rel="noopener ugc nofollow" target="_blank">沃尔什矩阵(维基百科)</a></p></figure><p id="67f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有几种不同的方式来思考<a class="ae kv" href="https://en.wikipedia.org/wiki/Walsh_function" rel="noopener ugc nofollow" target="_blank"> WD转换</a>。第一个，是作为一个离散的<a class="ae kv" href="https://en.wikipedia.org/wiki/Fourier_transform" rel="noopener ugc nofollow" target="_blank">傅立叶变换</a>。花点时间看看上面的沃尔什矩阵，你可以看到一个由低到高频率交替变化的模式。第一行(和列，因为矩阵是对称的)是一系列1。第二个是半1，和半1。第三行更频繁地交替1和-1，直到最后一行每隔一个位置交替1和-1。从“频谱分析”/傅立叶的角度来看，这些行的线性组合<a class="ae kv" href="https://en.wikipedia.org/wiki/Walsh_function" rel="noopener ugc nofollow" target="_blank">可以表示任何布尔函数</a>。也就是说，您可以使用这些行的组合来表示健身景观。<em class="ls">WD变换是用于识别沃尔什矩阵系数的过程(“沃尔什频谱”)，该矩阵可精确分解布尔函数。</em>WD变换的稀疏性是指非零系数的数量。</p><p id="8c5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">边注</em>:沃尔什矩阵根据被分解的布尔函数的维数改变维数。上面的矩阵是16阶的(2⁴或四维布尔函数)。显然，更大的空间需要更大的沃尔什矩阵。</p><p id="34c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从生物学角度来看，WD变换的一个补充方法是列举生物序列中所有可能的上位性相互作用(具体来说，“平均上位性”)。关于这种联系的全面解释，请参见德克萨斯大学阮冈纳赞小组的这篇<a class="ae kv" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4919011/#pcbi.1004771.s005" rel="noopener ugc nofollow" target="_blank"> 2016年论文【3】。想法是沃尔什矩阵的每一行对应于用于计算上位相互作用的系数。</a></p><p id="732f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，考虑一个三维布尔函数(2或8种可能的特征组合)。在生物景观中，这种布尔表示可能意味着许多事情，例如在蛋白质的位置1、2或3处特定突变的存在/不存在。每个特征组合的适合度由y值表示，因此“y101”是样本的y值，其中第一个和第三个元素为1，第三个元素为0。</p><p id="ef18" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">特定位置对适应度的影响可能略有不同，这取决于存在的其他突变。例如，背景(00*)中位置3 (001)的效果将被计算为[y001-y000]。背景中位置3的影响(01*)将被计算为[y011-y010]。这两个效应可以用1/2*[(y001-y000) + (y011-y010)]来平均。结果是位置3对整体健康的平均影响(<a class="ae kv" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4919011/#pcbi.1004771.s005" rel="noopener ugc nofollow" target="_blank">见这里的等式7</a>)。高阶上位相互作用可以用同样的方法计算。以矩阵形式表示完整的一系列方程会产生沃尔什矩阵，使用这些函数分解适应度景观会产生完整的上位相互作用集，这相当于WD谱。</p><p id="af7e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，WD变换允许我们将适合度测量转换为上位交互。我们可以通过计算非零值的数量来衡量这些相互作用的稀疏性。</p><p id="5488" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">用WD变换正则化DNN</strong></p><p id="3ebb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在知道，WD变换提供了一种量化DNN学会的上位相互作用的稀疏性的方法。通过计算非零系数的数量，我们量化了深度网络已经学会如何稀疏(或不稀疏)地表示变量之间的相互作用。我们可能看不到这将如何明显地转化为一个改进的DNN架构(至少，我不这样认为)，但这有什么关系呢？通过<a class="ae kv" rel="noopener" target="_blank" href="/how-does-back-propagation-in-artificial-neural-networks-work-c7cad873ea7#:~:text=Back%2Dpropagation%20is%20the%20essence,reliable%20by%20increasing%20its%20generalization.">反向传播</a>的奇迹，我们需要做的就是定义一个合适的损失函数。</p><p id="ea4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，Listgarten和Ramchandran小组以如下方式将WD变换包含在损失函数中。对于每次迭代训练:</p><ol class=""><li id="ed25" class="lz ma iq ky b kz la lc ld lf mb lj mc ln md lr mo mf mg mh bi translated">列举布尔输入特征的所有可能组合，并使用当前的DNN权重来计算<em class="ls">预测的</em>适应度前景。</li><li id="d15c" class="lz ma iq ky b kz mi lc mj lf mk lj ml ln mm lr mo mf mg mh bi translated">对预测的适应值执行WD变换</li><li id="0cb9" class="lz ma iq ky b kz mi lc mj lf mk lj ml ln mm lr mo mf mg mh bi translated">计算WD变换中非零系数的数量，并将该数量添加到损失函数中，乘以权重。</li></ol><p id="8833" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">边注</em>:步骤3是理想化的过程，但实际上，L0范数(计算非零系数的数量)是不可微的，因此不适合反向传播。相反，他们放宽了对L1范数(系数绝对值之和)的限制。</p><p id="4399" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总之，在输入为布尔型而输出为适应值的回归场景中，损失函数变为均方误差(MSE)加上WD变换的加权L1范数。给定这个损失函数，模型被训练以最小化预测<em class="ls">误差</em>和解决方案的<em class="ls">稀疏度</em>。</p><p id="a58a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用这个损失函数正则化DNN在本文中被称为“上位网”(EN)。</p><p id="157b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">上位网络提高生物景观预测任务的性能</strong></p><p id="c224" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作者在几个公共数据集上证明，当使用上述EN loss函数进行正则化时，训练用于预测生物序列适合度的DNNs表现更好。约束完全DNN来学习输入变量之间的稀疏上位关系允许模型更好地泛化。例如，下图显示了GFP荧光数据集的改进。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/598dc4dff86347a66269a2a707b9d183.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iOztSIJf3wma0WzRpTggng.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">有和没有EN正则化的DNN预测的GFP荧光。<a class="ae kv" href="https://www.nature.com/articles/s41467-021-25371-3" rel="noopener ugc nofollow" target="_blank">图3来自2021年Listgarten和Ramchandran的论文</a>。</p></figure><p id="4ef2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">tensor flow中的上位网络实现</strong></p><p id="22c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们使用TensorFlow 2和Python在玩具数据集上实现一个上位网络。完整的实现可以在GitHub 上以Jupyter笔记本的形式获得。</p><p id="5bcd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的玩具数据集将是6维的。这意味着有6个布尔输入特征，以及这些特征的64种可能的组合。我们将包括一个稀疏函数和一个稠密函数。稀疏函数将包括1-4度的4项。密集函数将包括13个1-4度项。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="d15a" class="mv mw iq mr b gy mx my l mz na">sparse_fun = lambda x: 2*x[0]*x[3] - 2*x[2] + 2*x[0]*x[1]*x[4] \                          <br/>                       + 2*x[0]*x[2]*x[3]*x[5]</span><span id="4d84" class="mv mw iq mr b gy nb my l mz na">dense_fun = lambda x: 2*x[0]*x[3] - 2*x[2] + 2*x[0]*x[1]*x[4] \<br/>                      + 2*x[0]*x[2]*x[3]*x[5] \<br/>                      -  2*x[1]*x[3] - 2*x[5] + 2*x[3]*x[1]*x[5] \<br/>                      +  2*x[0]*x[4] - 2*x[0] + 2*x[3]*x[4]*x[5] \<br/>                      -  2*x[2]*x[5] + 2*x[1] - 2*x[2]*x[3]*x[4]</span><span id="b218" class="mv mw iq mr b gy nb my l mz na"># itertools.product for generating all combinations of features<br/>n_bits = 6<br/>all_comb =  list(product([0, 1], repeat=n_bits))</span></pre><p id="232c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了计算WH变换，我们将使用<a class="ae kv" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.hadamard.html" rel="noopener ugc nofollow" target="_blank"> scipy.linalg.hadamard </a>函数来生成沃尔什矩阵，然后使用张量流函数执行矩阵乘法(以便梯度带可以跟踪运算)。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="51cd" class="mv mw iq mr b gy mx my l mz na">from <a class="ae kv" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.hadamard.html" rel="noopener ugc nofollow" target="_blank">scipy.linalg</a> import hadamard</span><span id="3abc" class="mv mw iq mr b gy nb my l mz na"># bool model predicts fitness using Boolean inputs<br/>y_all = tensorflow.squeeze(bool_model(all_comb))</span><span id="a645" class="mv mw iq mr b gy nb my l mz na"># Walsh-Hadamard matrix (without normalizing constant)<br/>had_mat = tensorflow.convert_to_tensor(<br/>    hadamard(2**n_bits),<br/>    dtype=tensorflow.float32<br/>)</span><span id="f81a" class="mv mw iq mr b gy nb my l mz na"># Multiply Walsh matrix by predicted fitness values<br/>wh_t = tensorflow.linalg.matvec(had_mat, y_all)</span><span id="0b05" class="mv mw iq mr b gy nb my l mz na"># Sum absolute value of coefficients<br/>coeff_l1 = tensorflow.reduce_sum(<br/>    tensorflow.cast(<br/>        tf.abs(wh_t),<br/>        dtype=tensorflow.float64)<br/>    )</span></pre><p id="0319" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，用于训练模型的损失函数变成MSE加上WH变换的系数之和，如下所示:</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="1f79" class="mv mw iq mr b gy mx my l mz na">mse = tensorflow.reduce_mean(<br/>    tensorflow.math.squared_difference(y_pred, y_true)<br/>)</span><span id="0d29" class="mv mw iq mr b gy nb my l mz na"># Here wh_weight balances the contribution of MSE and WH transform<br/># This code adds the weighted factors together into a single loss<br/>loss_sum = tensorflow.add(<br/>    tensorflow.cast(mse, dtype=tensorflow.float32),<br/>    tensorflow.cast(wh_weight * coeff_l1, dtype=tensorflow.float32)<br/>)</span></pre><p id="6962" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们有了运行实验的基本组件(参见<a class="ae kv" href="https://github.com/mbi2gs/epistatic_net_demo" rel="noopener ugc nofollow" target="_blank">全Jupyter笔记本</a>了解所有细节)。我们比较了三种类型的DNN正则化:非正则化，分层L2正则化，WH稀疏正则化(又名上位网)。我们可以在25%的稀疏函数景观上训练每个模型，然后尝试推广到剩余的75%。将使用MSE作为损失函数来训练每个模型，除了上位网，上位网将使用MSE加上稀疏性约束。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/1d4822512bda59b597f13f2fcf7563b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*dsgJVXGExhlrTa0gARXEbA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">上位网(“wh_reg”)在稀疏和密集数据集上都更好地概括，因为它偏向于学习稀疏特征交互。每个方框代表10个随机的训练/测试分割。顶行显示了模型预测与保留数据的皮尔逊相关性。底部一行显示了训练模型的相互作用系数大小的总和。左栏是稀疏的风景，右栏是密集的。</p></figure><p id="7be1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从上图中我们可以看到，稀疏度约束的“wh _ reg”DNN在稀疏和密集景观(顶行，皮尔逊相关系数)中都能更好地推广测试数据。不出所料，同一个模型学会了用更大的稀疏性来表示特征交互(底部一行)。</p><p id="c830" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">结论</strong></p><p id="7c4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据问题空间定制ML模型的归纳偏差可以带来显著的改进。就特征交互而言，生物适应度景观往往非常稀疏。使用WH变换来测量上位稀疏性，并将该度量包括在损失函数中，导致在预测生物序列的适合度时模型性能大大提高(一种称为<a class="ae kv" href="https://www.nature.com/articles/s41467-021-25371-3" rel="noopener ugc nofollow" target="_blank">上位网</a>的训练过程)。我们已经在TensorFlow 中完成了上位网络的<a class="ae kv" href="https://github.com/mbi2gs/epistatic_net_demo" rel="noopener ugc nofollow" target="_blank">实现，并演示了它的完美工作！</a></p><p id="ba41" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">参考文献</strong></p><p id="6735" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1] D. Brooks，A. Aghazadeh和J. Listgarten，<a class="ae kv" href="https://www.pnas.org/doi/10.1073/pnas.2109649118" rel="noopener ugc nofollow" target="_blank">关于适应度函数的稀疏性及其对学习的影响</a> (2021)，<em class="ls"> PNAS </em> 119(1):e2109649118</p><p id="6e99" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] A. Aghazadeh，H. Nisonoff，O. Ocal，D. Brookes，Y. Huang，O. Koyluoglu，J. Listgarten和K. Ramchandran，<a class="ae kv" href="https://www.nature.com/articles/s41467-021-25371-3" rel="noopener ugc nofollow" target="_blank">上位网允许深度神经网络的稀疏谱正则化以推断适应度函数</a> (2021)，<em class="ls">Nature communication s</em>12:5225</p><p id="0716" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3] F. Poelwijk，V. Krishna和r .阮冈纳赞，<a class="ae kv" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4919011/#pcbi.1004771.s005" rel="noopener ugc nofollow" target="_blank">突变的背景依赖性:形式主义的联系</a> (2016)，<em class="ls"> PLoS计算生物学</em> 12(6):e1004771</p></div></div>    
</body>
</html>