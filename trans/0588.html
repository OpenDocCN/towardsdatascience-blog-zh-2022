<html>
<head>
<title>The Fight Against Fake News with Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用深度学习对抗假新闻</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-fight-against-fake-news-with-deep-learning-6c41dd9eaae4#2022-02-23">https://towardsdatascience.com/the-fight-against-fake-news-with-deep-learning-6c41dd9eaae4#2022-02-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8d9a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">基于LSTMs和BERT的假新闻检测</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4e11c3595c7fa73f408e2d95ed2d8fff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b55KgVH1x8nr-MLm0IMrVA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">马库斯·温克勒在<a class="ae kv" href="https://www.pexels.com/photo/close-up-shot-of-a-typewriter-4160060/" rel="noopener ugc nofollow" target="_blank">像素上拍摄的照片</a></p></figure><p id="2430" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们玩一个游戏。我给你两个标题，你告诉我哪个是假新闻。准备好了吗？我们开始吧:</p><blockquote class="ls lt lu"><p id="f2fc" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">“南希·佩洛西挪用社会保障资金用于弹劾调查”</p><p id="53a3" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">“联邦应急管理局抵达遭受龙卷风袭击的肯塔基州——带着疫苗”</p></blockquote><p id="8126" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">嗯……事实证明，<strong class="ky ir"><em class="lv"/></strong>两个标题都是假新闻的例子。</p><p id="a963" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">残酷的现实是，错误信息和虚假信息在互联网上猖獗。根据益普索公共事务公司(Ipsos Public Affairs)为加拿大国际治理创新中心(Centre for International Governance Innovation)进行的2019年民意调查显示，<strong class="ky ir"> 90%的加拿大人曾被假新闻所迷惑。</strong></p><p id="ca72" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这让我想到，有没有可能创建一个模型来检测一篇给定文章的标题是否是假新闻？嗯，事实证明确实如此！在这篇文章中，我将与你分享我用LSTMs和BERT建立这个文本分类模型的经验。</p><p id="3859" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在我的Github库<a class="ae kv" href="https://github.com/shayaf84/Fake-News-Detection" rel="noopener ugc nofollow" target="_blank">这里</a>找到我使用的代码</p><h1 id="17ae" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">查找数据集</h1><p id="b173" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">我们将使用维多利亚大学的ISOT假新闻数据集。它包含超过12，600篇来自Reuters.com的真实新闻文章和超过12，600篇由Politifact(美国的一个事实核查组织)标记的虚假新闻文章。该数据集包含与各种主题相关的文章，尽管主要是政治新闻和世界新闻。在这个<a class="ae kv" href="https://www.uvic.ca/ecs/ece/isot/datasets/fake-news/index.php" rel="noopener ugc nofollow" target="_blank">链接</a>处检查一下。</p><h1 id="4358" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">探索性数据分析</h1><p id="5959" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">首先，让我们对数据进行一些分析，以便了解数据中可能的趋势。因为真实新闻和虚假新闻文件是分开的，所以我们需要标记并连接这两个数据帧。对于真实的新闻数据集，我们循环行数，并向NumPy数组添加0(表示类)。该数组将被添加到真实新闻数据帧中。对于假新闻数据集，我们重复这个过程，但是向NumPy数组添加1。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/93cde431276652b66dd21658a6c1e945.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h05zWQCQprb2xu9dJrUliA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/9130bf6001374ea20c4eb49e090239bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*yiekGH8KgAM2NO6_x8pg3w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="263e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为我们有21，417个真实新闻样本和23，481个虚假新闻样本，所以真实新闻和虚假新闻的比例大约是48:52。这意味着我们的数据集相对平衡。对于我们的项目，我们只需要标题和类列。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/a54556723e45a837d02dcfefb29ae36d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bNMFi118yJmT6zi0tlkOJQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">最终数据帧|作者图片</p></figure><p id="4a7e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">太好了！既然我们清理了数据集，我们可以分析我们在其中发现的趋势。为了测量数据集的大小，我们将分析标题的平均、最小和最大字符长度。我们将用直方图来绘制这个频率。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/860d5a738e34574f30cd2d207fec231e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*TjgOpU_uRoA4Ktl25nzq-w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/6be7af77f0330897c64d2dcfda1f9135.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R4E6LS1ggrqbj_uJLlEfwQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="95c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们看到每个条目中的字符数范围从8到286。长度为50-100的样本浓度很高。这可以在数据集中的平均长度约为80时进一步看出。</p><h1 id="3171" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">预处理我们的数据</h1><p id="4552" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">我们将使用python的字符串库进行一些初始预处理。它们包括:</p><ul class=""><li id="5291" class="nd ne iq ky b kz la lc ld lf nf lj ng ln nh lr ni nj nk nl bi translated"><strong class="ky ir">小写所有字符</strong></li><li id="1148" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated"><strong class="ky ir">去掉标点符号</strong></li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="7ff7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">之后，我们将需要使用<strong class="ky ir"> NLTK库</strong>对我们的数据集进行进一步的预处理。它包括:</p><ul class=""><li id="98ac" class="nd ne iq ky b kz la lc ld lf nf lj ng ln nh lr ni nj nk nl bi translated"><strong class="ky ir">标记化:</strong>将一个文本分割成一个更小的称为标记的单元(每个单词将是一个数组中的一个索引)</li><li id="8a86" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated"><strong class="ky ir">词汇化:</strong>去除单词的屈折词尾。例如，单词“children”将被词条化为“child”</li><li id="2054" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated"><strong class="ky ir">删除停用词:</strong>常用词如“the”或“for”将被删除，因为它们会占用我们数据集中的空间。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="0ac0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这意味着我们处理后的数据帧将如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/e4ef76ee3dedcdf6e405c5480bed8f7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*APgHRs7GF9FGCKy86_uCjw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="2d8c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将基于这些数据构建两个模型来对文本进行分类:</p><ul class=""><li id="9be3" class="nd ne iq ky b kz la lc ld lf nf lj ng ln nh lr ni nj nk nl bi translated">一个<strong class="ky ir"> LSTM模型</strong>(我们将使用Tensorflow的wiki-words-250嵌入)</li><li id="e867" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">伯特模型。</li></ul><h1 id="71e7" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">创建LSTM模型</h1><p id="66e9" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">首先，我们将把数据集分割成80:20的训练:测试比例。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="7235" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们希望我们的模型基于文本数据进行预测，我们需要将其转换为矢量格式，这样我们的计算机就可以对其进行处理。</p><p id="4abf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Tensorflow的wiki-words-250使用Word2Vec跳格结构。通过基于输入单词预测上下文来训练Skip-gram。</p><p id="ceae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，如果我们有这样一句话:</p><blockquote class="ls lt lu"><p id="2546" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">我将乘飞机去度假</p></blockquote><p id="2e9f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将传入单词“vacation”作为输入，并将窗口大小指定为1。窗口大小表示要预测的目标单词前后的单词。在这种情况下，它们是单词“go”和“airplane”(不包括停用词，并且“go”是“going”的词汇化形式)。</p><p id="4467" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们对单词进行热编码，所以我们的输入向量的大小是1 x V，其中V是我们的词汇量。该表示将乘以具有V行(我们的词汇表中的每个单词一行)和E列的权重矩阵，其中E是表示每个嵌入大小的超参数。因为输入向量是一位热编码的，所以除了一个值(代表我们正在输入的单词)之外，所有值都是0。因此，当乘以权重矩阵时，输出是1xE向量，表示该单词的嵌入。</p><p id="9ce5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1xE向量将被传递到输出层，由softmax回归分类器组成。它由V个神经元组成(对应于词汇的一键编码)，每个单词的输出在0和1之间，表示该单词在窗口大小中的概率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/3f56cac4bcab3e2117514a223c3a43a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5HMZ7EArwJhod3vLusMDeQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">word 2 vec skip program表示|作者图片</p></figure><p id="d5fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Tensorflow的wiki-words-250由大小为250的单词嵌入组成。它可以通过遍历每个单词并计算每个单词的嵌入来应用于我们的模型。我们将需要应用pad_sequences函数来考虑不同长度的样本。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/26488ec350dccf1c138e89fa019fe346.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*HC__GRjYrYstbhhFjcmw2A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="6fdc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，在训练数据中有35，918个样本，最大长度是34个单词(较少的单词被填充)，并且每个单词具有250个特征。</p><p id="9063" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以将同样的程序应用于我们的测试数据。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="d512" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们构建我们的模型。它将包括:</p><ul class=""><li id="ed3d" class="nd ne iq ky b kz la lc ld lf nf lj ng ln nh lr ni nj nk nl bi translated"><strong class="ky ir"> 1 LSTM层</strong>有50个单位</li><li id="9a3e" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated"><strong class="ky ir"> 2个致密层</strong>(一个有20个神经元，第二个有5个)，具有ReLU激活功能</li><li id="82b4" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated"><strong class="ky ir"> 1密集输出层</strong>具有s形激活功能</li></ul><p id="0365" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用Adam优化器，一个二进制交叉熵损失和一个准确性的性能指标。该模型将被训练超过10个时期。请随意进一步调整这些超参数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/bbbd26636d9ce1c65df565678b874fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sNT3VkwpznsT6LUQz6mjSw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="525b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如上所述，我们的模型对测试数据的最高准确率为91.5%。</p><h1 id="a0ec" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">介绍伯特</h1><p id="e2fa" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">如果我问你英语中有最多定义的单词，你会说什么？</p><p id="c0bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据《牛津英语词典》第二版，这个词是“set”。</p><p id="badd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你想一想，我们可以在不同的上下文中用这个词造出许多不同的句子。例如:</p><blockquote class="ls lt lu"><p id="8538" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">我的铅笔是一套<strong class="ky ir">文具用品</strong>的一部分<br/> <br/>我的队友<strong class="ky ir">把</strong>排球给了我</p><p id="5cf5" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">我摆好桌子准备吃饭</p></blockquote><p id="db86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Word2Vec的问题在于，不管单词的使用方式如何，它都会生成相同的嵌入。为了解决这个问题，我们可以使用BERT，它可以<strong class="ky ir">生成上下文化的嵌入</strong>。</p><p id="bad2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">BERT代表“来自变压器的双向编码器表示”。它使用一个转换器模型，利用注意力机制来生成情境化的嵌入。</p><p id="544b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">转换器模型使用编码器-解码器架构。编码器层生成一个连续的表示，它由从输入中学习到的信息组成。解码器层创建一个输出，将之前的输入传递到模型中。<strong class="ky ir"> BERT只使用了一个编码器</strong>，因为它的目标是生成一个矢量表示来从文本中获取信息。</p><h1 id="b304" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">预训练和微调BERT</h1><p id="cd12" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">采用两种方法来训练BERT。第一种叫做掩蔽语言建模。在传递序列之前，<strong class="ky ir"> 15%的单词被替换为一个【掩码】标记</strong>。该模型将使用非屏蔽词提供的上下文来预测屏蔽词。这由以下人员完成</p><ul class=""><li id="5b30" class="nd ne iq ky b kz la lc ld lf nf lj ng ln nh lr ni nj nk nl bi translated"><strong class="ky ir">在编码器输出上应用分类层</strong>，包括嵌入矩阵。因此，它将与词汇表的大小相同。</li><li id="00d0" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated"><strong class="ky ir">用softmax函数计算单词的概率</strong>。</li></ul><p id="4d32" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二种方法是下一句预测。该模型将接收2个句子作为输入，并且<strong class="ky ir">预测第二个句子是否跟随第一个</strong>的二进制值。在训练时，50%的输入是成对的，而另外50%是来自语料库的随机句子。为了区分这两个句子，</p><ul class=""><li id="fa68" class="nd ne iq ky b kz la lc ld lf nf lj ng ln nh lr ni nj nk nl bi translated">第一句开头加一个<strong class="ky ir">【CLS】记号</strong>，每句结尾加一个<strong class="ky ir">【SEP】记号</strong>。</li><li id="b807" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">每个记号(单词)都有一个<strong class="ky ir">位置嵌入</strong>来从文本中的位置辨别信息。这一点很重要，因为在transformer模型中没有递归，所以对单词的位置没有固有的理解</li><li id="5083" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">嵌入了的<strong class="ky ir">句子被添加到每个记号(进一步区分句子)。</strong></li></ul><p id="c698" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了执行下一个句子预测的分类，【CLS】嵌入的<strong class="ky ir">输出，表示“句子分类的集合序列表示法”，通过带有softmax </strong>的分类层<strong class="ky ir">，返回两个句子连续的概率。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/a7c838b61ed88ca8a9e691bae8d80cf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z8Qv29PxM8NXjjToSr0ibg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">掩蔽语言建模和下一句预测|作者图片</p></figure><p id="4c48" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为我们的任务是分类，要微调BERT，只需在BERT的[CLS]令牌输出上添加分类层。</p><h1 id="a41a" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">实施BERT</h1><p id="78d3" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">我们将使用Tensorflow-hub的BERT预处理器和编码器。不要让文本通过我们之前描述的框架(去掉大写，应用词汇化等)。这是用BERT预处理器抽象出来的。</p><p id="f1c7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的数据帧最初应该是这样的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/a54556723e45a837d02dcfefb29ae36d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bNMFi118yJmT6zi0tlkOJQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">最终数据帧|作者图片</p></figure><p id="d158" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">之后，我们可以按照80:20的训练:测试比率将模型分为训练和测试数据。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="beba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在可以用Tensorflow-hub导入BERT预处理程序和编码器</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="f5b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们可以开发我们的神经网络。它必须是一个功能模型，其中前一层的输出必须是下一层的参数。该模型包括:</p><ul class=""><li id="4a07" class="nd ne iq ky b kz la lc ld lf nf lj ng ln nh lr ni nj nk nl bi translated"><strong class="ky ir"> 1输入层:</strong>这将代表将要传入模型的句子)。</li><li id="51f2" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated"><strong class="ky ir">Bert _ preprocess层:</strong>这里我们传递输入来预处理文本。</li><li id="ab89" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated"><strong class="ky ir">bert _ encoder层:</strong>这里我们将预处理过的令牌传递给BERT编码器。</li><li id="ee23" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated"><strong class="ky ir"> 1脱层</strong>比率0.2。BERT编码器的pooled_output被传递给它(下面将详细介绍)</li><li id="c5af" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated"><strong class="ky ir"> 2个致密层</strong>，分别有10个和1个神经元。第一个将使用ReLU激活函数，第二个将使用sigmoid。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="5afd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以看到“pooled_output”将被传递到dropout层。该值表示文本的整体序列表示。如前所述，它是[CLS]令牌输出的表示。</p><p id="f548" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用Adam优化器，一个二进制交叉熵损失，以及一个准确性的性能指标。该模型将被训练超过5个时期。请随意进一步调整这些超参数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/ae03dede3b15f25d5a7016db6f80f8b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d_lu8KUu-up30n2OywS8Iw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="bae0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从上面可以看到，在测试数据(来自model.evaluate()方法)上，模型达到了91.14%的准确率。</p><h1 id="0eab" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">创建Web应用程序</h1><p id="7a00" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">我们可以使用HTML和Flask创建一个web应用程序，允许用户与我们制作的模型进行交互。</p><p id="5376" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">到模型前端的链接可以在<a class="ae kv" href="https://github.com/shayaf84/Fake-News-Detection/blob/master/templates/index.html" rel="noopener ugc nofollow" target="_blank">这里找到</a>，到后端的链接可以在<a class="ae kv" href="https://github.com/shayaf84/Fake-News-Detection/blob/master/app.py" rel="noopener ugc nofollow" target="_blank">这里找到</a>。你可以通过Google Colab和ngrok在本地托管这个模型。</p><p id="c3ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">界面将如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/36456da8ebb9e6f2b2ec503dbfbfbc93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_jKBBn_4MYn0fvkb9BLMzQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="fe7f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有三种型号可供用户选择。</p><ul class=""><li id="44a7" class="nd ne iq ky b kz la lc ld lf nf lj ng ln nh lr ni nj nk nl bi translated">使用Gensim Doc2Vec的<strong class="ky ir">基线密集神经网络</strong></li><li id="9bb9" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">使用wiki-words-250嵌入的<strong class="ky ir"> LSTM模型</strong>(使用Word2Vec架构)</li><li id="2714" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated"><strong class="ky ir">伯特模型</strong></li></ul><p id="d181" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用户可以输入标题并选择型号。该应用程序将返回一个分类和一个概率，如果概率高于50%，模型预测标题来自假新闻来源。因此，如果概率低于50%，模型将预测标题来自真实的新闻来源。</p></div><div class="ab cl ny nz hu oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="ij ik il im in"><p id="bed0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这篇文章展示了我们如何利用NLP中的技术来区分假新闻和真实新闻。你可能会问，“接下来我们能做什么？”嗯，我们的过程并没有结束。为了进一步改进我们的模型，我们可以:</p><ul class=""><li id="d505" class="nd ne iq ky b kz la lc ld lf nf lj ng ln nh lr ni nj nk nl bi translated">根据更多数据训练我们的模型</li><li id="3e91" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">调整我们模型的超参数</li><li id="59bb" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">部署我们创建的web应用程序</li></ul></div><div class="ab cl ny nz hu oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="ij ik il im in"><p id="4286" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">希望你喜欢阅读这篇文章！欢迎在<a class="ae kv" href="https://www.linkedin.com/in/shaya-farahmand-168307220/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>上加我，敬请关注更多内容！</p></div><div class="ab cl ny nz hu oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="ij ik il im in"><h2 id="724a" class="of ma iq bd mb og oh dn mf oi oj dp mj lf ok ol ml lj om on mn ln oo op mp oq bi translated">文献学</h2><p id="26a6" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">[1] <a class="ae kv" href="https://www.tensorflow.org/text/tutorials/classify_text_with_bert" rel="noopener ugc nofollow" target="_blank">用BERT </a> (2022)，Tensorflow对文本进行分类</p><p id="770e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] E .汤普森。<a class="ae kv" href="https://www.cbc.ca/news/politics/fake-news-facebook-twitter-poll-1.5169916" rel="noopener ugc nofollow" target="_blank">民意调查发现90%的加拿大人已经陷入假新闻</a> (2019)，加拿大广播公司</p><p id="8998" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]艾哈迈德、特拉奥雷和萨阿德。使用文本分类检测观点垃圾邮件和假新闻(2018)，《安全与隐私杂志》，第1卷，第1期，Wiley，</p><p id="4c6c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4]艾哈迈德、特拉奥雷和萨阿德。使用N-Gram分析和机器学习技术检测在线假新闻，载于:Traore I，Woungang I，Awad A. (eds)分布式和云环境中的智能、安全和可靠的系统。ISDDC 2017。计算机科学讲义，第10618卷。施普林格，湛(第127- 138页)。</p><p id="e18d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[5] J. Devlin、M. Chan、K. Lee和K. Toutanova。<a class="ae kv" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a> (2019)，arXiv</p><p id="6fb1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[6] R. Horev，<a class="ae kv" rel="noopener" target="_blank" href="/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270"> BERT解释道:面向数据科学的NLP </a> (2018)的最先进语言模型</p><p id="16e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[7] R. Kulshrestha，<a class="ae kv" rel="noopener" target="_blank" href="/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314#:~:text=In%20the%20CBOW%20model%2C%20the,used%20to%20predict%20the%20context%20."> NLP 101: Word2Vec- Skip-gram和CBOW </a> (2019)，走向数据科学</p></div></div>    
</body>
</html>