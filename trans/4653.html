<html>
<head>
<title>In Search of the Perfect Machine Learning Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">寻找完美的机器学习模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/in-search-of-the-perfect-machine-learning-model-cf4e97b95e64#2022-10-17">https://towardsdatascience.com/in-search-of-the-perfect-machine-learning-model-cf4e97b95e64#2022-10-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7466" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">概率表现和几乎免费的午餐</strong></h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/44aaa8056034f6dcfc56e752bfcb6aad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JBa1qWNv_tCkCOF4pRciuQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@wflwong?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">王华伦</a>在<a class="ae kv" href="https://unsplash.com/s/photos/searching?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="8f63" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首席研究员:戴夫·古根海姆博士</p><h1 id="a5bf" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">简介</strong></h1><p id="dd2d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在机器学习中，没有免费的午餐定理(NFLT)表明，当每个学习模型的性能在所有可能的问题上平均时，它们表现得一样好。因为这种平等，NFLT是明确的——预测分析没有单一的最佳算法(<a class="ae kv" href="https://www.brainfuel.io/blog/machine-learning-and-its-no-free-lunch-theorem/" rel="noopener ugc nofollow" target="_blank">机器学习，也没有免费的午餐定理| Brainfuel博客</a>)。</p><p id="b9b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与此同时，学习模型也有了显著的改进，特别是在打包和提升集合方面。随机森林模型是bagging的一个版本，针对179个不同的分类器和121个数据集进行了测试，发现它们在更多时候更准确(Fernández-Delgado，Cernadas，Barro和Amorim，2014年)。但XGBoost算法是boosting的一个版本，于2015年推出，此后一直统治着其他学习模型(Chen et al .，2015)。它因为创造了许多Kaggle竞赛获奖者而获得了这个称号(<a class="ae kv" href="https://www.kaggle.com/getting-started/145362" rel="noopener ugc nofollow" target="_blank">XGBOOST是什么？|数据科学与机器学习| Kaggle </a>)。它的部分吸引力在于广泛的超参数调整功能，这些功能允许对不同数据集进行定制拟合(<a class="ae kv" href="https://xgboost.readthedocs.io/en/stable/parameter.html" rel="noopener ugc nofollow" target="_blank"> XGBoost参数— xgboost 1.6.2文档</a>)。</p><p id="4560" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为此，我们将关注随机森林和xgboost模型，将其作为核心的底层集成，我们将向其添加额外的学习模型，从而创建“极端集成”，以找到一个能够很好地概括的模型。这项研究将探索极端系综的默认配置，并将它们与高度调优的XGBoost模型进行比较，以确定超调的功效和实际准确性的限制。最后，在“堂吉诃德”时刻，我们将尝试从实际意义上发现一个通用模型，该模型在所有数据集上都处于或接近顶级水平。</p><p id="a233" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">研究问题</strong></p><ol class=""><li id="7dde" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">如果我们预测多种未来，而不是只有一种，我们的模型会是什么样子？</li></ol><p id="0139" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.数据质量如何影响多未来模型性能？</p><p id="704b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.超参数调优是做什么的？</p><p id="5eff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">4.是否需要超参数调整，或者未调整的模型能否达到同样的性能？</p><p id="9531" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">5.如果没有免费的午餐，那么有几乎免费的午餐吗？</p><p id="288f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">触发警告</strong></p><p id="a862" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请查看以下陈述:</p><ol class=""><li id="68b5" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">数据只是另一种机器学习资源。</li><li id="d008" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">训练/测试分割的随机种子值可以是您想要的任何整数，因此1、42、137和314159都是好的。</li><li id="f21a" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">超参数调整是数据挖掘过程中最重要的步骤之一，超调整的XGBoost模型是最好的全方位机器学习算法。</li><li id="d462" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">Kaggle竞赛反映了真实的数据科学，准确率为90.00%的人击败了所有达到89.99%的人。</li></ol><p id="49ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你认同这些说法中的任何一个，那么这就是你的“红色药丸”时刻——在继续阅读之前，也许你应该找回你的情感支持动物，并泡一杯热茶。</p><p id="35f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为系好金凤花，这将是一段颠簸的旅程。</p><h1 id="ca27" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">车型</strong></h1><p id="78a1" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这项研究创建了21个模型，从默认的随机森林和xgboost到极端集成的发明，其中bagging或boosting与其他学习算法相结合，以从数据中捕捉更多信息(有关更多信息，请参见图1-4)。</p><p id="740b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随机森林或xgboost与逻辑回归相结合，用于获取线性信息，和/或与具有径向基核的支持向量分类器相结合，用于发现平滑曲线关系。此外，特征缩放集成被添加到支持向量分类器中，因为它在原始特征缩放研究中表现出优异的性能(<a class="ae kv" rel="noopener" target="_blank" href="/the-mystery-of-feature-scaling-is-finally-solved-29a7bb58efc2">特征缩放的奥秘最终被解决|由戴夫·古根海姆|走向数据科学</a>)。特征缩放集成没有与逻辑回归相结合，因为早期的研究仅显示了多项数据的改进(<a class="ae kv" rel="noopener" target="_blank" href="/logistic-regression-and-the-feature-scaling-ensemble-e78a56fc6c1">逻辑回归和特征缩放集成|由Dave Guggenheim |向数据科学发展</a>)，并且这里的所有数据都表示二元分类。</p><p id="4ba2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于极端集成，使用投票分类器(<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html" rel="noopener ugc nofollow" target="_blank">sk learn . ensemble . voting classifier-scikit-learn 1 . 1 . 2文档</a>)或堆叠分类器(<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html?highlight=stackingclassifier#sklearn.ensemble.StackingClassifier" rel="noopener ugc nofollow" target="_blank">sk learn . ensemble . stacking classifier-scikit-learn 1 . 1 . 2文档</a>)来组合算法的默认配置。</p><p id="59a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">更多详细信息请参见以下模型描述:</p><p id="dc49" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1)RF:random _ state = 1的默认RandomForestClassifier</p><p id="ee65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2)XGB:random _ state = 1的默认XGBoostClassifier</p><p id="1120" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3) RF_XGB VOTE:默认随机森林和xgboost与VotingClassifier相结合</p><p id="466e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">4) RF_XGB STACK:默认随机森林和xgboost与StackingClassifier相结合</p><p id="4c9b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">5) RF_LOG投票:使用投票分类器的默认随机森林和逻辑回归</p><p id="464f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">6) RF_LOG堆栈:具有堆栈分类器的默认随机森林和逻辑回归</p><p id="6ba9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">7) RF_SVM投票:具有投票分类器的默认随机森林和支持向量分类器</p><p id="21b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">8) RF_SVM堆栈:具有堆栈分类器的默认随机森林和支持向量分类器</p><p id="4143" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">9)RF _ SVM _对数投票:默认随机森林、支持向量分类器和具有投票分类器的逻辑回归</p><p id="f60b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">10)RF _ SVM _对数堆栈:默认随机森林、支持向量分类器和具有堆栈分类器的逻辑回归</p><p id="9b54" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">11) RF_SVM_FSE投票:默认随机森林和支持向量分类器，其具有带有投票分类器的特征缩放集成</p><p id="e164" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">12) RF_SVM_FSE堆栈:具有特征缩放集成的默认随机森林和支持向量分类器与堆栈分类器</p><p id="7dfb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">13) XGB_LOG VOTE:使用VotingClassifier的默认XGBoost和逻辑回归</p><p id="5a83" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">14) XGB_LOG堆栈:使用StackingClassifier的默认XGBoost和逻辑回归</p><p id="59f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">15) XGB_SVM投票:默认XGBoost和支持向量分类器，带有投票分类器</p><p id="33fc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">16) XGB_SVM堆栈:具有StackingClassifier的默认XGBoost和支持向量分类器</p><p id="ad91" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">17) XGB_SVM_FSE投票:默认XGBoost和支持向量分类器，具有与投票分类器相结合的特征缩放集成</p><p id="5291" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">18) XGB_SVM_FSE堆栈:默认XGBoost和支持向量分类器，具有与堆栈分类器相结合的特征缩放集成</p><p id="2fd1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">19) XGB_SVM_LOG投票:默认XGBoost、支持向量分类器和具有投票分类器的逻辑回归</p><p id="4f3f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">20)XGB _ SVM _日志堆栈:默认XGBoost、支持向量分类器和带有堆栈分类器的逻辑回归</p><p id="8003" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">21) XGB调优:超调优XGBoost模型，使用具有多个调优周期递归试探法。</p><p id="d597" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">逻辑回归详情</strong></p><p id="f57d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">许多教科书会告诉你选择l1或套索正则化而不是l2，因为它在特征选择方面有额外的能力。他们没有告诉你的是，l1的运行时间可能是它的50-100倍。一个模型组使用l1运行了超过72小时而没有完成(风暴期间断电)；使用l2在两个小时内完成了相同的模型集。是的，我现在有一个不间断电源系统。</p><p id="1ea7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">低训练样本数<em class="nd">(&lt;2000):</em></strong><em class="nd">LogisticRegressionCV(penalty = " L1 "，Cs=100，solver='liblinear '，class_weight = None，cv=10，max_iter=20000，scoring="accuracy "，random_state=1) </em></p><p id="b718" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">高训练样本数<em class="nd">(&gt;= 2000):</em></strong><em class="nd">LogisticRegressionCV(penalty = " L2 "，Cs=50，solver='liblinear '，class_weight = None，cv=10，max_iter=20000，scoring="accuracy "，random_state=1) </em></p><p id="e9aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nd">make _ pipeline(standard scaler()，log_model) </em></p><p id="ae7c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">支持向量分类器详情</strong></p><p id="6760" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nd"> SVC(kernel = 'rbf '，gamma = 'auto '，random_state = 1，probability=True) # True启用5重CV </em></p><p id="5edb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nd">make _ pipeline(standard scaler()，svm_model) </em></p><p id="01d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">支持向量分类器与特征尺度集成</strong></p><p id="e93e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nd"> SVC(kernel = 'rbf '，gamma = 'auto '，random_state = 1，probability=True) </em></p><p id="44ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nd">make _ pipeline(standard scaler()，svm_model) </em></p><p id="218b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nd">make _ pipeline(robust scaler(copy = True，quantile_range=(25.0，75.0)，with_centering=True，with_scaling=True)，svm_model) </em></p><p id="a8f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">voting classifier(RF _ SVM _ FSE显示)</strong></p><p id="21c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nd">voting classifier(estimators =[(' RF '，rf_model)，(' std _标准'，标准_处理器)，(' std _罗布'，罗布_处理器)]，voting = '软')</em></p><p id="d48a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">堆叠分类器(RF_SVM_FSE显示)</strong></p><p id="ea10" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nd">估计器= [('RF '，rf_model)，(' SVM标准'，标准_处理器)，(' std _罗布'，罗布_处理器)] </em></p><p id="bc27" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nd">stacking classifier(estimators = estimators，final _ estimator = LogisticRegressionCV(random _ state = 1，cv=10，max_iter=10000)) </em></p><p id="ecb6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在更世俗的方面，当一个极端集合包含一个逻辑回归模型时，那么使用“<em class="nd"> drop_first = True </em>”的一键编码被编码。如果不是，那么'<em class="nd"> drop_first = False </em>'就是标准。</p><p id="21ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">超调xgboost分类器</strong></p><p id="2371" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有许多不同的超调算法使用贝叶斯优化(optuna、Hyperopt等。)，但这是一个使用GridSearchCV的有趣实现的例子，它递归地迭代精化的超参数。在修改了这种自动调整启发式算法以适应分类问题之后，我测试了这种算法，发现尽管运行时间很长，但它经常实现同类最佳的性能。以下是详细情况:</p><p id="7770" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://github.com/SylwiaOliwia2/xgboost-AutoTune" rel="noopener ugc nofollow" target="_blank">sylwiaoliwia 2/xgboost-auto tune:在Python中自动调优xgboost的总结。(github.com)</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/87d0b1ea5c570528a1b3b7af87e0d62c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2cNLjgaJPYNj4CI6Ys7ioQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1基本模型和随机森林极限集合(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/44047f77d0416cdb8f09e9e06192ecce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hSa8kliZ_wiCmXse8yEp1w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2更多随机森林极限合集(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/56d873d8c9be080291ea4a6069e86b28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CknPy1elq2-1Rlb-9DusRA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3 XGBoost Extreme合集(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/887dad1ad18e79e7ab40e1fb9ffe77fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Po3KBaxy-6byjKkF060yJA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4更多XGBoost Extreme系综和超调XGBoost(图片由作者提供)</p></figure><p id="44e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是我们如何确定哪个是最好的模型呢？</p><h1 id="7100" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">介绍性能概率图</strong></h1><p id="f2bd" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">单点准确性不一定不诚实，但肯定是不真诚的。数据科学竞赛之于商业分析，就像真人秀之于现实世界一样，因为使用单个随机种子值对数据进行单次分割，就可以预测一个预定义的未来，一个预先编写好的未来。一些数据具有这种远见。但是我们知道方差，一个随机函数，在大多数情况下有其他的计划——也就是引起混乱。</p><p id="c9f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">性能概率图(PPG) </strong>展示了更接近真实的情况，其中使用100个训练/测试数据的排列生成100个模型，同时保持相同的比率(训练/测试和类别不平衡)。生成的直方图不仅展示了新数据的潜在性能，还展示了实现相同性能的可能性(见图5)。核密度估计器表示概率密度函数，而直方图对应于概率质量函数。</p><p id="0c92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您的业务决策需要87.9%的准确性，即图5中的中值，那么您将有50%的机会是正确的(这种情况与“一半正确”相混淆)。如果你的决策需要94%，这个模型将实现它，即使只是短暂的。所有剩下的模型都代表分析失败。相反，如果你把你的决策阈值设置为81%，你的模型在大多数时候都会成功。PPG是保守的，因为它没有考虑数据漂移，或者新数据如何超出我们当前人口的界限。思考这一现象的一种方式是<strong class="ky ir">对于训练/测试分割的每个排列，我们向模型呈现相同的预测器质量但不同的样本质量，并且每个模型将这些属性转换成实现准确度的概率</strong>。</p><p id="c715" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为使用PPGs的初步调查，我们将比较箱线图和直方图，以寻找整个范围移动到更高精度的模型，而不仅仅是单个新的异常值结果。但是需要更多的工作来有效地比较这些通常非正态的图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/2ae4b36b2ff1ff54df9f4738c646a62b.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*Lb3aoeFH1L2YkhAf8uaMWA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5性能概率图(图片由作者提供)</p></figure><p id="476c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本研究中使用了12个开源数据集，代表了混合的数据类型和复杂性。这里有全浮点和全整数数据，以及数字和分类类型的融合。提出的一个关键指标是样本与预测因子的比率，或样本比率，因为广义误差、预测因子和使用Vapnik-Chervonenkis维数界限的这个方程的训练样本数之间的关系，所以包括这个指标:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/1669bd2afa1136fc9539465c32f38ee8.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*xujQx0lnmR-ob71ZZj_vHQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图6样本大小与使用VC维的一般误差(图片由作者提供)</p></figure><p id="c89a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在这里了解更多关于这项技术的内容:<a class="ae kv" href="https://www.cs.rpi.edu/~magdon/courses/LFD-Slides/SlidesLect07.pdf" rel="noopener ugc nofollow" target="_blank">slides lect 07 . DVI(rpi.edu)</a>。本系列讲座基于从数据中学习(Abu-Mostafa，Magdon-Ismail，&amp; Lin，2012)。本研究中的数据集可容纳从最低约12个样本/预测值到超过440个样本/预测值。</p><p id="6ef3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将采样误差视为一个物理常数，因为它决定了所有模型的精度上限。采样比率将向我们展示关于XGB模型调优(也称为超调)的一些非常重要的东西。</p><p id="30ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为N，期望的样本数出现在等式的两边，如果你在Excel中编码，那么你需要在选项菜单中启用“迭代计算”,然后为N设置一个种子值以准备解决方案。我建议使用10 *预测因子的数量作为一个好的起点，这也是加州理工学院推荐的所需最小样本数。哦，如果你有一个大的样本比率和预测数，你将需要在Wolfram Alpha ( <a class="ae kv" href="https://www.wolframalpha.com/" rel="noopener ugc nofollow" target="_blank"> Wolfram|Alpha:计算智能(wolframalpha.com)</a>中执行计算，因为Excel不能处理极大的数字。</p><p id="d96d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">除电信公司流失外，所有缺失值都用miss forest(<a class="ae kv" href="https://pypi.org/project/MissForest/" rel="noopener ugc nofollow" target="_blank">miss forest PyPI</a>)进行估算；它的11个缺失值从7000多个样本中被删除。所有无信息预测都被丢弃(id等。).当然，所有数据处理都是通过对训练数据使用“fit_transform”和对测试数据使用“transform”来执行的，每个排列和每个模型都保持了数据分区的完整性。测试规模由三个设定点组成:25%、30%和50%，以管理样本比例，但仍保持普遍性。</p><p id="9a09" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">和往常一样，<strong class="ky ir">所有的性能结果，所有的25，200+预测，都是基于测试数据</strong>，那些模型还没有看到的样本。</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h1 id="613e" class="ls lt iq bd lu lv nq lx ly lz nr mb mc jw ns jx me jz nt ka mg kc nu kd mi mj bi translated"><strong class="ak">数据集及其预测性能</strong></h1><p id="e674" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated"><strong class="ky ir">数据集#1 </strong></p><p id="5bbd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">澳大利亚信贷来源:<a class="ae kv" href="https://archive.ics.uci.edu/ml/datasets/statlog+(australian+credit+approval)" rel="noopener ugc nofollow" target="_blank"> UCI机器学习库:Statlog(澳大利亚信贷审批)数据集</a></p><p id="5483" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">样本比率:每个预测值12.32个样本</p><p id="b065" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">呈现给模型的预测值:float64(3)，int64(3)，uint8(36)</p><p id="435f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预期抽样误差:7.36%，置信区间为90%</p><p id="4436" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据Delmaster和Hancock (2001年，第12页),该数据集的训练/测试分割被设置为实现二元分类问题所需的最小样本数。68).</p><p id="f612" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有关基线默认随机森林、默认极端梯度提升和默认极端集合的详细信息，请参见图7。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/9c2e14eace379c477faf5ae22e116f04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tvWsC7-ecm9pLWuglikDtQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图7澳大利亚信贷基线RF_XGB箱线图(图片由作者提供)</p></figure><p id="0275" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意RF和XGB范围之间的重叠，这些是模型相同的性能量。我们可以看到，结合这两个整体并没有提高性能。</p><p id="e4e7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">默认XGB型号的最大PPG范围是从81.5%到93%，这表明存在数据质量问题。您可以将中位数精度视为预测值质量的代理，将PPG离差视为样本质量(即所有观测值的一致性)。大约88%的中值准确度可能对许多决策很有效，因此预测器可能是好的。</p><p id="be16" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">很容易看出，100次排列中的每一次的超调谐过程都没有将PPG缩小到一个很窄的范围内，或者使其更加精确(见图8)；事实上，这与未调整的XGB模型非常相似，但四分位间距(IQR)缩小到了随机森林模型。这显示了超调的局限性— <strong class="ky ir">由于改进很小，数据质量差无法从模型中“调整”出来</strong>。数据比模型更重要的另一个标志。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/0d49487feb93ccb06ab61fe7258092db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0E8E-ucZAKLQtff7G5h9hA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图8澳大利亚信用XGB调谐盒图和PPG(图片由作者提供)</p></figure><p id="a6f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从所有模型中捕获描述性统计数据，我们将尝试从中找到“最佳”模型，而不使用单点性能值。更多信息参见表1。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/b10543a1910e7979eed2d5285aec98db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*Guel_brWHbKLRPqrV2S92A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表1按模型分类的澳大利亚信贷描述性统计(图片按作者分类)</p></figure><p id="f16d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从每个数据集中过滤2100个结果需要设计排序算法来发现“最佳”模型。使用了两种不同的排序方法，并且出现在列表<strong class="ky ir">和</strong>中与顶级模型的精确度在0.3%以内(百分之零点三)的任何模型都被选为顶级模型。严格的精度阈值被选择作为实际需求和对新数据的适应性之间的折衷，但这将在即将到来的工作中重新讨论。</p><p id="54d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">排序#1中位数排序</strong></p><p id="6cfd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">a.50%从最大到最小</p><p id="6da9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1.IQR从最小到最大</p><p id="dad4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">I .从最大到最小为75%</p><p id="5b5c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">排序# 2 75%百分比排序</strong></p><p id="4151" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">a.75%从最大到最小</p><p id="3198" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1.IQR从最小到最大</p><p id="2c55" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">I .从最大到最小各占50%</p><p id="b6b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">均值排序对于算法选择来说不是一个好的度量，因为PPG很少是正态分布，而是具有尖峰，这使得模态量子成为一个更有趣的度量。详细情况请参考图8。</p><p id="8e2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于极低的样本数和较差的数据质量，中值范围很广，只有两个模型通过了严格的0.3%测试(表2中的前两个)，但更多的模型使用第75个百分位排序在下一个量程进行分组:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/e21d0b69ebece902ac2c44f4685e5187.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*QRVfkGaqQ9mLp673WnBWfw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表2澳大利亚信贷最佳表现模型(图片由作者提供)</p></figure><p id="5e70" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总体最佳极端系综(XGB_SVM投票和XGB_SVM堆栈)的有趣之处在于，投票具有较窄的IQR，而堆栈具有较高的中值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/30223cf720c96a48c03ad1fed31ad6ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_3rs6xaQSdGupmTMs6_x0w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图9a澳大利亚信贷最佳表现模型箱线图(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/d55bdc9aed62b960856f80802525454d.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*Iop9jA0KWyBrmg-33wZvew.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图9b澳大利亚信贷最佳表现模型PPG(图片由作者提供)</p></figure><p id="53cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了解析数据质量误差和抽样误差之间的区别，就像我们在澳大利亚信贷中看到的那样，数据集#2将确认数据质量的巨大贡献。</p><p id="5c89" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">数据集#2 </strong></p><p id="9a75" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Wisc诊断来源:<a class="ae kv" href="https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)" rel="noopener ugc nofollow" target="_blank"> UCI机器学习知识库:乳腺癌Wisconsin(诊断)数据集</a></p><p id="c2b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">样本比率:13.28</p><p id="1501" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">呈现给模型的预测值:float64(30)</p><p id="4289" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预期抽样误差:6.97%，置信区间为90%</p><p id="84c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在样本比率仅为～13的情况下，中值都接近或高于96%，这表明可能唯一的误差来自于样本比率。与数据集#1相比，样本质量有所提高，箱线图范围约为6.5%，这可能是由于这30种浮点生物测量值的一致性及其相对相似性。请参见图10，了解低采样率和良好预测性能数据集的详细信息。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/56fac8d910047a7c9427119a06d67258.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pns9J4-qfYKDICGiSe4TWA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图10 Wisc诊断RF_XGB基线箱线图(图片由作者提供)</p></figure><p id="3b7d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，与未经调整的版本相比，超调整的XGBoost模型缩小了IQR，但降低了第75百分位。更多信息参见图11。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/046e2d0e62bb20f2266ed320ebc1c619.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D6dk4awRHmVsBx79jU2YYg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图11 Wisc诊断超调XGB箱线图和PPG(图片由作者提供)</p></figure><p id="40a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用规定的两种排序机制和0.3%的准确度阈值，这些是由中位数排序确定的该数据集的排名最高的模型:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/bf2754fe3883075860bf02fffb128c7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*oTJ6dHYe9LZGEEkGv6IuLg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表3 Wisc Diag排名榜(图片由作者提供)</p></figure><p id="1110" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，有一个单一的顶级模型，接下来的八个被分组到一个较低的量子级别，共享的中值为六位小数。总体最佳极限合奏，XGB _ SVM _日志投票看起来是这样的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/5487e0fed29401527a4ccd94dfe6da64.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*BuDH3oc6Ji2JiB8H8p-1qA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图12 Wisc Diag最佳表现车型PPG(图片由作者提供)</p></figure><p id="0031" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于该数据集，尽管样本数量较少，但由于预测器的复杂性导致100个模型的运行时间较长(使用英特尔12700K、32MB DDR4 3200 RAM和2TB PCIe4固态硬盘时超过3天)，因此对逻辑回归模型采用了L2正则化。</p><p id="e00a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">这就是如何分析数据集的，所有剩余的数据集将在附录A中讨论，以加快我们对完美模型的搜索。</strong></p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h1 id="d62f" class="ls lt iq bd lu lv nq lx ly lz nr mb mc jw ns jx me jz nt ka mg kc nu kd mi mj bi translated"><strong class="ak">寻找完美的模特</strong></h1><p id="fd3f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们收集并分析了所有结果，但在开始搜索之前，我们需要解释图40–42中的信息，即:</p><p id="3e13" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">样本比率</strong>:每个预测器的样本比率。</p><p id="5dc5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">惩罚</strong>:用于逻辑回归模型的正则化惩罚。</p><p id="29e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">最差模型</strong>:根据排序机制，这是该数据集所有21个模型中性能最低的模型。</p><p id="2973" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> PPG最大范围</strong>:使用最差模型，这是所有100个模型的最大精度范围，从一致性来看表示数据质量。记住——更高的T4中值准确度=更好的预测器,更窄的PPG范围=样本之间更好的同质性。</p><p id="117a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">击败宣传</strong>:这显示了我们的通用模型极限合奏是否胜过超调XGB模型。</p><p id="c12e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">除了澳大利亚信贷和输血，模型排名中的每个模型与表现最佳的模型相差不超过0.3%，处于第一位。并非所有型号都出现在这些列表中，因此性能最差的型号可能不会出现在此处。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/ce23b389506aecd2ff39cfde764c5d95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KU3DMt1McV0AvsUbEUb4mQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图40完美模型搜索路径1(图片由作者提供)</p></figure><p id="63a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">快速浏览一下图40中的表格，可以发现在这些数据集上没有一个表现最好的模型。从最严格的角度来看，没有免费的午餐定理成立。然而，使用我们严格的0.3%阈值(同样是0.3%)，有一个通用模型出现在“噪声”之外——XGB _ SVM _对数叠加极端系综(见图42-44)。诚然，澳大利亚信贷和输血是例外，但这是数据质量超过抽样误差的重要一课，在这两种情况下，极端集合仍优于超调谐XGB模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/6e5d6c1d46d186d4123b3e53cdcfbbeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*YI-PSPBDEwSx_bdPYeMGmw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图41完美模型搜索路径2(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/f43a84ebf2a392240539ee90b1cfc3dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*1VOFQ5l1WnJ9PlG9mU7Tog.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图42完美模型搜索路径3(图片由作者提供)</p></figure><p id="0baf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当采样比达到40.36(spam base数据集)时，超调XGB模型终于出现在了榜首，但它落后于extreme ensemble。这种定位在电信客户流失中重复出现，但超调XGB模型和extreme ensemble都没有出现在输血的列表中，输血是一个既有不良预测因素(低中值)又有高样本异质性(宽PPG)的数据集。尽管没有达到0.3%的阈值，并且比顶部低一个量程，但XGB _ SVM _对数堆栈模型仍然以93.5的采样比率击败了超调优XGB，因为超调优模型低两个量程。</p><p id="0cc0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不过，通过最后两个数据集，超调XGB性能优于极端集成，因此该模型似乎更喜欢非常大的采样比。基本上，在每个预测器超过100+样本的某个点上，超调XGB模型优于大多数极端集成。但在此之前，它的总体表现很差，仅在这项研究中的42%的数据集上名列前茅。另一方面，XGB_SVM_LOG堆栈极端集合在85%的时间里达到顶级。<strong class="ky ir">调谐XGB模型需要比建模所需更多的样本，而极端集合不需要调谐。</strong></p><p id="c667" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">没有完美的模型，但这里有两个可能适用于连续系列中的所有数据集——几乎免费的午餐就在这里。从12到大约100+的最小样本比率，使用极端集合，通用模型。100+以上，转超调XGB型号。从实际意义上来说，这两个模型应该可以在任何地方处理表格数据。而且不是只有一个注定的未来，而是有许多可能的未来。</p><p id="0868" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">特殊奖金数据集</strong></p><p id="f1ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为额外的检查和测试采样率上限，对MAGIC Gamma望远镜数据执行了另一个2100模型运行，其采样率为951: <a class="ae kv" href="https://archive.ics.uci.edu/ml/datasets/magic+gamma+telescope" rel="noopener ugc nofollow" target="_blank"> UCI机器学习库:MAGIC Gamma望远镜数据集</a>。结果见图43:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/7de086b5c0d016fc6826408142e189b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*iG0xQnI3quxfppS-sTpNQg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图43 MAGIC Gamma性能结果(图片由作者提供)</p></figure><p id="0441" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，由于采样比率非常高，超调XGB模型是最好的模型，但通用模型XGB_SVM_LOG堆栈仍然使用0.3%阈值的中值排序机制进入了短名单。</p><p id="7d46" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">extreme ensemble有计算成本，需要用低级语言重新编码以提高速度，就像XGB一样。</p><h1 id="4591" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">结论</strong></h1><p id="2a69" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这项研究打开了极端组合的大门，还需要更多的工作来探索这些算法组合。此外，需要更多的研究来开发性能概率图之间的比较分析，也许是基于它们从建模过程中分离误差成分的独特能力。</p><p id="5b15" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">也就是说，假设采样误差为“常数”，PPG允许我们将模型误差区分为两个不同的组:</p><p id="1f50" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1)预测器质量以中值准确度衡量。</p><p id="185b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2)通过PPG系列本身测量的样品质量(一致性)。</p><p id="4ca6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">更高的中值精度由单个模型决定，并且受到采样误差和预测器质量的影响。虽然数据质量是存在的，但它是通过中值计算“平均”出来的。</p><p id="36dc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">相反，更宽的PPG范围是由样本一致性(一种数据质量形式)决定的，因为模型精度之间的差异是由数据划分的差异(即方差的产生)预见的。这种误差源的分离可以指导未来的行为。例如，银行营销数据集的PPG最大范围为0.46%，因此训练/测试分割无关紧要，因为这些数据高度相似。了解这一点将有助于我们专注于提高预测器的质量，因为这是妨碍更好性能的误差。</p><p id="a54c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">由于PPG的范围很窄，我们可以回到决策阈值的单点精确度，因为大量可能的未来已经崩溃为那些密切相关的。</strong></p><p id="3dd3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是其他一些重要的要点:</p><p id="31cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1.数据质量是首要的指示，是道德的要求。花时间获取更好的数据，而不是探索另一种算法，因为<strong class="ky ir">如果数据质量差，所有算法都会有学习障碍</strong>。</p><p id="bc9d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.超调一个XGB模型只能在大采样率的情况下提供高性能，因为<strong class="ky ir">调优需要的样本远多于建模</strong>；在那之前，它是一个表现不佳的模型。<strong class="ky ir">但是如果你有那么大的抽样率，那么这就是可以使用的模型</strong>。</p><p id="a54d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.有一个极端的系综，<strong class="ky ir">XGB _ SVM _日志堆栈</strong>在这项研究的12个数据集的10个中表现最佳——<strong class="ky ir">几乎免费的午餐</strong>。此外，它还登上了Magic Gamma数据集的入围名单——这是又一次确认，使它在13个获奖者中占了11个。</p><p id="c0df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">计划中的研究将探索超调XGB模型持续处于顶级的样本比率。鉴于极端集合中的三个模型中有两个对异常值具有鲁棒性，其他工作正在设计中，以调查PPG最大值范围的样本质量来源。</p><p id="ecb4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">要点</strong>:如果您不知道您的PPG的宽度，那么输入一个随机的种子值进行数据分区就是掷骰子，纯粹是为了让您的模型在未来得到适当的定位。</p><p id="18e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">参考文献</strong></p><p id="4abd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Abu-Mostafa，Y. S .、Magdon-Ismail，m .、和Lin，H.-T. (2012年)。<em class="nd">从数据中学习</em>(第4卷)。美国纽约AMLBook:</p><p id="b804" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">陈、汤、何、贝内斯蒂、米、霍季洛维奇、唐、赵、h……其他。(2015).Xgboost:极限梯度提升。<em class="nd"> R包版本0.4–2</em>，<em class="nd"> 1 </em> (4)，1–4。</p><p id="9003" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">德尔马斯特和汉考克(2001年)。数据挖掘解释。马萨诸塞州波士顿:数字出版社</p><p id="4861" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Fernández-Delgado，m .，Cernadas，e .，Barro，s .，和Amorim，D. (2014年)。我们需要数百个分类器来解决现实世界的分类问题吗？<em class="nd">《机器学习研究杂志》</em>，<em class="nd"> 15 </em> (1)，3133–3181。</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h1 id="34a4" class="ls lt iq bd lu lv nq lx ly lz nr mb mc jw ns jx me jz nt ka mg kc nu kd mi mj bi translated">附录A</h1><p id="e220" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated"><strong class="ky ir">数据集#3 </strong></p><p id="96d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Lending Club来源:<a class="ae kv" href="https://www.kaggle.com/datasets/wordsforthewise/lending-club" rel="noopener ugc nofollow" target="_blank">所有Lending Club贷款数据| Kaggle </a></p><p id="9321" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">样本比率:15.35</p><p id="4988" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">呈现给模型的预测值:float64(4)，int64(1)，uint8(15)</p><p id="ed50" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预期抽样误差:90%置信区间时为6.38%</p><p id="1e86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">缺失值插补:缺失森林</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/725faed52a35a7afc01ba9264f3d4264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jLf3EjRmPTKHWpnYcpPgOQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图13 Lending Club基线RF_XGB箱线图(图片由作者提供)</p></figure><p id="4d5a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，超调XGB模型已经将整个PPG范围移到更高的地面。详情参见图14。这足以将超调XGB模型转移到0.3%阈值内的最高性能列表中，尽管是在最后一位(见表4)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/eb1ebe455843249462ab19a5c981eaaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YP2VyE2k_MAfIkrt8RH4eA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图14借贷俱乐部XGB_TUNE Boxplot和PPG</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/e9c28d7c58499a1a76041e0503886e8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*pdQ5KR4XrX7lo4K7y4359w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表4 Lending Club排名靠前的车型(图片由作者提供)</p></figure><p id="781b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管超调模型有所改进，但这是一个表现最佳的极端组合模型。参见图15，了解RF_LOG堆栈性能概率质量函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/9a5e4d23aed20718c145a631a70c9c7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*QrKHZq6HfvLq11F93dx_IA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图15借贷俱乐部最佳表现模式(图片由作者提供)</p></figure><p id="582e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">数据集#4 </strong></p><p id="8ccf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">德国信用来源:<a class="ae kv" href="https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)" rel="noopener ugc nofollow" target="_blank"> UCI机器学习知识库:Statlog(德国信用数据)数据集</a></p><p id="d148" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">样本比率:16.67</p><p id="4070" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">呈现给模型的预测值:int64(30)</p><p id="2bea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预期抽样误差:90%置信区间时为6.31%</p><p id="96e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">全整数预测器显示了随机森林模型相对于XGBoost的明显优势(见图16)。过度调整XGB模型并没有改善结果的范围，只是略微缩小了四分位数范围(见图17)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/a62126775810777e97bf7ffddc68b2ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FBMVKndTVnCKX-zDkxQLRw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图16德国信贷基线RF_XGB箱线图(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/37c418c16bfb28654c7840d16b88c482.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m6YB-O9u-cnb_xpm1U2caw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图17 Lending Club XGB_TUNE Boxplot和PPG(图片由作者提供)</p></figure><p id="7599" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">数据集#5 </strong></p><p id="e5c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">HR流失来源:<a class="ae kv" href="https://www.kaggle.com/datasets/yasserh/ibm-attrition-dataset" rel="noopener ugc nofollow" target="_blank"> IBM流失数据集| Kaggle </a></p><p id="f1fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">样本比率:19.34</p><p id="a253" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">呈现给模型的预测值:int64(19)，uint8(19)</p><p id="5911" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预期抽样误差:5.99%，置信区间为90%</p><p id="1ff0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">采样比率再次增加，数据类型现在是整数和二进制预测值的均匀混合。在基线箱线图中，极端系综显示出最好的性能，投票分类器挤掉了堆叠分类器；投票运行速度比堆叠快得多，所以当你可以选择一个投票极端合奏时，推荐。详情参见图18。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/22ed50b2a05277d8feb4fea62f3b8695.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wv4R2ef5CSiBj2uXorG5fw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图18 HR流失基线RF_XGB箱线图(图片由作者提供)</p></figure><p id="16c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">超调XGB模型确实限制了最小-最大距离，并且与默认模型相比，它将IQR压缩到更窄的范围内，所有这些都是准确性和稳定性提高的迹象(见图19)。即便如此，超调模型也没有在这个阈值为0.3%的数据集上排名第一(见表6)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/93fa6231427bb81020fb26890c35d28f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4y0cX2mdPNo9TNVFdEuMsg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图19 HR Churn XGB调谐盒图和PPG(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/74592b8376587bc03bfcaf60be822587.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*oqKchTqzezkVZowyCFzByQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表6人力资源流失排名靠前的模型(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/ea14e69c39e13eecfd661ba44f6b76ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*_fC_sWOHpIEK4D744VO15Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图20人力资源流失最佳表现模型PPG(图片由作者提供)</p></figure><p id="09b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">数据集#6 </strong></p><p id="3366" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ILPD: <a class="ae kv" href="https://archive.ics.uci.edu/ml/datasets/ILPD+(Indian+Liver+Patient+Dataset)" rel="noopener ugc nofollow" target="_blank"> UCI机器学习知识库:ILPD(印度肝病患者数据集)数据集</a></p><p id="f339" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">样本比率:26.5</p><p id="7288" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">呈现给模型的预测值:float64(5)，int64(4)，uint8(2)</p><p id="0ee8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预期抽样误差:4.84%，置信区间为90%</p><p id="6c14" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不考虑模型，该数据集显示了非常强的模式和弱的预测器以及宽的PPG范围(更多细节见图22和23)。请注意，默认的随机森林模型在这个数据上胜过了默认的XGB模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/0a9d0f614b17c39aceb34a5282024698.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W_YsawOOLzfC4XE2Mm8TSg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图21 ILPD基线RF_XGB箱线图(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/a55e88746129e26688792769780f130e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*510U7B6BOiohCiDrPcSmow.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图22 ILPD XGB调谐盒图和PPG(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/f8a3cf76d074cc19a07cc2c3af0953a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*Xs_FsJ5_zWkhzPYPVmD8eQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表7 ILPD排名靠前的车型(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/0a6816eefba4b9433cdb587d32dec5dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*HLv9uZjHA17JsDMQCLS07g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图23 ILPD最佳表演模特PPG(图片由作者提供)</p></figure><p id="9285" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="nd">剩余的数据集将只作为结果呈现给限定的文本。</em>T3】</strong></p><p id="71cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">数据集#7 </strong></p><p id="807a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">NBA新秀:<a class="ae kv" href="https://www.kaggle.com/competitions/iust-nba-rookies/data" rel="noopener ugc nofollow" target="_blank"> NBA新秀|卡格尔</a></p><p id="f654" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">样本比率:34.97</p><p id="8a79" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">呈现给模型的预测值:float64(18)，int64(1)</p><p id="fdf9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预期抽样误差:4.43%，置信区间为90%</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/efe48b91312aec3935926dea3e7d1062.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ol51ECy52ahlBAAf7_b6Dg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图24 NBA新秀基线RF_XGB箱线图(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/e206c45e65b68f77653c3c08a84da5f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hqj1jMmH1Sp6AZDaORSdTQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图25 NBA新秀XGB TUNE Boxplot和PPG(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/04b2bf734ee20f17ca6099b8c83045d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*HVnV5Lij8-V318S1xziHlA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表8 NBA新秀顶级模特(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/4c6f1595c60152fea0dc5137b63dfb7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*Ns15QvkrgIZnfRRPNWM43w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图26 NBA新秀最佳表演模特PPG(作者图片)</p></figure><p id="787d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">数据集#8 </strong></p><p id="8bf1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Spambase: <a class="ae kv" href="https://archive.ics.uci.edu/ml/datasets/spambase" rel="noopener ugc nofollow" target="_blank"> UCI机器学习库:Spambase数据集</a></p><p id="4c4e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">样本比率:40.36</p><p id="7afb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">呈现给模型的预测值:float64(55)，int64(2)</p><p id="4365" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预期抽样误差:4.41%，置信区间为90%</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/79195eda24a3f70d2f4513db27351b00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FOapNREyafdBOtjvvtJ3rA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图27 Spambase基线RF_XGB箱线图(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/95caa7a2d1968cf965edc0eb38383f2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aC1bI2eAGfa_9izqppEv3A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图28 Spambase XGB调谐盒图和PPG(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/bdfb95755b19daedd33766ec63ed1e4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*q9DkUiO3hbrWYo_6n-P38g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表9 Spambase排名靠前的模型(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/17f0c11d51c28268587be5ad67a262ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*iAeHfLbwgXLntEhdT7MzhQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图29 Spambase顶级表演模特PPG(图片由作者提供)</p></figure><p id="4314" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">数据集#9 </strong></p><p id="1475" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">电信客户流失:<a class="ae kv" href="https://www.kaggle.com/datasets/blastchar/telco-customer-churn" rel="noopener ugc nofollow" target="_blank">电信客户流失|卡格尔</a></p><p id="aca9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">样本比率:78.25</p><p id="5f8f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">呈现给模型的预测值:float64(2)，int64(2)，uint8(41)</p><p id="0898" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预期抽样误差:3.24%，置信区间为90%</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/bafc6aaac40c42be3050f31a15910503.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b1rOy2uTjdBLglP1Zp94WQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图30电信客户流失基线RF_XGB箱线图(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/772080037b017a89b927666355a87eb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3uTpDB8qv5xHkb41-l3RPg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图31电信客户流失XGB调谐盒图和PPG(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/08e9bd3fd208106091bc8099d44d8bb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*1TDo_MQI2m8GdjxDWaJrCw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表10电信客户流失排名模型(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/18d00e5a9e2782c6b57e091ea4a73ea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*wZRKiM4T9ETvwBCCb4k5pA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图31电信客户流失最佳表现模型PPG(图片由作者提供)</p></figure><p id="1394" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">数据集#10 </strong></p><p id="b2dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输血:<a class="ae kv" href="https://archive.ics.uci.edu/ml/datasets/Blood+Transfusion+Service+Center" rel="noopener ugc nofollow" target="_blank"> UCI机器学习知识库:输血服务中心数据集</a></p><p id="d6b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">样本比率:93.5</p><p id="dcd6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">呈现给模型的预测值:int64(4)</p><p id="f0d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预期抽样误差:2.62%，置信区间为90%</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pe"><img src="../Images/88cb44b5f2f22665cb51027767c12a30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rId0QiF3fjH_DmvaGxF6Mg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图32输血基线RF_XGB箱线图(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pf"><img src="../Images/391b4263f97f15d472a4fd047dcec861.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-22vOSDfOSvUyVNAWVBQ-A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图33输血XGB调谐盒图和PPG(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/72aeb1676f8d3b69338ed1dd681db4cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*aXETRUSMLdy2N9-GqplXmA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表12输血顶级模型(图片由作者提供)</p></figure><p id="153e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">数据集#11 </strong></p><p id="e57c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">成人收入:<a class="ae kv" href="https://archive.ics.uci.edu/ml/datasets/adult" rel="noopener ugc nofollow" target="_blank"> UCI机器学习知识库:成人数据集</a></p><p id="0b24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">样本比率:246.67</p><p id="eeb5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">呈现给模型的预测因子:int64(6)，uint8(60)</p><p id="76bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预期抽样误差:1.18%，置信区间为90%</p><p id="03bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意:预测值“国家”被删除，因为许多国家的样本太少。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ph"><img src="../Images/41064fb82a738006d65797f88a40b10b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xpeDXxll4QaKle43asPw8g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图34成人收入基线RF_XGB箱线图(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pi"><img src="../Images/3192e453ae3ce25619ee6d0386b32df8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UK9TSC_xt86k8sFBQzrW7Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图35成人收入XGB调盒图和PPG(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/3867b37a894545717e786f9b66c39ed1.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*irotELW7l3DubxUMUSdxfQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表13成人收入最高的模型(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/524fa48a4e7e711535e979101de8d6bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*yacEP49anejH3Z22IErU-A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图36你会选择哪个决策阈值？(图片由作者提供)</p></figure><p id="e91b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">第12号数据集</strong></p><p id="6066" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">银行营销:<a class="ae kv" href="https://archive.ics.uci.edu/ml/datasets/bank+marketing" rel="noopener ugc nofollow" target="_blank"> UCI机器学习知识库:银行营销数据集</a></p><p id="890b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">样本比率:443.25</p><p id="9fbf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">呈现给模型的预测因子:int64(7)，uint8(44)</p><p id="6502" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预期抽样误差:1.10%，置信区间为90%</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pl"><img src="../Images/23b5b03db89187ecd5cb99577334e312.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cs20McfAp64QfOcUjI1a3Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图37银行营销基线RF_XGB箱线图(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pm"><img src="../Images/52a84e57e5ba0dfde9a48fba77bc5d81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cYXR5ec0rupreskxj1Af0A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图38银行营销XGB TUNE Boxplot和PPG(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/80e6b742ea29521916475537d3fdf9a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*pTsmTshdLs2BPuIfIoVumQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表14银行营销顶级模型(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/3377b8aa3f98b01dd82de1c8c3d4e858.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*eRnzPKHEuas_u5KMCGsFpA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图39银行营销最佳表现模型PPG(图片由作者提供)</p></figure></div></div>    
</body>
</html>