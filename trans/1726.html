<html>
<head>
<title>Intergenerational Mobility in the US — Regression (4/5)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">美国的代际流动——回归(4/5)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/intergenerational-mobility-in-the-us-regression-85b25499e6e6#2022-04-22">https://towardsdatascience.com/intergenerational-mobility-in-the-us-regression-85b25499e6e6#2022-04-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><blockquote class="jn jo jp"><p id="1836" class="jq jr js jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ij bi translated">本系列由Ibukun Aribilola和Valdrin Jonuzi共同撰写，是社会公益数据科学教程的一部分。</p></blockquote><p id="cfe3" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">在<a class="ae ks" href="https://apps.streamlitusercontent.com/ibukunlola/dssg_final_project/main/finalised_notebooks%2FRegression%2Fregression.py/+/LINK" rel="noopener ugc nofollow" target="_blank">上一篇文章</a>中，我们使用Pandas Profiling和地理空间制图来获得对数据集的整体理解。我们还研究了如何使用主成分分析和反向消除来缩减变量列表，以提高分析性能。</p><p id="d349" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">在本文中，我们将讨论交叉验证以及为什么要这样做，扩展本系列第三篇文章中的普通最小二乘(OLS)回归模型，并使用XGBoost运行梯度推进回归模型。最后，我们将使用均方根误差和r平方等指标来讨论这些模型的性能。本节使用的代码的完整笔记本可以在<a class="ae ks" href="https://github.com/valdrinj/dssg_final_project/blob/main/finalised_notebooks/Regression/IA_regression.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="0bea" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">交叉验证</h1><p id="582d" class="pw-post-body-paragraph jq jr iq jt b ju lr jw jx jy ls ka kb kp lt ke kf kq lu ki kj kr lv km kn ko ij bi translated">在设计预测模型时，我们需要采取多种措施来确保模型的稳健性。换句话说，模型在外部数据集上产生的结果应该与在训练数据集上产生的结果相似。交叉验证是一种帮助我们建立稳健模型的程序，因为它涉及对数据集进行重新采样，根据数据子集训练模型，并根据不同的数据子集测试模型，以查看它与测试数据集的吻合程度。它也被称为k倍交叉验证，k指的是数据被分成的子集的数量。</p><p id="5920" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">在此分析中，我们通过将数据集随机分为两组(训练集和测试集)来进行双重交叉验证。我们使用来自<a class="ae ks" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" rel="noopener ugc nofollow" target="_blank"> sklearn </a>的<code class="fe lw lx ly lz b">train_test_split</code>函数来随机拆分数据集，其中三分之二在训练集中，三分之一在测试集中。</p><pre class="ma mb mc md gt me lz mf mg aw mh bi"><span id="9b43" class="mi ku iq lz b gy mj mk l ml mm">from sklearn.model_selection import train_test_split<br/><em class="js"># load data</em><br/>df = pd.read_csv("../../data/processed/processed_data.csv")</span><span id="7ca5" class="mi ku iq lz b gy mn mk l ml mm"><em class="js"># start with all 125 predictors in the dataset</em><br/>predictors = df[df.columns[6:]]<br/><em class="js">#pull out the outcome variable column</em><br/>outcome = df['causal_p75_cty_kr26']</span><span id="dec9" class="mi ku iq lz b gy mn mk l ml mm"><em class="js">#cross-validation, train-test split</em><br/>pred_train, pred_test, outcome_train, outcome_test = train_test_split(<br/>    predictors, outcome, test_size=1/3, random_state=10)</span></pre><h1 id="96a6" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">OLS回归</h1><p id="78e7" class="pw-post-body-paragraph jq jr iq jt b ju lr jw jx jy ls ka kb kp lt ke kf kq lu ki kj kr lv km kn ko ij bi translated">让我们回顾一下上一篇文章。逆向选择过程留给我们10个变量。下图显示了反向选择过程的输出，这是一个包含这10个变量的OLS回归模型。</p><figure class="ma mb mc md gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mo"><img src="../Images/be3faa04b67b7cc08dfe0e9b0da2a3cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6aOo1XePEI4sb94AhFu96A.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">图片来自作者。</p></figure><p id="5709" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">这个模型的(差)表现将在下一小节讨论，所以让我们集中解释变量对代际流动性的影响大小。一些影响最大的变量是<code class="fe lw lx ly lz b">frac_5</code>、<code class="fe lw lx ly lz b">frac_9</code>和<code class="fe lw lx ly lz b">num_inst_pc</code>。换句话说，在国民收入分配的第五和第九个十分位数中，父母每增加一个分数，代际流动性就分别增加24.4和15.7。另一方面，根据这个效应大小为-12.9的模型，人均大学数量对代际流动有负面影响。</p><h1 id="0eba" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">韵律学</h1><p id="e65b" class="pw-post-body-paragraph jq jr iq jt b ju lr jw jx jy ls ka kb kp lt ke kf kq lu ki kj kr lv km kn ko ij bi translated">你可能会发现自己想知道这个模型的结果意味着什么，或者如何判断这个模型是否“好”。有几个指标可以帮助我们确定我们的回归模型是否能很好地预测结果变量。在本文中，我们将关注其中的两个指标:均方根误差(RMSE)和r平方。</p><h2 id="329c" class="mi ku iq bd kv na nb dn kz nc nd dp ld kp ne nf lh kq ng nh ll kr ni nj lp nk bi translated">均方根误差</h2><p id="41a7" class="pw-post-body-paragraph jq jr iq jt b ju lr jw jx jy ls ka kb kp lt ke kf kq lu ki kj kr lv km kn ko ij bi translated">RMSE是通过计算模型的预测结果值和实际结果值之差的平方根得到的一个指标。目标是使RMSE为零或尽可能接近零。Python的“statsmodels”库有一个rmse特性，可以自动计算这个值。在下面的代码单元格中，我们用我们开始的所有125个变量计算OLS模型的RMSE(向后选择的第2步)。这个模型的RMSE是2.44。</p><pre class="ma mb mc md gt me lz mf mg aw mh bi"><span id="d063" class="mi ku iq lz b gy mj mk l ml mm">import statsmodels.api as sm</span><span id="4d74" class="mi ku iq lz b gy mn mk l ml mm"><em class="js"># Run regression model on all variables</em><br/>OLS_mod = sm.OLS(outcome_train, pred_train).fit()<br/>y_pred_init = OLS_mod.predict(pred_test)<br/>rmse_init = sm.tools.eval_measures.rmse(y_pred_init, outcome_test)<br/>print("RMSE:", rmse_init)<br/>OLS_mod.summary()</span></pre><p id="7bd5" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">下面的代码单元格显示了我们如何计算有10个变量的后向选择模型的RMSE。这个降维模型的RMSE是2.28，与第一个OLS模型相比下降了6.5%，这是一个改进。</p><pre class="ma mb mc md gt me lz mf mg aw mh bi"><span id="ed57" class="mi ku iq lz b gy mj mk l ml mm">p = 0.05<br/>back_elim = backwards_elimination(outcome_train, pred_train, p)<br/>ols = back_elim[0]<br/>features = back_elim[1]<br/><em class="js"># use model to predict outcome variable</em><br/>y_pred = ols.predict(pred_test[features.columns])<br/><em class="js"># calculate rmse</em><br/>rmse = sm.tools.eval_measures.rmse(y_pred, outcome_test)<br/>print("RMSE: %.3f, <br/>Features: %s" % (rmse, features.columns.values), "<br/>Model summary:<br/>", ols.summary())</span></pre><h2 id="c709" class="mi ku iq bd kv na nb dn kz nc nd dp ld kp ne nf lh kq ng nh ll kr ni nj lp nk bi translated">r平方</h2><p id="236a" class="pw-post-body-paragraph jq jr iq jt b ju lr jw jx jy ls ka kb kp lt ke kf kq lu ki kj kr lv km kn ko ij bi translated">r平方值是一个度量标准，它描述了输入变量在结果变量中所占的百分比。在包含所有125个输入变量的第一个OLS模型中，r平方值为0.078或7.8%，这仅仅意味着还有其他代际流动性的预测因素没有包括在我们的数据中。第二个模型有10个输入变量，其r平方值为0.029或2.9%，这是有意义的，因为我们已经放弃了许多可能在某种程度上预测代际流动的变量。</p><p id="4c83" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">我们还可以在预测值与实际值的曲线图上直观显示两个模型的拟合情况。该图传递了与RMSE相同的信息，即预测值和实际值之间的差异。拟合良好的模型将显示位于45度线附近的分散点，表明它们高度相关。后向消除模型比第一个模型具有更好的拟合，如第三个图中其直线的陡度所突出显示的。</p><figure class="ma mb mc md gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nl"><img src="../Images/25df3dd08732ccc8bc3503d049bba691.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4RIajEbESOBjzOEcwvw0lA.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">图片来自作者。</p></figure><p id="fe6e" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">总之，两个OLS回归模型都不是很稳健，因为模型中没有包括某些变量，这些变量解释了90%以上的代际流动性差异。但是，后淘汰模型有更好的拟合。</p><h1 id="e49b" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">XGBoost回归</h1><p id="0ba7" class="pw-post-body-paragraph jq jr iq jt b ju lr jw jx jy ls ka kb kp lt ke kf kq lu ki kj kr lv km kn ko ij bi translated">第二个回归模型使用<a class="ae ks" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank">梯度提升树算法</a>来预测结果变量。我们使用Python中的<a class="ae ks" href="https://xgboost.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>模块运行回归模型并进行特征选择，即减少模型中包含的变量数量。让我们从将这个分析所需的包导入Python开始。</p><pre class="ma mb mc md gt me lz mf mg aw mh bi"><span id="814c" class="mi ku iq lz b gy mj mk l ml mm"><em class="js"># load relevant packages</em><br/>from sklearn.feature_selection import SelectFromModel<br/>from sklearn import metrics<br/>from sklearn.compose import ColumnTransformer<br/>from xgboost import plot_importance<br/>from xgboost import XGBRegressor</span></pre><p id="c2a8" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">在高层次上，XGBoost中的特性选择非常类似于本系列第三篇文章中概述的OLS特性选择步骤，主要区别在于变量选择标准。此分析的选择标准是特征重要性分数f-score，也称为“增益”。增益衡量变量对其所添加的分支带来的精度提高。在<a class="ae ks" rel="noopener" target="_blank" href="/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7">这篇文章</a>中阅读更多关于特性重要性分数的信息。下面的代码单元格显示了如何用所有125个输入变量拟合一个回归模型，并生成一个重要性图。</p><pre class="ma mb mc md gt me lz mf mg aw mh bi"><span id="aa97" class="mi ku iq lz b gy mj mk l ml mm"><em class="js"># fit model with training data</em><br/>model = XGBRegressor()<br/>model.fit(pred_train, outcome_train)<br/><em class="js"># feature importance</em><br/>print(model.feature_importances_)<br/><em class="js"># plot importance</em><br/>fig, ax = plt.subplots(figsize=(10, 35))<br/>plot_importance(model, ax=ax, grid=False, importance_type="gain")<br/>plt.show()</span></pre><p id="62ef" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">下面是重要性图的一个片段。它以降序显示每个变量及其重要性分数，最“重要”的变量位于顶部。</p><figure class="ma mb mc md gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nm"><img src="../Images/06dffbf91a59a652b5e7fbb79b6980e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HVStmeYZcTHYrCSQbP8Gdw.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">图片来自作者。</p></figure><p id="379a" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">我们可以使用RMSE评估该模型的拟合度，代码如下所示。</p><pre class="ma mb mc md gt me lz mf mg aw mh bi"><span id="a435" class="mi ku iq lz b gy mj mk l ml mm"><em class="js"># run training model on the test dataset</em><br/>y_pred = model.predict(pred_test)<br/><em class="js"># evaluate the model using the mean squared error</em><br/>rmse = np.sqrt(metrics.mean_squared_error(outcome_test, y_pred))<br/>print(" Root Mean squared error:", rmse)</span></pre><p id="714f" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">特征选择是通过按重要性分数的升序对变量进行排序，并使用具有相同或更高特征重要性分数的变量运行回归分析来完成的。在这种情况下，重要性分数是阈值或变量选择标准。对于每个模型，我们计算RMSE，以便我们可以检查每个模型的性能，然后选择一个最适合我们的模型。下面的代码单元显示了我们如何完成这个特性选择过程。</p><pre class="ma mb mc md gt me lz mf mg aw mh bi"><span id="f4b7" class="mi ku iq lz b gy mj mk l ml mm">thresholds = np.sort(model.feature_importances_)<br/>for thresh in thresholds:<br/>	<em class="js"># Select features using threshold=importance. Only variables with equal or higher importance will be selected</em><br/>	selection = SelectFromModel(model, threshold=thresh, prefit=True)</span><span id="85b7" class="mi ku iq lz b gy mn mk l ml mm">	<em class="js"># Get indices of selected variables and find the corresponding column names. Source: Liang (https://stackoverflow.com/a/42527851)</em><br/>	feature_idx = selection.get_support()<br/>	feature_name = pred_train.columns[feature_idx]</span><span id="2a1a" class="mi ku iq lz b gy mn mk l ml mm">	<em class="js"># Subset the predictor dataframe with the selected features</em><br/>	select_X_train = selection.transform(pred_train)</span><span id="acee" class="mi ku iq lz b gy mn mk l ml mm">	<em class="js"># Train model</em><br/>	selection_model = XGBRegressor()<br/>	selection_model.fit(select_X_train, outcome_train)</span><span id="b827" class="mi ku iq lz b gy mn mk l ml mm">	<em class="js"># Evaluate model</em><br/>	select_X_test = selection.transform(pred_test)<br/>	y_pred = selection_model.predict(select_X_test)<br/>	rmse = np.sqrt(metrics.mean_squared_error(outcome_test, y_pred))</span><span id="4a9b" class="mi ku iq lz b gy mn mk l ml mm">	<em class="js"># Only print out the list of features when we have 10 or less features to save space </em><br/>	if select_X_train.shape[1]&lt;=10:<br/>		print("Thresh=%.3f, n=%d, RMSE: %.3f, Features: %s" % (thresh, select_X_train.shape[1], rmse, feature_name))<br/>	else:<br/>		print("Thresh=%.3f, n=%d, RMSE: %.3f" % (thresh, select_X_train.shape[1], rmse))</span></pre><p id="cb9e" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">这个代码单元的结果是125行文本，每一行都说明了重要性分数阈值、满足阈值的变量数量以及模型的RMSE。</p><figure class="ma mb mc md gt mp gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/315eb415b2b65d280c5158f8faa7a919.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*ZEpud2tI0EgUqQ2Cf5B53Q.png"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">图片来自作者。</p></figure><p id="e57b" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">RMSE最低(2.680)的模型以本科院校人数为输入变量。第二好的模型是RMSE模式(2.684)，使用本科院校的人数和人均院校数量作为输入变量。</p><p id="208c" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">虽然XGBoost模型产生的模型的RMSEs没有OLS回归模型小，但XGBoost仍然是一个很好的工具，因为它产生的预测模型通常表现更好。</p><p id="e80f" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">XGBoost部分的代码改编自Jason Brownlee的文章。</p><h1 id="bc94" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">结论</h1><p id="3613" class="pw-post-body-paragraph jq jr iq jt b ju lr jw jx jy ls ka kb kp lt ke kf kq lu ki kj kr lv km kn ko ij bi translated">在本文中，我们讨论了交叉验证及其对防止预测模型过度拟合的重要性。我们还评估了反向消除前后的OLS回归模型。我们讨论了作为替代回归工具的XGBoost，并介绍了如何运行XGBoost模型和进行特性选择。我们使用RMSE和r平方作为我们的回归模型的度量，并发现后向后消除模型比前向后消除模型和最佳XGBoost模型具有更好的拟合。在特征选择之后出现的一些值得注意的变量是人均大学数量、大学生数量以及在国民收入排名中处于第五十分位数的父母的比例。在下一篇文章中，我们将学习如何使用分类方法来预测代际流动以及这种方法的利弊。</p></div></div>    
</body>
</html>