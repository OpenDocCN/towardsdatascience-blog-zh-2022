<html>
<head>
<title>Saga of the Lottery Ticket Hypothesis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">彩票假说的传奇</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/saga-of-the-lottery-ticket-hypothesis-af30091f5cb#2022-08-12">https://towardsdatascience.com/saga-of-the-lottery-ticket-hypothesis-af30091f5cb#2022-08-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="daca" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">中奖彩票是如何被发现、揭穿和重新发现的</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/96d547c867b9a93600968892ce8f11b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jb07XiONxy2jMpRqQYjDqw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者创作)</p></figure><p id="60e3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">彩票假设(LTH)与神经网络修剪相关，可以通过下面的陈述简明地概括出来[1]:</p><blockquote class="lu lv lw"><p id="7451" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">“密集的、随机初始化的前馈网络包含子网(中奖票)，当独立训练时，在相似的迭代次数下，达到与原始网络相当的测试精度。”</p></blockquote><p id="4c53" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然有点难以解释，但这种说法表明，如果我们:</p><ul class=""><li id="48de" class="mb mc it la b lb lc le lf lh md ll me lp mf lt mg mh mi mj bi translated">训练神经网络收敛</li><li id="8f7d" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">修剪它的权重，产生一个更小的子网</li><li id="e321" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">将子网权重倒回其原始的随机初始化值</li><li id="60c8" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">训练产生的子网收敛</li></ul><p id="1ec0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，子网(通常被称为“中奖票”)将实现匹配或超过子网所源自的原始密集神经网络性能的测试精度。</p><p id="b01d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这样的观察对于深度学习社区来说很有趣，因为这意味着随机子网络存在于密集网络中，即<em class="lx"> (i) </em>更小/计算效率更高，并且<em class="lx"> (ii) </em>可以被独立训练以表现良好。如果这些中奖彩票可以很容易地被识别出来，那么通过简单地训练子网络而不是完整的密集模型，神经网络的训练成本就可以大大降低。</p><p id="4a54" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过LTH启发的发现来降低训练复杂性的目标尚未实现，因为发现中奖彩票需要密集网络的全部(或部分[2])预训练，这是一个计算繁重的过程。但是，深度学习社区继续研究LTH，因为它有可能通知和简化神经网络训练。</p><h1 id="dff8" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">背景资料</h1><p id="2259" class="pw-post-body-paragraph ky kz it la b lb nh ju ld le ni jx lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">在进入主题之前，我将提供理解LTH所需的相关背景信息。我的目标是通过提供相关背景的全面总结来建立对该主题的基础理解。然而，在某些情况下，提供一个主题的完整概述是不实际的或不在范围之内的，所以我提供了外部资源的链接，可以用来更深入地理解一个观点。</p><h2 id="6f92" class="nm mq it bd mr nn no dn mv np nq dp mz lh nr ns nb ll nt nu nd lp nv nw nf nx bi translated"><strong class="ak">什么是神经网络修剪？</strong></h2><p id="368d" class="pw-post-body-paragraph ky kz it la b lb nh ju ld le ni jx lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">为了充分理解LTH，人们必须熟悉LTH所基于的<strong class="la iu">神经网络修剪</strong>【3，4，5】。剪枝背后的基本思想是从一个大型、密集的网络中删除参数，产生一个更小、计算效率更高的(可能是稀疏的)子网。虽然我在这里引用了一些关于神经网络修剪的最有用的论文，但是网上有许多高质量的资源<a class="ae ny" href="https://jacobgil.github.io/deeplearning/pruning-deep-learning" rel="noopener ugc nofollow" target="_blank">有助于探索。</a></p><p id="334f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">理想情况下，修剪后的子网应该表现得与密集网络相似，尽管如果移除大量参数，情况可能并非如此。因此，修剪过程的目标是找到并移除不会显著损害网络性能的参数。</p><p id="5026" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在大多数情况下，神经网络修剪遵循预训练、修剪和微调的三步过程。密集网络首先被部分地或收敛地预训练。然后，在这个密集网络上执行修剪，并且在修剪发生之后，进一步训练/微调所产生的子网。</p><h2 id="d8d8" class="nm mq it bd mr nn no dn mv np nq dp mz lh nr ns nb ll nt nu nd lp nv nw nf nx bi translated">不同类型的修剪</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/ee75e48f5b289f7462e88038492a5d0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nsw35QxZxU7LdvGFtZvuKA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">描述前馈和卷积网络的结构化和非结构化修剪方法(由作者创建)</p></figure><p id="1c6f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">存在许多用于神经网络修剪的技术，但是它们可以粗略地分为两组— <strong class="la iu">结构化和非结构化修剪</strong>。</p><p id="5a00" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">非结构化修剪</strong>对修剪过程没有任何限制。网络中的任何和所有权重可以被单独修剪/移除，这意味着在执行修剪之后，所得到的网络通常是稀疏的。</p><p id="5aa2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">相比之下，<strong class="la iu">结构化修剪</strong>移除整个权重“组”(例如，卷积滤波器、前馈层中的神经元、变压器中的注意力头等)。)一起从基础网络中分离出来，以避免产生的子网中的稀疏性。因此，子网只是一个更小、更密集的模型。</p><p id="a98c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">结构化和非结构化修剪方法(如上图所示)都被广泛使用——没有一种方法一定“更好”。非结构化修剪允许在不降低性能的情况下达到更高的稀疏度，因为它对修剪过程的约束更少。然而，结构化修剪具有产生密集子网的额外好处，从而允许避免使用用于稀疏矩阵乘法的专用库，这通常会使网络训练/推理慢得多。</p><h2 id="8bfa" class="nm mq it bd mr nn no dn mv np nq dp mz lh nr ns nb ll nt nu nd lp nv nw nf nx bi translated">迭代幅度修剪(IMP)</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/5babdc39fb2f4586e8e4d4c9bcd02fd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nT-Pwl4h7bzvfV1wfm2vqg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="ob">基于信道比例因子的卷积神经网络的IMP(来自【8】)</em></p></figure><p id="5161" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最常用于研究LTH的神经网络修剪类型是<strong class="la iu">迭代幅度修剪(IMP)</strong>【4】，其操作如下:</p><ol class=""><li id="c781" class="mb mc it la b lb lc le lf lh md ll me lp mf lt oc mh mi mj bi translated">从一个训练有素的密集模型开始</li><li id="6a9a" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt oc mh mi mj bi translated">选择并修剪网络中的最低幅度权重</li><li id="e572" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt oc mh mi mj bi translated">微调/训练产生的子网进行收敛</li><li id="f1e8" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt oc mh mi mj bi translated">重复步骤(2)-(3)，直到达到所需的修剪比率</li></ol><p id="98a8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">尽管IMP背后的想法看起来很简单，但是这种方法在实践中非常有效，并且已经证明了自己是一个难以超越的基线[4，5]。因为IMP运行得如此之好，许多有用的解释都可以在网上找到。</p><p id="60a2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当应用于结构化修剪或用于更复杂的网络变体时，IMP有时会被修改。例如，用于卷积神经网络中结构化滤波器修剪的IMP的一个变体为网络中的每个通道分配单独的缩放因子，然后基于这些因子的大小而不是权重本身来修剪滤波器[8]。请参见上图中的示意图。</p><h1 id="405b" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated"><strong class="ak">出版物</strong></h1><p id="c812" class="pw-post-body-paragraph ky kz it la b lb nh ju ld le ni jx lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">在此，我将概述与LTH相关的三份重要文件。我考虑的第一个作品提出了LTH，而其他作品则进一步分析了这个想法并提供了有用的见解。有趣的是，LTH在其提议后不久就被部分揭穿，随后在原作者的后续论文中得到证实(有所修改)。围绕这个话题的争论导致了研究界的许多困惑，我将尽力在这里澄清这些困惑。</p><h2 id="c16d" class="nm mq it bd mr nn no dn mv np nq dp mz lh nr ns nb ll nt nu nd lp nv nw nf nx bi translated"><strong class="ak">彩票假说:寻找稀疏的、可训练的神经网络[1] </strong></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/7c137bbfe1bb41480a98f1823e3d173d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kULl3ZpNdsxvIn4MqjdM4w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="ob">描绘在LTH发现中奖彩票的IMP +倒带(作者创作)</em></p></figure><p id="16f8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">主旨。</strong>模型可以很容易地削减到总参数的&lt; 10%。但是，如果我们:</p><ul class=""><li id="c964" class="mb mc it la b lb lc le lf lh md ll me lp mf lt mg mh mi mj bi translated">采用修剪过的网络的结构/架构</li><li id="a09d" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">随机重新初始化其参数</li><li id="acf4" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">从头开始训练网络</li></ul><p id="70a0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">子网的性能会很差。这篇论文发现，在随机初始化的密集网络中存在特殊的子网，称为“中奖票”，当独立训练时，这些子网可以与完全训练的密集网络的性能相匹配。这种现象被创造为彩票假说(LTH)，正如本综述的第一段所解释的。</p><p id="0ae7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">方法论。</strong> <em class="lx"> </em>为了验证LTH，作者采用了一种非结构化的剪枝方法。使用的修剪技术非常类似于IMP，只有一个小的不同。也就是说，为了产生中奖彩票，作者进行如下操作:</p><ol class=""><li id="bcfe" class="mb mc it la b lb lc le lf lh md ll me lp mf lt oc mh mi mj bi translated">从一个训练有素的密集模型开始</li><li id="2933" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt oc mh mi mj bi translated">选择并修剪网络中的最低幅度权重</li><li id="da35" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt oc mh mi mj bi translated">将剩余的子网参数倒回它们的初始随机值</li><li id="dfd0" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt oc mh mi mj bi translated">训练子网收敛</li><li id="8630" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt oc mh mi mj bi translated">重复步骤(2)-(4)，直到达到所需的修剪比率</li></ol><p id="bcbb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上述方法与IMP的主要区别在于模型参数的重绕。修剪后，参数被重置为其原始的随机值，而不是简单地微调现有的模型参数。</p><p id="decd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过这种方式，获胜的彩票——它的结构和权重——存在于随机初始化的密集网络中。因此，这个子网被显示已经赢得了“初始化彩票”，因为它可以从零开始被训练到比完全训练的密集网络或具有相同结构的随机重新初始化的子网更高的精度。</p><p id="de80" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">调查结果。</strong></p><ul class=""><li id="9065" class="mb mc it la b lb lc le lf lh md ll me lp mf lt mg mh mi mj bi translated">使用全连接网络和小规模CNN在MNIST和CIFAR10数据集上对LTH进行了经验验证。发现原始网络大小的10–20%的中奖票持续存在。</li><li id="03ed" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">在许多情况下，中奖票比原始稠密网络具有更好的泛化性能。</li><li id="4b74" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">在培训前随机重新初始化中奖彩票会损害性能。将参数恢复到初始随机值对于匹配或超越密集网络性能至关重要。</li></ul><h2 id="5f60" class="nm mq it bd mr nn no dn mv np nq dp mz lh nr ns nb ll nt nu nd lp nv nw nf nx bi translated"><strong class="ak">重新思考网络修剪的价值【5】</strong></h2><p id="87c9" class="pw-post-body-paragraph ky kz it la b lb nh ju ld le ni jx lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated"><strong class="la iu">大意。</strong> <em class="lx"> </em>简而言之，本文表明随机重新初始化并从头训练的子网与通过IMP获得的子网表现相似。这一结果与神经网络修剪文献[1，4]中的先前发现相矛盾，揭示了训练大型、过度参数化的模型对于获得更有效的子网络实际上是不必要的。</p><p id="6633" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">尽管该结果与一般的神经网络修剪更相关(即，不是特别的LTH)，但是本文中的结果表明:</p><ul class=""><li id="c37a" class="mb mc it la b lb lc le lf lh md ll me lp mf lt mg mh mi mj bi translated">将子网参数恢复到初始值并不是LTH方法的必要组成部分——通过在迭代修剪期间随机重新初始化子网参数，可以获得相同的结果。</li><li id="d025" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">LTH是一种无法在使用结构化剪枝和复杂网络架构的大规模实验中复制的现象。</li></ul><p id="f847" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">方法论。</strong> <em class="lx"> </em>与最初的LTH出版物不同，这项工作中的作者在他们的大部分实验中利用了结构化修剪方法。虽然在某些情况下采用非结构化剪枝，但结构化剪枝更常用于大规模实验(例如，卷积网络的结构化滤波器剪枝[4])，使这种方法更合适。</p><p id="a324" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">研究了多种网络修剪方法(参见第4节标题和[5]中的描述)，这些方法执行:</p><ul class=""><li id="bf78" class="mb mc it la b lb lc le lf lh md ll me lp mf lt mg mh mi mj bi translated"><strong class="la iu">预定义结构修剪</strong>:对每一层进行固定、均匀的修剪</li><li id="984d" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated"><strong class="la iu">自动结构修剪</strong>:动态决定每层应该修剪多少，而不是均匀修剪</li><li id="5e6f" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated"><strong class="la iu">非结构化IMP </strong>:参见前面关于IMP的讨论</li></ul><p id="39a8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于这些方法中的每一种，作者都遵循前面描述的三步修剪过程。对于第三步，从各种不同的参数初始化开始训练得到的子网:</p><ul class=""><li id="2ed4" class="mb mc it la b lb lc le lf lh md ll me lp mf lt mg mh mi mj bi translated">将子网参数恢复到它们的初始随机值</li><li id="6af7" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">保留/微调现有子网参数</li><li id="4c69" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">随机重新初始化子网参数</li></ul><p id="6aaf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作者发现，在大多数情况下，修剪后随机重新初始化子网参数匹配或超过其他方法的性能。因此，对于如此大规模的实验，LTH无法得到验证。</p><p id="c25a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">调查结果。</strong></p><ul class=""><li id="3319" class="mb mc it la b lb lc le lf lh md ll me lp mf lt mg mh mi mj bi translated">子网参数的随机重新初始化足以实现有竞争力的子网性能，表明LTH在本研究探索的大规模实验中不成立。</li><li id="79a2" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">LTH和本文之间的发现差异可能是由<em class="lx"> (i) </em>主要使用结构化剪枝进行分析，<em class="lx"> (ii) </em>使用更大的模型和数据集，或者<em class="lx"> (iii) </em>优化和超参数设置的差异造成的。</li><li id="c251" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">从神经网络剪枝中获得的中奖票或高性能子网的参数并不重要，揭示了神经网络剪枝可能只不过是<a class="ae ny" href="https://lilianweng.github.io/posts/2020-08-06-nas/" rel="noopener ugc nofollow" target="_blank">神经架构搜索</a>的一种替代形式。</li></ul><h2 id="2cd9" class="nm mq it bd mr nn no dn mv np nq dp mz lh nr ns nb ll nt nu nd lp nv nw nf nx bi translated"><strong class="ak">线性模式连通性和彩票假说【6】</strong></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/7dd104e88af17c5cb180e8bdec5acea4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*E1-B-3zM0DXbzYRNUnpteg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="ob">当您在两个网络之间进行插值时，误差增加到超过两个网络的平均误差时，就会出现不稳定性(来自【6】)</em></p></figure><p id="cea4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">主旨。</strong>这项工作探索了上面论文中概述的问题— <em class="lx">为什么LTH在大规模实验中不成立？</em>作者发现，使用【1】中提出的方法，在大规模实验中无法发现中奖彩票。也就是说，这种子网不能达到匹配或超过密集网络的精度。</p><p id="06b3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，有趣的是，如果修改这个过程，将权重倒回到某个早期的训练迭代k，而不是严格地倒回到初始化，那么得到的子网表现良好。因此，如果方法稍微修改如下，LTH在大规模实验中仍然成立:</p><ol class=""><li id="0adc" class="mb mc it la b lb lc le lf lh md ll me lp mf lt oc mh mi mj bi translated">从一个训练有素的密集模型开始</li><li id="269c" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt oc mh mi mj bi translated">选择并修剪网络中的最低幅度权重</li><li id="81ec" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt oc mh mi mj bi translated">将子网参数倒回训练迭代k的值</li><li id="4693" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt oc mh mi mj bi translated">训练子网收敛</li><li id="535c" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt oc mh mi mj bi translated">重复步骤(2)-(4)，直到达到所需的修剪比率</li></ol><p id="2345" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为这种子网不是在随机初始化的密集网络中发现的，所以作者称它们为“匹配”子网，而不是赢得入场券。</p><p id="1b04" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">方法论。</strong>为了发现LTH内这种有趣的重量重绕特性，作者采用了一种基于<strong class="la iu">线性模式连通性</strong>的新型不稳定性分析。他们的分析方法执行以下步骤:</p><ul class=""><li id="c3b5" class="mb mc it la b lb lc le lf lh md ll me lp mf lt mg mh mi mj bi translated">创建网络的两个副本。</li><li id="49a2" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">用不同的随机种子独立地训练这些网络，使得网络在用随机梯度下降训练期间经历不同的随机噪声和数据排序。</li><li id="7661" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">确定两个生成的网络是否通过非递增误差区域线性连接。</li></ul><p id="c877" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">实际上，上述方法的最后一步是通过测试两个网络权重之间固定数量的线性插值点来完成的。在这些点中的每一点上，测试插值网络的误差，以确保它不超过实际网络的平均误差值。</p><p id="5467" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过执行如上所述的不稳定性分析，作者发现网络在训练早期变得稳定，但在初始化时不稳定。此外，匹配子网总是稳定的，表明匹配子网不能从随机初始化的不稳定参数中导出。这样的观察解释了为什么LTH仅在权重被重新绕到一些早期训练迭代时成立，而不是一直到初始化。</p><p id="c4c4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">调查结果。</strong></p><ul class=""><li id="665c" class="mb mc it la b lb lc le lf lh md ll me lp mf lt mg mh mi mj bi translated">LTH在比例上不成立。但是，“匹配”子网可以通过修改LTH来发现，以将权重倒回到训练的某个早期迭代，而不是一直到初始化。</li><li id="5b3d" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">只有非常小规模的网络和数据集(如MNIST上的LeNet)在初始化时是稳定的，这解释了为什么最初的LTH公式[1]只能在这样的小规模数据集上得到验证。</li><li id="4e7d" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">给定对LTH的这种修改，即使在ImageNet上使用最先进的卷积神经网络架构的大规模实验中，也可以一致地找到匹配的子网。</li></ul><h1 id="6935" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">外卖食品</h1><p id="de94" class="pw-post-body-paragraph ky kz it la b lb nh ju ld le ni jx lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">通过研究LTH，我们了解了几个有用的概念:</p><ul class=""><li id="76ba" class="mb mc it la b lb lc le lf lh md ll me lp mf lt mg mh mi mj bi translated"><strong class="la iu">神经网络修剪</strong>:通过去除不需要的权重，从一个经过充分训练的密集网络中生成一个更小、更高效的网络</li><li id="fa5b" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated"><strong class="la iu">结构化与非结构化剪枝</strong>:两类剪枝方法，对剪枝过程施加不同的约束</li><li id="7564" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">迭代幅度修剪:一种简单而有效的修剪方法，迭代地丢弃网络中最低幅度的权重</li><li id="9443" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated"><strong class="la iu">线性模式连通性</strong>:用于描述网络对随机梯度下降训练产生的噪声“稳定”的特性。</li></ul><p id="eeba" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在概述的工作中，我们首先了解到——在小规模实验中——密集、随机初始化的神经网络包含子网络(即中奖彩票),这些子网络可以用IMP发现并隔离训练以实现高精度[1]。</p><p id="5b31" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这一发现后来与研究相矛盾，研究表明，在大规模实验中，这种中奖彩票可以随机重新初始化，仍然可以达到相当的准确性，这表明除了IMP [5]发现的网络结构之外，这些中奖彩票没有什么特殊之处。</p><p id="b5bf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当表明虽然在随机初始化的密集网络中可能不存在中奖票，但是可以通过IMP在已经少量训练的网络中找到匹配子网(即，匹配或超过密集网络性能的子网)时，这种矛盾的结果得到解决[6]。因此，只要稍加修改，LTH在大尺度上仍然有效。</p><h2 id="1d42" class="nm mq it bd mr nn no dn mv np nq dp mz lh nr ns nb ll nt nu nd lp nv nw nf nx bi translated">结论</h2><p id="1237" class="pw-post-body-paragraph ky kz it la b lb nh ju ld le ni jx lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">非常感谢你阅读这篇文章。如果你喜欢它，请订阅我的<a class="ae ny" href="https://cameronrwolfe.me/signup" rel="noopener ugc nofollow" target="_blank">深度(学习)焦点时事通讯</a>，在那里，我每两周对深度学习研究中的一个相关主题进行情境化、解释和检查。欢迎在<a class="ae ny" href="https://medium.com/@wolfecameron" rel="noopener">媒体</a>上关注我或者访问我的<a class="ae ny" href="https://cameronrwolfe.me/" rel="noopener ugc nofollow" target="_blank">网站</a>，那里有我的社交媒体和其他内容的链接。如果您有任何建议/反馈，请直接联系我或留下评论。</p><h2 id="8602" class="nm mq it bd mr nn no dn mv np nq dp mz lh nr ns nb ll nt nu nd lp nv nw nf nx bi translated">文献学</h2><p id="f190" class="pw-post-body-paragraph ky kz it la b lb nh ju ld le ni jx lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">[1]弗兰克，乔纳森和迈克尔卡宾。"彩票假说:寻找稀疏的、可训练的神经网络."<em class="lx"> arXiv预印本arXiv:1803.03635 </em> (2018)。</p><p id="22bd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2]尤，浩然等，“画早鸟票:走向更高效的深度网络训练”<em class="lx"> arXiv预印本arXiv:1909.11957 </em> (2019)。</p><p id="db2c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[3] LeCun、Yann、John Denker和Sara Solla。"最佳脑损伤"<em class="lx">神经信息处理系统进展</em> 2 (1989)。</p><p id="d658" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[4]李，郝等.“高效神经网络的剪枝滤波器”<em class="lx"> arXiv预印本arXiv:1608.08710 </em> (2016)。</p><p id="1e07" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[5]刘，庄等.“网络剪枝的价值再思考”<em class="lx"> arXiv预印本arXiv:1810.05270 </em> (2018)。</p><p id="9d00" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[6] Frankle，Jonathan等人，“线性模式连接和彩票假说”<em class="lx">机器学习国际会议</em>。PMLR，2020年。</p><p id="dacf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[7]尤，浩然等，“画早鸟票:走向更高效的深度网络训练”<em class="lx"> arXiv预印本arXiv:1909.11957 </em> (2019)。</p><p id="04b3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[8]刘，庄等.“通过网络瘦身学习高效卷积网络”IEEE计算机视觉国际会议论文集。2017.</p></div></div>    
</body>
</html>