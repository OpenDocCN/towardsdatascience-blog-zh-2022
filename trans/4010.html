<html>
<head>
<title>Synthetic Tabular Data Generation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综合表格数据生成</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/synthetic-tabular-data-generation-34eb94a992ed#2022-09-07">https://towardsdatascience.com/synthetic-tabular-data-generation-34eb94a992ed#2022-09-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="791b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">介绍nbsynthetic:一个简单但功能强大的用于小数据集的表格数据生成开源库。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2b00c219f6def6ef4c69f2c89af32ae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*asPQZDa7xvprIQmRfVQKzA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。版权所有NextBrain.ml</p></figure><p id="358e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这篇文章中，我们介绍了由NextBrain.ai创建的一个简单而健壮的无监督合成表格数据生成python库的开源项目<a class="ae lr" href="https://github.com/NextBrain-ml/nbsynthetic" rel="noopener ugc nofollow" target="_blank"><strong class="kx ir"/></a><em class="ls">，</em>。</p><ol class=""><li id="5902" class="lt lu iq kx b ky kz lb lc le lv li lw lm lx lq ly lz ma mb bi translated">简单:采用基于<a class="ae lr" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>的简单稳定的无监督GAN(生成对抗网络)架构设计。</li><li id="1a9e" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">鲁棒:具有特定的超参数调整，以确保训练的稳定性，同时最大限度地降低计算成本。</li></ol><p id="d47c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">优势:</p><ul class=""><li id="1e00" class="lt lu iq kx b ky kz lb lc le lv li lw lm lx lq mh lz ma mb bi translated">因为它是基于一个无人监管的架构，用户不需要有一个预定义的目标。</li><li id="850c" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq mh lz ma mb bi translated">它主要用于具有连续和分类特征的小型数据集。</li><li id="3c53" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq mh lz ma mb bi translated">由于它们的简单性，模型可以在CPU上运行。</li><li id="5811" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq mh lz ma mb bi translated">包括用于快速输入数据准备和特征工程的模块。</li><li id="a77e" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq mh lz ma mb bi translated">包括了用于运行统计测试和比较真实和合成数据(我们不喜欢“假数据”这个术语)的模块。它还包括一个特殊的统计测试(最大均值差异-MMD)，测量映射到再生核希尔伯特空间(RKHS)的两个样本的均值之间的距离。</li><li id="21d8" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq mh lz ma mb bi translated">绘图工具包括比较原始和合成数据的概率分布。</li></ul><p id="2582" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae lr" href="https://github.com/NextBrain-ml/nbsynthetic" rel="noopener ugc nofollow" target="_blank">在这里</a>你可以找到<strong class="kx ir"> nbsynthetic项目</strong>(库、文档和例子)。</p><h1 id="658c" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">介绍</h1><h1 id="b641" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">为什么是表格综合数据生成库？</h1><p id="5fae" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi nf translated">随着图像、视频和语音生成的应用，合成数据正经历着它的辉煌时代。最近，人们对应用程序的生成模型越来越感兴趣，例如创建新类型的艺术或模拟视频序列。然而，表格数据的发展似乎没有那么雄心勃勃，尽管它是世界上最常见的数据类型。合成表格数据正在颠覆自动驾驶汽车、医疗保健和金融服务等行业。医疗保健行业接受了这一新想法，特别是在解决患者隐私问题方面，但也在研究项目中模拟合成基因组数据集或患者病历。</p><blockquote class="no"><p id="004d" class="np nq iq bd nr ns nt nu nv nw nx lq dk translated">全球每天有近7亿人使用电子表格来处理以表格数据形式呈现的小样本数据。</p></blockquote><p id="741f" class="pw-post-body-paragraph kv kw iq kx b ky ny jr la lb nz ju ld le oa lg lh li ob lk ll lm oc lo lp lq ij bi translated">全球每天有近7亿人使用电子表格来处理以表格数据形式呈现的小样本数据。这些信息通常用于制定决策和获得洞察力。然而，由于记录不完整或数据量小(缺乏统计意义)，它通常被认为是“质量差”的数据。机器学习在这些应用中非常有价值。但是，正如任何数据科学家都知道的那样，ML的当前技术状态集中在大型数据集上，排除了大量潜在的ML用户。此外，我们必须解决现代统计学的要求，当应用于小样本数据时，警告ML算法的低可靠性。</p><blockquote class="no"><p id="1a8f" class="np nq iq bd nr ns nt nu nv nw nx lq dk translated">合成表格数据正在颠覆自动驾驶汽车、医疗保健和金融服务等行业。</p></blockquote><p id="5a1b" class="pw-post-body-paragraph kv kw iq kx b ky ny jr la lb nz ju ld le oa lg lh li ob lk ll lm oc lo lp lq ij bi translated">例如，我们正在帮助一家大型精神病医院进行数据分析项目。他们带着基于过去十年收集的数据的综合研究来找我们。精神病住院治疗是至关重要的，这项研究的目标是改善早期预警和预防方案。我们以一个38列300行的电子表格的形式得到了结果。有许多空值(只有7行包含所有38个特征值)。当然，对于任何数据科学家来说，这都是很少的数据，对于统计学家来说就更少了。然而，对他们来说，收集这些数据是一项具有挑战性的工作。有了这些数据，任何统计方法的有效性都会受到质疑。通过创建合成数据集，我们能够提供具有统计有效性的可靠信息，并解决隐私问题，这是患者记录管理的一个关键点。</p><h1 id="1ab6" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">无监督生成对抗网络</h1><p id="01e4" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">生成对抗网络，或GANs，是这些生成应用的核心技术。gan是由Ian Goodfellow在2014年提出的[1]。这个想法是设计两个独立的神经网络，让它们相互对抗。第一个神经网络从生成与输入数据统计相似的新数据开始。第二个神经网络的任务是识别哪些数据是人工创建的，哪些不是。两个网络不断相互竞争:第一个试图欺骗第二个，第二个试图弄清楚第一个在做什么。当第二网络不能“辨别”数据是来自第一网络输出还是来自原始数据时，游戏结束。我们称第一个网络<strong class="kx ir">为发生器</strong>，第二个网络<strong class="kx ir">为鉴别器</strong>。</p><blockquote class="no"><p id="8c96" class="np nq iq bd nr ns nt nu nv nw nx lq dk translated">T <!-- -->根据定义，同时训练发电机和鉴别器模型通常是不稳定的。</p></blockquote><p id="8b67" class="pw-post-body-paragraph kv kw iq kx b ky ny jr la lb nz ju ld le oa lg lh li ob lk ll lm oc lo lp lq ij bi translated">同时训练生成器和鉴别器模型根据定义一般是不稳定的[2]，所以GANs的主要缺点是训练不稳定和模式崩溃。GANs的发展为解决这一问题带来了有趣的想法，例如向鉴别器引入额外的信息，以便获得更好的准确性，并为框架提供更多的稳定性(条件GANs或cGANs)。这种变体方法需要一个“目标”或参考类，以附加信息调节GAN输出。但是，当我们处理上述目标用户时，我们发现许多数据集没有单一的目标要素，因为用户希望对不同的要素进行预测，以便获得关于其数据的更多见解。</p><blockquote class="no"><p id="7ecf" class="np nq iq bd nr ns nt nu nv nw nx lq dk translated">例如，如果用户想要使用另一个特征作为目标来解决ML问题，将合成数据调整为单个特征也会在生成的数据中引入偏差。这就是为什么无条件GAN或无监督GAN(也称为香草GAN)对此类问题感兴趣，因为它不需要选择目标。</p></blockquote><p id="7383" class="pw-post-body-paragraph kv kw iq kx b ky ny jr la lb nz ju ld le oa lg lh li ob lk ll lm oc lo lp lq ij bi translated">虽然我们可以通过为GAN提供可靠的目标类(cGAN中的“额外”条件)来提高准确性，但对于这些拥有中小型数据集、数据贫乏且需要获得可操作的一般见解的活跃电子表格用户来说，无监督GAN是一种多功能工具。但是它也有一些局限性。</p><h1 id="d5b7" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated"><strong class="ak">构建简单、健壮的无监督GAN的技巧和诀窍</strong></h1><p id="5ce6" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">GANs在训练过程中不稳定的原因是当发生器(<em class="ls"> G </em>和鉴别器(<em class="ls"> D </em>)同时训练时，在一个非合作博弈中一个模型的改进是以另一个模型为代价的[3]。<strong class="kx ir"> nbsynthetic </strong>使用Keras开源软件库<strong class="kx ir">。</strong>其架构基于线性拓扑，使用基本的顺序架构，在发生器和鉴别器中都有三个隐藏层。我们的GAN模型具有顺序架构，其中<em class="ls"> G </em>和<em class="ls"> D </em>相连。</p><p id="1356" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该模型的设计考虑了以下因素:</p><h2 id="5976" class="od mj iq bd mk oe of dn mo og oh dp ms le oi oj mu li ok ol mw lm om on my oo bi translated"><strong class="ak"> 1。初始化。</strong></h2><p id="66a2" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">初始化是为神经网络模型的优化(学习或训练)定义起始点的过程。当这个过程不是最优时，训练过程可能由于其不稳定性而失败。我们将通过随机初始化权重来打破GAN的对称性。这个想法是为了避免一层中的所有神经元学习相同的信息。然后，我们将使用批量标准化[5],它通过将每个单元的输入标准化到零均值和单元方差来稳定学习。</p><h2 id="37ce" class="od mj iq bd mk oe of dn mo og oh dp ms le oi oj mu li ok ol mw lm om on my oo bi translated"><strong class="ak"> 2。收敛</strong></h2><p id="1a1a" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">正如我们所见，作为甘斯博弈的结果，两个模型(<em class="ls"> G </em>和<em class="ls"> D </em>)都无法收敛[4]。在图像生成中，避免这些收敛失败的主要策略是使用卷积网[5]。卷积层将输入要素映射到更高级别的表示，同时通过在多个下采样步骤中丢弃不相关的信息来保持其分辨率[6]。但是这种策略已经显示了图像和语音生成的相关进展，对于小样本表格数据来说似乎并不理想，因为我们可能会在每一步中丢失信息。实际上，当处理小样本表格数据集时，我们意识到许多为提高图像识别中GANs的准确性而进行的改进是不利的。我们选择了简单而密集的架构作为<em class="ls"> nbsynthetic </em>的最佳方案。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/ca9ed1ec6045569f517a351eefbcb542.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*61RRzoGLRG6D6dmIczLmkA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在GAN中收敛。作者图片</p></figure><h2 id="b190" class="od mj iq bd mk oe of dn mo og oh dp ms le oi oj mu li ok ol mw lm om on my oo bi translated"><strong class="ak"> 3。激活功能</strong></h2><p id="2336" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">LeakyReLU(一种基于ReLU的激活函数，但其负值斜率较小，而不是平坦斜率)在GAN架构中很常见。然而，两者都有不同的使命。<em class="ls"> G </em>必须生成尽可能接近原始数据的数据表示，而<em class="ls"> D </em>必须决定(分类)输出是否不同于输入数据。为了建立发生器和鉴别器<em class="ls">时序模型，我们使用<em class="ls"> LeakyReLU </em>激活函数<em class="ls">。</em>对于模型编译，我们将使用一个<em class="ls"> tanh </em>激活函数(范围为-1到1)用于<em class="ls"> G、</em>和<em class="ls"> D </em>我们将使用一个范围为0到1的<em class="ls"> sigmoid </em>函数，因为它必须简单地“决定”数据是否有效。</em></p><p id="139b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="ls"> tanh </em>(和<em class="ls"> sigmoid </em>而不是<em class="ls"> LeaskyReLU </em>)是具有连续逆的连续函数。因此<em class="ls"> tanh </em>层(生成器)保留了输出层中的拓扑属性，但导致比<em class="ls"> sigmoid </em>函数高得多的梯度。这个想法是帮助网络更快地达到全局最小值，以避免当我们有连续和分类特征的混合时混淆它。</p><p id="ebd6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们不能使用<em class="ls">线性</em>激活函数，因为分类特征会混淆GAN。鉴别器的任务是将输出分类到与输入数据相同的类别中。使用<em class="ls"> sigmoid </em>单元(鉴别器)进行分类相当于试图在最终的图层表示中找到一个分隔类的超平面。在这种情况下，使用具有较低梯度的sigmoid函数将提高分类精度。</p><h2 id="0247" class="od mj iq bd mk oe of dn mo og oh dp ms le oi oj mu li ok ol mw lm om on my oo bi translated"><strong class="ak"> 4。优化</strong></h2><p id="b683" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated"><em class="ls"> G </em>和<em class="ls"> D </em>都使用自适应矩估计优化器Adam通过随机梯度下降进行训练，以计算每个参数的自适应学习率【7】。我们使用一个小的学习率(<em class="ls"> lr = 0.0002 </em>)和一个低于默认值0.9 ( <em class="ls"> β1 = 0.4 </em>)的简化动量项(或梯度均值)，目的是减少不稳定性。</p><h2 id="14b4" class="od mj iq bd mk oe of dn mo og oh dp ms le oi oj mu li ok ol mw lm om on my oo bi translated"><strong class="ak"> 5。噪声注入</strong></h2><p id="9e5f" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated"><em class="ls"> G </em>使用固定长度的随机向量<em class="ls"> ξ </em>作为输入创建样本。这个向量就是潜在空间。当多维向量空间中的点与输入数据匹配时，在训练后将生成数据分布的压缩表示。<em class="ls"> ξ </em>通常从高斯分布中采样，应该能够提高GANs【8】的数值稳定性。这个过程也称为噪声注入。用噪声训练时的误差函数类似于正则化函数[9]，其中正则化函数的系数<em class="ls"> λ </em>由噪声方差控制。</p><p id="f3b9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">均匀分布实际上是具有最大标准差的正态分布。因此，使用均匀分布，我们增加了方差值，从而增加了系数<em class="ls"> λ的值。使用简单的网络结构和小样本表格输入数据，我们可以通过增加系数<em class="ls"> λ </em>来减少训练过程中的过拟合。我们的实验支持了这个假设。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/a2afa2d214736ad01fa69ba608af9a60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZnyZl5-PgdRHoq3CohsN4A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">噪声注入的GAN模型。作者图片</p></figure><h2 id="411b" class="od mj iq bd mk oe of dn mo og oh dp ms le oi oj mu li ok ol mw lm om on my oo bi translated">6。输入数据准备</h2><p id="c41b" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">输入数据准备可能是无监督GAN最重要的元素。该网络预计接收中低样本数据(最多100个要素，少于1000个实例)。此外，数据可以同时包含连续列和分类列。连续列不一定遵循正态分布，并且可能包含异常值。分类列可以是布尔值或多类。</p><p id="5d05" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">数据准备中最重要的决策点是识别两种数据类型，以便区别对待它们。我们只需要将分类列从-1扩展到1。(因为我们使用的是tanh激活函数)。然而，为了对输入和异常值的不同概率分布具有鲁棒性，我们必须转换连续数据。我们将使用分位数变换将所有不同类型的输入概率分布映射为均匀分布[10]。因此，由于噪声注入(潜在空间)也是均匀分布，生成器<em class="ls"> G </em>将只处理具有连续均匀分布的数据作为输入。</p><h1 id="2181" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">图书馆中的实用程序</h1><p id="ebfb" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">生成合成数据的一个重要挑战是确保新数据非常“接近”原始数据。有几个统计测试来确定两个样本是否属于同一概率分布。学生的<em class="ls">t-检验、Wilcoxon符号秩检验</em>(配对学生t-检验的非参数变量)以及针对数字特征的<em class="ls"> Kolmogorov-Smirnov检验</em>都包含在该库中【11】。这些检验以一对一的方式比较输入数据集中每个要素和合成数据的概率分布(称为“双样本检验”，也称为“同质性问题”)。</p><blockquote class="no"><p id="733e" class="np nq iq bd nr ns nt nu nv nw nx lq dk translated">现代统计测试非常强大，但必须做出某些假设。</p></blockquote><p id="17ef" class="pw-post-body-paragraph kv kw iq kx b ky ny jr la lb nz ju ld le oa lg lh li ob lk ll lm oc lo lp lq ij bi translated">当一些特征可能与假设相冲突而其他特征不冲突时，这些假设使得将这些测试应用于公共数据集变得困难。该数据还包含具有不同概率分布的不同类型特征的组合，因此单个测试不能同时对所有这些特征有效。例如，一些测试依赖于正态假设(数据遵循正态分布)，然而我们可以拥有具有几乎任何分布特征的数据。例如，Student的t检验是一种比较手段的检验，而Wilcoxon的检验是数据的排序。例如，如果您正在分析有许多异常值的数据，Wilcoxon的测试可能更合适。学生的t检验依赖于正态假设；即样本呈正态分布。</p><p id="d745" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">另一方面，Wilcoxon检验只对连续值有效。Kolmogorov-Smirnov拟合优度检验[11]用于确定样本是否来自特定分布的人群。它仅适用于像Wilcoxon检验这样的连续分布，对于超前于正态的分布可能更稳健。</p><p id="6946" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了解决这个问题，我们通过使用<em class="ls">最大均值差异检验(MMD)</em>【12】提出了一个完全不同的解决方案。MMD是一种统计检验，检查两个样本是否属于不同的分布。该测试计算两个样本之间的均值差异，这两个样本被映射到再生核希尔伯特空间(RKHS)[13]。最大均值差异已广泛应用于机器学习和非参数检验。</p><p id="c39b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">基于从它们中提取的样本，MMD通过找到一个平滑函数来评估两个分布<em class="ls"> p </em>和<em class="ls"> q </em>是否不同，该平滑函数在从<em class="ls"> p </em>提取的点上较大，在从<em class="ls"> q </em>提取的点上较小。统计测量是两个样本的平均函数值之间的差；当这种差异很大时，样本很可能来自不同的分布。我们选择这种测试的另一个原因是，它在处理小样本数据时表现更好(这是大多数统计测试中非常常见的假设)。</p><h1 id="493a" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">检查数据拓扑</h1><p id="47dc" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">拓扑数据分析，或TDA [14]，是一种从不同的角度处理数据的新方法。在比较原始数据和合成数据时，应用这种尖端方法有多种优势:</p><ul class=""><li id="5aa1" class="lt lu iq kx b ky kz lb lc le lv li lw lm lx lq mh lz ma mb bi translated">定量分析忽略了隐藏在数据中的本质信息。此外，在许多数据表示中，不清楚实际数据距离有多少值，因此测量值并不总是合理的。</li><li id="b598" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq mh lz ma mb bi translated">TDA关心的是<strong class="kx ir">距离</strong>和<strong class="kx ir">聚类</strong>，以便在t <strong class="kx ir">拓扑空间</strong>中表示数据。为了构建一个拓扑空间，我们必须将数据点转换成<strong class="kx ir">单纯复形</strong>。这些是将空间表示为点、区间、三角形和通过连接点(也称为过滤)形成的其他更高维类似物的联合。通过增加点周围的半径来连接空间中的点的效果导致了被称为<strong class="kx ir">单形</strong>的几何对象的创建(这就是它们被称为单形复形的原因)。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/e933d1dc3b805a80fcc665c15c58c09e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MpdR-WvpdiKzuuGSe6cN7g.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">TDA步骤:从点云到持久性图。作者图片</p></figure><p id="8f63" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">持续同源性的步骤如上图所示。首先，我们定义一个<strong class="kx ir">点云</strong>。然后，我们使用过滤方法来创建简单的复合体，最后，我们识别数据的<strong class="kx ir">拓扑签名</strong>(这就是我们如何称之为链接和循环)并在<strong class="kx ir">持久性图</strong>中表示它们。这些图表提供了一种有用的方法来总结数据点云或函数的拓扑结构。数据拓扑空间对于输入数据集是同伦等价的。</p><p id="6474" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下图显示了输入数据集的持久性图与使用nbsynthetic从第一个数据集生成的合成持久性图之间的比较。正如我们所看到的，两个图是非常相似的签名。我们可以看到链路(红色点——H0)具有相似的分布，这意味着两者具有非常相似的拓扑特征。在一个合成数据集(长度是原始数据集的十倍)中，似乎有一个环(绿色点——H1)，甚至有一个空洞(H2)，但似乎也有噪声。</p><p id="473a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们还可以应用定量分析测试来检查两个图是否等价。我们可以使用Mann Whitney U检验[16]，它用于检验两个样本是否可能来自同一个总体。在图中使用的数据中，链路的p值为1，环路的p值为1(我们可以拒绝零假设)，这意味着两个图是相等的。也就是说，生成的合成数据具有与原始输入数据相同的拓扑。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/6638340d9064027f2aad0c06c20029cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GnKlW_BesQ0s1LtHPQjqGQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">输入数据集和使用<strong class="bd ot"> nbsynthetic </strong>生成的合成数据集的持久性图的比较。图片作者。</p></figure><h1 id="e54d" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">限制</h1><p id="bc50" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">下图展示了从三维树点云中创建的<em class="ls"> nbsynthetic </em>数据。我们可以看到，“合成树”与“真树”几乎没有什么共同之处。如果我们如前所述运行<em class="ls"> MMD测试</em>，MMD值是0.12。我们通常接受小于0.05的MMD值作为原始数据和合成数据之间“接近程度”的量度(我们实验中的普通值在0.001和0.02之间)。选择这个例子是因为它清楚地展示了我们的综合局限性:处理低维输入数据(3)和只包含连续列的数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/b509fda56671cbbcfd6c8c7f29e45e95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WmZZgwlpwhC-42JvUn60ow.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">树木点云的合成数据生成。图片作者。</p></figure><p id="be77" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在正则化过程中，低维输入会混淆GAN，导致不可理解的输出。合成数据在所有轴上都在崩溃，这意味着GAN的鉴别器无法区分真实数据和生成的数据。结果，当只有连续的特征馈入时，GAN不能产生输入数据的合适表示。但是，当我们向输入数据添加具有分类数据类型的额外要素时，MMD值会自动下降，这意味着输入数据会得到更准确的表示。我们必须记住，我们的生成器的输入潜变量(或噪声注入)具有均匀分布。如果我们切换到正态分布，精确度也会提高(尽管精确度也不是很高)。似乎范畴特征作为“引用”或“条件”输入操作，就像外部类在条件gan或cGANs中所做的那样。这一限制有助于我们更好地理解GAN的工作原理。</p><p id="189e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们必须记住<em class="ls"> nbsynthetic </em>并不能修复输入数据的缺陷，比如不平衡数据或严重偏斜的数据分布。合成数据不一定要了解输入数据特征的精确分布，但会很接近。我们对GAN网络的要求是理解这些特征如何相互连接；也就是理解模式。然后，可能有必要对合成数据执行额外的变换，以解决异方差等问题或减少不平衡目标数据产生的偏差。</p><p id="6ff3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该项目的下一步是包括一个转换输入数据的模块，以避免这些限制。</p><h1 id="4f99" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">结论</h1><p id="9ccc" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">我们引入了一个用于合成表格数据生成的库，用于小型(和中型)样本数据集。为了降低复杂性和计算成本，我们使用了具有简单线性拓扑的无监督GAN。为了使其可靠，我们敏锐地分析了超参数调整，以生成尽可能“接近”原始数据的合成数据。我们还探索了用统计工具量化这种密切关系的最佳方法。在nbsynthetic git hub repository教程中，您会发现库中尚未提供的其他方法，如迁移学习或拓扑数据分析(也在这里介绍)。</p><p id="9f08" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们希望在两个方向上继续改进图书馆:</p><ol class=""><li id="73d4" class="lt lu iq kx b ky kz lb lc le lv li lw lm lx lq ly lz ma mb bi translated">针对上述输入数据中仅提供连续特征的情况，探索替代GAN架构。</li><li id="3177" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">我们正在探索更多的方法来量化原始数据和生成数据的差异。拓扑数据分析是最有前途的方法。</li></ol></div><div class="ab cl ov ow hu ox" role="separator"><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa"/></div><div class="ij ik il im in"><blockquote class="no"><p id="686e" class="np nq iq bd nr ns nt nu nv nw nx lq dk translated">你可以在这里  <strong class="ak">找到更多关于这个项目<a class="ae lr" href="https://github.com/NextBrain-ml/nbsynthetic" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">的信息。</strong></a></strong></p></blockquote></div><div class="ab cl ov ow hu ox" role="separator"><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa"/></div><div class="ij ik il im in"><h1 id="21e9" class="mi mj iq bd mk ml pc mn mo mp pd mr ms jw pe jx mu jz pf ka mw kc pg kd my mz bi translated">参考</h1><ol class=""><li id="db14" class="lt lu iq kx b ky na lb nb le ph li pi lm pj lq ly lz ma mb bi translated">Goodfellow，I .、Pouget-Abadie，j .、Mirza，m .、Xu，b .、Warde-Farley，d .、Ozair，s .、和Bengio，y .等人(2014年)。生成对抗网络。神经信息处理系统进展，27。</li><li id="9948" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">Arjovsky，m . &amp; Bottou，L. (2017年)。生成性对抗网络训练的原则性方法。arXiv预印本arXiv:1701.04862</li><li id="6148" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">纳什J. (1951年)。非合作博弈。数学年鉴，286–295页。</li><li id="e601" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">古德费勒岛(2016年)。Nips 2016教程:生成性对抗网络。<em class="ls"> arXiv预印本arXiv:1701.00160 </em>。</li><li id="24cd" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">拉德福德大学、梅斯大学和钦塔拉大学(2015年)。深度卷积生成对抗网络的无监督表示学习。<em class="ls"> arXiv预印本arXiv:1511.06434 </em>。</li><li id="1030" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">Riad，r .，Teboul，o .，Grangier，d .，&amp; Zeghidour，N. (2022)。卷积神经网络的学习进展。<em class="ls"> arXiv预印本arXiv:2202.01653 </em>。</li><li id="f2bc" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">Ruder，S. (2016年)。梯度下降优化算法综述。<em class="ls"> arXiv预印本arXiv:1609.04747 </em>。</li><li id="3210" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">冯，赵，丁，查，张志军(2021年7月)。了解gans中的噪声注入。在<em class="ls">机器学习国际会议</em>(第3284–3293页)。PMLR。</li><li id="c56d" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">主教，C. M. (1995年)。用噪声训练相当于Tikhonov正则化。神经计算，7(1):108–116。</li><li id="83e6" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">Sci，Pedregosa <em class="ls">等</em>sci kit-learn:Python中的机器学习(2011)。JMLR第12卷，第2825-2830页。</li><li id="5330" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">查克拉瓦蒂，拉哈和罗伊，(1967)。<em class="ls">应用统计方法手册，第一卷</em>，约翰威利父子。</li><li id="eabc" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">Ilya Tolstikhin、Bharath K. Sriperumbudur和Bernhard schlkopf(2016年)。径向核下最大平均偏差的极小极大估计。《第30届国际神经信息处理系统会议录》(NIPS'16)。1938-1946年，美国纽约州红钩镇柯伦联合公司。</li><li id="de42" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">A.Gretton、K. M. Borgwardt、M. Rasch、B. Schö lkopf和A. Smola。(2007).两样本问题的核方法。在b . schlkopf，J. Platt和T. Hoffman编辑的《神经信息处理系统进展》19，第513-520页，剑桥，马萨诸塞州。麻省理工出版社。</li><li id="4565" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">卡尔松，G. (2009年)。拓扑和数据。<em class="ls">《美国数学会公报》</em>，<em class="ls"> 46 </em> (2)，255–308。</li><li id="5332" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">E.瑞尔，<em class="ls">语境中的范畴理论</em>，Courier Dover Publications，2017。</li><li id="c076" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">Blumberg，A. J .，Gal，I .，Mandell，M. A .，&amp; Pancia，M. (2014)。度量空间上持久同调的稳健统计量、假设检验和置信区间。<em class="ls">计算数学基础</em>，<em class="ls"> 14 </em> (4)，745–789。<a class="ae lr" href="https://doi.org/10.1007/s10208-014-9201-4" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1007/s10208-014-9201-4</a></li></ol><p id="da5b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="ls"> nbsytnethic是由</em><a class="ae lr" href="http://nextbrain.ml" rel="noopener ugc nofollow" target="_blank"><em class="ls">next brain . a</em></a><em class="ls">I研究团队</em>开发的开源项目</p></div></div>    
</body>
</html>