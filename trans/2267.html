<html>
<head>
<title>How to ensemble Clustering Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何集成聚类算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-ensemble-clustering-algorithms-bf78d7602265#2022-05-19">https://towardsdatascience.com/how-to-ensemble-clustering-algorithms-bf78d7602265#2022-05-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8dc3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">意思是你从未见过的</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e468e7d55d8da3e3e1d597ab9f4e8963.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6BrlJLVBY1jN1R65"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@miinyuii?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">杜凡</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="556e" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="0b8a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在人工智能背景下，集成是一种试图通过聚合多个机器学习模型的预测来提高性能的技术。它可以被认为是一种元学习，其中外部代理通过判断其他代理的单独预测来计算最终预测。</p><p id="f760" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">有几种方法可以集成监督算法，如bagging和boosting。特别是，boosting技术(尤其是XGBoost)成为任何机器学习应用程序中不可或缺的工具。</p><p id="e5ee" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">另一方面，在无监督的问题中，我们没有听到很多关于集成技术。那么，有可能在聚类算法中使用这种技术吗？这不仅是可能的，而且这些年来已经发展了许多策略[1]。这篇文章探讨了其中的两个。</p><h1 id="0a37" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">相似矩阵</h1><p id="0d3d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">与监督方法不同，在监督方法中，集成可以通过将学习者堆叠在其他学习者之上来直接完成，聚类不是那么简单。</p><p id="64a3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在聚类中，数据的最终标签本身没有意义，一个点是在聚类1中还是在聚类2中并不重要，重要的是谁也在这个聚类中。因此，连接两个不同聚类结果的信息并不像在分类中比较最终标签是否相同那样简单。需要某种方法来计算不同分区上存在多少一致性。</p><p id="409d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">相似性矩阵是将这些信息转换成数据结构的简单方法。它的构造如下:</p><p id="7136" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">考虑到我们的数据集上有<em class="mp"> n </em>个点，相似性矩阵将是一个<em class="mp"> n </em> x <em class="mp"> n </em>矩阵，其中位置(<em class="mp"> i，j) </em>包含点I和j在同一个聚类中落了多少次。下图显示了该矩阵的一个示例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/13201078d44f0a8cb672ca086727fe37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9FYX8ZjaPwIuwWSxba--ZA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">相似性矩阵示例。图片由作者提供。</p></figure><p id="12a1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">表示这些信息的更好方法是使用百分比而不是固体计数。在这种归一化形式中，每个位置(<em class="mp"> i，j </em>)表示I和j一起落在同一聚类中的概率。这种标准化是通过将每一行除以其对角线值(总计数)来获得的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/0a4bfeebbdbbb7165d7b61478db5203d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TKTstvx07yzcoy10Ehs5iw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">标准化相似性矩阵示例。图片由作者提供。</p></figure><p id="cd4c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这种方法更适合机器学习，因为所有值都落在范围[0，1]内。标准化的相似性矩阵将是我们接下来部分的基本构建模块。</p><h1 id="3885" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">具有图连通分量的系综</h1><p id="10d9" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">“图形连接组件”看起来像一个七眼怪物，但却是一个非常简单的技术。这种方法认为出现在一起超过X%的时间(例如10%或50%)的两个点(在同一聚类中)应该在同一最终聚类中。</p><p id="83d6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果我们将相似性矩阵视为一个图，其中每个点都是一个节点，并且来自(I，j)的边的权重等于矩阵中(I，j)的值，那么这种方法实质上是移除值低于阈值(X%)的边，断开节点。保持连接的节点组是我们的最终集群。这就是这种方法被称为“连接组件”的原因。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/801c333909303862e33501cabf185532.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iPDooHJ5KESti3sNn6nxcw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">从图表中删除边。图片作者。</p></figure><p id="3d58" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因为它基于简单的阈值创建新的聚类，所以可以认为它等同于用于监督模型的简单bagging集成方法。</p><p id="1526" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们看一个Python中的例子。</p><p id="3453" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">第一步是创建聚类数据集。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ms mt l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/6793ef8b9941f14c8c11c7360a1afa2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*UKVwtw7PON9dBNuOHH8dJg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">聚类数据集。图片由作者提供。</p></figure><p id="5f83" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">下面的代码使用K-Means应用了这种集成聚类技术。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ms mt l"/></div></figure><blockquote class="mv mw mx"><p id="4cbb" class="lo lp mp lq b lr mk jr lt lu ml ju lw my mm lz ma mz mn md me na mo mh mi mj ij bi translated">完整代码可在<a class="ae kv" href="https://github.com/jaumpedro214/posts/blob/main/ensamble_clustering" rel="noopener ugc nofollow" target="_blank"> github </a>上获得，为了提高可读性，省略了实现细节。</p></blockquote><p id="3df7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们来解释一下代码。</p><p id="3824" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">NUM_KMEANS=32是集合中使用的k均值模型的总数。</p><p id="a028" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">该代码使用MiniBatchKMeans来减少训练时间。设置n_init=1是必不可少的，因为在默认情况下，sklearn的K-means模型会执行多次，并且只保留最好的结果。因为我们希望存储所有结果，所以值n_init=1确保了这一点。减少max_iter和batch_size也可以提高代码速度。</p><p id="d91d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">ClusterSimilarityMatrix只是一个创建相似性矩阵的简单模块。它接收聚类分区并迭代更新计数。相似矩阵建立后，进行归一化处理。</p><p id="8813" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">下面一行代码是模型的核心。</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="5cac" class="ng kx iq nc b gy nh ni l nj nk">graph = (norm_sim_matrix&gt;MIN_PROBABILITY).astype(int)</span></pre><p id="c1d4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在这里，低于MIN_PROBABILITY=0.6的概率变成0(从图中移除边)，而高于MIN _ PROBABILITY = 0.6的概率变成1(保持图中的边)。</p><p id="e395" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">MIN_PROBABILITY的值作为一个<em class="mp">超参数</em>，应该被细化以达到预期的结果。如果它的值设置得太高(0.9–1.0)，数据可能会分布在许多小簇中，如果它设置得太低(0.2–0.4)，它可能会找到大簇。</p><p id="85b2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在那之后，只是找到图的连通部分的问题。幸运的是，scipy包已经给了我们代码。</p><p id="0b16" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">所以，让我们看看结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/ff85549ff46458866496476a00c93811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dp133qIPmPm__PeCXPxQWw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">k均值图结果。图片作者。</p></figure><p id="ade5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">正如我们所看到的，集成技术的结果与数据应该如何划分的预期更加一致。</p><p id="b232" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这个技术挺有意思的。众所周知，K-means只能找到球状星团，使用预定义的k值将空间划分为Voronoi单元。但是，通过在这种简单的集合技术中应用k-means的<em class="mp">大军，他们能够找到更复杂形状的星团，如HDBSCAN。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/82dd1d6713a1246063e8268af7a5e896.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6IVSq0A9DXPLq7ydXSGcPg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">世卫组织会赢吗？图片由作者提供。</p></figure><h1 id="4e34" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">与其他聚类模型集成</h1><p id="c2c7" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">许多聚类算法利用数据的中间表示，例如邻居图或距离矩阵，以便找到最终的聚类。例如，著名的HDBSCAN使用<em class="mp">最小生成树</em>来表示数据。</p><p id="de65" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因此，许多聚类算法接受相似性/距离矩阵作为输入，而不是常规的点列表。因此，前面介绍的相似性矩阵可以输入其中。</p><p id="b4fe" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这种方法背后的思想是使用一组“较弱的”聚类算法为后面的算法创建一个“较强的”基础。这种方法类似于监督机器学习中的推进技术。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/3592a83549f6034256c7de9d7aef7662.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*7ncRyVWR1V0gBmCfy5PayA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">K-均值和谱的集成。图片作者。</p></figure><p id="177b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们看下面的例子，在这个例子中，我们使用常规的K-Means来“提高”谱聚类算法的性能。</p><blockquote class="mv mw mx"><p id="c7ff" class="lo lp mp lq b lr mk jr lt lu ml ju lw my mm lz ma mz mn md me na mo mh mi mj ij bi translated">重要的是要记住“改进”和“更好”的概念在集群中可能是棘手的，因为没有集群是/应该是什么样子的明确绝对定义[2]。</p></blockquote><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="88b4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">像以前一样，创建了一组NUM _均值K均值模型。aggregator_clt是负责聚类相似性矩阵的算法，创建最终的聚类标签。</p><p id="22f5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">128个K均值模型被用作“较弱”的学习器，谱聚类被用作聚合器。设置affinity="precomputed "很重要，这样模型才能理解相似性矩阵是为<em class="mp">传递的。</em>合体()【法。</p><blockquote class="mv mw mx"><p id="3064" class="lo lp mp lq b lr mk jr lt lu ml ju lw my mm lz ma mz mn md me na mo mh mi mj ij bi translated"><strong class="lq ir"> <em class="iq">注</em> : </strong> <br/>谱聚类算法接受相似性矩阵，但并非所有超参数(sklearn)中具有affinity="precomputed "或metric="precomputed "的聚类模型都是如此。有的需要一个<em class="iq">距离矩阵</em>，正好相反。将归一化相似度矩阵转换为距离矩阵的简单方法是应用对数变换，距离= -log(相似度)，只需使用一些技术来避免0≥相似度或相似度&gt; 1</p></blockquote><p id="11ea" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">EnsembleClustering是一个简单的类，用于封装相似性矩阵的创建和模型的训练。</p><p id="819a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">所以，让我们看看结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/38c9ce1a22a120bd8e84ace1acf5166b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*loqPvgqwQqBSQGtPJw7rOw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">集合K-均值+光谱与光谱独奏。图片由作者提供。</p></figure><p id="da30" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">谱聚类不是像k-means那样简单的模型，(奇怪的是，它可以使用K-Means作为实现的一个步骤)，但是它不能找到我们想要的聚类。集成方法在视觉上比单独的谱聚类执行得更好。</p><p id="f23f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">重要的是要记住，这篇文章使用迷你批处理k-means只是为了提高执行速度。任何聚类算法都可以用来构建相似性矩阵(只是要小心离群值)。因此，作为家庭作业，我建议尝试使用一组<em class="mp">异构</em>聚类算法运行代码，如下图所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/dec99dbd3b4658d4893b7492a4ec5b7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*2BX37QJH-0D6-m7yNGb3tA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用不同来源构建相似性矩阵。图片由作者提供。</p></figure><h1 id="11cd" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="0561" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">集成技术在许多监督机器学习任务中达到了最先进的水平。另一方面，在集群中，它们不太出名，实现起来也不那么简单。希望这篇文章能让我们了解如何在聚类上进行集成。</p><p id="fb83" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们能够使用K-means模型(最简单的聚类模型之一)来执行非球状聚类，而不需要传递K值。它还能够“改进”另一种聚类算法的结果。</p><p id="50e1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">当然，像生活中的一切一样，没有什么灵丹妙药，这篇文章也不是由“集群合奏公司”赞助的。</p><p id="1dda" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这篇文章的主要目的是介绍集成聚类的技术。尽管它们非常简单，但我希望这篇文章中涉及的技术能够帮助您理解这个过程是如何工作的。</p><p id="38a3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这篇短文只是触及了聚类集成主题的表面，请在下面的参考资料中搜索更多信息。包含代码的Github存储库也在参考资料部分。</p><p id="ce16" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">感谢您的阅读！:)</p><h1 id="2f3e" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">参考</h1><blockquote class="mv mw mx"><p id="21c0" class="lo lp mp lq b lr mk jr lt lu ml ju lw my mm lz ma mz mn md me na mo mh mi mj ij bi translated">GitHub上的代码:<br/><a class="ae kv" href="https://github.com/jaumpedro214/posts/tree/main/ensamble_clustering" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/jaumpedro 214/posts/tree/main/ensamble _ clustering</a></p></blockquote><p id="acc4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[1]t .布恩戈恩和n .伊姆翁(2018年)。集群集成:最近扩展和应用的方法综述。 <em class="mp">计算机科学评论</em>，<em class="mp"> 28 </em>，1–25。<br/> [2]亨宁，C. (2015)。<a class="ae kv" href="https://www.google.com.br/" rel="noopener ugc nofollow" target="_blank">什么是真正的集群？</a>。<em class="mp">模式识别字母</em>，<em class="mp"> 64 </em>，53–62。<br/>【3】Scikit-Learn，<a class="ae kv" href="https://scikit-learn.org/stable/modules/clustering.html" rel="noopener ugc nofollow" target="_blank">聚类算法官方文档</a><br/>【4】维基百科文章，<a class="ae kv" href="https://en.wikipedia.org/wiki/Component_(graph_theory)" rel="noopener ugc nofollow" target="_blank">组件(图论)</a>。</p></div></div>    
</body>
</html>