<html>
<head>
<title>Understanding AutoEncoders with an example: A step-by-step tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过示例了解自动编码器:分步指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-autoencoders-with-an-example-a-step-by-step-tutorial-693c3a4e9836#2022-05-31">https://towardsdatascience.com/understanding-autoencoders-with-an-example-a-step-by-step-tutorial-693c3a4e9836#2022-05-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="19a3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">第一部分:普通自动编码器</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/09d940b2df75f395bb94a462ba7262e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4bz0zqpIPd4djzlo"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Natalya Letunova 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="026e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="e64b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">自动编码器很酷！</strong>例如，它们可以用作<em class="mn">生成模型</em>，或者用作<em class="mn">异常检测器</em>。</p><p id="793d" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">此外，自动编码器背后的想法实际上非常简单:我们采用<strong class="lt iu">两个模型</strong>，一个<strong class="lt iu">编码器</strong>和一个<strong class="lt iu">解码器</strong>，并在它们中间放置一个“<strong class="lt iu"> <em class="mn">瓶颈</em> </strong>”。然后，我们给它<strong class="lt iu">相同的数据作为输入和输出</strong>，因此自动编码器同时<strong class="lt iu">学习输入的压缩表示</strong>(这是编码器将做的)，以及<strong class="lt iu">如何从该表示重构输入</strong>(这是解码器将做的)。</p><p id="370c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">看起来很简单，对吧？</p><p id="ddc2" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">然而，自动编码器的<strong class="lt iu">实现细节</strong>，<strong class="lt iu">是</strong> <strong class="lt iu">许多</strong>并且需要你密切注意以使它们正确。因此，我们从一个简单的<em class="mn">合成数据集</em>(我们将在整个系列中使用它)和一个<em class="mn">普通自动编码器(AE)开始这个系列的两篇文章。</em></p><p id="afee" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在第二篇文章中，我们将使用<em class="mn">变分自动编码器</em>、<em class="mn">重新参数化技巧</em>、<em class="mn"> Kullback-Leibler发散/损失</em>和<em class="mn">卷积变分自动编码器(CVAEs)。</em></p><p id="2431" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们邀请您在使用Google Colab运行随附笔记本的同时阅读这一系列文章，随附笔记本可从my GitHub的“<a class="ae ky" href="https://github.com/dvgodoy/AccompanyingNotebooks" rel="noopener ugc nofollow" target="_blank">随附笔记本</a>”存储库中获得:</p><div class="mt mu gp gr mv mw"><a href="https://colab.research.google.com/github/dvgodoy/AccompanyingNotebooks/blob/main/Understanding%20AutoEncoders.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">谷歌联合实验室——了解自动编码器</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">随附笔记本</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">colab.research.google.com</p></div></div><div class="nf l"><div class="ng l nh ni nj nf nk ks mw"/></div></div></a></div><p id="bfb1" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">此外，我构建了一个<em class="mn">目录</em>来帮助你浏览两篇文章的主题，如果你把它作为一个<strong class="lt iu">迷你课程</strong>并且一次一个主题地浏览内容的话。</p><h1 id="8e1f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">目录</h1><h2 id="66d2" class="nl la it bd lb nm nn dn lf no np dp lj ma nq nr ll me ns nt ln mi nu nv lp nw bi translated">第一部分:普通自动编码器(本文)</h2><ul class=""><li id="b41b" class="nx ny it lt b lu lv lx ly ma nz me oa mi ob mm oc od oe of bi translated"><a class="ae ky" href="https://medium.com/p/693c3a4e9836#d28b" rel="noopener">一个类似MNIST的圆形数据集</a></li><li id="012b" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><a class="ae ky" href="https://medium.com/p/693c3a4e9836#9828" rel="noopener">编码器</a></li><li id="6ba1" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><a class="ae ky" href="https://medium.com/p/693c3a4e9836#dc2a" rel="noopener">潜在空间</a></li><li id="c66e" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><a class="ae ky" href="https://medium.com/p/693c3a4e9836#bcda" rel="noopener">解码器</a></li><li id="8513" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><a class="ae ky" href="https://medium.com/p/693c3a4e9836#1fef" rel="noopener">损失函数</a></li><li id="ae6b" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><a class="ae ky" href="https://medium.com/p/693c3a4e9836#4600" rel="noopener">自动编码器(AE) </a></li><li id="4edb" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated">奖励:<a class="ae ky" href="https://medium.com/p/693c3a4e9836#1e0c" rel="noopener">自动编码器作为异常检测器</a></li></ul><h2 id="0a66" class="nl la it bd lb nm nn dn lf no np dp lj ma nq nr ll me ns nt ln mi nu nv lp nw bi translated">第二部分:可变自动编码器</h2><ul class=""><li id="7854" class="nx ny it lt b lu lv lx ly ma nz me oa mi ob mm oc od oe of bi translated"><a class="ae ky" href="https://medium.com/p/a79d2ea2945e#0e6a" rel="noopener">变分自动编码器(VAE) </a></li><li id="b810" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><a class="ae ky" href="https://medium.com/p/a79d2ea2945e#19b6" rel="noopener">重新参数化技巧</a></li><li id="5417" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><a class="ae ky" href="https://medium.com/p/a79d2ea2945e#3443" rel="noopener">库尔贝克-莱布勒分歧/损失</a></li><li id="b0e6" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><a class="ae ky" href="https://medium.com/p/a79d2ea2945e#13ed" rel="noopener">损失规模</a></li><li id="f7c8" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><a class="ae ky" href="https://medium.com/p/a79d2ea2945e#a1e5" rel="noopener">卷积变分自动编码器(CVAE) </a></li></ul><h1 id="d28b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">一个类似MNIST的圆形数据集</h1><p id="f906" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">手写数字的MNIST数据库(<em class="mn">修改的</em> <a class="ae ky" href="https://en.wikipedia.org/wiki/National_Institute_of_Standards_and_Technology" rel="noopener ugc nofollow" target="_blank"> <em class="mn">国家标准与技术研究所</em> </a> <em class="mn">数据库</em>)是使用图像作为输入的教程的首选数据集。这些单通道(灰度)图像的尺寸缩小到28x28像素，非常适合快速训练计算机视觉模型。</p><p id="80b0" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在本教程中，我们将保持使用28x28像素图像的悠久传统，但我们将生成一个比传统MNIST更简单的合成数据集-一个由不同大小的圆组成的数据集:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/174b1fc77bb198167594b215ac646dcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hhMeCJxdwoaOkG8_yJANrA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一个由不同大小的圆组成的类似MNIST的数据集。图片作者。</p></figure><p id="e569" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">为了生成这些圆，我们将使用Matplotlib在图形上绘制它们，然后使用PIL将这些图形转换为灰度图像，调整为典型的28x28大小。靠近中心(坐标0.5，0.5)画圆，每个圆的半径在0.03到0.47之间。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">生成类似MNIST的圆形数据集</p></figure><p id="ffeb" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在生成1000个圆的图像后，我们将构建一个<strong class="lt iu"> TensorDataset </strong>，其中<strong class="lt iu">像素值(现在在范围【0.0，1.0】内)是特征</strong>，半径是标签；以及一个数据加载器，用于在训练我们的模型时加载小批量数据。</p><p id="611c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">那么，让我们开始吧！</p><h1 id="9828" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">编码器</h1><p id="bbcf" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">编码器的作用是<strong class="lt iu">将一个输入</strong>(<strong class="lt iu"><em class="mn">x</em></strong>)—784像素(在我们的例子中是28 x28)—<strong class="lt iu">映射到一个压缩表示</strong>，也就是一个矢量。这个向量通常用字母<strong class="lt iu"> <em class="mn"> z </em> </strong>表示，可以是你想要的任何大小。</p><p id="d55c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在我们这个简单的例子中，我们将使用一个大小为1的向量，也就是说，每幅图像将被赋予一个值，并且只有一个值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/8ee9a4dd02ecf0cbfa727ca9e6941c12.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*It7tZgoIS0GFhJEHEn6jXQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">将输入(<strong class="bd op"> x </strong>)编码成压缩表示(<strong class="bd op"> z </strong>)。图片作者。</p></figure><blockquote class="oq or os"><p id="f2d4" class="lr ls mn lt b lu mo ju lw lx mp jx lz ot mq mc md ou mr mg mh ov ms mk ml mm im bi translated">"为什么向量的大小只有一个？"</p></blockquote><p id="593b" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这种选择背后的原因有两个:首先，它将使可视化更容易(例如，绘制直方图，沿单一维度重建图像)；第二，我还没有告诉你第二个原因(这将是一个主要的剧透)，因为你会在后面的“潜在空间分布(AE)”部分找到答案</p><p id="ea4d" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">因此，我们需要一个模型，它获取一幅图像并输出一个包含单个值的向量(<strong class="lt iu"> <em class="mn"> z </em> </strong>)。下面的模型做到了这一点——它将图像展平为784像素/特征，通过两个完全连接的隐藏层，最终输出一个值。</p><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="4819" class="nl la it ox b gy pb pc l pd pe">Encoder(<br/>  (base_model): Sequential(<br/>    (0): Flatten(start_dim=1, end_dim=-1)<br/>    (1): Linear(in_features=784, out_features=2048, bias=True)<br/>    (2): LeakyReLU(negative_slope=0.01)<br/>    (3): Linear(in_features=2048, out_features=2048, bias=True)<br/>    (4): LeakyReLU(negative_slope=0.01)<br/>  )<br/>  (lin_latent): Linear(in_features=2048, out_features=1, bias=True)<br/>)</span></pre><p id="2365" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">请注意，我将<strong class="lt iu">输出层(</strong> <code class="fe pf pg ph ox b"><strong class="lt iu">lin_latent</strong></code> <strong class="lt iu"> ) </strong>与“基础”模型分开。目前看起来这似乎是一个没有区别的区别，但是当我们在本系列的第二篇文章中处理<em class="mn">变型自动编码器</em>时，你会理解这个选择背后的原因。请原谅我。</p><p id="f2e9" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">此外，请记住，这个模型只是一个例子，它可以更简单，或更深入，甚至是一个卷积神经网络(我们将在第二篇文章中再次讨论)。</p><p id="2d96" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">现在，让我们看看代码中的模型:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="8f7a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">然后，让我们对数据集中的7号图像进行编码:</p><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="3842" class="nl la it ox b gy pb pc l pd pe">x, _ = circles_ds[7]<br/>z = encoder(x)<br/>z</span><span id="9457" class="nl la it ox b gy pi pc l pd pe"><strong class="ox iu">Output:<br/>tensor([[-0.1314]], grad_fn=&lt;AddmmBackward&gt;)</strong></span></pre><p id="9096" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这是一个大小为1的向量(<strong class="lt iu"> <em class="mn"> z </em> </strong>)。那是我们的…</p><h1 id="dc2a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">潜在空间</h1><p id="4ce8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">潜在空间就是我们上一节讲过的矢量(<em class="mn"> z </em> ) </strong>。就是这样！向量中的每个元素代表所谓的潜在空间中的一个维度。但是不要被行话吓倒，说到底，潜在空间只是一个矢量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/ee39e7add1fbefeaff59569b5f0e87a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:122/format:webp/1*Zymzy_E0nSdfISxHE0FNpw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">潜在空间(<strong class="bd op"> z </strong> ) —一个向量。图片作者。</p></figure><p id="faa9" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">而且，<strong class="lt iu">你要决定矢量的大小<em class="mn"> z </em> </strong>！请记住，随着潜在空间维数的增加(即向量的长度)，重构的输入更有可能接近原始输入。</p><blockquote class="oq or os"><p id="6544" class="lr ls mn lt b lu mo ju lw lx mp jx lz ot mq mc md ou mr mg mh ov ms mk ml mm im bi translated">"我如何选择潜在空间的<strong class="lt iu">尺寸</strong>？"</p></blockquote><p id="bd17" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">自动编码器背后的一般思想是通过“<em class="mn">瓶颈</em>”效应获得数据的<strong class="lt iu">压缩表示，因此唯一合理的是<strong class="lt iu">潜在空间的大小应当比输入</strong>的大小<em class="mn">小</em>。</strong></p><p id="94b6" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在我们的例子中，输入是包含784个像素/值的图像，我们选择的潜在空间尽可能小——只有一维——因为这将使可视化更容易。但是我们可以很好地选择其他值，比如说50，重建的图像可能会好得多。</p><blockquote class="oq or os"><p id="2254" class="lr ls mn lt b lu mo ju lw lx mp jx lz ot mq mc md ou mr mg mh ov ms mk ml mm im bi translated">话虽如此，还是有这样的情况，即<strong class="lt iu">潜在空间<em class="it">可能</em>比原始输入有更多的维度:<em class="it">去噪自动编码器</em> </strong>浮现在脑海中。这些模型背后的想法是使用图像的<strong class="lt iu">损坏/噪声版本作为输入</strong>，使用<strong class="lt iu">原始、干净的图像作为期望的重建输出</strong>。在这种情况下，潜在空间中的额外维度使模型能够“过滤”掉噪声。</p></blockquote><h1 id="bcda" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">解码器</h1><p id="f6e8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">解码器的作用是<strong class="lt iu">使用潜在空间中的一个点(<em class="mn"> z </em> ) </strong>，<strong class="lt iu"> </strong>在我们的例子中是一个包含单个值的向量，来尝试<strong class="lt iu">重构相应的输入(<em class="mn"> x~ </em> ) </strong>，即一个28×28像素的图像。这正是编码器工作的<strong class="lt iu">对立面</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/44ee1a99b2ac6ee94c97ba23d309573b.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*v8J53ikycEzvwRD143CRTQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">解码压缩表示(<strong class="bd op"> z </strong>)以重构输入(<strong class="bd op"> x </strong> ~)。图片作者。</p></figure><p id="6f52" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">理论上，您可以使用任何模型，该模型采用包含单个值的矢量(<strong class="lt iu"> <em class="mn"> z </em> </strong>)，并输出784个特征/像素。然而在实践中，通常<strong class="lt iu">使用的解码器是编码器</strong>的镜像，因此它们在模型复杂性方面是相似的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="551d" class="nl la it ox b gy pb pc l pd pe">Sequential(<br/>  (0): Linear(in_features=1, out_features=2048, bias=True)<br/>  (1): LeakyReLU(negative_slope=0.01)<br/>  (2): Linear(in_features=2048, out_features=2048, bias=True)<br/>  (3): LeakyReLU(negative_slope=0.01)<br/>  (4): Linear(in_features=2048, out_features=784, bias=True)<br/>  (5): Unflatten(dim=1, unflattened_size=(1, 28, 28))<br/>)</span></pre><p id="b8c8" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">如你所见，解码器使用<strong class="lt iu">与编码器相同的三个线性层，但顺序与</strong>相反，因此它是编码器的完美镜像。</p><p id="7926" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">让我们使用未经训练的解码器用它(不良地)重建图像#7:</p><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="bac3" class="nl la it ox b gy pb pc l pd pe">x_tilde = decoder(z)<br/>x_tilde, x_tilde.shape</span><span id="878d" class="nl la it ox b gy pi pc l pd pe"><strong class="ox iu">Output:<br/>tensor([[[[ 1.9056e-01, -4.4774e-02, ..., -2.6308e-02],<br/>          ...<br/>          [ 8.7409e-02, -6.3456e-03, ...,  1.8832e-02]]]], <br/>grad_fn=&lt;ViewBackward&gt;)<br/>torch.Size([1, 1, 28, 28]))</strong></span></pre><p id="3ef7" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">现在我有一个问题要问你:你看到上面的输出(<code class="fe pf pg ph ox b"><strong class="lt iu">x_tilde</strong></code>)有什么异常吗？</p><blockquote class="oq or os"><p id="6be1" class="lr ls mn lt b lu mo ju lw lx mp jx lz ot mq mc md ou mr mg mh ov ms mk ml mm im bi translated">提示:记住解码器的输出是试图重建输入(图像)。</p></blockquote><p id="acb1" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">如果你发现了消极的价值观，你就明白了！</p><p id="4384" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu">像素不应该有负值</strong>——它们可以是[0，255]范围内的整数，也可以是[0，1]范围内的浮点数，但绝不是负值。</p><blockquote class="oq or os"><p id="ea5e" class="lr ls mn lt b lu mo ju lw lx mp jx lz ot mq mc md ou mr mg mh ov ms mk ml mm im bi translated">“那么，解码器模型有错吗？”</p></blockquote><p id="5136" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">不一定，不。但是，当涉及到用于训练模型的损失函数时，您需要做出明智的选择。</p><h1 id="1fef" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">损失函数</h1><p id="6bc0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">由于解码器的<strong class="lt iu">最后一层是线性层</strong>，它将输出(-inf，+inf)范围内的值，但这本身并不是一个决定性因素——我们只需要意识到这一点，然后<strong class="lt iu">相应地选择损失函数</strong>。</p><p id="71e9" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在这种情况下，我们可以使用<strong class="lt iu">均方误差(MSE)作为损失函数</strong>，就好像我们在为每个像素运行<strong class="lt iu">回归任务一样。</strong></p><blockquote class="oq or os"><p id="1653" class="lr ls mn lt b lu mo ju lw lx mp jx lz ot mq mc md ou mr mg mh ov ms mk ml mm im bi translated">“听起来很奇怪；我可以在末尾添加一个sigmoid层，将所有内容都挤到(0，1)范围内吗？”</p></blockquote><p id="c23a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">当然，你可以！<strong class="lt iu">在解码器模型上加一个sigmoid层</strong>其实也是家常便饭，所以输出保证在像素范围内(0，1)。</p><blockquote class="oq or os"><p id="2331" class="lr ls mn lt b lu mo ju lw lx mp jx lz ot mq mc md ou mr mg mh ov ms mk ml mm im bi translated">“损失函数呢？”</p></blockquote><p id="3787" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">一方面，现在看起来好像我们正在为每个像素运行二进制分类任务，这将调用不同的损失函数，即二进制交叉熵。</p><blockquote class="oq or os"><p id="9173" class="lr ls mn lt b lu mo ju lw lx mp jx lz ot mq mc md ou mr mg mh ov ms mk ml mm im bi translated">另一方面，也有可能<strong class="lt iu">继续使用MSE而不是</strong>(这实际上相当普遍！)，将sigmoid图层简单地用作将输出挤压到所需范围的手段(毕竟，将输出视为像素具有值1.0的概率没有太大意义)。</p></blockquote><p id="372a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">所以，你可能会疑惑，“<em class="mn">MSE和BCE哪个损失函数更好？</em>“就像我们这个领域的许多事情一样，这个问题没有直接的答案，但我们遵循大卫·福斯特的建议，在本教程中坚持使用MSE:</p><blockquote class="oq or os"><p id="1c85" class="lr ls mn lt b lu mo ju lw lx mp jx lz ot mq mc md ou mr mg mh ov ms mk ml mm im bi translated">“二进制交叉熵对严重错误的极端预测处以更重的惩罚，因此它倾向于将像素预测推到范围的中间。这导致图像不太清晰。”</p><p id="35fc" class="lr ls mn lt b lu mo ju lw lx mp jx lz ot mq mc md ou mr mg mh ov ms mk ml mm im bi translated">来源:“生成性深度学习”，作者大卫·福斯特</p></blockquote><h1 id="4600" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">自动编码器</h1><blockquote class="oq or os"><p id="d841" class="lr ls mn lt b lu mo ju lw lx mp jx lz ot mq mc md ou mr mg mh ov ms mk ml mm im bi translated">“转发:当编码器遇到解码器时”</p></blockquote><p id="275a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这看起来像是80年代的电影名，但在我们的例子中，<strong class="lt iu">编码器和解码器是<em class="mn">字面上的</em>为彼此而生</strong> :-)</p><p id="2f97" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">那么，自动编码器是如何工作的呢？这是一系列简短的步骤:</p><ul class=""><li id="d663" class="nx ny it lt b lu mo lx mp ma pk me pl mi pm mm oc od oe of bi translated"><strong class="lt iu">编码器</strong>接收<strong class="lt iu">输入(<em class="mn">x</em>)</strong>)<strong class="lt iu">将其映射到一个向量(<em class="mn"> z </em> ) </strong>，即潜在空间；</li><li id="dcee" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><strong class="lt iu">解码器</strong>从编码器接收<strong class="lt iu">向量(<em class="mn"> z </em> ) </strong>、潜在空间，<strong class="lt iu">生成重构输入(<em class="mn"> x~ </em>)。</strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pn"><img src="../Images/e3fad852142c2e811418132be0f2060e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Se0N8mR5BaewmEo05oO1yw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">当编码器遇到解码器时。图片作者。</p></figure><p id="3dbe" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">很简单，对吧？我们只是把两块拼在一起。如果我们使用前面章节中的模型，它看起来像这样:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="7cac" class="nl la it ox b gy pb pc l pd pe">AutoEncoder(<br/>  (enc): Encoder(<br/>    (base_model): Sequential(<br/>      (0): Flatten(start_dim=1, end_dim=-1)<br/>      (1): Linear(in_features=784, out_features=2048, bias=True)<br/>      (2): LeakyReLU(negative_slope=0.01)<br/>      (3): Linear(in_features=2048, out_features=2048, bias=True)<br/>      (4): LeakyReLU(negative_slope=0.01)<br/>    )<br/>    (lin_latent): Linear(in_features=2048, out_features=1, bias=True)<br/>  )<br/>  (dec): Sequential(<br/>    (0): Linear(in_features=1, out_features=2048, bias=True)<br/>    (1): LeakyReLU(negative_slope=0.01)<br/>    (2): Linear(in_features=2048, out_features=2048, bias=True)<br/>    (3): LeakyReLU(negative_slope=0.01)<br/>    (4): Linear(in_features=2048, out_features=784, bias=True)<br/>    (5): Unflatten(dim=1, unflattened_size=(1, 28, 28))<br/>  )<br/>)</span></pre><p id="7af4" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">此外，很容易看出<em class="mn">解码器是编码器</em>的镜像，如下表所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi po"><img src="../Images/9dced11429dd32e8266e4b80a5600b88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*fh6duE1_Ld3OQPVWG428dg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">将图层“匹配”为镜像。图片作者。</p></figure><p id="1767" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">现在，让我们看看如何执行…</p><h2 id="b1db" class="nl la it bd lb nm nn dn lf no np dp lj ma nq nr ll me ns nt ln mi nu nv lp nw bi translated">模型训练(AE)</h2><p id="2dfa" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这是PyTorch中一个典型的训练循环:执行正向传递，计算损耗，使用<code class="fe pf pg ph ox b"><strong class="lt iu">backward()</strong></code>计算梯度，更新参数，将梯度归零。一切照旧！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><pre class="kj kk kl km gt ow ox oy oz aw pa bi"><span id="9c21" class="nl la it ox b gy pb pc l pd pe">Epoch 001 | Loss &gt;&gt; 0.1535<br/>Epoch 002 | Loss &gt;&gt; 0.0220<br/>...<br/>Epoch 009 | Loss &gt;&gt; 0.0117<br/>Epoch 010 | Loss &gt;&gt; 0.0109</span></pre><p id="d2c6" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">如果您需要了解如何在PyTorch中培训模特，请查看:</p><div class="mt mu gp gr mv mw"><a rel="noopener follow" target="_blank" href="/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">通过示例了解PyTorch:分步指南</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">一种结构化的、渐进的、基于基本原则的方法。</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="pp l nh ni nj nf nk ks mw"/></div></div></a></div><h2 id="711b" class="nl la it bd lb nm nn dn lf no np dp lj ma nq nr ll me ns nt ln mi nu nv lp nw bi translated">重建示例</h2><p id="6911" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在模型已经训练好了，让我们对数据集的图像#7进行编码和解码(即重建):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/a6af7786c9fb36525fd5dcfc9b87c11c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Y9Cvo1gQIyidudEKSZvaA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一个重建的圆。图片作者。</p></figure><p id="d29e" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">还不错，对吧？这个圆圈被清晰地重建了，但看起来像是其他更小的圆圈也被模糊地重建了。让我们试着理解为什么会发生这种情况…</p><h2 id="4786" class="nl la it bd lb nm nn dn lf no np dp lj ma nq nr ll me ns nt ln mi nu nv lp nw bi translated">潜在空间分布</h2><p id="408e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果我们为我们数据集中的每个图像绘制相应的<strong class="lt iu">潜在空间(<em class="mn"> z </em> ) </strong>的<strong class="lt iu">直方图</strong>，我们将得到左边的图。有几件事需要注意:</p><p id="9291" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">(1)略低于1的<strong class="lt iu">巨大尖峰</strong>，这意味着<strong class="lt iu">我们数据集中的许多图像被映射到一个微小的区间</strong> [0.6，0.9]；</p><p id="7eba" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">(2)所有点都在[-0.50，1.62]内——任何超出这个区间的都是“空”的潜在空间；</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/c624117e13e805da9a202cef53af1bef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qLNd9lLRHdbNy9BWnZKr1Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">左图:潜在空间的分布；右图:潜在空间与半径。图片作者。</p></figure><p id="c89a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">为了进一步研究它，我们可以<strong class="lt iu">为每一幅图像绘制潜在空间与圆的半径</strong>的关系，这就是右边的图。</p><blockquote class="oq or os"><p id="fb26" class="lr ls mn lt b lu mo ju lw lx mp jx lz ot mq mc md ou mr mg mh ov ms mk ml mm im bi translated">“半径？为什么？”</p></blockquote><p id="2b08" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">由于我们的圆是画在图像的中心(除了一点点抖动)，最好描述圆的一个特征是它的半径。我们的自动编码器的任务是将圆映射到潜在空间中的一维空间中。所以，<strong class="lt iu">可能，<em class="mn">只是可能</em>，它可能已经学会将圆映射到一个实际代表半径的潜在空间</strong>。那会是<em class="mn">牛逼</em>吧？</p><p id="c112" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">如果是这样的话，我们应该会看到一个圆的半径和它对应的一维潜在空间之间的<em class="mn">线性关系。但是，很明显，在右边的图中看不到这种关系，至少在半径的整个范围内(或者如果你更喜欢拉丁复数的话，在半径的整个范围内)看不到这种关系！).</em></p><p id="c420" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们在那里发现了什么？</p><p id="bba2" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">(3)<strong class="lt iu">半径小于0.25 </strong>左右的圆都被<strong class="lt iu">映射到潜在空间</strong>的[0.6，0.9]区间(图的垂直部分)；</p><p id="1a1a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">(4)对于0.25到0.3 左右的<strong class="lt iu">半径，半径与映射的潜在空间</strong>之间存在<strong class="lt iu">线性关系；</strong></p><p id="9f3b" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">(5)对于大于0.3 左右的<strong class="lt iu">半径，半径与映射的潜在空间</strong>之间大致呈<strong class="lt iu">的负线性关系。</strong></p><blockquote class="oq or os"><p id="6914" class="lr ls mn lt b lu mo ju lw lx mp jx lz ot mq mc md ou mr mg mh ov ms mk ml mm im bi translated">“实际上，这意味着什么？”</p></blockquote><p id="ecc4" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">让我们重建一些图像来更好地说明它！</p><h2 id="8e80" class="nl la it bd lb nm nn dn lf no np dp lj ma nq nr ll me ns nt ln mi nu nv lp nw bi translated">重建(AE)</h2><p id="ee4a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们正在重建五幅图像，对应于潜在空间中的五个点(-3.0，-0.5，0.0，0.9，3.0)，这可能有助于我们更好地理解正在发生的事情:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ps"><img src="../Images/3bd950fd7f0a50062ff090349886dc19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eIFE6Mcfm7-qE5xfFY3PfQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">潜在空间中所选点的重建图像(AE)。图片作者。</p></figure><ul class=""><li id="a98f" class="nx ny it lt b lu mo lx mp ma pk me pl mi pm mm oc od oe of bi translated"><strong class="lt iu"> [-3.0]和【3.0】</strong>，处于<em class="mn">空</em>“潜在空间”的两端，<em class="mn">在</em>之前没有圆经过的地方，然而，自动编码器仍然在生成圆，尽管它们似乎也有一些<em class="mn">噪声</em>；</li><li id="854a" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><strong class="lt iu">[-0.5】</strong><strong class="lt iu">【0.0】</strong>，位于映射的潜在空间的<em class="mn">边缘</em>，分别对应大约0.4的半径；它们是重建圆圈的成功案例，尽管它们似乎也包含一个微弱的、更小的内圈；</li><li id="2ebf" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><strong class="lt iu">【0.9】</strong>，对应潜在空间的“<em class="mn">拥堵</em>”区域，看起来像是不同半径的小圆的“<em class="mn">混合体</em>”。</li></ul><blockquote class="pt"><p id="7c43" class="pu pv it bd pw px py pz qa qb qc mm dk translated">"大胆地去一个圈子从未去过的地方！"</p><p id="381f" class="pu pv it bd pw px qd qe qf qg qh mm dk translated">皮卡德上尉</p></blockquote><p id="5e62" class="pw-post-body-paragraph lr ls it lt b lu qi ju lw lx qj jx lz ma qk mc md me ql mg mh mi qm mk ml mm im bi translated">我们当然可以做得更好，但是我们需要使用更好的模型，即<strong class="lt iu">可变自动编码器</strong>，这将是本系列第二篇文章的主题。</p><h1 id="1e0c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">额外收获:自动编码器作为异常检测器</h1><p id="29dd" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这一部分的灵感来自于我读过的最有趣的文章之一——艾米莉·杜邦的《<a class="ae ky" href="https://emiliendupont.github.io/2018/03/14/mnist-chicken/" rel="noopener ugc nofollow" target="_blank">让一只鸡穿过MNIST模特</a>》。它非常详细地说明了自动编码器作为异常检测器的作用。</p><p id="aa4d" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我在这里只是触及这个话题的表面，所以你可以得到它的要点。但是，我决定用鸭子代替鸡，因为我更喜欢鸭子。所以，让我们从一只鸭子开始，最好是一只烤鸭子！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qn"><img src="../Images/b8a5dc1ec16e9fb2f3dba93b077b05a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*Pb6aQn1pMXlOD3zJ4wdyEQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一只鸭子，又高又肥！改编自<a class="ae ky" href="https://commons.wikimedia.org/wiki/File:Anas_platyrhynchos_Rusanivka4.JPG" rel="noopener ugc nofollow" target="_blank">这张</a>图片来自维基共享。</p></figure><blockquote class="oq or os"><p id="0c6a" class="lr ls mn lt b lu mo ju lw lx mp jx lz ot mq mc md ou mr mg mh ov ms mk ml mm im bi translated">“那是什么<strong class="lt iu">鸭子</strong>？!"</p></blockquote><p id="7476" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">显然，<strong class="lt iu">鸭不是圆</strong>。尽管如此，如果我们给我们的<strong class="lt iu">编码器</strong>输入一个28x28像素的鸭子图像，它会输出相应的<strong class="lt iu">潜在空间</strong>，无论如何。</p><blockquote class="oq or os"><p id="0a33" class="lr ls mn lt b lu mo ju lw lx mp jx lz ot mq mc md ou mr mg mh ov ms mk ml mm im bi translated">"把一只鸭子映射到圆的潜在空间有什么意义？"</p></blockquote><p id="fc92" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">潜伏空间本身没那么有趣，但是<strong class="lt iu">重构鸭才是！</strong>如果我们给我们的<strong class="lt iu">解码器</strong>输入一个潜在空间，无论如何它都会输出一个<strong class="lt iu">重建图像</strong>。让我们看看会发生什么…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/8054db34564627895c1993a3c24d706e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MkI7kh5q0th_Sa7tg3Gw7w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">重建一只鸭子——或者试着去重建！图片作者。</p></figure><p id="6fb5" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">嗯，<strong class="lt iu">重建图像</strong>看起来更像一个<strong class="lt iu">圆</strong>而不是一只鸭子！这不应该是一个惊喜，毕竟，我们在圆形图像上训练了我们的自动编码器。</p><blockquote class="oq or os"><p id="acd2" class="lr ls mn lt b lu mo ju lw lx mp jx lz ot mq mc md ou mr mg mh ov ms mk ml mm im bi translated">“我不明白；这怎么‘有趣’了？”</p></blockquote><p id="1ffc" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu">重建图像看起来根本不像原始图像</strong>的事实相当强烈地表明<strong class="lt iu">原始图像(一只鸭子)不属于用于训练自动编码器</strong>的相同数据分布(圆)。</p><blockquote class="oq or os"><p id="53a2" class="lr ls mn lt b lu mo ju lw lx mp jx lz ot mq mc md ou mr mg mh ov ms mk ml mm im bi translated">这就是有趣的部分——<strong class="lt iu">我们可以使用重建损失</strong>(即原始图像和重建图像的差异)<strong class="lt iu">将图像分类为异常</strong>。</p><p id="5998" class="lr ls mn lt b lu mo ju lw lx mp jx lz ot mq mc md ou mr mg mh ov ms mk ml mm im bi translated">一旦你知道一幅图像不属于用于训练模型的原始数据集中的其他图像，<strong class="lt iu">就很少有或没有理由再使用该图像作为输入来进行预测</strong>。</p></blockquote><p id="62e0" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">假设我们让我们的鸭子通过一个训练有素的MNIST模型来对数字进行分类(这就是Emilien在他的文章中所做的)。分类器可能会预测我们的鸭子是“5”或“8”，但这些预测显然是无意义的。<strong class="lt iu">分类器无法做得更好，因为它无法输出“<em class="mn">我不知道</em>”作为预测</strong>。但是，如果我们<strong class="lt iu">使用自动编码器来检测异常输入</strong>——在将它们提交给另一个模型之前——我们可以轻松实现这一点。</p><h1 id="1eae" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">最后的想法</h1><p id="a0cf" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">感谢您坚持到这篇长文的结尾:-)，但是，尽管它<em class="mn">是</em>一篇长文，但它只是一个介绍，并且它只涵盖了您需要熟悉的开始试验自动编码器的基本工具和技术。</p><p id="aae2" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在本系列的第二篇文章中，我们将了解<em class="mn">变分自动编码器</em>、<em class="mn">重新参数化技巧</em>、<em class="mn"> Kullback-Leibler发散/损失</em>和<em class="mn">卷积变分自动编码器(CVAEs)。</em></p><p id="285c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">与此同时，如果你想了解更多关于自动编码器和生成模型的知识，我推荐大卫·福斯特的<em class="mn">生成深度学习</em>，作者是奥赖利。</p><p id="2fd7" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">还有，如果你想了解更多PyTorch，计算机视觉，NLP的知识，给我自己的系列丛书，<a class="ae ky" href="https://pytorchstepbystep.com/" rel="noopener ugc nofollow" target="_blank"> <em class="mn">深度学习用PyTorch循序渐进</em> </a>，一试:-)</p></div><div class="ab cl qo qp hx qq" role="separator"><span class="qr bw bk qs qt qu"/><span class="qr bw bk qs qt qu"/><span class="qr bw bk qs qt"/></div><div class="im in io ip iq"><p id="6825" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><em class="mn">如果您有任何想法、意见或问题，请在下面留下评论或通过我的</em> <a class="ae ky" href="https://bio.link/dvgodoy" rel="noopener ugc nofollow" target="_blank"> <em class="mn">个人资料链接</em> </a> <em class="mn">页面联系。</em></p><p id="a43b" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><em class="mn">如果你喜欢我的文章，请考虑使用我的推荐页面</em> <a class="ae ky" href="https://dvgodoy.medium.com/membership" rel="noopener"> <em class="mn">注册一个中级会员</em> </a> <em class="mn">来直接支持我的工作。对于每一个新用户，我从中获得一小笔佣金:-) </em></p></div></div>    
</body>
</html>