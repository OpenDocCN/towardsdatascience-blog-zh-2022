<html>
<head>
<title>New Scikit-Learn V.1.1.0 Update Release Top Highlight</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">新的Scikit-Learn V.1.1.0更新版本主要亮点</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/new-scikit-learn-v-1-1-0-update-release-top-highlight-f2f94985c63e#2022-05-18">https://towardsdatascience.com/new-scikit-learn-v-1-1-0-update-release-top-highlight-f2f94985c63e#2022-05-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="40b1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">来自Scikit的激动人心的更新-了解</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7e4f161553eeba072f034082d7e2ea16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pdtD_5wd5nJy-a4e"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@homajob?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">斯科特·格雷厄姆</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="0bfe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn </a>是使用Python环境的数据科学家的主要机器学习包。该软件包提供了许多在我们日常工作中使用的有用的API。</p><p id="ee1f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2022年5月，Scikit-Learn发布了新的V.1.1.0更新，提供了各种令人兴奋的功能更新。有哪些更新？让我们开始吧。</p><p id="2e6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，请在我们离开之前将Scikit-Learn更新到最新版本。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="1aab" class="ma mb it lw b gy mc md l me mf">pip install --upgrade scikit-learn</span></pre></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="a917" class="mn mb it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">平分意味着</h1><p id="cd74" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">平分K均值是Scikit-Learn中无监督机器学习K均值的一个新的附加变体。该方法在聚类过程中实现了简单的除法分层算法。</p><p id="21c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在正常的K-Means中，聚类过程是通过同时创建K个质心来进行的。将通过计算簇内和簇间相似性来评估所得的簇。星团的质心类似于重心。</p><p id="d754" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，在分割K-Means时，我们并没有同时创建K形心。相反，质心是基于前一个聚类逐步选取的。我们每次都要分割集群，直到达到K的数目。</p><p id="db83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用二等分K-Means有几个优点，包括:</p><ul class=""><li id="510d" class="nj nk it lb b lc ld lf lg li nl lm nm lq nn lu no np nq nr bi translated">如果有大量集群，效率会更高</li><li id="ff7a" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">更便宜的计算成本</li><li id="b8e8" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">它不会产生空簇</li><li id="ea61" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">聚类结果是有序的，将创建一个可见的层次结构。</li></ul><p id="3222" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们尝试一个简单的比较正常的K-均值和平分K-均值。我将使用seaborn包中的一个样本数据集来模拟结果。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="1c84" class="ma mb it lw b gy mc md l me mf">import numpy as np<br/>from sklearn.cluster import KMeans<br/>import seaborn as sns<br/>from sklearn.cluster import KMeans, BisectingKMeans<br/>import matplotlib.pyplot as plt</span><span id="5494" class="ma mb it lw b gy nx md l me mf">mpg = sns.load_dataset('mpg')<br/>mpg = mpg.dropna().reset_index(drop = True)</span><span id="9796" class="ma mb it lw b gy nx md l me mf">X = np.array(mpg[['mpg', 'acceleration']])</span><span id="31b6" class="ma mb it lw b gy nx md l me mf">km = KMeans(n_clusters=5, random_state=0).fit(X)<br/>bisect_km = BisectingKMeans(n_clusters=5, random_state=0).fit(X)</span><span id="35cb" class="ma mb it lw b gy nx md l me mf">fig, ax = plt.subplots(1, 2, figsize=(10, 5))<br/>ax[0].scatter(X[:, 0], X[:, 1], s=10, c=km.labels_)<br/>ax[0].scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=20, c="r")<br/>ax[0].set_title("KMeans")</span><span id="3a4d" class="ma mb it lw b gy nx md l me mf">ax[1].scatter(X[:, 0], X[:, 1], s=10, c=bisect_km.labels_)<br/>ax[1].scatter(<br/>    bisect_km.cluster_centers_[:, 0], bisect_km.cluster_centers_[:, 1], s=20, c="r"<br/>)<br/>_ = ax[1].set_title("BisectingKMeans")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/33021611aa66dfa78241599200b49e77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*6grefRptrEg_VOFf9Kb0eQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="c527" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上图所示，二等分K-Means可以有效地、直观地为最远部分的数据创建一个聚类。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="c438" class="mn mb it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">基于HistGradientBoostingRegressor的分位数损失函数建模</h1><p id="e35b" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">Scikit-Learn中的HistGradientBoostingRegressor是一个梯度推进回归器，是一个集成树模型，具有基于直方图的学习模型。</p><p id="6129" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于直方图的模型比普通的梯度推进回归模型更有效，因为该算法将连续特征绑定到用于训练目的的离散箱中，而不是通常的分裂技术。</p><p id="84f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据文档，HistGradientBoostingRegressor模型适用于超过10.000个样本的大数据集。</p><p id="5b30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在最近的更新中，HistGradientBoostingRegressor增加了一个新的分位数损失函数供我们使用。分位数损失函数预测可能的范围，我们称之为预测区间。您可以在这篇文章<a class="ae ky" rel="noopener" target="_blank" href="/quantile-regression-from-linear-models-to-trees-to-deep-learning-af3738b527c3">中进一步阅读分位数回归建模。</a></p><p id="c034" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过一个额外的分位数损失函数，我们现在可以通过<code class="fe nz oa ob lw b">loss="quantile"</code>并使用新的参数<code class="fe nz oa ob lw b">quantile</code>。使用下面的教程，我们可以跟踪每个分位数的回归预测。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="a584" class="ma mb it lw b gy mc md l me mf">from sklearn.ensemble import HistGradientBoostingRegressor<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="89c2" class="ma mb it lw b gy nx md l me mf"># Simple regression function for X * cos(X)<br/>rng = np.random.RandomState(55)<br/>X_1d = np.linspace(0, 10, num=2000)<br/>X = X_1d.reshape(-1, 1)<br/>y = X_1d * np.cos(X_1d) + rng.normal(scale=X_1d / 3)</span><span id="7c7e" class="ma mb it lw b gy nx md l me mf">quantiles = [0.9, 0.5, 0.1]<br/>parameters = dict(loss="quantile", max_bins=32, max_iter=50)<br/>hist_quantiles = {<br/>    f"quantile={quantile:.2f}": HistGradientBoostingRegressor(<br/>        **parameters, quantile=quantile<br/>    ).fit(X, y)<br/>    for quantile in quantiles<br/>}</span><span id="2d25" class="ma mb it lw b gy nx md l me mf">fig, ax = plt.subplots()<br/>ax.plot(X_1d, y, "o", alpha=0.5, markersize=1)<br/>for quantile, hist in hist_quantiles.items():<br/>    ax.plot(X_1d, hist.predict(X), label=quantile)<br/>_ = ax.legend(loc="lower left")<br/>plt.title('Sample HistGradientBoostingRegressor with Quantile Loss')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/cf614fcee6bf7c164087030cdd1ec60b.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*uMkzKaN8anGewlg5Ogzj_Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="6326" class="mn mb it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">OneHotEncoder中不常见的类别，并在所有变形金刚中获取功能名称</h1><p id="bcb4" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">一键编码是应用于分类特征以产生数字特征的常见分类过程。使用Scikit-Learn <code class="fe nz oa ob lw b">OneHotEncoder</code>，我们可以根据我们的数据开发出用于生产环境的变压器。如果你从来没有听说过什么是独热编码，你可以在下面的文章中读到它。</p><div class="od oe gp gr of og"><a rel="noopener follow" target="_blank" href="/4-categorical-encoding-concepts-to-know-for-data-scientists-e144851c6383"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd iu gy z fp ol fr fs om fu fw is bi translated">数据科学家需要了解的4个分类编码概念</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">对你的工作有帮助的分类编码概念和应用</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="oq l or os ot op ou ks og"/></div></div></a></div><p id="6d03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在最新的更新中，Scikit-Learn添加了一个新参数<code class="fe nz oa ob lw b">OneHotEncoder</code>来对所有罕见值进行分组，而不是将每个罕见值创建到新的数字特征中。让我们用一个样本数据集来尝试一下。我将使用Seaborn的tips样本数据集。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="fa82" class="ma mb it lw b gy mc md l me mf">import seaborn as sns<br/>tips = sns.load_dataset('tips')<br/>tips['size'].value_counts()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/40a7205d2730844a46004266a5d5f1fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*hWoXJlcB8H_jqEDotYylvQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="6170" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们从上图中看到的，与其他值相比，1、5和6的大小类别值并不常见。在这种情况下，我想将它们分组。我们可以用下面的代码来实现。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="c67e" class="ma mb it lw b gy mc md l me mf">from sklearn.preprocessing import OneHotEncoder</span><span id="f885" class="ma mb it lw b gy nx md l me mf">#We establish that any values frequency below 6 is considered as rare<br/>enc = OneHotEncoder(min_frequency=6, sparse=False).fit(np.array(tips['size']).reshape(-1, 1))<br/>enc.infrequent_categories_</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/1efd6af87abcc59991cc12d348b33abd.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*-eY0GKYsRi5nZ3O_-5NW5g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="ebe4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不常见的类别正是我们所期望的。如果我们尝试将分类值转换成数字列，这就是我们将得到的结果。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="2890" class="ma mb it lw b gy mc md l me mf">encoded = enc.transform(np.array([[1], [2], [3], [4],[5],[6]]))<br/>pd.DataFrame(encoded, columns=enc.get_feature_names_out())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/2be9c703c5714d2a3bdc083e39fa68ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*jhtgP9wmZXNXDxb8PLcJPQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="6582" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有不常用的类别现在都被认为是一个特征。这将帮助我们最大限度地减少创建的特征数量，避免维数灾难。</p><p id="58d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，Scikit-Learn中的所有变形金刚现在都允许我们获得特性的名称。<code class="fe nz oa ob lw b">get_features_names_out</code>属性将为转换器输出中的每一列提供字符串名称。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="8d0b" class="ma mb it lw b gy mc md l me mf">enc.get_feature_names_out()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/15bc8ba3cf2eb235e6ad89da3ef19378.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*5PqyvlAN9uYfrYhOd8utMQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="7177" class="mn mb it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">关于特征选择的附加参数</h1><p id="cd37" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">Scikit-Learn最近向变压器<code class="fe nz oa ob lw b">SequentialFeatureSelector</code>添加了新参数<code class="fe nz oa ob lw b">n_features_to_select=’auto'</code>，并将callable传递给变压器<code class="fe nz oa ob lw b">SelectFromModel</code>的<code class="fe nz oa ob lw b">max_features</code>。</p><p id="db3e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SequentialFeatureSelector是一种贪婪搜索算法，用于进行特征前向选择或后向选择以形成特征子集。该估计器将根据估计器交叉验证分数在每次迭代中添加或删除特性。</p><p id="335e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用<code class="fe nz oa ob lw b">'auto'</code>参数，当分数提高不超过<code class="fe nz oa ob lw b">tol</code>参数时，结束特征将自动结束。如果<code class="fe nz oa ob lw b">tol</code>参数为<code class="fe nz oa ob lw b">None</code>，则选择一半的特征。</p><p id="d75b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在变形金刚<code class="fe nz oa ob lw b">SelectFromModel</code>中，如果我们将callable传递给<code class="fe nz oa ob lw b">max_features</code>参数，那么允许的最大特性数量将使用<code class="fe nz oa ob lw b">max_feaures(X)</code>的输出。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="c24a" class="mn mb it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">迷你批处理</h1><p id="71a0" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">非负矩阵分解或NMF是一种用于降维和特征提取的多元分析算法。该方法常用于自然语言处理、推荐引擎、面部识别等。你可以在这里阅读更多关于NMF的信息。</p><p id="3f78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MiniBatchNMF是一种在线优化NMF的方法，它通过将数据划分为小批量来优化NMF模型。该模型适用于大型数据集，因为该过程会更快，但预测精度较低。这里的可以参考教程<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchNMF.html#sklearn.decomposition.MiniBatchNMF" rel="noopener ugc nofollow" target="_blank">。</a></p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="2fa3" class="mn mb it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated"><strong class="ak">结论</strong></h1><p id="61dd" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">Scikit-Learn最近已将软件包更新到版本1.1.0，以下是重点更新:</p><ul class=""><li id="566d" class="nj nk it lb b lc ld lf lg li nl lm nm lq nn lu no np nq nr bi translated">新的二分均值算法</li><li id="1e86" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">HistGradientBoostingRegressor的新分位数损失</li><li id="3293" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">OneHotEncoder中不常见的类别</li><li id="aaf5" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">从变形金刚中获取特征的名称</li><li id="f5a6" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">关于特征选择的附加参数</li><li id="3cd9" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">新的MiniBatchNMF算法</li></ul><p id="f042" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望有帮助！</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><blockquote class="oz pa pb"><p id="88ef" class="kz la pc lb b lc ld ju le lf lg jx lh pd lj lk ll pe ln lo lp pf lr ls lt lu im bi translated">在我的<a class="ae ky" href="https://bio.link/cornelli" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">社交媒体上访问我</strong> </a> <strong class="lb iu"> </strong>进行更深入的交谈或有任何问题。</p></blockquote><blockquote class="pg"><p id="e91f" class="ph pi it bd pj pk pl pm pn po pp lu dk translated"><em class="pq">如果您不是作为中等会员认购，请考虑通过</em> <a class="ae ky" href="https://cornelliusyudhawijaya.medium.com/membership" rel="noopener"> <em class="pq">我的推荐</em> </a> <em class="pq">进行认购。</em></p></blockquote></div></div>    
</body>
</html>