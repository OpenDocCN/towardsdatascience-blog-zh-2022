<html>
<head>
<title>Comprehend Dropout: Deep Learning by doing toy examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解辍学:通过做玩具例子进行深度学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/comprehend-dropout-deep-learning-by-doing-toy-examples-d07f985c8e2f#2022-04-26">https://towardsdatascience.com/comprehend-dropout-deep-learning-by-doing-toy-examples-d07f985c8e2f#2022-04-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="96f7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">丢弃是深度神经网络中主要的正则化技术之一。这个故事有助于你深刻理解什么是辍学，它是如何运作的。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/3635e585c99f01af5a1f4026b8b45615.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*MdBQoBAUCJxfS2VOczQzwQ.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">完全连接的网络(由作者创建)</p></figure><p id="64c2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在深度学习中，尤其是在物体检测中，很容易发生过拟合。过度拟合意味着模型非常复杂，以至于它非常适合训练集，但在测试集上失败。失败意味着它有时甚至检测到测试图像中的噪声。</p><p id="9a51" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在对象检测中，通常使用预训练的骨干进行训练，或者继续使用预训练的模型进行训练。这就是为什么在几个时代之后，验证的损失高于训练的损失。在这种情况下，添加一个下降层是有帮助的。</p><p id="4e15" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在Pytorch中，我们可以简单地通过以下方式添加一个Dropout层:</p><pre class="kj kk kl km gt lq lr ls lt aw lu bi"><span id="6061" class="lv lw it lr b gy lx ly l lz ma">from torch import nn<br/>dropout = nn.Dropout(p=0.2)</span></pre><p id="4c1f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是引擎盖下会发生什么呢？</p><h1 id="49c7" class="mb lw it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated"><strong class="ak">辍学正规化方案</strong></h1><p id="f40c" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">Dropout技术通过选择隐藏层中的一些神经元，从原始神经网络创建子神经网络。选择是重新采样神经网络中的节点(仅隐藏层中的节点)并定义一些遮罩。</p><p id="332c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">辍学不是为了偏向节点！</strong>剔除是一种正则化技术，其思想是减少由权重引起的过拟合。因此，正则化不适用于偏差节点，因为它们不接收任何输入。因此，放弃它们无助于改进预测。</p><h1 id="f8aa" class="mb lw it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">玩具示例</h1><p id="8302" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated"><strong class="kw iu">考虑下面的全连接网络，其中激活函数是ReLU。</strong>(要看网络和矩阵的关系，看上面的GIF。)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi mx"><img src="../Images/7bc08f16ce1b7f71616128435e50b311.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AL2JiCQhvAQMWztu9ph-pA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">辍学玩具示例</p></figure><p id="39b9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">在本例中，目标是使用以下漏失掩码预测x = (1，1)。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nc"><img src="../Images/8ac6d26ea5427124132e2d227b40d2d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wy6Pi1yokFjSgKgOE67XJA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">漏屏</p></figure><p id="12df" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了计算预测，我们应该计算隐藏层中的值。</p><h2 id="9c29" class="lv lw it bd mc nd ne dn mg nf ng dp mk ld nh ni mm lh nj nk mo ll nl nm mq nn bi translated">第一个隐藏层:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi no"><img src="../Images/9c467378382e253d184762c9a4f44f0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UG-qadUeIIpa6geDeGqS5g.png"/></div></div></figure><p id="e296" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中g⁽ ⁾是激活函数，则</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi no"><img src="../Images/ff92b6e0674cca9eb03c3448af509345.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tuNuQI8adzBdL9R6BCnmBA.png"/></div></div></figure><p id="7138" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">注意:</strong>第一个丢弃层μ⁰是所有节点的丢弃层。因此，对结果没有任何影响。</p><h2 id="f491" class="lv lw it bd mc nd ne dn mg nf ng dp mk ld nh ni mm lh nj nk mo ll nl nm mq nn bi translated">第二个隐藏层:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi no"><img src="../Images/174cc62f22ca62223c702a438ecf62aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5A_W3xRjs9amDSeSLxlVCw.png"/></div></div></figure><p id="fe17" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中g⁽ ⁾是<em class="np"> ReLU </em>激活函数，则</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi no"><img src="../Images/05b2b89b418e5a4d04b5ba14d29dc435.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_pk151JQQoFvToOtagxm1A.png"/></div></div></figure><h2 id="ed11" class="lv lw it bd mc nd ne dn mg nf ng dp mk ld nh ni mm lh nj nk mo ll nl nm mq nn bi translated"><strong class="ak">第三个隐藏层:</strong></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi no"><img src="../Images/7c708809e507218c2fbbd69f80258c44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JQKVGL4KK1j6dUHIqmmCnw.png"/></div></div></figure><p id="de03" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中g⁽ ⁾是<em class="np"> ReLU </em>激活函数，则</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi no"><img src="../Images/afae036b9969856889f5981165ddb8ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SITODrnbwmeBwhK73dn1vQ.png"/></div></div></figure><h2 id="eb23" class="lv lw it bd mc nd ne dn mg nf ng dp mk ld nh ni mm lh nj nk mo ll nl nm mq nn bi translated">输出层:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi no"><img src="../Images/ac786503a107ff10e0ad7fd098389553.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cQJm7Qgj7JBQS9cBtybwlQ.png"/></div></div></figure><h2 id="cfb1" class="lv lw it bd mc nd ne dn mg nf ng dp mk ld nh ni mm lh nj nk mo ll nl nm mq nn bi translated">练习:</h2><p id="69da" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">如果缺失掩码为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nc"><img src="../Images/8abe5a1c8ebf032cc297aeaa8fc2c612.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7oQjskW7gPyT8rJm35BgjQ.png"/></div></div></figure><p id="89eb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后的答案是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi no"><img src="../Images/302b2fb088b2bf8f2e88c6f1c43a5c3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Vs65IKYwdvUYeN93UMn2A.png"/></div></div></figure><h1 id="e5a9" class="mb lw it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">摘要</h1><p id="a2bd" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">这个故事有助于你理解辍学技巧。我计划在机器学习和深度学习中增加更多这样的玩具例子。因此，如果你喜欢阅读更多，请继续关注😊。</p><div class="nq nr gp gr ns nt"><a href="https://sciencenotes.medium.com/membership" rel="noopener follow" target="_blank"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd iu gy z fp ny fr fs nz fu fw is bi translated">加入媒体阅读伟大的教程和故事！</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">我写机器学习、深度学习和数据科学教程。升级阅读更多…</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">sciencenotes.medium.com</p></div></div><div class="oc l"><div class="od l oe of og oc oh ko nt"/></div></div></a></div></div></div>    
</body>
</html>