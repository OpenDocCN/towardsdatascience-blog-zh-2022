<html>
<head>
<title>How to Implement and Evaluate Decision Tree classifiers from scikit-learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何实现和评估来自scikit-learn的决策树分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-implement-and-evaluate-decision-tree-classifiers-from-scikit-learn-36ef7f037a78#2022-02-08">https://towardsdatascience.com/how-to-implement-and-evaluate-decision-tree-classifiers-from-scikit-learn-36ef7f037a78#2022-02-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4d99" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">UCL数据科学学会研讨会13:什么是决策树，决策树的实现，可视化和评估</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ff1c01ef98501e56d8bf7d38bf65c4a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yNE5qpW-0ucQHFnG"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">吉利·斯图尔特在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="89ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今年，作为UCL数据科学协会的科学负责人，该协会将在整个学年举办一系列20场研讨会，主题包括Python(数据科学家工具包)简介和机器学习方法。每个人的目标是创建一系列的小博客文章，这些文章将概述主要观点，并为任何希望跟进的人提供完整研讨会的链接。所有这些都可以在我们的<a class="ae ky" href="https://github.com/UCL-DSS" rel="noopener ugc nofollow" target="_blank"> GitHub </a>资源库中找到，并将在全年更新新的研讨会和挑战。</p><p id="0b8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本系列的第十三个研讨会是Python数据科学研讨会系列的一部分，涵盖了Sklearn中的决策树分类器。在本次研讨会中，我们将介绍什么是决策树、实现模型、可视化模型以及评估模型。一如既往，这篇博客文章是整个研讨会的总结，可以在这里找到<a class="ae ky" href="https://github.com/UCL-DSS/DecisionTree-classifiers-worksop" rel="noopener ugc nofollow" target="_blank">这里</a>我们还将讨论什么是分类器，从模型中提取特征重要性，以及如何通过网格搜索来改进模型。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="283a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您错过了我们之前的任何研讨会，您可以在这里找到:</p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/non-linear-regression-with-decision-trees-and-random-forest-afae406df27d"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">决策树和随机森林的非线性回归</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">UCL数据科学协会研讨会12b:决策树和随机森林在Python中的实现及性能…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/an-introduction-lasso-and-ridge-regression-using-scitkit-learn-d3427700679c"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">使用scitkit-learn介绍套索和岭回归</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">UCL数据科学学会12a研讨会:偏差-方差权衡，套索实施，山脊实施，及其…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mu l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/introduction-to-logistic-regression-predicting-diabetes-bc3a88f1e60e"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">逻辑回归简介:预测糖尿病</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">UCL数据科学学会研讨会11:什么是逻辑回归、数据探索、实施和评估</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mv l mq mr ms mo mt ks mf"/></div></div></a></div></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="c57c" class="mw mx it bd my mz na dn nb nc nd dp ne li nf ng nh lm ni nj nk lq nl nm nn no bi translated">什么是决策树</h2><p id="d6ad" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">决策树遵循树状结构(因此得名),其中节点代表特定属性，分支代表决策规则，叶节点代表结果。我们稍后将展示这个结构，这样你就能明白我们的意思，但你可以想象它就像你在高中数学中画的决策树之一，只是规模要复杂得多。该算法本身的工作原理是根据每个节点的不同属性分割数据，同时试图减少选择度量(通常是基尼指数)。本质上，决策树分类器的目标是根据属性分割数据，同时能够将数据准确地分类到预定义的组中(我们的目标变量)。</p><h2 id="d0a8" class="mw mx it bd my mz na dn nb nc nd dp ne li nf ng nh lm ni nj nk lq nl nm nn no bi translated">决策树实现</h2><p id="a7d6" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">对于这个决策树实现，我们将使用来自<code class="fe nu nv nw nx b">sklearn</code>的iris数据集，它相对容易理解并且易于实现。来自<code class="fe nu nv nw nx b">scikit-learn</code>的决策树分类器的好处是目标变量可以是分类的也可以是数字的。为了清楚起见，我们使用单个的花名作为实现的类别，这使得可视化和理解输入变得容易。</p><p id="8934" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了正确格式的数据，我们可以创建决策树，这样我们就可以尝试预测不同流的分类。从<code class="fe nu nv nw nx b">scikit-learn</code>导入必要的函数后，我们可以在限制树的范围的同时设置一个<code class="fe nu nv nw nx b">max_depth = 3</code>来限制过度拟合的可能性。这意味着这棵树沿着一个树枝最多可以做三次分裂。这可以在我们的训练数据集上实现，如下所示:</p><pre class="kj kk kl km gt ny nx nz oa aw ob bi"><span id="ed52" class="mw mx it nx b gy oc od l oe of">from sklearn.tree import DecisionTreeClassifier</span><span id="07a7" class="mw mx it nx b gy og od l oe of">clf = DecisionTreeClassifier(max_depth =3, random_state = 42)<br/>clf.fit(X_train, y_train)</span></pre><h2 id="0e37" class="mw mx it bd my mz na dn nb nc nd dp ne li nf ng nh lm ni nj nk lq nl nm nn no bi translated">可视化决策树</h2><p id="cc06" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">在某些情况下，当我们的实现并不复杂时，我们可能想要理解算法是如何运行的。这是使用决策树分类器的好处之一，因为当您的数据有限或实现有限时，我们可以实际看到树是如何形成的。这可以通过两种主要方式实现:</p><ol class=""><li id="cadf" class="oh oi it lb b lc ld lf lg li oj lm ok lq ol lu om on oo op bi translated"><strong class="lb iu">为树形图</strong></li></ol><pre class="kj kk kl km gt ny nx nz oa aw ob bi"><span id="33f8" class="mw mx it nx b gy oc od l oe of">#import relevant packages<br/>from sklearn import tree<br/>import matplotlib.pyplot as plt</span><span id="2103" class="mw mx it nx b gy og od l oe of">#plt the figure, setting a black background<br/>plt.figure(figsize=(30,10), facecolor ='k')<br/>#create the tree plot<br/>a = tree.plot_tree(clf,<br/>                   #use the feature names stored<br/>                   feature_names = feature_names,<br/>                   #use the class names stored<br/>                   class_names = labels,<br/>                   rounded = True,<br/>                   filled = True,<br/>                   fontsize=14)<br/>#show the plot<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/4b89799975f65382357c0777c48b19ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XD-RMMpiVbCgTLBXZp1XBA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="1edd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2) <strong class="lb iu">作为基于文本的图表</strong></p><pre class="kj kk kl km gt ny nx nz oa aw ob bi"><span id="6081" class="mw mx it nx b gy oc od l oe of">#import relevant functions<br/>from sklearn.tree import export_text</span><span id="1cad" class="mw mx it nx b gy og od l oe of">#export the decision rules<br/>tree_rules = export_text(clf,<br/>                        feature_names = list(feature_names))</span><span id="c848" class="mw mx it nx b gy og od l oe of">#print the result<br/>print(tree_rules)</span><span id="bc7f" class="mw mx it nx b gy og od l oe of">#out:<br/>|--- PetalLengthCm &lt;= 2.45<br/>|   |--- class: Iris-setosa<br/>|--- PetalLengthCm &gt;  2.45<br/>|   |--- PetalWidthCm &lt;= 1.75<br/>|   |   |--- PetalLengthCm &lt;= 5.35<br/>|   |   |   |--- class: Iris-versicolor<br/>|   |   |--- PetalLengthCm &gt;  5.35<br/>|   |   |   |--- class: Iris-virginica<br/>|   |--- PetalWidthCm &gt;  1.75<br/>|   |   |--- PetalLengthCm &lt;= 4.85<br/>|   |   |   |--- class: Iris-virginica<br/>|   |   |--- PetalLengthCm &gt;  4.85<br/>|   |   |   |--- class: Iris-virginica</span></pre><p id="f59b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从这里我们可以看到，第一次分割是基于花瓣长度，如果一朵花的花瓣长度小于2.45厘米，该算法会将其识别为Iris-Setosa(上图中的橙色框)。那些花瓣长度比这更长的花瓣则继续进一步分裂以获得更精确的分类。我们还可以看到<code class="fe nu nv nw nx b">max_depth</code>超参数的位置，因为沿着一个分支的最大分裂数是3，这意味着我们可以很容易地将结果可视化。</p><h2 id="fce0" class="mw mx it bd my mz na dn nb nc nd dp ne li nf ng nh lm ni nj nk lq nl nm nn no bi translated">决策树评估</h2><p id="285a" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">虽然我们可以看到模型如何根据训练数据进行训练，但我们最感兴趣的是模型如何处理看不见的数据(我们的测试数据集)。为此，我们可以对测试数据运行训练好的模型，看看它能预测什么。这可以通过以下方式实现:</p><pre class="kj kk kl km gt ny nx nz oa aw ob bi"><span id="54b6" class="mw mx it nx b gy oc od l oe of">test_pred_decision_tree = clf.predict(test_x)</span></pre><p id="31ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们可以看到模型在各种方面的表现。可视化这种性能(尤其是分类)的最佳方式之一是通过混淆矩阵。通过在一个轴上显示预测值，在另一个轴上显示实际值，我们可以直观地看到预测标签和真实标签是如何匹配的。这有助于确定我们在哪里可能得到假阳性或假阴性，从而确定算法的执行情况。</p><pre class="kj kk kl km gt ny nx nz oa aw ob bi"><span id="1714" class="mw mx it nx b gy oc od l oe of">#import the relevant packages<br/>from sklearn import metrics<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt#get the confusion matrix<br/>confusion_matrix = metrics.confusion_matrix(test_lab,  <br/>                                            test_pred_decision_tree)#turn this into a dataframe<br/>matrix_df = pd.DataFrame(confusion_matrix)#plot the result<br/>ax = plt.axes()<br/>sns.set(font_scale=1.3)<br/>plt.figure(figsize=(10,7))<br/>sns.heatmap(matrix_df, annot=True, fmt="g", ax=ax, cmap="magma")#set axis titles<br/>ax.set_title('Confusion Matrix - Decision Tree')<br/>ax.set_xlabel("Predicted label", fontsize =15)<br/>ax.set_xticklabels(['']+labels)<br/>ax.set_ylabel("True Label", fontsize=15)<br/>ax.set_yticklabels(list(labels), rotation = 0)plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/63261a85514a70cc4b577a8c301b4ada.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/0*67htrjpIaI42DkOC.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="8dcc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在这里可以看到，在测试数据集上，只有一个值未能从Iris-virginca类中预测出来，而算法建议这应该是Iris-versicolor。</p><p id="e5c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这很好，但是这到底意味着什么呢？当我们的数据集中有许多不同的类时，又该怎么办呢？为此，我们可以使用<code class="fe nu nv nw nx b">scikit-learn</code>库中的内置指标来提取分类报告，以告诉我们算法的执行情况。我们可以这样做:</p><pre class="kj kk kl km gt ny nx nz oa aw ob bi"><span id="1c5c" class="mw mx it nx b gy oc od l oe of">print(metrics.classification_report(test_lab,<br/>                                    test_pred_decision_tree))</span><span id="77dc" class="mw mx it nx b gy og od l oe of">#out: </span><span id="680c" class="mw mx it nx b gy og od l oe of">                precision    recall  f1-score   support<br/><br/>    Iris-setosa       1.00      1.00      1.00        23<br/>Iris-versicolor       0.95      1.00      0.97        19<br/> Iris-virginica       1.00      0.94      0.97        18<br/><br/>       accuracy                           0.98        60<br/>      macro avg       0.98      0.98      0.98        60<br/>   weighted avg       0.98      0.98      0.98        60</span></pre><p id="bc95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们可以看到每个组和整体的精度(我们预测某个类中有多少值在该类中)、召回率(每个类中有多少值被赋予了正确的标签)和f1得分(精度和召回率的加权平均值)的主要指标。您使用此模型的目标是什么，以及您的实现在哪里增加了价值，这将取决于您对误报还是漏报更感兴趣，您最重视哪个指标。</p><p id="6459" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，准确度分数是真阳性和真阴性在分配的标签总数中的分数:</p><blockquote class="os ot ou"><p id="1a2d" class="kz la ov lb b lc ld ju le lf lg jx lh ow lj lk ll ox ln lo lp oy lr ls lt lu im bi translated">总和(对角线)/总和(所有方框)</p></blockquote><p id="2f2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当数据中的目标变量类平衡良好时，这是一个很好的度量。</p><p id="0b9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">精度分数告诉我们，我们预测的某个类中有多少值实际上在该类中。这意味着它为我们提供了关于假阳性的信息(不是阳性但被标记为阳性的样本)。这被分配为:</p><blockquote class="os ot ou"><p id="711d" class="kz la ov lb b lc ld ju le lf lg jx lh ow lj lk ll ox ln lo lp oy lr ls lt lu im bi translated">真阳性(对角线上的数字)/所有阳性(列和)</p></blockquote><p id="1e8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每堂课。当您担心错误预测该类中的某个值时，这很有用。</p><p id="2768" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Recall告诉我们每个类中有多少值被分配了正确的标签。这给了我们关于假阴性的信息。对于任何类别，这等于:</p><blockquote class="os ot ou"><p id="afd8" class="kz la ov lb b lc ld ju le lf lg jx lh ow lj lk ll ox ln lo lp oy lr ls lt lu im bi translated">真正数(对角线上的数字)/所有赋值(行和)</p></blockquote><p id="eb0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并且在假阴性成本高的情况下是有用的度量，例如在癌症筛查中。</p><p id="a800" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，F1分数是精度和召回率的加权平均值，1为最好，0为最差。这使用调和平均值，而不是算术平均值，因此该值更接近较小的数字。这防止了在两个参数中的一个非常高而另一个很低的情况下高估模型的性能。这在您不太关心精确度或召回率，但关心每个类的性能的情况下很有用。</p><h2 id="25cc" class="mw mx it bd my mz na dn nb nc nd dp ne li nf ng nh lm ni nj nk lq nl nm nn no bi translated">为什么要使用决策树？</h2><p id="9fd4" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">使用决策树的优势在于:</p><ul class=""><li id="2e02" class="oh oi it lb b lc ld lf lg li oj lm ok lq ol lu oz on oo op bi translated">它们很容易解释(取决于数据的大小和树的深度)</li><li id="98d9" class="oh oi it lb b lc pa lf pb li pc lm pd lq pe lu oz on oo op bi translated">他们可以通过<code class="fe nu nv nw nx b">scikit-learn</code>处理数字和分类数据</li><li id="2e86" class="oh oi it lb b lc pa lf pb li pc lm pd lq pe lu oz on oo op bi translated">它们可以限制模型中不良预测的影响</li><li id="f1c3" class="oh oi it lb b lc pa lf pb li pc lm pd lq pe lu oz on oo op bi translated">您可以提取它们的结构，以便能够可视化它们是如何工作的</li></ul><p id="8579" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，使用决策树也有缺点或不足之处，它们是:</p><ul class=""><li id="3a01" class="oh oi it lb b lc ld lf lg li oj lm ok lq ol lu oz on oo op bi translated">当一个阶级在我们的目标预测器中占优势时，他们会挣扎</li><li id="a890" class="oh oi it lb b lc pa lf pb li pc lm pd lq pe lu oz on oo op bi translated">当允许数据无休止地增长时，它们可能会使你的数据溢出</li><li id="a671" class="oh oi it lb b lc pa lf pb li pc lm pd lq pe lu oz on oo op bi translated">数据的微小变化可能会产生非常不同的结果</li></ul><p id="dc86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些只是在您实现决策树时需要记住的事情，因为它们在实践中非常有用，并且可以与其他分类算法(如k-最近邻或随机森林)一起使用，以帮助做出关于分类的决策。</p><p id="a6cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，您可以轻松地用Python实现和评估决策树。在实践本身中，提供了关于分类器、评估指标、特征重要性以及如何通过调整超参数来改进模型的更多信息，这些信息都可以在<a class="ae ky" href="https://github.com/UCL-DSS/DecisionTree-classifiers-worksop" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="247a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您想了解我们协会的更多信息，请随时关注我们的社交网站:</p><p id="a976" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">脸书:<a class="ae ky" href="https://www.facebook.com/ucldata" rel="noopener ugc nofollow" target="_blank">https://www.facebook.com/ucldata</a></p><p id="a7be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">insta gram:<a class="ae ky" href="https://www.instagram.com/ucl.datasci/" rel="noopener ugc nofollow" target="_blank">https://www.instagram.com/ucl.datasci/</a></p><p id="6dea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">领英:<a class="ae ky" href="https://www.linkedin.com/company/ucldata/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/company/ucldata/</a></p><p id="1f5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想了解来自UCL数据科学协会和其他优秀作者的最新消息，请随时使用我下面的推荐代码注册medium。</p><div class="mc md gp gr me mf"><a href="https://philip-wilkinson.medium.com/membership" rel="noopener follow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">通过我的推荐链接加入媒体-菲利普·威尔金森</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">philip-wilkinson.medium.com</p></div></div><div class="mo l"><div class="pf l mq mr ms mo mt ks mf"/></div></div></a></div></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="f6ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我的其他媒体文章可在以下网址找到:</p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/git-and-github-basics-for-data-scientists-b9fd96f8a02a"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">面向数据科学家的Git和GitHub基础知识</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">UCL数据科学研讨会8:什么是Git，创建本地存储库，提交第一批文件，链接到远程…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="pg l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/easy-grouped-bar-charts-in-python-b6161cdd563d"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">Python中的简易分组条形图</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">如何创建每个条目有两个、三个或更多条的条形图</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="ph l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/an-introduction-to-object-oriented-programming-for-data-scientists-879106d90d89"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">面向数据科学家的面向对象编程介绍</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">面向对象的基础知识，适合那些以前可能没有接触过这个概念或者想知道更多的人</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="pi l mq mr ms mo mt ks mf"/></div></div></a></div></div></div>    
</body>
</html>