<html>
<head>
<title>Essential Explainable AI Python frameworks that you should know about</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你应该知道的基本的可解释的人工智能Python框架</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/essential-explainable-ai-python-frameworks-that-you-should-know-about-84d5063b75e9#2022-11-09">https://towardsdatascience.com/essential-explainable-ai-python-frameworks-that-you-should-know-about-84d5063b75e9#2022-11-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="732b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">实践中应用可解释人工智能的9大Python框架</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/200eade63e54a367646ff5c1192edee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KICJSG6-PcS_RzeU"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:<a class="ae ky" href="https://unsplash.com/photos/zwd435-ewb4" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="e535" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://amzn.to/3cY4c2h" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">可解释的人工智能</strong> </a> <strong class="lb iu"> </strong>是确保AI和ML解决方案透明、可信、负责和符合道德的最有效实践，以便有效地解决所有关于算法透明度、风险缓解和后备计划的监管要求。AI和ML可解释性技术为这些算法在其解决方案生命周期的每个阶段如何运行提供了必要的可见性，允许最终用户理解<em class="lv">为什么</em>和<em class="lv">查询如何与AI和ML模型的结果相关。在本文中，我们将介绍成为应用XAI专家需要学习的九大XAI python框架。</em></p><h1 id="d0b3" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">什么是可解释的人工智能(XAI)？</h1><blockquote class="mo mp mq"><p id="ca60" class="kz la lv lb b lc ld ju le lf lg jx lh mr lj lk ll ms ln lo lp mt lr ls lt lu im bi translated"><a class="ae ky" href="https://amzn.to/3cY4c2h" rel="noopener ugc nofollow" target="_blank"> <em class="it">【可解释的人工智能(XAI) </em> </a> <em class="it">是解释复杂“黑箱”人工智能模型的功能的方法，这些模型证明了这些模型产生的预测背后的原因。</em></p></blockquote><p id="ba4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想要一个45分钟的简短视频来简要介绍XAI，那么你可以观看我在2021年APAC举行的<strong class="lb iu">人工智能加速器节上发表的关于XAI的演讲:</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">可解释的人工智能:使ML和DL模型更易解释(作者谈)</p></figure><p id="3be9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你也可以浏览我的书<a class="ae ky" href="https://amzn.to/3cY4c2h" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">应用机器学习可解释技术</strong> </a> <strong class="lb iu"> </strong>并看看<a class="ae ky" href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/" rel="noopener ugc nofollow" target="_blank">代码库</a>来获得这些XAI方法的实践经验。在本文中，我将参考我的书<a class="ae ky" href="https://amzn.to/3cY4c2h" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">应用机器学习可解释技术</strong> </a> <strong class="lb iu">中讨论的一些XAI框架。</strong></p><div class="mw mx gp gr my mz"><a href="https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&amp;pd_rd_w=Wr6SJ&amp;content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&amp;pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&amp;pf_rd_r=6P2PM599T97MRG7NZD9J&amp;pd_rd_wg=m4qUW&amp;pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&amp;linkCode=li3&amp;tag=adib0073-20&amp;linkId=35506e1847de5c011fc57aa66c2b1d8e&amp;language=en_US&amp;ref_=as_li_ss_il" rel="noopener  ugc nofollow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd iu gy z fp ne fr fs nf fu fw is bi translated">应用机器学习可解释技术:使ML模型可解释和可信…</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">应用机器学习可解释技术:使ML模型可解释和可信赖的实践…</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">www.amazon.com</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn ks mz"/></div></div></a></div><p id="9b4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想要这本书的详细反馈，下面的视频可能对你有用:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="5c36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我提供我对9大XAI python框架的建议:</p><h1 id="b8b4" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak"> 1。本地可解释的模型不可知的解释</strong> ( <strong class="ak">石灰</strong>)</h1><p id="2936" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">LIME是一种新颖的、模型不可知的局部解释技术，用于通过学习预测周围的局部模型来解释黑盒模型。LIME提供了对模型的直观的全局理解，这对于非专家用户也是有帮助的。这项技术最早是在研究论文<em class="lv">“我为什么要相信你？”解释任何分类器</em>的预测<em class="lv">里贝罗等人</em>。(https://arxiv . org/ABS/1602.04938)。该算法通过使用近似的局部可解释模型，以忠实的方式很好地解释了任何分类器或回归器。它为任何黑箱模型提供了建立信任的全局视角；因此，它允许您在人类可解释的表示上识别可解释的模型，这在局部上忠实于算法。因此，它主要通过<em class="lv">学习可解释数据表示</em>、<em class="lv">在保真度-可解释度权衡中保持平衡</em>和<em class="lv">搜索局部探索</em>来发挥作用。</p><p id="30d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">GitHub</strong>:<a class="ae ky" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank">https://github.com/marcotcr/lime</a></p><p id="51ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">安装</strong> : `pip安装石灰'</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h1 id="4869" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">2.<strong class="ak">沙普利添加剂解说</strong> ( <strong class="ak"> SHAP </strong>)</h1><p id="e26f" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">2017年<em class="lv">，Scott Lundberg </em>和<em class="lv"> Su-In Lee </em>首先从他们的论文<em class="lv">中介绍了SHAP框架，一种解释模型预测的统一方法</em>(<a class="ae ky" href="https://arxiv.org/abs/1705.07874" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1705.07874</a>)。这个框架背后的基本思想是基于合作博弈理论中的Shapley值的概念。SHAP算法考虑<em class="lv">附加特征重要性</em>来解释底层模型特征的集体贡献。数学上，Shapley值被定义为<em class="lv">在特征空间</em>中所有可能的值范围内单个特征值的平均边际贡献。但是对沙普利值的数学理解是相当复杂的，但是在罗伊德·S·沙普利的研究论文《n人游戏的数值》中有很好的解释对博弈论的贡献2.28 (1953) 。</p><p id="0453" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">GitHub</strong>:【https://github.com/slundberg/shap】T42</p><p id="bdb9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">安装</strong> : `pip安装形状'</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/c27ffea5dea3e65250e6770907f53989.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KvYXb4mi62NriZrBw6h2QA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:<a class="ae ky" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"> GitHub </a></p></figure><h1 id="73f7" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak"> 3。用概念激活向量(TCAV)进行测试</strong></h1><p id="60e5" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated"><strong class="lb iu"> TCAV </strong>是来自谷歌人工智能的模型可解释性框架，它在实践中实现了基于概念的解释方法的思想。该算法依赖于<strong class="lb iu">概念激活向量(CAV) </strong>，它使用人类友好的概念提供对ML模型内部状态的解释。从更专业的角度来说，TCAV使用方向导数来量化对人类友好的高级概念对模型预测的重要性。例如，在描述发型时，TCAV可以使用诸如<em class="lv">卷发</em>、<em class="lv">直发</em>或<em class="lv">发色</em>等概念。这些用户定义的概念不是算法在训练过程中使用的数据集的输入要素。</p><p id="f7b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">GitHub</strong>:<a class="ae ky" href="https://github.com/tensorflow/tcav" rel="noopener ugc nofollow" target="_blank">https://github.com/tensorflow/tcav</a></p><p id="7963" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">安装</strong> : `pip安装tcav '</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/a634c7c3350512e37b85a0bea1584819.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QUXcZxQJ-eUQwi5zcg-O5Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TCAV帮助我们通过神经网络解决了用户定义的图像分类概念重要性的关键问题</p></figure><h1 id="e1d3" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak"> 4。用于探索和解释的模型不可知语言(DALEX) </strong></h1><p id="3c85" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">DALEX  ( <strong class="lb iu">探索和解释的模型不可知语言</strong>)是极少数被广泛使用的XAI框架之一，它试图解决可解释性的大部分维度。DALEX是模型不可知的，可以提供一些关于底层数据集的元数据，为解释提供一些上下文。这个框架为您提供了对模型性能和模型公平性的洞察，并且它还提供了全局和局部模型的可解释性。</p><p id="4921" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DALEX框架的开发人员希望遵守以下要求列表，他们定义这些要求是为了解释复杂的黑盒算法:</p><ul class=""><li id="2308" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated"><strong class="lb iu">预测的理由</strong>:DALEX的开发者认为，ML模型用户应该能够理解最终预测的变量或特征属性。</li><li id="69ed" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated"><strong class="lb iu">预测的推测</strong>:假设假设情景或了解数据集的特定特征对模型结果的敏感性是DALEX开发人员考虑的其他因素。</li><li id="6508" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated"><strong class="lb iu">预测的验证</strong>:对于一个模型的每一个预测结果，用户应该能够验证证实该模型特定预测的证据的强度。</li></ul><p id="2449" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">GitHub</strong>:<a class="ae ky" href="https://github.com/ModelOriented/DALEX" rel="noopener ugc nofollow" target="_blank">https://github.com/ModelOriented/DALEX</a></p><p id="6b96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">安装</strong> : `pip安装dalex -U '</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/a7cb590b3f29840c8789b3f61d2e83bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fdxOFcCb5NsPJkhG.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">DALEX可解释性金字塔(来源:<a class="ae ky" href="https://github.com/ModelOriented/DALEX/blob/master/LICENSE" rel="noopener ugc nofollow" target="_blank"> GitHub DALEX项目</a></p></figure><h1 id="1133" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">5.解释器仪表板</h1><p id="21f1" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">该框架允许定制仪表板，但我认为默认版本包括了模型可解释性的所有支持方面。生成的基于web应用程序的仪表板可以直接从实时仪表板导出为静态网页。否则，仪表板可以通过自动化的<strong class="lb iu">持续集成</strong> ( <strong class="lb iu"> CI </strong> )/ <strong class="lb iu">持续部署</strong> ( <strong class="lb iu"> CD </strong>)部署过程以编程方式部署为web app。我建议你浏览一下这个框架的官方文档(<a class="ae ky" href="https://explainerdashboard.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">https://explainerdashboard.readthedocs.io/en/latest/</a>)。</p><p id="dd80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">GitHub</strong>:<a class="ae ky" href="https://github.com/oegedijk/explainerdashboard" rel="noopener ugc nofollow" target="_blank">https://github.com/oegedijk/explainerdashboard</a></p><p id="f711" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">安装</strong> : `pip安装说明仪表板'</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/176a80dcea9c9ede505f8f2ab6597b75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*p7rdehDZ8hvhyaDZ.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源— <a class="ae ky" href="https://github.com/oegedijk/explainerdashboard/blob/master/LICENSE.txt" rel="noopener ugc nofollow" target="_blank">讲解dashboard GitHub项目</a></p></figure><h1 id="6fcf" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">6.解释性语言</h1><p id="e6a2" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">interpret ml(https://interpret.ml/)是微软的XAI工具包。它旨在为ML模型的调试、结果解释和监管审计提供对ML模型的全面理解。有了这个Python模块，我们可以训练<em class="lv">可解释的玻璃盒子模型</em>或者<em class="lv">解释黑盒模型</em>。</p><p id="09f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">微软研究院开发了另一种叫做<strong class="lb iu">可解释Boosting Machine</strong>(<strong class="lb iu">EBM</strong>)的算法，将Boosting、bagging、自动交互检测等现代ML技术引入到<strong class="lb iu">广义可加模型</strong> ( <strong class="lb iu"> GAMs </strong>)等经典算法中。研究人员还发现，EBM作为随机森林和梯度增强树是准确的，但与这种黑盒模型不同，EBM是可解释的和透明的。因此，EBM是内置于InterpretML框架中的玻璃盒子模型。</p><p id="6c88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">GitHub</strong>:<a class="ae ky" href="https://github.com/interpretml/interpret" rel="noopener ugc nofollow" target="_blank">https://github.com/interpretml/interpret</a></p><p id="36fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">安装</strong> : `pip安装解释'</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/16f896b9d055739a5481abd423b363ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vzYwOnvMz7zYpoAh.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源— <a class="ae ky" href="https://github.com/interpretml/interpret/blob/develop/LICENSE" rel="noopener ugc nofollow" target="_blank"> InterpretML GitHub项目</a></p></figure><h1 id="9e93" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak"> 7。不在场证明</strong></h1><p id="5385" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated"><a class="ae ky" href="https://docs.seldon.io/projects/alibi" rel="noopener ugc nofollow" target="_blank"> Alibi </a>是另一个开源的Python库，旨在机器学习模型检验和解释。该库的重点是为分类和回归模型提供黑盒、白盒、局部和全局解释方法的高质量实现。</p><p id="1685" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">GitHub</strong>:【https://github.com/SeldonIO/alibi】T4</p><p id="e547" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">安装</strong> : `pip安装alibi '</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/4e696b21546b77d10ee11f7faf69866d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UkHnEEE6nq-zfik3.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://github.com/SeldonIO/alibi/blob/master/LICENSE" rel="noopener ugc nofollow" target="_blank"> ALIBI GitHub项目</a></p></figure><h1 id="461e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">8.不同的反事实解释(DiCE)</h1><p id="719f" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated"><strong class="lb iu">多样的反事实解释</strong> ( <strong class="lb iu">骰子</strong>)是另一个流行的XAI框架，特别是对于反事实解释。有趣的是，DiCE也是微软研究院的关键XAI框架之一，但是它还没有与InterpretML模块集成(我想知道为什么！).我发现反事实解释的整个想法非常接近于给出可操作建议的理想的人性化解释。这篇来自微软的博客讨论了DiCE框架背后的动机和想法:<a class="ae ky" href="https://www.microsoft.com/en-us/research/blog/open-source-library-provides-explanation-for-machine-learning-through-diverse-counterfactuals/" rel="noopener ugc nofollow" target="_blank">https://www . Microsoft . com/en-us/research/blog/open-source-library-provide-explain-for-machine-learning-through-diversity-counter factuals/</a>。与ALIBI CFE相比，我发现DiCE能够以最小的超参数调整产生更合适的CFE。这就是为什么我觉得提到骰子很重要，因为它主要是为基于示例的解释而设计的。</p><p id="2db8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">GitHub</strong>:<a class="ae ky" href="https://github.com/interpretml/DiCE" rel="noopener ugc nofollow" target="_blank">https://github.com/interpretml/DiCE</a></p><p id="82c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">安装</strong> : `pip安装骰子-ml '</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/505c6eebd299a308bd91a61cfc698fa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ITvwCLEBV19QnaCs"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://github.com/interpretml/DiCE/blob/main/LICENSE" rel="noopener ugc nofollow" target="_blank"> DiCE GitHub项目</a></p></figure><h1 id="c50c" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">9.像我五岁一样解释(ELI5)</h1><p id="d99f" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated"><em class="lv"> ELI5 </em>，或<em class="lv"> Explain Like I'm Five </em>，是一个用于调试、检查和解释ML分类器的Python XAI库。它是最初开发的XAI框架之一，以最简化的格式解释黑盒模型。它支持广泛的ML建模框架，如scikit-learn兼容模型、Keras等。它还集成了LIME解释器，可以处理表格数据集以及文本和图像等非结构化数据。https://eli5.readthedocs.io/en/latest/的<a class="ae ky" href="https://eli5.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">提供了库文档，https://github.com/eli5-org/eli5</a><a class="ae ky" href="https://github.com/eli5-org/eli5" rel="noopener ugc nofollow" target="_blank">的</a>提供了GitHub项目。</p><p id="70bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">GitHub</strong>:<a class="ae ky" href="https://github.com/TeamHG-Memex/eli5" rel="noopener ugc nofollow" target="_blank">https://github.com/TeamHG-Memex/eli5</a></p><p id="e814" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">安装</strong> : `pip安装eli5 '</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/a29dafc9fa4c09cca5437da01286dccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/0*97Wcq9lby5ruLPqh.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://github.com/TeamHG-Memex/eli5/blob/master/LICENSE.txt" rel="noopener ugc nofollow" target="_blank"> ELI5 GitHub项目</a></p></figure><h1 id="5364" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">摘要</h1><p id="aa8d" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">本文中介绍的框架是我在模型可解释性方面的首选库。然而，我不建议盲目地应用这些框架，因为你需要对问题和目标受众有一个透彻的理解，以便对人工智能模型进行适当的解释。我推荐阅读这本书:<strong class="lb iu"/><a class="ae ky" href="https://amzn.to/3cY4c2h" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">应用机器学习可解释技术</strong></a><strong class="lb iu"/>并探索<a class="ae ky" href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>以获得实际操作的代码示例。</p><h1 id="dc71" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">作者关于TDS的其他XAI相关文章:</h1><ol class=""><li id="bb64" class="nv nw it lb b lc no lf np li oo lm op lq oq lu or ob oc od bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/explainable-machine-learning-for-models-trained-on-text-data-combining-shap-with-transformer-5095ea7f3a8">对基于文本数据训练的模型的可解释机器学习:将SHAP与变压器模型相结合</a></li><li id="f51f" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu or ob oc od bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/euca-an-effective-xai-framework-to-bring-artificial-intelligence-closer-to-end-users-74bb0136ffb1">EUCA——一个有效的XAI框架，让人工智能更贴近终端用户</a></li><li id="7b4b" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu or ob oc od bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/understand-the-working-of-shap-based-on-shapley-values-used-in-xai-in-the-most-simple-way-d61e4947aa4e">理解可解释人工智能中使用的SHAP和沙普利值的工作原理</a></li><li id="6a4b" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu or ob oc od bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/how-to-explain-image-classifiers-using-lime-e364097335b4">如何用石灰解释图像分类器</a></li></ol><div class="mw mx gp gr my mz"><a href="https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&amp;pd_rd_w=Wr6SJ&amp;content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&amp;pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&amp;pf_rd_r=6P2PM599T97MRG7NZD9J&amp;pd_rd_wg=m4qUW&amp;pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&amp;linkCode=li3&amp;tag=adib0073-20&amp;linkId=35506e1847de5c011fc57aa66c2b1d8e&amp;language=en_US&amp;ref_=as_li_ss_il" rel="noopener  ugc nofollow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd iu gy z fp ne fr fs nf fu fw is bi translated">应用机器学习可解释技术:使ML模型可解释和可信…</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">应用机器学习可解释技术:使ML模型可解释和可信赖的实践…</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">www.amazon.com</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn ks mz"/></div></div></a></div><h1 id="4fd9" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">参考</h1><ol class=""><li id="4fbf" class="nv nw it lb b lc no lf np li oo lm op lq oq lu or ob oc od bi translated"><a class="ae ky" href="https://amzn.to/3cY4c2h" rel="noopener ugc nofollow" target="_blank">应用机器学习解释技术</a></li><li id="3581" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu or ob oc od bi translated">GitHub repo自《应用机器学习可解释技术》——<a class="ae ky" href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/packt publishing/Applied-Machine-Learning-explability-Techniques/</a></li></ol></div></div>    
</body>
</html>