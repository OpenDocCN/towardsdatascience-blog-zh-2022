<html>
<head>
<title>Why SHAP values might not be perfect</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么SHAP价值观可能不完美</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-shap-values-might-not-be-perfect-cbc8056056be#2022-08-26">https://towardsdatascience.com/why-shap-values-might-not-be-perfect-cbc8056056be#2022-08-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a611" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">SHAP价值观弱点的两个例子和可能的解决方案概述</h2></div><p id="7d61" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">SHAP值似乎消除了机器学习模型的复杂性和解释难度之间的权衡，鼓励研究人员和数据科学家设计算法，而不用担心如何理解任何黑盒给出的预测。但是SHAP能解释所有的财产吗？</p><p id="a55f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我们将通过一些例子来讨论SHAP价值观的一个重要弱点。可能的解决方案概述也将很快提交。</p><h1 id="7566" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">SHAP价值观及其局限性</h1><p id="d8b3" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">SHAP值基于合作博弈理论，通过在特征之间公平分配“支出”来量化导致预测的特征之间的交互。</p><p id="9b58" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，Frye等人[1]认为，SHAP值有一个明显的局限性，因为它们忽略了数据中的所有因果结构。更准确地说，这种框架在模型解释中将所有特征放在同等的地位上，要求属性平等地分布在信息相同的特征上。</p><p id="74bb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，考虑一个将资历和工资作为输入并预测此人是否能从银行获得贷款的模型。SHAP看重的一个形象告诉我们，他的资历和薪水都有很大的SHAP价值，这有助于他成功申请。然而，他的工资主要来自他的资历，在这种情况下，没有理由因为他的资历而给他的工资分配一个大的SHAP值。</p><p id="9b7f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">SHAP值框架中因果结构的缺失可能是严重的，在某些用例中会导致错误的决策，因为考虑用大SHAP值改进某个特征，而这些值可能只是其他特征的结果。回到贷款的例子，形象SHAP告诉一个人，你没有得到贷款，因为你的工资低，他引诱你换一份工资更高的工作。然而，申请仍然可能被拒绝，因为新工作会导致资历变浅。</p><p id="4650" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将在接下来的章节中提供一些更详细的例子。</p><h1 id="eb9c" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">熊猫图像示例</h1><p id="5f03" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">本节考虑一个用于图像分类的预训练深度学习模型的示例，Inception V1，具有<a class="ae ly" href="https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt" rel="noopener ugc nofollow" target="_blank"> 1001个类别标签</a>，包括大熊猫、树蛙等。在<a class="ae ly" href="https://tfhub.dev/google/imagenet/inception_v1/classification/5" rel="noopener ugc nofollow" target="_blank">tensor flow hub</a>上可用。详细的实现在<a class="ae ly" href="https://colab.research.google.com/drive/1sqdDEkgzZk4Q0LfVn8ysW6xwxgqcESvb#scrollTo=f2P3QSDSeRK1" rel="noopener ugc nofollow" target="_blank">笔记本</a>中提供。</p><p id="94ad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下实验考虑包含5幅大熊猫图像的以下集合，每幅图像被预处理为大小为(500，500，3)范围从0到255的数组，并且我们希望计算由初始V1模型给出的图像的前三分之一可能类别的SHAP值，以了解哪些像素对最终分类结果的贡献更大。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi lz"><img src="../Images/109d6d522a0d490b2e8a8a41be2e9431.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fmot0poi37YvNtt9f7dP6Q.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">作者图片:一组熊猫图片</p></figure><p id="4174" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为此，我们编写了下面几行代码:</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mp mq l"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">计算SHAP值</p></figure><p id="6b6a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">前两行除了重新缩放图像数组以适应模型的输入之外什么都不做。我们使用与输入图像大小相同的遮罩作为SHAP值计算的背景。<em class="mr"> image_plot </em>函数给出了结果的直观图示:</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/c177de78f2e93218a16cd64780ee3442.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*oWN7CEWB4v89_ub2NIr5-g.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">图片作者:SHAP价值观的大熊猫形象</p></figure><p id="9ebb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">深入查看结果，尤其是最后两张图像，我们立即注意到，熊猫身体中的像素具有较大的SHAP值，即我们标记了一个圆的位置:</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/b551b02eabfd745eb548059e782626a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*hnoyWbi1cEiRohIF5Tdhog.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">具有与熊猫无关的大SHAP值的像素</p></figure><p id="b092" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种归因可能有一个直接的解释:注意到这些像素形成了大熊猫最喜欢的竹子，我们认为它们的频繁存在带来了这种有偏见的重要性。如果我们知道熊猫的存在是竹子的因果祖先，那么将竹子的重要性归因于熊猫身体当然更有意义，而不幸的是，SHAP值无法通过假设所有特征都是独立的来捕捉如此重要的信息。</p><h1 id="2838" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">可能的解决方案</h1><p id="800e" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">好消息是，研究人员已经注意到，在当前的SHAP价值观框架中，这种因果结构并不存在。这里我们给出了两种方法:非对称SHAP值[1]和偶然SHAP值[2]。</p><p id="db16" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">非对称SHAP值(asv)主要消除了当前SHAP值框架的对称性，即如果两个特征值对所有可能的联盟的贡献相等，则它们的贡献应该相同。相比之下，当将所有“收益”加在一起时，引入了包含特征因果关系的权重作为概率度量。这样的设计更强调从根本原因方面的解释，而不是对直接原因的解释。</p><p id="787a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一方面，Heskes等人[2]指出<em class="mr">“不需要借助非对称的Shapley值来整合因果知识”</em>，并提出了一个因果SHAP值(CSVs)框架。该定义只不过是通过用Pearl[3]的微积分定义价值函数来合并干预。这种设计确实将一个特征的重要性分解为两部分:直接和间接，并且通过考虑这两种不同类型对最终预测的影响，将导致更好的决策。</p><p id="751a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我邀请读者关注我以后的文章，以获得关于CSV的具体示例的更详细的解释和实现。</p><h1 id="64dc" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">参考</h1><p id="43b6" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">[1] C. Frye、C. Rowat和I .格非，“不对称的shapley值:将因果知识纳入模型不可知的可解释性”，载于<em class="mr">神经信息处理系统进展33:神经信息处理系统2020年年度会议，NeurIPS 2020，2020年12月6-12日，virtual </em>，H. Larochelle，M. Ranzato，R. Hadsell，M. Balcan和H. Lin编辑。, 2020.</p><p id="9c47" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] D. Janzing，L. Minorics和P. Blöbaum，“可解释人工智能中的特征相关性量化:一个因果问题”，载于<em class="mr">第23届人工智能和统计国际会议，AISTATS 2020，2020年8月26-28日，在线【意大利西西里岛巴勒莫】</em>，ser。机器学习研究会议录。，第108卷。</p><p id="a372" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3]Do-Calculus重温朱迪亚珍珠主题演讲，2012年8月17日，UAI-2012年会议，加利福尼亚州卡特琳娜【https://ftp.cs.ucla.edu/pub/stat_ser/r402.pdf T4】</p></div></div>    
</body>
</html>