<html>
<head>
<title>Semi-Supervised Approach for Transformers [Part 2]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变压器的半监督方法(二)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/semi-supervised-approach-for-transformers-part-2-8f8bf6edd912#2022-09-07">https://towardsdatascience.com/semi-supervised-approach-for-transformers-part-2-8f8bf6edd912#2022-09-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7744" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用于微调罗伯塔模型和训练自然语言推理分类任务的连体网络结构</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7fe7d6dc9dcbfaa8b16fbf33b1a9deca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4_1ZfBDO1MdORxAXTmPmMw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">法耶·康尼什在<a class="ae kv" href="https://unsplash.com/s/photos/reflection?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="d3c0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文将使用暹罗架构来微调RoBERTa模型。我们还将在微调和训练数据集时，使用主成分分析和TSNE来观察数据集的嵌入分布。训练将在用于自然语言推理分类的微调RoBERTa模型(SNLI的子集)上进行。</p><p id="3ebd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://github.com/divyanshuraj6815/END-NLP/blob/main/Experiment%2022/Semi_Supervised_NLI_(Small)_Training_(Siamese).ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">为本文代号</strong> </a>。要理解变形金刚半监督方法背后的直觉，请阅读<a class="ae kv" rel="noopener" target="_blank" href="/semi-supervised-approach-for-transformers-38722f1b5ce3"> <strong class="ky ir">第1部分</strong> </a>。</p><h2 id="97bd" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">资料组</h2><p id="43c5" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated"><a class="ae kv" href="https://huggingface.co/datasets/snli" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> SNLI </strong> </a>数据集的子集用于训练。<strong class="ky ir">20000个训练样本</strong>，10000个验证样本，10000个测试样本。NLI(自然语言推理)数据集有一个前提和一个假设。该模型的目标是预测假设是否与前提相矛盾、蕴涵或中立。</p><p id="2aaf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">原始数据集有大约550，000个训练样本。在全套设备上做这个实验会得到更好的结果。</p><p id="1910" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们来看一下数据集分析:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/ffdcee16d3122bb94e1dd960ebc30d7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XnVK3nN7tI-losv4alh55g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据集分析(图片由作者提供)</p></figure><p id="faa8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以从上图中观察到数据集是平衡的。将前提与假设串联后，最大句子长度可估计为<strong class="ky ir"> 64个单词</strong>。</p><h2 id="da27" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">模型</h2><p id="125c" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">为此，我们将使用两种型号。第一个是暹罗建筑微调罗伯塔。第二个模型将有一个分类头来预测句子之间是包含、矛盾还是中立。</p><p id="9539" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">句子变压器型号:</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">为RoBERTa创建句子转换器模型的代码片段(作者)</p></figure><p id="6146" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该模型的输出是768特征或上下文向量。我们将同时计算前提和假设的嵌入，并使用<strong class="ky ir">余弦相似性损失</strong>来微调模型。</p><ul class=""><li id="436d" class="mt mu iq ky b kz la lc ld lf mv lj mw ln mx lr my mz na nb bi translated">-1表示矛盾</li><li id="0ea4" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">0代表空档</li><li id="25c6" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">1用于限定</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/1196daa560e1fe7aa49eca3927615062.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*gCf4TbP6OlkNySHjESptMQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">暹罗建筑(图片来自<a class="ae kv" href="https://sbert.net/examples/training/sts/README.html" rel="noopener ugc nofollow" target="_blank">sbert.net</a></p></figure><p id="f859" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上图代表了架构。微调完成后，RoBERTa模型的权重将被保存以供后续任务使用。</p><p id="ac78" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">序列分类自动模型:</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">为<strong class="ak">序列分类</strong>加载微调过的RoBERTa的代码片段(作者)</p></figure><p id="61cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在用same体系结构微调模型后，我们将使用相同的RoBERTa权重来定义序列分类模型。在这个模型中，新的权重将只分配给最后一个“分类器”层。这将在具有循环学习率和Adam优化器的训练数据上进一步训练。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/ea9390a2756dd3ce8fbca6775ac5a6c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y_PDqPdlaRk-b3BtPInBlQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nj">学习率余弦调度器</strong>(图片来自<a class="ae kv" href="https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.get_cosine_with_hard_restarts_schedule_with_warmup" rel="noopener ugc nofollow" target="_blank">拥抱脸</a>)</p></figure><p id="ed36" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">具有2个周期的余弦调度器的学习速率2e-5用于训练。这是为了避免过度拟合。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/15e75ec93fa176f06135620d943450ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KSbx77Vx4OlrYr8RqJRKFQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">损失图(<strong class="bd nj">左</strong>)和准确度图(<strong class="bd nj">右</strong>)(图片由作者提供)</p></figure><p id="aeae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上图显示了序列分类的损失和准确度曲线。达到了87%的测试精度。</p><blockquote class="nl nm nn"><p id="6f51" class="kw kx no ky b kz la jr lb lc ld ju le np lg lh li nq lk ll lm nr lo lp lq lr ij bi translated">训练结束后，我们会节省重量。这些权重可以再次加载到句子转换器模型中，并训练几个时期以获得更好的句子嵌入。</p></blockquote><h2 id="9089" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">句子嵌入分析</h2><p id="052a" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们已经对模型进行了三次训练:</p><ul class=""><li id="dcf6" class="mt mu iq ky b kz la lc ld lf mv lj mw ln mx lr my mz na nb bi translated">将原来的罗伯塔重量加载到句子转换器中，以使用暹罗架构进行训练。我们可以保存这些微调过的权重。</li><li id="73ec" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">使用RoBERTa将上述训练中微调的权重加载到序列分类模型中。训练后，我们保存这些重量。</li><li id="5596" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">将上述步骤中的权重加载到句子转换器中，用连体结构训练几个时期，然后保存权重。</li></ul><p id="c521" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">训练集中所有句子对的句子嵌入被保存3次:</p><ul class=""><li id="ed3c" class="mt mu iq ky b kz la lc ld lf mv lj mw ln mx lr my mz na nb bi translated">原始罗伯塔重量。</li><li id="ca11" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">用连体建筑微调后。</li><li id="1dc4" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">经过序列分类和微调再次与暹罗架构。</li></ul><p id="f814" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这3个版本的句子嵌入，我们将运行主成分分析和TSNE拟合来分析数据集微调的效果。下图显示了分析结果:</p><p id="62a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">图1: </strong>我们对句子嵌入的所有3个实例运行n_components = 3的PCA，并绘制二维图。</p><div class="kg kh ki kj gt ab cb"><figure class="ns kk nt nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/e0f6301be8146c0f277580b34eb0ea4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*Kic8Jz1k_ePPk4y5AjjVHg.png"/></div></figure><figure class="ns kk ny nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/2d5771a81d1afe766e0ee3b751c3dba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*U65CQmHMfxrhejhKazZipA.png"/></div></figure><figure class="ns kk nz nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/98692a15f971277a9c84eb85a55f4cea.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*9Jfit7LT9zD7kv9NoONlcA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk oa di ob oc translated">PCA (n = 3)，图为<strong class="bd nj"> PCA-x，PCA-y </strong>:原罗伯塔(<strong class="bd nj">左</strong>)，微调用暹罗(<strong class="bd nj">中心</strong>，NLI分类后(<strong class="bd nj">右</strong>)。(图片由作者提供)</p></figure></div><p id="1e44" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">图2: </strong>我们对句子嵌入的所有3个实例运行n_components = 3的PCA，并为所有数据点绘制3D图。</p><div class="kg kh ki kj gt ab cb"><figure class="ns kk od nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/c23dd990ba2e037193ec624010122d95.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*CyY_hnAI3TlrCKNrc9oSKw.png"/></div></figure><figure class="ns kk od nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/d6b09f6d130f1ed744751ea4aafeda79.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*K1ns95FuDLeYBQdiWHd7xg.png"/></div></figure><figure class="ns kk od nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/0237c72362c159ac68d20501cc37f5e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*cI7uWeHd2Dg2GGPm-N_tng.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk oa di ob oc translated">PCA (n = 3)，图形为<strong class="bd nj"> PCA-x，PCA-y，PCA-z </strong>:原罗伯塔(<strong class="bd nj">左</strong>，微调为连体(<strong class="bd nj">居中</strong>，NLI分类后(<strong class="bd nj">右</strong>)。(图片由作者提供)</p></figure></div><p id="29b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">图表3: </strong>我们对所有3个句子嵌入实例运行n_components = 2的TSNE拟合，并绘制图表。</p><div class="kg kh ki kj gt ab cb"><figure class="ns kk oe nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/16c3b0292cc84bc43704a5840dab44f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*NDxwurc1E8yCu_ZNtsFNdQ.png"/></div></figure><figure class="ns kk of nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/d2252c80c4caded9d753e52ebd0c4b4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*sgBcv_T2nxPrbSFqxOg50g.png"/></div></figure><figure class="ns kk of nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/d9aa743fbeecda66b817deaa0c43b4ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*-CMKK0MEO_7eLg_0wY99Qg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk og di oh oc translated">TSNE (n = 2)，图形为<strong class="bd nj"> X，Y </strong>:原罗伯塔(<strong class="bd nj">左</strong>)，微调为连体(<strong class="bd nj">居中</strong>)，NLI分类后(<strong class="bd nj">右</strong>)。(图片由作者提供)</p></figure></div><p id="50ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">图4: </strong>我们运行n_components = 50的PCA，并在所有3个句子嵌入实例上运行n_component = 2的TSNE拟合。在此图中，我们比较了前两个图和当前图。</p><p id="ce7e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">图表4.1: </strong>为原始RoBERTa权重。对此的观察是，所有的数据点是分散的，并且没有逻辑足以基于它们的标签显示句子对嵌入之间的清楚区别。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/03336663ac7a16dd4127c662827f71a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ig20-3-xiywm51i-Ml1SUg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nj">PCA[</strong>n = 3<strong class="bd nj">]【X，Y】</strong>(<strong class="bd nj">左</strong>)<strong class="bd nj">TSNE[</strong>n = 2<strong class="bd nj">]【X，Y】</strong>(<strong class="bd nj">中心</strong>)<strong class="bd nj">TSNE[</strong>n = 2，PCA[n = 50]<strong class="bd nj">]【X，Y】</strong>(<strong class="bd nj">右【T41</strong></p></figure><p id="ed68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">图4.2: </strong>微调连体结构后，我们可以观察到基于标签的句子对嵌入之间的一些区别。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/4ba2e444cccb022867d90f6ed6889e85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0SukfuUfeADtOz9X6CAnhQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nj">PCA[</strong>n = 3<strong class="bd nj">]【X，Y】</strong>(<strong class="bd nj">左</strong>)<strong class="bd nj">TSNE[</strong>n = 2<strong class="bd nj">]【X，Y】</strong>(<strong class="bd nj">中心</strong>)<strong class="bd nj">TSNE[</strong>n = 2，PCA【n = 50】<strong class="bd nj">【X，Y】</strong>(<strong class="bd nj">右</strong></p></figure><p id="c91e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">图4.3: </strong>在对暹罗架构进行序列分类和微调之后，我们可以观察到，基于标签的句子对嵌入之间存在明显的区别。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/4d540ae523373c4d3339705c08476185.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uHil_sFbwtM7jci9vTr-JA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nj">PCA[</strong>n = 3<strong class="bd nj">]【X，Y】</strong>(<strong class="bd nj">左</strong>)<strong class="bd nj">TSNE[</strong>n = 2<strong class="bd nj">]【X，Y】</strong>(<strong class="bd nj">中心</strong>)<strong class="bd nj">TSNE[</strong>n = 2，PCA[n = 50]<strong class="bd nj">]【X，Y】</strong>(<strong class="bd nj">右</strong>)对于<strong class="bd nj">罗伯</strong></p></figure><p id="2263" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我已经介绍了用半监督方法训练一个transformers模型的影响，这种方法总是会产生一个健壮的模型。这个实验是在20，000个数据点的小集合上完成的。在实际数据集上这样做会有更大的影响。在huggingface上已经有可以直接使用的带有连体结构的微调模型。</p><p id="3ebf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可以使用其他技术来更好地聚类或可视化句子对嵌入。这些可视化也可以用于任何其他NLP任务。</p><h2 id="e503" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">参考资料:</h2><ol class=""><li id="a3ca" class="mt mu iq ky b kz ml lc mm lf ok lj ol ln om lr on mz na nb bi translated"><a class="ae kv" href="https://arxiv.org/abs/1908.10084" rel="noopener ugc nofollow" target="_blank">句子伯特</a></li><li id="c87d" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr on mz na nb bi translated">你所需要的只是注意力</li><li id="7b91" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr on mz na nb bi translated"><a class="ae kv" href="https://sbert.net/examples/training/sts/README.html" rel="noopener ugc nofollow" target="_blank">用于语义文本相似性的句子转换器</a></li><li id="b154" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr on mz na nb bi translated"><a class="ae kv" href="https://hamedhelali.github.io/project/Fine-tuning-BERT-For-NLI/" rel="noopener ugc nofollow" target="_blank">为自然语言推理微调BERT</a></li><li id="919a" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr on mz na nb bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b">在Python中使用PCA和t-SNE可视化高维数据集</a></li></ol></div></div>    
</body>
</html>