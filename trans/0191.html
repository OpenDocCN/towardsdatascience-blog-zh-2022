<html>
<head>
<title>How To Attain a Deep Understanding of Soft and Hard Voting in Ensemble Machine Leaning Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何深入理解集成机器学习方法中的软投票和硬投票</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-attain-a-deep-understanding-of-soft-and-hard-voting-in-ensemble-machine-leaning-methods-32a057fc63fa#2022-02-08">https://towardsdatascience.com/how-to-attain-a-deep-understanding-of-soft-and-hard-voting-in-ensemble-machine-leaning-methods-32a057fc63fa#2022-02-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e4e8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何通过从头开始构建执行投票的算法来完全理解软投票和硬投票的工作原理</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/75d0b5a8ac58048c0be2dee5953b919e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EYH52eXcAk3NHTVkqU0VwQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/s/photos/vote?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@element5digital?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Element5数码</a>拍摄</p></figure><h2 id="8f72" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">背景</h2><p id="4b41" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">不久前，我在阅读一本名为《接近(几乎)任何机器学习问题》的书，作者是Abhishek Thakur(参见<a class="ae ky" href="https://www.amazon.co.uk/Approaching-Almost-Machine-Learning-Problem-ebook/dp/B089P13QHT" rel="noopener ugc nofollow" target="_blank">https://www . Amazon . co . uk/approximate-Almost-Machine-Learning-Problem-ebook/DP/b 089 p 13 qht</a>)，当时我看到了一些在集成学习中实现软投票和硬投票的代码。</p><p id="004b" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">我有三个问题-</p><ol class=""><li id="d4a0" class="mt mu it lx b ly mo mb mp li mv lm mw lq mx mn my mz na nb bi translated">我不完全理解代码。</li><li id="f2f7" class="mt mu it lx b ly nc mb nd li ne lm nf lq ng mn my mz na nb bi translated">它只适用于二进制类(恰好有两个类的分类问题)。</li><li id="3530" class="mt mu it lx b ly nc mb nd li ne lm nf lq ng mn my mz na nb bi translated">我不明白为什么没有人使用sckikit-learn实现。</li></ol><p id="ca10" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">事实证明，第三点的答案是，你可以学到很多关于这些算法在做什么，从而通过从头开始构建它们来获得更全面的理解，因此我开始自己构建它们，并有一些次要目标-</p><ol class=""><li id="7d28" class="mt mu it lx b ly mo mb mp li mv lm mw lq mx mn my mz na nb bi translated">抓住机会尽可能多地学习numpy数组、数组切片和矢量化运算。</li><li id="f2b1" class="mt mu it lx b ly nc mb nd li ne lm nf lq ng mn my mz na nb bi translated">挑战自己不断重构代码，直到我优化了算法，遵循“Pyhtonic方式”,尽可能减少代码行。</li></ol><p id="f2ec" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">本文的其余部分对我的发现提供了完整的解释。</p><h2 id="08c3" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">快速回顾集合方法中的软投票和硬投票</h2><p id="29d2" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">集成方法将两个或多个独立的机器学习算法的结果集合在一起，试图产生比任何单个算法都更准确的集体结果。</p><p id="c551" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">在软投票中，每个类别的概率被平均以产生结果。例如，如果算法一以40%的概率预测到一个物体是岩石，而算法二以80%的概率预测到它是一个地雷，那么该集合将以(80 + 40) / 2 = 60%的概率预测到该物体是一个地雷。</p><p id="ca13" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">在硬投票中，考虑每个算法的预测，集合选择具有最高票数的类别。例如，如果三种算法预测特定葡萄酒的颜色为“白色”、“白色”和“红色”，则整体将预测“红色”。</p><h2 id="c7b4" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">入门指南</h2><p id="46dd" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">我们将需要导入几个库，设置一些常量，并在输出中设置科学符号为“off ”,以提高结果的可读性..</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="a822" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">我们还需要一些数据作为分类的输入。<code class="fe nj nk nl nm b">make_classification_dataframe</code>助手函数将数据创建为结构良好的<code class="fe nj nk nl nm b">DataFrame</code>,带有特性名称和命名目标。</p><p id="89aa" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">注意，类的数量被设置为3。这一点很重要，因为我们将为非二进制分类(即超过2个类别)求解软投票和硬投票算法。</p><p id="1a51" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">为非二进制情况精心设计的解决方案适用于二进制，但我见过的其他代码示例只适用于二进制。</p><p id="8419" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">例如，在岩石和矿井的例子中，如果对象可以是“岩石”、“矿井”或“炸弹”，则有3个类需要可以处理非二进制情况的算法(只有“岩石”和“矿井”)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/f41bc714e296b49ddc97b672f6b53782.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PV0XcOnbCdXYcNpBGQo1BQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="21e8" class="no la it bd lb np nq nr le ns nt nu lh jz nv ka ll kc nw kd lp kf nx kg lt ny bi translated">交叉折叠验证</h1><p id="96b7" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">交叉折叠验证被选为评估算法性能的方法，而不是更简单的<code class="fe nj nk nl nm b">train_test_split</code>，因为它通常提供更准确的算法性能指标。</p><p id="deb6" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><code class="fe nj nk nl nm b">scikit-learn</code>中实现的一个问题是，没有办法访问我们从头开始构建软投票和硬投票所需的潜在概率和折叠预测。</p><p id="a4d6" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><code class="fe nj nk nl nm b">cross_val_predict</code>助手函数提供了实现这一点的代码，完整的解释可以在我关于如何为交叉折叠验证绘制混淆矩阵的文章中找到</p><div class="nz oa gp gr ob oc"><a rel="noopener follow" target="_blank" href="/how-to-plot-a-confusion-matrix-from-a-k-fold-cross-validation-b607317e9874"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">如何从K倍交叉验证中绘制混淆矩阵</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">如何为分类机的k折叠交叉验证评估中的所有折叠绘制混淆矩阵…</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="om l on oo op ol oq ks oc"/></div></div></a></div><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="b091" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">请注意，我为本文添加了<code class="fe nj nk nl nm b">predicted_proba</code>,并引入了对<code class="fe nj nk nl nm b">try, except</code>块的需求，因为并非所有算法都支持概率，并且没有一致的警告或错误来显式捕获。</p><p id="2f1c" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">在我们开始之前，让我们快速看一下一个算法的<code class="fe nj nk nl nm b">cross_val_predict</code>..</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><pre class="kj kk kl km gt or nm os ot aw ou bi"><span id="ea02" class="kz la it nm b gy ov ow l ox oy">Wall time: 309 ms<br/>Accuracy of Logistic Regression: 0.6821</span><span id="3273" class="kz la it nm b gy oz ow l ox oy">array([0, 0, 1, ..., 0, 2, 1])</span></pre><p id="9b41" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">一切都相当简单明了。<code class="fe nj nk nl nm b">cross_val_predict</code>已返回概率和预测类，预测类已显示在单元格输出中。</p><p id="0f93" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">第一组数据被预测为属于类0、第二类0、第三类1等。</p><h2 id="1123" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">为多个分类器进行预测</h2><p id="b688" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">我们需要做的下一件事是为几个分类器生成一组预测和概率，这样投票就有了聚合的对象。选择的算法是一个随机森林，一个XG boost和一个额外的随机森林，使用下面的代码实现…</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><pre class="kj kk kl km gt or nm os ot aw ou bi"><span id="f72c" class="kz la it nm b gy ov ow l ox oy">Wall time: 17.1 s<br/>Accuracy of Random Forest: 0.8742<br/>Wall time: 24.6 s<br/>Accuracy of XG Boost: 0.8838<br/>Wall time: 6.2 s<br/>Accuracy of Extra Random Trees: 0.8754</span></pre><p id="8057" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><code class="fe nj nk nl nm b">predictions</code>是一个简单的列表，包含每个算法的一个预测类数组...</p><pre class="kj kk kl km gt or nm os ot aw ou bi"><span id="f43a" class="kz la it nm b gy ov ow l ox oy">[array([2, 0, 0, ..., 0, 2, 1]),<br/> array([2, 0, 2, ..., 0, 2, 1], dtype=int64),<br/> array([2, 0, 0, ..., 0, 2, 1])]</span></pre><p id="42b7" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><code class="fe nj nk nl nm b">predicted_probas</code>有点复杂。这是一个简单的列表，包含每个预测算法的数组。每个数组都有形状(10000，3)。对此解释如下-</p><ul class=""><li id="86d9" class="mt mu it lx b ly mo mb mp li mv lm mw lq mx mn pa mz na nb bi translated">10，000是样本数据集中的数据点数。每个数组针对每组数据都有一行</li><li id="4318" class="mt mu it lx b ly nc mb nd li ne lm nf lq ng mn pa mz na nb bi translated">3是我们的非二元分类器中的类的数量</li></ul><p id="bef6" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">输出中的第一行可以解释如下-</p><p id="892e" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">对于第一种算法，第一组数据(即<code class="fe nj nk nl nm b">DataFrame</code>中的第一行)有17%的概率属于类别0，2%的概率属于类别1，81%的概率属于类别2。</p><p id="eb50" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">我记得我第一次看到来自<code class="fe nj nk nl nm b">predcited_proba</code>方法的输出时，我感到非常困惑，所以希望这个解释能拯救我第一次感觉到的困惑！</p><pre class="kj kk kl km gt or nm os ot aw ou bi"><span id="561e" class="kz la it nm b gy ov ow l ox oy">[array([[0.17, 0.02, 0.81],<br/>        [0.58, 0.07, 0.35],<br/>        [0.54, 0.1 , 0.36],<br/>        ...,<br/>        [0.46, 0.08, 0.46],<br/>        [0.15, 0.  , 0.85],<br/>        [0.01, 0.97, 0.02]]),<br/> array([[0.05611309, 0.00085733, 0.94302952],<br/>        [0.95303732, 0.00187497, 0.04508775],<br/>        [0.4653917 , 0.01353438, 0.52107394],<br/>        ...,<br/>        [0.75208634, 0.0398241 , 0.20808953],<br/>        [0.02066649, 0.00156501, 0.97776848],<br/>        [0.00079027, 0.99868006, 0.00052966]]),<br/> array([[0.33, 0.02, 0.65],<br/>        [0.54, 0.14, 0.32],<br/>        [0.51, 0.17, 0.32],<br/>        ...,<br/>        [0.52, 0.06, 0.42],<br/>        [0.1 , 0.03, 0.87],<br/>        [0.05, 0.93, 0.02]])]</span></pre><h2 id="7f0c" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">从头开始实现软投票和硬投票</h2><p id="e817" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">这里我们已经到了文章的主要部分。软投票和硬投票只需几行Python代码就可以实现，如下所示。</p><p id="e5cc" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">然而，这并不是我的第一次尝试。我的第一次尝试没有那么“Pythonic化”，包含了更多的代码，但实际上它的可读性更好。</p><p id="e42f" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">如果你想看我的第一次尝试，看看这个— …</p><p id="4481" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">如果你完全理解下面的代码，你可以跳过下一节，但我认为它非常复杂，很难阅读和理解，所以我在下面提供了完整的解释。</p><p id="9e0f" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">在我们继续之前，有必要指出，软投票的表现比表现最好的单个算法高出0.3% (88.68%对88.38%)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><pre class="kj kk kl km gt or nm os ot aw ou bi"><span id="1510" class="kz la it nm b gy ov ow l ox oy">Accuracy of Random Forrest: 0.8742<br/>Accuracy of XG Boost: 0.8838<br/>Accuracy of Extra Random Trees: 0.8754<br/>Accuracy of Soft Voting: 0.8868<br/>Accuracy of Hard Voting: 0.881</span></pre><h2 id="7173" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">投票算法的解释</h2><h2 id="345b" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">软投票的代码</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><pre class="kj kk kl km gt or nm os ot aw ou bi"><span id="f9f1" class="kz la it nm b gy ov ow l ox oy">array([[0.18537103, 0.01361911, 0.80100984],<br/>       [0.69101244, 0.07062499, 0.23836258],<br/>       [0.50513057, 0.09451146, 0.40035798],<br/>       ...,<br/>       [0.57736211, 0.05994137, 0.36269651],<br/>       [0.09022216, 0.01052167, 0.89925616],<br/>       [0.02026342, 0.96622669, 0.01350989]])</span></pre><p id="b0b2" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">numpy <code class="fe nj nk nl nm b">mean</code>函数沿0轴(列)取平均值。理论上，这应该是软投票的全部内容，因为这已经创建了3组输出的平均值，并且看起来是正确的。</p><p id="e3f8" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">但是，舍入误差行的值并不总是等于1，这很麻烦，因为每个数据点都属于概率为1的三个类别之一…</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><pre class="kj kk kl km gt or nm os ot aw ou bi"><span id="df1f" class="kz la it nm b gy ov ow l ox oy">0.9999999826153119</span><span id="027e" class="kz la it nm b gy oz ow l ox oy">array([0.18537103, 0.01361911, 0.80100984])</span></pre><p id="15a0" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">这是非常不幸的，但很容易补救。因为我们希望所有3个类的概率对于每个数据点加1，所以我们需要做的就是将最后一列中的值设置为1-其他列中值的总和…</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><pre class="kj kk kl km gt or nm os ot aw ou bi"><span id="0d07" class="kz la it nm b gy ov ow l ox oy">1.0</span><span id="6e44" class="kz la it nm b gy oz ow l ox oy">array([0.18537103, 0.01361911, 0.80100986])</span></pre><p id="a8e8" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">现在一切看起来都很好，每一行的概率加起来是1，就像它们应该的那样。</p><p id="d887" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">然而，我们仍然需要知道类别预测(即，对于每一行，软投票预测了类别0、1或2)。幸运的是,<code class="fe nj nk nl nm b">numpy</code>库中精彩的<code class="fe nj nk nl nm b">argmax</code>函数能够在一行代码中实现这一点...</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><pre class="kj kk kl km gt or nm os ot aw ou bi"><span id="b88d" class="kz la it nm b gy ov ow l ox oy">array([2, 0, 0, ..., 0, 2, 1], dtype=int64)</span></pre><p id="34db" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><code class="fe nj nk nl nm b">argmax</code>函数只是沿着<code class="fe nj nk nl nm b">axis</code>参数中指定的轴选取数组中最大值的索引，因此它为第一行选取2，为第二行选取0，为第三行选取0，依此类推。</p><h2 id="bec5" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">硬投票的代码</h2><p id="ce3d" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">使用Python列表理解和<code class="fe nj nk nl nm b">numpy</code>数组函数，可以在一行代码中实现硬投票算法...</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="6c9b" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><code class="fe nj nk nl nm b">np.array(predictions).T</code>语法只是转置了数组的数组，因此它不是3行和10，000列，而是10，000行和3列...</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><pre class="kj kk kl km gt or nm os ot aw ou bi"><span id="6bb8" class="kz la it nm b gy ov ow l ox oy">(3, 10000)</span><span id="9d0f" class="kz la it nm b gy oz ow l ox oy">array([[2, 2, 2],<br/>       [0, 0, 0],<br/>       [0, 2, 0],<br/>       ...,<br/>       [0, 0, 0],<br/>       [2, 2, 2],<br/>       [1, 1, 1]], dtype=int64)</span></pre><p id="4117" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">然后list comprehension有效地获取每个元素(行)并对其应用<code class="fe nj nk nl nm b">statistics.mode</code>，从而选择从算法中获得最多投票的分类...</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><pre class="kj kk kl km gt or nm os ot aw ou bi"><span id="68d2" class="kz la it nm b gy ov ow l ox oy">array([2, 0, 0, ..., 0, 2, 1], dtype=int64)</span></pre><h2 id="fc75" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">使用Scikit-Learn</h2><p id="ab5a" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">手工制作的软投票和硬投票算法确保了我们对正在发生的事情有一个全面的了解，所以是时候走捷径，使用<code class="fe nj nk nl nm b">scikit-learn VotingClassifier</code>为我们做所有的工作了...</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><pre class="kj kk kl km gt or nm os ot aw ou bi"><span id="5ce5" class="kz la it nm b gy ov ow l ox oy">Wall time: 1min 4s<br/>Wall time: 55.3 s<br/>Accuracy of SciKit-Learn Soft Voting: 0.8868<br/>Accuracy of SciKit-Learn Hard Voting: 0.881</span></pre><p id="6f11" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">令人欣慰的是，<code class="fe nj nk nl nm b">scikit-learn</code>实现产生了与scratch构建算法完全相同的结果——软投票的准确率为88.68%，硬投票的准确率为88.1%。</p><h2 id="9a21" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">好吧，让我们把厨房的水槽扔过去！</h2><p id="ce6c" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">那么，通过在投票中包含广泛的分类算法，我们究竟可以在准确度测量方面实现多大的改进呢？</p><p id="b126" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">在这里，我们将把投票中使用的算法数量扩展到6个，看看我们能从整体中挤出多少性能…</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h2 id="8872" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">方法1:使用临时构建的投票</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><pre class="kj kk kl km gt or nm os ot aw ou bi"><span id="9b2e" class="kz la it nm b gy ov ow l ox oy">Wall time: 14.9 s<br/>Accuracy of Random Forest: 0.8742<br/>Wall time: 32.8 s<br/>Accuracy of XG Boost: 0.8838<br/>Wall time: 5.78 s<br/>Accuracy of Extra Random Trees: 0.8754<br/>Wall time: 3min 2s<br/>Accuracy of Neural Network: 0.8612<br/>Wall time: 36.2 s<br/>Accuracy of Support Vector Machine: 0.8674<br/>Wall time: 1.65 s<br/>Accuracy of Light GMB: 0.8828<br/>Accuracy of Soft Voting: 0.8914<br/>Accuracy of Hard Voting: 0.8851<br/>Wall time: 4min 34s</span></pre><h2 id="89f4" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">方法2:使用SciKit-Learn和cross_val_predict</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><pre class="kj kk kl km gt or nm os ot aw ou bi"><span id="82bf" class="kz la it nm b gy ov ow l ox oy">Wall time: 4min 11s<br/>Wall time: 4min 41s<br/>Accuracy of SciKit-Learn Soft Voting: 0.8914<br/>Accuracy of SciKit-Learn Hard Voting: 0.8859<br/>Wall time: 8min 52s</span></pre><h2 id="5211" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">方法3:使用SciKit-Learn和cross_val_score</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><pre class="kj kk kl km gt or nm os ot aw ou bi"><span id="535e" class="kz la it nm b gy ov ow l ox oy">Accuracy of SciKit-Learn Soft Voting using cross_val_score: 0.8914<br/>Wall time: 4min 46s</span></pre><h1 id="1d7c" class="no la it bd lb np nq nr le ns nt nu lh jz nv ka ll kc nw kd lp kf nx kg lt ny bi translated">结论</h1><p id="89be" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">3种不同的方法对软投票准确性的评分一致，这验证了研究并表明临时构建投票算法工作正常。</p><p id="60ae" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">向混合中添加神经网络、支持向量机和光GMB，使得软投票的准确度从88.68%提高了0.46%，达到89.14%，并且新的软投票准确度比最佳个体算法(XG Boost为88.38%)好0.76%。还需要注意的是，加入比XG Boost精度分数低的光照GMB将整体性能提高了0.1%！</p><p id="884a" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">这些都是边际收益，所以选择使用软投票最终是增加复杂性与实现改进的影响和含义之间的平衡。</p><p id="9ab1" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">如果这是一场Kaggle竞争，那么0.76%可能会很大，可能会让你在排行榜上一飞冲天。如果你对矿石进行分类，你会想要额外的准确性，而如果你对红葡萄酒和白葡萄酒进行分类，你可能会喜欢简单的分类。</p><p id="14d4" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">总之，通过这项研究并从头开始构建软投票和硬投票算法，对理解它们有了很大的改善，这将帮助数据科学家在构建现实世界的预测机器学习算法时做出正确的选择。</p><h2 id="0859" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">最后一点:Scikit的局限性——学习硬投票</h2><p id="ce16" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">在这篇文章的研究中，我遇到了一个完全的障碍，出现了一个我无法理解的错误消息，结果发现这是因为在某个地方的库中有一个bug，这意味着下面的组合不起作用</p><ol class=""><li id="2232" class="mt mu it lx b ly mo mb mp li mv lm mw lq mx mn my mz na nb bi translated">数据集中的目标是非二进制的(即分类有两个以上的类)。</li><li id="92e1" class="mt mu it lx b ly nc mb nd li ne lm nf lq ng mn my mz na nb bi translated"><code class="fe nj nk nl nm b">scikit-learn</code>通过设置<code class="fe nj nk nl nm b">voting="hard"</code>被用于硬表决。</li><li id="5698" class="mt mu it lx b ly nc mb nd li ne lm nf lq ng mn my mz na nb bi translated">估计器包括一个<code class="fe nj nk nl nm b">catboost.CatBoostClassifier</code>(其他一些分类器产生相同的结果)。</li></ol><p id="1ba1" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">您可以使用下面的代码尝试这种组合，您将获得一个<code class="fe nj nk nl nm b">ValueError: could not broadcast input array from shape (2000,1) into shape (2000)</code>。注意:我特意使用了<code class="fe nj nk nl nm b">cross_val_score</code>来保证这个bug不是由<code class="fe nj nk nl nm b">cross_val_predict</code>帮助函数中的任何东西引起的。</p><p id="c8d3" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">然而，本文中开发的从头实现硬投票的代码确实支持这种组合，所以如果您需要在非二进制分类的硬投票中包含一个<code class="fe nj nk nl nm b">CatBoost</code>,本文提供了唯一的方法，直到错误被修复。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><pre class="kj kk kl km gt or nm os ot aw ou bi"><span id="3acd" class="kz la it nm b gy ov ow l ox oy"><strong class="nm iu">ValueError</strong>: could not broadcast input array from shape (2000,1) into shape (2000)</span></pre><h1 id="d75f" class="no la it bd lb np nq nr le ns nt nu lh jz nv ka ll kc nw kd lp kf nx kg lt ny bi translated">感谢您的阅读！</h1><p id="c910" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">如果你喜欢读这篇文章，为什么不看看我在https://grahamharrison-86487.medium.com/的其他文章呢？此外，我很乐意听到您对这篇文章、我的任何其他文章或任何与数据科学和数据分析相关的内容的看法。</p><p id="1ec2" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">如果你想联系我讨论这些话题，请在LinkedIn上找我—<a class="ae ky" href="https://www.linkedin.com/in/grahamharrison1" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/grahamharrison1</a>或者发邮件给我<a class="ae ky" href="mailto:GHarrison@lincolncollege.ac.uk" rel="noopener ugc nofollow" target="_blank">ghar rison @ Lincoln college . AC . uk</a>。</p><p id="83a9" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">如果你想通过订阅来支持作者和全世界1000个为文章写作做出贡献的人，请使用下面的链接(注意:如果你使用这个链接免费注册，作者将收到一定比例的费用)。</p><div class="nz oa gp gr ob oc"><a href="https://grahamharrison-86487.medium.com/membership" rel="noopener follow" target="_blank"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">通过我的推荐链接加入媒体-格雷厄姆哈里森</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">grahamharrison-86487.medium.com</p></div></div><div class="ol l"><div class="pb l on oo op ol oq ks oc"/></div></div></a></div></div></div>    
</body>
</html>