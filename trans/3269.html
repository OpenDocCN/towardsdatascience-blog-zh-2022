<html>
<head>
<title>NSVQ: Improved Vector Quantization technique for Neural Networks Training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NSVQ:用于神经网络训练的改进矢量量化技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/improving-vector-quantization-in-vector-quantized-variational-autoencoders-vq-vae-915f5814b5ce#2022-07-20">https://towardsdatascience.com/improving-vector-quantization-in-vector-quantized-variational-autoencoders-vq-vae-915f5814b5ce#2022-07-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="77b0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用于机器学习优化的高效矢量量化。向量量化变分自动编码器)，优于直通估计器</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a11cfd341542f684b9b0c27d650402bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mQbE21i6fsfqpU3L"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@vackground?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">vackground.com</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="d371" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">矢量量化(Vector quantization，VQ)是一种类似k-means聚类算法的数据压缩技术，它通过一些有代表性的称为码本的矢量来模拟数据的概率密度函数。矢量量化最近被广泛用于基于机器学习的应用中，尤其是那些基于矢量量化变分自动编码器(VQ-VAE)的应用；例如图像生成[1]、语音编码[2]、语音转换[3]、音乐生成[4]和文本到语音合成[5]。</p><p id="4dc9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据下图，矢量量化是将所有数据点(在voronoi-cell中)映射到其最近的码本点的操作。换句话说，它使用下面的公式从一组码本向量(c_k)s中找到最接近输入数据向量(x)的码本向量(c_k*)，并且通过用最接近的码本(c_k*)替换x的值来将x映射(量化)到x_hat。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ls lt l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">矢量量化公式</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/9e73f559e3360fc068c6c647e4115c0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_j4iFnsv7Xt5CoeZ40rZoQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">矢量量化运算(图片来自<a class="ae kv" href="https://speechprocessingbook.aalto.fi/Modelling/Vector_quantization_VQ.html" rel="noopener ugc nofollow" target="_blank">此处</a>)</p></figure><p id="fa72" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">显然，矢量量化不能直接用于基于机器学习的优化，因为没有为它定义真正的梯度(对于上述方程)。因此，我们需要应用一些技巧来估计VQ函数的梯度，并使其通过反向传播来传递这些梯度并更新码本向量。</p><p id="f4a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">VQ-VAE在Oord等人的论文<a class="ae kv" href="https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf" rel="noopener ugc nofollow" target="_blank">神经离散表示学习</a>中首次提出。他们应用VQ来模拟变分自动编码器的潜在空间的离散表示。换句话说，他们将VQ应用于编码器的输出，并为此找到最佳嵌入(码本)，然后将这些嵌入传递给解码器。为了在反向传播(反向传递)中传递VQ函数的梯度，本文和上述所有文章都使用直通估计器(STE) [7]，它只是简单地复制VQ模块上的梯度，如下图所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lv"><img src="../Images/901d27807c18f56afbe5c1208bbc26b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pTECMRydvo-e3j58W75Hnw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">直通估计器(图片来自<a class="ae kv" href="https://www.hassanaskary.com/python/pytorch/deep%20learning/2020/09/19/intuitive-explanation-of-straight-through-estimators.html" rel="noopener ugc nofollow" target="_blank">此处</a></p></figure><p id="8e5d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，直通估计器(STE)没有考虑量化的影响，并导致梯度和矢量量化的真实行为之间的不匹配。此外，对于使用STE的方法，有必要在全局损失函数中增加一个额外的损失项，以使VQ码本(嵌入)得到更新。因此，附加损失项的加权系数是一个新的超参数，需要手动调整。奥尔德等人[6]引入的VQ-VAE损失函数如下所示。公式中的第二项和第三项是VQ模块所需的损失项，加权系数为α和β。关于损失函数的更多细节，请参见论文。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lw lt l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">奥尔德等人[6]在VQ-VAE论文中使用的损失函数</p></figure><p id="bbcc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我们想介绍我们最近提出的基于机器学习方法的矢量量化技术，它以“<a class="ae kv" href="https://ieeexplore.ieee.org/abstract/document/9696322" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> NSVQ:机器学习矢量量化中的噪声替换</strong></a>【8】的标题发表。NSVQ是一种技术，其中通过将噪声添加到输入向量来模拟向量量化误差，使得模拟的噪声将获得原始VQ误差分布的形状。NSVQ的框图如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lx"><img src="../Images/d8e52541584a6f01cb0b0e1ecf715152.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xvwZLWlnBY41-4dlyEZE-Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">NSVQ框图:矢量量化中的噪声替代(图片由作者提供)</p></figure><p id="2951" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们在三个不同的实验场景中测试我们的NSVQ方法，</p><ol class=""><li id="cba1" class="ly lz iq ky b kz la lc ld lf ma lj mb ln mc lr md me mf mg bi translated">语音编码:对语音信号的频谱包络进行建模(<a class="ae kv" href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/vali21_interspeech.pdf" rel="noopener ugc nofollow" target="_blank">在【9】</a>中提出的语音编码方法)</li><li id="62c5" class="ly lz iq ky b kz mh lc mi lf mj lj mk ln ml lr md me mf mg bi translated">图像压缩:对[6]中提出的VQ-VAE的潜在表示进行建模</li><li id="b295" class="ly lz iq ky b kz mh lc mi lf mj lj mk ln ml lr md me mf mg bi translated">玩具示例:对一些众所周知的数据分布进行建模。</li></ol><p id="7926" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基于<a class="ae kv" href="https://ieeexplore.ieee.org/abstract/document/9696322" rel="noopener ugc nofollow" target="_blank">我们的论文</a>【8】中的结果(在下面的图中也提供了这些结果)，已经表明NSVQ不仅可以通过后向通道中的梯度，而且可以为码本提供比STE更精确的梯度。此外，与STE相比，NSVQ导致更高的量化精度和更快的收敛。此外，在多次运行单个实验时，NSVQ的性能更具确定性(表现出更小的性能差异)。使用NSVQ的一大好处是，它不需要在全局损失函数中添加任何额外的损失项。因此，它不会导致任何额外的超参数调整(STE额外损失项的系数)。</p><p id="7160" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在语音编码场景中，我们使用语音质量感知评估(PESQ)和感知加权信噪比(pSNR)作为客观度量来评估STE和我们提出的NSVQ方法的解码语音质量。下图显示了结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mm"><img src="../Images/dafd904159e9c665dead6528905c6e4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fobBGu61-2QmsNsvuIwktA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在语音编码场景中，在8、9.6、13.2、16.4、24.4和32 kbit/s的总比特率下，针对12比特VQ，在PESQ和pSNR度量方面，所提出的NSVQ和STE的性能；实线指的是PESQ和pSNR的平均值，对应的填充区域指的是它们的95%分位数。(图片由作者提供)</p></figure><p id="4166" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在图像压缩场景中，我们采用结构相似性指数度量(SSIM)和峰值信噪比(峰值SNR)作为客观度量来评估VQ-VAE解码器端的重建图像的质量。下图展示了结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mn"><img src="../Images/296a967f7f4a0d89f2c6e89137f20a9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P0LWep6XHRHZT-e3nnfr5A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在图像压缩场景中，在15 k训练更新和超过20次单独实验之后，STE的SSIM和峰值SNR值以及针对不同VQ比特率提出的NSVQ实线指的是SSIM和峰值SNR的平均值，相应的填充区域指的是它们的95%分位数。(图片由作者提供)</p></figure><p id="b9ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据上图，当增加VQ比特率时，所提出的NSVQ获得严格上升的SSIM和峰值SNR值，这证实了它的行为与比特率的增加一致。这种行为是我们对真正的VQ的期望。然而，STE方法不遵循相同的行为。我们认为原因是STE没有考虑VQ函数在反向传播中的影响。</p><p id="311c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在图像压缩场景中，除了客观评价之外，我们还研究了另外两件事；训练收敛速度和码本向量混乱度(平均码本使用率)。这是结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mo"><img src="../Images/04840b957aea2c55d934ac9b1f6a790d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xZS1wtQHkWXTCXBo3OvdxA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在图像压缩场景中，在15 k训练更新和超过20次单独实验之后，STE的平滑训练误差(MSE)和平滑平均码本使用(困惑)以及针对11比特VQ提出的NSVQ实线指的是MSE和困惑度的平均值，对应的填充区域指的是它们的95%分位数。(图片由作者提供)</p></figure><p id="b4ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文的第三个场景中，我们使用我们提出的NSVQ、STE和MiniBatchKmeans(来自scikit-learn库)方法对几个著名的机器学习数据分布进行建模。这里，我们仅示出了使用8比特(2⁸=256码本)在2D空间中对<em class="mp">瑞士滚</em>分布应用的矢量量化。我们从数据分布中初始化码本向量的位置，以使VQ更具挑战性。下图显示了NSVQ、STE和MiniBatchKmeans方法的最终优化码本位置。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/02e739d5a20f9238cb78cd3852709d4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZYByHhJjFqfzRtEIzX7npA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用<strong class="bd mr"> NSVQ </strong>方法的8位矢量量化的最终优化码本(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/759039ec867128862217b0e83c805520.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6diWUUxradTGbc7ZnAtA_g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用<strong class="bd mr"> STE </strong>方法的8位矢量量化的最终优化码本(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/7fef817ff3433aaba208cfe425711162.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xFbvFZR5ZEuiQPbtZDAU9Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用<strong class="bd mr"> MiniBatchKmeans </strong>方法的8位矢量量化的最终优化码本(图片由作者提供)</p></figure><p id="9d4a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据这些图，NSVQ捕获<em class="mp">瑞士卷</em>分布形状比STE好得多，并且具有少得多的死码本数量(从数据分布中选择的并且对解码阶段无用的码本)。因此，可以得出结论，与STE相比，NSVQ对码本的初始化不太敏感。请注意，NSVQ的这种行为部分是由于我们论文中提到的<em class="mp">码本替换</em>功能。</p><p id="e378" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，NSVQ以比MiniBatchKmeans方法更统一和同类的方式定位码书。查看上图中黑框中的密集码本分布，它显示了MiniBatchKmeans方法的码本位置。这种现象在NSVQ方法中没有发生(注意，数据分布的性质在黑盒区域中并不密集)。对于所有这三种方法，我们已经计算了原始数据分布与其矢量量化版本之间的均方误差(MSE)。根据我们在<a class="ae kv" href="https://ieeexplore.ieee.org/abstract/document/9696322" rel="noopener ugc nofollow" target="_blank">论文</a>【8】中发表的MSE值，NSVQ用比STE和MiniBatchKmeans方法更低的MSE值量化数据。</p><p id="49b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我必须提到，NSVQ技术适用于训练任何机器学习应用程序，这需要在计算图中的某个位置进行矢量量化。然而，请注意<strong class="ky ir"> NSVQ仅应用于训练阶段</strong>以便学习矢量量化码本。但是，它不用于推理阶段，因为我们不需要在反向传播中传递任何梯度。因此，在推理阶段，使用上面提到的原始矢量量化公式会更好、更准确。</p><p id="3549" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们在以下公开网页中提供NSVQ 的<strong class="ky ir"> PyTorch代码:</strong></p><div class="mu mv gp gr mw mx"><a href="https://gitlab.com/speech-interaction-technology-aalto-university/nsvq" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd ir gy z fp nc fr fs nd fu fw ip bi translated">阿尔托大学/ NSVQ GitLab的语音交互技术</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">论文“机器学习矢量量化中的噪声替换”代码的Python实现…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">gitlab.com</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl kp mx"/></div></div></a></div></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><h1 id="30fe" class="nt nu iq bd nv nw nx ny nz oa ob oc od jw oe jx of jz og ka oh kc oi kd oj ok bi translated">确认</h1><p id="b81a" class="pw-post-body-paragraph kw kx iq ky b kz ol jr lb lc om ju le lf on lh li lj oo ll lm ln op lp lq lr ij bi translated">特别感谢我的博士生导师<a class="ae kv" href="https://research.aalto.fi/en/persons/tom-bäckström" rel="noopener ugc nofollow" target="_blank">Tom bck strm</a>教授，他支持我，也是这项工作的另一位贡献者。</p><h1 id="5ec8" class="nt nu iq bd nv nw oq ny nz oa or oc od jw os jx of jz ot ka oh kc ou kd oj ok bi translated">参考</h1><p id="3448" class="pw-post-body-paragraph kw kx iq ky b kz ol jr lb lc om ju le lf on lh li lj oo ll lm ln op lp lq lr ij bi translated">A. Razavi，A. van den Oord和O. Vinyals，“用VQ-VAE-2号生成多样的高保真图像”，在<em class="mp"> Proc。第33届Int。糖膏剂神经感染。过程。系统。</em>，2019年第32卷，第14866–14876页。</p><p id="823e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">【2】</strong>c . Garbacea，A. V. den Oord，Y. Li，F. S. C. Lim，A. Luebs，O. Vinyals和T. C. Walters，<em class="mp"> Proc .IEEE Int。糖膏剂声音。，语音信号处理。(ICASSP) </em>，2019年5月，第735–739页。</p><p id="607d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">【3】</strong>s . Ding和R. Gutierrez-Osuna，“非并行语音转换中矢量量化变分自动编码器的分组潜在嵌入”，<em class="mp">Inter-speech</em>论文集，2019，第724–728页。</p><p id="fea2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">【4】</strong>p . Dhariwal，H. Jun，C. Payne，J. W. Kim，a .拉德福德，I. Sutskever，《点唱机:音乐的生成模型》，<em class="mp"> arXiv预印本arXiv:2005.00341 </em>，2020年。</p><p id="0288" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"/>a . TJ andra，B. Sisman，M. Zhang，S. Sakti，H. Li，S. Nakamura，“用于Zerospeech challenge 2019的VQVAE无监督单元发现和多尺度code2spec反相器”，<em class="mp"> arXiv预印本arXiv:1905.11449 </em>，2019。</p><p id="c8e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">【6】</strong>a . van den Oord，O. Vinyals，K. Kavukcuoglu，“神经离散表征学习”，载于<em class="mp">第31届国际神经信息处理系统会议论文集</em>，2017年，第6309–6318页。</p><p id="4a14" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">【7】</strong>y . beng io，n . Le \u onard，a .库维尔，“通过随机神经元估计或传播梯度用于条件计算”，<em class="mp"> arXiv预印本arXiv:1308.3432 </em>，2013年。</p><p id="ba1f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">【8】</strong>m . h .瓦利和t . bck strm，“NSVQ:用于机器学习的矢量量化中的噪声替换”，<em class="mp"> IEEE Access </em>，第10卷，第13 598–13 610页，2022年。</p><p id="a2ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"/>m . h .瓦利和t .贝克斯特罗姆，“语音和音频编码频谱包络的端到端优化多级矢量量化”，载于Pr <em class="mp"> oc。Interspeech，【2021年8月，第3355–3359页。</em></p></div></div>    
</body>
</html>