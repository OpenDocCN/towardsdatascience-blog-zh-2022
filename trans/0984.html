<html>
<head>
<title>Uncertainty In Deep Learning — Bayesian CNN | TensorFlow Probability</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的不确定性—贝叶斯 CNN |张量流概率</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/uncertainty-in-deep-learning-bayesian-cnn-tensorflow-probability-758d7482bef6#2022-03-15">https://towardsdatascience.com/uncertainty-in-deep-learning-bayesian-cnn-tensorflow-probability-758d7482bef6#2022-03-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cb9e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">创建一个深度学习模型，知道它不知道什么。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2adf1e4cf5ec5ba58da1ccb3a3b7533e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*O9PTkWIKQwgxB0u-"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">埃文·丹尼斯在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="838c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是深度学习<strong class="ky ir">不确定性系列<strong class="ky ir">第四</strong> <strong class="ky ir">部分</strong>。</strong></p><ul class=""><li id="4eca" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-brief-introduction-1f9a5de3ae04">第 1 部分—简介</a></li><li id="374d" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-aleatoric-uncertainty-and-maximum-likelihood-estimation-c7449ee13712">第 2 部分—随机不确定性和最大似然估计</a></li><li id="4a26" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-epistemic-uncertainty-and-bayes-by-backprop-e6353eeadebb">第 3 部分——认知不确定性和反向投影贝叶斯</a></li><li id="1d24" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">第 4 部分——实现完全概率贝叶斯 CNN </strong></li><li id="e38f" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-experiments-with-bayesian-cnn-1ca37ddb6954">第五部分——贝叶斯 CNN 实验</a></li><li id="0fb1" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">第六部分——贝叶斯推理和变形金刚</li></ul><h1 id="6f34" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">介绍</h1><p id="298b" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在本文中，我们将探索如何使用张量流概率(TFP)实现完全概率贝叶斯 CNN 模型。在<a class="ae kv" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-aleatoric-uncertainty-and-maximum-likelihood-estimation-c7449ee13712">第二部分</a>中，我们创建的模型只能捕捉<strong class="ky ir">随机不确定性，或数据不确定性</strong>。</p><p id="1783" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在接下来的部分(<a class="ae kv" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-epistemic-uncertainty-and-bayes-by-backprop-e6353eeadebb">第 3 部分</a>)我们<strong class="ky ir">改变了我们的模型层，使它们具有概率分布，而不是固定的权重和偏差</strong>。通过这种方式，我们能够捕捉模型参数上的<strong class="ky ir">认知不确定性</strong>。</p><p id="8cc8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，是时候将<a class="ae kv" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-aleatoric-uncertainty-and-maximum-likelihood-estimation-c7449ee13712">第二部分</a>和<a class="ae kv" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-epistemic-uncertainty-and-bayes-by-backprop-e6353eeadebb">第三部分</a>组合在一起了。换句话说，我们将在本帖中介绍如何<strong class="ky ir">创建一个基本的 BNN </strong>，并解释图层参数。</p><p id="38ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">文章组织如下:</p><ul class=""><li id="4abc" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">变化和重新参数化层</strong></li><li id="8faf" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">正常 CNN 和参数</strong></li><li id="a999" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">重新参数化层中的自定义优先级</strong></li><li id="d4b3" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">KL 散度的近似值</strong></li><li id="380c" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">创建全概率贝叶斯 CNN </strong></li><li id="a6b4" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">这个模型知道自己不知道的！</strong></li><li id="a007" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">结论&amp;下一步</strong></li></ul><p id="4d03" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们开始之前，让我们看看导入:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><h1 id="843a" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated"><strong class="ak">变层和重新参数化层</strong></h1><p id="3800" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">首先，从 2022 年 2 月 27 日<strong class="ky ir">起</strong>在 TFP 中没有像我们在第 3 部分中使用的<strong class="ky ir">dense varianial</strong>那样的<strong class="ky ir">conv variant</strong>层。相反，</p><ul class=""><li id="25b8" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">卷积 1 去参数化</li><li id="2ad3" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">卷积 2 重新参数化</li><li id="4225" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">卷积 3 重新参数化</li></ul><p id="eab0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">层是存在的。底层的<strong class="ky ir">算法与我们在<a class="ae kv" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-epistemic-uncertainty-and-bayes-by-backprop-e6353eeadebb">第 3 部分</a>中讨论的</strong>算法相同。</p><p id="2e99" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看<strong class="ky ir">convolution 2d 重新参数化</strong>层的参数:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="faa2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe nf ng nh ni b">divergence_fn</code> =回想一下:“我们希望先验和变分后验之间的<strong class="ky ir">KL-散度</strong>尽可能低<strong class="ky ir"/>，同时保持<strong class="ky ir">预期可能性尽可能高(</strong> <a class="ae kv" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-epistemic-uncertainty-and-bayes-by-backprop-e6353eeadebb"> <strong class="ky ir">第 3 部分</strong> </a> <strong class="ky ir">)”。</strong>因此，根据我们对先验函数和后验函数的选择，如果可以解析地计算 KL-散度，并且已在 TFP 中注册，它将使用精确的计算。这就是<code class="fe nf ng nh ni b">kl_use_exact</code>参数在<code class="fe nf ng nh ni b">DenseVariational</code>层实际做的事情！但是在<strong class="ky ir">convolution 2d 重新参数化</strong>中我们提供了它。最后，我们将它除以样本总数，这是我们在第 3 部分看到的原因。</p><p id="cf40" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果 KL 无法解析计算，我们将看看我们应该提供什么。</p><p id="e533" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe nf ng nh ni b">prior_fn &amp; posterior_fn</code> =这些是<code class="fe nf ng nh ni b">prior</code>和<code class="fe nf ng nh ni b">variational_posterior</code>功能(第三部分)，我们可以定制它们。例如，我们可以使用拉普拉斯先验等。</p><p id="4f5d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe nf ng nh ni b">is_singular</code> =如果设置为<code class="fe nf ng nh ni b">True</code>，数值再次变成点估计值，而不是概率值。考虑这个场景:</p><pre class="kg kh ki kj gt nj ni nk nl aw nm bi"><span id="7f49" class="nn mh iq ni b gy no np l nq nr">bias_prior_fn = None,<br/>bias_posterior_fn = ...normal_fn(is_singular=True)</span></pre><p id="6689" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的代码中，偏差参数的学习方式与常规卷积层相同，它固定为一个值。</p><p id="a787" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当<code class="fe nf ng nh ni b">is_singular=True</code>被传递时，一个<code class="fe nf ng nh ni b">tfd.Deterministic</code>的实例将被创建。这就是点估计值的获得方式。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><p id="213d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们已经看到了重新参数化层的参数。我们可以开始写模型了。</p><h1 id="610f" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">普通 CNN</h1><p id="214b" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">首先，让我们从如何创建一个普通的 CNN 开始:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="a795" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">相当简单，只是一个普通的 CNN 网络！我们将把这个模型转换成贝叶斯卷积神经网络。</p><p id="5228" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">并且注意这个型号总共有<strong class="ky ir"> 98.442 个参数</strong>。</p><h1 id="befb" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">概率贝叶斯 CNN</h1><h2 id="f3a8" class="nn mh iq bd mi nz oa dn mm ob oc dp mq lf od oe ms lj of og mu ln oh oi mw oj bi translated">重新参数化图层的自定义先验</h2><p id="d3fb" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">由于<strong class="ky ir">重新参数化</strong>层在方法参数方面不同于<strong class="ky ir">密集变量化</strong>层，我们需要在编写自定义先验&amp;后验时考虑这一点。</p><p id="6c23" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，如果我们想要提供自定义病历，它们应该定义如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="67b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，每个功能都需要<code class="fe nf ng nh ni b">dtype, shape, trainable, add_variable_fn</code>。让我们打开<code class="fe nf ng nh ni b">batch_ndims</code>和<code class="fe nf ng nh ni b">reinterpreted_batch_ndims</code>的包装:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="8333" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们使用正态分布来观察<code class="fe nf ng nh ni b">reinterpreted_batch_ndims</code>如何影响分布。</p><p id="64c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">重新解释 _ 批处理 _ndims: 0 </strong></p><ul class=""><li id="ad6e" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir"> batch_shape(4，)</strong> =四个正态分布</li><li id="973b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">样本形状:(4，)</strong> =四个样本</li><li id="3bd0" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">将它们放在一起:</strong>来自四个正态分布的四个样本。</li></ul><p id="eb95" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">重新解释 _ 批处理 _ndims: 1 → </strong>要重新解释为事件形状的批处理形状的数量。在我们的例子中是 1。</p><ul class=""><li id="d8f8" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir"> event_shape(4，)</strong>=多元正态分布。所以这个正态分布是由四个随机变量组成的。</li><li id="15d9" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">样本形状:(4，)</strong> =四个样本</li><li id="58a4" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">将它们放在一起:</strong>来自多元正态分布的四个样本。</li></ul><p id="85d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于后验概率需要是可训练的，我们将在下一部分中看到<strong class="ky ir">如何创建自定义后验概率。我不想在这篇文章中过多地描述 BNN 的具体部分。</strong></p><h2 id="91e4" class="nn mh iq bd mi nz oa dn mm ob oc dp mq lf od oe ms lj of og mu ln oh oi mw oj bi translated">逼近 KL 散度</h2><p id="b3a0" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">我不会在这里讨论数学，因为这超出了本文的范围。</p><p id="a918" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">KL 散度的蒙特卡罗近似可以实现如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="76a8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">出于演示目的，我们将使用这个近似函数。</p><h2 id="b071" class="nn mh iq bd mi nz oa dn mm ob oc dp mq lf od oe ms lj of og mu ln oh oi mw oj bi translated">创建模型</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="4dba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们将创建一个函数来获得<strong class="ky ir">convolution 2d 重新参数化</strong>层<strong class="ky ir">。</strong>由于这篇文章涵盖了基础知识，我们将使用<code class="fe nf ng nh ni b">mnist</code>数据集。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="33c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个模型中，我们使用默认的<strong class="ky ir">值作为<strong class="ky ir">后</strong>和<strong class="ky ir">前</strong>的值。我们将在本系列的下一部分</strong>中看到如何完全定制它们。</p><p id="609f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">输出</strong>也是一个<strong class="ky ir">分布对象(onehotcategorial)</strong>，所以我们可以直接使用<strong class="ky ir">负对数似然</strong>作为损失函数。这意味着我们也可以用这个模型捕捉任意的不确定性。</p><p id="9213" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里重要的一点，BNN 的<strong class="ky ir">总参数是普通 CNN<strong class="ky ir">型号</strong>的<strong class="ky ir">两倍</strong>。这是因为每个权重和偏差现在都有均值和方差，所以我们用<strong class="ky ir"> Bayes-by-Backprop </strong>学习这些值。这让我们能够代表模型参数的认知不确定性。</strong></p><p id="4e06" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于我们可以<strong class="ky ir">捕捉任意的和认知的不确定性</strong>，我们称这个模型为<strong class="ky ir">全概率贝叶斯神经网络。</strong></p><p id="19af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe nf ng nh ni b">divergence_fn</code>是我们为 KL 近似创建的函数。这里，我们可以使用 KL-Divergence 的分析值，因为先验和后验选择是具有均值和方差的正态分布。</p><p id="b742" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是贝叶斯 CNN 模型的一个缺点，它可能会变得计算密集，这取决于变分后验概率的选择。如果我们还想通过使用 <code class="fe nf ng nh ni b">MultivariateNormalTriL</code>来<strong class="ky ir">学习协方差，那么将会有更多的参数。</strong></p><h2 id="7d0b" class="nn mh iq bd mi nz oa dn mm ob oc dp mq lf od oe ms lj of og mu ln oh oi mw oj bi translated">分析模型预测和输出</h2><p id="deda" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">首先，让我们从数据集中抽取一些样本:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><p id="664b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将定义一个效用函数来分析预测和输出。现在，每次向前传递将给出不同的值，因为我们从模型参数中采样以获得预测结果。它可以被解释为集成分类器。</p><p id="5f54" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们创建空数组来存储输出。我选择<code class="fe nf ng nh ni b">forward_passes</code>为 10，你可以增加到任何合理的数字，比如 100 或者 200。<code class="fe nf ng nh ni b">forward_passes</code>定义从<strong class="ky ir">(第 8 行)</strong>模型中获取的样本数量。</p><p id="c13e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">模型输出一个分布对象，为了得到概率我们使用<code class="fe nf ng nh ni b">mean()</code>方法<strong class="ky ir">(第 10 行)</strong>。同样的事情也适用于<code class="fe nf ng nh ni b">extracted_std</code>。</p><p id="4d95" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每门课会有 10 个不同的预测。在绘制它们之前，我们独立地获得每个类别的 95%预测区间<strong class="ky ir">(第 31 行-第 33 行)</strong>。如果我们用 100 作为<code class="fe nf ng nh ni b">forward_passes</code>，那么将会有 100 种不同的预测。</p><p id="64d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">绘图过程很简单，我在这里不赘述。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="0c9b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们测试一下这个功能:</p><pre class="kg kh ki kj gt nj ni nk nl aw nm bi"><span id="5815" class="nn mh iq ni b gy no np l nq nr">analyse_model_prediction(example_images[284], example_labels[284])</span></pre><p id="ed2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将给出输出:</p><pre class="kg kh ki kj gt nj ni nk nl aw nm bi"><span id="f3a9" class="nn mh iq ni b gy no np l nq nr">Label 8 has the highest std in this prediction with the value 0.157</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/c121d8598d579581853db1d95b5d04e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5CFE4KqHKcpoSe7ub_RloQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="fa34" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在进入细节之前，考虑一下最好的情况。如果对于给定图像，模型在每次向前传递时输出 1.0，则计算 95%的预测间隔也将给出 1.0</p><p id="4b81" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，柱会更短，表明预测的不确定性较低。<strong class="ky ir">因此，关键的一点是，柱越高，不确定性就越高！</strong></p><p id="69c7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个预测中，我们看到模型将概率指定为 8。然而，它们之间的范围概率有明显的区别。在向前传递中，输出大约是 0.5，而在另一个传递中，输出是 1.0。<strong class="ky ir">因此，我们得出结论，模型对该预测不太确定。</strong></p><pre class="kg kh ki kj gt nj ni nk nl aw nm bi"><span id="1b89" class="nn mh iq ni b gy no np l nq nr">analyse_model_prediction(example_images[50], example_labels[50])</span></pre><p id="354c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输出如下所示:</p><pre class="kg kh ki kj gt nj ni nk nl aw nm bi"><span id="bd1f" class="nn mh iq ni b gy no np l nq nr">Label 0 has the highest std in this prediction with the value 0.001</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/7a11401ba83d2a12b97b1f479ee12886.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kBX32ksxVL8keOHZyPQu-w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="0089" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">瞧啊。模型在每次向前传递中分配了非常高的概率。我们说这个模型对这个预测是有把握的。</p><p id="d5be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">这个模型居然知道自己不知道的东西！</strong></p><p id="ba4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下结论之前，让我们添加随机噪声向量，看看它如何影响预测。</p><pre class="kg kh ki kj gt nj ni nk nl aw nm bi"><span id="e1d1" class="nn mh iq ni b gy no np l nq nr">noise_vector = np.random.uniform(size = (28, 28, 1), <br/>                                 low = 0, high = 0.5)</span><span id="8d0a" class="nn mh iq ni b gy ol np l nq nr"># Make sure that values are in (0, 1).<br/>noisy_image = np.clip(example_images[50] + noise_vector, 0, 1)</span><span id="eb3a" class="nn mh iq ni b gy ol np l nq nr">analyse_model_prediction(noisy_image, example_labels[50])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/27a532f2524dd3fda1ea122bad713aa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SFGP_98Yfi-iP94RfikAzw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="3ebe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">类 0 的概率仍然很高，但是检查结果可能是值得的。想象你正在使用一个贝叶斯 CNN，如果你得到这样的输出，你会怎么做或者你会怎么解释它？</p><p id="a5db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我的解释是，模型认为它属于 0 类，但是有一些不确定性，所以我可能会看一下。</p><p id="b8dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们增加图像中的噪声，使其更加扭曲，会发生什么？</p><pre class="kg kh ki kj gt nj ni nk nl aw nm bi"><span id="e4c2" class="nn mh iq ni b gy no np l nq nr">noise_vector = np.random.uniform(size = (28, 28, 1), <br/>                                 low = 0, high = 0.5)</span><span id="1b3b" class="nn mh iq ni b gy ol np l nq nr">noisy_image = np.clip(example_images[50] + noise_vector*2, 0, 1)<br/>analyse_model_prediction(noisy_image, example_labels[50])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/dc73e9770dfb778112eb199e5a695fb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VucF4VE6AwcP3fpjHzcN8A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="ea64" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们使用一个普通的 CNN 模型，对于某个类，输出可能仍然很高。这就是为什么正常的神经网络被认为对自己的预测过于自信。</p><p id="2298" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是贝叶斯 CNN 模型说它不能正确地分类这个图像。当它不知道的时候是可以的，这比分配一个错误的标签要好。我们可以通过观察高柱得到这个结论，有 3 个非常高的柱和 3 个中等高的柱。这足以说明不确定性很高。</p><p id="0482" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后一个样本，噪声矢量:</p><pre class="kg kh ki kj gt nj ni nk nl aw nm bi"><span id="74d6" class="nn mh iq ni b gy no np l nq nr">analyse_model_prediction(np.random.uniform(size = (28, 28, 1),<br/>                         low = 0, high = 1))</span></pre><p id="408d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将产生以下输出:</p><pre class="kg kh ki kj gt nj ni nk nl aw nm bi"><span id="f804" class="nn mh iq ni b gy no np l nq nr">Std Array: [0.27027504 0.22355586 0.19433676 0.08276099 0.1712302  0.14369398<br/> 0.31018993 0.13080781 0.47434729 0.18379491]</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/3e459b4f81281c2625215089934d2fe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WCG5T3fdOXByUrionuI_lg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="cef4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果是预料之中的，没有不确定性低的单一预测。这个模型说它不能正确分类这个向量。</p><h1 id="3d70" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">结论和下一步措施</h1><p id="8a9d" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在本文中，我们:</p><ul class=""><li id="22fe" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">查看<strong class="ky ir">convolution 2d 重新参数化</strong>层。</li><li id="17fa" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">如果 KL 散度不能用张量流概率进行解析计算，我看到了如何近似 KL 散度。</li><li id="0968" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">创造了一个完全概率的<strong class="ky ir">贝叶斯 CNN </strong>。</li></ul><p id="782a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以从这里得到密码和笔记本。</p><p id="1be2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">在下一部分</strong>中，我们将<strong class="ky ir">使用<strong class="ky ir">自定义先验和后验函数</strong>同时使用<strong class="ky ir">真实数据集</strong>自定义</strong>模型。</p></div></div>    
</body>
</html>