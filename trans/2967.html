<html>
<head>
<title>Self-Destructive RL Agents</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自毁RL药剂</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/suicidal-rl-agents-68159fc8f15a#2022-06-28">https://towardsdatascience.com/suicidal-rl-agents-68159fc8f15a#2022-06-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9afc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">对RL奖励函数的无害改变会产生令人惊讶的行为</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/54ec576aa410ee93b03455f4a8ecafff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2hqoDofIpXfOOK56htksEw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由来自Pixabay的Stefan Keller提供。</p></figure><p id="7c18" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">问题设置</strong></p><p id="b9c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">强化学习是AI/ML的一个流行分支，它试图通过与世界互动和最大化奖励信号来学习最佳行为。例如，当玩游戏时，代理将决定如何与世界互动，环境游戏将相应地给予奖励。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/739b7547f4f4db98e6bcefe15e852e4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*WcqnnCm05-WAp0RW1htD7Q.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">雅达利突破。这里代理人的奖励信号是游戏的分数。来自<a class="ae kv" href="https://www.gymlibrary.ml/environments/atari/breakout/" rel="noopener ugc nofollow" target="_blank"> OpenAI健身房</a>的环境。</p></figure><p id="03f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数学优化是寻找“最佳元素”的研究。这通常是寻找一个函数的最大值或最小值，并且可以附加约束。例如，监督学习通常被记为最小化神经网络的预测值和数据集的目标值之间的损失函数的任务。然后我们用随机梯度下降法来解决这个问题。</p><p id="27a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当第一次学习强化学习时，将它视为一个简单的优化问题是非常诱人的，但可能会产生误导。让我们假设一个有限的范围设置，这样RL的目标是找到一个策略π:S→A，最大化未贴现的总回报:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/8c93bdc8e06c2451cec83e120c2026aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*l8eEc0Gfu61X5Kz0N1cHAA.png"/></div></figure><p id="5606" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们使用有限的范围和γ=1来使讨论更容易理解，但是对于一般情况可以得到相同的教训。现在考虑一个新的奖励函数，它只是用一个常数抵消了原来的奖励:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lu"><img src="../Images/da3707816ab6ff1bfde78930aba24005.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/1*U5yixht7pjYxUJf16mL23g.png"/></div></div></figure><p id="5b6b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">天真地，我们可能会得出这样的结论:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lv"><img src="../Images/92097b0b5a1778cf296c48b14c497223.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IjOziNtLWmJz7CqWtNCl8A.png"/></div></div></figure><p id="f05c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果这是真的，那么原始奖励函数的最优策略π∫似乎与新的修改奖励的最优策略相同。代理人想要最大化的目标函数只是被值(N+1)c修改了，这个值应该是独立于其行为的吧？其实不一定是这样的！事实上，对于常数c的不同选择，可以观察到非常不同的行为。</p><p id="d4b6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是为什么呢？简而言之，N实际上是一个随机变量，也是与政策相关的。大多数RL环境都有终止情节的条件，代理的行为可以影响情节的持续时间。</p><p id="8b31" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看一下这个问题的一个具体实例，以便更好地理解发生了什么。</p></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><p id="634c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">直观的例子</strong></p><p id="46ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可能最容易玩深度RL和发展直觉的环境是横竿问题。概括一下:最初的奖励是为弹弓还活着的每个时间点分配r=+1，如果:</p><ol class=""><li id="0d4f" class="md me iq ky b kz la lc ld lf mf lj mg ln mh lr mi mj mk ml bi translated">杆的角度超过阈值:<br/> ∣θ∣ &gt; 12</li><li id="cc4b" class="md me iq ky b kz mm lc mn lf mo lj mp ln mq lr mi mj mk ml bi translated">推车出界:<br/> |x| &gt; 2.4</li><li id="94c2" class="md me iq ky b kz mm lc mn lf mo lj mp ln mq lr mi mj mk ml bi translated">达到时间限制(N=500)。</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/104359cb375d5771fa3c65baa43fef71.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*vLFP0bAvHYGXRlKfoCVcfg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自<a class="ae kv" href="https://www.gymlibrary.ml/environments/classic_control/cart_pole/" rel="noopener ugc nofollow" target="_blank">奥体馆</a>的翻筋斗环境。</p></figure><p id="2154" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了建立一个基线，让我们看看不同代理的训练表现，作为常数偏移c的函数。为此，我们使用<a class="ae kv" href="https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html" rel="noopener ugc nofollow" target="_blank">稳定基线3 </a>和MLP Q网络。下面是16个不同代理的平均训练曲线。训练在2，500，000步后停止(尽管可以实现更好的性能)，因为这只是为了说明正在发生的事情。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/024e8f66c3f7b19637153a9e12b5c108.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*6l236h_KuLabNo7iuugYow.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">DQN钢管舞训练——基础奖励(r=1)。图片作者。</p></figure><p id="bad1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这没什么奇怪的。代理正在学习如预期的那样保持杆子平衡。我手动调整的唯一非默认参数是设置<code class="fe mt mu mv mw b">exploration_fraction=0.2</code></p><p id="d08e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们改变奖励函数，再次训练。对于热身，我们设置c = 0.5，即在每步中，代理将获得r=0.5的奖励，直到剧集结束。然后，你认为会发生什么？为了进行比较，我们将绘制一段时间内环境的步数(恰好是原始奖励)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/88c68be6badafe091eb905be2fd9d752.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*5A129HhP8KJFYTXLtPVLJg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">DQN钢管舞训练，r=0.5 (c=-0.5)。图片作者。</p></figure><p id="d6fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们看到的，没有发生太戏剧性的事情。达到收敛的时间略有不同，但总的来说，代理人学会了最大化新的奖励，平均来说，所有代理人在大约150万步后学会了平衡极点。</p><p id="948e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">到目前为止，一切顺利。但是如果我们设置c = 2会发生什么呢？也就是说，在每一步，代理人都会得到r = 1的回报。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/c80e69934e14b79246db12b446c85b26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*960UokIzf48jCMjZILyFVA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">DQN钢管舞训练，r=-1 (c=-2)。图片作者。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/c192913d63188bc8a265bdea4d20d342.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/1*luqxNi-XpwMMBtd21n4gjw.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">悬赏黑客的典型例子！来自<a class="ae kv" href="https://www.gymlibrary.ml/environments/classic_control/cart_pole/" rel="noopener ugc nofollow" target="_blank"> OpenAI健身房</a>的环境。</p></figure><p id="01c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">完全不同的行为！实际上，代理人学会尽快结束这一集。它实际上变成了自我毁灭，对我们心中的任务毫无用处，即使奖励被一个常数修改了！当然，对此的解释来自于这样一个事实，即事件的长度是可变的，并且会受到代理人行为的影响。​</p></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><p id="fd95" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">总结思路</strong></p><p id="b79f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我觉得这种行为乍一看并不是最直观的。为什么这个代理无法学习平衡策略？毕竟，奖励函数只是被一个常数抵消了。原因是强化学习比传统优化要微妙复杂得多。在间歇式设置中，有些情况会干扰培训环境的运行。这意味着不能保证剧集能持续完整的N步。这就是为什么上述天真的简化不成立！</p><p id="a0d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，特别是代理人意识到，如果它<em class="my">尽快自毁</em>，那么负面奖励将停止累积！</p><blockquote class="mz na nb"><p id="9448" class="kw kx my ky b kz la jr lb lc ld ju le nc lg lh li nd lk ll lm ne lo lp lq lr ij bi translated">如果代理人只知道消极的回报，那么它会知道一个更短的插曲是减少它的整体痛苦的最好方法。</p></blockquote><p id="4844" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个故事的寓意很明确:</p><p id="d697" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="my">改变一个RL环境的奖励函数可以导致截然不同的最优策略，即使是一个不明显的变化，比如一个加法常数。</em> </strong></p><p id="0a5c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">澄清一下——令人惊讶的不是奖励函数本身的变化会导致不同的行为。毕竟，如果我们相信<a class="ae kv" href="http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html" rel="noopener ugc nofollow" target="_blank">奖励假说</a>，那么奖励函数就是编码最佳行为的东西。令人惊讶的是，即使是乘法或加法常数也能产生如此大的影响。</p><p id="3e39" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，您可能已经注意到，c = 0.5情况下的训练曲线看起来有所不同。用这种改进的奖励函数训练的代理收敛到相同的最优解，但速度不同。这与通过反向传播训练神经网络时梯度传播的方式有关，因此更多的是来自训练类型而不是来自RL的理论基础的人为因素。这就是为什么也有一个“奖励成形”领域，其中奖励函数被稍微调整，试图更快地收敛到期望的政策。</p></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><p id="985d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢阅读！</p></div></div>    
</body>
</html>