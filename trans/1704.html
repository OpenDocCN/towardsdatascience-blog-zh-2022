<html>
<head>
<title>Get Uncertainty Estimates in Regression Neural Networks for Free</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">免费获取回归神经网络中的不确定性估计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/get-uncertainty-estimates-in-neural-networks-for-free-48f2edb82c8f#2022-04-22">https://towardsdatascience.com/get-uncertainty-estimates-in-neural-networks-for-free-48f2edb82c8f#2022-04-22</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="a11a" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">给定正确的损失函数，标准的神经网络也可以输出不确定性</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/3f7d105ccb2d01a6120b580ec965dd5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*v-iLerKzzVVArXz7"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">Christina Deravedisian 在<a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="960c" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi lw translated"><span class="l lx ly lz bm ma mb mc md me di"> W </span>当我们建立一个机器学习模型时，我们通常以这样的方式设计它，它输出<strong class="lc iv">一个单一的数字</strong>作为预测。来自<a class="ae kz" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>的大多数模型是这样工作的:基于树的模型、线性模型、最近邻算法等等。同样适用于<a class="ae kz" href="https://xgboost.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>和其他boosting算法，以及深度学习框架，如<a class="ae kz" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>或<a class="ae kz" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>。</p><p id="e7ca" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">虽然这通常是好的，但最好也有一个围绕这个<em class="mf">点估计</em>的不确定性度量。这是因为“我们将销售1000辆50型汽车”和“我们将销售1000辆5000辆汽车”之间的差异是巨大的:从第一个陈述中，你可以得出该公司将销售1000辆左右的汽车，或多或少，而第二个陈述告诉你该车型根本没有任何线索。</p><blockquote class="mg mh mi"><p id="9465" class="la lb mf lc b ld le jv lf lg lh jy li mj lk ll lm mk lo lp lq ml ls lt lu lv in bi translated"><strong class="lc iv"> <em class="iu">注:</em> </strong> <em class="iu">较低的不确定性并不意味着模型是正确的。作为人，它也可以对一些完全错误的事情非常固执己见。因此，像往常一样，评估模型的质量是至关重要的，在这种情况下也是如此。</em></p></blockquote><h1 id="07e2" class="mm mn iu bd mo mp mq mr ms mt mu mv mw ka mx kb my kd mz ke na kg nb kh nc nd bi translated">贝叶斯推理</h1><p id="b65f" class="pw-post-body-paragraph la lb iu lc b ld ne jv lf lg nf jy li lj ng ll lm ln nh lp lq lr ni lt lu lv in bi translated">如果你读过我关于贝叶斯推理的文章(<em class="mf">谢谢！你已经知道如何创建模型，不仅输出一个点，而是输出一个完整的目标分布。</em></p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nj"><img src="../Images/f9cc4c5566007b715e7d0c13378e373b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*94lOvc2p1VWX97uigAWbYw.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图片由作者提供。</p></figure><div class="nk nl gq gs nm nn"><a rel="noopener follow" target="_blank" href="/a-gentle-introduction-to-bayesian-inference-6a7552e313cb"><div class="no ab fp"><div class="np ab nq cl cj nr"><h2 class="bd iv gz z fq ns fs ft nt fv fx it bi translated">贝叶斯推理的简明介绍</h2><div class="nu l"><h3 class="bd b gz z fq ns fs ft nt fv fx dk translated">了解频率主义者和贝叶斯推理方法之间的区别</h3></div><div class="nv l"><p class="bd b dl z fq ns fs ft nt fv fx dk translated">towardsdatascience.com</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob kt nn"/></div></div></a></div><div class="nk nl gq gs nm nn"><a rel="noopener follow" target="_blank" href="/bayesian-linear-regression-in-python-via-pymc3-ab8c2c498211"><div class="no ab fp"><div class="np ab nq cl cj nr"><h2 class="bd iv gz z fq ns fs ft nt fv fx it bi translated">通过PyMC3在Python中实现贝叶斯线性回归</h2><div class="nu l"><h3 class="bd b gz z fq ns fs ft nt fv fx dk translated">了解如何推断模型参数并对新数据进行预测，包括不确定性估计！</h3></div><div class="nv l"><p class="bd b dl z fq ns fs ft nt fv fx dk translated">towardsdatascience.com</p></div></div><div class="nw l"><div class="oc l ny nz oa nw ob kt nn"/></div></div></a></div><p id="db40" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">正因为如此，我们可以通过观察预测的分布或一些导出的数字(如标准差)来了解模型何时不确定。分布越窄(标准差越小)，模型越确定。</p><p id="3b28" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">不过，这一次我们不会去贝氏这里。</strong>虽然贝叶斯推理是您应该在某个时候学习的一个很好的领域，但它有几个缺点:</p><ul class=""><li id="423e" class="od oe iu lc b ld le lg lh lj of ln og lr oh lv oi oj ok ol bi translated">它在计算上甚至比神经网络更复杂</li><li id="5eba" class="od oe iu lc b ld om lg on lj oo ln op lr oq lv oi oj ok ol bi translated">数学上更难理解，而且</li><li id="30d1" class="od oe iu lc b ld om lg on lj oo ln op lr oq lv oi oj ok ol bi translated">你必须了解新的图书馆。</li></ul><p id="f06d" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">因此，这篇文章是为那些了解他们的深度学习框架，并希望在没有太多麻烦的情况下包括一些不确定性估计的人准备的。然而，如果你想在某个时候进入<strong class="lc iv">完全贝叶斯神经网络</strong>的领域，试试PyTorch的<a class="ae kz" href="https://www.tensorflow.org/probability" rel="noopener ugc nofollow" target="_blank"> Tensorflow Probability </a>或<a class="ae kz" href="https://pyro.ai/" rel="noopener ugc nofollow" target="_blank"> Pyro </a>之类的库。</p><h1 id="a38e" class="mm mn iu bd mo mp mq mr ms mt mu mv mw ka mx kb my kd mz ke na kg nb kh nc nd bi translated">使神经网络揭示它们的不确定性</h1><blockquote class="mg mh mi"><p id="051a" class="la lb mf lc b ld le jv lf lg lh jy li mj lk ll lm mk lo lp lq ml ls lt lu lv in bi translated"><strong class="lc iv">免责声明:</strong>同样，我不知道以下方法是否出现在任何论文或书籍中。它刚刚出现在我的脑海里，我想把它写下来。如果你知道任何来源，请在评论中给我一个提示，我会添加到文章中。谢谢！<br/> <strong class="lc iv"> <em class="iu">更新:</em> </strong> <a class="ae kz" href="https://www.linkedin.com/in/tetris/matias-valdenegro-toro-29383717/" rel="noopener ugc nofollow" target="_blank"> <em class="iu">马蒂亚斯·瓦尔德内格罗·托罗</em> </a> <em class="iu">指出我即将介绍的损失在他们的论文</em> 中被称为方差衰减<em class="iu"> </em> <a class="ae kz" href="https://arxiv.org/abs/2204.09308" rel="noopener ugc nofollow" target="_blank"> <em class="iu">。</em></a></p></blockquote><p id="0fa0" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">从现在开始，让我们坚持使用我们最喜欢的面包和黄油神经网络。在本文中，我将使用Tensorflow代码，但是您也可以轻松地将所有代码都用于PyTorch或其他框架。这里我们将考虑一个回归问题，但是类似的论点也可以用于分类任务。</p><h2 id="4f2e" class="or mn iu bd mo os ot dn ms ou ov dp mw lj ow ox my ln oy oz na lr pa pb nc pc bi translated">推导均方误差</h2><p id="628e" class="pw-post-body-paragraph la lb iu lc b ld ne jv lf lg nf jy li lj ng ll lm ln nh lp lq lr ni lt lu lv in bi translated">为了理解如何得到不确定性估计，我们必须先理解如何得到点估计。然后，我们将以一种简单的方式概括这一思想。因此，简单回顾一下，下面是<strong class="lc iv">均方误差</strong> (MSE)损失函数:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj pd"><img src="../Images/731baaa09d5b56471c7655503acb40c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r8exMUhyOe8dAO-eYt_tWA.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图片由作者提供。</p></figure><p id="05a5" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这很直观:某些真实值<em class="mf"> yᵢ </em>和模型预测值<em class="mf"> ŷᵢ </em>之间的差距越大，亏损就越高。但是，当指数中的2替换为4时，我们可以用同样的方式进行讨论。或者去掉2，用绝对值|<em class="mf">yᵢ</em>–<em class="mf">ŷᵢ|</em>代替(<a class="ae kz" href="https://en.wikipedia.org/wiki/Mean_absolute_error" rel="noopener ugc nofollow" target="_blank">平均绝对误差</a>，MAE)。</p><p id="6b28" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">那么，MSE有什么特别之处呢？其中包含哪些假设？让我们来找出答案。⚠️ <strong class="lc iv">危险:数学领先。如果这太多，直接跳到实现部分。这个结果很容易应用，即使你还不能理解这个理论。</strong> ⚠️</p></div><div class="ab cl pe pf hy pg" role="separator"><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj"/></div><div class="in io ip iq ir"><p id="4e3f" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">假设如下:</p><blockquote class="mg mh mi"><p id="1ca6" class="la lb mf lc b ld le jv lf lg lh jy li mj lk ll lm mk lo lp lq ml ls lt lu lv in bi translated"><em class="iu">给定输入特征</em> x <em class="iu">，真标</em> y <em class="iu">按正态分布</em><strong class="lc iv"><em class="iu"/></strong><em class="iu">用</em> <strong class="lc iv"> <em class="iu">表示</em>μ<em class="iu">(</em>x<em class="iu">)</em></strong><em class="iu"/><strong class="lc iv"><em class="iu">标准差</em></strong>，<em class="iu">即</em>y~N(μ <em class="iu"> ( </em> x <em class="iu">)，</em> σ)。<em class="iu">这意味着观察到的标签来自某个真值</em> μ <em class="iu"> ( </em> x <em class="iu">)，但被某个误差破坏，标准偏差为</em> σ。<em class="iu">这个误差也叫</em>噪声<em class="iu">。</em> <strong class="lc iv"> <em class="iu">注意，很多时候我们写</em> </strong> <em class="iu"> </em> <strong class="lc iv"> ŷ <em class="iu">而不是</em> μ <em class="iu"> ( </em> x <em class="iu">)。</em>T57】</strong></p></blockquote><p id="4faa" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">神经网络(和大多数其他模型)的任务是预测这个<strong class="lc iv"><em class="mf">μ</em>(x)</strong><em class="mf"/>给定<em class="mf"> x. </em>这使得平均预测正确<strong class="lc iv"/>，这是我们能做的最好的事情，因为我们不能预测噪声。<strong class="lc iv"> </strong>现在的表达<em class="mf">y</em>~<em class="mf">N</em>(<em class="mf">μ(</em>x<em class="mf">)，σ </em>)只是表示以下意思:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj pl"><img src="../Images/36bf76fcbcb617e1390c2ccf4eecaf70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*enrEnna4nyBDAaZSy99DGQ.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图片由作者提供。</p></figure><p id="866a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这只是用均值<em class="mf"> ŷ=μ( </em> x <em class="mf"> ) </em>和标准差<em class="mf"> σ </em>描述单个标签<em class="mf"> y. </em>分布的正态分布的密度函数，现在我们没有单个观测值<em class="mf"> y </em>及其对应的预测值<em class="mf"> ŷ </em>，而是几个，比如说<em class="mf"> n </em>。假设所有的观察都是随机独立的，我们得到</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj pm"><img src="../Images/60d4e54787b9d789b55d7107791588d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dAQNyix1pa6hYSqXCYPoew.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图片由作者提供。</p></figure><p id="019f" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">现在训练一个神经网络基本上意味着统计学家所谓的<em class="mf">最大似然估计。</em>这是一种比较花哨的说法<em class="mf"> </em>我们要<strong class="lc iv">最大化</strong>上面的密度函数，也叫<em class="mf">似然函数</em>。</p><p id="b568" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">现在，我们可以将最大似然估计与MSE最小化联系起来，如下所示:</p><ol class=""><li id="ed4f" class="od oe iu lc b ld le lg lh lj of ln og lr oh lv pn oj ok ol bi translated">最大化似然函数</li><li id="a804" class="od oe iu lc b ld om lg on lj oo ln op lr oq lv pn oj ok ol bi translated">意味着最大化最右边的项</li><li id="8d49" class="od oe iu lc b ld om lg on lj oo ln op lr oq lv pn oj ok ol bi translated">意味着最大化<em class="mf"> e </em>的指数</li><li id="a98e" class="od oe iu lc b ld om lg on lj oo ln op lr oq lv pn oj ok ol bi translated">意味着最小化指数中的和，</li><li id="ae50" class="od oe iu lc b ld om lg on lj oo ln op lr oq lv pn oj ok ol bi translated">意味着最小化MSE(除以<em class="mf"> n </em>不会改变最佳参数)。</li></ol><p id="429b" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">或者作为一张图片:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj po"><img src="../Images/5ba0f910062ef638b1af1a02a75605d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pV5_YuML2m-2pb7qezwkoQ.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图片由作者提供。</p></figure><h2 id="d790" class="or mn iu bd mo os ot dn ms ou ov dp mw lj ow ox my ln oy oz na lr pa pb nc pc bi translated">概化MSE损失</h2><p id="6e2a" class="pw-post-body-paragraph la lb iu lc b ld ne jv lf lg nf jy li lj ng ll lm ln nh lp lq lr ni lt lu lv in bi translated">如果你熬过了最后一节，那么恭喜你，你已经走得很远了，你几乎达到了你的目标！我们只需要做一个简单的观察:</p><blockquote class="pp"><p id="4aed" class="pq pr iu bd ps pt pu pv pw px py lv dk translated">我们将<em class="pz"> σ视为常数，在进行最大似然法时基本上将其忽略。</em></p></blockquote><p id="3d5d" class="pw-post-body-paragraph la lb iu lc b ld qa jv lf lg qb jy li lj qc ll lm ln qd lp lq lr qe lt lu lv in bi translated">但是<em class="mf"> σ </em>也正是我们想要估计的！这是因为它从定义上抓住了预测中的不确定性。那么，我们让我们的模型<strong class="lc iv">在<em class="mf"> μ </em> ( <em class="mf"> x </em> ) </strong>之外再输出一个值<em class="mf"> σ </em> ( <em class="mf"> x </em>)怎么样？这意味着，即使对于简单的回归，模型也将有两个输出:一个是对真实值<em class="mf"> μ </em> ( <em class="mf"> x </em>)的估计，另一个是给定<em class="mf"> x </em>的不确定性估计<em class="mf"> σ </em> ( <em class="mf"> x </em>)。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj qf"><img src="../Images/82e645db3aaa331830446124019a001a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8BHLsk0KPKopgEgcG9Z63g.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图片由作者提供。</p></figure><p id="b01c" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">现在，我们可以用<em class="mf"> σ </em> ( <em class="mf"> xᵢ </em>)代替上面等式中的所有<em class="mf"> σ </em>，我们最终得到如下陈述:最大化似然函数意味着最大化该项</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj qg"><img src="../Images/f5cd52a7cd9c994ac5a527045c3d975f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MpJhd5ZEC4A-yQUdG8SHiw.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图片由作者提供。</p></figure><p id="8c66" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这反过来意味着最小化指数中的巨大总和，这是我们最新导出的损失函数(没有吸引人的名字，请在评论中提出建议)😉):</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj qh"><img src="../Images/0bb804a0134b39b4987c5eda1267d6c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lgiTAmIVy_gDOTxabAmejQ.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图片由作者提供。</p></figure><p id="4d58" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">注意，我在中加入了1/ <em class="mf"> n </em>，但是这并没有改变最优解，就像MSE的情况一样。</p><blockquote class="mg mh mi"><p id="6373" class="la lb mf lc b ld le jv lf lg lh jy li mj lk ll lm mk lo lp lq ml ls lt lu lv in bi translated"><strong class="lc iv"> <em class="iu">注:</em> </strong> <em class="iu">这种损失有一些有趣的性质。首先，它仍然包含MSE位(</em>yᵢ<em class="iu">–</em>μ<em class="iu">(</em>xᵢ<em class="iu">))。此外，还有两个涉及</em> σ <em class="iu">的术语:ln( </em> σ <em class="iu"> ( </em> x <em class="iu">))以及1/ </em> σ <em class="iu"> ( </em> x <em class="iu">)。</em></p><p id="6ba3" class="la lb mf lc b ld le jv lf lg lh jy li mj lk ll lm mk lo lp lq ml ls lt lu lv in bi translated"><em class="iu">为了保持较低的损耗，</em> <strong class="lc iv"> <em class="iu">型号不能输出非常大的值给</em><em class="iu">(</em>×63)</strong><em class="iu">，因为随着</em>σ<em class="iu">(</em>×T70)的增长，ln(σ<em class="iu">(</em>×T74】)增加该模型也不能输出接近于零的非常小的值，因为这样一来项1/  σ <em class="iu"> ( </em> x <em class="iu">)就变大了。这样，模型被迫输出一个</em>合理的<em class="iu">猜测为</em>σ<em class="iu">(</em>x<em class="iu">)</em><em class="iu">到</em> <strong class="lc iv"> <em class="iu">平衡两项的罚值</em> </strong> <em class="iu">。</em></p><p id="87fd" class="la lb mf lc b ld le jv lf lg lh jy li mj lk ll lm mk lo lp lq ml ls lt lu lv in bi translated">只有当(yᵢ<em class="iu">—</em>μ<em class="iu">(</em>xᵢ<em class="iu">))很小，即预测值非常接近真实值时，模型才能够输出很小的标准差</em> σ <em class="iu"> ( </em> x <em class="iu">)。在这种情况下，模型对其预测相当有把握。</em></p></blockquote><p id="b998" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">好了，理论到此为止。我们现在应该编码了！</p><h1 id="39d8" class="mm mn iu bd mo mp mq mr ms mt mu mv mw ka mx kb my kd mz ke na kg nb kh nc nd bi translated">Tensorflow中的实现</h1><p id="1659" class="pw-post-body-paragraph la lb iu lc b ld ne jv lf lg nf jy li lj ng ll lm ln nh lp lq lr ni lt lu lv in bi translated">好了，我们已经知道，我们需要两样东西来制造一个标准的神经网络输出不确定性:</p><ol class=""><li id="17e2" class="od oe iu lc b ld le lg lh lj of ln og lr oh lv pn oj ok ol bi translated">包含预测标准偏差(=不确定性)的第二个输出节点，以及</li><li id="2747" class="od oe iu lc b ld om lg on lj oo ln op lr oq lv pn oj ok ol bi translated">如上所述的客户损失函数。</li></ol><p id="01dc" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在你选择的任何深度学习框架中实现这两件事应该都很容易。我们将在Tensorflow中进行，因为上次我已经选择了PyTorch来解释可解释的神经网络。😎</p><div class="nk nl gq gs nm nn"><a rel="noopener follow" target="_blank" href="/interpretable-neural-networks-with-pytorch-76f1c31260fe"><div class="no ab fp"><div class="np ab nq cl cj nr"><h2 class="bd iv gz z fq ns fs ft nt fv fx it bi translated">PyTorch可解释神经网络</h2><div class="nu l"><h3 class="bd b gz z fq ns fs ft nt fv fx dk translated">了解如何使用PyTorch构建可通过设计解释的前馈神经网络</h3></div><div class="nv l"><p class="bd b dl z fq ns fs ft nt fv fx dk translated">towardsdatascience.com</p></div></div><div class="nw l"><div class="qi l ny nz oa nw ob kt nn"/></div></div></a></div><p id="30c6" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">让我们从一个简单的例子开始。</p><h2 id="84fe" class="or mn iu bd mo os ot dn ms ou ov dp mw lj ow ox my ln oy oz na lr pa pb nc pc bi translated">不断的噪音</h2><p id="be8d" class="pw-post-body-paragraph la lb iu lc b ld ne jv lf lg nf jy li lj ng ll lm ln nh lp lq lr ni lt lu lv in bi translated">首先，我们将创建一个由1000个点组成的玩具数据集，通过</p><pre class="kk kl km kn gu qj qk ql bn qm qn bi"><span id="be1c" class="qo mn iu qk b be qp qq l qr qs">import tensorflow as tf<br/><br/>tf.random.set_seed(0)<br/><br/>X = tf.random.uniform(minval=-1, maxval=7, shape=(1000,))<br/>y = tf.sin(X) + tf.random.normal(mean=0, stddev=0.3, shape=(1000,))</span></pre><p id="24b2" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我们可以将这个数据集可视化:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj qt"><img src="../Images/f433d41ea7a1b81ee71ab2b555f31d21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h6yIjU6TnL-m_BZyD3MZZA.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图片由作者提供。</p></figure><p id="c577" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">好的，所以它仅仅是一个正弦波加上了<em class="mf"> N </em> (0，0.3)分布噪声。在最好的情况下，模型的实际预测遵循正弦波，而每个不确定性估计在0.3左右。我们通过以下方式建立一个简单的前馈网络</p><pre class="kk kl km kn gu qj qk ql bn qm qn bi"><span id="ceef" class="qo mn iu qk b be qp qq l qr qs">model = tf.keras.Sequential([<br/>    tf.keras.layers.Dense(32, activation='relu'),<br/>    tf.keras.layers.Dense(32, activation='relu'),<br/>    tf.keras.layers.Dense(32, activation='relu'),<br/>    tf.keras.layers.Dense(2) # Output = (μ, ln(σ))<br/>])</span></pre><p id="9a21" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">好了，我们已经通过定义一个有两个输出的神经网络处理了第一个要素。</p></div><div class="ab cl pe pf hy pg" role="separator"><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj"/></div><div class="in io ip iq ir"><p id="4442" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">为了简化计算，我们直接假设第二个输出不是<em class="mf"> σ </em> ( <em class="mf"> xᵢ </em>)，而是ln( <em class="mf"> σ </em> ( <em class="mf"> xᵢ </em>))。</strong>我们这样做是因为最后一层的两个神经元可以输出任意的实数值，尤其是比零小<strong class="lc iv">的数值，这对标准差没有意义。但是标准偏差的对数可以是任何实数，因此域匹配。而且我们无论如何都需要损失函数中的ln( <em class="mf"> σ </em> ( <em class="mf"> xᵢ </em>))，那就去争取吧。说到损失函数，我们可以通过</strong></p><pre class="kk kl km kn gu qj qk ql bn qm qn bi"><span id="9a0d" class="qo mn iu qk b be qp qq l qr qs">def loss(y_true, y_pred):<br/>    mu = y_pred[:, :1] # first output neuron<br/>    log_sig = y_pred[:, 1:] # second output neuron<br/>    sig = tf.exp(log_sig) # undo the log<br/>    <br/>    return tf.reduce_mean(2*log_sig + ((y_true-mu)/sig)**2)</span></pre><p id="cffa" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">剩下的就是一切照旧了。你用这个损失函数和拟合来编译模型。</p><pre class="kk kl km kn gu qj qk ql bn qm qn bi"><span id="f388" class="qo mn iu qk b be qp qq l qr qs">model.compile(loss=loss)<br/><br/>model.fit(<br/>    tf.reshape(X, shape=(1000, 1)),<br/>    tf.reshape(y, shape=(1000, 1)),<br/>    batch_size=32,<br/>    epochs=100<br/>)</span></pre><p id="4217" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">让我们检查模型已经学习的不确定性估计:</p><pre class="kk kl km kn gu qj qk ql bn qm qn bi"><span id="12db" class="qo mn iu qk b be qp qq l qr qs">print(tf.exp(model(X)[:20, 1]))<br/><br/># Output:<br/># tf.Tensor(<br/># [0.29860803 0.27371496 0.32216415 0.32288837 0.31084406 0.30166912<br/># 0.32059005 0.3331769  0.31244662 0.31863096 0.30940703 0.32042852<br/># 0.3231969  0.29584357 0.31141806 0.32493973 0.3169802  0.32060665<br/># 0.30542135 0.31733593], shape=(20,), dtype=float32)</span></pre><p id="11b2" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我看不错！该模型了解到噪声具有大约0.3的标准偏差。这是模型所学内容的可视化:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj qt"><img src="../Images/73239f49346e7b6a017ddf3049391f1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0DG4OdcWHLHMDl1XBGwU8w.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图片由作者提供。</p></figure><p id="f451" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我们就喜欢这样。实际预测<em class="mf"> μ </em> <strong class="lc iv"> <em class="mf"> </em> </strong>遵循数据，而不确定性<em class="mf"> σ </em>刚好高到足以捕捉标签<em class="mf"> y </em>中的噪声。</p><h2 id="7184" class="or mn iu bd mo os ot dn ms ou ov dp mw lj ow ox my ln oy oz na lr pa pb nc pc bi translated">变化的噪声</h2><p id="d695" class="pw-post-body-paragraph la lb iu lc b ld ne jv lf lg nf jy li lj ng ll lm ln nh lp lq lr ni lt lu lv in bi translated">我们现在通过引入非恒定噪声来增加一点趣味，统计学家称之为<em class="mf">异方差</em>。看看这个:</p><pre class="kk kl km kn gu qj qk ql bn qm qn bi"><span id="dd16" class="qo mn iu qk b be qp qq l qr qs">tf.random.set_seed(0)<br/><br/>X = tf.random.uniform(minval=-1, maxval=7, shape=(1000,))<br/>sig = 0.1*(X+1)<br/>y = tf.sin(X) + tf.random.normal(mean=0, stddev=sig, shape=(1000,))</span></pre><p id="51d8" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这创建了一个数据集，其特征<em class="mf"> X </em>中的噪声增加。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj qu"><img src="../Images/82415eca52a0a3c477a03342a2468c6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UeHWPEcLA-oRQku3hl81Cw.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图片由作者提供。</p></figure><p id="e3ec" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">基本事实仍然是一样的:它是一个正弦波，模型应该能够捕捉到这一点。然而，该模型还应该知道，<em class="mf"> X </em>的值越高，意味着不确定性越高。</p><p id="b77c" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">剧透:如果你在新的数据集上重新训练如上相同的模型，这正是你将看到的。</strong></p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj qu"><img src="../Images/bc4c7e78f56b9c4351faac668508c9a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1MQGjsRfzhcSfXnsdGr4yw.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图片由作者提供。</p></figure><p id="8656" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在我看来很可爱。</p><h1 id="5032" class="mm mn iu bd mo mp mq mr ms mt mu mv mw ka mx kb my kd mz ke na kg nb kh nc nd bi translated">结论</h1><p id="998b" class="pw-post-body-paragraph la lb iu lc b ld ne jv lf lg nf jy li lj ng ll lm ln nh lp lq lr ni lt lu lv in bi translated">在本文中，您了解了如何调整神经网络，使其能够输出对不确定性的估计以及实际预测。它只需要一个额外的输出神经元和一个比MSE稍微复杂一点的损失函数。</p><p id="baa4" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">不确定性估计的好处是，它们让您评估模型对其预测的信心，您知道是否可以信任模型的预测。它们还允许您报告估计的下限或上限，这在计算最好或最坏的情况时很有价值。</p><p id="ca52" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">另一种获得不确定性估计的流行方法是使用贝叶斯推理。然而，数学更复杂，而且比我在这里向你展示的解决方案要慢得多。此外，我发现(深度)贝叶斯学习的软件包目前不像Tensorflow或PyTorch那样容易使用，尽管这可能会在贝叶斯方法获得更多牵引力时发生变化。不过，我喜欢这个话题，所以也来看看吧！😉</p><p id="f436" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我在这里给你的是一个简单的工具，让你避开贝叶斯的争论，不需要你改变太多的日常行为，同时仍然给你一个贝叶斯世界的巨大好处。</p><p id="5f16" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">奖金(感谢</strong> <a class="ae kz" href="https://www.linkedin.com/in/carlosayam/" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iv">的巨大投入)卡洛斯阿雅-莫雷诺</strong></a><strong class="lc iv">:</strong>获得不确定性估计的另一种方法是使用自举。基本上，如果你使用随机森林，这就是正在发生的事情:你通过子采样从你的原始数据集创建<em class="mf"> b </em>个更小的数据集，在每个数据集上训练一个模型，然后你得到<em class="mf"> b </em>个输入的不同预测。这些<em class="mf"> b </em>预测的平均值是你的最终预测，而这些<em class="mf"> b </em>预测的标准差是不确定性的度量。例如，如果所有<em class="mf"> b </em>模型的输出都差不多，那么不确定性就会很小。</p><p id="bcc9" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">然而，这种方法的问题是，你需要训练不同的车型，这可能相当昂贵。在随机森林中，它工作得很好，因为单个决策树快速且容易适应。对于神经网络来说，事情看起来更黑暗。杰里米·尼克松(Jeremy Nixon)等人也做了<a class="ae kz" href="https://www.gatsby.ucl.ac.uk/~balaji/why_arent_bootstrapped_neural_networks_better.pdf" rel="noopener ugc nofollow" target="_blank">研究</a>，即使抛开计算问题，自举神经网络可能也不会太有益。</p></div><div class="ab cl pe pf hy pg" role="separator"><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj"/></div><div class="in io ip iq ir"><p id="775f" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我希望你今天学到了新的、有趣的、有用的东西。感谢阅读！</p><p id="46a5" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">作为最后一点，如果你</strong></p><ol class=""><li id="5341" class="od oe iu lc b ld le lg lh lj of ln og lr oh lv pn oj ok ol bi translated"><strong class="lc iv">想支持我多写点机器学习和</strong></li><li id="432c" class="od oe iu lc b ld om lg on lj oo ln op lr oq lv pn oj ok ol bi translated"><strong class="lc iv">无论如何都计划获得一个中等订阅，</strong></li></ol><p id="9b3c" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">为什么不做</strong> <a class="ae kz" href="https://dr-robert-kuebler.medium.com/membership" rel="noopener"> <strong class="lc iv">通过这个环节</strong> </a> <strong class="lc iv">？这将对我帮助很大！😊</strong></p><p id="fa1c" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><em class="mf">说白了，给你的价格不变，但大约一半的订阅费直接归我。</em></p><p id="925f" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">非常感谢，如果你考虑支持我的话！</p><blockquote class="pp"><p id="442d" class="pq pr iu bd ps pt pu pv pw px py lv dk translated"><em class="pz">如有问题，在</em><a class="ae kz" href="https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/" rel="noopener ugc nofollow" target="_blank"><em class="pz">LinkedIn</em></a><em class="pz">上写我！</em></p></blockquote></div></div>    
</body>
</html>