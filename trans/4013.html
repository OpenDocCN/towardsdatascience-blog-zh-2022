<html>
<head>
<title>Smoothly Transition from Pandas to PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从熊猫到PySpark的平稳过渡</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/equivalents-between-pandas-and-pyspark-c8b5ba57dc1d#2022-09-07">https://towardsdatascience.com/equivalents-between-pandas-and-pyspark-c8b5ba57dc1d#2022-09-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bdf5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">熊猫大战PySpark 101</h2></div><p id="1b3c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作者:<a class="ae le" href="https://medium.com/u/d38873cbc5aa?source=post_page-----40d1ab7243c2--------------------------------" rel="noopener">阿玛尔·哈斯尼</a> &amp; <a class="ae le" href="https://medium.com/u/7f47bdb8b8c0?source=post_page-----40d1ab7243c2--------------------------------" rel="noopener">迪亚·赫米拉</a></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/8a29eda56a3a3ff4103de32036e263b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EDJcxpPju9x60ddilPi0NQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@jeremythomasphoto?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">杰瑞米·托马斯</a>在<a class="ae le" href="https://unsplash.com/s/photos/change?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="7b01" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">熊猫是每个数据科学家的首选图书馆。对于每个希望操作数据和执行一些数据分析的人来说，这是必不可少的。</p><p id="7655" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，尽管它的实用性和广泛的功能，我们很快就开始看到它在处理大型数据集时的局限性。在这种情况下，过渡到PySpark变得至关重要，因为它提供了在多台机器上运行操作的可能性，不像Pandas。在本文中，我们将提供PySpark中pandas方法的等效方法，以及现成的代码片段，以便于PySpark新手完成任务😉</p><blockquote class="lv"><p id="6fac" class="lw lx it bd ly lz ma mb mc md me ld dk translated">PySpark提供了在多台机器上运行操作的可能性，不像Pandas</p></blockquote><pre class="mf mg mh mi mj mk ml mm mn aw mo bi"><span id="6918" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">Table Of Contents</strong><br/>· <a class="ae le" href="#8396" rel="noopener ugc nofollow">DataFrame creation</a><br/>· <a class="ae le" href="#dfc7" rel="noopener ugc nofollow">Specifying columns types</a><br/>· <a class="ae le" href="#7382" rel="noopener ugc nofollow">Reading and writing files</a><br/>· <a class="ae le" href="#710b" rel="noopener ugc nofollow">Filtering</a><br/>    ∘ <a class="ae le" href="#50d3" rel="noopener ugc nofollow">Specific columns</a><br/>    ∘ <a class="ae le" href="#f04d" rel="noopener ugc nofollow">Specific lines</a><br/>    ∘ <a class="ae le" href="#e50f" rel="noopener ugc nofollow">Using a condition</a><br/>· <a class="ae le" href="#901e" rel="noopener ugc nofollow">Add a column</a><br/>· <a class="ae le" href="#6990" rel="noopener ugc nofollow">Concatenate dataframes</a><br/>    ∘ <a class="ae le" href="#0b25" rel="noopener ugc nofollow">Two Dataframes</a><br/>    ∘ <a class="ae le" href="#da7a" rel="noopener ugc nofollow">Multiple Dataframes</a><br/>· <a class="ae le" href="#a849" rel="noopener ugc nofollow">Computing specified statistics</a><br/>· <a class="ae le" href="#8b6c" rel="noopener ugc nofollow">Aggregations</a><br/>· <a class="ae le" href="#27da" rel="noopener ugc nofollow">Apply a transformation over a column</a></span></pre></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="d76e" class="nc mq it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">入门指南</h1><p id="aca3" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">在深入研究对等物之前，我们首先需要为后面做准备。不言而喻，第一步是导入所需的库:</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="861a" class="mp mq it ml b gy mr ms l mt mu">import pandas as pd<br/>import pyspark.sql.functions as F</span></pre><p id="fce1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PySpark功能的入口点是SparkSession类。通过SparkSession实例，您可以创建数据帧、应用各种转换、读写文件等。要定义SparkSession，您可以使用以下内容:</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="d29b" class="mp mq it ml b gy mr ms l mt mu">from pyspark.sql import SparkSession<br/>spark = SparkSession\<br/>.builder\<br/>.appName('SparkByExamples.com')\<br/>.getOrCreate()</span></pre><p id="151b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在一切都准备好了，让我们直接进入熊猫大战PySpark的部分吧！</p><h1 id="8396" class="nc mq it bd nd ne ny ng nh ni nz nk nl jz oa ka nn kc ob kd np kf oc kg nr ns bi translated">数据帧创建</h1><p id="ad0d" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">首先，让我们定义一个我们将使用的数据样本:</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="50bc" class="mp mq it ml b gy mr ms l mt mu">columns = ["employee","department","state","salary","age"]<br/>data = [("Alain","Sales","Paris",60000,34),<br/>        ("Ahmed","Sales","Lyon",80000,45),<br/>        ("Ines","Sales","Nice",55000,30),<br/>        ("Fatima","Finance","Paris",90000,28),<br/>        ("Marie","Finance","Nantes",100000,40)]</span></pre><p id="e78a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要创建一只<strong class="kk iu">熊猫</strong> <code class="fe od oe of ml b"> DataFrame</code>，我们可以使用下面的:</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="e55b" class="mp mq it ml b gy mr ms l mt mu">df = pd.DataFrame(data=data, columns=columns)<br/><br/># Show a few lines<br/>df.head(2)</span></pre><p id="bc70" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> PySpark </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="07b5" class="mp mq it ml b gy mr ms l mt mu">df = spark.createDataFrame(data).toDF(*columns)<br/><br/># Show a few lines<br/>df.limit(2).show()</span></pre><h1 id="dfc7" class="nc mq it bd nd ne ny ng nh ni nz nk nl jz oa ka nn kc ob kd np kf oc kg nr ns bi translated">指定列类型</h1><p id="0453" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated"><strong class="kk iu">熊猫</strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="ced4" class="mp mq it ml b gy mr ms l mt mu">types_dict = {<br/>    "employee": pd.Series([r[0] for r in data], dtype='str'),<br/>    "department": pd.Series([r[1] for r in data], dtype='str'),<br/>    "state": pd.Series([r[2] for r in data], dtype='str'),<br/>    "salary": pd.Series([r[3] for r in data], dtype='int'),<br/>    "age": pd.Series([r[4] for r in data], dtype='int')<br/>}<br/><br/>df = pd.DataFrame(types_dict)</span></pre><p id="b565" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以通过执行以下代码行来检查您的类型:</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="5204" class="mp mq it ml b gy mr ms l mt mu">df.dtypes</span></pre><p id="c242" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> PySpark </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="0ef8" class="mp mq it ml b gy mr ms l mt mu">from pyspark.sql.types import StructType,StructField, StringType, IntegerType<br/><br/>schema = StructType([ \<br/>    StructField("employee",StringType(),True), \<br/>    StructField("department",StringType(),True), \<br/>    StructField("state",StringType(),True), \<br/>    StructField("salary", IntegerType(), True), \<br/>    StructField("age", IntegerType(), True) \<br/>  ])<br/><br/>df = spark.createDataFrame(data=data,schema=schema)</span></pre><p id="6fa4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以通过执行以下命令来检查数据帧的模式:</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="ccae" class="mp mq it ml b gy mr ms l mt mu">df.dtypes<br/># OR<br/>df.printSchema()</span></pre><h1 id="7382" class="nc mq it bd nd ne ny ng nh ni nz nk nl jz oa ka nn kc ob kd np kf oc kg nr ns bi translated">读取和写入文件</h1><p id="db75" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">熊猫和PySpark的阅读和写作是如此的相似。语法如下:<strong class="kk iu">熊猫</strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="9a64" class="mp mq it ml b gy mr ms l mt mu">df = pd.read_csv(path, sep=';', header=True)<br/>df.to_csv(path, ';', index=False)</span></pre><p id="4f7d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> PySpark </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="be3c" class="mp mq it ml b gy mr ms l mt mu">df = spark.read.csv(path, sep=';')<br/>df.coalesce(n).write.mode('overwrite').csv(path, sep=';')</span></pre><p id="5b48" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">注1💡:</strong>您可以指定要对其进行分区的列:</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="04a8" class="mp mq it ml b gy mr ms l mt mu">df.partitionBy("department","state").write.mode('overwrite').csv(path, sep=';')</span></pre><p id="70ba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">注2💡:</strong>您可以通过在上面的所有代码行中更改CSV by parquet来读写不同的格式，比如parquet格式</p><h1 id="710b" class="nc mq it bd nd ne ny ng nh ni nz nk nl jz oa ka nn kc ob kd np kf oc kg nr ns bi translated">过滤</h1><h2 id="50d3" class="mp mq it bd nd og oh dn nh oi oj dp nl kr ok ol nn kv om on np kz oo op nr oq bi translated">特定列</h2><p id="c6c6" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">选择熊猫中的某些列是这样完成的:<strong class="kk iu">熊猫</strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="062f" class="mp mq it ml b gy mr ms l mt mu">columns_subset = ['employee', 'salary']<br/><br/>df[columns_subset].head()<br/><br/>df.loc[:, columns_subset].head()</span></pre><p id="50c8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">而在PySpark中，我们需要对列列表使用select方法:<strong class="kk iu"> PySpark </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="de23" class="mp mq it ml b gy mr ms l mt mu">columns_subset = ['employee', 'salary']<br/><br/>df.select(columns_subset).show(5)</span></pre><h2 id="f04d" class="mp mq it bd nd og oh dn nh oi oj dp nl kr ok ol nn kv om on np kz oo op nr oq bi translated">特定线路</h2><p id="7691" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">要选择一系列线条，您可以在Pandas中使用<code class="fe od oe of ml b">iloc</code>方法:</p><p id="d2e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">熊猫</strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="334f" class="mp mq it ml b gy mr ms l mt mu"># Take a sample ( first 2 lines )<br/><br/>df.iloc[:2].head()</span></pre><p id="6218" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在Spark中，不可能获得任何范围的行号。然而，可以像这样选择前n行:</p><p id="f102" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> PySpark </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="2cf3" class="mp mq it ml b gy mr ms l mt mu">df.take(2).head()<br/># Or<br/>df.limit(2).head()</span></pre><p id="6483" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">注</strong>💡:请记住spark，数据可能分布在不同的计算节点上，并且“第一”行可能会因运行而异，因为没有底层顺序</p><h2 id="e50f" class="mp mq it bd nd og oh dn nh oi oj dp nl kr ok ol nn kv om on np kz oo op nr oq bi translated">使用条件</h2><p id="5916" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">可以根据特定条件过滤数据。Pandas中的语法如下:</p><p id="2e9e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">熊猫</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="1f8f" class="mp mq it ml b gy mr ms l mt mu"># First method<br/>flt = (df['salary'] &gt;= 90_000) &amp; (df['state'] == 'Paris')<br/>filtered_df = df[flt]<br/><br/># Second Method: Using query which is generally faster<br/>filtered_df = df.query('(salary &gt;= 90_000) and (state == "Paris")')<br/># Or<br/>target_state = "Paris"<br/>filtered_df = df.query('(salary &gt;= 90_000) and (state == @target_state)')</span></pre><p id="14c3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在Spark中，通过使用<code class="fe od oe of ml b">filter</code>方法或执行SQL查询可以得到相同的结果。语法如下:</p><p id="3393" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> PySpark </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="e4b9" class="mp mq it ml b gy mr ms l mt mu"># First method<br/>filtered_df = df.filter((F.col('salary') &gt;= 90_000) &amp; (F.col('state') == 'Paris'))<br/><br/># Second Method:<br/>df.createOrReplaceTempView("people")<br/><br/>filtered_df = spark.sql("""<br/>SELECT * FROM people<br/>WHERE (salary &gt;= 90000) and (state == "Paris")<br/>""") <br/><br/># OR<br/>filtered_df = df.filter(F.expr('(salary &gt;= 90000) and (state == "Paris")'))</span></pre><h1 id="901e" class="nc mq it bd nd ne ny ng nh ni nz nk nl jz oa ka nn kc ob kd np kf oc kg nr ns bi translated">添加列</h1><p id="6949" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">在Pandas中，有几种方法可以添加列:</p><p id="7251" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">熊猫</strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="db78" class="mp mq it ml b gy mr ms l mt mu">seniority = [3, 5, 2, 4, 10]<br/># Method 1<br/>df['seniority'] = seniority<br/><br/># Method 2<br/>df.insert(2, "seniority", seniority, True)</span></pre><p id="9b41" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在PySpark中有一个叫做<code class="fe od oe of ml b">withColumn</code>的特殊方法，可以用来添加一个列:</p><p id="b90e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PySpark </p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="b195" class="mp mq it ml b gy mr ms l mt mu">from itertools import chain</span><span id="1915" class="mp mq it ml b gy or ms l mt mu">seniority= {<br/>    'Alain': 3,<br/>    'Ahmed': 5,<br/>    'Ines': 2,<br/>    'Fatima': 4,<br/>    'Marie': 10,<br/>}</span><span id="1415" class="mp mq it ml b gy or ms l mt mu">mapping = create_map([lit(x) for x in chain(*seniority.items())])</span><span id="7138" class="mp mq it ml b gy or ms l mt mu">df.withColumn('<!-- -->seniority<!-- -->', mapping.getItem(F.col("employee")))</span></pre><h1 id="6990" class="nc mq it bd nd ne ny ng nh ni nz nk nl jz oa ka nn kc ob kd np kf oc kg nr ns bi translated">连接数据帧</h1><h2 id="0b25" class="mp mq it bd nd og oh dn nh oi oj dp nl kr ok ol nn kv om on np kz oo op nr oq bi translated">两个数据帧</h2><p id="29a2" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated"><strong class="kk iu">熊猫</strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="5115" class="mp mq it ml b gy mr ms l mt mu">df_to_add = pd.DataFrame(data=[("Robert","Advertisement","Paris",55000,27)], columns=columns)<br/>df = pd.concat([df, df_to_add], ignore_index = True)</span></pre><p id="4121" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> PySpark </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="2749" class="mp mq it ml b gy mr ms l mt mu">df_to_add = spark.createDataFrame([("Robert","Advertisement","Paris",55000,27)]).toDF(*columns)<br/>df = df.union(df_to_add)</span></pre><h2 id="da7a" class="mp mq it bd nd og oh dn nh oi oj dp nl kr ok ol nn kv om on np kz oo op nr oq bi translated">多个数据帧</h2><p id="18ce" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">熊猫</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="6aa8" class="mp mq it ml b gy mr ms l mt mu">dfs = [df, df1, df2,...,dfn]<br/>df = pd.concat(dfs, ignore_index = True)</span></pre><p id="8b02" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PySpark的方法<code class="fe od oe of ml b">unionAll</code>只连接了两个数据帧。解决这一限制的方法是根据需要多次迭代连接。为了获得更简洁优雅的语法，我们将避免循环，并使用reduce方法来应用<code class="fe od oe of ml b">unionAll</code>:</p><p id="ca98" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> PySpark </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="76cd" class="mp mq it ml b gy mr ms l mt mu">from functools import reduce<br/>from pyspark.sql import DataFrame<br/><br/>def unionAll(*dfs):<br/>    return reduce(DataFrame.unionAll, dfs)<br/><br/>dfs = [df, df1, df2,...,dfn]<br/>df = unionAll(*dfs)</span></pre><h2 id="a849" class="mp mq it bd nd og oh dn nh oi oj dp nl kr ok ol nn kv om on np kz oo op nr oq bi translated">计算指定的统计数据</h2><p id="4be0" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">在某些情况下，我们需要通过一些统计KPI来执行一些数据分析。Pandas和PySpark都提供了非常容易地获得数据帧中每一列的以下信息的可能性:</p><ul class=""><li id="e3cd" class="os ot it kk b kl km ko kp kr ou kv ov kz ow ld ox oy oz pa bi translated">列元素的计数</li><li id="b772" class="os ot it kk b kl pb ko pc kr pd kv pe kz pf ld ox oy oz pa bi translated">列元素的平均值</li><li id="5f98" class="os ot it kk b kl pb ko pc kr pd kv pe kz pf ld ox oy oz pa bi translated">性传播疾病</li><li id="441d" class="os ot it kk b kl pb ko pc kr pd kv pe kz pf ld ox oy oz pa bi translated">最小值</li><li id="c00d" class="os ot it kk b kl pb ko pc kr pd kv pe kz pf ld ox oy oz pa bi translated">三个百分点:25%、50%和75%</li><li id="a0b1" class="os ot it kk b kl pb ko pc kr pd kv pe kz pf ld ox oy oz pa bi translated">最大值</li></ul><p id="611d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以通过执行以下行来计算这些值:</p><p id="3596" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">熊猫</strong>和<strong class="kk iu"> PySpark </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="bbec" class="mp mq it ml b gy mr ms l mt mu">df.summary()<br/>#OR<br/>df.describe() # the method describe doesn't return the percentiles</span></pre><h2 id="8b6c" class="mp mq it bd nd og oh dn nh oi oj dp nl kr ok ol nn kv om on np kz oo op nr oq bi translated">聚集</h2><p id="052b" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">为了执行一些聚合，语法几乎是Pandas和PySpark: <strong class="kk iu"> Pandas </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="88bb" class="mp mq it ml b gy mr ms l mt mu">df.groupby('department').agg({'employee': 'count', 'salary':'max', 'age':'mean'})</span></pre><p id="670f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> PySpark </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="f2ff" class="mp mq it ml b gy mr ms l mt mu">df.groupBy('department').agg({'employee': 'count', 'salary':'max', 'age':'mean'})</span></pre><p id="4b3a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，熊猫和PySpark的结果需要一些调整才能相似。1.在pandas中，分组依据的列成为索引:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pg"><img src="../Images/4496e70482da115672dbd07100dbd5c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*au70oO6h8rFW1_9F.png"/></div></div></figure><p id="50ce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要将它作为一个列取回，我们需要应用<code class="fe od oe of ml b"> reset_index</code>方法:<strong class="kk iu">熊猫</strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="1d16" class="mp mq it ml b gy mr ms l mt mu">df.groupby('department').agg({'employee': 'count', 'salary':'max', 'age':'mean'}).reset_index()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ph"><img src="../Images/a7acd662984232b165f8d81c9523348a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Ed5DSp1cJR7n9VXN.png"/></div></div></figure><ol class=""><li id="c8f9" class="os ot it kk b kl km ko kp kr ou kv ov kz ow ld pi oy oz pa bi translated">在<strong class="kk iu"> PySpark </strong>中，列名在结果数据帧中被修改，提到了执行的聚合:</li></ol><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pj"><img src="../Images/f0779685fba253ad6d7d9200800f3cdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eLe_FS2exMDFzSzm.png"/></div></div></figure><p id="9d0e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您希望避免这种情况，您需要像这样使用别名方法:</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="7eaa" class="mp mq it ml b gy mr ms l mt mu">df.groupBy('department').agg(F.count('employee').alias('employee'), F.max('salary').alias('salary'), F.mean('age').alias('age'))</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pk"><img src="../Images/a04cc8d7a8ef8fe2ef5afb30ccb7b1a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vamC4GOFUpSK87BG.png"/></div></div></figure><h2 id="27da" class="mp mq it bd nd og oh dn nh oi oj dp nl kr ok ol nn kv om on np kz oo op nr oq bi translated">对列应用变换</h2><p id="91c1" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">要对列应用某种转换，PySpark中不再提供apply方法。相反，我们可以使用一个名为<code class="fe od oe of ml b">udf</code>(或者用户定义的函数)的方法来封装python函数。</p><p id="484c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，如果工资低于60000英镑，我们需要增加15%的工资，如果超过60000英镑，我们需要增加5%的工资。</p><p id="1683" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">pandas中的语法如下:</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="46d5" class="mp mq it ml b gy mr ms l mt mu">df['new_salary'] = df['salary'].apply(lambda x: x*1.15 if x&lt;= 60000 else x*1.05)</span></pre><p id="385d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PySpark中的对等用法如下:</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="85c9" class="mp mq it ml b gy mr ms l mt mu">from pyspark.sql.types import FloatType<br/><br/>df.withColumn('new_salary', F.udf(lambda x: x*1.15 if x&lt;= 60000 else x*1.05, FloatType())('salary'))</span></pre><p id="1fa1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">⚠️注意到<code class="fe od oe of ml b">udf</code>方法需要明确指定数据类型(在我们的例子中是FloatType)</p><h2 id="6dd2" class="mp mq it bd nd og oh dn nh oi oj dp nl kr ok ol nn kv om on np kz oo op nr oq bi translated">最后的想法</h2><p id="bba9" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">总之，很明显，Pandas和PySpark的语法有很多相似之处。这将极大地促进从一个到另一个的过渡。</p><p id="e25e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用PySpark在处理大型数据集时会给你带来很大的优势，因为它允许并行计算。但是，如果您正在处理的数据集很小，那么恢复到唯一的熊猫会很快变得更有效。</p><p id="4d77" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为这篇文章是关于从pandas到PySpark的平稳过渡，所以有必要提到一个Pandas的等价API，叫做<a class="ae le" href="https://koalas.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">考拉</a>，它工作在Apache Spark上，因此填补了两者之间的空白。</p><p id="9f43" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">谢谢你坚持到现在。注意安全，下一个故事再见😊！</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><p id="ccde" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您有兴趣了解关于scikit-learn的更多信息，请查看以下文章:</p><div class="pl pm gp gr pn po"><a rel="noopener follow" target="_blank" href="/4-scikit-learn-tools-every-data-scientist-should-use-4ee942958d9e"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd iu gy z fp pt fr fs pu fu fw is bi translated">4 sci kit——每个数据科学家都应该使用的学习工具</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">复合估算器和变压器</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="py l pz qa qb px qc lp po"/></div></div></a></div><div class="pl pm gp gr pn po"><a rel="noopener follow" target="_blank" href="/5-hyperparameter-optimization-methods-you-should-use-521e47d7feb0"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd iu gy z fp pt fr fs pu fu fw is bi translated">每个数据科学家都应该使用的5种超参数优化方法</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">网格搜索，连续减半和贝叶斯网格搜索…</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="qd l pz qa qb px qc lp po"/></div></div></a></div></div></div>    
</body>
</html>