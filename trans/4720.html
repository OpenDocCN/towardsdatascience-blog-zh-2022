<html>
<head>
<title>Ace your Machine Learning Interview — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">赢得机器学习面试——第1部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ace-your-machine-learning-interview-part-1-e6a5897e6844#2022-10-20">https://towardsdatascience.com/ace-your-machine-learning-interview-part-1-e6a5897e6844#2022-10-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/8d45968dd995b39916c7d8335c92fe25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cSG0hUwSeF9KHq9-"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">照片由<a class="ae jd" href="https://unsplash.com/@linkedinsalesnavigator?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> LinkedIn销售解决方案</a>在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><div class=""/><div class=""><h2 id="8cc6" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">深入线性回归、套索回归和岭回归及其假设</h2></div><h2 id="0174" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">介绍</h2><p id="0772" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">这些天我在机器学习领域有几个面试，因为我已经搬到国外，需要找一份新工作。</p><p id="cafb" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">大公司和小创业公司总是想确保你知道机器学习的基础知识，所以我用一些时间来重温这些基础知识。因此，我决定分享一系列关于在机器学习中处理面试时你需要知道什么的文章，希望它也能帮助到你们中的一些人。</p><h2 id="80d5" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">线性回归</h2><figure class="mq mr ms mt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mp"><img src="../Images/2c8cfbbf5b5f2edd99dbd8f1e7a19c88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*stlCmvrvMBEFDHOn_wikTA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">线性回归(作者图片)</p></figure><p id="ebb6" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">当我们谈论线性回归时，我们有一组点，为了方便起见，您可以认为这些点绘制在二维平面上(x:要素，y:标注),我们希望用直线拟合这些点。</p><p id="9304" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">也就是说，我们想要找到通过上图中的点之间的右<em class="mu">的直线。</em></p><p id="5a23" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">我们知道，用红色表示直线的方程是这样的类型:<strong class="lt jh"> <em class="mu"> y = a*x + b </em> </strong>。所以找到正确的直线意味着<strong class="lt jh">估计</strong>我们需要绘制直线的参数<strong class="lt jh"> <em class="mu"> a </em> </strong>和<strong class="lt jh"> <em class="mu"> b </em> </strong>。</p><p id="243d" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">但是我们如何找到这些我们可以称之为<strong class="lt jh"> <em class="mu">的参数θ = (a，b) </em> </strong>？</p><p id="3c7d" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">直观地说，这种机制很简单，我们从任何一条直线开始，反复调整它，直到找到一条更好的直线。</p><figure class="mq mr ms mt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mv"><img src="../Images/d8eb3657f6b8171e5b51b04ec6e87ee7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AY26dKBT6Puv_imJ0M9DSg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">按迭代调整(图片按作者)</p></figure><p id="8cfa" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">但是我们如何在每次迭代中提高对<em class="mu"> θ = (a，b) </em>的估计呢？要做到这一点，我们需要找到一种方法，能够告诉每一行有多少错误。在上图中，ITER 0线比ITER 1线错得更多。现在我们需要找到一种形式主义来明确这一点，即使它在视觉上是微不足道的。</p><p id="28e2" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">这就是所谓的<strong class="lt jh">损失函数</strong>帮助我们的地方。这个函数，给定一条线和我们的数据集点，将返回一个数字，这个数字相当于网络产生的误差，太棒了！所讨论的损失函数被称为<strong class="lt jh">均方误差(MSE) </strong>。</p><figure class="mq mr ms mt gt is gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/c9cabbb26c8fb2146fb28c89d69c8d5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*UQqK1XbhGNNmZZOhHo_gVg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">MSE(作者图片)</p></figure><p id="091d" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">在此函数中，对所有点的误差进行平均，即每个点到直线的距离。事实上，如果你仔细观察括号，我们会发现实际的y值<strong class="lt jh"><em class="mu"/></strong>和预测的y值<strong class="lt jh"><em class="mu"/></strong>之间存在差异。注意，预测的<em class="mu"> y </em>将是<strong class="lt jh"> <em class="mu"> a*x + b </em> </strong>的结果(其中最初的<em class="mu"> a </em>和<em class="mu"> b </em>是随机取的)。</p><p id="272e" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">现在我们也有了一种方法来量化我们的直线的误差，我们需要一种方法来迭代地改进参数<em class="mu"> θ = (a，b) </em>。</p><p id="6634" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">为此，我们使用<strong class="lt jh">梯度下降算法</strong>！算法告诉我们以下列方式更新参数。</p><figure class="mq mr ms mt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mx"><img src="../Images/86127adf1914f385f654c6098a6309b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CTJjXjzurl-WBSNK5uy_lg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">更新规则(按作者排列的图像)</p></figure><p id="1ce1" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">然后，我们只需要计算损失函数相对于两个参数(a和b) 的<strong class="lt jh">偏导数，并设置一个<strong class="lt jh">学习速率α </strong>，它将是允许我们或多或少快速收敛(从而找到最优线)的超参数。如果我们选择太低的学习率，我们可能会收敛得太慢。相反，高学习率可能不会让我们收敛。</strong></p><p id="569e" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">我们可以重复这个迭代，直到损失函数的值，然后我们的网络的误差低，在这一点上，我们会找到一条线拟合我们的点！</p><p id="3181" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">我们可以直观地看到这种不断更新参数以最小化损失的过程，就像通过逐步下降来寻找函数的最小值一样。</p><figure class="mq mr ms mt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi my"><img src="../Images/888cd02eb4170a7bff260a60b461fc7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LrYOX--afFtpDHTrAQsfbg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">梯度下降(图片由作者提供)</p></figure><h2 id="c3f7" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated"><strong class="ak">多元线性回归</strong></h2><p id="1480" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">显然，在我们有n维点的情况下，线性回归可以被推广，因此具有n-1个特征，而不是一个。在这种情况下，数据集中的每个点将由n-1个分量组成。</p><figure class="mq mr ms mt gt is gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/83a467ddaa9bc4aad7732e954ab43672.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*fa_DOfW4kooApCYIAQIwcA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">多元线性回归数据集(图片由作者提供)</p></figure><p id="b6b5" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">在这种情况下，点<em class="mu"> i、</em>的预测将按以下方式计算。</p><figure class="mq mr ms mt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi na"><img src="../Images/77487c81b07a127d3c0f5ae86d21f352.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y81vg4LzYvOzxz2GXWfIWw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">多元线性回归预测(图片由作者提供)</p></figure><p id="1cf6" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">在这里，我们将不得不<strong class="lt jh">估计所有</strong> <strong class="lt jh">参数</strong> <strong class="lt jh"> <em class="mu"> a1，a2，..安、乙</em>、T37】。</strong></p><h2 id="144a" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">多重共线性问题</h2><p id="3ee1" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">当两个或多个要素高度相关时，会出现多重共线性问题。线性回归的最大优点之一是它的可解释性。在类型为<strong class="lt jh"> <em class="mu">的模型中，y = a1*x1 + a2*x2 </em> </strong>我知道每当<em class="mu"> x1 </em>增加一个单位<em class="mu"> y </em>增加<em class="mu"> a1 </em>个单位。这是直接从公式推导出来的。但是在<em class="mu"> x1 </em>和<em class="mu"> x2 </em>关联的情况下，当<em class="mu"> x1 </em>增加一个单位时，<em class="mu"> x2 </em>也以某种方式增加，我无法再判断<em class="mu"> y </em>会如何变化！当我有了<em class="mu"> n </em>的特征时，想想真是一团糟！<strong class="lt jh">要进行多重共线性检测，可以使用相关矩阵</strong>，通常用热图绘制。当变量相关时，热图具有更强烈的值。</p><figure class="mq mr ms mt gt is gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/41d3cdd131d08e166ffb607cac9687c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*7zCpvyrzvRmvlSMYMEoHcQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">热图示例(图片由作者提供)</p></figure><h2 id="04e2" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">里脊回归</h2><p id="9675" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">岭回归是线性回归的一种变体，只是对损失函数做了一点小小的修改。</p><p id="7ac8" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">使用的损失函数如下。</p><figure class="mq mr ms mt gt is gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/e3f6f804845031ea36a86472aef7f464.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*KoWDTAzfk_IVj102rAFWpA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">岭损失函数(图片由作者提供)</p></figure><p id="28ad" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">与之前看到的MSE不同的是橙色部分，我们称之为<strong class="lt jh">罚</strong>。惩罚由模型(直线)参数的平方和构成。所以参数<em class="mu"> a1越大，…，an </em>网络的误差越大。这样，损失函数迫使梯度下降算法找到一条具有小参数的线，所以<strong class="lt jh">它是一个附加约束</strong>，<strong class="lt jh">它用来限制模型的学习能力，使其不会陷入过拟合</strong>。λ是乘法因子，是网络的另一个超参数。</p><h2 id="6fcb" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">套索回归</h2><p id="de83" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">套索回归和岭很像，只是罚分略有变化。</p><figure class="mq mr ms mt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nd"><img src="../Images/717e4812067112eac4d8e3aec9c0eecb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*elxM1bviuWeFDDHzWVSshg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">套索损失函数(作者图片)</p></figure><p id="15a3" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">在这种情况下，我们有参数的模而不是平方。套索经常导致某些系数<em class="mu"> a1，…an </em>为零。所以这意味着一些功能将被取消。正因为如此，<strong class="lt jh">我们也可以用lasso回归作为特征选择，来搞清楚哪些特征最没用！</strong></p><h1 id="359e" class="ne kw jg bd kx nf ng nh la ni nj nk ld km nl kn lh kp nm kq ll ks nn kt lp no bi translated">基本假设</h1><p id="cdad" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">在应用线性回归之前，我们需要了解<strong class="lt jh">个假设</strong>，特别是4个假设。</p><ol class=""><li id="4e65" class="np nq jg lt b lu mk lx ml le nr li ns lm nt mj nu nv nw nx bi translated"><strong class="lt jh">线性</strong>:显然，必须能够用直线(或多维超平面)拟合数据。</li><li id="9b89" class="np nq jg lt b lu ny lx nz le oa li ob lm oc mj nu nv nw nx bi translated"><strong class="lt jh">同方差</strong>:即残差的方差对于x的任何值都保持不变，用例子更容易理解。</li></ol><figure class="mq mr ms mt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi od"><img src="../Images/a9dd162a64934f573f3e7b8acc95c62b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sm_OQ6wr1wnnINtgnSdx1Q.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">同质性(图片由作者提供)</p></figure><p id="2124" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">3.<strong class="lt jh">独立性</strong>:观察是相互独立的。</p><p id="337a" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">4.<strong class="lt jh">误差的正态性</strong>:残差必须近似正态分布(您可以使用QQ图来检查)。</p><h2 id="ed02" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">优势</h2><ol class=""><li id="0d9b" class="np nq jg lt b lu lv lx ly le oe li of lm og mj nu nv nw nx bi translated">线性回归对于可线性分离的数据表现特别好</li><li id="9d20" class="np nq jg lt b lu ny lx nz le oa li ob lm oc mj nu nv nw nx bi translated">易于实施和培训</li><li id="d0c6" class="np nq jg lt b lu ny lx nz le oa li ob lm oc mj nu nv nw nx bi translated">它可以处理过度拟合使用正则化(套索，山脊)</li></ol><h2 id="2542" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated"><strong class="ak">劣势</strong></h2><ol class=""><li id="e844" class="np nq jg lt b lu lv lx ly le oe li of lm og mj nu nv nw nx bi translated">有时需要大量的特征工程</li><li id="7ac4" class="np nq jg lt b lu ny lx nz le oa li ob lm oc mj nu nv nw nx bi translated">如果这些特性是相关的，它可能会影响性能</li><li id="7cb6" class="np nq jg lt b lu ny lx nz le oa li ob lm oc mj nu nv nw nx bi translated">它对噪音很敏感。</li></ol><h2 id="a79a" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">我们来编码吧！</h2><p id="fdb1" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">我之前写的一篇关于实现线性回归的完整文章可以在这里找到:<em class="mu"/><a class="ae jd" rel="noopener" target="_blank" href="/linear-regression-and-gradient-descent-using-only-numpy-53104a834f75"><strong class="lt jh"><em class="mu">仅使用Numpy的线性回归和梯度下降</em></strong></a><strong class="lt jh"><em class="mu"/></strong><em class="mu">。</em>现在让我们看看如何使用sklearn用几行代码实现一个简单的线性回归。</p><figure class="mq mr ms mt gt is"><div class="bz fp l di"><div class="oh oi l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">使用skLearn进行线性回归</p></figure><h1 id="73ff" class="ne kw jg bd kx nf ng nh la ni nj nk ld km nl kn lh kp nm kq ll ks nn kt lp no bi translated">最后的想法</h1><p id="d86f" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">我以前写过一篇关于如何只用NumPy实现线性回归的文章，你可以在这里找到文章<a class="ae jd" rel="noopener" target="_blank" href="/linear-regression-and-gradient-descent-using-only-numpy-53104a834f75"/>！线性回归是开始学习最大似然的基本算法。如果你很好地理解了这一点，那么理解像神经网络这样更复杂的算法就容易多了。</p><blockquote class="oj ok ol"><p id="53b9" class="lr ls mu lt b lu mk kh lw lx ml kk lz om mm mb mc on mn me mf oo mo mh mi mj ij bi translated">线性回归的一个演变是<strong class="lt jh">多项式回归</strong>，这是一个更复杂的模型，也可以适合引入更复杂特征的非线性数据集，请查看这里:<a class="ae jd" href="https://en.wikipedia.org/wiki/Polynomial_regression" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Polynomial_regression</a>。</p></blockquote><h1 id="c7d6" class="ne kw jg bd kx nf ng nh la ni nj nk ld km nl kn lh kp nm kq ll ks nn kt lp no bi translated">结束了</h1><p id="f84f" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">马尔切洛·波利蒂</p><p id="1e75" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated"><a class="ae jd" href="https://www.linkedin.com/in/marcello-politi/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>，<a class="ae jd" href="https://twitter.com/_March08_" rel="noopener ugc nofollow" target="_blank"> Twitter </a>，<a class="ae jd" href="https://march-08.github.io/digital-cv/" rel="noopener ugc nofollow" target="_blank"> CV </a></p></div></div>    
</body>
</html>