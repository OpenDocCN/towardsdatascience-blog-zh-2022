<html>
<head>
<title>Understanding MixNMatch: Creating A More Realistic Synthetic Image</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解MixNMatch:创建更真实的合成图像</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-mixnmatch-creating-a-more-realistic-synthetic-image-40f9ba039689#2022-05-06">https://towardsdatascience.com/understanding-mixnmatch-creating-a-more-realistic-synthetic-image-40f9ba039689#2022-05-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="90bc" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">将多个真实图像中的不同因素组合成一个合成图像</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/eccd922613b765a2580c92c789b9e704.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RJEi7IMcr7WjvKNdwWfM0w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图MixNMatch创成式模型的概述</p></figure><p id="2ecd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我最近偶然发现了这篇名为<a class="ae lu" href="https://arxiv.org/pdf/1911.11758.pdf" rel="noopener ugc nofollow" target="_blank"> MixNMatch </a>的论文，该论文旨在将多个真实图像中的不同因素组合成一个单一的合成图像——只需最少的监督。这篇文章旨在详细说明，并要求一些深度学习和生成模型的背景。如果你正在寻找一个TLDR；版本，你可以在这里查看我的推特帖子<a class="ae lu" href="https://twitter.com/nahidalam/status/1517895642433032192" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="bae7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你喜欢这个帖子，请分享给你的网络。</p><h1 id="3309" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">摘要</h1><p id="d675" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">在其核心，MixNMatch是一种使用条件生成对抗网络(GAN)的条件图像生成技术。MixNMatch将来自不同实像的多个因子解开并编码成一个单一的合成图像。具体来说，它将不同真实图像的图像背景、姿态、形状和纹理组合成一个单一的合成图像，只需最少的监督。</p><blockquote class="ms"><p id="89c7" class="mt mu it bd mv mw mx my mz na nb lt dk translated">在训练过程中，MixNMatch只需要一个松散的包围盒来模拟背景，而不需要物体的姿态，形状或纹理。</p></blockquote><h1 id="b948" class="lv lw it bd lx ly lz ma mb mc md me mf jz nc ka mh kc nd kd mj kf ne kg ml mm bi translated">问题</h1><p id="f824" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">在最少监督的情况下学习解开表示是一个极具挑战性的问题，因为产生数据的潜在因素通常是高度相关和交织的。</p><ul class=""><li id="0807" class="nf ng it la b lb lc le lf lh nh ll ni lp nj lt nk nl nm nn bi translated">有大量工作通过拍摄两个输入参考图像来理清两个因素。例如-一个参考图像用于外观，另一个用于姿势。但他们不能理清其他因素，如前景与背景外观或姿势与形状。由于只有两个因素可以控制，这些方法不能随意改变，例如，对象的背景，形状和纹理，同时保持其姿势不变。</li><li id="93ec" class="nf ng it la b lb no le np lh nq ll nr lp ns lt nk nl nm nn bi translated">另一组工作需要以关键点/掩模注释的形式进行强有力的监督；限制了它们的可伸缩性，并且仍然无法理清MixNMatch中列出的所有四个因素</li></ul><h1 id="96df" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">主要思想</h1><p id="c30f" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">MixNMatch学会在最少监督的情况下，从真实图像中解开并编码背景、对象姿势、形状和纹理潜在因素。为了实现这一点，他们提出了一个可以同时学习的框架</p><ul class=""><li id="dc8a" class="nf ng it la b lb lc le lf lh nh ll ni lp nj lt nk nl nm nn bi translated">编码器，其将来自真实图像的潜在因素编码到解开的潜在代码空间中，以及</li><li id="0cf1" class="nf ng it la b lb no le np lh nq ll nr lp ns lt nk nl nm nn bi translated">一种从解开的代码空间中提取潜在因素用于图像生成的生成器。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/bc6f526cd560a50244bd4c70ae9774da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gcQv9m8cCk8TFnPGMwP9Dg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:来自<a class="ae lu" href="https://arxiv.org/pdf/1911.11758.pdf" rel="noopener ugc nofollow" target="_blank">的MixNMatch架构在这里</a></p></figure><p id="f8ff" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">MixNMatch的生成器建立在<a class="ae lu" href="https://arxiv.org/abs/1811.11155" rel="noopener ugc nofollow" target="_blank"> FineGAN </a>的基础上——这是一个生成模型，它学习使用信息论在最少监督的情况下分层理清背景、对象姿势、形状和纹理。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/2b5c19a83aef3cc38ad494a0c08fc984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tnFNtJzXQfrUKv4A7IgH-A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图FineGAN如何从<a class="ae lu" href="https://arxiv.org/abs/1811.11155" rel="noopener ugc nofollow" target="_blank">这里</a>工作</p></figure><p id="725a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，FineGAN仅取决于采样的<em class="nv">潜在代码</em>，而不能直接取决于用于图像生成的真实图像。因此，我们需要一种方法来从真实图像中提取控制背景、物体姿态、形状和纹理的潜在代码，同时保留FineGAN的层次分解属性。那么我们有什么选择呢？</p><ul class=""><li id="7eb9" class="nf ng it la b lb lc le lf lh nh ll ni lp nj lt nk nl nm nn bi translated">FineGAN的简单扩展；意思是——训练一个编码器将假图像映射成潜在代码。这是行不通的，因为真实图像和虚假图像之间的域差距</li><li id="79dc" class="nf ng it la b lb no le np lh nq ll nr lp ns lt nk nl nm nn bi translated">执行对抗学习，由此学习真实图像及其从编码器提取的潜在代码的联合分布，以及采样潜在代码和来自生成器的相应生成图像的联合分布，以使其不可区分，类似于阿里和甘比</li></ul><p id="6ec2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="nv">对抗式学习方法的优势是什么？</em></p><blockquote class="ms"><p id="3fc5" class="mt mu it bd mv mw mx my mz na nb lt dk translated">通过实施匹配的联合图像码分布，编码器学习产生与采样码的分布相匹配的潜在码，该潜在码具有期望的解纠缠特性，而发生器学习产生真实的图像。</p></blockquote><p id="2948" class="pw-post-body-paragraph ky kz it la b lb nw ju ld le nx jx lg lh ny lj lk ll nz ln lo lp oa lr ls lt im bi translated">更具体地说，对于每个真实图像x，作者提出四个单独的编码器来提取其<em class="nv"> z、b、p、c </em>码。但是，您不希望将这些代码直接放入生成器来重建图像。因为这将把这个模型变成一个“简单的”自动编码器。这并没有保留FineGAN的解缠结特性(分解成背景、姿势、形状、纹理)。因此，作者利用阿里和甘比的想法来帮助编码器学习逆映射；即从实像到代码空间的投影，以保持期望的解缠结属性的方式。</p><h1 id="181a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">方法</h1><p id="6683" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">因为MixNMatch是基于FineGAN的，所以让我们先快速回顾一下FineGAN是如何工作的。</p><h2 id="ee31" class="ob lw it bd lx oc od dn mb oe of dp mf lh og oh mh ll oi oj mj lp ok ol ml om bi translated">FineGAN如何工作</h2><p id="f186" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">作为输入，FineGAN采用四个随机采样的嵌入，又名。潜在代码(<em class="nv"> z，b，c，p </em>)如图2所示。然后，它分三个阶段分层生成图像(图3)。</p><ul class=""><li id="c857" class="nf ng it la b lb lc le lf lh nh ll ni lp nj lt nk nl nm nn bi translated">阶段1:背景阶段，模型只生成背景，以潜在的一键背景码<em class="nv"> b </em>为条件</li><li id="008c" class="nf ng it la b lb no le np lh nq ll nr lp ns lt nk nl nm nn bi translated">阶段2:父阶段，模型根据潜在的一键父代码<em class="nv"> p </em>和连续代码<em class="nv"> z </em>生成物体的形状和<br/>姿态，并将其缝合到现有的背景图像上</li><li id="0758" class="nf ng it la b lb no le np lh nq ll nr lp ns lt nk nl nm nn bi translated">阶段3:子阶段，模型填充物体的纹理，以潜在的一键子代码<em class="nv"> c </em>为条件</li></ul><p id="6aa7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在父阶段和子阶段，FineGAN还会生成对象的遮罩，以便在没有监督的情况下捕捉形状和纹理。</p><blockquote class="ms"><p id="1b97" class="mt mu it bd mv mw mx my mz na nb lt dk translated"><em class="on">在训练期间，FineGAN实施了两个约束</em></p></blockquote><ol class=""><li id="5dd7" class="nf ng it la b lb nw le nx lh oo ll op lp oq lt or nl nm nn bi translated">约束#1:它将子代码分组到一个不相交的集合中，因此每个集合包含相同的父代码。例如，这种约束使得不同种类的鸭子具有相同的形状但不同的质地。</li><li id="ea44" class="nf ng it la b lb no le np lh nq ll nr lp ns lt or nl nm nn bi translated">约束#2:它还强制要求对于每个生成的图像，子代码和背景代码对总是相同的——这意味着鸭子的背景总是有水</li></ol><p id="bd4e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">【FineGAN如何理清不同的特征？</p><ul class=""><li id="6431" class="nf ng it la b lb lc le lf lh nh ll ni lp nj lt nk nl nm nn bi translated">为了解开背景，FineGAN依赖于对象的边界框。</li><li id="19d0" class="nf ng it la b lb no le np lh nq ll nr lp ns lt nk nl nm nn bi translated">为了理清其他因素，FineGAN依赖于InfoGAN [5]中的信息论，并在潜在代码的关系之间施加约束。</li></ul><h2 id="23aa" class="ob lw it bd lx oc od dn mb oe of dp mf lh og oh mh ll oi oj mj lp ok ol ml om bi translated">MixNMatch和FineGAN有什么不同</h2><blockquote class="ms"><p id="2000" class="mt mu it bd mv mw mx my mz na nb lt dk translated">MixNMatch的主要思想是执行对抗学习，使得由编码器产生的成对图像码分布和由生成器产生的成对图像码分布相匹配。MixNMatch处鉴频器的输入是一个图像码对。</p></blockquote><h2 id="8644" class="ob lw it bd lx oc os dn mb oe ot dp mf lh ou oh mh ll ov oj mj lp ow ol ml om bi translated"><em class="on">在mix match</em>解缠绕是如何工作的</h2><p id="673c" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">FineGAN强加的约束对于真实图像可能是困难的。如果你回顾约束#2，你会发现它并不总是成立的。例如，鸭子可能不总是在水里。因此，为了绕过约束#2，作者采用了以下步骤</p><ul class=""><li id="df2f" class="nf ng it la b lb lc le lf lh nh ll ni lp nj lt nk nl nm nn bi translated">训练四个独立的鉴别器，每种代码类型一个。这防止了任何鉴别器看到其他代码，因此不能基于代码之间的关系进行鉴别。</li><li id="da1e" class="nf ng it la b lb no le np lh nq ll nr lp ns lt nk nl nm nn bi translated">在训练编码器时，用随机采样的代码生成的假图像也被提供作为输入。这些随机采样的代码消除了这些限制。这意味着，在这些生成的图像中，任何前景纹理都可以与任何任意背景和任意形状相结合。</li></ul><h2 id="d270" class="ob lw it bd lx oc od dn mb oe of dp mf lh og oh mh ll oi oj mj lp ok ol ml om bi translated">如何捕捉精确的姿势和形状</h2><p id="6766" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">到目前为止，我们已经了解了MixNMatch如何将图像的4个不同方面分开，并将其编码成代码。然后，MixNMatch的生成器将这4个因素结合起来，生成“真实”的合成图像。作者称这个过程为“代码模式”。</p><p id="8ce9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是“编码模式”不能保持精确的<em class="nv">像素级</em>姿态和形状，而这对于某些应用是必要的。为什么它不能保持精确的像素级？因为姿势/形状的潜在代码维度不够大，不足以捕捉那些细粒度的特征。为了解决这个问题，作者引入了一种叫做“特征模式”的东西</p><blockquote class="ms"><p id="e182" class="mt mu it bd mv mw mx my mz na nb lt dk translated">关键思想不是将参考图像编码成低维形状码，而是直接学习从图像到高维特征空间的映射，该映射保持参考图像的空间对齐的形状和姿态(像素级)细节。</p></blockquote><p id="8c93" class="pw-post-body-paragraph ky kz it la b lb nw ju ld le nx jx lg lh ny lj lk ll nz ln lo lp oa lr ls lt im bi translated">您可以在论文的第3.4节中找到更多相关信息。</p><h1 id="c03b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><ol class=""><li id="e769" class="nf ng it la b lb mn le mo lh ox ll oy lp oz lt or nl nm nn bi translated">Krishna Kumar Singh、Utkarsh Ojha和Yong Jae Lee。<em class="nv">用于细粒度对象生成和发现的无监督分层解缠</em>。在CVPR，2019年。</li><li id="2cc5" class="nf ng it la b lb no le np lh nq ll nr lp ns lt or nl nm nn bi translated">杰夫·多纳休、菲利普·克亨布尔和特雷弗·达雷尔。<em class="nv">对抗性特征学习</em>。2017年在ICLR。</li><li id="7770" class="nf ng it la b lb no le np lh nq ll nr lp ns lt or nl nm nn bi translated">文森特·杜穆林、伊斯梅尔·贝尔加齐、本·普尔、亚历克斯·兰姆、马丁·阿约夫斯基、奥利维尔·马斯托皮埃罗和亚伦·库维尔。<em class="nv">对抗性学习推理</em>。2017年在ICLR</li><li id="2c4a" class="nf ng it la b lb no le np lh nq ll nr lp ns lt or nl nm nn bi translated"><a class="ae lu" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mirza%2C+M" rel="noopener ugc nofollow" target="_blank">迈赫迪·米尔扎</a>，<a class="ae lu" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Osindero%2C+S" rel="noopener ugc nofollow" target="_blank">西蒙·奥森德罗</a> <em class="nv">条件生成对抗网</em></li><li id="75d8" class="nf ng it la b lb no le np lh nq ll nr lp ns lt or nl nm nn bi translated">陈曦、闫端、雷因·胡特夫特、约翰·舒尔曼、伊利亚·苏茨基弗和彼得·阿贝耳。Infogan: <em class="nv">通过信息最大化生成对抗网的可解释表示学习</em>。在NeurIPS，2016。</li></ol></div></div>    
</body>
</html>