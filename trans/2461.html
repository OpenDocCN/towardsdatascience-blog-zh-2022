<html>
<head>
<title>Parallelize your massive SHAP computations with MLlib and PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用MLlib和PySpark并行处理您的大规模SHAP计算</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/parallelize-your-massive-shap-computations-with-mllib-and-pyspark-b00accc8667c#2022-05-28">https://towardsdatascience.com/parallelize-your-massive-shap-computations-with-mllib-and-pyspark-b00accc8667c#2022-05-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7149" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用SHAP有效解释模型的分步指南。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3973882322412b427ca74dedae879045.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6Ucu9KQzSmi4H43s"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kv" href="https://unsplash.com/@pietrozj?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Pietro Jeng </a>拍摄</p></figure><h2 id="d4ae" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">ml lib简介</strong></h2><p id="d728" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">Apache Spark的机器学习库(MLlib)主要是为了可伸缩性和速度而设计的，它利用Spark运行时来处理监督学习(如分类和回归)中常见的<strong class="lu ir">分布式</strong>用例，以及非监督学习(如聚类和协作过滤)和其他情况(如降维)。在这篇文章中，我将介绍我们如何使用SHAP来解释梯度增强树(GBT)模型，该模型在一定程度上符合我们的数据。</p><h2 id="355c" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">什么是梯度增强树？</h2><p id="6859" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">在我们理解什么是梯度增强树之前，我们需要理解增强。Boosting是一种集成技术，其中<strong class="lu ir">顺序地</strong>组合多个弱学习者以实现整体强学习者。在梯度提升树的情况下，每个弱学习器是一个决策树，该决策树顺序地最小化由该序列中的前一个决策树生成的误差(在回归的情况下是MSE，在分类的情况下是log loss)。要阅读更多关于GBTs的细节，请参考<a class="ae kv" rel="noopener" target="_blank" href="/gradient-boosted-decision-trees-explained-9259bd8205af">这篇</a>博客文章。</p><h2 id="faa6" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">了解我们的进口</h2><pre class="kg kh ki kj gt ml mm mn mo aw mp bi"><span id="beba" class="kw kx iq mm b gy mq mr l ms mt">from pyspark.sql import SparkSession<br/>from pyspark import SparkContext, SparkConf<br/>from pyspark.ml.classification import GBTClassificationModel<br/>import pyspark.sql.functions as F<br/>from pyspark.sql.types import *</span></pre><p id="583b" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated">前两个导入用于初始化Spark会话。它将被用来把我们的熊猫数据帧转换成spark帧。第三个导入用于将我们的GBT模型加载到内存中，该模型将被传递给我们的SHAP解释器来生成解释。倒数第二个也是最后一个导入用于执行SQL函数和使用SQL类型。这些将在我们的用户定义函数(UDF)中使用，我将在后面描述。</p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h2 id="22cd" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">将MLlib GBT特征向量转换为熊猫数据帧</h2><p id="ffd5" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">SHAP解释器将一个数据帧作为输入。然而，训练MLlib GBT模型需要数据预处理。更具体地说，我们数据中的分类变量需要使用<strong class="lu ir">分类索引</strong>或<strong class="lu ir">一键编码转换成数字变量。</strong>要了解更多关于如何训练GBT模特的信息，请参考<a class="ae kv" rel="noopener" target="_blank" href="/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa">这篇</a>文章。产生的“特性”列<strong class="lu ir"> </strong>是一个<strong class="lu ir"> </strong> <a class="ae kv" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.linalg.SparseVector.html#pyspark.ml.linalg.SparseVector" rel="noopener ugc nofollow" target="_blank">稀疏向量</a>(要了解更多信息，请查看<a class="ae kv" href="https://docs.databricks.com/applications/machine-learning/train-model/mllib/index.html#advanced-apache-spark-mllib-example" rel="noopener ugc nofollow" target="_blank">这个</a>示例中的“预处理数据”部分)。它看起来像下面的东西:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/4c565970b312e8efb17108876500cacf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E75gSwsCYd6be4nk3oX-MA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">SparseVector特性列描述— 1。默认索引值，2。向量长度，3。功能列的索引列表，4。位于3的相应索引处的数据值列表。[图片由作者提供]</p></figure><p id="1cd4" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated">上面显示的“特性”列是针对单个训练实例的。我们需要为所有的训练实例转换这个SparseVector。一种方法是迭代处理每一行，并添加到我们的熊猫数据帧中，我们将把它提供给我们的SHAP解释器(哎哟！).有一种更快的方法，它利用了我们将所有数据都加载到内存中的事实(如果没有，我们可以成批地加载它，并对每个内存中的批处理执行预处理)。用Shikhar Dua的话说:</p><blockquote class="nh ni nj"><p id="17a4" class="ls lt nk lu b lv mu jr lx ly mv ju ma nl mw mc md nm mx mf mg nn my mi mj mk ij bi translated">1.创建字典列表，其中每个字典对应一个输入数据行。</p><p id="111a" class="ls lt nk lu b lv mu jr lx ly mv ju ma nl mw mc md nm mx mf mg nn my mi mj mk ij bi translated">2.根据此列表创建数据框。</p></blockquote><p id="9995" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated">所以，基于上面的方法，我们得到这样的东西:</p><pre class="kg kh ki kj gt ml mm mn mo aw mp bi"><span id="daa9" class="kw kx iq mm b gy mq mr l ms mt">rows_list = []<br/>for row in spark_df.rdd.collect(): <br/>    dict1 = {} <br/>    dict1.update({k:v for k,v in zip(spark_df.cols,row.features)})<br/>    rows_list.append(dict1) <br/>pandas_df = pd.DataFrame(rows_list)</span></pre><p id="036f" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated">如果说<em class="nk"> rdd.collect() </em>看起来很吓人，其实解释起来还是挺简单的。<strong class="lu ir">弹性分布式数据集(RDD) </strong>是基本的Spark数据结构，它们是<strong class="lu ir"> </strong>和<strong class="lu ir">不可变的</strong>对象分布。RDD中的每个数据集被进一步细分为逻辑分区，这些分区可以在Spark集群的不同工作节点中进行计算。因此，PySpark RDD <em class="nk"> collect() </em>所做的就是从所有工作节点检索数据到驱动节点。正如您可能猜到的，这是一个内存瓶颈，如果我们处理的数据超过了驱动程序节点的内存容量，我们需要增加RDD分区的数量，并通过分区索引对它们进行过滤。点击阅读如何操作<a class="ae kv" href="https://umbertogriffo.gitbook.io/apache-spark-best-practices-and-tuning/rdd/dont_collect_large_rdds" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="aafb" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated">不要相信我的执行力。看看统计数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/bc26370c00065dd17dd8cebaa5534ed9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*lcMZA4BDfT2QQQQ0oFO00Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">向pandas数据帧插入行的性能分析。[来源(感谢米哈伊尔_萨姆和彼得·莫滕森):<a class="ae kv" href="https://stackoverflow.com/questions/10715965/create-a-pandas-dataframe-by-appending-one-row-at-a-time/17496530#17496530" rel="noopener ugc nofollow" target="_blank">此处</a></p></figure><p id="2b0c" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated">以下是我的一个Databricks笔记本计划作业运行的指标:</p><blockquote class="nh ni nj"><p id="c08f" class="ls lt nk lu b lv mu jr lx ly mv ju ma nl mw mc md nm mx mf mg nn my mi mj mk ij bi translated">输入大小:11.9 GiB (~12.78GB)，所有任务的总时间:20分钟，记录数:165.16K</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/18537add960e144b3c5aee916668869d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9btPIGmGkU38tX4847j3iw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">运行上述单元的阶段执行的125个已完成任务的摘要度量。[图片由作者提供]</p></figure></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h2 id="ea98" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">与SHAP图书馆合作</h2><p id="233a" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">我们现在准备将预处理过的数据集传递给SHAP树解释器。请记住，SHAP是一种局部特征归因方法，它将<strong class="lu ir">单个</strong>预测解释为我们模型的特征的shapley值的代数和。</p><p id="9576" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated">我们使用TreeExplainer的原因如下:</p><ol class=""><li id="c3e6" class="nq nr iq lu b lv mu ly mv lf ns lj nt ln nu mk nv nw nx ny bi translated"><strong class="lu ir">合适:</strong> TreeExplainer是一个为基于树的模型(Random Forest，XGBoost，LightGBM，GBT等)计算SHAP值的类。</li><li id="c84c" class="nq nr iq lu b lv nz ly oa lf ob lj oc ln od mk nv nw nx ny bi translated"><strong class="lu ir">精确</strong>:它不是通过随机采样来模拟缺失特征，而是通过简单地忽略依赖于缺失特征的决策路径来利用树结构。因此，TreeExplainer的输出是确定性的，不会随背景数据集而变化。</li><li id="9bdd" class="nq nr iq lu b lv nz ly oa lf ob lj oc ln od mk nv nw nx ny bi translated"><strong class="lu ir">高效</strong>:所有组合不是在每个可能的特征组合(或其子集)上迭代，而是同时通过树推送，使用更复杂的算法来跟踪每个组合的结果——将复杂度从所有可能组合的O( <strong class="lu ir"> TL2ᵐ </strong>)降低到多项式O( <strong class="lu ir"> TLD </strong>)(其中<strong class="lu ir"> m </strong>是特征的数量，<strong class="lu ir"> T </strong>是树的数量，<strong class="lu ir"> L </strong>是最大叶子数量，<strong class="lu ir"/></li></ol><p id="030b" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated"><em class="nk"> check_additivity = False </em>标志运行验证检查，以验证SHAP值的总和是否等于模型的输出。然而，该标志要求运行Spark不支持的预测，因此需要将其设置为False，因为它无论如何都会被忽略。一旦我们得到了SHAP值，我们就把它从一个Numpy数组转换成一个pandas dataframe，这样它就很容易解释了。</p><blockquote class="nh ni nj"><p id="e3ec" class="ls lt nk lu b lv mu jr lx ly mv ju ma nl mw mc md nm mx mf mg nn my mi mj mk ij bi translated">需要注意的一点是，当我们将Spark数据帧转换为pandas时，数据集的顺序保持不变，但是<strong class="lu ir">相反的情况不成立</strong>。</p></blockquote><p id="6cc2" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated">以上几点将我们引向下面的代码片段:</p><pre class="kg kh ki kj gt ml mm mn mo aw mp bi"><span id="28e1" class="kw kx iq mm b gy mq mr l ms mt">gbt = GBTClassificationModel.load('your-model-path') <br/>explainer = shap.TreeExplainer(gbt)<br/>shap_values = explainer(pandas_df, check_additivity = False)<br/>shap_pandas_df = pd.DataFrame(shap_values.values, cols = pandas_df.columns)</span></pre><h2 id="995d" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">Pyspark UDFs简介以及何时使用它们</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/6147af33835e4bf42268bfa1aa273813.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*8yUt7Mib_XUZ4clJk3gUag.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">PySpark UDFs如何将单个任务分配给worker (executor)节点[Source: <a class="ae kv" href="https://medium.com/quantumblack/spark-udf-deep-insights-in-performance-f0a95a4d8c62#:~:text=In%20these%20circumstances%2C%20PySpark%20UDF,two%20types%20of%20PySpark%20UDFs." rel="noopener">此处</a> ]</p></figure><p id="5779" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated">用户定义的函数是复杂的自定义函数，对数据集的特定行进行操作。当本地火花函数被认为不足以解决问题时，通常使用这些函数。Spark函数天生比UDF快，因为它本身是一个JVM结构，其方法是通过对Java APIs的本地调用来实现的。然而，PySpark UDFs是Python实现，需要在Python解释器和JVM之间移动数据(参考上图中的箭头4)。这不可避免地引入了一些处理延迟。</p><p id="dd1e" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated">如果不能容忍任何处理延迟，最好的办法是创建一个Python包装器，从PySpark本身调用Scala UDF。这个博客展示了一个很好的例子。然而，使用PySpark UDF对我的用例来说已经足够了，因为它易于理解和编码。</p><p id="9685" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated">下面的代码解释了在每个worker/executor节点上执行的Python函数。我们只是选取最高的SHAP值(绝对值，因为我们也想找到最有影响力的负面特征)并将其附加到相应的<em class="nk"> pos_features </em>和<em class="nk"> neg_features </em>列表，然后将这两个列表附加到返回给调用者的<em class="nk"> features </em>列表。</p><pre class="kg kh ki kj gt ml mm mn mo aw mp bi"><span id="42ab" class="kw kx iq mm b gy mq mr l ms mt">def shap_udf(row):<br/>    dict = {} <br/>    pos_features = [] <br/>    neg_features = [] <br/>    for feature in row.columns: <br/>        dict[feature] = row[feature] </span><span id="3225" class="kw kx iq mm b gy of mr l ms mt">    dict_importance = {key: value for key, value in<br/>    sorted(dict.items(), key=lambda item: __builtin__.abs(item[1]),   <br/>    reverse = True)} </span><span id="774a" class="kw kx iq mm b gy of mr l ms mt">    for k,v in dict_importance.items(): <br/>        if __builtin__.abs(v) &gt;= &lt;your-threshold-shap-value&gt;: <br/>             if v &gt; 0: <br/>                 pos_features.append((k,v)) <br/>             else: <br/>                 neg_features.append((k,v)) <br/>   features = [] <br/>   features.append(pos_features[:5]) <br/>   features.append(neg_features[:5]) </span><span id="3d9d" class="kw kx iq mm b gy of mr l ms mt">   return features</span></pre><p id="f3f4" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated">然后，我们用我们的Python函数名注册我们的PySpark UDF(在我的例子中，它是<em class="nk"> shap_udf </em>)，并在<em class="nk"> F.udf() </em>的参数中指定函数的返回类型(在Python和Java中是强制的)。在外部ArrayType()中有两个列表，一个用于正面特性，另一个用于负面特性。因为每个单独的列表最多包含5个(feature-name，shap-value) StructType()对，所以它表示内部ArrayType()。下面是代码:</p><pre class="kg kh ki kj gt ml mm mn mo aw mp bi"><span id="8f62" class="kw kx iq mm b gy mq mr l ms mt">udf_obj = F.udf(shap_udf, ArrayType(ArrayType(StructType([ StructField(‘Feature’, StringType()), <br/>StructField(‘Shap_Value’, FloatType()),<br/>]))))</span></pre><p id="ab63" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated">现在，我们只是创建一个新的Spark数据帧，它有一个名为‘Shap _ Importance’的列，该列为<em class="nk"> spark_shapdf </em>数据帧的每一行调用我们的UDF。为了区分正面和负面特性，我们在一个名为<em class="nk"> final_sparkdf </em>的新Spark数据帧中创建了两列。我们最终的代码片段如下所示:</p><pre class="kg kh ki kj gt ml mm mn mo aw mp bi"><span id="96d6" class="kw kx iq mm b gy mq mr l ms mt">new_sparkdf = spark_df.withColumn(‘Shap_Importance’, udf_obj(F.struct([spark_shapdf[x] for x in spark_shapdf.columns])))</span><span id="8762" class="kw kx iq mm b gy of mr l ms mt">final_sparkdf = new_sparkdf.withColumn(‘Positive_Shap’, final_sparkdf.Shap_Importance[0]).withColumn(‘Negative_Shap’, new_sparkdf.Shap_Importance[1])</span></pre><p id="8718" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated">最后，我们已经提取了每个测试实例的GBT模型的所有重要特性，而没有使用任何显式的for循环！合并后的代码可以在下面的GitHub gist中找到。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="og oh l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">从拟合的GBT模型中获取最具影响力的正负SHAP值</p></figure><p id="02e6" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated">这是我第一次尝试写一篇文章，如果有任何事实或统计上的不一致，请联系我，我将非常乐意与你一起学习！:)</p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h2 id="d05d" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">参考</h2><p id="4d0a" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">[1]Soner y ldr RM，<a class="ae kv" rel="noopener" target="_blank" href="/gradient-boosted-decision-trees-explained-9259bd8205af">梯度推动决策树-向数据科学解释</a> (2020)</p><p id="c3d0" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated">[2] Susan Li，<a class="ae kv" rel="noopener" target="_blank" href="/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa">使用PySpark和MLlib的机器学习—解决一个二元分类问题</a> (2018)，走向数据科学</p><p id="92aa" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated">[3] Stephen Offer，<a class="ae kv" href="https://databricks.com/blog/2020/11/16/how-to-train-xgboost-with-spark.html" rel="noopener ugc nofollow" target="_blank">如何用Spark训练XGBoost</a>(2020)，数据科学与ML</p><p id="1497" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated">[4] <a class="ae kv" href="https://docs.databricks.com/applications/machine-learning/train-model/mllib/index.html#advanced-apache-spark-mllib-example" rel="noopener ugc nofollow" target="_blank">在数据块上使用Apache Spark ml lib</a>(2021)，数据块</p><p id="c2f1" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated">[5] Umberto Griffo，<a class="ae kv" href="https://umbertogriffo.gitbook.io/apache-spark-best-practices-and-tuning/rdd/dont_collect_large_rdds" rel="noopener ugc nofollow" target="_blank">不要收集大型rdd</a>(2020)，Apache Spark最佳实践和调优</p><p id="9fd2" class="pw-post-body-paragraph ls lt iq lu b lv mu jr lx ly mv ju ma lf mw mc md lj mx mf mg ln my mi mj mk ij bi translated">[6]尼基莱什·努卡拉，·朱，·布拉恰利，汤姆·戈登伯格(2019)，斯帕克·—<a class="ae kv" href="https://medium.com/quantumblack/spark-udf-deep-insights-in-performance-f0a95a4d8c62#:~:text=In%20these%20circumstances%2C%20PySpark%20UDF,two%20types%20of%20PySpark%20UDFs." rel="noopener">对性能的深刻见解</a>，昆腾布莱克</p></div></div>    
</body>
</html>