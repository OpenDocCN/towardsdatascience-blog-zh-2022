<html>
<head>
<title>Fastcoref — A Practical Package for Coreference Resolution</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">fastcoref——一个实用的共指消解包</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fastcoref-a-practical-package-for-coreference-resolution-bfbcb749e464#2022-09-10">https://towardsdatascience.com/fastcoref-a-practical-package-for-coreference-resolution-bfbcb749e464#2022-09-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="549f" class="pw-subtitle-paragraph jo ip iq bd b jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dk translated">理解自然语言处理中的共指消解的F-coref模型背后的主要工作，以及如何通过fastcoref直观包使用它</h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/6a3777230f5842bed77a1e785d276d48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*25BzT4ytYmYcdzfK"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">fastcoref——瑞安·斯通在<a class="ae kw" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="5fe5" class="kx ky iq bd kz la lb lc ld le lf lg lh jx li jy lj ka lk kb ll kd lm ke ln lo bi translated">介绍</h1><p id="b32d" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">本周，以色列巴尔伊兰大学的Shon Otmazgin，Arie Cattan和Yoav Goldberg教授发布了一个名为'<em class="ml">【fastcoref '</em>【2】的共指消解包，它快速、准确且易于使用。在本文中，我们将了解这个包背后的主要工作以及如何使用它。你也可以在由Ari Bronstein 创建的huggingface中看到这个伟大包的一个<a class="ae kw" href="https://huggingface.co/spaces/pythiccoder/FastCoref" rel="noopener ugc nofollow" target="_blank">演示。</a></p><p id="215b" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">共指消解是识别文本提及(又名:提及检测)和链接文档中相互引用的实体(又名:共指决策)的任务。让我们看看劳拉·杨的这幅漫画，试着理解哪些实体是相同的。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/434bfaece1348fb8c14e8cc6c11d15bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*atEVQPGDo5bW7El7.jpg"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">劳拉·杨关于杨经济学的漫画</p></figure><p id="c302" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">“爸爸”、“他”和“他的”指的是同一个实体，识别这些实体并将它们链接到它们的前身是我们在NLP共指解析中所说的。我假设你知道共指消解任务和模型，如果不知道的话<a class="ae kw" href="https://galhever.medium.com/a-review-to-co-reference-resolution-models-f44b4360a00" rel="noopener">这里</a>是一篇很好的文章，可以给你更多的Gal Hever的背景知识。</p><p id="fd59" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">共指任务是用于附加NLP任务的基本任务，例如:信息抽取、问题回答、摘要、机器翻译等等。尽管它作为一个基本的NLP任务很重要，但当前的研究未能实现快速工作或在有限资源下工作的体面模型。作者介绍的模型，<em class="ml">F-coref’</em>【1】，由于结合了<strong class="lr ir">硬目标蒸馏模型</strong>和<strong class="lr ir">高效分批实现</strong>(他们命名为“剩余分批”)。</p><h1 id="c5c4" class="kx ky iq bd kz la lb lc ld le lf lg lh jx li jy lj ka lk kb ll kd lm ke ln lo bi translated">背景——S2E和林姆斯模型</h1><p id="edfc" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">一般的神经协同参考模型由三个模块组成。一个语境化的编码器，将单词嵌入到向量中，一个提及记分器，一旦我们有了提及，一个共指记分器。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ms"><img src="../Images/816741d87b497ce0441d50933502a9b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lR_j9KNhXyEfw_A_1uQEfw.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">通用共同参考模型管道</p></figure><p id="1144" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated"><em class="ml"> F-coref </em>模型基于S2E模型[3]。S2E是一个神经模型，它将每个区间表示为其开始和结束标记的函数。它的架构包括:Longformer(一个上下文化的编码器)，一个参数化的提及评分函数(f_m)和一个参数化的成对先行评分函数(f_a)。</p><p id="79f9" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">注意，为了节省计算时间，antecedent函数只对提及分数最高的λT区间进行评分。T-令牌数。λ-修剪超参数设置为0.4，以实现高提及召回率。</p><p id="3ece" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">分数计算为两个提及分数和配对前提分数之和。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/0a8a379bb56e8a9fa7bda3f83e65d24f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*0si2l5wzS0-CKPVQflhNiQ.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">分数计算为两个提及分数和配对前提分数的总和[1]</p></figure><p id="8452" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">整个模型由26层和494M参数组成。</p><p id="1da3" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">最近的工作，LINGMESS模型[4]，通过为每种类型的提及引入多个评分器来提高共指准确性。这导致了SOTA精确度，但是该模型不如S2E有效。</p><h1 id="404d" class="kx ky iq bd kz la lb lc ld le lf lg lh jx li jy lj ka lk kb ll kd lm ke ln lo bi translated">F-coref模型</h1><p id="3043" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">现在让我们来了解一下<em class="ml"> F-coref </em>的要素是什么，才能达到极高的效率。我们将首先讨论知识提炼，然后讨论最大化并行性。</p><h1 id="20c1" class="kx ky iq bd kz la lb lc ld le lf lg lh jx li jy lj ka lk kb ll kd lm ke ln lo bi translated">知识蒸馏—缩小模型尺寸</h1><p id="17d3" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">知识提炼是将知识从大模型转移到小模型的过程。当我们的目标是建立快速模型时，我们需要较少的参数。知识提炼是减少模型中的参数数量，同时仍然保持其大部分准确性的一个很好的方法。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/35b3807f46e78359e506bd65a9f9a935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Gq8ZFrjKuuqjVlMM"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><a class="ae kw" href="https://unsplash.com/@sofatutor" rel="noopener ugc nofollow" target="_blank">主持人</a>在<a class="ae kw" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的“师生蒸馏”照片</p></figure><h2 id="5e04" class="mu ky iq bd kz mv mw dn ld mx my dp lh ly mz na lj mc nb nc ll mg nd ne ln nf bi translated">师生模式</h2><p id="0c8d" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">为了训练F-coref，作者使用LINGMESS作为教师模型。学生模型的构建与S2E模型相似，但稍有修改:层数更少，因此参数也更少，相对较慢的Longformer被DistilRoBERTa取代。总的来说，参数的数量从494M减少到91M，层数从26层减少到8层。<br/>他们用<strong class="lr ir">硬目标知识提炼</strong>。也就是说，教师模型充当未标记数据的注释器，学生模型从这些注释中学习，而不是学生根据教师模型的逻辑进行学习的软版本。软提取对于共指任务不起作用的原因是提及修剪和传递性的违反(这很有趣，可以帮助您选择提取模型，您可以在论文[1]中了解更多)。</p><h1 id="d7f9" class="kx ky iq bd kz la lb lc ld le lf lg lh jx li jy lj ka lk kb ll kd lm ke ln lo bi translated">最大化并行性——让它更快</h1><p id="fb89" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">作者使用了更低的λ进行修剪(0.25而不是0.4)，他们声称这在不影响性能的情况下减少了2.56对的数量。他们还使用了一个新版本的动态批处理，将文档数量分批到某个阈值，他们将其命名为<em class="ml">“剩余文件分批”</em>。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/5d4c9742448406a7ee9f1cf7f6f1672c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YRincqOd7Pb1fY7N"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><a class="ae kw" href="https://unsplash.com/@maplerockdesign" rel="noopener ugc nofollow" target="_blank"> Richad Bell </a>在<strong class="bd ng"> </strong> <a class="ae kw" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的“剩余配料”照片</p></figure><h2 id="dc53" class="mu ky iq bd kz mv mw dn ld mx my dp lh ly mz na lj mc nb nc ll mg nd ne ln nf bi translated">剩菜配料</h2><p id="4f3e" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">共指模型中最耗时的部分是编码器。传统上，长文档被分割成不重叠的<em class="ml"> max_length </em>的片段，每个片段被编码。如果文档比<em class="ml"> max_length </em>长，它将被分割成两个或多个片段，最后一个片段将被填充到<em class="ml"> max_length </em>。这导致大量填充令牌。</p><p id="4dee" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">作者建议的改变是创建两个单独的批次，一个用于没有填充的完整片段，另一个用于剩余片段。然后，他们将第二批填充到<em class="ml">最大剩余长度</em>，而不是将剩余部分填充到<em class="ml">最大长度</em>。注意，这种剩余的批处理对其他ML任务也是有用的。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nh"><img src="../Images/a5cd0ea0010e564b11d7202fedec6e43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z2MRIxETbHyhwUAeQnDhRQ.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">传统配料与剩余配料的比较[1]</p></figure><h1 id="cf50" class="kx ky iq bd kz la lb lc ld le lf lg lh jx li jy lj ka lk kb ll kd lm ke ln lo bi translated"><em class="jn"> F-coref </em>结果</h1><p id="cc7d" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">现在我们要测试新模型在推理时间和F1分数方面的表现。我们预计F1的分数会比SOTA联合参考模型略低，但推断时间会快得多。</p><p id="b652" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">实验装置使用多新闻数据集对笔记数据集训练师生模型。F-coref 的F1平均得分为78.5。如果我们将其与之前讨论的两个模型进行比较，<em class="ml"> F-coref </em>与LINGMESS(老师)相比下降了2.9 F1分，与s2e模型相比下降了1.8 F1分。</p><p id="beb9" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">现在，作者比较了2.8K文档推理的每个共指模型的时间。我们看到<em class="ml"> F-coref </em>比之前最快的型号平均三次跑快一个<strong class="lr ir">数量级。我们看到最显著的时间减少是由于蒸馏模型。后来，配料和剩余配料进一步将时间减少到只有25秒。</strong></p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ni"><img src="../Images/34511410cc07159ca92d4b336991cfc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XPedDtNFX34RNW0E-BwrlQ.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">F-coref比以前最快的模型快一个数量级[1]</p></figure><p id="e8eb" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">对于应用数据科学来说，这种权衡看起来很好，因为在应用数据科学中，资源有限，需要快速的推理时间。</p><h1 id="8b98" class="kx ky iq bd kz la lb lc ld le lf lg lh jx li jy lj ka lk kb ll kd lm ke ln lo bi translated">一个直观的软件包</h1><p id="d6c9" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">作者还发布了一个非常直观的软件包供您使用。让我们看看如何使用它。</p><ol class=""><li id="b3f7" class="nj nk iq lr b ls mm lv mn ly nl mc nm mg nn mk no np nq nr bi translated"><strong class="lr ir">安装包</strong> <br/>包是pip可安装的</li></ol><pre class="kh ki kj kk gt ns nt nu nv aw nw bi"><span id="87f4" class="mu ky iq nt b gy nx ny l nz oa">pip install fastcoref</span></pre><p id="ccf3" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">2.<strong class="lr ir">导入模型</strong> <br/>导入<em class="ml"> F-coref </em>模型</p><pre class="kh ki kj kk gt ns nt nu nv aw nw bi"><span id="d99e" class="mu ky iq nt b gy nx ny l nz oa">from fastcoref import FCoref<br/>model = FCoref(device='cuda:0')</span></pre><p id="e9ca" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">3.<strong class="lr ir">预测共指</strong></p><pre class="kh ki kj kk gt ns nt nu nv aw nw bi"><span id="72a5" class="mu ky iq nt b gy nx ny l nz oa">preds = model.predict(<br/> texts=[‘Fastcoref is a great package. I am glad Noa introduced it to me.’]<br/>)</span><span id="1a20" class="mu ky iq nt b gy ob ny l nz oa">preds[0].get_clusters()</span><span id="bd0a" class="mu ky iq nt b gy ob ny l nz oa">&gt;&gt; [['Fastcoref', 'it'], ['I', 'me']]</span></pre><p id="ed6c" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">当比较<em class="ml"> F-coref </em>和LingMessCore模型在CPU上的简单示例的推理时间时，我们看到<em class="ml"> F-coref </em>实现了1/5的wall时间和1/18的CPU时间。</p><p id="ed1d" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">F-coref</p><pre class="kh ki kj kk gt ns nt nu nv aw nw bi"><span id="a277" class="mu ky iq nt b gy nx ny l nz oa">CPU times: user 595 ms, sys: 22 ms, total: 617 ms<br/>Wall time: 494 ms</span></pre><p id="3d26" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">凌梅斯科尔</p><pre class="kh ki kj kk gt ns nt nu nv aw nw bi"><span id="6932" class="mu ky iq nt b gy nx ny l nz oa">CPU times: user 10.7 s, sys: 617 ms, total: 11.3 s<br/>Wall time: 2.42 s</span></pre><p id="80de" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">请注意，您也可以训练蒸馏您自己的模型(参见github的说明)</p><h1 id="cc15" class="kx ky iq bd kz la lb lc ld le lf lg lh jx li jy lj ka lk kb ll kd lm ke ln lo bi translated">结论</h1><p id="8676" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">如果你有一个需要大规模处理的共指任务，我相信使用<em class="ml"> fastcoref </em>是合适的。该软件包易于使用，速度快，并保持同样的精度。</p><h1 id="45ba" class="kx ky iq bd kz la lb lc ld le lf lg lh jx li jy lj ka lk kb ll kd lm ke ln lo bi translated">资源</h1><p id="d610" class="pw-post-body-paragraph lp lq iq lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">1.F-COREF:快速、准确且易于使用的共指消解(Otmazgin等人)</p><div class="oc od gp gr oe of"><a href="https://arxiv.org/abs/2209.04280" rel="noopener  ugc nofollow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd ir gy z fp ok fr fs ol fu fw ip bi translated">F-COREF:快速、准确且易于使用的共指消解</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">我们介绍fastcoref，这是一个python包，用于快速、准确且易于使用的英语共指解析。的…</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">arxiv.org</p></div></div><div class="oo l"><div class="op l oq or os oo ot kq of"/></div></div></a></div><p id="a8fc" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">2.Fastcoref软件包(Otmazgin等人)</p><div class="oc od gp gr oe of"><a href="https://github.com/shon-otmazgin/fastcoref" rel="noopener  ugc nofollow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd ir gy z fp ok fr fs ol fu fw ip bi translated">GitHub - shon-otmazgin/fastcoref</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">fastcoref Python包为共指信息提供了一个简单快速的API，只需要几行代码…</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">github.com</p></div></div><div class="oo l"><div class="ou l oq or os oo ot kq of"/></div></div></a></div><p id="c102" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">3.无跨度表示的共指消解(Kirstain等人，ACL 2021)</p><div class="oc od gp gr oe of"><a href="https://aclanthology.org/2021.acl-short.3/" rel="noopener  ugc nofollow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd ir gy z fp ok fr fs ol fu fw ip bi translated">无跨度表示的共指消解</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">尤瓦尔·克尔斯坦，奥里·拉姆，奥迈尔·利维。计算科学协会第59届年会论文集</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">aclanthology.org</p></div></div></div></a></div><p id="177c" class="pw-post-body-paragraph lp lq iq lr b ls mm js lu lv mn jv lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">4.LingMess:基于语言学的多重专家打分器，用于共指消解</p><div class="oc od gp gr oe of"><a href="https://arxiv.org/abs/2205.12644" rel="noopener  ugc nofollow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd ir gy z fp ok fr fs ol fu fw ip bi translated">LingMess:基于语言学的多重专家打分器，用于共指消解</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">虽然共指消解通常涉及各种语言挑战，但最近的模型是基于单个…</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">arxiv.org</p></div></div><div class="oo l"><div class="ov l oq or os oo ot kq of"/></div></div></a></div></div></div>    
</body>
</html>