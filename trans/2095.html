<html>
<head>
<title>Procedural pun and word game generation with Tries and GPT-3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">程序双关语和文字游戏生成与尝试和GPT-3</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/procedural-pun-and-word-game-generation-with-tries-and-gpt-3-3661d81a4204#2022-05-10">https://towardsdatascience.com/procedural-pun-and-word-game-generation-with-tries-and-gpt-3-3661d81a4204#2022-05-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2833" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我们能教一个人工智能按需产生源源不断的笑话吗？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3d67291bd81ed7e209a7e85c37abf265.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DD0Vhsx8HZJ0V28p"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">埃里克·克鲁尔在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><blockquote class="kw kx ky"><p id="ebe6" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">游泳者为什么要吟诵？成为一名游泳商人！</p></blockquote><p id="da61" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">到本文结束时，你将拥有一个点击按钮就能产生成千上万个笑话的人工智能。</p><p id="ca42" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">我一直喜欢各种各样的双关语。有些聪明，有些笨拙可爱。我最喜欢的是同形双关语，它使用的单词书写相似，但具有不同的意思或声音。海盗可能会大喊“我的裤子！”，而通勤者可能会抱怨地铁用户数量的增加。以一种巧妙的方式将这些联系起来，创造了一个令人愉快的脑海形象:一个海盗在地铁里被偷了裤子。想出这些本身就是一件有趣的事情，但是作为一名计算机科学家，我开始怀疑我们是否可以完全自动化这种类型的幽默。幽默被认为是将我们与机器区分开来的独特的人类特征。笑话吐机会是什么样子？</p><p id="ad91" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">这篇文章探讨了一个简短的下午如何生成程序性笑话。它需要一些巧妙的数组结构来构建可应用的笑话基线，并利用神经网络将这些基线转化为笑话。最后，我们有了一台可以工作的笑话机器，它可以在几秒钟内产生成千上万个笑话。该程序最初是为我的母语(芬兰语)编写的，但由于流行的NLP模型，我将该模型改编为我们时代的<em class="lb">通用语</em>，英语。通过非常有限的调谐，同样的原理可以用于用法语、西班牙语或蒙古语讲笑话。</p><p id="1802" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">你可能已经猜到了，文章的第一个笑话是由一台机器生成的。</p><h2 id="1e32" class="lz ma iq bd mb mc md dn me mf mg dp mh lw mi mj mk lx ml mm mn ly mo mp mq mr bi translated">一个好笑话需要多少次尝试？</h2><blockquote class="kw kx ky"><p id="e16d" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">一个</p></blockquote><p id="d68d" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">从技术上来说，只需要一次尝试就能产生所有的笑话。我们的目标很简单:想出一种方法来计算英语语料库中的单词可以被划分和重新排列成新单词的所有方式。在这个挑战中，我们将看到一组非常特殊的单词，其中一个很好的例子可能是<em class="lb"> maples </em>和<em class="lb"> lessons </em>。枫树和课程可以排列成<em class="lb">地图课程</em>和<em class="lb">枫树之子</em>。开个玩笑，你可能会立刻想象一个多伦多曲棍球迷在研究一张地图。确切地说，我们正在寻找满足以下原则的两个词:</p><ol class=""><li id="6d54" class="ms mt iq lc b ld le lg lh lw mu lx mv ly mw lv mx my mz na bi translated">这些单词都是标准的、可以理解的英语(或者斯瓦希里语、匈牙利语或者任何其他语言)</li><li id="3795" class="ms mt iq lc b ld nb lg nc lw nd lx ne ly nf lv mx my mz na bi translated">这些单词有一个共同的部分——让我们称之为<em class="lb">连接器</em>。该部分在第一个单词的末尾和第二个单词的开头(在我们的例子中，这是<em class="lb"> les </em></li><li id="ec51" class="ms mt iq lc b ld nb lg nc lw nd lx ne ly nf lv mx my mz na bi translated">即使我们去掉这个共同的部分，单词也是有意义的:<em class="lb">地图</em>和<em class="lb">儿子</em></li><li id="9201" class="ms mt iq lc b ld nb lg nc lw nd lx ne ly nf lv mx my mz na bi translated">对于我的应用程序，我想将连接器限制为英语中的单词<em class="lb">而不是</em>，并且至少有几个字符长。虽然这肯定是可能的，但我只是觉得没有那些明显的笑话，比如<em class="lb">梭子</em>和<em class="lb">棍子</em>(连接符:<em class="lb"> s </em>)或<em class="lb">工作簿</em>和<em class="lb">书虫</em>(连接符:<em class="lb">书</em>)，这些笑话会更加巧妙。</li></ol><p id="e435" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">最简单的方法是遍历英语中的所有单词，检查其中是否有与其他单词重叠的子串。英语有大约170，0 00个单词(取决于你问谁，你在哪里划分复合词)，即使通过限制更有用的单词，我们最终也能得到一个数万单词的字典。我用一本有58109个单词的字典找到了<a class="ae kv" href="http://www.mieliestronk.com/wordlist.html" rel="noopener ugc nofollow" target="_blank">这里</a>。遍历所有这些单词，将它们相互匹配并找出子串的复杂度是O(m *n)，其中n是单词的数量，m是单词的平均长度。我们能做得比蛮力更好吗？研究了一下之后，我决定创建一个<a class="ae kv" href="https://en.wikipedia.org/wiki/Trie" rel="noopener ugc nofollow" target="_blank"> Trie </a>的实现，它的复杂度总是为O( <em class="lb"> n </em>)。如果您不熟悉尝试，它们是搜索树的一个子组，其中搜索字符串是一系列字符(单词)，每个字符将树分成子树。单词的最后一个字母在包含要获取的值的叶子处结束。从某种意义上说，它们是紧凑的哈希表，其中的哈希键充当字符串。在我们的应用程序中，我们对使用该结构获取数据不感兴趣，而是构建一个包含所有单词逻辑的可查询结构。下图阐明了该系统:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="ab gu cl ng"><img src="../Images/66515fb4240152b522341df40c2db092.png" data-original-src="https://miro.medium.com/v2/format:webp/1*jr5qlQ-V5LRJLeiP7G_Z3g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="19b8" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">蓝色节点是<em class="lb">问</em>和<em class="lb">灰</em>以及<em class="lb">看护</em>和<em class="lb">推车</em>的共同根。另外，<em class="lb"> as </em>和<em class="lb"> car </em>都是有效的英文单词。回想一下我们的先决条件列表，如果一个节点包含子节点并且本身就是一个单词，我们可以简单地存储——这将立即满足所有三点。从以这种方式构建的trie中收集所有单词的复杂度是O( <em class="lb"> n </em>)。</p><p id="fb07" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">显然，这只是解决了问题的前半部分，我们仍然需要想出一种方法来解释单词的常见结尾。但我们稍后会回到这个话题。下面是Tries的实现，我对它进行了修改，以便在构造期间获取元数据而不是叶值。</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="d44f" class="lz ma iq ni b gy nm nn l no np">class trieNode():<br/><br/>  def __init__(self, letter = None):<br/><br/>    # Keep track of children as nodes and letters<br/>    self.children = []<br/>    self.children_nodes = [] <br/>    self.is_leaf = False<br/>    self.letter = letter<br/><br/>    # Utility: store the "words" up to this point, <br/>    # as well as all the child strings that follow this.<br/>    # This exhanges some memory for easier lookup later<br/>    # Additionally, we store how many times a node has been visited <br/>    self.string = None<br/>    self.child_strings = []<br/>    self.visited = 0<br/><br/>class Trie():<br/><br/>  def __init__(self):<br/>    self.root = trieNode()<br/>    self.n_nodes = 0<br/><br/>  def insert(self, key):<br/>    <br/>    # Start at the root<br/>    pointer = self.root<br/>    <br/>    idx = 0<br/><br/>    for i in key:<br/>      if i in pointer.children: # If the child exists<br/>        order = pointer.children.index(i) # simply move forward<br/>        pointer = pointer.children_nodes[order]<br/>      else: # Otherwise create and append a new node<br/>        pointer.children.append(i)<br/>        pointer.children_nodes.append(trieNode(i))<br/>        self.n_nodes += 1<br/>        pointer = pointer.children_nodes[-1]<br/>        pointer.string = key[0:(idx + 1)]<br/>      <br/>      # Update the other values<br/>      pointer.visited += 1<br/><br/>      # ...and if the node is a leaf, or if we should simply add new children<br/>      idx += 1<br/>      if idx == len(key):<br/>        pointer.is_leaf = True<br/>      else:<br/>        pointer.child_strings.append(key[(idx):len(key)])</span></pre><p id="9b2d" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">这使我们能够创建一个树模型，其中每个节点跟踪它是否是一个单词的结尾，它是否有孩子以及那些孩子是什么。作为一个额外的好处，创建这个表是一个单遍操作，之后我们可以无休止地查询结构，以获取任何以单词结尾的节点(要求2 &amp; 3)和具有不同长度结尾的节点(要求3 &amp; 4)。对于我的80k+单词的字典来说，这导致了一个由143 638个节点组成的trie，或者比单词数多60k多一点的节点。这只花了几秒钟。你可以把字典压缩成这样的数据结构，这真的很令人吃惊。</p><p id="1fdb" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">如果您想了解代码，可以在这里查看Colab<a class="ae kv" href="https://colab.research.google.com/drive/1pQJtILGQwOU4ZJfnAaqtTU1vzx6H6i3I?usp=sharing" rel="noopener ugc nofollow" target="_blank"/>。我将只在这里粘贴更有趣的代码，所以继续复制您自己的游戏场以获得完整的体验。</p><h2 id="18ab" class="lz ma iq bd mb mc md dn me mf mg dp mh lw mi mj mk lx ml mm mn ly mo mp mq mr bi translated">文字游戏来了</h2><p id="3338" class="pw-post-body-paragraph kz la iq lc b ld nq jr lf lg nr ju li lw ns ll lm lx nt lp lq ly nu lt lu lv ij bi translated">还记得我说过，我们会回到如何想出词尾吗？现在是时候了。到目前为止，我们已经有了一种为许多结尾创建公共开头的方法。再想想我们如何实现相反的结果:获得许多前缀的共同结尾。</p><p id="272a" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi">…</p><p id="5a75" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">没错，我们可以利用现有资源。如果我们简单地翻转每个单词并创建我们的Trie，我们将得到一个由常见词尾和首字母前缀组成的结构。有一点工作在幕后来回翻动单词，但最终我们可以把所有的东西放在一起。</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="6adc" class="lz ma iq ni b gy nm nn l no np">def create_dict(trie):<br/><br/>  result = {}<br/><br/>  def crawl(trie):<br/><br/>    if len(trie.children_nodes) == 0:<br/>      return<br/><br/>    if trie.is_leaf and len(trie.children) &gt; 0:<br/>      for child_string in trie.child_strings:<br/>        if child_string not in result.keys():<br/>          result[child_string] = []<br/>        result[child_string].append(trie.string)<br/><br/>    for child in trie.children_nodes:<br/><br/>      crawl(child)<br/>  <br/>  crawl(trie)<br/><br/>  return result<br/><br/>def create_flipped_dict(trie):<br/><br/>  result = {}<br/><br/>  def crawl(trie):<br/><br/>    if len(trie.children_nodes) == 0:<br/>      return<br/><br/>    if trie.is_leaf and len(trie.children) &gt; 0:<br/>      for child_string in trie.child_strings:<br/>        flipped_string = child_string[::-1]<br/>        if flipped_string not in result.keys():<br/>          result[flipped_string] = []<br/>        result[flipped_string].append(trie.string[::-1])<br/><br/>    for child in trie.children_nodes:<br/><br/>      crawl(child)<br/>  <br/>  crawl(trie)<br/><br/>  return result<br/><br/>class jokeGenerator():<br/><br/>  def __init__(self):<br/><br/>    self.trie = Trie()<br/>    self.flipped_trie = Trie()<br/>    self.words = None<br/><br/>    self.result = None<br/>    self.flipped_result = None<br/>    self.common_keys = None<br/>    <br/>    self.wordplays = None<br/><br/>    self.tokenizer = None<br/>    self.model = None<br/><br/>  def loadWords(self, source):<br/><br/>    words = pd.read_csv(source, na_filter = False)<br/>    words = words.values.tolist()<br/>    words = [x[0] for x in words]    <br/><br/>    print(f'Loading {len(words)} words')<br/><br/>    i = 0<br/>    n_words = len(words)<br/>    for word in words:<br/>      i += 1<br/>      if i % int(n_words/10) == 0:<br/>        print(f'{int((i+1)/n_words*100)}% ({i}/{n_words})')<br/>      self.trie.insert(word)<br/>      self.flipped_trie.insert(word[::-1])<br/><br/>    print(f'Done')<br/><br/>    self.words = words<br/><br/>  # normal: all words<br/>  # not_short: the connector is longer than 2 characters<br/>  # not_a_word: all words, where the connecting word is not a word in itself<br/>  # not_a_short_word: all words, where the connecting word is not a word in itself and it is more than 2 chracters<br/>  # not_a_short_word_or_ing: all words, where the connecting word is not a word in itself and it is more than 2 chracters and is not "ing"<br/>  def generateWords(self, type = 'normal'):<br/><br/>    if self.flipped_trie == None or self.trie == None:<br/>      print('You must load the words first: loadWords(source)')<br/><br/>    self.flipped_result = create_flipped_dict(self.flipped_trie.root)<br/>    self.result = create_dict(self.trie.root)<br/><br/>    common_keys = list(set(self.result.keys()).intersection(self.flipped_result.keys()))<br/><br/>    if type == 'normal':<br/>      self.common_keys = common_keys<br/>    elif type == 'not_short':<br/>      self.common_keys = [x for x in common_keys if (len(x) &gt; 2)]<br/>    elif type == 'not_a_word':<br/>      self.common_keys = [x for x in common_keys if (x not in self.words and x != '-')]<br/>    elif type == 'not_a_short_word':<br/>      self.common_keys = [x for x in common_keys if (x not in self.words and x != '-' and len(x) &gt; 2)]<br/>    elif type == 'not_a_short_word_or_ing':<br/>      self.common_keys = [x for x in common_keys if (x not in self.words and x != '-' and x != 'ing' and len(x) &gt; 2)]<br/><br/>    self.wordplays = {}<br/>    for c_key in self.common_keys:<br/>      for r in self.result[c_key]:<br/>        for f_r in self.flipped_result[c_key]:<br/>          self.wordplays[f'{r}_{c_key}_{f_r}'] = [f'{r}', f'{c_key}',f'{f_r}']</span></pre><p id="636c" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">对于我们以这种方式收集的开始-结束对，我们可以简单地搜索前缀和后缀的共同匹配(联合)，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="ab gu cl ng"><img src="../Images/95b0f20e76e9b545a17d9ae318c134d3.png" data-original-src="https://miro.medium.com/v2/format:webp/1*UyUt63mDjK_4V6xRsjAXSA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="a5ed" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">哒哒，我们已经准备好了我们的同形双关语列表。事实上，10 174 817是我们的最终分数。这些大部分都不符合我们任意的第四个要求，所以让我们把它降低一点。您可能会注意到<em class="lb">generate words</em>-方法，它有许多过滤器。这些过滤连接器的长度，删除单词并确保它不会被过度表示(例如“ing”，它的两端和开头都是一堆英语单词)。按照我们最严格的标准，我们最终会有大约10 000个文字游戏。你可以在这里找到完整的名单:【https://oskarniemenoja.fi/files/wordplays.txt】T4，但这里有一些我最喜欢的:</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="c7a5" class="lz ma iq ni b gy nm nn l no np">['absorb', 'ent', 'rapping'], or absorbent entrapping<br/>['strut', 'ter', 'rains'], or strutter terrains<br/>['hi', 'res', 'cued'], or hires rescued<br/>['environ', 'ment', 'ally'], or environment mentally<br/>['hips', 'ter', 'race'], or hipster terrace<br/>['spa', 'res', 'ponds'], or spares responds<br/>['for', 'mal', 'formation'], or formal malformation<br/>['for', 'mer', 'its'], or former merits<br/>['sit', 'com', 'posers'], or sitcom composers<br/>['by', 'tes', 'table'], or bytes testable</span></pre><p id="362c" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">我们可以继续下去，但这些作为一个不知疲倦的缪斯为你不同质量的双关语加油。最棒的是，它们的数量高达1000万。</p><h2 id="f470" class="lz ma iq bd mb mc md dn me mf mg dp mh lw mi mj mk lx ml mm mn ly mo mp mq mr bi translated">听起来像是自然语言处理的工作</h2><p id="0644" class="pw-post-body-paragraph kz la iq lc b ld nq jr lf lg nr ju li lw ns ll lm lx nt lp lq ly nu lt lu lv ij bi translated">这一切都很好，我们有了一台机器，可以以你能想象的任何速度吐出双关语。但是想出笑话的角色还是在人类身上。我们能不能进一步自动化，并有一个真正的端到端的管道来产生幽默？我们当然可以。让我们给我们的类添加几个方法:</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="c193" class="lz ma iq ni b gy nm nn l no np">def loadModels(self, language = 'english'):<br/><br/>    if language == 'finnish':<br/>      self.tokenizer = GPT2Tokenizer.from_pretrained('Finnish-NLP/gpt2-finnish') <br/>      self.model = GPT2LMHeadModel.from_pretrained('Finnish-NLP/gpt2-finnish')<br/>    elif language == 'english':<br/>      self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large') <br/>      self.model = GPT2LMHeadModel.from_pretrained('gpt2-large')<br/>    # Adding new languages is laughably easy. Search for a passable model on Huggingface, chuck it here and let the script do it's magic.<br/><br/>  def generateJoke(self, first_string = '', second_string = '', n = 1, length = 30):<br/>    joke = self.wordplays[random.choice(list(self.wordplays.keys()))]<br/>    joke_words = joke[0] + joke[1] + ' ' + joke[2]   <br/>    joke_string = first_string + ' ' + joke_words + ' ' + second_string<br/><br/>    input_ids = self.tokenizer.encode(joke_string, return_tensors = 'pt')<br/><br/>    output = self.model.generate(input_ids, <br/>                        max_length = length,<br/>                        num_beams = n, <br/>                        num_return_sequences = n,<br/>                        no_repeat_ngram_size  = 3)<br/>    <br/>    result = []<br/>    for op in output:<br/>      result.append(self.tokenizer.decode(op))<br/><br/>    return result</span></pre><p id="cfe5" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">这些从<a class="ae kv" href="https://huggingface.co" rel="noopener ugc nofollow" target="_blank"> Huggingface </a>加载一个预先训练好的transformer模型，并使用我们预先生成的文字游戏中的提示生成一个字符串。我提供了两种语言，但是你可以自由地测试你自己的语言，并在评论中写下你的经历。可以说，同样的原则应该适用于任何语言，如果您只是将任何目标语言的字典输入到Tries中，并获取一个用您选择的语言训练的正确的转换器。然而，在实践中，我注意到芬兰语版本比英语版本差得多，所以你的里程可能会有所不同。</p><p id="58f1" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">即便如此，我们离真正的人工智能幽默又近了一步。看看这个坏小子吐出什么来。</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="5e87" class="lz ma iq ni b gy nm nn l no np">Prompt: Here is a funny story about environment ions and environ mentions<br/>Result: that I found on the internet. It is a story about a man who was trying to get a job in a chemical plant. He was told that the job was for a man with a PhD in chemistry. He went to the job interview and was told he was not qualified. He then went to his supervisor and said that he was qualified.</span></pre><p id="f816" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">以一种可爱的方式，这有点接近20世纪早期的幽默——与社会规范和读者的期望相矛盾。它还注意到这个笑话是关于环境化学的，甚至评论这个笑话是“在互联网上找到的”。虽然它没有赢得任何奖项，但它是可以理解的，看起来很可能像一个幽默感很差的人写的东西。谁会因为这个可爱的小机器人尽力而为而生气呢？</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="0e75" class="lz ma iq ni b gy nm nn l no np">Prompt: Tell me a joke about entrainment and mentally<br/>Result: weirdness. I'm not sure if I'm going to be able to do this. I'm not a comedian. I don't have a lot of experience with comedy. I've never done a standup show. I have no idea what I'm doing. I just want to do it. I want to be a comedian</span></pre><p id="293b" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">尽管如此，这些都不是我们所理解的幽默。看起来变形金刚在笑话方面没有得到足够的训练，而是尽力去写散文。我花了一些时间寻找一个专门针对笑话进行训练的模型，但它并没有产生太多不同的结果。我们能做得更好吗？事实证明，我们可以。</p><h2 id="2227" class="lz ma iq bd mb mc md dn me mf mg dp mh lw mi mj mk lx ml mm mn ly mo mp mq mr bi translated">请打开GPT-3</h2><p id="93ae" class="pw-post-body-paragraph kz la iq lc b ld nq jr lf lg nr ju li lw ns ll lm lx nt lp lq ly nu lt lu lv ij bi translated">OpenAI的可爱的人们已经将他们的GPT-3模型投入了公开测试，尽管它不是免费的(产生一个笑话需要大约0.003€，或者每1000个笑话需要3€)，但它在人类语言的细微差别方面表现得相当好。你可以在这里注册你的api密匙<a class="ae kv" href="https://beta.openai.com" rel="noopener ugc nofollow" target="_blank"/>。让我们修改代码来查询API。</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="2cb5" class="lz ma iq ni b gy nm nn l no np">import os<br/>import openai<br/><br/>words = eng_gen.wordplays[random.choice(list(eng_gen.wordplays.keys()))]<br/>input_word = ''.join(words[0:2]) + ' ' + words[2] + ' and ' + words[0] + ' ' + ''.join(words[1:3])<br/><br/>openai.api_key = "api_key_here"<br/>result = openai.Completion.create(<br/>  engine="text-davinci-002",<br/>  prompt=f"Write a joke containing the words '{input_word}'",<br/>  max_tokens=60,<br/>  temperature = 0.3,<br/>  n = 1,<br/>  echo = True<br/>)<br/><br/>result.to_dict()['choices'][0]['text']</span></pre><p id="b4c3" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">让我们看看结果如何</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="edf5" class="lz ma iq ni b gy nm nn l no np">Prompt: Write a joke containing the words 'potent angle and pot entangle'<br/>Result: What do you call a potent angle that's also good at entangling? A pot-entangle!</span></pre><p id="e983" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">你看看那个。它不仅能理解笑话的典型措辞，还能将单词本身的细微含义联系起来。这实际上让我笑了，它是100%程序生成的。我可以捣烂这个按钮，得到无数的笑话。实际上，在我们结束之前，让我们收拾一下，就这么做吧。</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="eeff" class="lz ma iq ni b gy nm nn l no np">def create_jokes(wordplays, n, api_key):<br/><br/>  results = pd.DataFrame({'input' : [], 'prompt' : [], 'answer' : []})<br/>  openai.api_key = api_key<br/><br/>  for i in range(n):<br/><br/>    words = wordplays[random.choice(list(wordplays.keys()))]<br/>    input_word = ''.join(words[0:2]) + ' ' + words[2] + ' and ' + words[0] + ' ' + ''.join(words[1:3])<br/>    prompt = f"Write a joke containing the words '{input_word}'"<br/><br/>    result = openai.Completion.create(<br/>      engine="text-davinci-002",<br/>      prompt=prompt,<br/>      max_tokens=60,<br/>      temperature = 0.3,<br/>      n = 1<br/>    )<br/>    answer = [input_word, prompt, result.to_dict()['choices'][0]['text']]<br/><br/>    results.loc[len(results)] = answer<br/><br/>  return results<br/><br/>create_jokes(eng_gen.wordplays, 10, "an_api_key")</span></pre><p id="5c53" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">让我们看看它为我们准备了什么。</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="aa87" class="lz ma iq ni b gy nm nn l no np">Why did the cables sons take cab lessons? So they could get a job as a taxi driver!<br/>  <br/>I was going to use a cleanser rated and clean serrated knife to make dinner, but then I realized that would be overkill.<br/><br/>What do you call a bagger who's also a manic German? A Bagger Manic Germanic!<br/><br/>What do you call a superintendent who's good at enticement? A superintend-ice-nt!</span></pre><p id="ab99" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">这些都出奇的不差。诚然，我必须按几次按钮才能得到一套令人尊敬的笑话，但尽管如此，这些都没有任何一个人扭曲和转折的话。你可以自己尝试一下colab，看看你能想到什么。</p><h2 id="8381" class="lz ma iq bd mb mc md dn me mf mg dp mh lw mi mj mk lx ml mm mn ly mo mp mq mr bi translated">结束语</h2><p id="2f93" class="pw-post-body-paragraph kz la iq lc b ld nq jr lf lg nr ju li lw ns ll lm lx nt lp lq ly nu lt lu lv ij bi translated">总而言之，这比我最初预期的要好得多。我开始创建一个不错的单词对字典来激发我自己的想象力。有了一些聪明的数据结构，我们现在可以用我们想要的任何语言获得无穷无尽的数据结构。将这些插入现成的NLP模型，产生了令人惊讶的好结果。有没有喜剧演员因为这个AI而倒闭？大概不会。但我可以很容易地看到一个由一个写笑话的机器人和一个语音合成器在舞台上共同表演组成的利基之旅。或者充满程序性笑话的书。这些类型的程序可以帮助作家、喜剧演员或编剧克服白皮书综合症，或者只是作为机器人如何实现令人惊讶的逼真效果的介绍。</p><p id="4b75" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">幽默是人类的典型特征之一。很难说机器能感觉到情感，悲伤、快乐或真正的笑声。几天前，我会认为他们缺乏语境意识，无法创造巧妙的文字游戏，将语言转换成新的含义。这个小实验是一个有趣的小练习，但却令人惊讶地深入探究了人类最终是什么。看着一个机器人写出可以理解、有时甚至是巧妙幽默的散文，感觉就像打破了一堵我不知道存在的墙。这种感觉更像是和电脑一起工作，而不是简单地命令它做什么。</p></div></div>    
</body>
</html>