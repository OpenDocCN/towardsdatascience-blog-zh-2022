<html>
<head>
<title>Pushing Explainable AI: Neural Networks Are Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">推动可解释的人工智能:神经网络是决策树</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pushing-towards-the-explainable-ai-era-neural-networks-are-decision-trees-1603ab97eb1b#2022-10-29">https://towardsdatascience.com/pushing-towards-the-explainable-ai-era-neural-networks-are-decision-trees-1603ab97eb1b#2022-10-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2ccb" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">探索一篇旨在解释DNN行为的新论文</h2></div><p id="2555" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最近，来自AAC Technologies的一位伟大的研究员卡格拉尔·艾特金发表了一篇题为“神经网络是决策树”的论文我仔细阅读了它，试图理解这篇论文的重大发现到底是什么。许多数据科学家可能会同意，许多转换会将一种算法转换成另一种算法。然而，(深度)神经网络(DNNs)很难解释。那么，艾特金是否发现了一些新的东西，让我们离可解释的人工智能时代更近了一步？</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/07d93688bee3612dcf669d81ced419b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*74RaPRhbftEqt1TUNTlPSw.jpeg"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">不溅</p></figure><p id="b477" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，让我们探索这篇论文，并试图理解这是否真的是一个新发现。或者，我们将检查这是否只是任何数据科学家在处理DNN可解释性挑战时需要了解和记住的一个重要焦点。</p><h1 id="d345" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated"><strong class="ak">简介和背景</strong></h1><p id="9e71" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">Aytekin证明了任何具有分段线性激活函数的经典前馈DNN(如ReLU)都可以用决策树模型来表示。让我们回顾一下两者之间的主要区别:</p><blockquote class="mo mp mq"><p id="3d3b" class="kf kg mr kh b ki kj jr kk kl km ju kn ms kp kq kr mt kt ku kv mu kx ky kz la ij bi translated">DNN拟合参数，以<strong class="kh ir">转换</strong>输入，并间接指导其神经元的激活。</p><p id="d3f8" class="kf kg mr kh b ki kj jr kk kl km ju kn ms kp kq kr mt kt ku kv mu kx ky kz la ij bi translated">决策树<strong class="kh ir">明确适合</strong>参数来引导数据流。</p></blockquote><p id="7334" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇论文的动机是<strong class="kh ir">解决DNN模型的黑箱性质</strong>并以另一种方式解释DNN行为。这项工作处理全连接和卷积网络，并提出了一个直接等价的决策树表示。因此，本质上，它检查了从DNN到决策树模型的转换，即在它们之间采用非线性的权重序列，并将其转换为新的权重结构。艾特金讨论的另一个结果是相应的DNN在计算复杂性方面的优势(存储内存更少)。</p><p id="2695" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Frosst和Hinton在他们的工作[4]“将神经网络提炼为软决策树”中提出了一种使用决策树来解释DNNs的伟大方法。然而，他们的工作不同于Aytekin的论文，因为他们结合了DNN和决策树的优点。</p><h1 id="28db" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated"><strong class="ak">想法的关键部分</strong></h1><p id="b0c2" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">通过计算新的权重来构建生成树:建议的算法采用到达网络的信号，并搜索ReLUs被激活和未被激活的信号。最终，算法(转换)替换/放置一个1(或斜率值)和0的向量。</p><p id="7e31" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该算法在所有层上运行。对于每一层，它查看来自前一层的输入是什么，并计算每个输入的依赖关系。实际上，在每一层中，一个新的<strong class="kh ir">有效滤波器</strong>被选择，因此它将被应用于网络输入(基于先前的决定)。通过这样做，全连通DNN可以被表示为单个决策树，其中通过变换找到的有效矩阵充当分类规则。</p><p id="bc10" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您也可以针对卷积层实现它。主要区别在于，许多决策是在部分输入区域上做出的，而不是对图层的整个输入做出的。</p><p id="ff57" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关于<strong class="kh ir">维数和计算复杂度:</strong>得到的决策树中类别的数量显得巨大。在一个完全平衡的树中，我们需要2的树的深度的幂(难处理)。然而，我们还需要记住提供无损修剪的违反规则和冗余规则。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mv"><img src="../Images/f18bb0e342c24d60a3d75cc5c26aadac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ad4SFOXUXYE12AagzZpj7w.jpeg"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">作者图片</p></figure><h1 id="7fcc" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated"><strong class="ak">总结和注意事项</strong></h1><ul class=""><li id="17dd" class="mw mx iq kh b ki mj kl mk ko my ks mz kw na la nb nc nd ne bi translated">这个想法适用于分段线性激活函数的DNN</li><li id="6ee0" class="mw mx iq kh b ki nf kl ng ko nh ks ni kw nj la nb nc nd ne bi translated">神经网络是决策树这一观点的基础并不新鲜</li><li id="6bc7" class="mw mx iq kh b ki nf kl ng ko nh ks ni kw nj la nb nc nd ne bi translated">就我个人而言，我发现解释和数学描述非常简单明了[1]，激励我使用它并推进可解释的人工智能领域</li><li id="95f2" class="mw mx iq kh b ki nf kl ng ko nh ks ni kw nj la nb nc nd ne bi translated">需要有人在ResNet上测试这个想法😊</li></ul><h1 id="d88c" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">参考</h1><p id="e29a" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated"><strong class="kh ir">原文可见:</strong><a class="ae nk" href="https://arxiv.org/pdf/2210.05189.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">https://arxiv.org/pdf/2210.05189.pdf</strong></a></p><p id="04e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[1]艾特金，卡格拉尔。"神经网络是决策树."arXiv预印本arXiv:2210.05189  (2022)。</p><p id="7ed2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">如果你想看30分钟。关于论文的采访看这里:</strong></p><p id="4ed2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]伟大的扬尼克·基尔彻在YouTube上就这篇论文采访了亚历山大·马蒂克:<a class="ae nk" href="https://www.youtube.com/watch?v=_okxGdHM5b8&amp;ab_channel=YannicKilcher" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=_okxGdHM5b8&amp;ab _ channel =扬尼克·基尔彻</a></p><p id="4b6c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">一篇关于将近似理论应用于深度学习以研究DNN模型如何以分层方式组织信号的伟大论文:</strong></p><p id="62d7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3]兰德尔·巴勒斯特里罗。“深度学习的样条理论。”机器学习国际会议。PMLR，2018。</p><p id="e419" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">结合了决策树和DNNs力量的伟大作品:</strong></p><p id="504a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4]弗罗斯特、尼古拉斯和杰弗里·辛顿。"将神经网络提炼为软决策树."arXiv预印本arXiv:1711.09784 (2017)。</p><p id="4176" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">你可以在Medium上阅读一篇总结这项工作的帖子【4】:</strong></p><p id="f398" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[5]razor think Inc，Medium，2019，将神经网络提取为软决策树。</p><h1 id="6de1" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">关于作者</h1><p id="6e82" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">Barak Or是一位企业家和AI &amp;导航专家；前高通。Barak拥有Technion的工程学硕士学位、理学学士学位和经济学学士学位。Gemunder奖的获得者。巴拉克完成了他在人工智能和传感器融合领域的博士学位。多篇论文和专利的作者。他是ALMA Tech的创始人兼首席执行官。一家人工智能和高级导航公司。</p></div></div>    
</body>
</html>