<html>
<head>
<title>The Essential Data Science Reference Notebook</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基本数据科学参考笔记本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-essential-data-science-reference-notebook-d71ebba922b6#2022-12-09">https://towardsdatascience.com/the-essential-data-science-reference-notebook-d71ebba922b6#2022-12-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="55ba" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">针对 EDA、特征工程、模型拟合等的单一参考</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/38827238352a5a011ecd536e2084d150.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jPq9PLk0I6iTCqXU"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">詹姆斯·哈里逊在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="fc4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我最近完成了内部调动，加入了 Meta 的核心数据科学团队。在与我的同事 Barry Zhang 和 Nicolas Lepore 研究这些采访时，我们发现没有单一的资源用于典型的数据科学编码功能，如 EDA 绘图、数据清理、数据转换、超参数优化、模型拟合、无监督学习等。所以我们决定创造一个。即使在我们的采访之后，我也不断地回头参考这个笔记本，并相信它是常用的 DS 编码函数的最大资源之一。</p><p id="88a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章详细介绍了我们笔记本的各个部分，并在底部提供了下载链接，对于那些可能会觉得有用的人来说。</p><h2 id="e717" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">快速免责声明</h2><p id="1ab2" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">每个部分出现的顺序通常是这些步骤通常执行的顺序，但并不完美。例如，大多数数据转换，如缩放和一个热编码，应该在将数据分成训练和测试后<em class="mt">完成，但为了简单起见，这款笔记本之前就有。</em></p><h2 id="be21" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">数据检查</strong></h2><p id="712f" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">第一步是检查数据集。我们感兴趣的是数据的形状和大小，特征的类型(字符串/浮点/整数等)。)，这些要素的类的分布或数量，以及是否有缺失或重复的值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="4411" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">EDA 绘图</h2><p id="7810" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">一旦我们知道了数据的基本情况，比如列名是什么，有多大，是否有空值等等。我们可以将它形象化，以获得更好的理解。下面，我们使用热图来可视化数据中的相关性，并查看空值的分布中是否存在模式。除此之外，散点图矩阵将显示所有数值的成对散点图网格。对于简单的双变量分析，我们还包括一次性散点图、十六进制图、箱线图、计数图和直方图。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/70fb3374e80380df5788f1f84ea72727.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3XGCNmANITlaS47PzG9g9w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">散点图矩阵，图片由作者提供</p></figure><h2 id="32a6" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">数据清理</strong></h2><p id="a49f" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">现在我们将注意力转向空值、重复值和异常值。对于空值，找出导致空值的原因并查看它们是否随机分布总是很重要的。然而，在访谈的短时间内，通常可以用均值/中值(对于连续特征)或众数(对于分类特征)快速估算。更高级的技术包括用剩余的特征建立一个模型来预测空值，但是这通常不会在面试或带回家的作业中出现。</p><p id="8bf1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果空值的数量相对于数据集的大小来说很小，并且它们是随机分布的，我们也可以像通常处理重复项一样删除它们。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="47e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然基于树的模型或神经网络对异常值相当稳健，但对线性模型进行 winsorize 是个好主意。我们提供了 3 个函数，用于基于以下内容裁剪裁剪异常值:</p><blockquote class="mx my mz"><p id="47be" class="kz la mt lb b lc ld ju le lf lg jx lh na lj lk ll nb ln lo lp nc lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">百分位数:</em> </strong> <em class="it">例如，剪切高于第 99 个百分位数截止值和低于 1%截止值的所有值。<br/> </em> <strong class="lb iu"> <em class="it">标准差:</em> </strong> <em class="it">裁剪所有偏离平均值超过 3 个标准差的值。<br/> </em> <strong class="lb iu"> <em class="it">四分位范围:</em> </strong> <em class="it">裁剪所有值 1.5*IQR 之外的 IQR 的上下界(旁注:Shivam Chaudhary 有一个很大的</em> <a class="ae ky" rel="noopener" target="_blank" href="/why-1-5-in-iqr-method-of-outlier-detection-5d07fdc82097"> <em class="it">文章</em> </a> <em class="it">关于我们为什么用 1.5 作为比例因子)</em></p></blockquote><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="1852" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">数据转换</h2><p id="f472" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在拟合模型之前，我们必须获得可用形式的数据。这通常涉及对分类和/或连续特征的某种特征变换。我们首先使用。select_dtypes()方法。</p><p id="f256" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于连续要素，如果我们使用的线性模型本身无法拟合非线性趋势，我们可能希望生成多项式要素。使用 StandardScaler 或 MinMaxScaler 缩放数字要素也是一个好主意，这将有助于模型更快地收敛，并且在进行正则化(如 Lasso 和 Ridge 回归)时是必要的，因为这会影响系数的大小。最后，您可能希望通过选择 1/0 截止值的阈值将连续要素转换为二进制(“二进制化”)。</p><p id="9693" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于分类特征，几乎所有用 Python 编写的学习算法都不能使用字符串特征，所以我们需要对列进行一次性编码。在这个过程中，我们使用 sklearn 中的 OneHotEncoder 类将一个字符串特性转换为一系列 1/0 指示列，用于特性的每个类。我们包括“drop =‘first’”参数，以避免<a class="ae ky" href="https://medium.com/nerd-for-tech/what-is-dummy-variable-trap-how-it-can-be-handled-using-python-78ec17246331" rel="noopener">虚拟变量陷阱</a>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="4f3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，并不是每个人都喜欢使用 sklearn 进行这些转换，所以我们也为下面的分类变量提供了标准缩放、最小-最大缩放、一次热编码和二进制化的替代方法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="c5af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，如果在 EDA 过程中，您观察到存在较大的标注不平衡，则可能有必要对数据集进行上采样或下采样，以强制这些类得到更平等的表示。有更复杂的方法来实现这一点(过采样:SMOTE &amp; ADASYN 算法，欠采样:Tomek 链接和基于聚类),但同样是为了采访或带回家作业的目的，只采取随机样本替换就足够了。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="cbfc" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">监督学习</h2><p id="70e0" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">现在我们可以开始建模过程了。Sklearn 的 API 是标准化的，每个建模过程本质上都是:</p><pre class="kj kk kl km gt nd ne nf bn ng nh bi"><span id="8dab" class="ni lw it ne b be nj nk l nl nm">model = Model()<br/>model.fit(X_train, y_train)<br/>model.predict(X_test)</span></pre><p id="1ec0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面我们从 sklearn 和 xgboost 中导入并实例化一些最常见的模型，以及它们的一些主要超参数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="13ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在模型中使用的上述超参数是任意选择的。可能需要执行网格搜索，为该模型和数据集选择最佳超参数。这里我们展示了一个简单的网格搜索，使用 GridSerachCV 库为随机森林分类器挑选最佳的<em class="mt"> n 估计值</em>和<em class="mt">最大深度</em>超参数。实际上，全网格搜索是相当计算密集型的，所以最好选择随机网格搜索或贝叶斯搜索，正如 Maria Gorodetski 在她的文章<a class="ae ky" rel="noopener" target="_blank" href="/bayesian-optimization-for-hyperparameter-tuning-how-and-why-655b0ee0b399">中所描述的。为了简单起见，我们只展示了一个简单的网格搜索，参数很少。</a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="6e72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们不进行交叉验证，通常我们会将我们的数据分成训练和测试，以训练模型，然后根据以前从未见过的数据对其进行评估，以获得对其性能的公平评估。Sklearn 的 train_test_split 是实现这一点的完美函数，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="e1e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦我们训练好了模型，我们就用它来预测测试集的值。对于回归，我们可以看看 r、MSE 和 MAE。对于分类，我们有准确度、精确度、召回率、F1 和 AUC。Sklearn 还有一个方便的功能，如果给定模型，它将绘制 ROC 曲线，并测试特征和标签。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/c49da5e97aa476bba95c02dfebc61ac7.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*pOecALq--aO2r2D7dTwFXQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">sklearn 的 ROC 曲线，图片作者</p></figure><p id="f3dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然 sklearn 是用于 ML 的最普遍的库，但是它的线性和逻辑回归的输出在某种意义上是有限的，因为它们只给出特征的系数。Statsmodel 将提供系数及其 p 值和其他有价值的信息，如 F 统计量、德宾-沃森统计量、AIC、BIC、r、调整后的 r 等。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="391f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用随机森林模型，我们无法像使用线性模型那样获得系数，但我们能够提取特征重要性:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="7f11" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">无监督学习</h2><p id="6d13" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">为了使笔记本更加完整，我们简单地加入了一些无人监管的技术。主成分分析(PCA)有助于降低数据集的维数，同时仍然保留其中的大部分信息。这可以帮助可视化数据，组合相关的特征，并加速大型学习算法的训练。在下面的代码片段中，我们将数据集“X”缩减为两个组件，并绘制它。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="6df9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们以 KMeans 聚类结束。除非你有一个源于领域知识的理由来选择 kmeans 的集群数量，否则在实践中大多数人使用如下所示的<a class="ae ky" href="https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/" rel="noopener ugc nofollow" target="_blank">肘方法</a>。在这种方法中，通过惯性(数据点到它们最近的聚类中心点的平方距离的总和)来绘制聚类的数量，并选择图中扭结处的聚类的数量。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="b7a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦选择了聚类的数量，拟合和预测与 sklearn 中的其他模型相似，只是它没有标签，因为这是无监督的学习。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="39bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的代码是从几十本书、无数 Kaggle 提交的内容和多年的经验中提炼出来的。其中没有一个非常优雅或复杂，但它为大多数数据科学函数提供了一个极好的参考。你可以从<a class="ae ky" href="https://github.com/eonofrey/ds_reference_notebook" rel="noopener ugc nofollow" target="_blank"> my </a>或者<a class="ae ky" href="https://github.com/nub3Ar" rel="noopener ugc nofollow" target="_blank"> Barry 的</a> Github 下载完整的 Python 笔记本。要完全访问 Medium，请单击此处的<a class="ae ky" href="https://eonofrey.medium.com/membership" rel="noopener"/>！</p></div></div>    
</body>
</html>