<html>
<head>
<title>Explainable AI for High-Resolution Images</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高分辨率图像的可解释人工智能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explaining-ai-for-high-resolution-images-fba0c743c76e#2022-06-14">https://towardsdatascience.com/explaining-ai-for-high-resolution-images-fba0c743c76e#2022-06-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ec86" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">AI可以用来根据高分辨率图像做出自动化决策，但我们能理解那些决策吗？在本文中，我将讨论如何使用可解释的多示例学习来解决这个问题。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b9b8da0b0c68b33dad2337130be1c6de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MuDAGAidzv-WR7xqi36fng.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">卫星数据是高分辨率图像的一个例子。美国宇航局在<a class="ae ky" href="https://unsplash.com/s/photos/satellite?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片。</p></figure><p id="324f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现代计算机视觉数据集可以包含<strong class="lb iu">数百万张图像</strong>。然而，这些图像通常尺寸很小。比如在ImageNet [1]中，平均图像<strong class="lb iu">只有469 x 387像素</strong>。但是如果每个图像的大小超过10，000 x 10，000像素呢？<strong class="lb iu">还是</strong> <strong class="lb iu">超过10万x 10万？</strong></p><p id="4467" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我探索了<strong class="lb iu">人工智能如何适应</strong>高分辨率图像，然后讨论了<strong class="lb iu">解释这些人工智能模型如何做出决策的方法</strong>。</p><h2 id="5195" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">高分辨率数据</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/bb3987ee7d4d5e01155ed0809723f298.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UfN7xGBEZ_NkAV_9-mnb_A.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数字组织病理学数据是高分辨率影像的另一个例子。<a class="ae ky" href="https://unsplash.com/@nci?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">国立癌症研究所</a>在<a class="ae ky" href="https://unsplash.com/s/photos/histopathology?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片。</p></figure><p id="74e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我深入研究机器学习如何应用于高分辨率图像之前，有必要讨论一下这个领域中实际存在的数据类型。我在上面的图片中给出了两个例子，并在下面进一步讨论:</p><p id="9613" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">卫星数据— </strong>通过卫星观测地球，从太空收集到的数据<strong class="lb iu">越来越多。收集的数据量是巨大的；每天超过100兆字节。这样收集的数据通常具有非常高的分辨率，并且还可以具有多个光谱带(例如，红外线以及可见光)。</strong></p><p id="dcd1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">医疗数据</strong> —某些类型的<strong class="lb iu">医疗数据是高分辨率的</strong>。例如，身体的组织样本可以被染色，并在显微镜下进行检查。这些样本然后可以被数字化以产生<strong class="lb iu">整个幻灯片图像</strong> (WSIs)，这是非常高的分辨率(<strong class="lb iu"> 100，000 x 100，000像素！</strong>)。</p><p id="0167" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这两个例子中，对于传统的计算机视觉 <strong class="lb iu">方法</strong>来说，处理高分辨率图像<strong class="lb iu">是不可能的，因为:</strong></p><ol class=""><li id="524f" class="mp mq it lb b lc ld lf lg li mr lm ms lq mt lu mu mv mw mx bi translated">如果图像被<strong class="lb iu">保持在原始分辨率</strong>，机器学习模型就会变得<strong class="lb iu">太大而无法训练</strong>。这是因为模型中参数的数量与输入图像的大小成比例。</li><li id="82c5" class="mp mq it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated">如果图像被<strong class="lb iu">重新采样以变小</strong>，重要的<strong class="lb iu">信息通常会丢失</strong>。例如，在WSI数据中，如果图像被下采样，则单个细胞将不可识别。或者在卫星数据中，<strong class="lb iu">分辨特征，例如单个建筑物将是不可能的</strong>。</li></ol><p id="d1a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，我们如何解决这个问题呢？在下一节中，我将讨论一种被称为<strong class="lb iu">多实例学习</strong>的流行方法。</p><h2 id="d57c" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">高分辨率图像的多示例学习</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/b1f874c45c5f34928965bad6a661dbc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LKqyh8bSKWXMKuQxnU6_RA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ne">左</strong>:一般MIL模型的解剖图(作者创作)。<strong class="bd ne">右</strong>:使用注意力的特定类型MIL模型(改编自【2】)。</p></figure><p id="6299" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在多实例学习(MIL)中，数据被<strong class="lb iu">组织成实例包</strong>。对于高分辨率图像，这采取的形式是<strong class="lb iu">将原始高分辨率图像</strong>分割成更小的图像块。然而，MIL只要求在袋子标签上贴标签。这意味着<strong class="lb iu">我们不必标记每一个实例</strong>，这节省了大量的时间和金钱。下面是一个简短的动画，更详细地解释了这个过程。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MIL管道的简短动画。作者创作的视频。</p></figure><p id="dc88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对高分辨率图像使用MIL的结果是它<strong class="lb iu">处理了上面强调的两个问题</strong>。首先，由于模型现在只处理更小的补丁，模型参数的<strong class="lb iu">数量远远少于</strong>。这使得实际训练模型成为可能。第二，提取的面片<strong class="lb iu">保留了在原始高分辨率图像中捕获的精细细节</strong>，因此没有信息丢失。</p><p id="bda2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">鉴于MIL是将机器学习应用于高分辨率图像的合适方法，我们如何理解经过训练的模型如何做出决策？</p><h2 id="a8db" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">解读多示例学习</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/1671b7f970ea285429334b917dfdb5e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iQwSfVspwFOjBELu-UW5rA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模型不可知的可解释性允许我们理解MIL模型如何做出决策。作者创造的形象。</p></figure><p id="fd8d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理解MIL模型如何做出决策是我最近一直在研究的一个领域【3】，这导致了一种新的方法:多实例学习局部解释(MILLI)。我是从模型不可知的角度来处理这个问题的。这意味着没有对潜在的故障指示灯模型进行假设。因此，我的方法<strong class="lb iu">适用于任何类型的MIL型号</strong>；当前或未来。</p><p id="ad2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一般来说，可解释故障指示灯的目的是理解两个目标:</p><ol class=""><li id="ec94" class="mp mq it lb b lc ld lf lg li mr lm ms lq mt lu mu mv mw mx bi translated"><strong class="lb iu">包里哪些</strong>是重要的实例？这些是用来做决定的例子。</li><li id="9d38" class="mp mq it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated">这些实例支持什么样的结果？不同的实例可以支持不同的结果，因此仅仅<strong class="lb iu">确定关键实例并不能提供完整的解释</strong>。</li></ol><p id="0388" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于高分辨率图像，回答这两个问题可以通过<strong class="lb iu">突出显示图像</strong>中支持不同结果的重要补丁来实现。例如，数字组织病理学可以<strong class="lb iu">识别指示癌症风险的不同类型细胞核</strong>【4】。然后在模型预测的同时提供决策的解释。这可用于通知最终用户为什么会做出特定决策，<strong class="lb iu">促进对模型的信任。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/056645026b1a281e95fc8ecadd92f64a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mPTZqbO1wvtOmjI-DtspAg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从袋子中取出一个实例，观察模型的预测如何变化，这意味着我们可以理解一个实例有多重要。作者创造的形象。</p></figure><p id="6075" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MILLI允许我们通过利用所有MIL模型的一个共同属性来理解MIL模型如何做出决策。由于MIL箱包的尺寸不必相同(即，每个箱包可以有不同数量的实例)，所有MIL型号在设计上都必须能够处理不同尺寸的箱包。这意味着我们能够<strong class="lb iu">从袋子中移除实例，并仍然做出预测</strong>，这是MIL <strong class="lb iu"> </strong>独有的属性(在非MIL模型中，不可能简单地移除特征)。通过移除实例并观察预测中的变化，可以构建一幅图，显示哪些<strong class="lb iu">实例驱动模型的决策，以及<strong class="lb iu">这些实例支持哪些</strong>结果。</strong></p><p id="f139" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以这种方式接近可解释的MIL比现有的方法更有效。这些现有的方法包括某些类型的故障指示灯模型，这些模型是<strong class="lb iu">固有的可解释的。</strong>作为处理过程的一部分，这些模型会产生自己对决策的解释。例如，这可以包括进行<strong class="lb iu">实例预测以及袋子预测</strong> [5]或者分配值来指示模型对每个实例 [2】的关注程度<strong class="lb iu">。不仅MILLI提供的解释比固有的可解释模型更加有效和准确，<strong class="lb iu"> MILLI是模型不可知的</strong>，这意味着它适用于任何类型的MIL模型。相反，固有的可解释方法<strong class="lb iu">只适用于特定类型的模型</strong>，限制了它们的适用性。</strong></p><h2 id="b4f8" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">摘要</h2><p id="25f3" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">高分辨率图像无法使用传统的机器学习技术进行处理。相反，可以使用一种称为<strong class="lb iu">多实例学习</strong>的特殊方法。新技术使我们能够理解经过训练的多实例学习模型如何使用数据来做出决策，并且<strong class="lb iu">向最终用户提供决策的解释</strong>。</p><h2 id="203f" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">参考</h2><p id="86c7" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">[1] J. Deng，W. Dong，R. Socher，L.-J. Li，K. Li，和l .，“Imagenet:<br/>大规模分层图像数据库，”2009年<em class="no">IEEE计算机视觉和模式识别会议</em>，第248–255页，IEEE，2009。<br/>【2】m . Ilse，J. Tomczak，M. Welling，“<a class="ae ky" href="https://arxiv.org/pdf/1802.04712.pdf" rel="noopener ugc nofollow" target="_blank">基于注意力的深度多重in- <br/>姿态学习</a>”，载于<em class="no">国际机器学习会议</em>，第2127-<br/>2136页，PMLR，2018。<br/>【3】j . Early、C. Evers和S. Ramchurn，“用于<br/>多示例学习的<a class="ae ky" href="https://arxiv.org/abs/2201.11701" rel="noopener ugc nofollow" target="_blank">模型不可知可解释性”，载于<em class="no">国际学习表示会议</em>，2022年。<br/>【4】k . Sirinukunwattana，S. E. A. Raza，Y.-W. Tsang，D. R. Snead，I. A. Cree，<br/>和N. M. Rajpoot，</a><a class="ae ky" href="https://ieeexplore.ieee.org/abstract/document/7399414" rel="noopener ugc nofollow" target="_blank">用于常规结肠癌组织学图像中细胞核的检测和分类的位置敏感深度学习</a>，<em class="no"> IEEE医学成像汇刊</em>，第35卷，第5期，第1196–1206页，2016年。<br/>【5】x . Wang，Y. Yan，P. Tang，X. Bai，和W. Liu，<a class="ae ky" href="https://www.sciencedirect.com/science/article/pii/S0031320317303382" rel="noopener ugc nofollow" target="_blank">重温多实例<br/>神经网络</a>，<em class="no">模式识别</em>，第74卷，第15-24页，2018</p></div></div>    
</body>
</html>