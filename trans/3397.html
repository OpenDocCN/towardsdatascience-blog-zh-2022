<html>
<head>
<title>Getting Started with Bloom</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Bloom入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/getting-started-with-bloom-9e3295459b65#2022-07-28">https://towardsdatascience.com/getting-started-with-bloom-9e3295459b65#2022-07-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a37e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用Bloom生成文本的概述和Codelab</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9db8cf60a036fb6530db733b4916c10a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dRfXf70--bxEdfFVbwG5Xw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">帕特里克·托马索在<a class="ae kv" href="https://unsplash.com/s/photos/text?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h2 id="c422" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">目录</h2><ul class=""><li id="bf8c" class="ls lt iq lu b lv lw lx ly lf lz lj ma ln mb mc md me mf mg bi translated"><a class="ae kv" href="#9e12" rel="noopener ugc nofollow">什么是布鲁姆？一些注意事项</a></li><li id="0da5" class="ls lt iq lu b lv mh lx mi lf mj lj mk ln ml mc md me mf mg bi translated"><a class="ae kv" href="#9b20" rel="noopener ugc nofollow">设置您的环境</a></li><li id="0ca9" class="ls lt iq lu b lv mh lx mi lf mj lj mk ln ml mc md me mf mg bi translated"><a class="ae kv" href="#3e75" rel="noopener ugc nofollow">下载预先训练好的标记器&amp;模型</a></li><li id="bfb1" class="ls lt iq lu b lv mh lx mi lf mj lj mk ln ml mc md me mf mg bi translated"><a class="ae kv" href="#83a2" rel="noopener ugc nofollow">运行推理:更好反应的策略</a></li><li id="92b7" class="ls lt iq lu b lv mh lx mi lf mj lj mk ln ml mc md me mf mg bi translated"><a class="ae kv" href="#b4a4" rel="noopener ugc nofollow">结论&amp;下一步</a></li><li id="6118" class="ls lt iq lu b lv mh lx mi lf mj lj mk ln ml mc md me mf mg bi translated"><a class="ae kv" href="#0624" rel="noopener ugc nofollow">参考文献</a></li></ul><h2 id="9e12" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">什么是盛开，为什么我们要小心行事</h2><p id="a463" class="pw-post-body-paragraph mm mn iq lu b lv lw jr mo lx ly ju mp lf mq mr ms lj mt mu mv ln mw mx my mc ij bi translated"><a class="ae kv" href="https://huggingface.co/bigscience/bloom" rel="noopener ugc nofollow" target="_blank"> Bloom </a>是来自<a class="ae kv" href="https://bigscience.huggingface.co/" rel="noopener ugc nofollow" target="_blank"> BigScience </a>的一个新的176B参数多语言LLM(大型语言模型)，由<a class="ae kv" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> Huggingface </a>主持，与全球数百名研究人员和机构进行开放合作。除了贡献者的多样性之外，Bloom最值得注意的一点是，Bloom是完全开源的，Huggingface通过其transformers API向公众提供了他们完整的(以及一些较小的)预训练模型。对LLM进行研究的其他组织，包括OpenAI、Meta和Google，已经选择将他们的LLM主要保留在内部，或者限制访问严格控制的封闭测试组。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/58d930dd7db2101fed059e50cc273ce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v4EDkX5CyDutoP7kotqwkg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者提供的图片；布鲁姆发的短信。提示:“爱丽丝穿过小门”</p></figure><p id="b7f7" class="pw-post-body-paragraph mm mn iq lu b lv na jr mo lx nb ju mp lf nc mr ms lj nd mu mv ln ne mx my mc ij bi translated">关于在现实世界中使用这些模型的危险，已经有了一场对话，更不用说让他们公开化了。担忧的范围从强化不公平和系统性偏见，到加速网上错误信息的传播。比我更有能力的声音，并继续倡导更多的人负责，透明和公平的发展和使用这一技术。如果你不熟悉，我鼓励你在这里停下来，花一些时间了解一下像<a class="ae kv" href="https://twitter.com/timnitGebru" rel="noopener ugc nofollow" target="_blank">蒂姆尼特·格布鲁</a> ( <a class="ae kv" href="https://www.dair-institute.org/" rel="noopener ugc nofollow" target="_blank"> DAIR研究所</a>)、<a class="ae kv" href="https://twitter.com/mmitchell_ai" rel="noopener ugc nofollow" target="_blank">玛格丽特·米歇尔</a>和人工智能合作伙伴团队等许多人的工作。</p><p id="96a2" class="pw-post-body-paragraph mm mn iq lu b lv na jr mo lx nb ju mp lf nc mr ms lj nd mu mv ln ne mx my mc ij bi translated">因此，我鼓励每个人坚持<a class="ae kv" href="https://huggingface.co/bigscience/bloom#intended-use" rel="noopener ugc nofollow" target="_blank">预期用途</a>，并在继续本<code class="fe nf ng nh ni b">Hello World</code>风格的介绍性教程时，注意<a class="ae kv" href="https://huggingface.co/bigscience/bloom" rel="noopener ugc nofollow" target="_blank">布鲁姆的模型卡</a>上列出的<a class="ae kv" href="https://huggingface.co/bigscience/bloom#risks-and-limitations" rel="noopener ugc nofollow" target="_blank">风险和限制</a>。</p><h2 id="9b20" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">设置您的环境</h2><p id="4b16" class="pw-post-body-paragraph mm mn iq lu b lv lw jr mo lx ly ju mp lf mq mr ms lj mt mu mv ln mw mx my mc ij bi translated">我们将在PyTorch中使用通用Bloom模型的1.3B参数版本，仅使用CPU运行推理。当我在Google Cloud的Vertex服务上使用Python 3 Jupyter Lab VM时，你应该能够在几乎任何本地或托管的*nix Jupyter环境上使用。</p><p id="e7ec" class="pw-post-body-paragraph mm mn iq lu b lv na jr mo lx nb ju mp lf nc mr ms lj nd mu mv ln ne mx my mc ij bi translated">首先，我们需要设置一个虚拟环境作为洁净室，以安装所有正确版本的依赖项。我们将创建一个名为<code class="fe nf ng nh ni b">.venv</code>的环境(它也产生一个同名的隐藏目录)，然后激活它开始工作:</p><pre class="kg kh ki kj gt nj ni nk nl aw nm bi"><span id="d77e" class="kw kx iq ni b gy nn no l np nq">pip install venv<br/>python -m venv .venv<br/>source .venv/bin/activate</span></pre><p id="cbed" class="pw-post-body-paragraph mm mn iq lu b lv na jr mo lx nb ju mp lf nc mr ms lj nd mu mv ln ne mx my mc ij bi translated">接下来，我们将把需要的包安装到我们的<code class="fe nf ng nh ni b">.venv</code>环境中:</p><pre class="kg kh ki kj gt nj ni nk nl aw nm bi"><span id="4919" class="kw kx iq ni b gy nn no l np nq">pip install transformers<br/>pip install torch torchvision torchaudio --extra-index-url <a class="ae kv" href="https://download.pytorch.org/whl/cpu" rel="noopener ugc nofollow" target="_blank">https://download.pytorch.org/whl/cpu</a></span></pre><p id="43bf" class="pw-post-body-paragraph mm mn iq lu b lv na jr mo lx nb ju mp lf nc mr ms lj nd mu mv ln ne mx my mc ij bi translated">最后，我们需要退出我们的venv，向Jupyter Lab注册我们的新环境作为内核，并重新启动它:</p><pre class="kg kh ki kj gt nj ni nk nl aw nm bi"><span id="521b" class="kw kx iq ni b gy nn no l np nq">deactivate<br/>ipython kernel install --user --name=venv<br/>source .venv/bin/activate</span></pre><p id="433a" class="pw-post-body-paragraph mm mn iq lu b lv na jr mo lx nb ju mp lf nc mr ms lj nd mu mv ln ne mx my mc ij bi translated">当你在Jupyter Lab中选择一个内核选项时，你应该会看到一个选项<code class="fe nf ng nh ni b">venv</code>。让我们选择并连接到它。</p><h2 id="3e75" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">下载预先训练的标记器和模型</h2><p id="c53d" class="pw-post-body-paragraph mm mn iq lu b lv lw jr mo lx ly ju mp lf mq mr ms lj mt mu mv ln mw mx my mc ij bi translated">启动我们的示例笔记本(在<a class="ae kv" href="https://github.com/dptrsa-300/start_with_bloom/blob/main/bloomex_nb.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上也有)，我们首先从之前安装到<code class="fe nf ng nh ni b">venv</code>的包中导入一些模块:</p><pre class="kg kh ki kj gt nj ni nk nl aw nm bi"><span id="51ce" class="kw kx iq ni b gy nn no l np nq">import transformers<br/>from transformers import BloomForCausalLM<br/>from transformers import BloomTokenizerFast<br/>import torch</span></pre><p id="c3b5" class="pw-post-body-paragraph mm mn iq lu b lv na jr mo lx nb ju mp lf nc mr ms lj nd mu mv ln ne mx my mc ij bi translated">现在到主事件，我们下载预训练的Bloom 1.3B参数通用LLM。虽然我还没有准确地确定它的大小，但似乎这个版本的模型的权重和偏差占用了大约1.5Gb的空间。关键的是，我们还需要获取布鲁姆的记号赋予器。这将允许我们将输入文本(“提示”)转换成Bloom可以理解的嵌入文本:</p><pre class="kg kh ki kj gt nj ni nk nl aw nm bi"><span id="585e" class="kw kx iq ni b gy nn no l np nq">model = BloomForCausalLM.from_pretrained("bigscience/bloom-1b3")<br/>tokenizer = BloomTokenizerFast.from_pretrained("bigscience/bloom-1b3")</span></pre><p id="5141" class="pw-post-body-paragraph mm mn iq lu b lv na jr mo lx nb ju mp lf nc mr ms lj nd mu mv ln ne mx my mc ij bi translated">说到这里，我们来设置一些全局，包括我们的提示文字:</p><pre class="kg kh ki kj gt nj ni nk nl aw nm bi"><span id="b9df" class="kw kx iq ni b gy nn no l np nq">prompt = "It was a dark and stormy night"<br/>result_length = 50<br/>inputs = tokenizer(prompt, return_tensors="pt")</span></pre><p id="f61a" class="pw-post-body-paragraph mm mn iq lu b lv na jr mo lx nb ju mp lf nc mr ms lj nd mu mv ln ne mx my mc ij bi translated">一些注意事项:</p><ul class=""><li id="056d" class="ls lt iq lu b lv na lx nb lf nr lj ns ln nt mc md me mf mg bi translated"><code class="fe nf ng nh ni b">result_length</code>校准我们从模型得到的提示响应的大小(以令牌为单位)。</li><li id="5f3d" class="ls lt iq lu b lv mh lx mi lf mj lj mk ln ml mc md me mf mg bi translated"><code class="fe nf ng nh ni b">inputs</code>包含<code class="fe nf ng nh ni b">prompt</code>的嵌入表示，专门由PyTorch编码使用。如果我们使用TensorFlow，我们会通过<code class="fe nf ng nh ni b">return_tensors="tf"</code>。</li></ul><h2 id="83a2" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">运行推理:更好反应的策略</h2><p id="4b2b" class="pw-post-body-paragraph mm mn iq lu b lv lw jr mo lx ly ju mp lf mq mr ms lj mt mu mv ln mw mx my mc ij bi translated">在我们向模型发送提示之前，我们需要考虑哪些解码/搜索策略可能最适合我们的用例。使用自回归转换器(为下一个令牌预测而训练),我们有许多选项来搜索答案空间以获得最“合理”的输出。Patrick von Platen<a class="ae kv" href="https://huggingface.co/patrickvonplaten" rel="noopener ugc nofollow" target="_blank">(hugging face)的这篇</a><a class="ae kv" href="https://huggingface.co/blog/how-to-generate" rel="noopener ugc nofollow" target="_blank">好文章</a>很好地解释了我们将要尝试的3种技术背后的细节和数学，所以我不会在这里重新发明轮子。然而，我会给你TL；每个的灾难恢复版本:</p><ul class=""><li id="92c1" class="ls lt iq lu b lv na lx nb lf nr lj ns ln nt mc md me mf mg bi translated"><strong class="lu ir">贪婪搜索</strong>简单地在每个时间步长t+1选择下一个单词，该单词在t处跟随该单词的预测概率最高。这里的一个主要问题是，如果贪婪搜索前面有一个在t处概率低的单词，则贪婪搜索将错过在t+1处概率高的单词</li><li id="487f" class="ls lt iq lu b lv mh lx mi lf mj lj mk ln ml mc md me mf mg bi translated"><strong class="lu ir">波束搜索</strong>跟踪第n(<code class="fe nf ng nh ni b">num_beams</code>)个最可能的字<em class="nu">序列</em>并输出最可能的<em class="nu">序列</em>。听起来不错，但是当输出长度变化很大时，这种方法就失效了——就像开放式文本生成一样。贪婪搜索和波束搜索也产生输出，其分布与人类可能执行相同任务的方式不太一致(即，两者都容易产生相当重复、乏味的文本)。</li><li id="1d81" class="ls lt iq lu b lv mh lx mi lf mj lj mk ln ml mc md me mf mg bi translated"><strong class="lu ir">用Top-k + Top-p </strong>采样是三种方法的组合。通过<strong class="lu ir">采样</strong>，我们的意思是基于其条件概率分布随机选择下一个单词(<a class="ae kv" href="https://huggingface.co/blog/how-to-generate#sampling" rel="noopener ugc nofollow" target="_blank"> von Platen，2020 </a>)。在<strong class="lu ir"> Top-k </strong>中，我们选择<code class="fe nf ng nh ni b">k</code>个最有可能的单词，然后在下一次抽签之前在它们之间重新分配概率质量。<strong class="lu ir"> Top-p </strong>为top-k增加了一个额外的约束，因为我们从累积概率超过<code class="fe nf ng nh ni b">p</code>的最小单词集中进行选择。</li></ul><p id="1976" class="pw-post-body-paragraph mm mn iq lu b lv na jr mo lx nb ju mp lf nc mr ms lj nd mu mv ln ne mx my mc ij bi translated">现在，我们将尝试所有3种策略，以便比较结果。：</p><pre class="kg kh ki kj gt nj ni nk nl aw nm bi"><span id="783e" class="kw kx iq ni b gy nn no l np nq"># Greedy Search<br/>print(tokenizer.decode(model.generate(inputs["input_ids"], <br/>                       max_length=result_length<br/>                      )[0]))</span></pre><blockquote class="nv nw nx"><p id="3383" class="mm mn nu lu b lv na jr mo lx nb ju mp ny nc mr ms nz nd mu mv oa ne mx my mc ij bi translated">那是一个漆黑的暴风雨之夜，风刮得很大。雪下得很大，地上覆盖着雪。所有的马都冻僵在地上，男人们蜷缩成一团</p></blockquote><pre class="kg kh ki kj gt nj ni nk nl aw nm bi"><span id="20b5" class="kw kx iq ni b gy nn no l np nq"># Beam Search<br/>print(tokenizer.decode(model.generate(inputs["input_ids"],<br/>                       max_length=result_length, <br/>                       num_beams=2, <br/>                       no_repeat_ngram_size=2,<br/>                       early_stopping=True<br/>                      )[0]))</span></pre><blockquote class="nv nw nx"><p id="2326" class="mm mn nu lu b lv na jr mo lx nb ju mp ny nc mr ms nz nd mu mv oa ne mx my mc ij bi translated">那是一个漆黑的暴风雨之夜，风刮得很大。我在路中间，这时我听到一声巨响。它来自我马路对面的房子。一个男人</p></blockquote><pre class="kg kh ki kj gt nj ni nk nl aw nm bi"><span id="553d" class="kw kx iq ni b gy nn no l np nq"># Sampling Top-k + Top-p<br/>print(tokenizer.decode(model.generate(inputs["input_ids"],<br/>                       max_length=result_length, <br/>                       do_sample=True, <br/>                       top_k=50, <br/>                       top_p=0.9<br/>                      )[0]))</span></pre><blockquote class="nv nw nx"><p id="ec68" class="mm mn nu lu b lv na jr mo lx nb ju mp ny nc mr ms nz nd mu mv oa ne mx my mc ij bi translated">那是一个漆黑的暴风雨之夜。快到中午了。当我下车脱鞋时，一个男人走到我面前坐下。他留着小胡子，浓密的头发和棕色的眼睛。男性</p></blockquote><h2 id="b4a4" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">结论和后续步骤</h2><p id="1baf" class="pw-post-body-paragraph mm mn iq lu b lv lw jr mo lx ly ju mp lf mq mr ms lj mt mu mv ln mw mx my mc ij bi translated">就个人而言，所有这些结果似乎大多是合理的。您会发现，当您迭代并调整参数和提示时，一些策略可能会为您的特定用例产生更优的输出。事实上，<a class="ae kv" href="https://blog.andrewcantino.com/blog/2021/04/21/prompt-engineering-tips-and-tricks/" rel="noopener ugc nofollow" target="_blank">构造提示来哄LLM做一些有用的事情</a>本身就有点像艺术和科学。</p><p id="da6a" class="pw-post-body-paragraph mm mn iq lu b lv na jr mo lx nb ju mp lf nc mr ms lj nd mu mv ln ne mx my mc ij bi translated">另外，在<strong class="lu ir">采样top-k + top-p </strong>输出<strong class="lu ir"> </strong>中，术语“夜晚”和输出“几乎中午”之间的不一致说明了一个有价值的观点，即<a class="ae kv" href="https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/" rel="noopener ugc nofollow" target="_blank">很容易将</a>LLM误认为是推理机器，它们使用世界的内部模型来构建自己的响应(像人类一样)。事实上，我们不需要深度学习、大数据或者LLMs来证明<a class="ae kv" href="https://99percentinvisible.org/episode/the-eliza-effect/" rel="noopener ugc nofollow" target="_blank">人类会拟人化任何东西</a>。相反，我们应该看到LLM的本来面目:语法上可信的句子生成器，应该睁大眼睛(以及大量减轻工程和包容性设计)部署它们的局限性。</p><p id="0760" class="pw-post-body-paragraph mm mn iq lu b lv na jr mo lx nb ju mp lf nc mr ms lj nd mu mv ln ne mx my mc ij bi translated">考虑到这一点，我自己与布鲁姆的旅程将沿着几条线索前进；很大程度上侧重于使文本生成和分类标题适应现代审计中的问题。具体来说:</p><ul class=""><li id="940f" class="ls lt iq lu b lv na lx nb lf nr lj ns ln nt mc md me mf mg bi translated">代码摘要。布鲁姆能用通俗易懂的英语概括一个代码块的逻辑吗？</li><li id="cea9" class="ls lt iq lu b lv mh lx mi lf mj lj mk ln ml mc md me mf mg bi translated">令牌分类的迁移学习。Bloom能被训练识别过程文件中的风险和/或控制吗？</li><li id="7a6a" class="ls lt iq lu b lv mh lx mi lf mj lj mk ln ml mc md me mf mg bi translated">可靠性。对于生成的摘要和分类的事实准确性，我们可以在Bloom预测中构建什么保证(如果有的话)?</li></ul><p id="ffdc" class="pw-post-body-paragraph mm mn iq lu b lv na jr mo lx nb ju mp lf nc mr ms lj nd mu mv ln ne mx my mc ij bi translated">快乐发电！</p><h2 id="0624" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">参考</h2><ul class=""><li id="23e6" class="ls lt iq lu b lv lw lx ly lf lz lj ma ln mb mc md me mf mg bi translated"><a class="ae kv" href="https://huggingface.co/bigscience/bloom-1b3" rel="noopener ugc nofollow" target="_blank">布鲁姆模特卡</a>，2022，拥抱脸</li><li id="ee1d" class="ls lt iq lu b lv mh lx mi lf mj lj mk ln ml mc md me mf mg bi translated"><a class="ae kv" href="https://huggingface.co/docs/transformers/model_doc/bloom" rel="noopener ugc nofollow" target="_blank">绽放</a> <code class="fe nf ng nh ni b"><a class="ae kv" href="https://huggingface.co/docs/transformers/model_doc/bloom" rel="noopener ugc nofollow" target="_blank">transformers</a></code> <a class="ae kv" href="https://huggingface.co/docs/transformers/model_doc/bloom" rel="noopener ugc nofollow" target="_blank">文档</a>，2022，拥抱脸</li><li id="5581" class="ls lt iq lu b lv mh lx mi lf mj lj mk ln ml mc md me mf mg bi translated"><a class="ae kv" href="https://huggingface.co/blog/how-to-generate" rel="noopener ugc nofollow" target="_blank">如何生成文本:用变形金刚使用不同的解码方法进行语言生成</a>，2020，Patrick von Platen</li><li id="34e1" class="ls lt iq lu b lv mh lx mi lf mj lj mk ln ml mc md me mf mg bi translated"><code class="fe nf ng nh ni b"><a class="ae kv" href="https://docs.python.org/3/library/venv.html#module-venv" rel="noopener ugc nofollow" target="_blank">venv</a></code> <a class="ae kv" href="https://docs.python.org/3/library/venv.html#module-venv" rel="noopener ugc nofollow" target="_blank">模块文档</a>，2022，Python.org</li><li id="2305" class="ls lt iq lu b lv mh lx mi lf mj lj mk ln ml mc md me mf mg bi translated"><a class="ae kv" href="https://blog.andrewcantino.com/blog/2021/04/21/prompt-engineering-tips-and-tricks/" rel="noopener ugc nofollow" target="_blank">提示GPT-3的工程技巧和诀窍</a>，2021，安德鲁·坎蒂诺</li><li id="a309" class="ls lt iq lu b lv mh lx mi lf mj lj mk ln ml mc md me mf mg bi translated"><a class="ae kv" href="https://github.com/dptrsa-300/start_with_bloom/blob/main/bloomex_nb.ipynb" rel="noopener ugc nofollow" target="_blank">布鲁姆入门:样本笔记本</a>，2022，丹妮·塞隆</li></ul></div></div>    
</body>
</html>