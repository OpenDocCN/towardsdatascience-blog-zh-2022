<html>
<head>
<title>Predicting Diabetes with Machine Learning — Part II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用机器学习预测糖尿病——第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-diabetes-with-machine-learning-part-ii-a6f703e8cf04#2022-05-07">https://towardsdatascience.com/predicting-diabetes-with-machine-learning-part-ii-a6f703e8cf04#2022-05-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1221" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">预测糖尿病的不同ML模型概述的最终部分</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c1d135d8487b83921ba29bb829e30ac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hcfHefuU2bfL9wK65QgD4g.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">迈肯齐·约翰逊在<a class="ae ky" href="https://unsplash.com/s/photos/diabetes?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="bf61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是不同机器学习模型概述的第二部分，我使用scikit-learn库提供的著名的“糖尿病数据集”来比较它们在预测糖尿病方面的差异。</p><p id="bb94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在这里找到第一部《T4》。因为我已经通过不同的ML模型来比较它们，所以你先读第一部分是有准备的。此外，在第一部分，你会发现完整的探索性数据分析。</p><p id="7768" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，在本文的最后，您会发现我的GitHub存储库，我在那里存储了该分析的完整代码。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="fc65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在第一部分的结论中说，简单的线性回归模型对于这个ML问题不是一个好的模型；让我们看看如果我们尝试正则化模型会发生什么</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="9870" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[EDIT 04/06/2022]我要感谢Francis Van Schie与我联系，表明我在拟合多项式方法时犯了一个错误。现在错误已经被修正了。</p><h1 id="f177" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">1.线性正则化回归模型:Lasso模型</h1><p id="ee70" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我想尝试线性回归的正则化模型，我选择Lasso回归，因为当变量之间存在高度相关性时会使用Ridge，但相关矩阵表明情况并非如此。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="e3fe" class="ne md it na b gy nf ng l nh ni">#defining the lasso model<br/>model = Lasso()</span><span id="e80b" class="ne md it na b gy nj ng l nh ni">#define model evaluation method<br/>cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)</span><span id="a178" class="ne md it na b gy nj ng l nh ni"># efine grid<br/>grid = dict()<br/>grid['alpha'] = np.arange(0, 1, 0.01)</span><span id="5081" class="ne md it na b gy nj ng l nh ni">#define search<br/>search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)</span><span id="36ae" class="ne md it na b gy nj ng l nh ni">#performing the search on the train dataset<br/>results = search.fit(X_train, y_train)</span><span id="6041" class="ne md it na b gy nj ng l nh ni">#printing<br/>print(f'MAE:{results.best_score_: .2f}')<br/>print(f'Best Alpha:{results.best_params_}')</span><span id="71ec" class="ne md it na b gy nj ng l nh ni">------------------------</span><span id="435d" class="ne md it na b gy nj ng l nh ni">&gt;&gt;&gt;</span><span id="863c" class="ne md it na b gy nj ng l nh ni">MAE:-44.86<br/>Best Alpha:{'alpha': 0.01}</span></pre><p id="c7d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最好的alpha值是0.01，它给了我一个相当高的平均绝对误差(MAE为负的事实并不重要:sklearn因为其优化原因而使其为负；在任何情况下，要成为一个好值，它应该“更接近”于0)。还需要补充的是，alpha = 0.01是一个“非常小”的值；alpha = 0是正态(非正则化)回归的情况，除了高MAE，告诉我这个模型对于解决这种类型的ML问题不是太好。</p><p id="d372" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了数字的符号之外，MAE实际上与简单回归方法相同。在上面的例子中，网格是均匀的；现在，我想尝试使用“loguniform”方法来扩展它:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="ff62" class="ne md it na b gy nf ng l nh ni"># define model<br/>model = Lasso()</span><span id="b769" class="ne md it na b gy nj ng l nh ni"># define evaluation<br/>cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)</span><span id="a4fa" class="ne md it na b gy nj ng l nh ni"># define search space<br/>space = dict()<br/>space['alpha'] = loguniform(1e-5, 100)<br/>space['fit_intercept'] = [True, False]<br/>space['normalize'] = [True, False]</span><span id="9f41" class="ne md it na b gy nj ng l nh ni">#define search<br/>search = RandomizedSearchCV(model, space, n_iter=500, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv, random_state=1)</span><span id="805c" class="ne md it na b gy nj ng l nh ni"># execute search<br/>result = search.fit(X, y)</span><span id="5415" class="ne md it na b gy nj ng l nh ni">#printing<br/>print(f'MAE:{results.best_score_: .2f}')<br/>print(f'Best Alpha:{results.best_params_}')</span><span id="027f" class="ne md it na b gy nj ng l nh ni">-----------------------</span><span id="aab9" class="ne md it na b gy nj ng l nh ni">&gt;&gt;&gt;</span><span id="1dc7" class="ne md it na b gy nj ng l nh ni">MAE:-44.86<br/>Best Alpha:{'alpha': 0.01}</span></pre><p id="8711" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我发现与之前相同的最佳alpha值，因此我将使用alpha = 0.01进行拟合，以评估Lasso类型的正则化回归模型的性能。我预计性能会很差，因为MAE实际上与简单线性回归模型中看到的值相同，但是让我们来看看:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="7684" class="ne md it na b gy nf ng l nh ni">#lasso with best alpha<br/>model_best = Lasso(alpha=0.01).fit(X_train, y_train)</span><span id="03f9" class="ne md it na b gy nj ng l nh ni">#predictions<br/>y_test_pred = model_best.predict(X_test)<br/>y_train_pred = model_best.predict(X_train)</span><span id="a2c7" class="ne md it na b gy nj ng l nh ni">#R^2<br/>print(f'Coeff. of determination on train set:{model_best.score(X_train, y_train): .2f}') #train set<br/>print(f'Coeff. of determination on test set:{model_best.score(X_test, y_test): .2f}') #test set</span><span id="af67" class="ne md it na b gy nj ng l nh ni">---------------------------</span><span id="7ac1" class="ne md it na b gy nj ng l nh ni">&gt;&gt;&gt;</span><span id="c2a6" class="ne md it na b gy nj ng l nh ni">Coeff. of determination on train set: 0.53<br/>Coeff. of determination on test set: 0.46</span></pre><p id="c5f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些值与用简单线性回归模型获得的值太相似了；这告诉我们，即使是正则化的方法也不是一个好方法。这是意料之中的，因为最好的alpha是0.01，我们必须记住alpha=0是简单的线性回归情况。</p><p id="c8bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">无论如何，让我们来看几个形象化的例子:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="0411" class="ne md it na b gy nf ng l nh ni">#figure size<br/>plt.figure(figsize=(10, 7))</span><span id="8c91" class="ne md it na b gy nj ng l nh ni">#scatterplot of y_test and y_test_pred<br/>plt.scatter(y_test, y_test_pred)<br/>plt.plot(y_test, y_test, color='r')</span><span id="ffbd" class="ne md it na b gy nj ng l nh ni">#labeling<br/>plt.title('ACTUAL VS PREDICTED VALUES (TEST SET)')<br/>plt.xlabel('ACTUAL VALUES')<br/>plt.ylabel('PREDICTED VALUES')</span><span id="0ba3" class="ne md it na b gy nj ng l nh ni">#showig plot<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/bb82619c60aa94977a9d8179042ece9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*7NrR2JNgl6enrR5rWr-7DA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线性回归的实际值与预测值(通过Lasso模型)。图片作者。</p></figure><p id="5198" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上图可以看出，光斑没有明显的分布在线周围的趋势。现在，让我们来看看KDE:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/38147b693c928a2787b4782deb259e28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*QMr_YiSDNSuKlgFJbNAZaw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实际值和预测值(通过Lasso模型)的KDE。图片作者。</p></figure><p id="8b6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从图中可以看出，预测值的概率密度根本不接近真实值的概率密度。</p><p id="cd52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我对残差进行图形分析:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="78b9" class="ne md it na b gy nf ng l nh ni">#figure size<br/>plt.figure(figsize=(10, 7))</span><span id="bc38" class="ne md it na b gy nj ng l nh ni">#residual plot<br/>sns.residplot(x=y_test, y=y_test_pred)</span><span id="c52f" class="ne md it na b gy nj ng l nh ni">#labeling<br/>plt.title('REDISUALS VS PREDICTED VALUES')<br/>plt.xlabel('PREDICTED VALUES (DIABETES PROGRESSION)')<br/>plt.ylabel('REDISUALS')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/e1e0ae4131526b5a0a5081cec170b336.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*ajnf1yczhkA5BehA7WZiQg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">残差与预测值(通过Lasso模型)。图片作者。</p></figure><p id="9f00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">残差是随机分布的(上面的图中没有明确的模式)，这告诉我们选择的模型并不完全不好，但残差的高值(甚至超过100)太多了，这意味着模型的误差很高。没有低估或高估价值的特别倾向；然而，有一点倾向于具有高误差，特别是在具有低疾病进展值的区域，而对于高进展值，误差稍微减小，除了一些异常值。</p><p id="e161" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，该图也证实了这样一个事实，即线性回归模型(尽管是规则化的)对于这个ML问题不是一个好的模型，必须寻找另一个模型。因此，我们必须尝试不同的模型:让我们尝试多项式回归方法。</p><h1 id="ec48" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">2.多项式回归方法</h1><p id="48ad" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">考虑到MSE和RSME的值以及所看到的图形，我尝试了增加多项式次数的方法；也就是我尝试多项式回归。</p><p id="e06b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑到之前获得的结果，我将直接使用3次多项式，因为2次对我来说似乎有点短。然而，我不想通过插入一个太高的程度来夸大其词，因为这里是一个通过转换可用数据直接进行拟合，然后使用线性回归中已经看到的函数的问题；在实践中:如果我使用一个过高的多项式次数，我会冒在训练集上过度拟合的风险。</p><p id="4441" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我创建了三次和分裂多项式函数:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="0aa3" class="ne md it na b gy nf ng l nh ni">#creating the 3rd degree polinomial<br/>poly = PolynomialFeatures(degree=3, include_bias=False)</span><span id="5f85" class="ne md it na b gy nj ng l nh ni">#transforming the values in all X<br/>poly_features = poly.fit_transform(X)</span><span id="495d" class="ne md it na b gy nj ng l nh ni">#splitting<br/>X_train3, X_test3, y_train3, y_test3 = train_test_split(poly_features,y, test_size=0.2,random_state=42)</span></pre><p id="8b2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">创建多项式回归:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="829d" class="ne md it na b gy nf ng l nh ni">#creating the polynomial regression<br/>poly_reg = LinearRegression()</span><span id="7715" class="ne md it na b gy nj ng l nh ni">#fitting<br/>poly_reg.fit(X_train3, y_train3)</span><span id="1e7f" class="ne md it na b gy nj ng l nh ni">#predictions<br/>y_test3_pred = poly_reg.predict(X_test3)<br/>y_train3_pred = poly_reg.predict(X_train3)</span></pre><p id="3502" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">打印指标:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="668d" class="ne md it na b gy nf ng l nh ni">#R^2</span><span id="1336" class="ne md it na b gy nj ng l nh ni">#train set<br/>print(f'Coeff. of determination on train set:{poly_reg.score(X_train3, y_train3): .2f}') </span><span id="f45b" class="ne md it na b gy nj ng l nh ni">#test set<br/>print(f'Coeff. of determination on test set:{poly_reg.score(X_test3, y_test3): .2f}')</span><span id="a5b8" class="ne md it na b gy nj ng l nh ni">---------------</span><span id="c325" class="ne md it na b gy nj ng l nh ni">&gt;&gt;&gt;</span><span id="21a4" class="ne md it na b gy nj ng l nh ni">Coeff. of determination on train set: 0.88<br/>Coeff. of determination on test set: -17.42</span></pre><p id="0768" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型在训练集上的决定系数比线性回归模型好得多。但是，测试集上的决定系数在值上下降(相对于测试集上的决定系数)，这使我想到这里出现的一个明显的情况:过拟合(在测试集上)！让我们看看其他指标:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="9328" class="ne md it na b gy nf ng l nh ni">#model metrics</span><span id="5e84" class="ne md it na b gy nj ng l nh ni">print(f'The mean absolute error is:{metrics.mean_absolute_error(y_test3, y_test3_pred): .2f}')<br/>print(f'The root mean squared error is:{np.sqrt(metrics.mean_squared_error(y_test3, y_test3_pred)): .2f}')</span><span id="37b0" class="ne md it na b gy nj ng l nh ni">------------</span><span id="011f" class="ne md it na b gy nj ng l nh ni">&gt;&gt;&gt;</span><span id="e2f9" class="ne md it na b gy nj ng l nh ni">The mean absolute error is: 169.65<br/>The root mean squared error is: 312.43</span></pre><p id="01f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MAE和MSE比以前的线性模型计算出的要高得多！！我制作了一张实际值与预测值比较的散点图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/da69222eee31766b77061e57f0dc61cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*7wD1kJuEAW2gju2xiKdlZA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实际值与预测值(通过多项式模型)的对比。图片作者。</p></figure><p id="dd20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">记住，我们已经用三次多项式函数变换了起始域，正如我们现在可以看到的，数据在一条线(变换域中的一条线)周围非常明显地变粗了，但是有很高的异常值，这使我认为这个模型并不像我认为的那样适合这个特定的ML问题。最后，我也想和KDE一起做一个视觉化的展示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/c44a8e7fcddac80a0ef7f57678b28c27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*nzR8YWDAdkuzAlKtStrtJw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实际值与预测值(通过多项式模型)的KDE图。图片作者。</p></figure><p id="1b49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">KDE证实了多项式回归确实不是这里使用的好模型。此外，由于过度拟合，这是目前看到的最差的模型。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="5d98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我向一位资深数据科学家展示这项工作时，他告诉我:“很好；但是你知道一个ML模型，它‘转换’你的数据，在它们之间留下线性关系吗？”</p><p id="c734" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我的回答是“是的！”模型是支持向量回归机。我试过SVR，但是效果很差；既然您已经理解了这个方法，我就不在这里赘述了:如果您想看一看(和/或自己尝试一下)，您可以在我的GitHub repo中找到结果。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="77a4" class="mc md it bd me mf nn mh mi mj no ml mm jz np ka mo kc nq kd mq kf nr kg ms mt bi translated">结论</h1><p id="93f1" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在这一系列文章中，我们已经看到了不同的模型如何在给定的数据集上执行，以及我们可以从所使用的度量中得到什么；尝试不同的ML模型是我们必须做的，以找到一个可以给我们最好的预测；在这种情况下，我们发现线性回归模型在我们尝试的所有模型中是最好的，但确实不是绝对好；所以必须尝试其他模型来解决这个ML问题。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="22a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！</p><p id="3872" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在这里找到我的GitHub repo的完整代码<a class="ae ky" href="https://github.com/federico-trotta/predicting_diabetes_with_ML" rel="noopener ugc nofollow" target="_blank"/>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="c453" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ns">让我们连在一起！</em></p><p id="7194" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://federicotrotta.medium.com/" rel="noopener"> <strong class="lb iu"> <em class="ns">中型</em> </strong> </a></p><p id="afe3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://www.linkedin.com/in/federico-trotta/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="ns">LINKEDIN</em></strong></a><em class="ns">(向我发送连接请求)</em></p><p id="4d1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ns">如果你愿意，你可以</em> <a class="ae ky" href="https://federicotrotta.medium.com/subscribe" rel="noopener"> <strong class="lb iu"> <em class="ns">订阅我的邮件列表</em></strong></a><strong class="lb iu"><em class="ns"/></strong><em class="ns">这样你就可以一直保持更新了！</em></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="aa7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑成为会员:你可以免费支持我和其他像我一样的作家。点击 <a class="ae ky" href="https://federicotrotta.medium.com/membership" rel="noopener"> <strong class="lb iu"> <em class="ns">这里</em></strong></a><strong class="lb iu"><em class="ns"/></strong><em class="ns">成为会员。</em></p></div></div>    
</body>
</html>