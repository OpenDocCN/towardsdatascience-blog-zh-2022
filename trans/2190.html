<html>
<head>
<title>How to Perform Feature Selection in a Data Science Project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在数据科学项目中执行功能选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-perform-feature-selection-in-a-data-science-project-591ba96f86eb#2022-05-16">https://towardsdatascience.com/how-to-perform-feature-selection-in-a-data-science-project-591ba96f86eb#2022-05-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2c01" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">特征选择的四种方法和整个过程，并附有Python示例</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/95f97096c9a79553722f39131b2ab64b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UvlOyDsdIbPvTACqcLTffg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">弗拉季斯拉夫·巴比延科在<a class="ae ky" href="https://unsplash.com/s/photos/selection?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="f94f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated">特征选择是数据科学项目的重要组成部分。当您处理(非常)大的数据集时，您应该始终问自己两个问题:</p><ol class=""><li id="f438" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">这些特征代表什么？</li><li id="efea" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">这些特性都重要吗？</li></ol><p id="f31a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二个问题的答案将我们引向特性选择；事实上，您不希望在数据框中包含无意义的要素:这是对计算的浪费。</p><p id="8311" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们将看到四种功能选择的方法，我还将描述一个功能选择的过程(一般来说，你很难只应用下面的方法之一来做功能选择的工作)。</p><h1 id="5de7" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">1.相关矩阵</h1><p id="7382" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">相关矩阵帮助我们找到特征和标签之间的线性关系，甚至特征本身之间的线性关系。当一些特征高度相关时，我们可以决定丢弃它们，因为当两个特征高度相关时，它们对结果有相同的影响。</p><p id="c0d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，让我们看看我们能做些什么，用相关矩阵。下面，一个取自我的一个项目的例子；假设我们有一个数据框架“df”(数据集细节无关紧要)；让我们创建它的相关矩阵:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="06d8" class="nu mt it nq b gy nv nw l nx ny">import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="31ea" class="nu mt it nq b gy nz nw l nx ny">#figure size<br/>plt.figure(figsize=(20, 10))</span><span id="e104" class="nu mt it nq b gy nz nw l nx ny">#heat map for correlation coefficient<br/>sns.heatmap(df.corr(), annot=True, fmt="0.1")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/f81beab4e2835c643ec7cfda7de15227.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xMVNAaKy5G32LnfIW5r9Jw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">相关矩阵。图片作者。</p></figure><p id="469a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有许多相关的特征；例如:</p><ul class=""><li id="8c77" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu ob mk ml mm bi translated">基线值和直方图模式</li><li id="c607" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu ob mk ml mm bi translated">基线值和直方图中值</li><li id="3b49" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu ob mk ml mm bi translated">直方图模式、istogram均值和直方图中值</li><li id="7ead" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu ob mk ml mm bi translated">直方图_宽度和直方图_最小值</li></ul><p id="0e36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们只关注相关性，而不是这些特征(和标签)代表什么；我想看看这些变量之间可能的相关性的图示:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="a62d" class="nu mt it nq b gy nv nw l nx ny">#creating a subtdataframe<br/>a = df[['baseline value', 'histogram_mode', 'histogram_median', 'histogram_mean', 'histogram_width', 'histogram_min']]</span><span id="58c8" class="nu mt it nq b gy nz nw l nx ny">#creating a unique plot with the regressions<br/>g = sns.PairGrid(a)<br/>g = g.map_upper(sns.regplot, scatter_kws={'alpha':0.15}, line_kws={'color': 'red'})</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/626fd1afeff9f589c88e60a292f532ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_h1EJ_HYoCB1_BPH4GEy2A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有上述特征回归线的散点图。作者图片，</p></figure><p id="b995" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，从图表中可以看出，上述特征确实是高度相关的；所以，我可以选择删除一部分。例如，通过交叉各种图形，我选择消除以下特征<br/>(消除更多特征会导致一些数据丢失，因为它们都是相互交叉的):</p><ul class=""><li id="ac03" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu ob mk ml mm bi translated">直方图_最小值</li><li id="3dc7" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu ob mk ml mm bi translated">直方图_平均值</li></ul><h1 id="a0b3" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">2.套索回归</h1><p id="bfde" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">如果您正在处理线性回归问题，执行<strong class="lb iu">特征选择的另一种方法是应用套索正则化模型</strong>。</p><p id="2528" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种类型的正则化中，与线性模型相关的一些系数可以变为零，并且可以从模型中消除。这意味着Lasso回归模型也执行要素选择。如果你想知道更多的细节，我已经在这里写了一篇专门的文章。</p><h1 id="2260" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">3.互信息方法</h1><p id="9c38" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">前面看到的方法是在整个数据集上执行的。这个和下面的方法必须在分割数据集上执行(你可以在这里看到关于主题<a class="ae ky" href="https://stackoverflow.com/questions/56308116/should-feature-selection-be-done-before-train-test-split-or-after" rel="noopener ugc nofollow" target="_blank">的有趣讨论)。最后一条评论是最重要的一条)。</a></p><p id="4a62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们考虑一下本教程第1点中看到的数据集，并考虑我们刚刚分割了数据集。现在，我们有:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="2224" class="nu mt it nq b gy nv nw l nx ny">#mutual information selecting all features<br/>mutual = SelectKBest(score_func=mutual_info_classif, k='all')</span><span id="6dee" class="nu mt it nq b gy nz nw l nx ny">#learn relationship from training data<br/>mutual.fit(X_train, y_train)</span><span id="0586" class="nu mt it nq b gy nz nw l nx ny"># transform train input data<br/>X_train_mut = mutual.transform(X_train)</span><span id="2a4f" class="nu mt it nq b gy nz nw l nx ny"># transform test input data<br/>X_test_mut = mutual.transform(X_test)</span><span id="8705" class="nu mt it nq b gy nz nw l nx ny">#printing scores of the features<br/>for i in range(len(mutual.scores_)):<br/>    print('Feature %d: %f' % (i, mutual.scores_[i]))</span><span id="caeb" class="nu mt it nq b gy nz nw l nx ny">-------------------</span><span id="d953" class="nu mt it nq b gy nz nw l nx ny">&gt;&gt;&gt;</span><span id="22fe" class="nu mt it nq b gy nz nw l nx ny">Feature 0: 0.124999<br/>Feature 1: 0.139990<br/>Feature 2: 0.031640<br/>Feature 3: 0.092322<br/>Feature 4: 0.066883<br/>Feature 5: 0.002289<br/>Feature 6: 0.008455<br/>Feature 7: 0.194067<br/>Feature 8: 0.222438<br/>Feature 9: 0.144378<br/>Feature 10: 0.034891<br/>Feature 11: 0.118958<br/>Feature 12: 0.025970<br/>Feature 13: 0.033416<br/>Feature 14: 0.015075<br/>Feature 15: 0.108909<br/>Feature 16: 0.085122<br/>Feature 17: 0.103669<br/>Feature 18: 0.000000</span></pre><p id="b453" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，这种方法利用评分方法，在某种程度上赋予特征以重要性。让我们用图表来看一下:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="7381" class="nu mt it nq b gy nv nw l nx ny">#plot the scores<br/>plt.bar([i for i in range(len(mutual.scores_))], mutual.scores_)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/f09bd0b198a3fa99b01f74fd877a1322.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*ZuizwY-fI1GYmNPNBJHx8Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">互信息法评分。图片作者。</p></figure><p id="5d1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在结论中，我们将讨论何时使用这种方法。</p><h1 id="7834" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">4.方差分析f检验</h1><p id="1af8" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">让我们考虑和以前一样的数据集；我们刚刚拆分了数据，因此可以直接应用该方法:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="96b0" class="nu mt it nq b gy nv nw l nx ny"># configure to select all features<br/>an = SelectKBest(score_func=f_classif, k='all')</span><span id="a03c" class="nu mt it nq b gy nz nw l nx ny"># learn relationship from training data<br/>an.fit(X_train, y_train)</span><span id="134d" class="nu mt it nq b gy nz nw l nx ny"># transform train input data<br/>X_train_an = an.transform(X_train)</span><span id="e84a" class="nu mt it nq b gy nz nw l nx ny"># transform test input data<br/>X_test_an = an.transform(X_test)</span><span id="21af" class="nu mt it nq b gy nz nw l nx ny">#printing scores of the features<br/>for i in range(len(an.scores_)):<br/>    print('Feature %d: %f' % (i, mutual.scores_[i]))</span><span id="06fa" class="nu mt it nq b gy nz nw l nx ny">-------------------</span><span id="8324" class="nu mt it nq b gy nz nw l nx ny">&gt;&gt;&gt;</span><span id="1cf2" class="nu mt it nq b gy nz nw l nx ny">Feature 0: 0.117919<br/>Feature 1: 0.176444<br/>Feature 2: 0.006887<br/>Feature 3: 0.089149<br/>Feature 4: 0.064985<br/>Feature 5: 0.054356<br/>Feature 6: 0.090783<br/>Feature 7: 0.144446<br/>Feature 8: 0.191335<br/>Feature 9: 0.200292<br/>Feature 10: 0.081927<br/>Feature 11: 0.096509<br/>Feature 12: 0.000000<br/>Feature 13: 0.042977<br/>Feature 14: 0.105467<br/>Feature 15: 0.027062<br/>Feature 16: 0.072015<br/>Feature 17: 0.198037<br/>Feature 18: 0.018785</span></pre><p id="e964" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">甚至在这里用图形显示:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="ad71" class="nu mt it nq b gy nv nw l nx ny"># plot the scores<br/>plt.bar([i for i in range(len(an.scores_))], an.scores_)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/bdbaf7439f6c0979aa9a1891528b321e.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*L6zPtRp8-YlLO_zaWWbvVg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">方差分析f检验法得分。图片作者。</p></figure><p id="344a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如你所见，这种方法和互信息方法在特征重要性方面的结果确实不同；我们将在下一段中看到何时选择一个，何时选择另一个。</p><h1 id="ab83" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">特征选择和结论的过程</h1><p id="746b" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">要选择特征，您可以遵循以下过程:</p><ol class=""><li id="64e9" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">绘制<strong class="lb iu">相关矩阵，对整个数据集</strong>进行计算，并决定是否有一些可以消除的特征(我们说的是高度相关的特征。用回归线画一个散点图来确定线性路径，就像我上面做的那样)。</li><li id="866a" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">仅当问题是回归时，才选择套索回归变量。要使用的模型是线性的，并且存在“大量”要素。</li><li id="df17" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated"><strong class="lb iu">将数据集分成训练集和测试集，然后在互信息和Anova f-test之间选择一种方法。</strong> <br/> Anova f-test能够“感觉到”特征之间的线性相关性，而互信息“感觉到”任何类型的相关性，特别是“感觉到”非线性相关性(也可在此处阅读文档<a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/feature_selection/plot_f_test_vs_mi.html" rel="noopener ugc nofollow" target="_blank"/>)。在上面看到的情况下，考虑到使用相关矩阵获得的结果以及随后使用线性回归图消除两个特征，那么，一致地，互信息比Anova f-test指示更好，这是对于这种类型的问题真正重要的特征。</li></ol></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><p id="a86f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="om">我们一起连线吧！</em></p><p id="e151" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://federicotrotta.medium.com/" rel="noopener"> <em class="om">中等</em> </a></p><p id="fe32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://www.linkedin.com/in/federico-trotta/" rel="noopener ugc nofollow" target="_blank"> <em class="om"> LINKEDIN </em> </a> <em class="om">(向我发送连接请求)</em></p><p id="2b03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="om">如果你愿意，你可以</em> <a class="ae ky" href="https://federicotrotta.medium.com/subscribe" rel="noopener"> <em class="om">订阅我的邮件列表</em> </a> <em class="om">这样你就可以一直保持更新了！</em></p></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><p id="fec3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑成为会员:你可以免费支持我和其他像我一样的作家。点击 <a class="ae ky" href="https://federicotrotta.medium.com/membership" rel="noopener"> <em class="om">这里的</em> </a> <em class="om">成为会员。</em></p></div></div>    
</body>
</html>