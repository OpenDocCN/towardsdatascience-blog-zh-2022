<html>
<head>
<title>Text Summarization with NLP: TextRank vs Seq2Seq vs BART</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 NLP 的文本摘要:TextRank vs Seq2Seq vs BART</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09#2022-03-15">https://towardsdatascience.com/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09#2022-03-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/bf7a59d41b19f964ce71b34753c515bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LgqxDMP5qD1HE_uM33zZrg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><div class=""/><div class=""><h2 id="ca42" class="pw-subtitle-paragraph kf jh ji bd b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw dk translated">使用 Python、Gensim、Tensorflow、Transformers 进行自然语言处理</h2></div><h2 id="072c" class="kx ky ji bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">摘要</h2><p id="fdb5" class="pw-post-body-paragraph lt lu ji lv b lw lx kj ly lz ma km mb lg mc md me lk mf mg mh lo mi mj mk ml im bi translated">在这篇文章中，我将使用 NLP 和 Python 解释 3 种不同的文本摘要策略:老式的<em class="mm"> TextRank </em>(带有<em class="mm"> gensim </em>)、<em class="mm">T5】著名的<em class="mm"> Seq2Seq ( </em>带有<em class="mm"> tensorflow </em>)和尖端的<em class="mm"> BART </em>(带有<em class="mm"> transformers </em>)。</em></p><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mn"><img src="../Images/70c1cbc132ac76ac11ddec89d9c66977.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*N19i0l_xNBpdYJ8Q6DNTUQ.gif"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="9906" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated"><a class="ae mx" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"><strong class="lv jj">【NLP(自然语言处理)</strong> </a>是人工智能领域，研究计算机与人类语言之间的交互，特别是如何给计算机编程以处理和分析大量自然语言数据。最困难的 NLP 任务是输出不是单个标签或值(如分类和回归)，而是一个全新的文本(如翻译、摘要和对话)。</p><p id="8922" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated"><strong class="lv jj">文本摘要</strong>是在不改变文档含义的情况下，减少文档的句子和单词数量的问题。从原始文本数据中提取信息并将其用于摘要模型有不同的技术，总体来说，它们可以分为<strong class="lv jj">提取型</strong>和<strong class="lv jj">抽象型。</strong>提取方法选择文本中最重要的句子(不一定理解意思)，因此结果摘要只是全文的子集。相反，抽象模型使用高级 NLP(即单词嵌入)来理解文本的语义，并生成有意义的摘要。因此，抽象技术很难从头开始训练，因为它们需要大量的参数和数据。</p><p id="fa5b" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">本教程比较了老派的方法<em class="mm"> TextRank </em>(提取)、流行的编码器-解码器神经网络<em class="mm"> Seq2Seq </em>(抽象)和最先进的<em class="mm"> </em>基于注意力的<em class="mm">变形金刚</em>(抽象)，它们已经彻底改变了 NLP 领域。</p><p id="acc9" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">我将展示一些有用的 Python 代码，这些代码可以很容易地应用于其他类似的情况(只需复制、粘贴、运行)，并通过注释遍历每一行代码，以便您可以复制这个示例(下面是完整代码的链接)。</p><div class="is it gp gr iu my"><a href="https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/natural_language_processing/example_text_summarization.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd jj gy z fp nd fr fs ne fu fw jh bi translated">data science _ artificial intelligence _ Utils/example _ text _ summary . ipynb at master…</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">数据科学项目和人工智能用例的示例…</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">github.com</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm ja my"/></div></div></a></div><p id="9577" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">我将使用“<strong class="lv jj">CNN Daily Mail</strong>”<strong class="lv jj"/>数据集，其中为您提供了 CNN 和《每日邮报》记者用英语撰写的数千篇新闻文章，以及每篇文章的摘要(以下链接)。</p><div class="is it gp gr iu my"><a href="https://huggingface.co/datasets/cnn_dailymail" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd jj gy z fp nd fr fs ne fu fw jh bi translated">cnn_dailymail 拥抱脸的数据集</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">我们正在通过开源和开放科学来推进和民主化人工智能的旅程。</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">huggingface.co</p></div></div><div class="nh l"><div class="nn l nj nk nl nh nm ja my"/></div></div></a></div><p id="a2f9" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">特别是，我将经历:</p><ul class=""><li id="06f9" class="no np ji lv b lw ms lz mt lg nq lk nr lo ns ml nt nu nv nw bi translated">设置:导入包，读取数据，预处理。</li><li id="eec1" class="no np ji lv b lw nx lz ny lg nz lk oa lo ob ml nt nu nv nw bi translated">用<em class="mm"> gensim </em>拟合<em class="mm"> TextRank </em>以构建基线，并使用 ROUGE 指标和数据可视化评估结果。</li><li id="f090" class="no np ji lv b lw nx lz ny lg nz lk oa lo ob ml nt nu nv nw bi translated">用<em class="mm"> tensorflow/keras </em>拟合<em class="mm"> Seq2Seq </em>来训练深度学习模型。</li><li id="4694" class="no np ji lv b lw nx lz ny lg nz lk oa lo ob ml nt nu nv nw bi translated">通过 HuggingFace 使用预训练的<em class="mm"> BART </em>和<em class="mm">变形金刚</em>库。</li></ul></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h2 id="208e" class="kx ky ji bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">设置</h2><p id="c007" class="pw-post-body-paragraph lt lu ji lv b lw lx kj ly lz ma km mb lg mc md me lk mf mg mh lo mi mj mk ml im bi translated">首先，我需要导入以下库:</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="4e36" class="kx ky ji ok b gy oo op l oq or"><strong class="ok jj">## for data<br/></strong>import <strong class="ok jj">datasets  </strong>#(1.13.3)<strong class="ok jj"><br/></strong>import<strong class="ok jj"> pandas </strong>as<strong class="ok jj"> </strong>pd  #(0.25.1)<br/>import <strong class="ok jj">numpy  </strong>#(1.16.4)</span><span id="955f" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">## for plotting</strong><br/>import <strong class="ok jj">matplotlib</strong>.pyplot as plt  #(3.1.2)<br/>import <strong class="ok jj">seaborn </strong>as sns  #(0.9.0)</span><span id="105d" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">## for preprocessing</strong><br/>import <strong class="ok jj">re</strong><br/>import <strong class="ok jj">nltk  </strong>#(3.4.5)<br/>import <strong class="ok jj">contractions  </strong>#(0.0.18)</span><span id="84b7" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">## for textrank</strong><br/>import <strong class="ok jj">gensim  </strong>#(3.8.1)</span><span id="89df" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">## for evaluation<br/></strong>import <strong class="ok jj">rouge </strong> #(1.0.0)<br/>import <strong class="ok jj">difflib</strong></span><span id="eaa2" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">## for seq2seq</strong><br/>from <strong class="ok jj">tensorflow</strong>.keras import callbacks, models, layers, preprocessing as kprocessing #(2.6.0)</span><span id="4467" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">## for bart</strong><br/>import <strong class="ok jj">transformers  </strong>#(3.0.1)</span></pre><p id="6a9d" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">然后我通过 HuggingFace 使用<a class="ae mx" href="https://huggingface.co/docs/datasets/" rel="noopener ugc nofollow" target="_blank">专用库加载数据集:</a></p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="d871" class="kx ky ji ok b gy oo op l oq or"><strong class="ok jj">## load the full dataset of 300k articles<br/></strong>dataset = <strong class="ok jj">datasets</strong>.load_dataset("cnn_dailymail", '3.0.0')<br/>lst_dics = [dic for dic in dataset["train"]]</span><span id="47d1" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">## keep the first N articles if you want to keep it lite </strong><br/>dtf = <strong class="ok jj">pd</strong>.DataFrame(lst_dics).rename(columns={"article":"text", <br/>      "highlights":"y"})[["text","y"]].head(20000)<br/>dtf.head()</span></pre><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ot"><img src="../Images/b7b27a46c4db9d44454685280277aa18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZeAjpPFgan7nWWhEmLWBJw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="ad4f" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">让我们看一个随机的例子:</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="1669" class="kx ky ji ok b gy oo op l oq or">i = 1<br/>print("--- Full text ---")<br/>print(dtf["text"][i])<br/>print("--- Summary ---")<br/>print(dtf["y"][i])</span></pre><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ou"><img src="../Images/3fc88aa4c89f4e756d2510cbcd254239.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ievbb9PBizCCRKcCS-m86w.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来自 CNN 每日邮件数据集的文本</p></figure><p id="4796" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">在这里，我用红色手动标记了摘要中提到的信息。体育文章对机器来说很难，因为没有太多的空间来解释什么是重要的，什么是不重要的…标题必须报道主要结果。我将把这个例子放在测试集中来比较模型。</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="ad13" class="kx ky ji ok b gy oo op l oq or">dtf_train = dtf.iloc[i+1:]<br/>dtf_test = dtf.iloc[:i+1]</span></pre><h2 id="2b73" class="kx ky ji bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">文本排名</h2><p id="eb6e" class="pw-post-body-paragraph lt lu ji lv b lw lx kj ly lz ma km mb lg mc md me lk mf mg mh lo mi mj mk ml im bi translated"><a class="ae mx" href="https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf" rel="noopener ugc nofollow" target="_blank"><em class="mm">text rank</em></a>(2004)是一个基于图的文本处理排名模型，基于<a class="ae mx" href="https://en.wikipedia.org/wiki/PageRank" rel="noopener ugc nofollow" target="_blank"> <strong class="lv jj"> Google 的<em class="mm"> PageRank </em> </strong> </a>算法，在一个文本中寻找最相关的句子。<em class="mm"> PageRank </em>是 1998 年 Google 搜索引擎使用的第一个对网页进行排序的算法。简而言之，如果页面 A 链接到页面 B，页面 C，页面 B 链接到页面 C，排序将是页面 C，页面 B，页面 A。</p><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ov"><img src="../Images/c52fe6b7f4a01b79dcb47bfd4723c06e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RmghRL_9jRKl1Yf9N5ZBYw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="3512" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">TextRank 非常容易使用，因为它是无人监管的。首先，整个文本被分割成句子，然后该算法建立一个图，其中句子是节点，重叠的单词是链接。最后，<em class="mm"> PageRank </em>确定这个句子网络中最重要的节点。</p><p id="14c0" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">使用<a class="ae mx" href="https://radimrehurek.com/gensim/" rel="noopener ugc nofollow" target="_blank"> <em class="mm"> gensim </em> </a>库，您可以轻松地将<em class="mm"> TextRank </em>算法应用于您的数据:</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="cfb7" class="kx ky ji ok b gy oo op l oq or"><strong class="ok jj">'''<br/>Summarizes corpus with TextRank.<br/>:parameter    <br/>    :param corpus: str or list - dtf["text"]    <br/>    :param ratio: length of the summary (ex. 20% of the text)<br/>:return    <br/>    list of summaries<br/>'''</strong><br/>def <strong class="ok jj">textrank</strong>(corpus, ratio=0.2):    <br/>    if type(corpus) is str:        <br/>       corpus = [corpus]    <br/>    lst_summaries = [<strong class="ok jj">gensim</strong>.summarization.summarize(txt,  <br/>                     ratio=ratio) for txt in corpus]    <br/>return lst_summaries</span><span id="f931" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj"><br/>## Apply the function to corpus</strong><br/>predicted = <strong class="ok jj">textrank</strong>(corpus=dtf_test["text"], ratio=0.2)<br/>predicted[i]</span></pre><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ow"><img src="../Images/b3c4ac51446b69b076dbfc9d76560e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f7DIFKbWr1XGLS5Jg7poUQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="2431" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">如何才能<strong class="lv jj">评价</strong>这个结果呢？通常，我用两种方式来做:</p><ol class=""><li id="cd2b" class="no np ji lv b lw ms lz mt lg nq lk nr lo ns ml ox nu nv nw bi translated"><a class="ae mx" href="https://en.wikipedia.org/wiki/ROUGE_(metric)#:~:text=ROUGE%2C%20or%20Recall%2DOriented%20Understudy,software%20in%20natural%20language%20processing." rel="noopener ugc nofollow" target="_blank"><strong class="lv jj">ROUGE metrics</strong></a><strong class="lv jj"/>(面向回忆的 Gisting 评估替角):<br/>通过重叠<em class="mm"> n-grams </em>将自动生成的摘要与参考摘要进行比较的一组度量。</li></ol><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="94c1" class="kx ky ji ok b gy oo op l oq or"><strong class="ok jj">'''<br/>Calculate ROUGE score.<br/>:parameter    <br/>    :param y_test: string or list    <br/>    :param predicted: string or list<br/>'''</strong><br/>def <strong class="ok jj">evaluate_summary</strong>(y_test, predicted):    <br/>   rouge_score = <strong class="ok jj">rouge</strong>.Rouge()    <br/>   scores = rouge_score.get_scores(y_test, predicted, avg=True)       <br/>   score_1 = round(scores['rouge-1']['f'], 2)    <br/>   score_2 = round(scores['rouge-2']['f'], 2)    <br/>   score_L = round(scores['rouge-l']['f'], 2)    <br/>   print("rouge1:", score_1, "| rouge2:", score_2, "| rougeL:",<br/>         score_2, "--&gt; avg rouge:", round(np.mean(<br/>         [score_1,score_2,score_L]), 2))</span><span id="3849" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">## Apply the function to predicted<br/></strong>i = 5<br/><strong class="ok jj">evaluate_summary</strong>(dtf_test["y"][i], predicted[i])</span></pre><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oy"><img src="../Images/d72e781f769c0e603039f01b9834cc2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cBNfyuMus8ECO4UIkGhaiQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="7b91" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">结果显示，31%的<em class="mm"> unigrams </em> (ROUGE-1)和 7%的<em class="mm"> bigrams </em> (ROUGE-2)出现在两个摘要中，而<em class="mm">最长公共子序列</em> (ROUGE-L)匹配了 7%。总的来说，平均分 20%。请注意，胭脂分数并不衡量总结的流畅程度，因为我通常使用善良的老人类的眼睛。</p><p id="8ba9" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">2.<strong class="lv jj">可视化</strong>:显示两个文本，即摘要和原文，或预测摘要和真实摘要，并突出显示匹配部分。</p><figure class="mo mp mq mr gt iv"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="8ca2" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">我想你会发现这个功能非常有用，因为它可以在笔记本上突出显示两个文本的匹配子字符串。它可以用于单词级:</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="60d0" class="kx ky ji ok b gy oo op l oq or">match = <strong class="ok jj">display_string_matching</strong>(dtf_test["y"][i], predicted[i], both=True, <strong class="ok jj">sentences=False</strong>, titles=["Real Summary", "Predicted Summary"])</span><span id="4d3e" class="kx ky ji ok b gy os op l oq or">from <strong class="ok jj">IPython.core.display</strong> import display, HTML<br/>display(HTML(match))</span></pre><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/bfe422cad015dca790d2d79408197c2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BiIUWr7PNUjcGmYU_PK00Q.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="57e3" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">或者您可以设置<em class="mm"> sentences=True </em>，它将在句子级别而不是单词级别匹配文本:</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="1f46" class="kx ky ji ok b gy oo op l oq or">match = <strong class="ok jj">display_string_matching</strong>(dtf_test["text"][i], predicted[i], both=True, <strong class="ok jj">sentences=True</strong>, titles=["Full Text", "Predicted Summary"])<br/><br/>from <strong class="ok jj">IPython.core.display</strong> import display, HTML<br/>display(HTML(match))</span></pre><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pc"><img src="../Images/1d119d45312c478f30e6b1e27d6d05fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ghJfLs6h5HEUF27UzuSTg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="5473" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">该预测具有原始摘要中提到的大部分信息。正如提取算法所预期的，预测的摘要完全包含在文本中:该模型认为这 3 个句子是最重要的。我们可以将此作为下面抽象方法的基线。</p><h2 id="9aaa" class="kx ky ji bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">Seq2Seq</h2><p id="8a6e" class="pw-post-body-paragraph lt lu ji lv b lw lx kj ly lz ma km mb lg mc md me lk mf mg mh lo mi mj mk ml im bi translated"><a class="ae mx" href="https://en.wikipedia.org/wiki/Seq2seq" rel="noopener ugc nofollow" target="_blank">序列对序列模型</a> (2014)是以特定领域(即文本词汇)的序列为输入，输出另一领域(即摘要词汇)的新序列的神经网络。<em class="mm"> Seq2Seq </em>车型通常具有以下关键特征:</p><ul class=""><li id="6544" class="no np ji lv b lw ms lz mt lg nq lk nr lo ns ml nt nu nv nw bi translated"><strong class="lv jj">序列作为语料库</strong>:将文本填充成长度相同的序列，得到特征矩阵。</li><li id="61d2" class="no np ji lv b lw nx lz ny lg nz lk oa lo ob ml nt nu nv nw bi translated"><strong class="lv jj">单词嵌入</strong> <strong class="lv jj">机制</strong>:特征学习技术，将词汇表中的单词映射到实数向量，这些向量是根据每个单词出现在另一个单词之前或之后的概率分布计算的。</li><li id="1f2e" class="no np ji lv b lw nx lz ny lg nz lk oa lo ob ml nt nu nv nw bi translated"><strong class="lv jj">编码器-解码器结构:</strong>编码器处理输入序列，并返回其自身的内部状态，作为解码器的上下文，解码器在给定前一个字的情况下，预测目标序列的下一个字。</li><li id="d900" class="no np ji lv b lw nx lz ny lg nz lk oa lo ob ml nt nu nv nw bi translated"><strong class="lv jj">用于训练的模型和用于预测的模型</strong>:用于训练的模型不直接用于预测。事实上，我们将编码两个神经网络(都具有编码器-解码器结构)，一个用于训练，另一个(称为“推理模型”)通过利用来自训练模型的一些层来生成预测。</li></ul><p id="3a27" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">让我们从一些<strong class="lv jj">数据分析</strong>开始，这是下一个特征工程所需要的。<strong class="lv jj"> </strong>由于<strong class="lv jj"> </strong>我们要将文本转换成单词序列，我们必须在这里做出两个决定:</p><ol class=""><li id="5f9a" class="no np ji lv b lw ms lz mt lg nq lk nr lo ns ml ox nu nv nw bi translated">正确的序列大小，因为我们的语料库有不同的长度</li><li id="af71" class="no np ji lv b lw nx lz ny lg nz lk oa lo ob ml ox nu nv nw bi translated">我们的模型必须记住多少单词，因为罕见的单词应该被排除在外</li></ol><p id="757d" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">我将清理和分析数据来解决这两点。</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="7c1d" class="kx ky ji ok b gy oo op l oq or"><strong class="ok jj">## create stopwords</strong><br/>lst_stopwords = <strong class="ok jj">nltk</strong>.corpus.stopwords.words("english")<br/><strong class="ok jj">## add words that are too frequent</strong><br/>lst_stopwords = lst_stopwords + ["cnn","say","said","new"]</span><span id="ae0f" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj"><br/>## cleaning function</strong><br/>def <strong class="ok jj">utils_preprocess_text</strong>(txt, punkt=True, lower=True, slang=True, lst_stopwords=None, stemm=False, lemm=True):<br/>    <strong class="ok jj">### separate sentences with '. '</strong><br/>    txt = re.sub(r'\.(?=[^ \W\d])', '. ', str(txt))<br/>    <strong class="ok jj">### remove punctuations and characters</strong><br/>    txt = re.sub(r'[^\w\s]', '', txt) if punkt is True else txt<br/>    <strong class="ok jj">### strip</strong><br/>    txt = " ".join([word.strip() for word in txt.split()])<br/>    <strong class="ok jj">### lowercase</strong><br/>    txt = txt.lower() if lower is True else txt<br/>   <strong class="ok jj"> ### slang</strong><br/>    txt = contractions.fix(txt) if slang is True else txt   <br/>    <strong class="ok jj">### tokenize (convert from string to list)</strong><br/>    lst_txt = txt.split()<br/>    <strong class="ok jj">### stemming (remove -ing, -ly, ...)</strong><br/>    if stemm is True:<br/>        ps = nltk.stem.porter.PorterStemmer()<br/>        lst_txt = [ps.stem(word) for word in lst_txt]<br/>    <strong class="ok jj">### lemmatization (convert the word into root word)</strong><br/>    if lemm is True:<br/>        lem = nltk.stem.wordnet.WordNetLemmatizer()<br/>        lst_txt = [lem.lemmatize(word) for word in lst_txt]<br/>    <strong class="ok jj">### remove Stopwords</strong><br/>    if lst_stopwords is not None:<br/>        lst_txt = [word for word in lst_txt if word not in <br/>                   lst_stopwords]<br/>    <strong class="ok jj">### back to string</strong><br/>    txt = " ".join(lst_txt)<br/>    return txt</span><span id="ab7a" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj"><br/>## apply function to both text and summaries<br/></strong>dtf_train["<strong class="ok jj">text_clean</strong>"] = dtf_train["text"].apply(lambda x: <strong class="ok jj">utils_preprocess_text</strong>(x, punkt=True, lower=True, slang=True, lst_stopwords=lst_stopwords, stemm=False, lemm=True))</span><span id="ebf7" class="kx ky ji ok b gy os op l oq or">dtf_train["<strong class="ok jj">y_clean</strong>"] = dtf_train["y"].apply(lambda x: <strong class="ok jj">utils_preprocess_text</strong>(x, punkt=True, lower=True, slang=True, lst_stopwords=lst_stopwords, stemm=False, lemm=True))</span></pre><p id="5de9" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">现在我们可以通过统计单词来看看长度分布:</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="f7c0" class="kx ky ji ok b gy oo op l oq or"><strong class="ok jj">## count</strong><br/>dtf_train['<strong class="ok jj">word_count</strong>'] = dtf_train[column].apply(lambda x: len(<strong class="ok jj">nltk</strong>.word_tokenize(str(x))) )</span><span id="a2be" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">## plot</strong><br/><strong class="ok jj">sns</strong>.distplot(dtf_train["<strong class="ok jj">word_count</strong>"], hist=True, kde=True, kde_kws={"shade":True})</span></pre><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pd"><img src="../Images/7101d174ef2fa633cee54ab4549c59ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LJ5QzBCqutQQ3qwNHzYg6g.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者的图片(对 X 和 y 运行相同的代码)</p></figure><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="42fe" class="kx ky ji ok b gy oo op l oq or">X_len = 400<br/>y_len = 40</span></pre><p id="615c" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">我们来分析一下词频:</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="4cc1" class="kx ky ji ok b gy oo op l oq or">lst_tokens = <strong class="ok jj">nltk</strong>.tokenize.word_tokenize(dtf_train["<strong class="ok jj">text_clean</strong>"].str.cat(sep=" "))<br/>ngrams = [1]<br/>    <br/><strong class="ok jj">## calculate</strong><br/>dtf_freq = pd.DataFrame()<br/>for n in ngrams:<br/>   dic_words_freq = nltk.FreqDist(<strong class="ok jj">nltk</strong>.ngrams(lst_tokens, n))<br/>   dtf_n = pd.DataFrame(dic_words_freq.most_common(), columns=<br/>                        ["word","freq"])<br/>   dtf_n["ngrams"] = n<br/>   dtf_freq = dtf_freq.append(dtf_n)<br/>   dtf_freq["word"] = dtf_freq["word"].apply(lambda x: " <br/>                         ".join(string for string in x) )<br/>   dtf_freq_X= dtf_freq.sort_values(["ngrams","freq"], ascending=<br/>                         [True,False])<br/>    <br/><strong class="ok jj">## plot</strong><br/><strong class="ok jj">sns</strong>.barplot(x="freq", y="word", hue="ngrams", dodge=False,<br/> data=dtf_freq.groupby('ngrams')["ngrams","freq","word"].head(30))<br/>plt.show()</span></pre><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/681a61936d2ad1e617131ca57bdb6a75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tMpSo1_YTxtbcRpFW2Y79A.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者的图片(对 X 和 y 运行相同的代码)</p></figure><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="6e0a" class="kx ky ji ok b gy oo op l oq or">thres = 5 <strong class="ok jj">#&lt;-- min frequency</strong><br/>X_top_words = len(dtf_freq_X[dtf_freq_X["freq"]&gt;thres])<br/>y_top_words = len(dtf_freq_y[dtf_freq_y["freq"]&gt;thres])</span></pre><p id="2a26" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">之后，我们就有了继续进行<strong class="lv jj">特征工程</strong>所需的一切。通过使用<em class="mm"> tensorflow/keras </em>将预处理的语料库转换成序列列表来创建特征矩阵:</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="8e14" class="kx ky ji ok b gy oo op l oq or">lst_corpus = dtf_train["text_clean"]</span><span id="794a" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">## tokenize text</strong><br/>tokenizer = kprocessing.text.<strong class="ok jj">Tokenizer</strong>(num_words=<strong class="ok jj">X_top_words</strong>, lower=False, split=' ', oov_token=None, <br/>filters='!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n')<br/>tokenizer.fit_on_texts(lst_corpus)<br/>dic_vocabulary = {"&lt;PAD&gt;":0}<br/>dic_vocabulary.update(tokenizer.word_index)</span><span id="9f94" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">## create sequence</strong><br/>lst_text2seq= tokenizer.texts_to_sequences(lst_corpus)</span><span id="07de" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">## padding sequence</strong><br/>X_train = kprocessing.sequence.<strong class="ok jj">pad_sequences</strong>(lst_text2seq, <br/>                    maxlen=15, padding="post", truncating="post")</span></pre><p id="debf" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">特征矩阵<em class="mm"> X_train </em>的形状为<em class="mm"> N 个文档 X 个序列最大长度</em>。让我们想象一下:</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="8393" class="kx ky ji ok b gy oo op l oq or"><strong class="ok jj">sns</strong>.heatmap(X_train==0, vmin=0, vmax=1, cbar=False)<br/>plt.show()</span></pre><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pf"><img src="../Images/3a5c8ce7bc5e9cc80e8db9e6252f0a48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hwlLdLyLMnGlCI7vKd_uFg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片(<em class="pg"> N 个文档 x 个序列最大长度)</em></p></figure><p id="27ce" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">在继续之前，不要忘记使用 fitted tokenizer 对测试集进行同样的特征工程:</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="eed5" class="kx ky ji ok b gy oo op l oq or"><strong class="ok jj">## text to sequence with the fitted tokenizer</strong><br/>lst_text2seq = <strong class="ok jj">tokenizer</strong>.texts_to_sequences(dtf_test["<strong class="ok jj">text_clean</strong>"])</span><span id="36e9" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">## padding sequence</strong><br/>X_test = kprocessing.sequence.<strong class="ok jj">pad_sequences</strong>(lst_text2seq, maxlen=15,<br/>             padding="post", truncating="post")</span></pre><p id="30dc" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">现在让我们来看看总结。在应用相同的特征工程策略之前，我们需要在每个摘要中添加两个特殊的标记来确定文本的开始和结束。</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="8a15" class="kx ky ji ok b gy oo op l oq or"><strong class="ok jj"># Add START and END tokens to the summaries (y)</strong><br/>special_tokens = ("&lt;START&gt;", "&lt;END&gt;")<br/>dtf_train["y_clean"] = dtf_train['y_clean'].apply(lambda x: <br/>                     special_tokens[0]+' '+x+' '+special_tokens[1])<br/>dtf_test["y_clean"] = dtf_test['y_clean'].apply(lambda x: <br/>                     special_tokens[0]+' '+x+' '+special_tokens[1])</span><span id="1483" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj"># check example</strong><br/>dtf_test["y_clean"][i]</span></pre><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ph"><img src="../Images/61d679640b32f97071b02dbca71f99b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4QxDM9IqCMF2gyakv5Ud5Q.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="f7d7" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">现在，我们可以通过利用与之前相同的代码来创建带有摘要的特征矩阵(因此，创建一个新的标记器、填充器，并用合适的标记器转换测试集)。如果你打印出词汇表，你会在顶部看到特殊的记号。稍后，我们将使用<em class="mm">开始</em>标记开始预测，当<em class="mm">结束</em>标记出现时，预测的文本将停止。</p><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pi"><img src="../Images/e4b68f3f6ab75d5d9814876d276d8f67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pzlFdvgiixFzoAa_y8siXw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="df18" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">我们可以继续进行单词嵌入。这里有两个选择:从头开始训练我们的单词嵌入模型，或者使用预先训练好的模型。如果你走的是后者，那就按照这部分走，否则可以跳过，直接跳到模型设计。在 Python 中，你可以像这样从<a class="ae mx" href="https://github.com/RaRe-Technologies/gensim-data" rel="noopener ugc nofollow" target="_blank"><em class="mm">genism-data</em></a><em class="mm"/>加载一个预先训练好的单词嵌入模型:</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="8e07" class="kx ky ji ok b gy oo op l oq or">import <strong class="ok jj">gensim_api</strong></span><span id="04c7" class="kx ky ji ok b gy os op l oq or">nlp = gensim_api.load(<strong class="ok jj">"glove-wiki-gigaword-300"</strong>)</span></pre><p id="3392" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">我推荐斯坦福大学的<a class="ae mx" href="https://en.wikipedia.org/wiki/GloVe_(machine_learning)" rel="noopener ugc nofollow" target="_blank"> <em class="mm"> GloVe </em> </a>，这是一种在维基百科、Gigaword 和 Twitter 语料库上训练的无监督学习算法。您可以通过将任何单词转换为向量来测试它:</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="49d3" class="kx ky ji ok b gy oo op l oq or">word = "home"<br/>nlp[word].shape</span><span id="c643" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">&gt;&gt;&gt; (300,)</strong></span></pre><p id="dccb" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">这些单词向量可以在神经网络中用作权重。为了做到这一点，我们需要创建一个嵌入矩阵，使得 id 为<em class="mm"> N </em>的单词的向量位于第<em class="mm">N</em>行。</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="4c02" class="kx ky ji ok b gy oo op l oq or"><strong class="ok jj">## start the matrix (length of vocabulary x vector size) with all 0s</strong><br/>X_embeddings = np.zeros((len(X_dic_vocabulary)+1, 300))for word,idx in X_dic_vocabulary.items():<br/>    <strong class="ok jj">## update the row with vector</strong><br/>    try:<br/>        X_embeddings[idx] =  nlp[word]<br/>    <strong class="ok jj">## if word not in model then skip and the row stays all 0s</strong><br/>    except:<br/>        pass</span></pre><p id="7cf0" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">该代码生成从语料库 x 向量大小中提取的词汇的形状<em class="mm">长度的矩阵(300)。语料库矩阵将用于编码器嵌入层，而摘要矩阵将用于解码器嵌入层。输入序列中的每个 id 将被用作访问嵌入矩阵的索引。这个嵌入层的输出将是一个 2D 矩阵，对于输入序列中的每个单词 id 有一个单词向量(序列长度×向量大小)。让我们以句子“我喜欢这篇文章”为例:</em></p><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pj"><img src="../Images/ad8b0d8547d5a69128b1550de808b145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UI3-aAdZTTH_4Kw3iw8E5Q.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="65c7" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">终于到了构建<strong class="lv jj">编码器-解码器模型的时候了。</strong>首先，我们需要清楚什么是正确的输入和输出:</p><ul class=""><li id="11b7" class="no np ji lv b lw ms lz mt lg nq lk nr lo ns ml nt nu nv nw bi translated">输入是<em class="mm"> X </em>(文本序列)加上<em class="mm"> y </em>(摘要序列)，我们将隐藏摘要的最后一个单词</li><li id="5c1d" class="no np ji lv b lw nx lz ny lg nz lk oa lo ob ml nt nu nv nw bi translated">目标应该是没有<em class="mm">开始</em>标记的<em class="mm"> y </em>(概要序列)。</li></ul><p id="b3fb" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">基本上，您将输入文本交给编码器以理解上下文，然后您向解码器展示摘要如何开始，模型学习预测它如何结束。这是一种称为“教师强制”的训练策略，它使用目标而不是网络生成的输出，以便它可以学习预测<em class="mm"> start </em> token 之后的单词，然后是下一个单词，依此类推(为此，您必须使用时间分布密集层)。</p><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pk"><img src="../Images/39b470316a2f3ff8b5ba58bb81bf4255.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2jo6-r6v_ZeQ600gX-fxTQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="479e" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">我将提出两个不同版本的 Seq2Seq。下面是你能得到的最简单的算法:</p><ul class=""><li id="302b" class="no np ji lv b lw ms lz mt lg nq lk nr lo ns ml nt nu nv nw bi translated">一个嵌入层，它将从头开始创建一个单词嵌入，就像前面描述的那样。</li><li id="f4c0" class="no np ji lv b lw nx lz ny lg nz lk oa lo ob ml nt nu nv nw bi translated">一个单向<em class="mm"> LSTM </em>层，返回一个序列以及单元格状态和隐藏状态。</li><li id="e7c8" class="no np ji lv b lw nx lz ny lg nz lk oa lo ob ml nt nu nv nw bi translated">最终的时间分布密集层，它将相同的密集层(相同的权重)应用于<em class="mm"> LSTM </em>输出，每次一个时间步长，这样输出层只需要一个连接到每个<em class="mm"> LSTM </em>单元。</li></ul><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="b74b" class="kx ky ji ok b gy oo op l oq or">lstm_units = 250<br/>embeddings_size = 300<br/></span><span id="2904" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">##------------ ENCODER (embedding + lstm) ------------------------##<br/></strong>x_in = layers.Input(name="x_in", shape=(X_train.shape[1],))</span><span id="b64d" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">### embedding</strong><br/>layer_x_emb = layers.Embedding(name="x_emb", <br/>                               input_dim=len(X_dic_vocabulary),<br/>                               output_dim=embeddings_size, <br/>                               trainable=True)<br/>x_emb = layer_x_emb(x_in)</span><span id="0182" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">### lstm </strong><br/>layer_x_lstm = layers.LSTM(name="x_lstm", units=lstm_units, <br/>                           dropout=0.4, return_sequences=True, <br/>                           return_state=True)<br/>x_out, state_h, state_c = layer_x_lstm(x_emb)<br/></span><span id="aadc" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">##------------ DECODER (embedding + lstm + dense) ----------------##<br/></strong>y_in = layers.Input(name="y_in", shape=(None,))</span><span id="4eca" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">### embedding</strong><br/>layer_y_emb = layers.Embedding(name="y_emb", <br/>                               input_dim=len(y_dic_vocabulary), <br/>                               output_dim=embeddings_size, <br/>                               trainable=True)<br/>y_emb = layer_y_emb(y_in)</span><span id="53af" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">### lstm </strong><br/>layer_y_lstm = layers.LSTM(name="y_lstm", units=lstm_units, <br/>                           dropout=0.4, return_sequences=True, <br/>                           return_state=True)<br/>y_out, _, _ = layer_y_lstm(y_emb, initial_state=[state_h, state_c])</span><span id="3591" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">### final dense layers</strong><br/>layer_dense = layers.TimeDistributed(name="dense",          layer=layers.Dense(units=len(y_dic_vocabulary), activation='softmax'))<br/>y_out = layer_dense(y_out)<br/></span><span id="9d81" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">##---------------------------- COMPILE ---------------------------##</strong><br/>model = models.Model(inputs=[x_in, y_in], outputs=y_out, <br/>                     name="Seq2Seq")<br/>model.compile(optimizer='rmsprop',<br/>              loss='sparse_categorical_crossentropy', <br/>              metrics=['accuracy'])<br/>model.summary()</span></pre><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pl"><img src="../Images/1ac086099bf92ec2dae28a1370d916c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mIGsEW9m8ZMzlldWUsNGqQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="70d4" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">如果这对你来说还不够，下面是之前的<em class="mm"> Seq2Seq </em>算法的一个高级(并且非常重)版本:</p><ul class=""><li id="ca76" class="no np ji lv b lw ms lz mt lg nq lk nr lo ns ml nt nu nv nw bi translated">嵌入层，利用来自<em class="mm">手套</em>的预训练权重。</li><li id="1d39" class="no np ji lv b lw nx lz ny lg nz lk oa lo ob ml nt nu nv nw bi translated">3 个双向<em class="mm"> LSTM </em>层，在两个方向上处理序列。</li><li id="48af" class="no np ji lv b lw nx lz ny lg nz lk oa lo ob ml nt nu nv nw bi translated">最终时间分布密集层(同前)。</li></ul><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="42e3" class="kx ky ji ok b gy oo op l oq or">lstm_units = 250<br/></span><span id="e0aa" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">##-------- ENCODER (pre-trained embeddings + 3 bi-lstm) ----------##</strong><br/>x_in = layers.Input(name="x_in", shape=(X_train.shape[1],))</span><span id="c264" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">### embedding</strong><br/>layer_x_emb = layers.Embedding(name="x_emb",       <br/>          input_dim=X_embeddings.shape[0], <br/>          output_dim=X_embeddings.shape[1], <br/>          weights=[X_embeddings], trainable=False)<br/>x_emb = layer_x_emb(x_in)</span><span id="6e11" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">### bi-lstm 1</strong><br/>layer_x_bilstm = layers.Bidirectional(layers.LSTM(units=lstm_units, <br/>                 dropout=0.2, return_sequences=True, <br/>                 return_state=True), name="x_lstm_1")<br/>x_out, _, _, _, _ = layer_x_bilstm(x_emb)</span><span id="e506" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">### bi-lstm 2</strong><br/>layer_x_bilstm = layers.Bidirectional(layers.LSTM(units=lstm_units, <br/>                 dropout=0.2, return_sequences=True, <br/>                 return_state=True), name="x_lstm_2")<br/>x_out, _, _, _, _ = layer_x_bilstm(x_out)</span><span id="e4fa" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">### bi-lstm 3 (here final states are collected)</strong><br/>layer_x_bilstm = layers.Bidirectional(layers.LSTM(units=lstm_units, <br/>                 dropout=0.2, return_sequences=True, <br/>                 return_state=True), name="x_lstm_3")<br/>x_out, forward_h, forward_c, backward_h, backward_c = layer_x_bilstm(x_out)<br/>state_h = layers.Concatenate()([forward_h, backward_h])<br/>state_c = layers.Concatenate()([forward_c, backward_c])</span><span id="eb56" class="kx ky ji ok b gy os op l oq or"><br/><strong class="ok jj">##------ DECODER (pre-trained embeddings + lstm + dense) ---------##</strong><br/>y_in = layers.Input(name="y_in", shape=(None,))</span><span id="837a" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">### embedding</strong><br/>layer_y_emb = layers.Embedding(name="y_emb", <br/>               input_dim=y_embeddings.shape[0], <br/>               output_dim=y_embeddings.shape[1], <br/>               weights=[y_embeddings], trainable=False)<br/>y_emb = layer_y_emb(y_in)</span><span id="5bc8" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">### lstm</strong><br/>layer_y_lstm = layers.LSTM(name="y_lstm", units=lstm_units*2, dropout=0.2, return_sequences=True, return_state=True)<br/>y_out, _, _ = layer_y_lstm(y_emb, initial_state=[state_h, state_c])</span><span id="3c9f" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">### final dense layers</strong><br/>layer_dense = layers.TimeDistributed(name="dense", <br/>              layer=layers.Dense(units=len(y_dic_vocabulary), <br/>               activation='softmax'))<br/>y_out = layer_dense(y_out)</span><span id="1b12" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj"><br/>##---------------------- COMPILE ---------------------------------##<br/></strong>model = models.Model(inputs=[x_in, y_in], outputs=y_out, <br/>                     name="Seq2Seq")<br/>model.compile(optimizer='rmsprop',   <br/>              loss='sparse_categorical_crossentropy',<br/>              metrics=['accuracy'])<br/>model.summary()</span></pre><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pm"><img src="../Images/704873ac66147122ac1a2db883c5bd36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8-hTaSpsI8aueElcVtpJqA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="4301" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">在实际测试集上进行测试之前，我将保留一小部分训练集进行验证。</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="51ad" class="kx ky ji ok b gy oo op l oq or"><strong class="ok jj">## train</strong><br/>training = model.fit(x=[X_train, y_train[:,:-1]], <br/>                     y=y_train.reshape(y_train.shape[0], <br/>                                       y_train.shape[1], <br/>                                       1)[:,1:],<br/>                     batch_size=128, <br/>                     epochs=100, <br/>                     shuffle=True, <br/>                     verbose=1, <br/>                     validation_split=0.3,<br/>                     callbacks=[callbacks.<strong class="ok jj">EarlyStopping</strong>(<br/>                                monitor='val_loss', <br/>                                mode='min', verbose=1, patience=2)]<br/>                      )</span><span id="9bab" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">## plot loss and accuracy</strong><br/>metrics = [k for k in training.history.keys() if ("loss" not in k) and ("val" not in k)]<br/>fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)</span><span id="6f9e" class="kx ky ji ok b gy os op l oq or">ax[0].set(title="Training")<br/>ax11 = ax[0].twinx()<br/>ax[0].plot(training.history['loss'], color='black')<br/>ax[0].set_xlabel('Epochs')<br/>ax[0].set_ylabel('Loss', color='black')<br/>for metric in metrics:<br/>    ax11.plot(training.history[metric], label=metric)<br/>ax11.set_ylabel("Score", color='steelblue')<br/>ax11.legend()</span><span id="14a2" class="kx ky ji ok b gy os op l oq or">ax[1].set(title="Validation")<br/>ax22 = ax[1].twinx()<br/>ax[1].plot(training.history['val_loss'], color='black')<br/>ax[1].set_xlabel('Epochs')<br/>ax[1].set_ylabel('Loss', color='black')<br/>for metric in metrics:<br/>     ax22.plot(training.history['val_'+metric], label=metric)<br/>ax22.set_ylabel("Score", color="steelblue")<br/>plt.show()</span></pre><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pn"><img src="../Images/89b00f61340fe970ec25f58ef33e8016.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UZi1QhpK4TYHcSE5SFaQ6Q.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="0a6a" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">请注意，我在回调中使用了<a class="ae mx" href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping" rel="noopener ugc nofollow" target="_blank"> <em class="mm">提前停止</em> </a> <em class="mm"> </em>工具，当受监控的指标(即验证损失)停止改善时，该工具应停止训练。这对节省自己的时间特别有用，尤其是像这样漫长而痛苦的训练。我想补充的是，在不利用 GPU 的情况下运行<em class="mm"> Seq2Seq </em>算法是非常困难的，因为你同时在训练 2 个模型(编码器-解码器)。用有<a class="ae mx" rel="noopener" target="_blank" href="/installing-tensorflow-with-cuda-cudnn-and-gpu-support-on-windows-10-60693e46e781">NVIDIA GPU</a>或者<a class="ae mx" href="https://colab.research.google.com/notebooks/gpu.ipynb" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>的电脑会更好。</p><p id="ba17" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">即使训练完成了，也还没结束！为了测试<em class="mm"> Seq2Seq </em>模型，作为最后一步，我们需要构建<strong class="lv jj">推理模型来生成预测。</strong>预测编码器将新序列(<em class="mm"> X_test </em>)作为输入，并返回最后一个 LSTM 层的输出及其状态。</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="0a6b" class="kx ky ji ok b gy oo op l oq or"><strong class="ok jj"># Prediction Encoder</strong><br/>encoder_model = models.Model(inputs=x_in, outputs=[x_out, state_h, state_c], name="Prediction_Encoder")</span><span id="5008" class="kx ky ji ok b gy os op l oq or">encoder_model.summary()</span></pre><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi po"><img src="../Images/bd9e1f4a98139c5643d078b53905528f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JPk-Ha92wBmAFqpCJZonDw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="8d34" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">另一方面，预测解码器将<em class="mm">开始</em>标记、编码器的输出及其状态作为输入，并返回新的状态以及词汇表上的概率分布(概率最高的单词将是预测)。</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="3520" class="kx ky ji ok b gy oo op l oq or"><strong class="ok jj"># Prediction Decoder</strong></span><span id="268f" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">## double the lstm units if you used bidirectional lstm<br/></strong>lstm_units = lstm_units*2 if any("Bidirectional" in str(layer) for layer in model.layers) else lstm_units</span><span id="7a22" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">## states of the previous time step<br/></strong>encoder_out = layers.Input(shape=(X_train.shape[1], lstm_units))<br/>state_h, state_c = layers.Input(shape=(lstm_units,)), layers.Input(shape=(lstm_units,))</span><span id="6f7a" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">## decoder embeddings</strong><br/>y_emb2 = layer_y_emb(y_in)</span><span id="1a27" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">## lstm to predict the next word</strong><br/>y_out2, state_h2, state_c2 = layer_y_lstm(y_emb2, initial_state=[state_h, state_c])</span><span id="aff1" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">## softmax to generate probability distribution over the vocabulary</strong><br/>probs = layer_dense(y_out2)</span><span id="ed8a" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj">## compile</strong><br/>decoder_model = models.Model(inputs=[y_in, encoder_out, state_h, state_c], outputs=[probs, state_h2, state_c2], name="Prediction_Decoder")</span><span id="487b" class="kx ky ji ok b gy os op l oq or">decoder_model.summary()</span></pre><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pp"><img src="../Images/8b30e43df473e479e00dc94d917eb990.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eM38SBIbUB3z6DcvCu81IQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="11a6" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">在利用<em class="mm">开始</em>令牌和编码器状态进行第一次预测之后，解码器使用生成的字和新状态来预测新字和新状态。该迭代将继续进行，直到模型最终预测到<em class="mm">结束</em>标记或者预测的摘要达到其最大长度。</p><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pq"><img src="../Images/9fbcf5f123019267a309376b1a5d031d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*K9qlLBIycGXtfG7I9jFB2A.gif"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="0dc3" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">让我们对上述循环进行编码，以生成预测并测试<em class="mm"> Seq2Seq </em>模型:</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="51e3" class="kx ky ji ok b gy oo op l oq or"><strong class="ok jj"># Predict<br/></strong>max_seq_lenght = X_test.shape[1]<br/>predicted = []<br/>for x in X_test:<br/>   x = x.reshape(1,-1)</span><span id="27dd" class="kx ky ji ok b gy os op l oq or">   <strong class="ok jj">## encode X</strong><br/>   encoder_out, state_h, state_c = <strong class="ok jj">encoder_model</strong>.predict(x)</span><span id="eed0" class="kx ky ji ok b gy os op l oq or">   <strong class="ok jj">## prepare loop</strong><br/>   y_in = np.array([fitted_tokenizer.word_index[special_tokens[0]]])<br/>   predicted_text = ""<br/>   stop = False<br/>   while not stop:</span><span id="5be3" class="kx ky ji ok b gy os op l oq or">        <strong class="ok jj">## predict dictionary probability distribution</strong><br/>        probs, new_state_h, new_state_c = <strong class="ok jj">decoder_model</strong>.predict(<br/>                          [y_in, encoder_out, state_h, state_c])<br/>        <br/>        <strong class="ok jj">## get predicted word</strong><br/>        voc_idx = np.argmax(probs[0,-1,:])<br/>        pred_word = fitted_tokenizer.index_word[voc_idx]<br/>        <br/>        <strong class="ok jj">## check stop</strong><br/>        if (pred_word != special_tokens[1]) and <br/>           (len(predicted_text.split()) &lt; max_seq_lenght):<br/>            predicted_text = predicted_text +" "+ pred_word<br/>        else:<br/>            stop = True<br/>        <br/>       <strong class="ok jj"> ## next</strong><br/>        y_in = np.array([voc_idx])<br/>        state_h, state_c = new_state_h, new_state_c</span><span id="a544" class="kx ky ji ok b gy os op l oq or">   predicted_text = predicted_text.replace(<br/>                    special_tokens[0],"").strip()<br/>   predicted.append(predicted_text)</span></pre><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pr"><img src="../Images/cd4c68034819a7e08ed70dd8cca9c2b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VzIFrQNOBn2-uxfrbIbO6Q.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ps"><img src="../Images/58357028f6c52c2c83d325c05915077a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cAs0CT6UPTJRp97V42HwCw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="57d2" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">该模型理解上下文和关键信息，但它对词汇的预测很差。这是因为我在这个实验的完整数据集的一个小子集上运行了<em class="mm"> Seq2Seq " </em> lite"。如果你有一个强大的机器，你可以添加更多的数据和提高性能。</p><h2 id="9b78" class="kx ky ji bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">变形金刚(电影名)</h2><p id="4ba2" class="pw-post-body-paragraph lt lu ji lv b lw lx kj ly lz ma km mb lg mc md me lk mf mg mh lo mi mj mk ml im bi translated">变形金刚是谷歌的论文<a class="ae mx" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> <em class="mm">提出的一种新的建模技术</em></a><em class="mm"/>【2017】<em class="mm"/>在该论文中，展示了顺序模型(如<em class="mm"> LSTM </em>)可以完全被注意力机制取代，甚至获得更好的性能。这些语言模型可以通过同时处理序列和映射单词之间的依赖关系来执行任何 NLP 任务，而不管它们在文本中相距多远。因此，在他们的单词中嵌入同一个单词可以根据上下文有不同的向量。最著名的语言模型有 Google 的<em class="mm"/><a class="ae mx" href="https://en.wikipedia.org/wiki/BERT_(language_model)" rel="noopener ugc nofollow" target="_blank"><em class="mm">BERT</em></a>和 OpenAI 的<em class="mm"/><a class="ae mx" href="https://en.wikipedia.org/wiki/GPT-3" rel="noopener ugc nofollow" target="_blank"><em class="mm">GPT</em></a>，用数十亿个参数进行训练。</p><p id="b479" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">脸书的<a class="ae mx" href="https://huggingface.co/transformers/model_doc/bart.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lv jj"> <em class="mm">巴特</em> </strong> </a> <strong class="lv jj"> </strong>(双向自回归变换器)<strong class="lv jj"> </strong>使用标准的<em class="mm"> Seq2Seq </em>双向编码器(像<em class="mm">伯特</em>)和左右自回归解码器(像<em class="mm"> GPT </em>)。基本上，<em class="mm">巴特</em> = <em class="mm">伯特</em> + <em class="mm"> GPT </em>。</p><p id="baa8" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">变形金刚模型的主库是<a class="ae mx" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank"> <em class="mm">变形金刚</em> </a> <em class="mm"> </em>通过<a class="ae mx" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">拥抱面</a>:</p><pre class="mo mp mq mr gt oj ok ol om aw on bi"><span id="591f" class="kx ky ji ok b gy oo op l oq or"><strong class="ok jj">'''<br/>Summarizes corpus with Bart.<br/>:parameter    <br/>   :param corpus: list - dtf["text"]    <br/>   :param max_len: length of the summary<br/>:return    <br/>    list of summaries<br/>'''</strong><br/>def <strong class="ok jj">bart</strong>(corpus, max_len):    <br/>    nlp = <strong class="ok jj">transformers</strong>.pipeline("summarization")    <br/>    lst_summaries = [nlp(txt,               <br/>                         max_length=max_len<br/>                         )[0]["summary_text"].replace(" .", ".")                    <br/>                     for txt in corpus]    <br/>    return lst_summaries</span><span id="8bcd" class="kx ky ji ok b gy os op l oq or"><strong class="ok jj"><br/>## Apply the function to corpus<br/></strong>predicted = <strong class="ok jj">bart</strong>(corpus=dtf_test["text"], max_len=y_len)</span></pre><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pt"><img src="../Images/e0c5525e2ec9fc2dcf18322337f0f65d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*agZMLF7w2dvsQB6_l8BhQA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pu"><img src="../Images/a3d80217838fa73fe8df03707e880260.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ADV5NNoyzhAuBBoczY2Q7Q.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="593f" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">预测简短但有效。对于大多数 NLP 任务来说，Transformer 模型似乎是表现最好的。</p><h2 id="7c6b" class="kx ky ji bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">结论</h2><p id="df7f" class="pw-post-body-paragraph lt lu ji lv b lw lx kj ly lz ma km mb lg mc md me lk mf mg mh lo mi mj mk ml im bi translated">这篇文章是演示<strong class="lv jj">如何将不同的 NLP 模型应用到文本摘要用例</strong>的教程。我比较了 3 种流行的方法:无监督的<em class="mm"> TextRank </em>，基于单词嵌入的两种不同版本的有监督的<em class="mm"> Seq2Seq </em>，以及预训练的<em class="mm"> BART </em>。我经历了特征工程，模型设计，评估和可视化。</p><p id="9573" class="pw-post-body-paragraph lt lu ji lv b lw ms kj ly lz mt km mb lg mu md me lk mv mg mh lo mw mj mk ml im bi translated">我希望你喜欢它！如有问题和反馈，或者只是分享您感兴趣的项目，请随时联系我。</p><blockquote class="pv"><p id="289b" class="pw px ji bd py pz qa qb qc qd qe ml dk translated">👉<a class="ae mx" href="https://linktr.ee/maurodp" rel="noopener ugc nofollow" target="_blank">我们来连线</a>👈</p></blockquote></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><blockquote class="qf qg qh"><p id="6c1f" class="lt lu mm lv b lw ms kj ly lz mt km mb qi mu md me qj mv mg mh qk mw mj mk ml im bi translated">本文是系列文章<strong class="lv jj"> NLP 与 Python </strong>的一部分，参见:</p></blockquote><div class="is it gp gr iu my"><a rel="noopener follow" target="_blank" href="/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd jj gy z fp nd fr fs ne fu fw jh bi translated">基于自然语言处理的文本分类:Tf-Idf vs Word2Vec vs BERT</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">预处理、模型设计、评估、词袋的可解释性、词嵌入、语言模型</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">towardsdatascience.com</p></div></div><div class="nh l"><div class="ql l nj nk nl nh nm ja my"/></div></div></a></div><div class="is it gp gr iu my"><a rel="noopener follow" target="_blank" href="/text-analysis-feature-engineering-with-nlp-502d6ea9225d"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd jj gy z fp nd fr fs ne fu fw jh bi translated">使用自然语言处理的文本分析和特征工程</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">语言检测，文本清理，长度，情感，命名实体识别，N-grams 频率，词向量，主题…</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">towardsdatascience.com</p></div></div><div class="nh l"><div class="qm l nj nk nl nh nm ja my"/></div></div></a></div><div class="is it gp gr iu my"><a rel="noopener follow" target="_blank" href="/text-classification-with-no-model-training-935fe0e42180"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd jj gy z fp nd fr fs ne fu fw jh bi translated">用于无模型训练的文本分类的 BERT</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">如果没有带标签的训练集，请使用 BERT、单词嵌入和向量相似度</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">towardsdatascience.com</p></div></div><div class="nh l"><div class="qn l nj nk nl nh nm ja my"/></div></div></a></div><div class="is it gp gr iu my"><a rel="noopener follow" target="_blank" href="/ai-chatbot-with-nlp-speech-recognition-transformers-583716a299e9"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd jj gy z fp nd fr fs ne fu fw jh bi translated">带 NLP 的 AI 聊天机器人:语音识别+变形金刚</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">用 Python 构建一个会说话的聊天机器人，与你的人工智能进行对话</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">towardsdatascience.com</p></div></div><div class="nh l"><div class="qo l nj nk nl nh nm ja my"/></div></div></a></div></div></div>    
</body>
</html>