<html>
<head>
<title>Understand BLOOM, the Largest Open-Access AI, and Run It on Your Local Computer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解BLOOM，最大的开放存取人工智能，并在您的本地计算机上运行它</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32#2022-08-06">https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32#2022-08-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="dad4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">查看BLOOM解决数学、翻译和编码问题的实践。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/8465b73d27e504dfb518a80645fb9433.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UtRKa6Em1xZ1krGpTHOYjA.jpeg"/></div></div></figure><p id="1dea" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">BLOOM是一个开放访问的多语言模型，包含1760亿个参数，在384个A100–80GB GPU上训练了3.5个月。一个BLOOM检查点需要330 GB的磁盘空间，因此在台式计算机上运行这个模型似乎不可行。然而，你只需要足够的磁盘空间，至少16GB的内存，和一些耐心(你甚至不需要一个GPU)，在你的计算机上运行这个模型。</p><p id="ee08" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">BLOOM是1000多名科学家和神奇拥抱脸团队的合作成果。值得注意的是，这种大型多语言模型对每个人都开放。<strong class="kt ir">本教程结束时，你将学会如何在你的本地计算机上运行这个庞大的语言模型，并看到它生成如下文本:</strong></p><pre class="kg kh ki kj gt ln lo lp lq aw lr bi"><span id="3ec1" class="ls lt iq lo b gy lu lv l lw lx">- INPUT: "The SQL command to extract all the users whose name starts with A is: "<br/>  OUTPUT: "SELECT * FROM users WHERE name LIKE 'A%'"</span><span id="ec9b" class="ls lt iq lo b gy ly lv l lw lx">- INPUT: "The Spanish translation of thank you for your help is: "<br/>  OUTPUT: "gracias por su ayuda"</span><span id="787f" class="ls lt iq lo b gy ly lv l lw lx">- INPUT: "John is 4 times as old as Bob. Bob is 3 years younger than Mike. Mike is 10 years old. What is John's age? Let's think step by step. "<br/>  OUTPUT: "First, we need to find out how old Bob is. Bob is 3 years younger than Mike. So, Bob is 10–3=7 years old. Now, we need to find out how old John is. John is 4 times as old as Bob. So, John is 4 times 7=28 years old"</span></pre><p id="37f8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">本教程使用拥抱脸的<code class="fe lz ma mb lo b">transformers</code>库的一些组件，以及定制的Python代码，有策略地从磁盘加载模型权重，并生成一系列令牌。为了便于学习，本教程中的推理Python代码是从头开始编写的，没有使用Hugging Face Accelerate中的现成实现。对于生产，拥抱脸加速是更强大和多才多艺。本教程中的Python代码在配有i5 11gen处理器、16GB RAM和Samsung 980 PRO NVME硬盘的计算机上每3分钟生成一个令牌(快速硬盘可以显著提高推理速度)。</p><h1 id="2e2b" class="mc lt iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">布鲁姆建筑</h1><p id="c95d" class="pw-post-body-paragraph kr ks iq kt b ku mt jr kw kx mu ju kz la mv lc ld le mw lg lh li mx lk ll lm ij bi translated">BLOOM是一种因果模型语言，这意味着它被训练为下一个令牌预测器。这种基于一组前面的标记来预测句子中的下一个标记的明显简单的策略，已经显示出对于大型语言模型的一定程度的推理能力(arXiv:2205.11916)。这使得BLOOM和类似的模型能够在一个句子中连接多个概念，并以相当的准确性解决算术、翻译和编程等重要问题。BLOOM使用由输入嵌入层、70个转换器块和输出语言建模层组成的转换器架构，如下图所示。每个变压器块有一个自我注意层和一个多层感知器层，输入和后注意层规范。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/38c587e3c14ed310e8e84e8fc508214a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uwWJBgEx3Rtovbcb7HcRdA.jpeg"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">布鲁姆建筑</p></figure><p id="d4ae" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">要使用BLOOM预测句子中的下一个标记，我们只需将输入标记(以嵌入的形式)传递给70个BLOOM块中的每一个。假设这是一个顺序操作，我们可以一次只加载一个块到RAM中，以避免内存溢出。类似地，单词嵌入和输出语言建模层可以按需从磁盘加载。</p><h1 id="7ee8" class="mc lt iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">下载预先训练的BLOOM检查点</h1><p id="a28b" class="pw-post-body-paragraph kr ks iq kt b ku mt jr kw kx mu ju kz la mv lc ld le mw lg lh li mx lk ll lm ij bi translated">使用下面的代码从拥抱脸模型库中下载BLOOM (176-B版本):<a class="ae nd" href="https://huggingface.co/bigscience/bloom" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/bigscience/bloom</a>。这将下载特定的BLOOM检查点<code class="fe lz ma mb lo b">2a3d62e</code>。虽然BLOOM的模型大小在330GB左右，<code class="fe lz ma mb lo b">git lfs</code>下载额外的链接文件，那么下载大小差不多是700GB。确保您有足够的磁盘空间。</p><pre class="kg kh ki kj gt ln lo lp lq aw lr bi"><span id="ecd3" class="ls lt iq lo b gy lu lv l lw lx">git lfs install<br/>export GIT_LFS_SKIP_SMUDGE=1<br/>git clone <a class="ae nd" href="https://huggingface.co/bigscience/bloom" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/bigscience/bloom</a><br/>cd bloom<br/>git lfs fetch origin 2a3d62e<br/>git lfs checkout</span></pre><p id="1f66" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下载的文件夹包含一个分片的BLOOM检查点，如下所示。分片意味着检查点被分成72个不同的文件，命名为<code class="fe lz ma mb lo b">pytorch_model_00001-of-00072.bin</code>到<code class="fe lz ma mb lo b">pytorch_model_00001-of-00072.bin</code>，以便于处理。</p><pre class="kg kh ki kj gt ln lo lp lq aw lr bi"><span id="2ab4" class="ls lt iq lo b gy lu lv l lw lx">&gt; ls -la<br/>6.7 GB  pytorch_model_00001-of-00072.bin <br/>4.6 GB  pytorch_model_00002-of-00072.bin <br/>...<br/>4.6 GB  pytorch_model_00071-of-00072.bin<br/> 57 KB  pytorch_model_00072-of-00072.bin<br/>0.5 KB  config.json<br/> 14 MB  tokenizer.json<br/> 13 KB  pytorch_model.bin.index.json</span></pre><p id="f138" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">文件<code class="fe lz ma mb lo b">00001</code>包含单词嵌入和相关层规范，文件<code class="fe lz ma mb lo b">00002 </code>到<code class="fe lz ma mb lo b">00071 </code>包含70个BLOOM块，文件<code class="fe lz ma mb lo b">00072</code>包含最终层规范。输出语言建模层使用与单词嵌入相同的权重。如果你好奇的话，<code class="fe lz ma mb lo b">pytorch_model.bin.index.json</code>文件指定了BLOOM层是如何分布在碎片上的。</p><h1 id="970b" class="mc lt iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">推理</h1><p id="068c" class="pw-post-body-paragraph kr ks iq kt b ku mt jr kw kx mu ju kz la mv lc ld le mw lg lh li mx lk ll lm ij bi translated">现在我们用下载的BLOOM模型来做推断。首先我们需要安装拥抱脸<code class="fe lz ma mb lo b">transformers </code> v4.20.0，如下图。这个特定的版本是必需的，因为本教程中的定制Python代码使用了仅在这个特定版本的<code class="fe lz ma mb lo b">transformers</code>中可用的方法。</p><pre class="kg kh ki kj gt ln lo lp lq aw lr bi"><span id="aab9" class="ls lt iq lo b gy lu lv l lw lx">pip install transformers==4.20.0</span></pre><p id="b0dd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">第二，我们创建一个方法(<code class="fe lz ma mb lo b">get_state_dict</code>)，它接受一个碎片号(1到72)作为输入，从磁盘中读取碎片，并返回一个包含模型对象状态的字典。该方法允许从字典键中移除前缀，以便于使用<code class="fe lz ma mb lo b">torch.load_state_dict</code>将权重加载到模型对象中。我们还通过从下载的文件夹中加载来创建令牌化器和配置对象。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="dbba" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">第三，我们创建了三种方法来将状态字典加载到不同的模型对象中。我们在推理过程中使用这些方法，只将模型的特定部分加载到RAM中。这三种方法遵循相似的模式，包括:1)从磁盘读取碎片，2)创建模型对象，3)使用<code class="fe lz ma mb lo b">torch.load_state_dict</code>填充模型对象的权重，以及4)返回模型对象。唯一的例外是<code class="fe lz ma mb lo b">load_block </code>方法，它不创建新的块对象，而是覆盖作为参数传递的对象，以节省RAM内存。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="9469" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">第四，我们创建了一个方法来完全向前遍历BLOOM的所有层。该方法将令牌输入id的数组作为输入，并返回预测为句子中下一个的令牌id。该方法从创建注意屏蔽和位置编码(alibi)开始。然后，它在嵌入层上向前传递以创建初始的<code class="fe lz ma mb lo b">hidden_states</code>。接下来，它将<code class="fe lz ma mb lo b">hidden_states</code>依次通过70个BLOOM块和输出语言模型头来生成输出logits。<code class="fe lz ma mb lo b">argmax</code>获取输出逻辑并返回预测概率最高的令牌id。注意，在使用嵌入之后，我们删除它们以避免溢出内存。同样，每次我们调用bloom块时，我们从磁盘读取一个新的对象，但是覆盖现有的<code class="fe lz ma mb lo b">block </code>对象的权重以节省内存。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="9d20" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">最后，我们定义一个输入句子，将其标记化，然后依次调用forward方法来预测句子中的下一个标记，一次一个标记。注意，在每一步，我们将新生成的令牌与之前的令牌(<code class="fe lz ma mb lo b">input_ids</code>)连接起来，以进一步生成额外的令牌。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ne nf l"/></div></figure><pre class="kg kh ki kj gt ln lo lp lq aw lr bi"><span id="66eb" class="ls lt iq lo b gy lu lv l lw lx">INPUT: The SQL command to extract all the users whose name starts with A is:</span><span id="c6b3" class="ls lt iq lo b gy ly lv l lw lx">OUTPUT:<br/>Token 1 ....... SELECT<br/>Token 2 ....... *<br/>Token 3 ....... FROM<br/>Token 4 ....... users<br/>Token 5 ....... WHERE<br/>Token 6 ....... name<br/>Token 7 ....... LIKE<br/>Token 8 ....... 'A<br/>Token 9 ....... %'<br/>Token 10 ....... <br/><br/>The SQL command to extract all the users whose name starts with A is:  SELECT * FROM users WHERE name LIKE 'A%'</span></pre><p id="8de9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这个例子说明BLOOM可以生成一个有意义的SQL语句。您可以运行其他示例(例如，本教程开头提到的示例)来看看BLOOM有多强大。只需记住使用<code class="fe lz ma mb lo b">max_tokens </code>变量增加要生成的令牌数量。</p><h2 id="3b81" class="ls lt iq bd md ng nh dn mh ni nj dp ml la nk nl mn le nm nn mp li no np mr nq bi translated">结论</h2><p id="c1c1" class="pw-post-body-paragraph kr ks iq kt b ku mt jr kw kx mu ju kz la mv lc ld le mw lg lh li mx lk ll lm ij bi translated">由于其开放访问和多语言特性，BLOOM被认为是十年来最重要的人工智能模型之一。这项突破性的技术将彻底改变自然语言处理的研究和实践。通过阅读本教程，即使您的计算资源有限，也可以利用BLOOM的强大功能来生成文本。此外，您可以使用伟大的拥抱脸<code class="fe lz ma mb lo b">transformers</code>库来微调BLOOM以执行下游任务，如问题回答和文本分类。如果BLOOM的大版本对于您的应用程序或可用的计算资源来说太大，您可以利用拥抱人脸模型库中可用的较小版本的BLOOM(<a class="ae nd" href="https://huggingface.co/bigscience" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/bigscience</a>)。</p><p id="878d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在我网站的博客部分有一个Jupyter笔记本，里面有本教程的所有源代码:<a class="ae nd" href="https://arteagac.github.io/" rel="noopener ugc nofollow" target="_blank">https://arte agac . github . io</a></p></div></div>    
</body>
</html>