<html>
<head>
<title>Must-Read Papers in Computer Vision for the 2020s *Updated*</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2020年计算机视觉必读论文*更新*</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/must-read-papers-in-computer-vision-for-the-2020s-3943d5339ba4#2022-06-03">https://towardsdatascience.com/must-read-papers-in-computer-vision-for-the-2020s-3943d5339ba4#2022-06-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2a98" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">计算机视觉中深度学习技术的前沿</h2></div><p id="a2ee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">计算机视觉社区目前正在发生什么？如果你像我一样是一个狂热的计算机视觉爱好者，这里有几篇我近年来最喜欢的论文，我相信它们将对未来产生巨大的影响。</p><p id="1842" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">免责声明:这些论文是我认为的“基础”论文。有许多论文延伸到它们之外，具有很大的见解，我将逐渐在这里添加链接以供参考。</p><h1 id="9789" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">屏蔽的自动编码器是可扩展的视觉学习器</h1><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lu"><img src="../Images/9854b9ca29d571760c81efa4d972346c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Jng3DLa5Yjh-Xxc1b59nQ.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图一。屏蔽的自动编码器管道。从原始纸张中检索的图像。</p></figure><p id="6b7f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你跟踪深度学习技术有一段时间了，你可能听说过BERT，这是一种自我监督的技术，用于语言中，你可以屏蔽部分句子，进行预训练，以改善学习隐式表示。然而，在学习更好的特征提取方面，同样的技术不能转移到CNN上——直到最近引入了视觉变形金刚vit。</p><p id="8afa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于transformer架构，He等人研究了屏蔽是否适用，结果令人振奋:一个屏蔽的自动编码器伴随ViTs被证明是一种有效的预处理技术，用于分类和重建等下游任务。</p><p id="ebc5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">【论文链接:【https://arxiv.org/abs/2006.11239】<em class="lb"/><em class="lb"/></p><h2 id="fdca" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">相关论文:</h2><ul class=""><li id="8d83" class="mx my iq kh b ki mz kl na ko nb ks nc kw nd la ne nf ng nh bi translated"><em class="lb">作为时空学习者的蒙版自动编码器:</em><a class="ae mk" href="https://arxiv.org/abs/2205.09113" rel="noopener ugc nofollow" target="_blank"><em class="lb">https://arxiv.org/abs/2205.09113</em></a></li><li id="f847" class="mx my iq kh b ki ni kl nj ko nk ks nl kw nm la ne nf ng nh bi translated"><em class="lb"> Point-BERT:预训练带遮罩点建模的3D点云变形器:</em><a class="ae mk" href="https://arxiv.org/abs/2111.14819" rel="noopener ugc nofollow" target="_blank"><em class="lb">https://arxiv.org/abs/2111.14819</em></a></li></ul><h1 id="93fd" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">21世纪20年代的通信网</h1><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi nn"><img src="../Images/2e15155b5e1e9d71027632fad6c56d01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F8hWMxicnJ5lexpro0XDwA.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图二。ConvNeXt与变压器的比较。从原始纸张中检索的图像。</p></figure><p id="6a43" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一方面，尽管vit在许多视觉任务的论文中表现出了优越性，但有一项工作在分析卷积网络(ConvNet)的基础方面表现突出。Liu等人专注于“现代化”卷积网络，引入经验证明对vit有用的元素，并表明ConvNet确实可以在ImageNet等大型数据集上实现类似于vit的结果。</p><p id="8fd9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">论文链接:【https://arxiv.org/abs/2201.03545】<a class="ae mk" href="https://arxiv.org/abs/2201.03545" rel="noopener ugc nofollow" target="_blank"><em class="lb"/></a></em></p><h2 id="62e7" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">相关论文:</h2><ul class=""><li id="0a7b" class="mx my iq kh b ki mz kl na ko nb ks nc kw nd la ne nf ng nh bi translated"><em class="lb">将您的内核扩展到31x31:重温CNN中的大型内核设计:</em><a class="ae mk" href="https://arxiv.org/abs/2203.06717" rel="noopener ugc nofollow" target="_blank"><em class="lb">https://arxiv.org/abs/2203.06717</em></a></li></ul><h1 id="4214" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated"><strong class="ak"> NeRF:将场景表示为用于视图合成的神经辐射场</strong></h1><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi no"><img src="../Images/5a1b541f8edb2d817b869b4decf04c4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aMRKOJWTgn66jWsCHGgOUw.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图3。NeRF的管道。从原始纸张中检索的图像。</p></figure><p id="e61d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">移动到三维空间，在重建和新颖的视图合成方面也有惊人的突破。Mildenhall等人提出使用全连接网络来表示3D形状。也就是说，给定一些坐标和视角，网络输出特定空间的相应颜色。这允许以高质量和精确的质量生成未知角度的物体的新图像/视图。</p><p id="3d7f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">论文链接:</em><a class="ae mk" href="https://arxiv.org/abs/2201.03545" rel="noopener ugc nofollow" target="_blank"><em class="lb">https://arxiv.org/abs/2201.03545</em></a></p><h2 id="2355" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">相关论文:</h2><ul class=""><li id="4a77" class="mx my iq kh b ki mz kl na ko nb ks nc kw nd la ne nf ng nh bi translated"><em class="lb"> GNeRF:无姿态摄像机的GAN基神经辐射场:</em><a class="ae mk" href="https://arxiv.org/abs/2103.15606" rel="noopener ugc nofollow" target="_blank"><em class="lb">https://arxiv.org/abs/2103.15606</em></a></li><li id="0e20" class="mx my iq kh b ki ni kl nj ko nk ks nl kw nm la ne nf ng nh bi translated"><em class="lb"> BARF:束调节神经辐射场:</em><a class="ae mk" href="https://arxiv.org/abs/2104.06405" rel="noopener ugc nofollow" target="_blank"><em class="lb">https://arxiv.org/abs/2104.06405</em></a></li><li id="9a2a" class="mx my iq kh b ki ni kl nj ko nk ks nl kw nm la ne nf ng nh bi translated"><em class="lb"> CityNeRF:城市尺度的建筑NeRF:</em><a class="ae mk" href="https://arxiv.org/abs/2112.05504" rel="noopener ugc nofollow" target="_blank"><em class="lb">https://arxiv.org/abs/2112.05504</em></a></li><li id="3f9d" class="mx my iq kh b ki ni kl nj ko nk ks nl kw nm la ne nf ng nh bi translated"><em class="lb"> Block-NeRF:可扩展大场景神经视图合成:</em><a class="ae mk" href="https://arxiv.org/abs/2202.05263" rel="noopener ugc nofollow" target="_blank"><em class="lb">https://arxiv.org/abs/2202.05263</em></a></li></ul><h1 id="faaa" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated"><strong class="ak">去噪扩散概率模型</strong></h1><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi np"><img src="../Images/1591aa81fbfe3ef011a52b5c630158b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VYSuSLUNqMkZYUWEZhG5Kg.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图4。去噪扩散模型正反过程。从原始纸张中检索的图像。</p></figure><p id="dafc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当讨论生成网络时，人们通常会想到gan或VAEs。令人惊讶的是，扩散模型是近年来优秀的模型家族。简而言之，扩散模型在其正向过程中逐渐向图像添加噪声，同时学习反向过程。因此，逆向过程成为生成模型，其中图像可以从纯随机噪声中“提取”，类似于模型被训练的数据分布。</p><p id="b521" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">论文链接:</em>【https://arxiv.org/abs/2006.11239】<em class="lb"/></p><p id="d2ad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">相关论文:</strong></p><ul class=""><li id="795b" class="mx my iq kh b ki kj kl km ko nq ks nr kw ns la ne nf ng nh bi translated"><em class="lb">改进的去噪扩散概率模型:</em><a class="ae mk" href="https://arxiv.org/abs/2102.09672" rel="noopener ugc nofollow" target="_blank"><em class="lb"/></a></li><li id="6789" class="mx my iq kh b ki ni kl nj ko nk ks nl kw nm la ne nf ng nh bi translated"><em class="lb">具有深度语言理解的真实感文本到图像扩散模型:</em><a class="ae mk" href="https://arxiv.org/abs/2205.11487" rel="noopener ugc nofollow" target="_blank"><em class="lb">https://arxiv.org/abs/2205.11487</em></a></li><li id="3a8f" class="mx my iq kh b ki ni kl nj ko nk ks nl kw nm la ne nf ng nh bi translated"><em class="lb">三维点云生成的扩散概率模型:</em><a class="ae mk" href="https://arxiv.org/abs/2103.01458" rel="noopener ugc nofollow" target="_blank"><em class="lb">https://arxiv.org/abs/2103.01458</em></a></li></ul><p id="6da1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">感谢您坚持到现在</em>🙏<em class="lb">！</em> <em class="lb">我会在计算机视觉/深度学习的不同领域发布更多内容，所以</em> <a class="ae mk" href="https://taying-cheng.medium.com/membership" rel="noopener"> <em class="lb">加入并订阅</em> </a> <em class="lb">如果你有兴趣了解更多！</em></p></div></div>    
</body>
</html>