<html>
<head>
<title>Multiclass Text Classification Using Keras to Predict Emotions: A Comparison with and without Word Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Keras预测情感的多类文本分类:有无单词嵌入的比较</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multiclass-text-classification-using-keras-to-predict-emotions-a-comparison-with-and-without-word-5ef0a5eaa1a0#2022-04-19">https://towardsdatascience.com/multiclass-text-classification-using-keras-to-predict-emotions-a-comparison-with-and-without-word-5ef0a5eaa1a0#2022-04-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="68df" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">单词嵌入增加了文本分类模型的价值吗？让我们在这个探测情绪的多类预测任务中找出答案</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9ba48a61f95bb4778f641d99278edfe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sNEopFim0jLbqRzI"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@tengyart?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">腾雅特</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="3c6d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="a179" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">自然语言处理中的单词嵌入</strong></p><p id="ac99" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在自然语言表达中，相似的词出现在相似的上下文中。例如，在这个句子中——“我不舒服，我生病了。“我可能得了covid，”。单词— <em class="ms">不舒服</em>、<em class="ms">生病</em>和<em class="ms"> covid </em>是表示一个人生病的关键词，这是显而易见的。像<em class="ms">头痛</em>、<em class="ms">疼痛</em>、<em class="ms">跑步</em> - <em class="ms">鼻子</em>、<em class="ms">咳嗽</em>等单词也有类似的上下文。然而，关于<em class="ms">政治</em>或<em class="ms">体育</em>的句子中很少会出现这些单词。在<em class="ms">体育</em>的上下文中，最频繁出现的关键词可以是<em class="ms">速度</em>、<em class="ms">耐力</em>、<em class="ms">比赛</em>、<em class="ms">赢</em>、<em class="ms">输</em>、<em class="ms">分</em>、<em class="ms">分</em>等等。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/f2363baa4b24c8235948ba2720ef8861.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*84R92KUSXtf_itPC"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@glencarrie?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">格伦·凯莉</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="cefa" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因此，前一组词更多地出现在人们谈论健康或疾病的上下文中，而第二组关键词更可能出现在体育文章或新闻的上下文中。他们很少会重合，但如果重合，可能是关于一个可能生病的球员或谈论从事体力要求高的运动的后果，两者都有体育和健康的<em class="ms">重叠背景</em>。这些案例预计会相对较少。</p><p id="593b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因此，我们可以说，如果一个单词在其他文档中的类似上下文或单词分布中出现过，则在给定上下文或单词分布中找到该单词的概率更高。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><p id="0f5e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">那么什么是单词嵌入呢？</p><p id="e316" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="ms">单词嵌入是自然语言文本的密集矢量表示，包含关于给定单词上下文的信息</em>。该概念解决了其他语言建模策略的局限性，如以稀疏矩阵表示文本的<a class="ae ky" href="https://www.youtube.com/watch?v=kNLqQSqbO5k" rel="noopener ugc nofollow" target="_blank"> tf-idf和词袋</a>模型，这些模型包含冗余信息，使该过程在计算上昂贵且耗时，并增加了模型的复杂性。此外，它们也特别大，因为矩阵的大小通常和词汇表的大小一样，这就带来了“维数灾难”的问题。</p><p id="5ec4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">换句话说，在单词嵌入中，单词以这样一种方式被表示为向量(即数字),即在相似上下文中出现的单词在该词汇表的向量空间中间隔很近。为了进一步解释这一点，如果我们记下我们目前拥有的关键字，假设它们已经出现在N个文档中，那么,“生病”和“咳嗽”的向量的距离将小于“咳嗽”和“速度”或“咳嗽”和“胜利”之间的距离，而“胜利”和“匹配”之间的距离将小于“胜利”和“生病”。</p><p id="38fe" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是一种确定文本的数字表示的方法，它试图捕捉在多个文档中出现的单词之间的上下文相似性。</p><p id="837d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">获得单词嵌入有多种方式</strong>。word2vec算法学派用于使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank">ann</a>导出嵌入。Word2vec的经典实现是在Gensim中，您可以在连续单词包(CBOW)模型或跳过Gram模型之间进行选择。<em class="ms">CBOW通过尝试预测给定上下文中最合适的单词来学习表示，而在skip grams中，它通过尝试预测给定单词的最合适上下文来学习。</em></p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><p id="83b0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">关于多级分类</strong></p><p id="0340" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="ms">在机器学习中，一个</em> <strong class="lt iu"> <em class="ms">监督的多类分类任务</em> </strong> <em class="ms">是一个样本可以被分配给一组类中的一个且仅一个类。</em>例如，在情感分析任务中，样本可以是正面的，也可以是负面的，其中有两个类别可供选择。很多时候，情绪也可以是积极的、消极的或中性的，有三类可供选择。</p><p id="320c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">第一个例子是特殊类型的多类分类过程。由于有两类可供选择，即正类和负类，所以称之为<strong class="lt iu">二元分类任务</strong>。另一个典型的例子是在欺诈检测任务中，交易可能是欺诈或真实的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/64893d298382d34a603454597f528178.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ps4aZ_teYD66yMTa"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@pickawood?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">皮卡伍德</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="644e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里的第二个例子有两个以上的类可供选择。有两个以上类别可供选择的分类任务统称为<strong class="lt iu">多类别分类</strong>问题，其中<em class="ms">在两个以上的目标类别</em>中，一个且仅一个类别可以分配给一个样本。</p><p id="73db" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">多类分类的评估比二进制分类稍微复杂一些，因为您将评估一个NxN矩阵，其中N是任务中类的数量，而不是二进制分类的2x2矩阵。<strong class="lt iu">因此，为了简化，使用了平均技术，如算术平均(宏)、加权平均和总体精度。</strong>由于准确度不是最佳指标，通常根据数据集的类别分布在算术平均值和加权平均值之间做出选择。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><p id="f300" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi nc translated">他的案例研究是一个多类分类任务。在这里，我将预测与给定文本相关的情绪，从六个不同的类别中选择——快乐、悲伤、愤怒、爱、惊讶和恐惧。本文解决了以下问题:</p><ol class=""><li id="43f0" class="nl nm it lt b lu mn lx mo ma nn me no mi np mm nq nr ns nt bi translated"><em class="ms">使用Word2vec嵌入启动Keras嵌入层权重是否会显著提升模型的性能？</em></li><li id="89bb" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated"><em class="ms">从编程角度看哪个好？</em></li><li id="f4c8" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated"><em class="ms">通过word2vec模型、预测模型训练后更新的Keras嵌入层权重以及没有使用word2vec嵌入初始化层权重的情况下的相似性的比较。</em></li></ol><p id="21e4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了回答这些问题，我将使用<strong class="lt iu">两种嵌入策略</strong>来训练分类器:</p><p id="cbd0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">策略1: </strong> Gensim的嵌入，用于初始化Keras嵌入层的权重</p><p id="b9fe" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">策略2: </strong>使用反向传播进行改进，随机初始化嵌入层，即跳过单词嵌入的使用。</p><p id="af8e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="ms">注意:按照教程，Python库需求在这里</em>  <em class="ms">列出</em> <a class="ae ky" href="https://github.com/royn5618/Medium_Blog_Codes/blob/master/Emotion%20Detection/requirements.txt" rel="noopener ugc nofollow" target="_blank"> <em class="ms">。</em></a></p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="e9d3" class="kz la it bd lb lc nz le lf lg oa li lj jz ob ka ll kc oc kd ln kf od kg lp lq bi translated">一、关于数据</h1><p id="d974" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">一、数据来源</strong></p><p id="011d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我在Kaggle <a class="ae ky" href="https://www.kaggle.com/praveengovi/emotions-dataset-for-nlp" rel="noopener ugc nofollow" target="_blank">这里</a>和拥抱脸数据集<a class="ae ky" href="https://huggingface.co/datasets/emotion" rel="noopener ugc nofollow" target="_blank">这里</a>上使用了一个公开的数据集。数据集包含带有相应情感标签的文档列表。这些数据被分成用于构建机器学习模型的训练、测试&amp;验证集。</p><p id="59e1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">二世。基础数据统计</strong></p><ol class=""><li id="f905" class="nl nm it lt b lu mn lx mo ma nn me no mi np mm nq nr ns nt bi translated">16k训练、2k测试和2k验证实例。我将使用训练数据来分割和验证模型，并使用测试数据进行测试。我没有在本文中使用验证数据。</li><li id="7bdb" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">没有丢失的值</li><li id="78d0" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">以下是训练和测试数据的分布:</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在<a class="ae ky" href="https://chart-studio.plotly.com/" rel="noopener ugc nofollow" target="_blank"> Plotly图表工作室</a>上绘制的图表</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在<a class="ae ky" href="https://chart-studio.plotly.com/" rel="noopener ugc nofollow" target="_blank"> Plotly Chart Studio </a>上绘制的图形</p></figure><p id="ba01" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">处理没有缺失值的数据更容易😊显然，数据集是不平衡的。训练和测试数据遵循相同的情绪分布。大多数样本要么被标为“悲伤”，要么被标为“快乐”。为了简单起见，我将保留这个分布用于分类器训练。</p><h1 id="4027" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">二。数据预处理和特征工程</h1><p id="cd59" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu"> i .标签的OneHot编码— </strong>因为我们的标签是分类的，所以它们被转换成大小为<em class="ms"> 1 x 6 </em>的二进制数组，其中每个位置可以取值为<em class="ms"> 0 </em>或<em class="ms"> 1 </em>。在这个数组中，除了代表与数据样本相关联的标签的索引之外，所有的值都是<em class="ms"> 0 </em>。例如，如果在第一个位置我们有'<em class="ms"> joy </em>，并且样本被标记为'<em class="ms"> joy </em>'，那么该数组将看起来像<em class="ms"> [1，0，0，0，0，0] </em>，其中每隔一个位置引用其他标签。同样，假设第三个位置是'<em class="ms">悲伤</em>'，样本标记为'<em class="ms">悲伤</em>'，数组变成<em class="ms"> [0，0，1，0，0，0] </em>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og of l"/></div></figure><p id="aa18" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这段代码中，我首先加载了具有标准列名的数据集。我使用了<a class="ae ky" href="https://pandas.pydata.org/docs/user_guide/categorical.html" rel="noopener ugc nofollow" target="_blank"> Panda的<strong class="lt iu">类别列类型</strong> </a> <strong class="lt iu"> </strong>，它自动为列类别分配数字类别代码。接下来，在第10行，我已经使用了<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/one_hot" rel="noopener ugc nofollow" target="_blank"> TensorFlow的one_hot </a>方法为六种情绪构建了一个hot编码矩阵。</p><p id="8328" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">二。使用Gensim的Word2Vec模型训练— </strong>代码非常简单。<a class="ae ky" href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html" rel="noopener ugc nofollow" target="_blank"> Gensim的word2vec </a>所需的输入是样本的标记化形式。为此，我使用了NLTK的<a class="ae ky" href="https://www.nltk.org/api/nltk.tokenize.html" rel="noopener ugc nofollow" target="_blank"> word_tokenize </a>方法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og of l"/></div></figure><p id="4e63" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">接下来，我使用下面的代码为这个数据集生成嵌入。<strong class="lt iu">注意，我只使用了训练数据集。</strong>除此之外，我已经设置了通常的默认配置，并使用sg=1的skip-gram模型进行指示。和..窗口大小为20，这意味着模型将被训练，同时试图从给定的单词中预测前面的20个单词和后面的20个单词。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og of l"/></div></figure><p id="5238" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">使用word2vec的注意事项是</strong> —当您测试模型性能或将模型应用于未知数据样本时，您需要像准备训练样本一样预处理令牌。现在，如果word2vec模型看到一个未知单词，那么它将引发一个<a class="ae ky" href="https://realpython.com/python-keyerror/" rel="noopener ugc nofollow" target="_blank"> KeyError </a>。</p><p id="30c6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">为什么？</strong></p><p id="8954" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因为每个单词嵌入都是使用唯一标识该嵌入所针对的单词的关键字来存储的。对于新的vocab，密钥将不可用，因此会出现错误。这使得Word2Vec的定制模型的使用不灵活，因为它是在一个小数据集上训练的，这使得它无法捕获完整范围的词汇。但是，如果它是一个在大型数据集上预先训练的英语模型，那么在“正确”英语中出现的大多数单词都有可能在模型中被捕获。</p><p id="0092" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">有两种方法可以处理定制模型中的这种不灵活性— </strong></p><ol class=""><li id="2775" class="nl nm it lt b lu mn lx mo ma nn me no mi np mm nq nr ns nt bi translated">通过使用用于训练Word2Vec模型的词汇，从测试数据中比较并丢弃Vocab(OOV) 中的<strong class="lt iu">个单词。这样，您就不太可能遇到测试数据中缺少vocab的错误。</strong></li><li id="cba2" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated"><strong class="lt iu">错误处理</strong> —在这个方法中，对于你遇到的每一个错误，你都应该删除或者处理OOV词。</li></ol><p id="1491" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">你应该避免的解决方案:</strong>一起使用训练和测试集vocab，因为那可能导致数据泄漏。此外，这并不能保证看不见的实例不会失败。</p><p id="c291" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我选择了方法1，下面是实现。我简单地遍历了列表，删除了测试数据中没有出现在word2vec模型的vocab中的单词。请注意，如果您使用<a class="ae ky" href="https://www.w3schools.com/python/python_sets.asp" rel="noopener ugc nofollow" target="_blank"> Python的set操作</a>来删除令牌，令牌的顺序将被打乱，因此用处不大。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og of l"/></div></figure><p id="7067" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">三世。生成嵌入矩阵:</strong></p><p id="651a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了在Keras嵌入层中使用word2vec嵌入，我们需要在一个<em class="ms">vocab _ size x Embedding _ size</em>矩阵中组织权重，在本例中为<em class="ms"> 15210 x 300 </em>。下面是我如何使用Gensim获得这个。换句话说，这只是一个查找矩阵，其中第<em class="ms">行的单词向量是word2vec模型词汇表中第<em class="ms">行的单词向量。</em></em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og of l"/></div></figure><p id="1960" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">四。准备培训和测试数据</strong></p><p id="da95" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">接下来，准备训练和测试数据的时间到了，我已经用word2vec词汇表中的单词索引替换了标记。当分类器训练时，通过将标记索引与嵌入矩阵中的行号进行匹配来提取单词向量。</p><p id="9bf0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最终，样品的长度被标准化为<em class="ms"> 20 </em>。如果长度超过<em class="ms"> 20 </em>，则样本在末尾被截断，如果长度小于20，则在末尾再次用零填充，如第24行和第25行所示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og of l"/></div></figure><p id="1805" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">如何确定序列长度:</strong></p><p id="a0fe" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，为了使用Keras的嵌入层训练一个人工神经网络模型，我需要标准化输入文本长度。因此，我通过绘制每个样本中单词数量的直方图来分析样本长度。平均来说，大多数样本包含大约15个T21式的单词。为了捕捉比平均字数多一点的单词，我选择了<em class="ms"> 20 </em>，介于第50个和第75个百分位数之间。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在<a class="ae ky" href="https://chart-studio.plotly.com/" rel="noopener ugc nofollow" target="_blank"> Plotly Chart Studio </a>上绘制的图形</p></figure><h1 id="542b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">三。训练多类分类器</h1><p id="954d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在我们已经准备好了数据，是时候深入研究分类器训练了。在本文中，我将只关注<a class="ae ky" href="https://keras.io/api/layers/core_layers/embedding/" rel="noopener ugc nofollow" target="_blank"> Keras嵌入层</a>是如何工作的。<a class="ae ky" rel="noopener" target="_blank" href="/predicting-fake-news-using-nlp-and-machine-learning-scikit-learn-glove-keras-lstm-7bbd557c3443"> <em class="ms">关于如何使用Keras进行ML模型训练的详细概述包括代码在此。</em> </a></p><p id="bf63" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">那么，Keras嵌入层是做什么的呢？</strong></p><p id="876c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Keras嵌入层通常用于使用深度学习的文本建模任务。它简单地通过输出维数初始化输入维数矩阵，其中输入维数是词汇表的大小，输出维数是代表向量的大小，以构成整个vocab的所有单词嵌入的查找表。对于词汇表中代表单词或标记的每个输入整数，该数字将用于从查找表中找到单词嵌入的索引。</p><p id="967e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，如果我们有一个预训练的权重，我们可以使用<strong class="lt iu">权重</strong>参数加载它，并且我们已经生成了一个查找表。另一种情况是，您没有预先训练的权重，查找表是随机生成的(<strong class="lt iu">权重=无</strong>)，并使用预测中的误差进行改进。</p><h2 id="a256" class="oh la it bd lb oi oj dn lf ok ol dp lj ma om on ll me oo op ln mi oq or lp os bi translated"><strong class="ak"> i .使用Word2vec嵌入来训练分类器:</strong></h2><p id="f983" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本节中，我将展示用于训练分类器的代码。请注意，我在结尾使用了一个具有6个单元的全连接层(因为我们有6种情绪要预测)和一个“softmax”激活层。Softmax为多类别问题中的每个类别分配概率，这些概率的总和必须为<em class="ms"> 1.0 </em>。因此，一旦我们获得这些概率，我们就使用概率最高的标签作为与样本相关的最有可能的标签。</p><p id="f006" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">此外，我使用了<em class="ms"> 10 </em>纪元，并耐心地提前停止了<em class="ms"> 2 </em>纪元。<em class="ms">解释和细节可以在本节开头链接的博客中找到。</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og of l"/></div></figure><p id="6ba3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在训练时，分类器，基于确认损失选择的最佳模型，在第六个时期。下面是模型的训练和验证损失曲线。请注意，在最后一个时期，训练损失是最低的，而验证损失是均匀的，约为0.6。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在<a class="ae ky" href="https://chart-studio.plotly.com/" rel="noopener ugc nofollow" target="_blank"> Plotly Chart Studio </a>上绘制的图形</p></figure><p id="fda6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">型号性能:</strong></p><p id="fe23" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了评估模型性能，我从检查点重新加载了它-</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og of l"/></div></figure><p id="0254" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后，我用它来获得对<strong class="lt iu">训练集</strong>的预测，如下面的代码所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og of l"/></div></figure><p id="0942" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">并且在<strong class="lt iu">测试集</strong>上显示如下代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og of l"/></div></figure><p id="1eb1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后，我使用SkLearn的分类报告生成分类指标，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og of l"/></div></figure><p id="5258" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，让我们完成剩下的程序，在最后比较和评估这两种方法。</p><h2 id="45b9" class="oh la it bd lb oi oj dn lf ok ol dp lj ma om on ll me oo op ln mi oq or lp os bi translated"><strong class="ak">二。无Word2Vec嵌入初始化的模型训练</strong></h2><p id="d1de" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我使用Keras tokenizer和使用Keras嵌入层进行了相同的分类器训练</p><p id="60bd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">完整的代码如下:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og of l"/></div></figure><p id="e24a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在模型训练之后，这是训练和验证损失的样子:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在<a class="ae ky" href="https://chart-studio.plotly.com/" rel="noopener ugc nofollow" target="_blank"> Plotly Chart Studio </a>上绘制的图形</p></figure><h1 id="ed16" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">四。评估:</h1><p id="a9b4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">唷！所以你在这里成功了，让我们称之为—</p><p id="374f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">模型1: </strong>模型用 word2vec嵌入来训练<strong class="lt iu"/></p><p id="1941" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">模型2: </strong>模型在没有 word2vec嵌入的情况下训练<strong class="lt iu"/></p><h2 id="a518" class="oh la it bd lb oi oj dn lf ok ol dp lj ma om on ll me oo op ln mi oq or lp os bi translated">一、模型性能对比</h2><p id="971a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">产生的分类矩阵中代码和模型性能的基本差异总结如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/68aafc6c02e6bb59b8c9e7f2e52ce1e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CIjfVBL9xl92sXP4HZLTtA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模特表演对比总结|作者图片</p></figure><p id="c794" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">显然，性能没有显著不同，这些结果表明模型2在召回率和F1分数方面更好，而模型1在精确度方面更好。让我们仔细看看…</p><p id="4c93" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">微观、宏观、加权—准确度、精密度或召回—哪一个？</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/343984b09ec6b55350ab0d3f7510a38f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ogQ1FdsTMik46xtdFXUDYw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">两种型号的分类报告|作者图片</p></figure><p id="753f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">想象一下，你正在设计一个用于心理健康咨询的聊天机器人，这些预测被用来检测情绪和做出反应，不识别悲伤或愤怒的人的成本会很高，因为咨询可能会出错，这对每个人都很重要。辨识度越高，服务越好。在这种情况下，对于每一个假阴性，客户都会受到影响，这将影响你作为顾问的服务。因此，在这种情况下(通常在医学用例中)，回忆是重要的，因为它测量了多少预测实际上是正确预测的。</p><p id="8edc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">此外，由于我们的数据集是不平衡的，最合适的评分技术是<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html" rel="noopener ugc nofollow" target="_blank"> <em class="ms">加权</em> </a> <em class="ms">。</em>它考虑到了类的不平衡，并根据每个类的真实实例数计算标准化的指标。</p><p id="a8bd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="ms">因此，似乎模型2只是稍微好一点，请查看下面的报告解释和混淆矩阵比较。</em></p><p id="3f8a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">解读分类报告和混淆矩阵:</strong></p><p id="f142" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是两个混淆矩阵，混淆出现在相似的地方——观察这里的模式。两个模型都成功地预测了<em class="ms">快乐</em>和<em class="ms">悲伤</em>，模型2中的真阳性略多一些。相反，模型1在识别<em class="ms">愤怒</em>、<em class="ms">恐惧</em>和<em class="ms">爱</em>的<em class="ms">真阳性</em>方面表现稍好——这些类别具有较少的训练样本数量。似乎模型1错误地将其他样本归类为<em class="ms">愤怒，</em>比其他人更高。大约78%的<em class="ms">惊喜</em>样本被模型1错误地归类为<em class="ms">愤怒</em>，令人惊讶的是，在六十六个<em class="ms">惊喜</em>样本中只有一个被正确预测。因此，在分类报告中，<em class="ms">愤怒</em>和<em class="ms">惊讶</em>的准确率都很低。<em class="ms">惊喜</em>的精度看似很高，因为没有其他职业被错误地预测为<em class="ms">惊喜。</em>所以，回忆_ <em class="ms">惊喜</em>—1<em class="ms">(TP)</em>/1<em class="ms">(TP)</em>+0<em class="ms">(FP)</em>= 1</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在<a class="ae ky" href="https://chart-studio.plotly.com/" rel="noopener ugc nofollow" target="_blank"> Plotly图表工作室</a>绘制的图表</p></figure><p id="105f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在模型2中，预测正确的预测惊喜样本更多——66个中的42个(<em class="ms">TP</em>)(<em class="ms">所有阳性= TP + FN </em>)。因此，召回率更高，为64%。这个模型也较少与<em class="ms">愤怒</em>混淆，因此整体性能略有提高，因为在这个案例研究中他们是少数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在<a class="ae ky" href="https://chart-studio.plotly.com/" rel="noopener ugc nofollow" target="_blank"> Plotly Chart Studio </a>上绘制的图形</p></figure><h2 id="1d90" class="oh la it bd lb oi oj dn lf ok ol dp lj ma om on ll me oo op ln mi oq or lp os bi translated">二。方法比较</h2><p id="3797" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">就分类器的<strong class="lt iu">编程而言，使用word2vec来训练可能在预测时遇到未知词汇的模型稍微复杂一些，而Keras本质上处理词汇外的问题。</strong></p><p id="d8eb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">此外，使用word2vec嵌入时<strong class="lt iu"> <em class="ms">分类器训练时间</em> </strong>更长(也是历元数)，此外不要忘记word2vec模型训练时间。</p><h2 id="b26b" class="oh la it bd lb oi oj dn lf ok ol dp lj ma om on ll me oo op ln mi oq or lp os bi translated">三。Keras嵌入中的相似商</h2><p id="bcf9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本案例研究的最后步骤，我还使用Gensim将模型1和2的Keras嵌入层权重转换为键控矢量格式。所以，我现在有三种类型的词向量-</p><p id="0af1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">1.跳过gram嵌入<br/> 2。训练分类器<br/> 3时获得的更新的跳格嵌入。在没有任何初始嵌入权重的情况下训练分类器时获得的嵌入</p><p id="caac" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">以下是从跳过语法嵌入和两个嵌入层权重获得的各个词汇向量空间中更接近单词“happy”的十个单词的列表:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/ca35b187b59b0a883d89279702bb6961.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l-AII5Fdzg7avLaNktpKew.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">分别从Word2Vec模型、模型1和模型2获得相似性。笔记本中的结果不同，但可以用相同的方式解释|作者图片</p></figure><p id="ea18" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">需要注意的是，<strong class="lt iu">虽然word2vec被设计为捕获给定单词的上下文，但Keras嵌入层只是一个查找层</strong>，它的权重根据它正在解决的任务和传播的错误进行更新。由于它们具有<strong class="lt iu">根本不同的工作原理</strong>，这里的比较不能集中在所获得的相似性的质量上，因此对于没有初始权重的Keras嵌入层来说，与‘happy’最相似的前10个单词看起来没有太大的相关性。然而，在skip-gram模型中学习的嵌入在表示与“happy”一起出现的单词方面更好。</p><p id="f16d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">此外，注意，对于没有word2vec权重初始化的Keras模型，相似性得分在most_similar列表中也很低。由此得出结论，这些嵌入，或者更确切地说是<strong class="lt iu">单词向量</strong>，并没有借鉴单词嵌入的思想，后者采用了一种分布式语义方法来将文本编码为数字。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h2 id="28e2" class="oh la it bd lb oi oj dn lf ok ol dp lj ma om on ll me oo op ln mi oq or lp os bi translated"><strong class="ak">注意事项和未来工作:</strong></h2><ol class=""><li id="d290" class="nl nm it lt b lu lv lx ly ma pa me pb mi pc mm nq nr ns nt bi translated">我已经使用准确性来编译模型和损耗，以便进行监控。由于对于不平衡的数据集来说，准确性是欺骗性的，所以召回率或精确度会更合适。</li><li id="57c7" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">阶级不平衡没有得到解决。</li><li id="e84c" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">我重新运行了多次，模型在不同的执行中略微优于对方。然而，不管好不好，在大多数运行中，模型1的混淆矩阵更加丰富多彩。我认为，这是因为word2vec单词嵌入将更接近的向量分配给上下文中一起出现的单词<strong class="lt iu"> <em class="ms">，而不是类</em> </strong>，这使模型变得混乱。</li></ol></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="aa35" class="kz la it bd lb lc nz le lf lg oa li lj jz ob ka ll kc oc kd ln kf od kg lp lq bi translated">数据集引用:</h1><p id="243b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">[1]萨拉维亚，刘，洪春涛，黄，杨海红，吴，陈永胜(2018)。<a class="ae ky" href="http://dx.doi.org/10.18653/v1/D18-1404" rel="noopener ugc nofollow" target="_blank"> Carer:用于情绪识别的情境化情感表征。</a>载于<em class="ms">2018自然语言处理经验方法会议论文集</em>(第3687-3697页)</p><p id="d7ae" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://huggingface.co/datasets/emotion" rel="noopener ugc nofollow" target="_blank"> <em class="ms">许可证在拥抱面:未知</em> </a> <em class="ms"> | </em> <a class="ae ky" href="https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp/metadata" rel="noopener ugc nofollow" target="_blank"> <em class="ms">许可证在Kaggle: CC BY-SA 4.0 </em> </a></p><h1 id="1cc0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">资源和代码:</h1><p id="6964" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://github.com/royn5618/Medium_Blog_Codes/blob/master/Emotion%20Detection/DataExploration.ipynb" rel="noopener ugc nofollow" target="_blank">数据分析笔记本</a> | <a class="ae ky" href="https://github.com/royn5618/Medium_Blog_Codes/blob/master/Emotion%20Detection/EmotionClassifier.ipynb" rel="noopener ugc nofollow" target="_blank">分类器培训笔记本</a></p><p id="17ee" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">我最喜欢谈论单词嵌入:</strong></p><ol class=""><li id="06e3" class="nl nm it lt b lu mn lx mo ma nn me no mi np mm nq nr ns nt bi translated"><a class="ae ky" href="https://www.youtube.com/watch?v=zFScws0mb7M" rel="noopener ugc nofollow" target="_blank">Robert Meyer——用Doc2Vec和机器学习分类分析用户评论</a>【这个很搞笑……】</li><li id="2de3" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated"><a class="ae ky" href="https://www.youtube.com/watch?v=tAxrlAVw-Tk" rel="noopener ugc nofollow" target="_blank"> Lev Konstantinovskiy —文本相似性与Gensim中的下一代单词嵌入</a></li></ol><p id="3cdf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">有兴趣阅读更多关于提高该数据集模型性能的信息吗？</strong></p><div class="pd pe gp gr pf pg"><a rel="noopener follow" target="_blank" href="/how-i-improved-the-performance-of-a-multiclass-text-classifier-using-kerastune-and-other-basic-data-161a22625009"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd iu gy z fp pl fr fs pm fu fw is bi translated">使用KerasTuner和其他基本数据提高多类文本分类器的性能…</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">探索如何通过整合简单数据和/或NLP技术来提高基于文本的模型的性能…</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">towardsdatascience.com</p></div></div><div class="pp l"><div class="pq l pr ps pt pp pu ks pg"/></div></div></a></div></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><p id="5a9b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> <em class="ms">感谢光临！</em>T15】</strong></p><p id="3cb7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">我的链接:</strong> <a class="ae ky" href="https://medium.com/@nroy0110" rel="noopener">中</a>|<a class="ae ky" href="https://www.linkedin.com/in/nabanita-roy/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>|<a class="ae ky" href="https://github.com/royn5618" rel="noopener ugc nofollow" target="_blank">GitHub</a></p></div></div>    
</body>
</html>