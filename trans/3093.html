<html>
<head>
<title>Custom Named Entity Recognition with BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用BERT进行自定义命名实体识别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/custom-named-entity-recognition-with-bert-cf1fd4510804#2022-07-07">https://towardsdatascience.com/custom-named-entity-recognition-with-bert-cf1fd4510804#2022-07-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/11bbb6638a3de3c227ec359e3d7cad93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*s9l1e0Xwj7gbz7KX"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://unsplash.com/@alex_tsl?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">亚历山德拉</a>在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="0ba9" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">如何使用PyTorch和拥抱脸对文本中的命名实体进行分类</h2></div><h2 id="b62c" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">命名实体识别(NER)</h2><blockquote class="lr ls lt"><p id="2579" class="lu lv lw lx b ly lz kh ma mb mc kk md me mf mg mh mi mj mk ml mm mn mo mp mq ij bi translated">是<a class="ae jd" href="https://en.wikipedia.org/wiki/Information_extraction" rel="noopener ugc nofollow" target="_blank">信息提取</a>的一个子任务，它试图定位<a class="ae jd" href="https://en.wikipedia.org/wiki/Unstructured_data" rel="noopener ugc nofollow" target="_blank">非结构化文本</a>中提到的<a class="ae jd" href="https://en.wikipedia.org/wiki/Named_entity" rel="noopener ugc nofollow" target="_blank">命名实体</a>并将其分类成预定义的类别，如人名、组织、位置、<a class="ae jd" href="https://en.wikipedia.org/wiki/Medical_classification" rel="noopener ugc nofollow" target="_blank">医疗代码</a>、时间表达式、数量、货币值、百分比等。</p></blockquote><figure class="ms mt mu mv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mr"><img src="../Images/0a5a0e2e2e63bcdcfc733623e393744f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-LsB4eJt4tz4d7xWqoFJmw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><h2 id="a699" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">背景:</h2><p id="bd55" class="pw-post-body-paragraph lu lv jg lx b ly mw kh ma mb mx kk md le my mg mh li mz mk ml lm na mo mp mq ij bi translated"><em class="lw">在本文中，我们将使用一些我在之前的</em> <a class="ae jd" href="https://medium.com/@marcellopoliti/feature-extraction-with-bert-for-text-classification-533dde44dc2f" rel="noopener"> <em class="lw">文章中介绍过的概念。</em> </a></p><p id="0e20" class="pw-post-body-paragraph lu lv jg lx b ly lz kh ma mb mc kk md le mf mg mh li mj mk ml lm mn mo mp mq ij bi translated">BERT是一种基于Transformer编码器的语言模型。如果你对《变形金刚》不熟悉，我推荐你阅读<a class="ae jd" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">这篇惊人的文章</a>。</p><div class="ip iq gp gr ir nb"><a href="https://jalammar.github.io/illustrated-transformer/" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd jh gy z fp ng fr fs nh fu fw jf bi translated">图示的变压器</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">讨论:黑客新闻(65分，4条评论)，Reddit r/MachineLearning (29分，3条评论)翻译…</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">jalammar.github.io</p></div></div><div class="nk l"><div class="nl l nm nn no nk np ix nb"/></div></div></a></div><p id="14a2" class="pw-post-body-paragraph lu lv jg lx b ly lz kh ma mb mc kk md le mf mg mh li mj mk ml lm mn mo mp mq ij bi translated"><strong class="lx jh">伯特一言以蔽之</strong>:</p><ul class=""><li id="eb06" class="nq nr jg lx b ly lz mb mc le ns li nt lm nu mq nv nw nx ny bi translated">它将一个或多个句子的嵌入标记作为输入。</li><li id="15cc" class="nq nr jg lx b ly nz mb oa le ob li oc lm od mq nv nw nx ny bi translated">第一个令牌总是一个叫做<strong class="lx jh">【CLS】</strong>的特殊令牌。</li><li id="010b" class="nq nr jg lx b ly nz mb oa le ob li oc lm od mq nv nw nx ny bi translated">句子之间由另一个叫做<strong class="lx jh">【SEP】</strong>的特殊记号分隔。</li><li id="6019" class="nq nr jg lx b ly nz mb oa le ob li oc lm od mq nv nw nx ny bi translated">对于每个令牌，BERT输出一个称为<strong class="lx jh">隐藏状态</strong>的嵌入。</li><li id="4f8c" class="nq nr jg lx b ly nz mb oa le ob li oc lm od mq nv nw nx ny bi translated">伯特接受了<strong class="lx jh">掩蔽语言模型</strong>和<strong class="lx jh">下一句预测</strong>任务的训练。</li></ul><p id="f0aa" class="pw-post-body-paragraph lu lv jg lx b ly lz kh ma mb mc kk md le mf mg mh li mj mk ml lm mn mo mp mq ij bi translated">在<strong class="lx jh">屏蔽语言模型(MLM) </strong>中，一个输入单词(或标记)被屏蔽，伯特必须试图找出被屏蔽的单词是什么。对于<strong class="lx jh">下一个句子预测(NSP) </strong>任务，伯特的输入中给出了两个句子，他必须弄清楚第二个句子是否在语义上跟随第一个句子。</p><p id="a19e" class="pw-post-body-paragraph lu lv jg lx b ly lz kh ma mb mc kk md le mf mg mh li mj mk ml lm mn mo mp mq ij bi translated">你想想看，解决命名实体识别任务意味着<strong class="lx jh">用标签(人，位置，..).因此，完成这项任务最直观的方式是<strong class="lx jh">获取每个令牌的相应隐藏状态，并通过一个分类层</strong>将其输入。最终的分类器共享权重，因此实际上我们只有一个分类器，但对于演示目的，我认为更容易将它想象成有更多的分类器。</strong></p><figure class="ms mt mu mv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oe"><img src="../Images/f75697ea5b01dc66420096545d0c5724.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fkVi7K7TYnnwwCFzfyjk-g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><p id="273e" class="pw-post-body-paragraph lu lv jg lx b ly lz kh ma mb mc kk md le mf mg mh li mj mk ml lm mn mo mp mq ij bi translated">从上面的图片中你可以看到我们将使用一个叫做<strong class="lx jh">蒸馏伯特</strong>的伯特的轻量级版本。这个经过提炼的模型比原来的小了40%,但是在各种NLP任务上仍然保持了大约97%的性能。<br/>你可以注意到的另一件事是，BERT的输入不是原始单词而是令牌。BERT已经关联了一个预处理文本的标记器，以便它对模型有吸引力。分词器通常将单词拆分成子词，此外还会添加特殊的记号:<strong class="lx jh">【CLS】</strong>表示句子的开始，<strong class="lx jh">【SEP】</strong>分隔多个句子，以及<strong class="lx jh">【PAD】</strong>使每个句子具有相同数量的记号。</p><p id="1316" class="pw-post-body-paragraph lu lv jg lx b ly lz kh ma mb mc kk md le mf mg mh li mj mk ml lm mn mo mp mq ij bi translated">此外，每个标记嵌入与一个嵌入的<strong class="lx jh">句子相加，嵌入</strong>的句子是一个向量，该向量以某种方式添加了<strong class="lx jh">信息，即标记是指作为输入给BERT的第一个还是第二个句子</strong>。<br/>由于与递归神经网络不同，变压器模型中的计算是并行的，因此我们失去了时间维度，即辨别句子的第一个单词和第二个单词的能力等。<br/>因此，每个标记还被加和到一个<strong class="lx jh">位置嵌入中，该嵌入考虑了标记在序列中的位置</strong></p><p id="f1b3" class="pw-post-body-paragraph lu lv jg lx b ly lz kh ma mb mc kk md le mf mg mh li mj mk ml lm mn mo mp mq ij bi translated">如果你想了解更多关于BERT或他的<strong class="lx jh">单词标记器</strong>的信息，请查看以下资源:</p><div class="ip iq gp gr ir nb"><a href="https://jalammar.github.io/illustrated-bert/" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd jh gy z fp ng fr fs nh fu fw jf bi translated">有插图的伯特、埃尔莫等人(NLP如何破解迁移学习)</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">讨论:黑客新闻(98分，19条评论)，Reddit r/MachineLearning (164分，20条评论)翻译…</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">jalammar.github.io</p></div></div></div></a></div><div class="ip iq gp gr ir nb"><a href="https://huggingface.co/blog/bert-101" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd jh gy z fp ng fr fs nh fu fw jf bi translated">BERT 101 —最新的自然语言处理模型解释</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">BERT是来自变压器的双向编码器表示的缩写，是用于自然语言的机器学习(ML)模型</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">huggingface.co</p></div></div><div class="nk l"><div class="of l nm nn no nk np ix nb"/></div></div></a></div><div class="ip iq gp gr ir nb"><a rel="noopener follow" target="_blank" href="/wordpiece-subword-based-tokenization-algorithm-1fbd14394ed7"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd jh gy z fp ng fr fs nh fu fw jf bi translated">基于子词的标记化算法</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">了解最新的自然语言处理模型使用的基于子词的标记化算法</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">towardsdatascience.com</p></div></div><div class="nk l"><div class="og l nm nn no nk np ix nb"/></div></div></a></div><div class="ip iq gp gr ir nb"><a href="https://huggingface.co/docs/transformers/tokenizer_summary" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd jh gy z fp ng fr fs nh fu fw jf bi translated">标记化器概述</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">在这一页上，我们将更深入地了解标记化。正如我们在预处理教程中看到的，标记文本是…</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">huggingface.co</p></div></div><div class="nk l"><div class="oh l nm nn no nk np ix nb"/></div></div></a></div><h1 id="72e1" class="oi kw jg bd kx oj ok ol la om on oo ld km op kn lh kp oq kq ll ks or kt lp os bi translated">我们来编码吧！</h1><h1 id="7bba" class="oi kw jg bd kx oj ok ol la om on oo ld km op kn lh kp oq kq ll ks or kt lp os bi translated">资料组</h1><p id="29fc" class="pw-post-body-paragraph lu lv jg lx b ly mw kh ma mb mx kk md le my mg mh li mz mk ml lm na mo mp mq ij bi translated">我们要用的数据集叫做<a class="ae jd" href="https://www.kaggle.com/datasets/rajnathpatel/ner-data" rel="noopener ugc nofollow" target="_blank"><em class="lw">CoNLL-2003</em></a><em class="lw"/>，你可以在Kaggle上找到(带开放许可证)。</p><p id="e4d4" class="pw-post-body-paragraph lu lv jg lx b ly lz kh ma mb mc kk md le mf mg mh li mj mk ml lm mn mo mp mq ij bi translated">直接从Colab下载Kaggle数据集(记得上传你的个人Kaggle密钥)。</p><figure class="ms mt mu mv gt is"><div class="bz fp l di"><div class="ot ou l"/></div></figure><h2 id="d687" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">进口</h2><figure class="ms mt mu mv gt is"><div class="bz fp l di"><div class="ot ou l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">进口</p></figure><h2 id="60f5" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">分割数据集</h2><p id="8bbd" class="pw-post-body-paragraph lu lv jg lx b ly mw kh ma mb mx kk md le my mg mh li mz mk ml lm na mo mp mq ij bi translated">让我们加载数据帧的前N行，并更改列名。然后将数据帧分成训练集、开发集和测试集。</p><figure class="ms mt mu mv gt is"><div class="bz fp l di"><div class="ot ou l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">分割数据</p></figure><h1 id="2458" class="oi kw jg bd kx oj ok ol la om on oo ld km op kn lh kp oq kq ll ks or kt lp os bi translated">自定义类别</h1><p id="5510" class="pw-post-body-paragraph lu lv jg lx b ly mw kh ma mb mx kk md le my mg mh li mz mk ml lm na mo mp mq ij bi translated">我现在要定义这个项目需要的三个类。第一个类定义了我们的蒸馏模型。不需要在预训练的语言模型之上手动构建分类器，因为HuggingFace已经为我们提供了一个内置的模型，其中包含作为最后一层的分类器。这个型号叫做<strong class="lx jh"><em class="lw">DistilBertForTokenClassification</em></strong>。</p><h2 id="50eb" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">蒸馏器类</h2><p id="c26f" class="pw-post-body-paragraph lu lv jg lx b ly mw kh ma mb mx kk md le my mg mh li mz mk ml lm na mo mp mq ij bi translated">init方法将输入分类的维度，即我们可以预测的令牌数，并实例化预训练的模型。</p><p id="74fe" class="pw-post-body-paragraph lu lv jg lx b ly lz kh ma mb mc kk md le mf mg mh li mj mk ml lm mn mo mp mq ij bi translated">向前计算简单地采用<strong class="lx jh"> <em class="lw">输入_ id</em></strong>(令牌)和<strong class="lx jh"> <em class="lw">注意_屏蔽</em> </strong>(告诉我们令牌是否是填充符的0/1数组)<strong class="lx jh"> <em class="lw"> </em> </strong>和<strong class="lx jh"> <em class="lw"> </em> </strong>在输出字典中返回:{loss，logits}</p><figure class="ms mt mu mv gt is"><div class="bz fp l di"><div class="ot ou l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">模型类</p></figure><h2 id="a639" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">NerDataset类</h2><p id="7868" class="pw-post-body-paragraph lu lv jg lx b ly mw kh ma mb mx kk md le my mg mh li mz mk ml lm na mo mp mq ij bi translated">第二类是模块<em class="lw"> nn的扩展。数据集</em>，它使我们能够创建自定义数据集。</p><p id="b3fc" class="pw-post-body-paragraph lu lv jg lx b ly lz kh ma mb mc kk md le mf mg mh li mj mk ml lm mn mo mp mq ij bi translated">给定输入数据帧，该方法将对文本进行标记化，并将额外生成的标记与正确的标记进行匹配(如我们在<a class="ae jd" href="https://gist.github.com/March-08/1bff63505282bdd0f108109e5344e499" rel="noopener ugc nofollow" target="_blank">上一篇文章</a>中所述)。现在您可以索引数据集，它将返回一批文本和标签。</p><figure class="ms mt mu mv gt is"><div class="bz fp l di"><div class="ot ou l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">自定义数据集</p></figure><h2 id="dce6" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">度量跟踪类</h2><p id="9f29" class="pw-post-body-paragraph lu lv jg lx b ly mw kh ma mb mx kk md le my mg mh li mz mk ml lm na mo mp mq ij bi translated">有时计算火车循环中的所有指标很烦人，这就是为什么这个类帮助我们做到这一点。您只需要实例化一个新对象，并在每次对批处理进行预测时调用update方法。</p><figure class="ms mt mu mv gt is"><div class="bz fp l di"><div class="ot ou l"/></div></figure><h1 id="2365" class="oi kw jg bd kx oj ok ol la om on oo ld km op kn lh kp oq kq ll ks or kt lp os bi translated">自定义方法</h1><ul class=""><li id="e5fc" class="nq nr jg lx b ly mw mb mx le ov li ow lm ox mq nv nw nx ny bi translated"><strong class="lx jh"> tags_2_labels </strong>:获取标签列表和将标签映射到标签的字典的方法，并返回与原始标签关联的标签列表。</li></ul><figure class="ms mt mu mv gt is"><div class="bz fp l di"><div class="ot ou l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">标签对标签</p></figure><ul class=""><li id="13be" class="nq nr jg lx b ly lz mb mc le ns li nt lm nu mq nv nw nx ny bi translated"><strong class="lx jh"> tags_mapping </strong>:取输入一个数据帧的<em class="lw"> tags </em>列，返回:<strong class="lx jh"> (1) </strong>一个将标签映射到索引(标签)的字典<strong class="lx jh"> (2) </strong>将索引映射到标签的字典<strong class="lx jh"> (3) </strong>标签对应的标签<em class="lw">O</em><strong class="lx jh">(4)</strong><em class="lw"/>一组在训练数据中遇到的唯一标签，这些标签将定义分类器的维数。</li></ul><figure class="ms mt mu mv gt is"><div class="bz fp l di"><div class="ot ou l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">标签映射</p></figure><ul class=""><li id="6d22" class="nq nr jg lx b ly lz mb mc le ns li nt lm nu mq nv nw nx ny bi translated"><strong class="lx jh"> match_tokens_labels </strong>:从标记化的文本和原始标记(与单词而非标记相关联)中，它为每个单独的标记输出一个标记数组。它将一个标记与其原始单词的标签相关联。</li></ul><figure class="ms mt mu mv gt is"><div class="bz fp l di"><div class="ot ou l"/></div></figure><ul class=""><li id="f9af" class="nq nr jg lx b ly lz mb mc le ns li nt lm nu mq nv nw nx ny bi translated"><strong class="lx jh"> freeze_model </strong>:冻结模型的最后几层，防止灾难性遗忘。</li></ul><figure class="ms mt mu mv gt is"><div class="bz fp l di"><div class="ot ou l"/></div></figure><ul class=""><li id="50d9" class="nq nr jg lx b ly lz mb mc le ns li nt lm nu mq nv nw nx ny bi translated"><strong class="lx jh">列车_环线</strong>:通常的列车环线。</li></ul><figure class="ms mt mu mv gt is"><div class="bz fp l di"><div class="ot ou l"/></div></figure><h1 id="b6c4" class="oi kw jg bd kx oj ok ol la om on oo ld km op kn lh kp oq kq ll ks or kt lp os bi translated">主要的</h1><p id="aba8" class="pw-post-body-paragraph lu lv jg lx b ly mw kh ma mb mx kk md le my mg mh li mz mk ml lm na mo mp mq ij bi translated">现在让我们使用主作用域中的所有内容。</p><figure class="ms mt mu mv gt is"><div class="bz fp l di"><div class="ot ou l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">创建映射标记-标签</p></figure><figure class="ms mt mu mv gt is"><div class="bz fp l di"><div class="ot ou l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">标记文本</p></figure><figure class="ms mt mu mv gt is"><div class="bz fp l di"><div class="ot ou l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">火车模型</p></figure><p id="9dfa" class="pw-post-body-paragraph lu lv jg lx b ly lz kh ma mb mc kk md le mf mg mh li mj mk ml lm mn mo mp mq ij bi translated">现在，您应该会得到与这些类似的结果</p><figure class="ms mt mu mv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oy"><img src="../Images/6ee98be4e7a76be3746ac08776993d51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2zr9CDveb1SFaUAClzeWsA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">结果</p></figure><p id="1421" class="pw-post-body-paragraph lu lv jg lx b ly lz kh ma mb mc kk md le mf mg mh li mj mk ml lm mn mo mp mq ij bi translated">恭喜您，您基于预先训练的语言模型构建了您的命名实体识别模型！</p><h1 id="cc03" class="oi kw jg bd kx oj ok ol la om on oo ld km op kn lh kp oq kq ll ks or kt lp os bi translated">最后的想法</h1><p id="2f02" class="pw-post-body-paragraph lu lv jg lx b ly mw kh ma mb mx kk md le my mg mh li mz mk ml lm na mo mp mq ij bi translated">在这篇实践文章中，我们看到了如何利用预训练模型的功能来创建一个简单的模型，从而在很短的时间内解决命名实体识别难题。请记住，当您根据新数据训练整个模型时，您可能会过多地更改原始DistilBert权重，从而降低模型的性能。为此，您可以决定冻结除最后一层(分类层)之外的所有层，并防止出现名为<a class="ae jd" href="https://en.wikipedia.org/wiki/Catastrophic_interference" rel="noopener ugc nofollow" target="_blank"> <strong class="lx jh">精神错乱遗忘</strong> </a>的问题。在模型选择阶段，你可以通过冻结最后的k层来尝试不同的模型。</p><h1 id="f295" class="oi kw jg bd kx oj ok ol la om on oo ld km op kn lh kp oq kq ll ks or kt lp os bi translated">结束了</h1><p id="e0cd" class="pw-post-body-paragraph lu lv jg lx b ly mw kh ma mb mx kk md le my mg mh li mz mk ml lm na mo mp mq ij bi translated">马赛洛·波利蒂</p><p id="f548" class="pw-post-body-paragraph lu lv jg lx b ly lz kh ma mb mc kk md le mf mg mh li mj mk ml lm mn mo mp mq ij bi translated"><a class="ae jd" href="https://www.linkedin.com/in/marcello-politi/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>，<a class="ae jd" href="https://twitter.com/_March08_" rel="noopener ugc nofollow" target="_blank"> Twitter </a>，<a class="ae jd" href="https://march-08.github.io/digital-cv/" rel="noopener ugc nofollow" target="_blank"> CV </a></p></div></div>    
</body>
</html>