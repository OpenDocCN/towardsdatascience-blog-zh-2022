<html>
<head>
<title>Introduction to Physics-informed Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">物理学信息神经网络导论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/solving-differential-equations-with-neural-networks-afdcf7b8bcc4#2022-05-01">https://towardsdatascience.com/solving-differential-equations-with-neural-networks-afdcf7b8bcc4#2022-05-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b077" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">PyTorch实践教程</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1d856779a54d155d8f9a4b7f24b103a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yLngW74OHJZNmbFa"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@djmalecki?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">达维德·马泽基</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="8526" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在过去的几十年里，人工神经网络已经被用于解决各种应用领域的问题，例如计算机视觉、自然语言处理等等。最近，在科学机器学习(ML)社区中出现了另一个非常有前途的应用:使用人工神经网络解决偏微分方程(PDEs)，使用通常被称为<strong class="ky ir">物理信息神经网络</strong> (PINNs)的方法。PINNs最初是在开创性的工作[1]中引入的，今天它们不再局限于一个纯粹的研究主题，而是在行业中也获得了牵引力，足以进入2021年著名的Gartner新兴技术炒作周期。</p><p id="7e02" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">偏微分方程在从流体动力学到声学和结构工程的许多工程和基础科学领域中起着至关重要的作用。有限元建模(FEM)方法是工业中普遍采用的标准求解器。尽管FEM方法很受欢迎，但它们也显示出一些局限性，例如它们对于大型工业问题的计算成本(主要是由于所需的网格尺寸)以及利用外部数据源(例如传感器数据)来驱动PDEs解的问题。</p><p id="85dc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文中讨论的PINNs方法被认为是FEM方法的一种有前途的替代方法，可以弥补其中的一些局限性。这种方法与标准的监督式ML截然不同。事实上，它不是纯粹依赖数据，而是使用PDE本身的物理属性来指导训练过程。已知的数据点可以很容易地添加到基于物理的损失函数之上，以加快训练速度。</p><p id="6fe3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章简单介绍了PINNs背后的主要概念，然后展示了如何从头构建一个PINN来求解一个简单的一阶常微分方程。为了构建神经网络，我将使用令人惊叹的PyTorch库。我们开始吧！</p><h1 id="4d78" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">基于物理学的神经网络是如何工作的</h1><p id="a449" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">为了更好地理解PINNs，让我们从选择一个微分方程开始。为了简单起见，在本文中，我们将重点放在<a class="ae kv" href="https://mathworld.wolfram.com/LogisticEquation.html" rel="noopener ugc nofollow" target="_blank">逻辑微分方程</a>上，这是一个著名的一阶常微分方程，用于模拟人口增长:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/f627e5093703ed9a914dc382b9cdeefe.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*YWy_vOHuYU14JaDxjtk0eA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">模拟人口增长的一阶logistic微分方程</p></figure><p id="e906" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，函数<em class="mq"> f(t) </em>表示随时间的人口增长率<em class="mq"> t </em>并且参数<em class="mq"> R </em>产生最大人口增长率，并且它强烈地影响解的形状。为了完全指定该方程的解，需要施加边界条件，例如在<em class="mq"> t = 0 </em>处，例如:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/915598cbad4b246a3de7d8e0e06d504d.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*RZ6Z8m85cRaMvnWEBDbGcQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">logistic方程的边界条件。</p></figure><p id="4e4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管这个方程的解可以很容易地通过分析得出，但它代表了一个简单的游戏场来说明PINNs是如何工作的。下面解释的所有技术都适用于更复杂的常微分方程和偏微分方程。然而，对于更复杂的场景，为了获得良好的收敛性，将需要额外的技巧。</p><p id="16f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PINNs基于NNs的两个基本属性:</p><ul class=""><li id="e854" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated">已经正式证明[2]神经网络是<strong class="ky ir">通用函数逼近器</strong>。因此，一个神经网络，只要它足够深和有表现力，就可以逼近任何函数，因此也可以逼近上述微分方程的解。</li><li id="58ba" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">使用<strong class="ky ir">自动微分</strong> (AD)计算神经网络输出相对于其任何输入(当然还有反向传播期间的模型参数)的导数既简单又便宜。AD实际上是最初让神经网络如此高效和成功的原因。</li></ul><p id="565f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些都是很好的特性，但是我们怎样才能让神经网络真正学习到解决方案呢？PINNs [3，4]背后有一个令人惊讶的简单但非常聪明的想法:我们可以构造NN损失函数，使得当最小化时，PDE自动得到满足。换句话说，最重要的损耗贡献取为微分方程的残差，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/059b576795e67aa00bd3ba14333e3e9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*gnNDehbKLjjBkOb-i-ojRg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">微分方程残差。</p></figure><p id="4dad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中f <em class="mq"> _NN(t) </em>是具有一个输入的神经网络的输出，其导数使用AD计算。很容易看出，如果神经网络的输出符合上述方程，实际上就是在解微分方程。为了计算来自DE残差的实际损耗贡献，需要在方程域中指定一组点(通常称为<strong class="ky ir">同位点</strong>，并评估均方误差(MSE)或其他损耗函数，作为所有选定同位点的平均值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/cbdf92fb32ab01db2a4fef73649739b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*uyUC0m9vPR1VrdkznAqcRQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由一组配置点上平均的微分方程残差给出的损失贡献。</p></figure><p id="4e9c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，仅基于上述残差的损失并不能确保方程有唯一的解。因此，让我们以与上述完全相同的方式将边界条件添加到损失计算中:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/75bebc60d91d1bff4dc95de2c714bc4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*OLFgSyVWL5uOZ0UGhpbNtA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">加入MSE损失的边界条件损失贡献。</p></figure><p id="e1f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，最终损失简单地写道:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/4fabbd5e564366edf63132f4c1fd1d92.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*DPFl1xkNovUyr9s-XnqnoQ.png"/></div></figure><p id="9ec1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在优化过程中，这被最小化，并且NN输出被训练以考虑微分方程和给定的边界条件，从而逼近最终的DE解。</p><p id="25f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PINN框架非常灵活，使用上面介绍的思想，人们可以添加更多的边界条件，包括更复杂的条件，例如对<em class="mq"> f(x) </em>导数的约束，或者使用具有多个输入的NN处理时间相关和多维问题。</p><p id="e0bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们看看如何用PyTorch构建一个简单的神经网络来构建这样的损失函数。</p><h1 id="5855" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">从零开始建设PINN</h1><h2 id="e28e" class="nk lt iq bd lu nl nm dn ly nn no dp mc lf np nq me lj nr ns mg ln nt nu mi nv bi translated">神经网络</h2><p id="04d4" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">PINN的主要成分当然是神经网络本身。在这篇文章中，我们选择了一个基本的神经网络架构，它由一堆带有标准激活函数的线性层组成。由于我们有一个独立变量，即时间<em class="mq"> t </em>，NN应该将一个特征作为输入，并返回一个输出，该输出表示给定当前模型参数的最佳DE解决方案猜测。下面是这种架构的PyTorch实现，其中神经元和隐藏层的数量作为输入(超级)参数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">PyTorch实现了一个简单的PINN架构。</p></figure><p id="3685" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PINNs是一个非常活跃的研究领域，并且已经设计了更加复杂并且通常是针对问题定制的神经网络架构。关于这些架构的讨论超出了这篇介绍性博客的范围。</p><h2 id="9dbc" class="nk lt iq bd lu nl nm dn ly nn no dp mc lf np nq me lj nr ns mg ln nt nu mi nv bi translated">建立损失函数</h2><p id="2186" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">既然我们已经定义了我们的通用函数逼近器，让我们建立损失函数。如前所述，这是由DE残差项和边界条件项组成的，其中DE残差项用作物理意义上的正则化，边界条件项驱动网络在无限可能的解中收敛到所需的解。</p><p id="5fb2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，需要选择一组协同定位点。因为我们正在解决一个非常简单的问题，所以我们可以在时域中选择一个均匀间隔的网格:<code class="fe ny nz oa ob b">t = torch.linspace(0, 1, steps=30, requires_grad=True)</code>或者我们可以在优化器的每次迭代中从时域中随机采样新的同位点。对于更复杂的问题，同位点的选择极其重要，并且会强烈影响结果。</p><p id="9c79" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了计算模型输出及其导数，我们使用了最新的<code class="fe ny nz oa ob b">functorch</code>软件包，该软件包通过将参数从模型本身解耦，使PyTorch模型完全起作用。关于<code class="fe ny nz oa ob b">functorch</code>的全面介绍请参考<a class="ae kv" href="https://wandb.ai/functorch-examples/functorch-examples/reports/Working-with-FuncTorch-An-Introduction--VmlldzoxNzMxNDI1" rel="noopener ugc nofollow" target="_blank">这篇文章</a>。</p><p id="3d08" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PyTorch的函数方法在处理NN输出相对于其输入的(高阶)导数时非常方便，PINNs通常就是这种情况。在下面的代码中，我们首先使用<code class="fe ny nz oa ob b">functorch</code>使上面的模型函数化，然后我们生成正向传递和梯度计算的函数形式。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="2817" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，对<code class="fe ny nz oa ob b">grad</code>的调用可以无限制地组合，从而允许计算关于输入的任意阶导数。使用上面定义的函数，MSE损失可以很容易地计算为每个同位点的DE贡献和边界贡献之和。注意，给定正向传递和衍生的函数性质，损失函数也必须将模型<code class="fe ny nz oa ob b">params</code>作为输入参数:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="bdcd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样！上面定义的自定义损失确保在训练过程之后，NN将逼近所选微分方程的解。现在，让我们看看它是如何工作的。</p><h2 id="5694" class="nk lt iq bd lu nl nm dn ly nn no dp mc lf np nq me lj nr ns mg ln nt nu mi nv bi translated">用PINNs解微分方程</h2><p id="51de" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">PyTorch目前没有提供支持上述API函数的优化器。然而，令人惊叹的PyTorch社区来拯救我们，人们可以使用<code class="fe ny nz oa ob b"><a class="ae kv" href="https://github.com/metaopt/torchopt" rel="noopener ugc nofollow" target="_blank">torchopt</a></code> <a class="ae kv" href="https://github.com/metaopt/torchopt" rel="noopener ugc nofollow" target="_blank">库</a>获得大多数PyTorch优化器的功能版本。这个库的界面非常直观，任何PyTorch用户都会很快熟悉它。我们的功能模型的基本训练循环如下所示。请注意，我们在每次迭代中随机采样解域。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="e7b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看一些结果。我们使用学习率为0.1的Adam优化器，在每个优化时期从域中均匀采样30个训练点。考虑到所选微分方程的简单性，100个历元足以几乎完美地再现最大增长率设置为<em class="mq"> R = 1 </em>的分析结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/bfd49d6a2f36cc169d7d749e90e78f26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*wyMAYk0Bl00-w7eZFHEadA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">用物理信息神经网络方法求解逻辑斯谛方程。还显示了一组随机训练点。</p></figure><p id="74eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的图中，在100个均匀间隔的点上评估解决方案，每个时期的损耗变化(其中y轴是对数标度)如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/65e9b62c25288565db4fc13f6e0ca770.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*C2TWAPR95j3PmhW8lhA98w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用Adam优化器的损失函数值的演变。该图在y轴上有对数刻度。</p></figure><p id="2a01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里我们解决了一个非常简单的一维问题。对于更复杂的方程，收敛就不那么容易了。特别是对于与时间相关的问题，过去几年已经设计了许多有用的技巧，例如将解域分解为使用不同神经网络求解的不同部分，对不同损失贡献进行智能加权以避免收敛到平凡解等等。我将在以后的帖子中介绍其中的一些技巧，敬请关注。</p></div><div class="ab cl od oe hu of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="ij ik il im in"><p id="f234" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章对物理学神经网络进行了简单的高级介绍，这是一种解决(偏)微分方程的有前途的机器学习方法。虽然需要进一步的进展才能使PINNs常规应用于工业问题，但它们是一个真正活跃和令人兴奋的研究领域，代表了标准微分方程求解器的一个有前途的替代方案。</p><p id="15ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇帖子的完整代码可以在<a class="ae kv" href="https://github.com/madagra/basic-pinn" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。如有任何问题或评论，请随时通过<a class="ae kv" href="https://www.linkedin.com/in/mariodagrada/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我。</p><h1 id="ac4d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考资料和进一步阅读</h1><p id="5c18" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">[1] Raissi M，Perdikaris P，Karniadakis GE .<em class="mq">物理学信息神经网络:一种深度学习框架，用于解决涉及非线性偏微分方程的正向和反向问题。</em>计算物理学报(2019)</p><p id="4595" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] Kurt Hornik，Maxwell Stinchcombe和Halbert White，<em class="mq">多层前馈网络是通用逼近器，</em>神经网络<strong class="ky ir"> 2 </strong>，359–366(1989)</p><p id="c6f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3] George Em Karniadakis，Ioannis G. Kevrekidis，，Paris Perdikaris，Wang和刘洋，<em class="mq">物理学通知机器学习</em>，《自然评论物理学》<strong class="ky ir"> 3 </strong>，422–440(2021)</p><p id="ad4b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4]本·莫斯利，<a class="ae kv" href="https://benmoseley.blog/my-research/so-what-is-a-physics-informed-neural-network/" rel="noopener ugc nofollow" target="_blank">那么，什么是物理信息神经网络呢？—本·莫斯利</a></p></div></div>    
</body>
</html>