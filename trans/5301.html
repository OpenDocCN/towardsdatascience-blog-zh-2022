<html>
<head>
<title>Natural Language Process for Judicial Sentences with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 实现司法判决的自然语言处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/natural-language-process-for-judicial-sentences-with-python-73525bfc6b1e#2022-11-28">https://towardsdatascience.com/natural-language-process-for-judicial-sentences-with-python-73525bfc6b1e#2022-11-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/a56a17292cb969acc72d0f9b165270b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0ZnxXKmpqq01kl-Y.jpg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/</a></p></figure><div class=""/><div class=""><h2 id="6cde" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">第 5 部分:用 LDA 进行主题建模</h2></div><p id="9b52" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">主题建模是一种无监督的机器学习方法，其目的是识别一组文档中的趋势并对识别这些趋势的单词进行分组。</p><p id="f744" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最终的目标是每个主题都有一个单词分布。事实上，由于它是一种无监督的技术，我们得出了一般的聚类(如“主题 1”、“主题 2”、“主题 N”)而不是标签，因此我们需要某种“意义”来定义标签(并在进一步的监督任务中使用它们)。也就是说，如果与主题 1 最相关的 3 个词是“医疗”、“保健”和“保险”，则属于该群集的可能标签可能是“卫生系统”。</p><p id="a0d2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为此，我将使用潜在的狄利克雷分配(LDA)，这是一种主题建模技术，它依赖于这样的假设，即每个文档中的每个单词都来自一个主题，并且该主题是从主题的每个文档分布中选择的。更具体地说，我们所讨论的文档中主题的分布和主题中单词的分布就是<a class="ae jg" href="https://builtin.com/data-science/dirichlet-distribution" rel="noopener ugc nofollow" target="_blank">狄利克雷分布。</a></p><p id="69a8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们用 Python 初始化模型:</p><pre class="lu lv lw lx gt ly lz ma bn mb mc bi"><span id="8ff5" class="md me jj lz b be mf mg l mh mi">df_factor = pd.read_pickle('data/df_factor.pkl')<br/><br/>#this time I will use Lemmas as input<br/><br/>df_factor["Lemmas_string"] = df_factor.Lemmas.apply(str)<br/>instances = df_factor.Lemmas.apply(str.split) <br/><br/>dictionary = Dictionary(instances) <br/><br/>dictionary.filter_extremes(no_below=100, no_above=0.1) #this filter_extreme is keeping the dictionaries either happening<br/>                                                        #less than 100 times or more than 10% of the data<br/><br/>#initializing the lda corpus<br/><br/>ldacorpus = [dictionary.doc2bow(text) for text in instances]<br/>tfidfmodel = TfidfModel(ldacorpus)<br/><br/>model_corpus = tfidfmodel[ldacorpus]<br/><br/>len(model_corpus)<br/><br/>Output: 13087</span></pre><p id="dbef" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在出现了一个问题:我如何决定需要多少主题来聚类我的文档集？</p><p id="601c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个想法是，我们希望一个主题中的单词彼此连贯，所以我们评估主题最佳数量的方法是给每个分段分配一个所谓的<a class="ae jg" href="https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/" rel="noopener ugc nofollow" target="_blank">连贯分数</a>。</p><p id="2925" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有几种方法可以计算一致性分数，在本文中，有两个指标我可以决定:</p><ul class=""><li id="08b6" class="mj mk jj la b lb lc le lf lh ml ll mm lp mn lt mo mp mq mr bi translated">CV Coherence score →它基于标准化的点态互信息(你可以在这里阅读更多关于 PMI <a class="ae jg" href="https://medium.com/dataseries/understanding-pointwise-mutual-information-in-nlp-e4ef75ecb57a" rel="noopener">的信息</a></li><li id="288e" class="mj mk jj la b lb ms le mt lh mu ll mv lp mw lt mo mp mq mr bi translated">UMass Coherence score →它衡量两个单词在文档中同时出现的频率。</li></ul><pre class="lu lv lw lx gt ly lz ma bn mb mc bi"><span id="26c7" class="md me jj lz b be mf mg l mh mi">#these scores are needed to identify the number of topic which is the most consistent.<br/><br/>coherence_values = []<br/><br/>dev_size = 10000<br/>eval_size = len(model_corpus) - dev_size<br/><br/>for num_topics in range(5, 16):<br/>    model = LdaMulticore(corpus=model_corpus[:dev_size], <br/>                         id2word=dictionary, <br/>                         num_topics=num_topics)<br/><br/>    coherencemodel_umass = CoherenceModel(model=model, <br/>                                          texts=instances[dev_size:dev_size+eval_size], <br/>                                          dictionary=dictionary, <br/>                                          coherence='u_mass')<br/><br/>    coherencemodel_cv = CoherenceModel(model=model, <br/>                                       texts=instances[dev_size:dev_size+eval_size], <br/>                                       dictionary=dictionary, <br/>                                       coherence='c_v')<br/><br/>    umass_score = coherencemodel_umass.get_coherence()<br/>    cv_score = coherencemodel_cv.get_coherence()<br/>    <br/>    print(num_topics, umass_score, cv_score)<br/>    coherence_values.append((num_topics, umass_score, cv_score))<br/>    <br/>    <br/>#pickling results<br/>with open('data/coherence_values.pkl', 'wb') as f:<br/>       pickle.dump(coherence_values, f)</span></pre><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/3e20192e31ebdfb2fcd85fa8b5c826d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*KSHRR4F8-mWQgtGsQ014Vw.png"/></div></figure><p id="438d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们想象一下:</p><pre class="lu lv lw lx gt ly lz ma bn mb mc bi"><span id="0433" class="md me jj lz b be mf mg l mh mi">%matplotlib inline<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>sns.set_context('poster') # use large font<br/><br/><br/>scores = pd.DataFrame(coherence_values, columns=['num_topics', 'UMass', 'CV'])<br/>fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))<br/>scores.plot.line(x='num_topics', y='UMass', ax=ax[0], xticks=range(5,21))<br/>scores.plot.line(x='num_topics', y='CV', ax=ax[1], xticks=range(5,21))</span></pre><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi my"><img src="../Images/97a6fe210a3197779a636f625bf90d2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kW3OtMBNUf1e1e7b6-N47w.png"/></div></div></figure><pre class="lu lv lw lx gt ly lz ma bn mb mc bi"><span id="efee" class="md me jj lz b be mf mg l mh mi">coherence_values[3]<br/>(8, -5.123179828224228, 0.30521192070246617)</span></pre><p id="1146" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于两种测量方法——UMass 和 CV——我们都想要最高值。因此，我选择 num_topics = 8，因为它在 cv 和 UMass 上都有很高的分数(后者不是最高的，但排名第二)。</p><pre class="lu lv lw lx gt ly lz ma bn mb mc bi"><span id="d7e0" class="md me jj lz b be mf mg l mh mi"><br/>num_topics = 8 # number of topics<br/><br/># find chunksize to make about 200 updates<br/>num_passes = 10<br/>chunk_size = len(model_corpus) * num_passes/200<br/><br/><br/>model = LdaMulticore(num_topics = num_topics, <br/>                     corpus = model_corpus,  <br/>                     id2word = dictionary, <br/>                     workers = min(10, multiprocessing.cpu_count()-1), <br/>                     passes = num_passes, <br/>                     chunksize = chunk_size, <br/>                     alpha = 0.1<br/>                    )<br/></span></pre><p id="834a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一位训练了模型，让我们看看与每个主题相关的前 5 个词:</p><pre class="lu lv lw lx gt ly lz ma bn mb mc bi"><span id="999d" class="md me jj lz b be mf mg l mh mi">import re<br/>num_topics = 8<br/># transform the data into topic distros<br/>topic_corpus = model[model_corpus]<br/><br/># get the topic descriptors<br/>topic_sep = re.compile(r"0\.[0-9]{3}\*") <br/>model_topics = [(topic_no, re.sub(topic_sep, '', model_topic).split(' + ')) for topic_no, model_topic in<br/>                model.print_topics(num_topics=num_topics, num_words=5)]<br/><br/>descriptors = []<br/>for i, m in model_topics:<br/>    print(i+1, ", ".join(m[:5]))<br/>    descriptors.append(", ".join(m[:2]).replace('"', ''))</span></pre><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/2d132ba42f67bc0cb2187d6b49ea4092.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*aJDxRfPEC9jxAFZSbQUcGw.png"/></div></figure><p id="6c8e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通读这些主题，已经可以看到单词正在引导一个标签。也就是说，主题 8 可以是“气候变化”或“环境污染”。</p><h2 id="a2d2" class="na me jj bd nb nc nd dn ne nf ng dp nh lh ni nj nk ll nl nm nn lp no np nq nr bi translated">基于标记数据的 LDA 主题建模</h2><p id="d11e" class="pw-post-body-paragraph ky kz jj la b lb ns kk ld le nt kn lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">现在，我将在 single_data 数据集上执行 LDA，以便我们也可以可视化跨主题的类别分布。</p><pre class="lu lv lw lx gt ly lz ma bn mb mc bi"><span id="cc39" class="md me jj lz b be mf mg l mh mi">df = pd.read_pickle('data/df.pkl')<br/>df=df.rename(columns={'topic':'category'})<br/><br/><br/>df_factor = pd.read_pickle('data/df_factor.pkl')<br/><br/><br/># This dataset will have a column "category" that will report this category.<br/>categories = list(set([i for  l in df['category'].to_list() for i in l]))<br/><br/>data_single = df_factor.copy()[df_factor[categories].sum(axis = 1) == 1]<br/><br/>#len(categories)<br/># replacing 1 with correspondent category<br/>for column in categories:<br/>    data_single[column] = data_single[column].apply(lambda x: column if x == 1 else "")<br/><br/>data_single["Category"] = data_single[categories].apply(lambda x: "".join(x), axis = 1) # joing labels with whitespace<br/>data_single.head()</span></pre><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/7d162e91ab37dcd3c33b7048f9f274f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/1*MSkidR24sXx4WM5vFUP1wQ.gif"/></div></figure><pre class="lu lv lw lx gt ly lz ma bn mb mc bi"><span id="a1c6" class="md me jj lz b be mf mg l mh mi">#repeating the same steps as before, but this time using a shrunken version of the <br/>#dataset (only those records with 1 label)<br/><br/>data_single["Lemmas_string"] = data_single.Lemmas.apply(str)<br/>instances = data_single.Lemmas.apply(str.split) <br/><br/>dictionary = Dictionary(instances) <br/><br/>dictionary.filter_extremes(no_below=100, no_above=0.1) #this filter_extreme is keeping the dictionaries either happening<br/>                                                        #less than 100 times or more than 10% of the data<br/><br/>ldacorpus = [dictionary.doc2bow(text) for text in instances] #transforms the corpus in IDs<br/>tfidfmodel = TfidfModel(ldacorpus) #computing the tdidf of the corpus based on the document <br/>model_corpus = tfidfmodel[ldacorpus]</span></pre><p id="ea81" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们训练模型:</p><pre class="lu lv lw lx gt ly lz ma bn mb mc bi"><span id="771c" class="md me jj lz b be mf mg l mh mi">num_topics = 8 #derived from the coherence scores<br/><br/><br/>num_passes = 10<br/>chunk_size = len(model_corpus) * num_passes/200<br/><br/><br/>start = time.time() <br/><br/>model = LdaMulticore(num_topics=num_topics, <br/>                     corpus = model_corpus, <br/>                     id2word=dictionary, <br/>                     workers=min(10, multiprocessing.cpu_count()-1), <br/>                     passes=num_passes, <br/>                     chunksize=chunk_size, <br/>                     alpha = 0.5 <br/>                    )<br/></span></pre><p id="86f7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们再看一遍与主题相关的单词:</p><pre class="lu lv lw lx gt ly lz ma bn mb mc bi"><span id="7287" class="md me jj lz b be mf mg l mh mi">topic_corpus = model[model_corpus]<br/>#get the topic descriptors<br/>topic_sep = re.compile(r"0\.[0-9]{3}\*") <br/>model_topics = [(topic_no, re.sub(topic_sep, '', model_topic).split(' + ')) for topic_no, model_topic in<br/>                model.print_topics(num_topics=num_topics, num_words=5)]<br/><br/>descriptors = []<br/>for i, m in model_topics:<br/>    print(i+1, ", ".join(m[:5]))<br/>    descriptors.append(", ".join(m[:2]).replace('"', ''))</span></pre><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/b81ec329009ad106911e5879f69054a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*9SYXXVZrvnKq1WgTjqCmYw.png"/></div></figure><pre class="lu lv lw lx gt ly lz ma bn mb mc bi"><span id="15e2" class="md me jj lz b be mf mg l mh mi"># get a list of all the topic scores for each document<br/>scores = [[t[1] for t in topic_corpus[entry]] for entry in range(len(topic_corpus))]<br/>topic_distros = pd.DataFrame(data=scores, columns=descriptors)<br/>topic_distros["Category"] = data_single.Category</span></pre><p id="2249" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，让我们看一下主题在不同类别中的分布情况:</p><pre class="lu lv lw lx gt ly lz ma bn mb mc bi"><span id="e0dc" class="md me jj lz b be mf mg l mh mi">import matplotlib.pyplot as plt # make graphs<br/>import seaborn as sns # make prettier graphs<br/><br/>sns.set_context('poster') # use large font<br/><br/>fig, ax = plt.subplots(figsize=(50, 20)) #set graph size<br/>#aggregate topics by categories<br/>aggregate_by_category = topic_distros.groupby(topic_distros.Category).mean()<br/>#plot the graph<br/>aggregate_by_category[descriptors].plot.bar(ax=ax)<br/>#to move the legend out<br/>#plt.legend(loc='bottom left', bbox_to_anchor=(1.0, 0.5))</span></pre><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi my"><img src="../Images/a2a67c877a4956d9058569d6b234e94f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dd2YCmorzwwR9oLp_2K1Bw.png"/></div></div></figure><h2 id="23b5" class="na me jj bd nb nc nd dn ne nf ng dp nh lh ni nj nk ll nl nm nn lp no np nq nr bi translated">结论</h2><p id="448a" class="pw-post-body-paragraph ky kz jj la b lb ns kk ld le nt kn lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">对我们的司法判决运行 LDA 可以降低搜索和检查趋势的复杂性。事实上，我们有 42 个类别(参见<a class="ae jg" href="https://medium.com/towards-data-science/natural-language-process-for-judicial-sentences-with-python-part-2-964b0e12dd4a" rel="noopener">第 2 部分</a>了解数据集的描述性统计数据)和仅仅 8 个主题。我们还可以看到，有一些主题(如主题 3 和 5)在所有类别中不断出现，这意味着，无论司法判决的类别如何，这些判决的内容都可能涵盖这些主题。</p><p id="36b8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在下一篇文章中，我们将使用我们的文档集来执行文档嵌入，这是一种矢量化技术，允许我们的语料库成为 ML 模型的输入。所以敬请期待下一部分！</p><h1 id="e3d5" class="ny me jj bd nb nz oa ob ne oc od oe nh kp of kq nk ks og kt nn kv oh kw nq oi bi translated">参考</h1><ul class=""><li id="4a65" class="mj mk jj la b lb ns le nt lh oj ll ok lp ol lt mo mp mq mr bi translated"><a class="ae jg" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> NLTK::自然语言工具包</a></li><li id="80ad" class="mj mk jj la b lb ms le mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><a class="ae jg" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank">Python 中的 spaCy 工业级自然语言处理</a></li><li id="e4b5" class="mj mk jj la b lb ms le mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><a class="ae jg" href="https://www.justice.gov/news" rel="noopener ugc nofollow" target="_blank">司法新闻| DOJ |司法部</a></li><li id="2b20" class="mj mk jj la b lb ms le mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><a class="ae jg" href="https://www.kaggle.com/datasets/jbencina/department-of-justice-20092018-press-releases" rel="noopener ugc nofollow" target="_blank">司法部 2009–2018 年新闻发布| Kaggle </a></li><li id="c374" class="mj mk jj la b lb ms le mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><a class="ae jg" href="https://en.wikipedia.org/wiki/Distributional_semantics" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Distributional_semantics</a></li><li id="f381" class="mj mk jj la b lb ms le mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><a class="ae jg" href="https://aurelieherbelot.net/research/distributional-semantics-intro/" rel="noopener ugc nofollow" target="_blank">https://aurelieherbelot . net/research/distributional-semantics-intro/</a></li><li id="16b1" class="mj mk jj la b lb ms le mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><a class="ae jg" href="https://builtin.com/data-science/dirichlet-distribution" rel="noopener ugc nofollow" target="_blank">https://builtin.com/data-science/dirichlet-distribution</a></li><li id="0868" class="mj mk jj la b lb ms le mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><a class="ae jg" href="http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf" rel="noopener ugc nofollow" target="_blank">http://SVN . aksw . org/papers/2015/WSDM _ Topic _ Evaluation/public . pdf</a></li></ul></div></div>    
</body>
</html>