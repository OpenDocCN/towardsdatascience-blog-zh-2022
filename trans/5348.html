<html>
<head>
<title>Maximum Likelihood Estimation from scratch in TensorFlow Probability</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量流概率的最大似然估计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2#2022-11-30">https://towardsdatascience.com/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2#2022-11-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f7bb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">概率深度学习</h2></div><h1 id="784f" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">介绍</h1><p id="f8cf" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">本文属于“概率深度学习”系列。这个每周系列涵盖了深度学习的概率方法。主要目标是扩展深度学习模型，以量化不确定性，即知道他们不知道的东西。</p><p id="6866" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们使用张量流和张量流概率开发我们的模型。TensorFlow Probability 是一个构建在 TensorFlow 之上的 Python 库。我们将从能在张量流概率中找到的基本对象开始，并理解我们如何操纵它们。我们将在接下来的几周内逐步增加复杂性，并将我们的概率模型与现代硬件(如 GPU)上的深度学习相结合。</p><p id="e9ee" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">迄今发表的文章:</p><ol class=""><li id="c91b" class="mb mc it lc b ld lw lg lx lj md ln me lr mf lv mg mh mi mj bi translated"><a class="ae mk" href="https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1" rel="noopener">张量流概率简介:分布对象</a></li><li id="550e" class="mb mc it lc b ld ml lg mm lj mn ln mo lr mp lv mg mh mi mj bi translated"><a class="ae mk" href="https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15" rel="noopener">张量流概率简介:可训练参数</a></li><li id="6e85" class="mb mc it lc b ld ml lg mm lj mn ln mo lr mp lv mg mh mi mj bi translated"><a class="ae mk" rel="noopener" target="_blank" href="/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2">张量流概率中从零开始的最大似然估计</a></li><li id="6252" class="mb mc it lc b ld ml lg mm lj mn ln mo lr mp lv mg mh mi mj bi translated"><a class="ae mk" rel="noopener" target="_blank" href="/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00">tensor flow 中从头开始的概率线性回归</a></li><li id="2733" class="mb mc it lc b ld ml lg mm lj mn ln mo lr mp lv mg mh mi mj bi translated"><a class="ae mk" href="https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef" rel="noopener">使用 Tensorflow 进行概率回归与确定性回归</a></li><li id="5a02" class="mb mc it lc b ld ml lg mm lj mn ln mo lr mp lv mg mh mi mj bi translated"><a class="ae mk" href="https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5" rel="noopener"> Frequentist 与 Tensorflow 的贝叶斯统计</a></li></ol><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/009e5fa2c6478486d7667c75fd722bca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MQ75E2_BlPFeEPnE"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图 1:我们今天的口头禅:最大化对数似然和最小化负对数似然是一样的(<a class="ae mk" href="https://unsplash.com/photos/Rz5o0osnN6Q" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="e676" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">像往常一样，代码可以在我的<a class="ae mk" href="https://github.com/luisroque/probabilistic_deep_learning_with_TFP" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p><h1 id="ad12" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">最大似然估计</h1><p id="76c5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们回忆一下上一篇文章最后分享的关于<em class="ng">最大似然估计</em>的内容。</p><p id="f61c" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><em class="ng">最大似然估计</em>是深度学习模型中常用的训练程序。目标是在给定一些数据的情况下，估计概率分布的参数。简而言之，我们希望最大化我们在一些假设的统计模型下观察到的数据的概率，即概率分布。</p><p id="87b9" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们还引入了一些符号。连续随机变量的概率密度函数粗略地表示样本取特定值的概率。我们将这个函数表示为<em class="ng"> 𝑃 </em> ( <em class="ng"> 𝑥 </em> | <em class="ng"> 𝜃 </em>)，其中<em class="ng"> 𝑥 </em>是样本值，<em class="ng"> 𝜃 </em>是描述概率分布的参数:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nh"><img src="../Images/968cafb4cef18d13f28a3680f28e7c6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*E1Eo0xoDyQ-6ZI_CNYVTAg.png"/></div></div></figure><pre class="mr ms mt mu gt ni nj nk bn nl nm bi"><span id="07c5" class="nn kj it nj b be no np l nq nr">tfd.Normal(0, 1).prob(0)<br/><br/>&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.3989423&gt;</span></pre><p id="2d44" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">当从同一分布(我们通常假设)中独立地<em class="ng">抽取多个样本时，样本值<em class="ng"> 𝑥 </em> 1，…，<em class="ng"> 𝑥𝑛 </em>的概率密度函数是每个个体<em class="ng"> 𝑥𝑖 </em>的概率密度函数的乘积。正式书写:</em></p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/485e5dd7a73d0dfa16fffca2c297122f.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*JbT3VnTrptTBYNV6Dr2GXA.png"/></div></figure><p id="de86" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们可以通过一个例子很容易地计算出上述内容。假设我们有一个标准的高斯分布和一些样本:<em class="ng">𝑥</em>1 = 0.5，<em class="ng"> 𝑥 </em> 2=0，<em class="ng"> 𝑥 </em> 3=1.5。正如我们上面定义的，我们只需要计算每个样本的概率密度函数，然后将输出相乘。</p><pre class="mr ms mt mu gt ni nj nk bn nl nm bi"><span id="de4f" class="nn kj it nj b be no np l nq nr">X = [-0.5, 0, 1.5]<br/><br/>np.prod(tfd.Normal(0, 1).prob(X))<br/><br/>0.01819123</span></pre><p id="c9ff" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">现在，我想以某种方式给出一些关于概率密度函数和<em class="ng">可能性</em>函数之间差异的直觉。他们本质上是在计算相似的东西，但是观点相反。</p><p id="7f1b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">从概率密度函数开始，我们知道它们是我们样本的函数<em class="ng"> 𝑥 </em> 1，…，<em class="ng"> 𝑥𝑛 </em>。注意参数<em class="ng"> 𝜃 </em>被认为是固定的。所以，当参数<em class="ng"> 𝜃 </em>已知时，使用概率密度函数，我们的兴趣是找出相同样本<em class="ng"> 𝑥 </em> 1，…，<em class="ng"> 𝑥𝑛 </em>的概率。简而言之，当我们知道产生某个过程的分布，并且我们想从中推断出可能的值时，我们就使用这个函数。</p><p id="51c9" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">相反，在<em class="ng">似然</em>函数的情况下，我们已知的是样本，即观察数据<em class="ng"> 𝑥 </em> 1、…、<em class="ng"> 𝑥𝑛 </em>。这意味着我们的独立变量现在是<em class="ng"> 𝜃 </em>，因为我们不知道哪个分布产生了我们观察到的这个过程。因此，当我们知道某个过程的样本时，即我们收集了数据，但我们并不真正知道最初是什么分布产生了该过程时，我们就使用这个函数。既然我们知道这些数据，我们有兴趣对它们来自的分布做出<em class="ng">推论</em>。</p><p id="ac13" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">让我们引入更多的符号来帮助连接这些点。对于<em class="ng">可能性</em>函数，惯例是使用字母<em class="ng"> 𝐿 </em>，而对于概率密度函数，我们引入了上面的符号。我们可以这样写:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/019816ab04dfadc1624d7f1b36fa6823.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*QFERRsBcRN0fIB829bw-fw.png"/></div></figure><p id="188c" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们准备用参数<em class="ng"> 𝜇 </em>和<em class="ng"> 𝜎 </em>定义高斯分布的似然函数:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/4d7f9168f79a6b5a6f09e0ce83fac928.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*DNoyhQwI5bZTJJ_ztX3lGg.png"/></div></figure><p id="601b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">作为一个获得更多关于<em class="ng">可能性</em>函数的直觉的练习，我们可以生成足够的样本来直观地一瞥它的形状。请注意与我们在上一篇文章中计算的概率密度函数相比，计算<em class="ng">可能性</em>函数的不同之处。我们对从概率分布中生成样本不感兴趣，我们感兴趣的是生成使观测数据的概率最大化的参数<em class="ng"> 𝜃 </em>，即<em class="ng"> 𝑃 </em> ( <em class="ng"> 𝑥 </em> 1，…，<em class="ng"> 𝑥𝑛 </em> | <em class="ng"> 𝜃 </em>)。</p><p id="d5d6" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们使用与上述相同的样本:𝑥1 = 0.5，𝑥2 = 0，𝑥3 = 1.5。</p><pre class="mr ms mt mu gt ni nj nk bn nl nm bi"><span id="28bf" class="nn kj it nj b be no np l nq nr">X<br/><br/>[-0.5, 0, 1.5]</span></pre><p id="4764" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">为了能够构建 2D 可视化，我们可以创建在一个间隔上均匀间隔采样的潜在参数的网格，从[-2，2]和[0，3]之间的<em class="ng">𝜎</em>采样<em class="ng">𝜇</em>。由于我们对每个参数的 100 个值进行了采样，我们得到了<em class="ng"> 𝑛 </em> ^2 可能的组合。对于每个参数组合，我们需要计算每个样本的概率，并将它们相乘(按照我们上面分享的过程)。</p><pre class="mr ms mt mu gt ni nj nk bn nl nm bi"><span id="9399" class="nn kj it nj b be no np l nq nr">μ = np.linspace(-2, 2, 100)<br/>σ = np.linspace(0, 3, 100)<br/><br/>l_x = []<br/>for mu in μ:<br/>    for sigma in σ:<br/>        l_x.append(np.prod(tfd.Normal(mu, sigma).prob(X)))<br/>        <br/>l_x = np.asarray(l_x).reshape((100, 100)).T</span></pre><p id="15ec" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们现在准备绘制<em class="ng">可能性</em>函数。注意，它是观察样本的函数，回想一下，这些是固定的，参数是我们的自变量。</p><pre class="mr ms mt mu gt ni nj nk bn nl nm bi"><span id="0891" class="nn kj it nj b be no np l nq nr">plt.contourf(μ, σ, l_x)<br/>plt.xlabel('μ')<br/>plt.ylabel('σ')<br/>plt.colorbar()<br/>plt.title('Likelihood');</span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/ffdbc49eaca1404bc719c72f78325388.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*GPtWH_tVlj1qJ6Jz969wLg.png"/></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图 2: <em class="nw">样本的高斯似然函数𝑥</em>1 = 0.5，<em class="nw"> 𝑥 </em> 2=0，<em class="nw"> 𝑥 </em> 3=1.5</p></figure><p id="bd59" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">正如我们已经分享的，我们对最大化我们数据的概率感兴趣。这意味着我们想要找到<em class="ng">似然</em>函数的最大值，这可以借助微积分来实现。事实上，函数相对于参数的一阶导数的零点应该足以帮助我们找到原函数的最大值。</p><p id="4776" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们现在遇到一个新问题，这个问题我们在之前的文章中已经介绍过了——将许多小概率相乘在一起可能在数值上不稳定。为了克服这个问题，我们可以使用相同函数的对数变换。自然对数是一个单调递增的函数，这意味着如果 x 轴上的值增加，y 轴上的值也会增加。这很重要，因为它确保了概率对数的最大值出现在与原始概率函数相同的点上。它为我们做了另一件非常方便的事情，它将我们的乘积转化为总和。</p><p id="82be" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">让我们执行转换:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/86e79707c37b3d3a4973a5851f9c175a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*qnQh7-5YvEcxgTmQhGh4nQ.png"/></div></figure><p id="9134" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们就快到了，现在我们可以着手解决优化问题了。最大化我们数据的概率可以写成:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi ny"><img src="../Images/3cbc4ba0618dad1efbbe3cb70d97e9b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*wBQdA-2_oDwkvd_GV5crXg.png"/></div></div></figure><p id="05e0" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">如上所述，可以对上面导出的表达式进行求导以找到最大值。扩展我们的参数，我们有 log(<em class="ng">𝐿</em>(<em class="ng">𝑋</em>|<em class="ng">𝜇</em>，<em class="ng"> 𝜎 </em>)。由于它是两个变量<em class="ng"> 𝜇 </em>和<em class="ng">𝜎</em>的函数，我们使用偏导数来找到<em class="ng">最大似然估计</em>。</p><p id="c809" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">让我们专注于<em class="ng"> 𝜇 </em> ̂(帽子表示它是一个估计量，即我们的输出)，我们可以使用以下公式计算它:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/6ce411bf5301ff1f7c9e6187e7a6a305.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*A_Zj55IWssOVcPHPMOH6Bw.png"/></div></figure><p id="c6a1" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">为了找到最大值，我们需要找到临界值，因此我们需要将上面的表达式设置为零。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/96ad282e2849fca169dbb1a9df76d270.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*Qh-KZzLu2tEbovzd9cGoLw.png"/></div></figure><p id="42d2" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">然后，</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/99f4a33462478367f1b382af04b04db9.png" data-original-src="https://miro.medium.com/v2/resize:fit:266/format:webp/1*n4z1K920q8JPVRx3GULqkw.png"/></div></figure><p id="3724" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这是数据的平均值，你应该不会感到惊讶。</p><p id="c1a7" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们可以计算出样本的μ和σ的最大值:𝑥1 = 0.5，𝑥2 = 0，𝑥3 = 1.5，并与真实值进行比较。</p><pre class="mr ms mt mu gt ni nj nk bn nl nm bi"><span id="dd84" class="nn kj it nj b be no np l nq nr">idx_μ_max = np.argmax(l_x, axis=1)[-1]<br/>print(f'μ True Value: {np.array(X).mean()}')<br/>print(f'μ Calculated Value: {μ[idx_μ_max]}')<br/>print(f'σ True Value: {np.array(X).std()}')<br/>print(f'σ Calculated Value: {σ[np.nanargmax(l_x[:,idx_μ_max], axis=0)]}')<br/><br/>μ True Value: 0.3333333333333333<br/>μ Calculated Value: 0.3434343434343434<br/>σ True Value: 0.8498365855987975<br/>σ Calculated Value: 0.8484848484848485</span></pre><h1 id="5766" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">张量流概率的最大似然估计实现</h1><p id="a0ff" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们首先创建一个正态分布的随机变量，并从中抽取样本。通过绘制随机变量的直方图，我们可以看到分布的形状。</p><pre class="mr ms mt mu gt ni nj nk bn nl nm bi"><span id="deb7" class="nn kj it nj b be no np l nq nr">x_train = np.random.normal(loc=1, scale=5, size=1000).astype('float32')[:, np.newaxis]<br/><br/>plt.hist(x_train, bins=50);</span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/fdf65d7082e52effd915b6eaa5782b69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*KmCg7NaQdpAqJMXSZz0Ppg.png"/></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图 3:参数<em class="nw"> 𝜇 </em> =1、<em class="nw"> 𝜎 </em> =5 的正态分布随机变量直方图。</p></figure><p id="72c2" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们可以计算随机变量的平均值，这是我们想要使用<em class="ng">最大似然估计</em>学习的值。</p><pre class="mr ms mt mu gt ni nj nk bn nl nm bi"><span id="099f" class="nn kj it nj b be no np l nq nr">x_train.mean()<br/><br/>0.85486585</span></pre><p id="e5d3" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">正如我们在上一篇文章中看到的，我们可以将 TensorFlow <code class="fe od oe of nj b">Variable</code>对象定义为分布的参数。这向 TensorFlow 发出信号，表明我们希望在学习过程中学习这些参数，无论我们使用哪个参数。</p><pre class="mr ms mt mu gt ni nj nk bn nl nm bi"><span id="8b5a" class="nn kj it nj b be no np l nq nr">normal = tfd.Normal(loc=tf.Variable(0., name='loc'), scale=5)<br/>normal.trainable_variables<br/><br/>(&lt;tf.Variable 'loc:0' shape=() dtype=float32, numpy=0.0&gt;,)</span></pre><p id="be81" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">下一步是定义我们的损失函数。在这种情况下，我们已经看到了我们想要实现的目标——最大化我们的<em class="ng">可能性</em>函数的对数变换。然而，在深度学习中，我们通常会最小化我们的损失函数，如果我们将我们的<em class="ng">可能性</em>函数的符号改为负值，这可以很容易地实现。</p><pre class="mr ms mt mu gt ni nj nk bn nl nm bi"><span id="2cb8" class="nn kj it nj b be no np l nq nr">def nll(x_train):<br/>    return -tf.reduce_sum(normal.log_prob(x_train))</span></pre><p id="bc9b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">最后，我们可以建立我们的培训程序。我们将使用一个定制的训练循环，以便我们可以自己定义过程细节(即使用我们的定制损失函数)。</p><p id="1eeb" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">正如我们在之前的文章中所分享的，我们使用 API<code class="fe od oe of nj b">tf.GradientTape()</code>来访问 TensorFlow 的自动微分特性。接下来，我们简单地指定要训练的变量，最小化和应用梯度的损失函数。</p><pre class="mr ms mt mu gt ni nj nk bn nl nm bi"><span id="56c0" class="nn kj it nj b be no np l nq nr">@tf.function<br/>def get_loss_and_grads(x_train):<br/>    with tf.GradientTape() as tape:<br/>        tape.watch(normal.trainable_variables)<br/>        loss = nll(x_train)<br/>        grads = tape.gradient(loss, normal.trainable_variables)<br/>    return loss, grads<br/><br/>optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)</span></pre><p id="408d" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们已经准备好运行我们的训练程序了。</p><pre class="mr ms mt mu gt ni nj nk bn nl nm bi"><span id="1fb9" class="nn kj it nj b be no np l nq nr">@tf.function<br/>def get_loss_and_grads(x_train):<br/>    with tf.GradientTape() as tape:<br/>        tape.watch(normal.trainable_variables)<br/>        loss = nll(x_train)<br/>        grads = tape.gradient(loss, normal.trainable_variables)<br/>    return loss, grads<br/><br/>optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)<br/><br/>Step 000: Loss: 13768.004 Loc: 0.855<br/>Step 001: Loss: 13768.004 Loc: 0.855<br/>Step 002: Loss: 13768.004 Loc: 0.855<br/>...<br/>Step 1997: Loss: 13768.004 Loc: 0.855<br/>Step 1998: Loss: 13768.004 Loc: 0.855<br/>Step 1999: Loss: 13768.004 Loc: 0.855</span></pre><p id="e4ce" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们应该为此欢呼。我们已经通过最大化我们首先生成的采样数据的概率，计算了参数<em class="ng"> 𝜇 </em>的<em class="ng">最大似然估计</em>。正如我们在下面看到的，这很有效，我们能够得到一个非常接近原始值的𝜇值。</p><pre class="mr ms mt mu gt ni nj nk bn nl nm bi"><span id="8093" class="nn kj it nj b be no np l nq nr">print(f'True Value: {x_train.mean()}')<br/>print(f'Estimated Value: {normal.trainable_variables[0].numpy()}')<br/><br/>True Value: 0.8548658490180969<br/>Estimated Value: 0.8548658490180969</span></pre><h1 id="ddad" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">结论</h1><p id="a472" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">本文从理论和实践两方面介绍了使用张量流概率的最大似然估计 T2 过程。我们从陈述概率密度函数和<em class="ng">可能性</em>函数之间的差异开始。第一种方法是固定参数<em class="ng"> 𝜃 </em>，让样本成为自变量。相反，在<em class="ng">可能性</em>函数的情况下，数据是固定的(即观察到的)，参数<em class="ng"> 𝜃 </em>是我们想要学习的变量。然后，我们通过一个简单的例子直观地了解了<em class="ng">可能性</em>函数的形状。最后，我们通过定义张量流<code class="fe od oe of nj b">Variable</code>、负对数似然函数和应用梯度，使用张量流概率实现了一个定制的训练程序。</p><p id="f4b2" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">下周，我们将开始构建我们的第一个算法。到时候见！</p><p id="3951" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">保持联系:<a class="ae mk" href="https://www.linkedin.com/in/luisbrasroque/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a></p><h1 id="a644" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">参考资料和材料</h1><p id="1950" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">[1] — <a class="ae mk" href="https://www.coursera.org/specializations/deep-learning" rel="noopener ugc nofollow" target="_blank"> Coursera:深度学习专业化</a></p><p id="de04" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">[2] — <a class="ae mk" href="https://www.coursera.org/specializations/tensorflow2-deeplearning" rel="noopener ugc nofollow" target="_blank"> Coursera:深度学习的 tensor flow 2</a>专业化</p><p id="aff2" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">[3] — <a class="ae mk" href="https://www.tensorflow.org/probability/overview" rel="noopener ugc nofollow" target="_blank">张量流概率指南和教程</a></p><p id="b1df" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">[4] — <a class="ae mk" href="https://blog.tensorflow.org/search?label=TensorFlow+Probability&amp;max-results=20" rel="noopener ugc nofollow" target="_blank"> TensorFlow 博客中的 TensorFlow 概率帖子</a></p></div></div>    
</body>
</html>