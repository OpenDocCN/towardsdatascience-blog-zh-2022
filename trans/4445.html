<html>
<head>
<title>Concept Embedding Models: Beyond the Accuracy-Explainability Trade-Off</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">概念嵌入模型:超越精确性-可解释性的权衡</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/concept-embedding-models-beyond-the-accuracy-explainability-trade-off-f7ba02f28fad#2022-10-03">https://towardsdatascience.com/concept-embedding-models-beyond-the-accuracy-explainability-trade-off-f7ba02f28fad#2022-10-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="988a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由马特奥·埃斯皮诺萨·扎伦加和<a class="kl km ep" href="https://medium.com/u/c3b867b869ca?source=post_page-----f7ba02f28fad--------------------------------" rel="noopener" target="_blank">皮埃特罗·巴比洛</a>主演。摘自NeurIPS论文“<a class="ae kn" href="https://arxiv.org/abs/2209.09056" rel="noopener ugc nofollow" target="_blank">概念嵌入模型</a>”(<a class="ae kn" href="https://github.com/mateoespinosa/cem" rel="noopener ugc nofollow" target="_blank">GitHub</a>)。</p><p id="e27f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">TL；博士</strong></p><ul class=""><li id="4c2b" class="ko kp iq jp b jq jr ju jv jy kq kc kr kg ks kk kt ku kv kw bi translated"><strong class="jp ir">问题— </strong>人类对深度神经网络的信任目前是一个公开的问题，因为它们的决策过程是不透明的。目前的方法，如概念瓶颈模型，以降低准确性为代价使模型更易解释(反之亦然)。</li><li id="2174" class="ko kp iq jp b jq kx ju ky jy kz kc la kg lb kk kt ku kv kw bi translated"><strong class="jp ir">关键创新— </strong>一个受监督的概念嵌入层，它学习语义上有意义的概念表示，并允许简单的人工测试时干预。</li><li id="c5df" class="ko kp iq jp b jq kx ju ky jy kz kc la kg lb kk kt ku kv kw bi translated"><strong class="jp ir">解决方案——</strong><a class="ae kn" href="https://arxiv.org/abs/2209.09056" rel="noopener ugc nofollow" target="_blank">概念嵌入模型</a>增加人类对深度学习的信任，超越准确性-可解释性的权衡——既高度准确又可解释。</li></ul><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/320f0da9c040e0e132510a95790e5417.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*z6reZErt48xa0D5r1D0-Ww.png"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">概念嵌入模型超越了概念瓶颈模型中的准确性-可解释性权衡，具有近乎最优的任务准确性和概念一致性。最佳权衡用红星表示(右上角)。任务是学习两个向量之间点积的符号(+/-)。图片由作者提供。</p></figure></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="d148" class="lv lw iq bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">学习概念:可解释人工智能的新前沿</h1><p id="d4b7" class="pw-post-body-paragraph jn jo iq jp b jq mt js jt ju mu jw jx jy mv ka kb kc mw ke kf kg mx ki kj kk ij bi translated">概念瓶颈模型(或CBMs，[1])自2020年问世以来，已经成为可解释人工智能的一项重大成就。这些模型试图通过鼓励深度神经网络通过设计 变得更加可解释<strong class="jp ir"> <em class="my">来解决人类对人工智能缺乏信任的问题。为此，建立信任措施首先学习它们的输入(例如，猫和狗的图像)和一组"<strong class="jp ir"> <em class="my">概念</em> </strong>"之间的映射，这些概念对应于人类通常用来描述他们所看到的事物的高级信息单元(例如，"胡须"、"长尾"、"黑毛"等……)。这个映射函数，我们将称之为“概念编码器”，是通过——你猜对了:)——一个可微分和高度表达的模型(如深度神经网络)学习的！正是通过这个概念编码器，CBMs然后通过将概念映射到输出标签来解决感兴趣的下游任务(例如，将图像分类为狗或猫):</em></strong></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/62b088681a1ad11aadd695d32b33786b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*oAvRkXq_Twxe31npDwEELA.png"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">概念瓶颈模型将任务(<strong class="bd na"> Y </strong>)作为概念(<strong class="bd na"> C </strong>)的函数来学习。图片由作者提供。</p></figure><p id="e96e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用于将概念映射到任务标签的标签预测器可能是您最喜欢的可区分模型，尽管在实践中它往往是简单的，如单个完全连接的层。由于CBM能够在预测过程中学习中间概念，它们可以:</p><ul class=""><li id="e858" class="ko kp iq jp b jq jr ju jv jy kq kc kr kg ks kk kt ku kv kw bi translated">为他们的预测提供简单的<strong class="jp ir"> <em class="my">基于概念的解释</em> </strong>。这些是人类可以直观理解的解释，而不是需要用户高度脑力负荷的基于特征的解释(例如，显著性图)。例如，在图像分类中，基于概念的解释可能看起来像:“如果红色和圆形，那么它是苹果”，这比基于特征的解释更直观，例如:“如果pixel-#7238和pixel-#129，那么它是苹果”。</li><li id="2826" class="ko kp iq jp b jq kx ju ky jy kz kc la kg lb kk kt ku kv kw bi translated">允许<strong class="jp ir"> <em class="my">人类专家与模型</em> </strong>互动，在测试时改变预测错误的概念，通过专家互动显著提高任务准确性。</li></ul><p id="ff22" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">基于概念的解释和人类干预都极大地有助于<strong class="jp ir">增加人类对建立信任措施的</strong>信任。这是由于(I)它们的<em class="my">设计可解释</em>架构，(ii)CBM解释中使用的术语与专家使用的已知<em class="my">人类类别</em>之间的明确一致性，以及(iii)它们对<em class="my">人类交互</em>的本地支持，允许专家通过概念干预测试任务对所学概念的因果依赖性的强度。</p><blockquote class="nb"><p id="37d8" class="nc nd iq bd ne nf ng nh ni nj nk kk dk translated">概念瓶颈模型增加了人们的信任，提供了直观的基于概念的解释，并支持人们对概念的干预。</p></blockquote></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="07ed" class="lv lw iq bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">童话的结尾:CBMs中准确性与可解释性的权衡</h1><p id="894d" class="pw-post-body-paragraph jn jo iq jp b jq mt js jt ju mu jw jx jy mv ka kb kc mw ke kf kg mx ki kj kk ij bi translated">如同生活中的一切，建立信任措施也有不幸的阴暗面。在最初，概念瓶颈模型试图通过将分类任务限制在类似人类概念的中间水平上来“通过设计”提高可信度。然而，我们的爱情故事到这里就结束了，我们意识到，天下没有免费的午餐。虽然CBM在结构上比传统的深度神经网络更具可解释性，但这种新的可解释性可能会以任务准确性为代价！在训练模型所需的概念监督很少的现实情况下，这种情况尤其严重，从而质疑建立信任措施的实际效用。这可以从下图中看出，随着我们监督的概念数量的减少，我们观察到CBM的任务准确性急剧下降:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/a62c7e7b4f7eb3c5e02e42458603738a.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*dfMFjmS6vc2kEJFihqDaxQ.png"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">在缺乏概念监督的现实世界条件下，概念瓶颈模型的准确性迅速下降。CUB数据集。图片由作者提供。</p></figure><blockquote class="nb"><p id="0a11" class="nc nd iq bd ne nf nm nn no np nq kk dk translated">当概念监督不足或分类问题变得棘手时，普通概念瓶颈模型可能会在现实世界条件下崩溃。</p></blockquote><p id="37b6" class="pw-post-body-paragraph jn jo iq jp b jq nr js jt ju ns jw jx jy nt ka kb kc nu ke kf kg nv ki kj kk ij bi translated">为了解决这一限制，Manhipei等人[2]探索了CBMs的混合版本，其中概念层增加了一组无监督的神经元，允许模型使用这一额外的学习能力来解决分类任务:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/ab83327191a611ddf00cfd321e3207cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*MmNXoaPvtEXepRuRct0Bjg.png"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">混合概念瓶颈模型在概念层的无监督神经元中编码额外的信息。图片由作者提供。</p></figure><p id="75ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个技巧使得混合CBM在任务准确性方面优于普通CBM。然而，<strong class="jp ir">这种性能的提高是以模型的可解释性和可信度为代价的</strong>,因为:</p><ul class=""><li id="7c80" class="ko kp iq jp b jq jr ju jv jy kq kc kr kg ks kk kt ku kv kw bi translated">额外的神经元通常<strong class="jp ir">不符合</strong>任何已知的人类概念</li></ul><p id="4b36" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">和</p><ul class=""><li id="f86b" class="ko kp iq jp b jq jr ju jv jy kq kc kr kg ks kk kt ku kv kw bi translated">概念干预对输出标签<strong class="jp ir">的影响急剧下降</strong>，这表明模型正在使用额外的神经元而不是学习的概念来解决任务。</li></ul><p id="3e8b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从经验上来说，我们可以在任务准确性和基本事实标签与通过下图中不同概念瓶颈模型(我们还包括一个没有概念层作为参考的端到端模型)学习的概念的一致性之间观察到这种折衷:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/60f0a1f737bbde8c16c3d19b2b6a23fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*_53CM0HFCrXwVe9dB-fYbQ.png"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">概念瓶颈模型中的准确性-可解释性权衡(点数据集[3])。最佳权衡用红星表示(右上角)。任务是学习两个向量之间点积的符号(+/-)。图片由作者提供。</p></figure><blockquote class="nb"><p id="cb80" class="nc nd iq bd ne nf nm nn no np nq kk dk translated">普通和混合概念瓶颈模型在准确性和可解释性之间提供了不同的折衷，但是它们远非最佳！</p></blockquote><p id="cfee" class="pw-post-body-paragraph jn jo iq jp b jq nr js jt ju ns jw jx jy nt ka kb kc nu ke kf kg nv ki kj kk ij bi translated">这是所谓的“<strong class="jp ir">准确性-可解释性权衡</strong>”的一个简单例子:在许多重要的问题中，你可以得到:</p><ul class=""><li id="6dd5" class="ko kp iq jp b jq jr ju jv jy kq kc kr kg ks kk kt ku kv kw bi translated">一个<strong class="jp ir">更简单</strong>但<strong class="jp ir">不太精确</strong>的模型(例如Koh等人的CBM)</li></ul><p id="30a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">运筹学</p><ul class=""><li id="3e41" class="ko kp iq jp b jq jr ju jv jy kq kc kr kg ks kk kt ku kv kw bi translated">一个<strong class="jp ir">更精确</strong>但<strong class="jp ir">更难解释的</strong>模型(例如Manhipei等人的CBM)。</li></ul><p id="a62d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可解释性较差的模型的问题在于，人类往往不信任自己的预测，因为他们不理解决策过程，也无法检查这个过程是否有意义——因此质疑部署此类模型的道德和法律影响。</p><p id="e5a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">出于这些原因，这种<strong class="jp ir">准确性-可解释性权衡</strong>是“诅咒”现代机器学习(尤其是基于概念的模型)的最具挑战性的公开问题之一。这就是为什么解决这个问题是这个领域中的“金块”之一！</p><blockquote class="nb"><p id="9c30" class="nc nd iq bd ne nf ng nh ni nj nk kk dk translated">准确性-可解释性的权衡“诅咒”现代机器学习，包括概念瓶颈模型！在解决重要问题时，很难同时最大限度地提高准确性和可解释性。</p></blockquote></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="dbe5" class="lv lw iq bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">除了准确性和可解释性之间的权衡</h1><p id="dd7c" class="pw-post-body-paragraph jn jo iq jp b jq mt js jt ju mu jw jx jy mv ka kb kc mw ke kf kg mx ki kj kk ij bi translated">为了解决基于概念的模型中的这种<strong class="jp ir">准确性-可解释性权衡</strong>，我们最近提出了<a class="ae kn" href="https://arxiv.org/abs/2209.09056" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> <em class="my">概念嵌入模型</em> </strong> </a> (CEMs，[3])，这是一类基于概念的模型，其中:</p><ul class=""><li id="6a31" class="ko kp iq jp b jq jr ju jv jy kq kc kr kg ks kk kt ku kv kw bi translated">该架构将每个<strong class="jp ir"> <em class="my">概念</em> </strong>表示为一个<strong class="jp ir"> <em class="my">监督向量</em> </strong>。直观上，使用高维嵌入来表示每个概念允许<strong class="jp ir">额外的<em class="my">监督</em>学习能力</strong>，这与混合CBM相反，在混合CBM中，流经其<em class="my">非监督</em>瓶颈激活的信息是概念不可知的。</li><li id="9f93" class="ko kp iq jp b jq kx ju ky jy kz kc la kg lb kk kt ku kv kw bi translated">培训程序包括一个规范化策略(<em class="my"> RandInt </em>)，在培训 期间将CEMs暴露于<strong class="jp ir"> <em class="my">概念干预，以提高此类行动在测试时的有效性。</em></strong></li></ul><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi nx"><img src="../Images/9ca695ddac16af12ce5c4b682d2d84e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yPPvBdVl6aWaVWh2GQVt8A.png"/></div></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">概念嵌入模型将每个概念表示为一个监督向量。图片由作者提供。</p></figure><p id="4064" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们在简单的玩具数据集和具有挑战性的真实数据集上的实验表明，CEMs:</p><ul class=""><li id="31df" class="ko kp iq jp b jq jr ju jv jy kq kc kr kg ks kk kt ku kv kw bi translated"><strong class="jp ir">克服准确性与可解释性的权衡</strong>在解决分类问题时与等效的黑盒模型一样准确(甚至可能更准确！)并且在以下方面与传统的CBM一样可解释:( a)学习的概念表示与概念基本事实一致的程度:</li></ul><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi oc"><img src="../Images/80e189c3026444769560e666330cde7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pohd3z0DN2iAoQ0i2kbAXw.png"/></div></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">概念嵌入模型超越了概念瓶颈模型中准确性和可解释性的权衡。最佳权衡在每个图的右上角。图片由作者提供。</p></figure><ul class=""><li id="95b8" class="ko kp iq jp b jq jr ju jv jy kq kc kr kg ks kk kt ku kv kw bi translated"><strong class="jp ir">支持有效的人类干预</strong>允许人类专家与模型互动，通过在测试时修正预测错误的概念来提高预测准确性，这与混合CBM相反:</li></ul><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi gj"><img src="../Images/09141f1483ec61830b1cfb9b6a2b85a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xwp9WM3gle7XLFoI6DDeDw.png"/></div></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">RandInt的概念嵌入模型支持有效的人工干预(与混合概念瓶颈模型相反)。图片由作者提供。</p></figure><ul class=""><li id="95b0" class="ko kp iq jp b jq jr ju jv jy kq kc kr kg ks kk kt ku kv kw bi translated"><strong class="jp ir">扩展到缺乏概念监督的真实环境</strong>:</li></ul><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi od"><img src="../Images/0e8c85c2d1d4303f861ee7b7233a1346.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*ymYvE_ItwJtYSBQjslnV6Q.png"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">概念嵌入模型适用于缺乏概念监督的现实世界。图片由作者提供。</p></figure><ul class=""><li id="60d7" class="ko kp iq jp b jq jr ju jv jy kq kc kr kg ks kk kt ku kv kw bi translated"><strong class="jp ir">使用每个概念的预测激活状态，提供基于概念的解释</strong>作为普通的CBMs(例如，“如果红色和圆形，而不是翅膀，那么它是一个苹果”)。</li></ul><blockquote class="nb"><p id="5d6e" class="nc nd iq bd ne nf nm nn no np nq kk dk translated">概念嵌入模型超越了概念瓶颈模型中准确性和可解释性的权衡——它们既准确又可解释！</p></blockquote><p id="3d56" class="pw-post-body-paragraph jn jo iq jp b jq nr js jt ju ns jw jx jy nt ka kb kc nu ke kf kg nv ki kj kk ij bi translated">最后，通过监督整个嵌入，CEMs学习概念表示，与混合CBMs学习的无监督嵌入相比，概念的激活状态/真值之间的区别更明显，如下例所示:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi oe"><img src="../Images/da60593c6bfc63b5a14b93213788d940.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-xPxs6tP3ejpXmJvfBvSzg.png"/></div></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">CUB数据集中概念的低维表示“有白色翅膀”,显示每个样本的概念激活状态。CEMs嵌入显示在左侧(a)，混合CBMs嵌入显示在右侧(b)。图片由作者提供。</p></figure><p id="cdce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这表明CEM能够在每个概念向量中编码<strong class="jp ir">显式符号信息</strong>(即，概念是活动的还是非活动的)<strong class="jp ir">子符号概念语义</strong>(即，每个概念嵌入中的子集群)，这与混合CBM相反。</p></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="9660" class="lv lw iq bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">带回家的信息</h1><p id="6aa6" class="pw-post-body-paragraph jn jo iq jp b jq mt js jt ju mu jw jx jy mv ka kb kc mw ke kf kg mx ki kj kk ij bi translated">CEMs超越了当前的准确性和可解释性的权衡，显著增加了人类的信任度！</p><p id="0bb1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">特别是CEM:</p><ul class=""><li id="37df" class="ko kp iq jp b jq jr ju jv jy kq kc kr kg ks kk kt ku kv kw bi translated">作为<strong class="jp ir"> <em class="my">准确的</em> </strong>作为标准的端到端模型没有概念(而且往往更准确！);</li><li id="5b94" class="ko kp iq jp b jq kx ju ky jy kz kc la kg lb kk kt ku kv kw bi translated">作为<strong class="jp ir"> <em class="my">可解释的</em> </strong>作为标准概念瓶颈模型(而且往往更可解释！);</li><li id="2222" class="ko kp iq jp b jq kx ju ky jy kz kc la kg lb kk kt ku kv kw bi translated">作为<strong class="jp ir"> <em class="my">响应</em> </strong>人为干预的标准概念瓶颈模型。</li></ul><h1 id="0b93" class="lv lw iq bd lx ly of ma mb mc og me mf mg oh mi mj mk oi mm mn mo oj mq mr ms bi translated">参考</h1><p id="781e" class="pw-post-body-paragraph jn jo iq jp b jq mt js jt ju mu jw jx jy mv ka kb kc mw ke kf kg mx ki kj kk ij bi translated">[1]彭伟高，阮涛，邓耀祥，斯蒂芬·穆斯曼，，曾金，和佩西·梁。概念瓶颈模型。<em class="my">在机器学习国际会议上，第5338–5348页。PMLR(2020)。</em></p><p id="1c62" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[2]安妮塔·马欣佩、贾斯汀·克拉克、艾萨克·拉赫、终曲·多希-维勒兹和潘薇薇。黑盒概念学习模型的承诺和陷阱。arXiv预印本arXiv:2106.13314  (2021)。</p><p id="7959" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[3] Mateo Espinosa Zarlenga、Pietro Barbiero、Gabriele Ciravegna、Giuseppe Marra、Francesco Giannini、Michelangelo Diligenti、Zohreh Shams、Frederic Precioso、Stefano Melacci、Adrian Weller、Pietro莉雅和Mateja Jamnik“概念嵌入模型”arXiv预印本:2209.09056  (2022)。</p></div></div>    
</body>
</html>