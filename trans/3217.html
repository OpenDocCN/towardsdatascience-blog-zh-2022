<html>
<head>
<title>Fine-Tuning LayoutLM v3 for Invoice Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">微调用于发票处理的LayoutLM v3</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tuning-layoutlm-v3-for-invoice-processing-e64f8d2c87cf#2022-07-18">https://towardsdatascience.com/fine-tuning-layoutlm-v3-for-invoice-processing-e64f8d2c87cf#2022-07-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2615" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">并将其性能与LayoutLMv2进行比较</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2667a95a2a8257dbff84f02fa15cfae1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2C0xtEX4KMly98MXB6fWWA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1274307" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的图片由<a class="ae ky" href="https://pixabay.com/users/sabinevanerp-2145163/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1274307" rel="noopener ugc nofollow" target="_blank"> Sabine van Erp </a></p></figure><p id="be3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">文档理解是文档处理和抽取的第一步，也是最重要的一步。它是从非结构化或半结构化文档中提取信息并将其转换为结构化形式的过程。然后，这种结构化表示可用于支持各种下游任务，如信息检索、摘要、分类等。理解文档有许多不同的方法，但它们都有一个共同的目标:创建文档内容的结构化表示，以便用于进一步处理。</p><p id="0694" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于发票、收据或合同等半结构化文档，随着LayoutLM v1和v2的开发，微软的layoutLM模型显示出了巨大的前景。如需深入教程，请参考我之前的两篇文章“<a class="ae ky" rel="noopener" target="_blank" href="/fine-tuning-transformer-model-for-invoice-recognition-1e55869336d4">微调变压器模型用于发票识别</a>”和“<a class="ae ky" rel="noopener" target="_blank" href="/fine-tuning-layoutlm-v2-for-invoice-recognition-91bf2546b19e">微调LayoutLM v2用于发票识别</a>”。</p><p id="a08f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本教程中，我们将微调微软最新的LayoutLM v3，类似于我以前的教程，我们将比较其性能与layoutLM v2模型。</p><h1 id="da19" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">LayoutLM v3</h1><p id="0256" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">LayoutLM v3相对于其前身的主要优势是多模态转换器架构，它以统一的方式结合了文本和图像嵌入。不是依赖CNN来进行图像嵌入，文档图像被表示为图像块的线性投影，然后图像块被线性嵌入并与文本标记对齐，如下所示。这种方法的主要优点是所需参数的减少和总体计算量的降低。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/2508a87436b0f2459e41497510f9a3a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b7wg0sXG_suDmGYv5yg92w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">布局LM v3架构。<a class="ae ky" href="https://github.com/microsoft/unilm/tree/master/layoutlmv3" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><blockquote class="mt mu mv"><p id="898d" class="kz la mw lb b lc ld ju le lf lg jx lh mx lj lk ll my ln lo lp mz lr ls lt lu im bi translated">作者表明，“LayoutLMv3不仅在以文本为中心的任务(包括表单理解、收据理解和文档视觉问答)中，而且在以图像为中心的任务(如文档图像分类和文档布局分析)中，都实现了一流的性能”。</p></blockquote><h1 id="3652" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">微调LayoutLM v3</h1><p id="6b10" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">与我的上一篇文章类似，我们将使用220个带注释的发票的相同数据集来微调layoutLM v3模型。为了执行注释，我们使用了<a class="ae ky" href="https://ubiai.tools" rel="noopener ugc nofollow" target="_blank"> UBIAI文本注释工具</a>，因为它支持OCR解析、原生PDF/图像注释以及以与LayoutLM模型兼容的正确格式导出，而无需任何后期处理。此外，<a class="ae ky" href="https://ubiai.tools/Docs#modelcreation" rel="noopener ugc nofollow" target="_blank">可以在UBIAI平台中对layouLM模型</a>进行微调，并使用它自动标记您的数据，这可以节省大量手动注释时间。</p><p id="9d6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是关于如何使用该工具注释pdf和图像的精彩概述:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://www.youtube.com/@karndeepsingh" rel="noopener ugc nofollow" target="_blank"> Karndeep Singh </a>提供的UBIAI教程</p></figure><p id="7cb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从<a class="ae ky" href="https://ubiai.tools/" rel="noopener ugc nofollow" target="_blank"> UBIAI </a>导出注释文件后，我们将它上传到google drive文件夹。我们将使用google colab进行模型训练和推理。</p><p id="56de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以在下面的google colab中访问训练和推理脚本:</p><p id="0544" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">训练:</strong></p><div class="nc nd gp gr ne nf"><a href="https://colab.research.google.com/drive/1TuPQ1HdhMYjfQI9VRV3FMk7bpPYD4g6w?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">谷歌联合实验室</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">编辑描述</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">colab.research.google.com</p></div></div><div class="no l"><div class="np l nq nr ns no nt ks nf"/></div></div></a></div><p id="16ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">推论:</strong></p><div class="nc nd gp gr ne nf"><a href="https://colab.research.google.com/drive/1YDTKuIg4mkrCBz1bzqabL27_UzF2Z82F?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">谷歌联合实验室</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">编辑描述</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">colab.research.google.com</p></div></div><div class="no l"><div class="nu l nq nr ns no nt ks nf"/></div></div></a></div><ul class=""><li id="60ce" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated">第一步是打开一个google colab，连接你的google drive，从huggingface安装变形金刚包。请注意，与layoutLMv2不同，我们没有使用detectron 2包来微调实体提取模型。但是，对于布局检测(超出了本文的范围)，需要detectorn 2包:</li></ul><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="efe8" class="oj lw it of b gy ok ol l om on">from google.colab import drive</span><span id="bc31" class="oj lw it of b gy oo ol l om on">drive.mount('/content/drive')</span><span id="31c4" class="oj lw it of b gy oo ol l om on">!pip install -q git+https://github.com/huggingface/transformers.git</span><span id="0cb5" class="oj lw it of b gy oo ol l om on">! pip install -q git+https://github.com/huggingface/datasets.git "dill&lt;0.3.5" seqeval</span></pre><ul class=""><li id="ab55" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated">接下来，提取preprocess.py脚本来处理从UBIAI导出的ZIP文件:</li></ul><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="afa1" class="oj lw it of b gy ok ol l om on">! rm -r layoutlmv3FineTuning</span><span id="ed10" class="oj lw it of b gy oo ol l om on">! git clone -b main <a class="ae ky" href="https://github.com/UBIAI/layoutlmv3FineTuning.git" rel="noopener ugc nofollow" target="_blank">https://github.com/UBIAI/layoutlmv3FineTuning.git</a></span><span id="b978" class="oj lw it of b gy oo ol l om on">#!/bin/bash</span><span id="38f6" class="oj lw it of b gy oo ol l om on">IOB_DATA_PATH = "/content/drive/MyDrive/LayoutLM_data/Invoice_Project_mkWSi4Z.zip"</span><span id="c11a" class="oj lw it of b gy oo ol l om on">! cd /content/</span><span id="68df" class="oj lw it of b gy oo ol l om on">! rm -r data</span><span id="60a4" class="oj lw it of b gy oo ol l om on">! mkdir data</span><span id="8b4d" class="oj lw it of b gy oo ol l om on">! cp "$IOB_DATA_PATH" data/dataset.zip</span><span id="0cb9" class="oj lw it of b gy oo ol l om on">! cd data &amp;&amp; unzip -q dataset &amp;&amp; rm dataset.zip</span><span id="ecdd" class="oj lw it of b gy oo ol l om on">! cd ..</span></pre><ul class=""><li id="b9c2" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated">运行预处理脚本:</li></ul><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="6de6" class="oj lw it of b gy ok ol l om on">#!/bin/bash</span><span id="b976" class="oj lw it of b gy oo ol l om on">#preprocessing args</span><span id="ca91" class="oj lw it of b gy oo ol l om on">TEST_SIZE = 0.33</span><span id="0c28" class="oj lw it of b gy oo ol l om on">DATA_OUTPUT_PATH = "/content/"</span><span id="6df2" class="oj lw it of b gy oo ol l om on">! python3 layoutlmv3FineTuning/preprocess.py --valid_size $TEST_SIZE --output_path $DATA_OUTPUT_PATH</span></pre><ul class=""><li id="49e2" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated">加载数据集后处理:</li></ul><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="a3be" class="oj lw it of b gy ok ol l om on">from datasets import load_metric</span><span id="aff2" class="oj lw it of b gy oo ol l om on">from transformers import TrainingArguments, Trainer</span><span id="4972" class="oj lw it of b gy oo ol l om on">from transformers import LayoutLMv3ForTokenClassification,AutoProcessor</span><span id="e942" class="oj lw it of b gy oo ol l om on">from transformers.data.data_collator import default_data_collator</span><span id="8c64" class="oj lw it of b gy oo ol l om on">import torch</span><span id="9bcb" class="oj lw it of b gy oo ol l om on"># load datasets</span><span id="de68" class="oj lw it of b gy oo ol l om on">from datasets import load_from_disk</span><span id="d233" class="oj lw it of b gy oo ol l om on">train_dataset = load_from_disk(f'/content/train_split')</span><span id="b51c" class="oj lw it of b gy oo ol l om on">eval_dataset = load_from_disk(f'/content/eval_split')</span><span id="4408" class="oj lw it of b gy oo ol l om on">label_list = train_dataset.features["labels"].feature.names<br/>num_labels = len(label_list)<br/>label2id, id2label = dict(), dict()<br/>for i, label in enumerate(label_list):<br/>    label2id[label] = i<br/>    id2label[i] = label</span></pre><ul class=""><li id="5e14" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated">定义一些评估指标:</li></ul><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="e9f5" class="oj lw it of b gy ok ol l om on">metric = load_metric("seqeval")<br/>import numpy as np</span><span id="92f4" class="oj lw it of b gy oo ol l om on">return_entity_level_metrics = False</span><span id="87ab" class="oj lw it of b gy oo ol l om on">def compute_metrics(p):<br/>    predictions, labels = p<br/>    predictions = np.argmax(predictions, axis=2)</span><span id="33fb" class="oj lw it of b gy oo ol l om on"># Remove ignored index (special tokens)<br/>    true_predictions = [<br/>        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]<br/>        for prediction, label in zip(predictions, labels)<br/>    ]<br/>    true_labels = [<br/>        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]<br/>        for prediction, label in zip(predictions, labels)<br/>    ]</span><span id="c129" class="oj lw it of b gy oo ol l om on">results = metric.compute(predictions=true_predictions, references=true_labels,zero_division='0')<br/>    if return_entity_level_metrics:<br/>        # Unpack nested dictionaries<br/>        final_results = {}<br/>        for key, value in results.items():<br/>            if isinstance(value, dict):<br/>                for n, v in value.items():<br/>                    final_results[f"{key}_{n}"] = v<br/>            else:<br/>                final_results[key] = value<br/>        return final_results<br/>    else:<br/>        return {<br/>            "precision": results["overall_precision"],<br/>            "recall": results["overall_recall"],<br/>            "f1": results["overall_f1"],<br/>            "accuracy": results["overall_accuracy"],<br/>        }</span></pre><ul class=""><li id="e4f7" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated">加载、训练和评估模型:</li></ul><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="80e6" class="oj lw it of b gy ok ol l om on">model = LayoutLMv3ForTokenClassification.from_pretrained("microsoft/layoutlmv3-base",<br/>                                                         id2label=id2label,<br/>                                                         label2id=label2id)</span><span id="ec87" class="oj lw it of b gy oo ol l om on">processor = AutoProcessor.from_pretrained("microsoft/layoutlmv3-base", apply_ocr=False)</span><span id="8064" class="oj lw it of b gy oo ol l om on">NUM_TRAIN_EPOCHS = 50</span><span id="79f7" class="oj lw it of b gy oo ol l om on">PER_DEVICE_TRAIN_BATCH_SIZE = 1</span><span id="bb68" class="oj lw it of b gy oo ol l om on">PER_DEVICE_EVAL_BATCH_SIZE = 1</span><span id="a62c" class="oj lw it of b gy oo ol l om on">LEARNING_RATE = 4e-5</span><span id="95a1" class="oj lw it of b gy oo ol l om on">training_args = TrainingArguments(output_dir="test",<br/>                                  # max_steps=1500,<br/>                                  num_train_epochs=NUM_TRAIN_EPOCHS,<br/>                                  logging_strategy="epoch",<br/>                                  save_total_limit=1,<br/>                                  per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,<br/>                                  per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,<br/>                                  learning_rate=LEARNING_RATE,<br/>                                  evaluation_strategy="epoch",<br/>                                  save_strategy="epoch",<br/>                                  # eval_steps=100,<br/>                                  load_best_model_at_end=True,<br/>                                  metric_for_best_model="f1")</span><span id="070d" class="oj lw it of b gy oo ol l om on"># Initialize our Trainer<br/>trainer = Trainer(<br/>    model=model,<br/>    args=training_args,<br/>    train_dataset=train_dataset,<br/>    eval_dataset=eval_dataset,<br/>    tokenizer=processor,<br/>    data_collator=default_data_collator,<br/>    compute_metrics=compute_metrics,<br/>)</span><span id="7050" class="oj lw it of b gy oo ol l om on">trainer.train()<br/>trainer.evaluate()</span></pre><p id="ce32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练完成后，对测试数据集进行评估。下面是评测后的模型评分:</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="b121" class="oj lw it of b gy ok ol l om on">{'epoch': 50.0,<br/> 'eval_accuracy': 0.9521988527724665,<br/> 'eval_f1': 0.6913439635535308,<br/> 'eval_loss': 0.41490793228149414,<br/> 'eval_precision': 0.6362683438155137,<br/> 'eval_recall': 0.756857855361596,<br/> 'eval_runtime': 9.7501,<br/> 'eval_samples_per_second': 9.846,<br/> 'eval_steps_per_second': 9.846}</span></pre><p id="8c67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型能够实现0.69的F1分数、0.75的召回率和0.63的精确度。</p><p id="6318" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们对不属于训练数据集的新发票运行模型。</p><h1 id="b800" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">使用LayoutLM v3进行推理</h1><p id="070a" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">为了运行推理，我们将使用Tesseract对发票进行OCR，并将信息提供给我们训练好的模型来运行预测。为了简化这个过程，我们创建了一个定制的脚本，只有几行代码，让您可以接收OCR输出并使用模型运行预测。</p><ul class=""><li id="4366" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated">第一步，让我们导入一些重要的库并加载模型:</li></ul><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="cd8b" class="oj lw it of b gy ok ol l om on">#drive mount</span><span id="3a85" class="oj lw it of b gy oo ol l om on">from google.colab import drive</span><span id="1ec1" class="oj lw it of b gy oo ol l om on">drive.mount('/content/drive')</span><span id="7bea" class="oj lw it of b gy oo ol l om on">## install Hugging face Transformers library to load Layoutlmv3 Preprocessor</span><span id="a8eb" class="oj lw it of b gy oo ol l om on">!pip install -q git+https://github.com/huggingface/transformers.git</span><span id="a030" class="oj lw it of b gy oo ol l om on">## install tesseract OCR Engine</span><span id="fe76" class="oj lw it of b gy oo ol l om on">! sudo apt install tesseract-ocr</span><span id="9d91" class="oj lw it of b gy oo ol l om on">! sudo apt install libtesseract-dev</span><span id="e51e" class="oj lw it of b gy oo ol l om on">## install pytesseract , please click restart runtime button in the cell output and move forward in the notebook</span><span id="3e55" class="oj lw it of b gy oo ol l om on">! pip install pytesseract</span><span id="2a08" class="oj lw it of b gy oo ol l om on"># ! rm -r layoutlmv3FineTuning</span><span id="51eb" class="oj lw it of b gy oo ol l om on">! git clone https://github.com/salmenhsairi/layoutlmv3FineTuning.git</span><span id="1aec" class="oj lw it of b gy oo ol l om on">import os</span><span id="720a" class="oj lw it of b gy oo ol l om on">import torch</span><span id="0948" class="oj lw it of b gy oo ol l om on">import warnings</span><span id="02f4" class="oj lw it of b gy oo ol l om on">from PIL import Image</span><span id="e3f4" class="oj lw it of b gy oo ol l om on">warnings.filterwarnings('ignore')</span><span id="2dfe" class="oj lw it of b gy oo ol l om on"># move all inference images from /content to 'images' folder<br/>os.makedirs('/content/images',exist_ok=True)<br/>for image in os.listdir():<br/>  try:<br/>    img = Image.open(f'{os.curdir}/{image}')<br/>    os.system(f'mv "{image}" "images/{image}"')<br/>  except:<br/>    pass</span><span id="bd49" class="oj lw it of b gy oo ol l om on"># defining inference parameters</span><span id="bab8" class="oj lw it of b gy oo ol l om on">model_path = "/content/drive/MyDrive/LayoutLM_data/layoutlmv3.pth" # path to Layoutlmv3 model</span><span id="059b" class="oj lw it of b gy oo ol l om on">imag_path = "/content/images" # images folder<br/># if inference model is pth then convert it to pre-trained format<br/>if model_path.endswith('.pth'):<br/>  layoutlmv3_model = torch.load(model_path)<br/>  model_path = '/content/pre_trained_layoutlmv3'<br/>  layoutlmv3_model.save_pretrained(model_path)</span></pre><ul class=""><li id="6fb1" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated">我们现在准备使用模型运行预测</li></ul><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="d50d" class="oj lw it of b gy ok ol l om on"># Call inference module</span><span id="d955" class="oj lw it of b gy oo ol l om on">! python3 /content/layoutlmv3FineTuning/run_inference.py --model_path "$model_path" --images_path $imag_path</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/894b49809b380f6a1b9a22275c007a3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0wT4Rt1ABQpBJl2-i6YyPw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:输出LayoutLM v3</p></figure><p id="5575" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了220张带注释的发票，该模型能够正确预测卖家姓名、日期、发票号码和总价(TTC)！</p><p id="a1b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们仔细观察，我们会发现它犯了一个错误，将笔记本电脑的总价作为发票总价。鉴于模型的得分，这并不奇怪，并暗示需要更多的训练数据。</p><h1 id="0c0a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">LayoutLM v2与LayoutLM v3的比较</h1><p id="786a" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">除了计算量较小之外，layoutLM V3与v2相比是否提供了性能提升？为了回答这个问题，我们比较了同一张发票的两种模型输出。下面是我上一篇文章中显示的layoutLM v2输出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/ac8a9b0947bbda4ca2ee21727d374f35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-IXuF9i4JEIUBAnluN32oQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:输出LayoutLM v2</p></figure><p id="52ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们观察到几个区别:</p><ul class=""><li id="9940" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated">v3模型能够正确地检测大多数密钥，而v2无法预测invoice_ID、Invoice number_ID和Total_ID</li><li id="824d" class="nv nw it lb b lc or lf os li ot lm ou lq ov lu oa ob oc od bi translated">v2模型错误地将总价$1，445.00标记为MONTANT_HT(在法语中表示税前总价)，而v3模型正确地预测了总价。</li><li id="2500" class="nv nw it lb b lc or lf os li ot lm ou lq ov lu oa ob oc od bi translated">这两种型号都犯了一个错误，将笔记本电脑的价格标为总价。</li></ul><p id="8969" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于这个例子，layoutLM V3总体上表现出了更好的性能，但是我们需要在更大的数据集上进行测试来证实这一观察结果。</p><h1 id="8be1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="b9e3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">通过开源layoutLM模型，微软正在引领供应链、医疗保健、金融、银行等许多行业的数字化转型。</p><p id="d4c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个循序渐进的教程中，我们展示了如何在一个特定的用例(发票数据提取)上微调layoutLM V3。然后，我们将其性能与layoutLM V2进行了比较，发现性能略有提升，但仍需在更大的数据集上进行验证。</p><p id="90a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于性能和计算增益，我强烈推荐利用新的layoutLM v3。</p><p id="328a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您有兴趣以最高效和最简化的方式创建自己的训练数据集，请不要犹豫，免费尝试UBIAI OCR注释功能<a class="ae ky" href="https://ubiai.tools/Signup" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><p id="2535" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在推特上关注我们<a class="ae ky" href="https://twitter.com/UBIAI5" rel="noopener ugc nofollow" target="_blank"> @UBIAI5 </a>或<a class="ae ky" href="https://walidamamou.medium.com/subscribe" rel="noopener">订阅这里</a>！</p></div></div>    
</body>
</html>