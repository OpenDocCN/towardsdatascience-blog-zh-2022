<html>
<head>
<title>5-Minute Paper Explanations: Food AI Part II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">5分钟的书面解释:食品人工智能第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/5-minute-paper-explanations-food-ai-part-ii-c085b2789bd1#2022-08-18">https://towardsdatascience.com/5-minute-paper-explanations-food-ai-part-ii-c085b2789bd1#2022-08-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5df7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">im2recipe相关论文“分而治之跨模态配方检索:从最近邻基线到SoTA”的直观深入探讨</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/68708c8320a47b550f7801dde7fa1a3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*drewTtEmZEDLLqWI"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">乍得·蒙塔诺在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="a398" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">问题简介</h1><p id="25c4" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">欢迎来到人工智能食品系列论文的第二部分！<a class="ae kv" rel="noopener" target="_blank" href="/5-minute-paper-explanations-food-ai-part-i-9276b61873c1">上周</a>，我写了一篇文章，解释了开创性的im2recipe论文，该论文将跨通道技术引入到处理与食品应用相关的机器学习中，例如使用照片搜索正确的食谱，自动确定一道菜的卡路里数，或者提高各种食谱推荐和排名系统的性能。我建议读者参考那篇文章，了解问题的介绍和动机，以及关于Recipe1M数据集和评估指标的详细信息。</p><p id="2c84" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">正如上一篇文章中提到的，这些解释旨在绘制机器学习特定领域的研究进展。所以，今天，我们将关注2019年发表的题为“分而治之跨模态配方检索:从最近邻居基线到SoTA”的论文<a class="ae kv" href="https://arxiv.org/abs/1911.12763" rel="noopener ugc nofollow" target="_blank">。本文进一步研究了此处</a><a class="ae kv" href="http://pic2recipe.csail.mit.edu/im2recipe.pdf" rel="noopener ugc nofollow" target="_blank">引入的</a>和此处解释的<a class="ae kv" rel="noopener" target="_blank" href="/5-minute-paper-explanations-food-ai-part-i-9276b61873c1">的im2recipe问题，通过<strong class="lq ir">定义一个不同的和更好的基线来比较手头的检索任务</strong>。该基线旨在取代原始文件中的共同国家评估基线。</a></p><h1 id="a957" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">相关工作和改进</h1><p id="664b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">提高基线的原因是:<strong class="lq ir"> 1) </strong>“在1000个食谱的测试集上，召回@1的CCA基线是14.0。这一结果在两年内被超越，翻了两番，达到51.8。”作者认为，这更多地与“错误指定的基线”有关，而不是检索方法的真正改进。<strong class="lq ir"> 2) </strong> <strong class="lq ir">在最初的im2recipe论文中，模型是端到端训练的</strong>，因此很难理解模型的每个元素是如何单独执行的(这些元素是图像编码器、文本编码器等)。这可以通过建立一个概念简单且可解释的模型作为问题的基线来解决。</p><p id="b958" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">具体来说，模型中的检索功能是kNN算法的跨模式适应，称为<strong class="lq ir"> CkNN </strong>“应用于预先计算的图像-文本嵌入之上，其中图像和文本编码器<strong class="lq ir">不是使用CkNN进行端到端训练，而是独立使用自我监督</strong>这使得测试和分析它们在分离中的性能变得容易。</p><p id="e08c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，作者扩展了这种方法，以获得关于该问题的一个新的最先进的性能。</p><p id="9703" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">论文引用的一些重要相关工作(除了im2recipe论文):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/33fa809c16de5aa00a1166845bba2678.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*EVZOpRSlicRA8r0LZXYn2g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">AdaMine Double Triplet Loss(图片由作者提供，来自论文)</p></figure><ol class=""><li id="1c93" class="mq mr iq lq b lr mk lu ml lx ms mb mt mf mu mj mv mw mx my bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1804.11146.pdf" rel="noopener ugc nofollow" target="_blank"> AdaMine </a> -对准模块采用“双三重损失”作为损失函数进行改进。简单地说，这是应用于两组三元组的三元组损失:一组表示单个图像对实例，并帮助模型学习将来自单个配方的不同模态的嵌入映射在一起，另一组表示图像对实例的类别，其通过类别信息帮助模型学习哪些配方彼此相似。后者类似于im2recipe论文中的语义正则化。</li><li id="4b3c" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">R2GAN——使用GANs帮助学习更好的图像和文本嵌入</li><li id="adf1" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">ACME -这是CkNN论文发表时的最先进水平，并使用了“GANs +跨模态三重损失一起+使用对抗学习策略的模态对齐+跨模态翻译一致性损失”。不去探究它是如何工作的细节(那可能是另一篇文章)，但重点是CkNN可以实现对ACME真正有竞争力的结果，我们很快就会看到。</li></ol><h1 id="dcb6" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">体系结构</h1><h2 id="bf0d" class="ne kx iq bd ky nf ng dn lc nh ni dp lg lx nj nk li mb nl nm lk mf nn no lm np bi translated"><strong class="ak">编码器</strong></h2><p id="45a3" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated"><strong class="lq ir">文本编码器(平均单词嵌入):</strong>平均单词嵌入是一种非常基本的文本编码方式。如下图所示。对于一个文档(可以是一个句子或一个段落或任何其他东西)，文档的嵌入是通过计算文档中单词嵌入的平均值来生成的。单词嵌入本身可以以多种方式生成。</p><p id="fb9b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在论文中，这样做的方式是:“说明和配料被视为文档，而类标签是配方标题中包含的最常见的单字和双字(类生成类似于im2recipe)”。单词嵌入大小是<em class="nq"> d=300 </em>，并且当它被随机初始化时，训练使用<strong class="lq ir"> softmax激活，在文档中单词嵌入的平均值上具有二进制交叉熵损失</strong>来学习单词嵌入。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/885557906bd837b67f830562c9dccbca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*00l6gE97lYE23Yz68JTOfQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">文本形态的平均单词嵌入架构(图片由作者提供)</p></figure><p id="358e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">图像编码器:</strong>使用ResNet-50的分类层之前的最后一层来获得图像嵌入，再次使用二进制交叉熵对使用来自配方标题的相同技术生成的一组类别标签进行正则化。</p><p id="d0f7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这里注意，为文本编码器和图像编码器生成的标签集是不同的(准确地说，文本是3453，图像是5036)。这样做的原因是，由于编码器是独立训练的，我们不局限于只使用图像-文本对作为数据，而是我们可以使用只包括文本或只包括图像的食谱数据。因此，对于文本和图像，我们有不同数量的数据点和不同的类。</p><h2 id="c51f" class="ne kx iq bd ky nf ng dn lc nh ni dp lg lx nj nk li mb nl nm lk mf nn no lm np bi translated">交叉模态神经网络</h2><p id="40d5" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">CkNN和kNN一样，是一个非参数模型。它充当图像-文本嵌入对准模块，并且如同该提议的系统中的其他组件一样，它被独立地应用。首先，编码器被训练以获得图像和文本嵌入，然后CkNN被应用于它们之上。这不同于im2recipe论文，在im2 recipe论文中，系统被端到端地训练，并且我们进行了转换以将嵌入带到共享空间。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/fed0258f1fd0b82c19202122fea9be63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cyoo9HuLzLuz0g8VN__dtw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">CkNN(图片由作者提供，来自论文)</p></figure><p id="5e61" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">CkNN有两个部分:一个在文本嵌入空间工作，另一个在图像嵌入空间工作。让我们看看它在文本嵌入空间是如何工作的(按照图像中的数字):[1]</p><ol class=""><li id="e355" class="mq mr iq lq b lr mk lu ml lx ms mb mt mf mu mj mv mw mx my bi translated">使用文本编码器<em class="nq"> e(T) </em>对一个<strong class="lq ir">候选文本文档</strong>(即食谱说明和配料)<em class="nq"> T </em>进行编码</li><li id="6c23" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">使用文本嵌入空间中的余弦相似性，基于<strong class="lq ir">文本嵌入</strong>找到<em class="nq"> k </em> ₜ最近邻居，由<em class="nq"> Rₜ </em>表示。</li><li id="1293" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">利用Recipe1M数据集提取与<em class="nq"> Rₜ </em>相关联的<strong class="lq ir">组图像</strong> <em class="nq"> Iₜ </em></li><li id="8438" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">使用图像编码器<em class="nq"> e(I)对<strong class="lq ir">图像嵌入空间</strong>中的每个<em class="nq"> I ∈ Iₜ </em>进行编码。</em></li><li id="916c" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">返回获得的<strong class="lq ir">图像嵌入</strong>的平均值作为结果</li></ol><p id="6b65" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">图像嵌入空间的相应算法是:</p><ol class=""><li id="4b17" class="mq mr iq lq b lr mk lu ml lx ms mb mt mf mu mj mv mw mx my bi translated">使用图像编码器<em class="nq"> e(I) </em>对<strong class="lq ir">候选图像<em class="nq"> I </em> </strong> <em class="nq"> </em>进行编码</li><li id="90f5" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">使用图像嵌入空间中的余弦相似性，基于<strong class="lq ir">图像嵌入</strong>找到<em class="nq"> k </em> ᵢ最近邻，用<em class="nq"> R </em> ᵢ表示。</li><li id="0b44" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">利用Recipe1M数据集提取与<em class="nq"> Rᵢ </em>关联的<strong class="lq ir">文本集</strong> <em class="nq"> Tᵢ </em></li><li id="62e3" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">使用文本编码器<em class="nq"> e(T)对<strong class="lq ir">文本嵌入空间</strong>中的每个<em class="nq"> T∈ Tᵢ </em>进行编码。</em></li><li id="648a" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">返回获得的<strong class="lq ir">文本嵌入</strong>的平均值作为结果</li></ol><p id="d4ad" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们把图像空间中的组件称为<em class="nq"> Mᵢ </em>，把文本空间中的组件称为<em class="nq"> Mₜ.</em></p><h1 id="a1ac" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">损失函数</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/87629b495ff7e6e8ce4e74cfaeec03c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YXd1vUxHryz8bEAtuWjqmA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">CkNN损失函数(图片由作者提供，来自论文)</p></figure><p id="21a5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">损失函数基本上是在1)图像嵌入空间和2)文本嵌入空间中获得的图像和文本嵌入之间的距离的线性组合。轻松点。</p><h1 id="fa67" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">实验和结果</h1><p id="bd8c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我不把重点放在论文表格中可以看到的确切结果上，而是把重点放在他们的分析上。简而言之，对于新的基线CkNN和用于比较的其他方法，结果显示了以与原始im2recipe论文中相同的方式(加上也具有10，000的样本大小)测量的检索性能。</p><p id="5539" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了证明单独训练编码器然后将CkNN应用于它与在ACME(当时的最先进技术)中端到端训练系统一样有效，使用来自相关作品的预训练图像和文本编码器的不同组合来生成嵌入，然后在其上使用CkNN来测量检索性能。结果表明，端到端召回率@1为20.6，而CkNN为17.9，具有竞争力。该分析还表明，编码器的质量，而不是跨模态对齐，是改善先前研究结果背后的主要驱动力。</p><p id="fc19" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">扩展到SoTA </strong></p><p id="bd35" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，受上述与ACME的竞争结果的鼓舞，作者在CkNN之上添加了他们自己的对齐模块，使用三元组损失来创建图像和文本的联合嵌入空间。</p><p id="c604" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">“具有一个隐藏层、丢失和批量标准化的两个前馈神经网络用带余量的三重丢失来训练:一个用于图像(<em class="nq"> gᵢ </em>)特征，另一个用于文本(<em class="nq"> gₜ </em>)特征。”</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/c28e26e00c1f3dd070edd0d2bc5f1d77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sBLk34F4tRXpDTnQ6NYlsg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">校准模块(作者图片，来自论文)</p></figure><h1 id="c9a9" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">个人想法</h1><p id="daff" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">看了这篇论文让我觉得深度学习并不是唯一可走的路。当然，这里使用的图像编码器很深奥，但是ResNet已经统治这个领域有一段时间了。实际发生的比对是一个简单的kNN。添加一个MLP，让你得到一个国家的最先进的性能！这是一篇聪明的论文，它采用了im2recipe论文中引入的跨通道概念，而不是改善结果(正如大多数论文一样)，而是从另一方面提出了问题:如果基线太低怎么办？这给了你一个理智的检查。</p><p id="e510" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">总之，本文重新定义了im2recipe任务的基线。新基线是使用非常简单的架构创建的，该架构不是端到端训练的，这简化了模型探索和模型比较。在下一篇文章中，我们将看看如何在另一篇论文中使用这个新的基线来再次提高SoTA性能。敬请期待！</p></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><p id="feb1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这是我开始的一个新系列的第二部分，关于直觉的论文解释。我正在挑选行业中的一个子域，并浏览该域中的论文。如果你喜欢我写的东西，可以考虑订阅或者关注我<a class="ae kv" href="https://www.medium.com/@kunjmehta10" rel="noopener">这里</a>或者在<a class="ae kv" href="http://www.linkedin.com/in/kunjmehta" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>或者<a class="ae kv" href="https://www.twitter.com/@kunjmehta10" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上与我联系！关于我之前文章的代码，请访问我的<a class="ae kv" href="https://github.com/kunjmehta/Medium-Article-Codes" rel="noopener ugc nofollow" target="_blank"> GitHub </a></p><p id="1566" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">论文引用</strong></p><p id="55be" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[1] Mikhail Fain、Andrey Ponikar、Ryan Fox和Danushka Bollegala，“划分和征服跨模态配方检索:从最近邻居基线到SoTA”。2019年更正</p></div></div>    
</body>
</html>