<html>
<head>
<title>Dimensionality Reduction for Linearly Inseparable Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性不可分数据的降维</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dimensionality-reduction-for-linearly-inseparable-data-5030f0dc0f5e#2022-12-20">https://towardsdatascience.com/dimensionality-reduction-for-linearly-inseparable-data-5030f0dc0f5e#2022-12-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="df95" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">基于核主成分分析的非线性降维</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/71f2bed84e0dde262454209dfcc5d03e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ugWbG96fgOUf_cgT6S56eQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/ja/@steve_j?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">斯蒂夫·约翰森</a>在<a class="ae ky" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="1869" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">标准 PCA 适合于<em class="lv">线性</em>维度缩减，因为它在减少数据中的特征数量时进行线性变换。换句话说，标准 PCA 很好地处理<em class="lv">线性可分离的</em>数据，其中不同的类别可以通过绘制直线(在 2D 数据的情况下)或超平面(在 3D 和更高维数据的情况下)来清楚地分离。</p><p id="7827" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">标准 PCA 对于<em class="lv">线性不可分的</em>数据效果不佳，在这些数据中，不同的类别不能通过绘制直线或超平面来清楚地分开，而只能通过使用弯曲的判定边界来分开。</p><p id="1c13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于非线性降维，我们可以使用标准 PCA 的非线性形式<strong class="lb iu"> <em class="lv">核 PCA </em> </strong>。</p><blockquote class="lw"><p id="9bd6" class="lx ly it bd lz ma mb mc md me mf lu dk translated">标准 PCA 和核 PCA 都降低了数据的维数(特征的数量),但是只有核 PCA 可以使数据线性分离，同时仍然降低数据的维数——作者</p></blockquote><h1 id="d1fa" class="mg mh it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">内核和内核技巧</h1><p id="caea" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">内核绝招</em> </strong>是将线性不可分的数据转换到数据线性可分的更高维度的过程。这是通过使用内核实现的。内核是一个转换数据的函数。</p><h1 id="a0c3" class="mg mh it bd mi mj mk ml mm mn mo mp mq jz nd ka ms kc ne kd mu kf nf kg mw mx bi translated">KenelPCA()中的重要超参数</h1><p id="ffc2" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">内核 PCA 是通过使用 Scikit-learn 中的 Kernel PCA()类实现的。使用该类时，用户需要指定 3 个重要的超参数。</p><ul class=""><li id="1f12" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated"><strong class="lb iu"> n_components: </strong>我们想要保留的组件数量。当<code class="fe np nq nr ns b">n_components=None</code>(默认)时，所有组件被保留。阅读<a class="ae ky" rel="noopener" target="_blank" href="/how-to-select-the-best-number-of-principal-components-for-the-dataset-287e64b14c6d">这篇文章</a>可以找到更多关于调整这个超参数的信息。</li><li id="4984" class="ng nh it lb b lc nt lf nu li nv lm nw lq nx lu nl nm nn no bi translated"><strong class="lb iu">内核:</strong>用于 PCA 的内核类型。KernelPCA()类中使用了 4 种类型的核:<em class="lv">、</em>、<em class="lv">、【poly】、<em class="lv">、【rbf】、<em class="lv">、【sigmoid】、</em>。当内核为<em class="lv">‘线性’</em>时，将应用标准(线性)PCA。可以通过使用其他内核之一来执行非线性 PCA。<em class="lv">‘RBF’</em>(径向基函数)是最常见的一种。默认为<em class="lv">‘线性’</em>内核。</em></em></li><li id="9ddb" class="ng nh it lb b lc nt lf nu li nv lm nw lq nx lu nl nm nn no bi translated"><strong class="lb iu"> gamma: </strong>这就是所谓的<em class="lv">‘poly’</em>、<em class="lv">‘RBF’</em>和<em class="lv">‘sigmoid’</em>内核的<em class="lv">内核系数</em>。浮点值可用于 gamma。默认值为<code class="fe np nq nr ns b">None</code>，它使用由(1/特征数)计算的值。阅读<a class="ae ky" rel="noopener" target="_blank" href="/python-implementation-of-grid-search-and-random-search-for-hyperparameter-optimization-2d6a82ebf75c">这篇文章</a>以找到关于调整这个超参数的更多信息。</li></ul><h1 id="ba3c" class="mg mh it bd mi mj mk ml mm mn mo mp mq jz nd ka ms kc ne kd mu kf nf kg mw mx bi translated">应用主成分分析和核主成分分析的例子</h1><h2 id="da5d" class="ny mh it bd mi nz oa dn mm ob oc dp mq li od oe ms lm of og mu lq oh oi mw oj bi translated">生成非线性数据</h2><p id="734a" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">为了解释应用 PCA 和内核 PCA 的过程，我们将使用 Scikit-learn<a class="ae ky" href="https://pub.towardsai.net/7-scikit-learn-utilities-to-generate-artificial-synthetic-data-11624f0d2095#f4f9" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="lv">make _ moons</em></strong></a><em class="lv"/>函数生成<strong class="lb iu"> <em class="lv"> moons 数据</em> </strong>(线性不可分)。</p><p id="996a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">卫星数据包含两个交错的半圆，可以如下显示。</p><pre class="kj kk kl km gt ok ns ol bn om on bi"><span id="225f" class="oo mh it ns b be op oq l or os">import matplotlib.pyplot as plt<br/>plt.figure(figsize=[7, 5])<br/><br/>from sklearn.datasets import make_moons<br/>X, y = make_moons(n_samples=100, noise=None, <br/>                  random_state=0)<br/><br/>plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='plasma')<br/>plt.title('Linearly inseparable data')<br/>plt.savefig("original_data.png")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/1fdea118934546e2d40010f29f9b285f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*Q6v3BgJkd8acHXG6Hob_UQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="4153" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">生成的数据中有 100 个观察值。每个点代表一个观察。数据中有两类。它们由两种不同的颜色(黄色和蓝色)表示。这两个类是线性不可分的。我们不能通过画一条直线来区分这两个阶级。</p><h2 id="5022" class="ny mh it bd mi nz oa dn mm ob oc dp mq li od oe ms lm of og mu lq oh oi mw oj bi translated">对线性不可分的数据应用线性 PCA</h2><p id="2abd" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">在对数据应用核 PCA 之前，我们将首先尝试应用线性 PCA 并查看输出。</p><blockquote class="ou ov ow"><p id="cb61" class="kz la lv lb b lc ld ju le lf lg jx lh ox lj lk ll oy ln lo lp oz lr ls lt lu im bi translated"><strong class="lb iu">示例 1: </strong>通过保持 2 个分量来应用线性 PCA</p></blockquote><p id="154f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，我们通过保留 2 个主成分将线性 PCA 应用于卫星数据。卫星数据的原始维度是二维的。因此，当保持两个(全部)分量时，维数不会降低。</p><pre class="kj kk kl km gt ok ns ol bn om on bi"><span id="d5d7" class="oo mh it ns b be op oq l or os">from sklearn.decomposition import PCA<br/><br/>pca = PCA(n_components=2)<br/>X_pca = pca.fit_transform(X)<br/><br/>plt.figure(figsize=[7, 5])<br/>plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, s=50, cmap='plasma')<br/>plt.title('First 2 components after linear PCA')<br/>plt.xlabel('PC1')<br/>plt.ylabel('PC2')<br/>plt.savefig("linear_PCA_2_components.png")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/a19d256371fd6a6c5aaf513a8fbbb8bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*p6Fnw4_GvWZT_rmc4HPSLw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><blockquote class="lw"><p id="c42a" class="lx ly it bd lz ma pa pb pc pd pe lu dk translated">在通过保留两个分量将线性 PCA 应用于 2 维非线性卫星数据之后，数据的维数没有减少，并且类别仍然不是线性可分的。</p></blockquote></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><blockquote class="ou ov ow"><p id="573d" class="kz la lv lb b lc ld ju le lf lg jx lh ox lj lk ll oy ln lo lp oz lr ls lt lu im bi translated"><strong class="lb iu">示例 2: </strong>通过仅保留一个分量来应用线性 PCA</p></blockquote><p id="3945" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，我们只保留一个主成分，将线性主成分分析应用于卫星数据。卫星数据的原始维度是二维的。所以保留一个分量，维度会减少一半。</p><pre class="kj kk kl km gt ok ns ol bn om on bi"><span id="292f" class="oo mh it ns b be op oq l or os">import numpy as np<br/>from sklearn.decomposition import PCA<br/><br/>pca = PCA(n_components=1)<br/>X_pca = pca.fit_transform(X)<br/><br/>plt.figure(figsize=[7, 5])<br/>plt.scatter(X_pca[:, 0], np.zeros((100,1)), c=y, s=50, cmap='plasma')<br/>plt.title('First component after linear PCA')<br/>plt.xlabel('PC1')<br/>plt.savefig("linear_PCA_1_component.png")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/3f0f12e8f4abb6a5cd18444982b7618c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*IWk9drAkzkIpBX_iwoqf9Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><blockquote class="lw"><p id="f5fa" class="lx ly it bd lz ma pa pb pc pd pe lu dk translated">在通过保留一个分量将线性 PCA 应用于二维非线性卫星数据之后，数据的维数已经减少了一半，但是类别仍然不是线性可分的。</p></blockquote><p id="0cff" class="pw-post-body-paragraph kz la it lb b lc pm ju le lf pn jx lh li po lk ll lm pp lo lp lq pq ls lt lu im bi translated">虽然线性 PCA 可以降低线性不可分数据的维数，但这两类仍然是线性不可分的。</p></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><h2 id="ed27" class="ny mh it bd mi nz oa dn mm ob oc dp mq li od oe ms lm of og mu lq oh oi mw oj bi translated">对线性不可分数据应用核主成分分析</h2><p id="49cc" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">现在，我们将内核 PCA 应用于数据，并查看输出。</p><blockquote class="ou ov ow"><p id="1765" class="kz la lv lb b lc ld ju le lf lg jx lh ox lj lk ll oy ln lo lp oz lr ls lt lu im bi translated"><strong class="lb iu">示例 3: </strong>通过保留 2 个分量来应用内核 PCA</p></blockquote><p id="036a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，我们通过保留 2 个主成分将核 PCA 应用于卫星数据。卫星数据的原始维度是二维的。因此，在应用核主成分分析后，维数不会减少。</p><pre class="kj kk kl km gt ok ns ol bn om on bi"><span id="80b7" class="oo mh it ns b be op oq l or os">from sklearn.decomposition import KernelPCA<br/><br/>kpca = KernelPCA(n_components=2, kernel='rbf', gamma=15)<br/>X_kpca = kpca.fit_transform(X)<br/><br/>plt.figure(figsize=[7, 5])<br/>plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=y, s=50, cmap='plasma')<br/>plt.title('First 2 components after kernel PCA')<br/>plt.axvline(x=0.0, linestyle='dashed', color='black', linewidth=1.2)<br/>plt.xlabel('PC1')<br/>plt.ylabel('PC2')<br/>plt.savefig("kernel_PCA_2_components.png")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/824e8d40aa50f0aeb3dfc4c732a0c074.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*LSgR2tfZyfLzm8bXz4lUKw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><blockquote class="lw"><p id="6456" class="lx ly it bd lz ma pa pb pc pd pe lu dk translated">在通过保留两个分量将核 PCA 应用于 2 维非线性卫星数据之后，数据的维数没有减少*，但是类现在是线性可分的！</p></blockquote><blockquote class="ou ov ow"><p id="5ec7" class="kz la lv lb b lc pm ju le lf pn jx lh ox po lk ll oy pp lo lp oz pq ls lt lu im bi translated"><strong class="lb iu">*幕后:</strong>对线性不可分的数据应用核主成分分析时，核(此处为<em class="it">‘RBF’</em>)函数将原始数据临时投影到一个新的高维特征空间中，在该空间中类变得线性可分。然后，该算法将高维数据投影回二维数据，该二维数据可以绘制成 2D 图，如上述示例所示。这个过程是不可见的。所以，降维过程已经在幕后发生了——来源:<a class="ae ky" rel="noopener" target="_blank" href="/11-dimensionality-reduction-techniques-you-should-know-in-2021-dcb9500d388b">2021 年你应该知道的 11 种降维技术</a>(我自己的文章)</p></blockquote></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><blockquote class="ou ov ow"><p id="bad8" class="kz la lv lb b lc ld ju le lf lg jx lh ox lj lk ll oy ln lo lp oz lr ls lt lu im bi translated"><strong class="lb iu">示例 4: </strong>通过仅保留一个分量来应用内核 PCA</p></blockquote><p id="dc46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们只保留一个主成分，将核主成分分析应用于卫星数据。卫星数据的原始维度是二维的。因此，应用核主成分分析后，维数将减少一半。</p><pre class="kj kk kl km gt ok ns ol bn om on bi"><span id="5f0c" class="oo mh it ns b be op oq l or os">import numpy as np<br/>from sklearn.decomposition import KernelPCA<br/><br/>kpca = KernelPCA(n_components=1, kernel='rbf', gamma=15)<br/>X_kpca = kpca.fit_transform(X)<br/><br/>plt.figure(figsize=[7, 5])<br/>plt.scatter(X_kpca[:, 0], np.zeros((100,1)), c=y, s=50, cmap='plasma')<br/>plt.axvline(x=0.0, linestyle='dashed', color='black', linewidth=1.2)<br/>plt.title('First component after kernel PCA')<br/>plt.xlabel('PC1')<br/>plt.savefig("kernel_PCA_1_component.png")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/5837c8f6ac61e4a617b91b6c925148a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*OLyNqN2IAf1VXpHM2YP_Iw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><blockquote class="lw"><p id="7b3d" class="lx ly it bd lz ma pa pb pc pd pe lu dk translated">在通过保留一个分量将核 PCA 应用于 2 维非线性卫星数据之后，数据的维数已经降低，并且类现在是线性可分的！</p></blockquote><p id="b6d6" class="pw-post-body-paragraph kz la it lb b lc pm ju le lf pn jx lh li po lk ll lm pp lo lp lq pq ls lt lu im bi translated">只有内核 PCA 可以使数据线性分离，同时仍然降低数据的维数！</p></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><h1 id="d1f0" class="mg mh it bd mi mj pr ml mm mn ps mp mq jz pt ka ms kc pu kd mu kf pv kg mw mx bi translated">使用内核 PCA 的限制</h1><ul class=""><li id="821e" class="ng nh it lb b lc my lf mz li pw lm px lq py lu nl nm nn no bi translated">用户应将<em class="lv">【RBF】</em><em class="lv">【poly】</em>和<em class="lv">【sigmoid】</em>内核<em class="lv"> </em>的<strong class="lb iu"><em class="lv">gamma</em></strong>值指定为超参数。它需要实现超参数调谐技术，例如<a class="ae ky" rel="noopener" target="_blank" href="/python-implementation-of-grid-search-and-random-search-for-hyperparameter-optimization-2d6a82ebf75c"> <em class="lv">随机搜索</em>或<em class="lv">网格搜索</em> </a>。</li><li id="2186" class="ng nh it lb b lc nt lf nu li nv lm nw lq nx lu nl nm nn no bi translated">与 PCA 不同，<strong class="lb iu"> <em class="lv"> n_component </em> </strong>超参数不接受浮点值。因此，我们不能通过指定需要由主成分解释的方差来选择最佳的成分数。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><p id="34af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天的帖子到此结束。</p><p id="3142" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您有任何问题或反馈，请告诉我。</p><h2 id="6689" class="ny mh it bd mi nz oa dn mm ob oc dp mq li od oe ms lm of og mu lq oh oi mw oj bi translated">人工智能课程怎么样？</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75"><div class="gh gi pz"><img src="../Images/40dd8bc0fdacdeb8e59632515d294a8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*MFDUghbNyNAGxX8RhzYivQ.png"/></div></a><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd qa">点击图片进入课程！</strong>(作者截图)</p></figure><h2 id="f3b8" class="ny mh it bd mi nz oa dn mm ob oc dp mq li od oe ms lm of og mu lq oh oi mw oj bi translated">阅读下一篇(强烈推荐)</h2><ul class=""><li id="855f" class="ng nh it lb b lc my lf mz li pw lm px lq py lu nl nm nn no bi translated"><strong class="lb iu">2021 年你应该知道的 11 种降维技术</strong></li></ul><div class="qb qc gp gr qd qe"><a rel="noopener follow" target="_blank" href="/11-dimensionality-reduction-techniques-you-should-know-in-2021-dcb9500d388b"><div class="qf ab fo"><div class="qg ab qh cl cj qi"><h2 class="bd iu gy z fp qj fr fs qk fu fw is bi translated">2021 年你应该知道的 11 种降维技术</h2><div class="ql l"><h3 class="bd b gy z fp qj fr fs qk fu fw dk translated">减少数据集的大小，同时尽可能保留变化</h3></div><div class="qm l"><p class="bd b dl z fp qj fr fs qk fu fw dk translated">towardsdatascience.com</p></div></div><div class="qn l"><div class="qo l qp qq qr qn qs ks qe"/></div></div></a></div><ul class=""><li id="cc36" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated"><strong class="lb iu">降维的 11 种不同用途</strong></li></ul><div class="qb qc gp gr qd qe"><a rel="noopener follow" target="_blank" href="/11-different-uses-of-dimensionality-reduction-4325d62b4fa6"><div class="qf ab fo"><div class="qg ab qh cl cj qi"><h2 class="bd iu gy z fp qj fr fs qk fu fw is bi translated">降维的 11 种不同用途</h2><div class="ql l"><h3 class="bd b gy z fp qj fr fs qk fu fw dk translated">整个 ML 都是降维及其应用。让我们看看他们的行动吧！</h3></div><div class="qm l"><p class="bd b dl z fp qj fr fs qk fu fw dk translated">towardsdatascience.com</p></div></div><div class="qn l"><div class="qt l qp qq qr qn qs ks qe"/></div></div></a></div><ul class=""><li id="9ac7" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated"><strong class="lb iu">为数据集选择最佳数量的主成分</strong></li></ul><div class="qb qc gp gr qd qe"><a rel="noopener follow" target="_blank" href="/how-to-select-the-best-number-of-principal-components-for-the-dataset-287e64b14c6d"><div class="qf ab fo"><div class="qg ab qh cl cj qi"><h2 class="bd iu gy z fp qj fr fs qk fu fw is bi translated">如何为数据集选择最佳数量的主成分</h2><div class="ql l"><h3 class="bd b gy z fp qj fr fs qk fu fw dk translated">你应该遵循的六种方法</h3></div><div class="qm l"><p class="bd b dl z fp qj fr fs qk fu fw dk translated">towardsdatascience.com</p></div></div><div class="qn l"><div class="qu l qp qq qr qn qs ks qe"/></div></div></a></div><ul class=""><li id="9e5a" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated"><strong class="lb iu">用 Scikit 学习主成分分析(PCA)</strong></li></ul><div class="qb qc gp gr qd qe"><a rel="noopener follow" target="_blank" href="/principal-component-analysis-pca-with-scikit-learn-1e84a0c731b0"><div class="qf ab fo"><div class="qg ab qh cl cj qi"><h2 class="bd iu gy z fp qj fr fs qk fu fw is bi translated">使用 Scikit-learn 的主成分分析(PCA)</h2><div class="ql l"><h3 class="bd b gy z fp qj fr fs qk fu fw dk translated">无监督机器学习降维算法</h3></div><div class="qm l"><p class="bd b dl z fp qj fr fs qk fu fw dk translated">towardsdatascience.com</p></div></div><div class="qn l"><div class="qv l qp qq qr qn qs ks qe"/></div></div></a></div><ul class=""><li id="73e5" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated"><strong class="lb iu">主成分分析—回答了 18 个问题</strong></li></ul><div class="qb qc gp gr qd qe"><a href="https://rukshanpramoditha.medium.com/principal-component-analysis-18-questions-answered-4abd72041ccd" rel="noopener follow" target="_blank"><div class="qf ab fo"><div class="qg ab qh cl cj qi"><h2 class="bd iu gy z fp qj fr fs qk fu fw is bi translated">主成分分析—回答了 18 个问题</h2><div class="ql l"><h3 class="bd b gy z fp qj fr fs qk fu fw dk translated">一站式解决关于 PCA 的大部分问题</h3></div><div class="qm l"><p class="bd b dl z fp qj fr fs qk fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="qn l"><div class="qw l qp qq qr qn qs ks qe"/></div></div></a></div></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><h2 id="8794" class="ny mh it bd mi nz oa dn mm ob oc dp mq li od oe ms lm of og mu lq oh oi mw oj bi translated">支持我当作家</h2><p id="541b" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">我希望你喜欢阅读这篇文章。如果你愿意支持我成为一名作家，请考虑 <a class="ae ky" href="https://rukshanpramoditha.medium.com/membership" rel="noopener"> <strong class="lb iu"> <em class="lv">注册会员</em> </strong> </a> <em class="lv">以获得无限制的媒体访问权限。它只需要每月 5 美元，我会收到你的会员费的一部分。</em></p><div class="qb qc gp gr qd qe"><a href="https://rukshanpramoditha.medium.com/membership" rel="noopener follow" target="_blank"><div class="qf ab fo"><div class="qg ab qh cl cj qi"><h2 class="bd iu gy z fp qj fr fs qk fu fw is bi translated">通过我的推荐链接加入 Medium</h2><div class="ql l"><h3 class="bd b gy z fp qj fr fs qk fu fw dk translated">阅读 Rukshan Pramoditha(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接…</h3></div><div class="qm l"><p class="bd b dl z fp qj fr fs qk fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="qn l"><div class="qx l qp qq qr qn qs ks qe"/></div></div></a></div><p id="5a66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非常感谢你一直以来的支持！下一篇文章再见。祝大家学习愉快！</p><p id="9f6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="qy qz ep" href="https://medium.com/u/f90a3bb1d400?source=post_page-----5030f0dc0f5e--------------------------------" rel="noopener" target="_blank">鲁克山·普拉莫迪塔</a><br/><strong class="lb iu">2022–12–19</strong></p></div></div>    
</body>
</html>