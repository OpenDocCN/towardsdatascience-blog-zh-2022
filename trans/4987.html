<html>
<head>
<title>Demystifying efficient self-attention</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭开高效自我关注的神秘面纱</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/demystifying-efficient-self-attention-b3de61b9b0fb#2022-11-07">https://towardsdatascience.com/demystifying-efficient-self-attention-b3de61b9b0fb#2022-11-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2f81" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">实用概述</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f0a235ab35e7cbf410b3a8fd5d00c1f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*u2nal_Ez5DAnZT8_"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。人工智能-使用<a class="ae kv" href="https://openai.com/dall-e-2/" rel="noopener ugc nofollow" target="_blank">达尔-E-2 </a>生成</p></figure><h1 id="bd77" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="d5ce" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">Transformer架构[1]对于近年来深度学习领域的一些最大突破至关重要。特别是在自然语言处理(NLP)领域，预训练的自动编码模型(如BERT [2])和自回归模型(如GPT-3 [3])一直在努力超越最先进的技术，并达到类似人类的文本生成水平。Transformer最重要的创新之一是使用关注层作为路由信息的主要方式。</p><p id="a9c3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">顾名思义，注意力的目标是让模型关注输入的重要部分。从人类的角度来看，这是有意义的:当我们看一个输入(例如，一个图像或一个文本)时，有些部分对我们的理解来说比其他部分更重要。我们可以将输入的某些部分相互联系起来，并理解长期的背景。这些对于我们的理解都是必不可少的，注意力机制允许变形金刚模型以类似的方式学习。虽然这已被证明是非常有效的，但注意机制有一个实际问题:它们与输入长度成二次方关系。幸运的是，有很多研究致力于提高注意力的效率。</p><p id="b6b0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这篇博文旨在通过直观的解释，对不同类型的高效注意力提供一个全面的概述。这并不是对已经撰写的每篇论文的完整概述，而是对底层方法和技术的覆盖，并带有深入的示例。</p><h1 id="2813" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">注意力入门</h1><p id="ea20" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在深入研究具体方法之前，让我们先回顾一下自我关注机制的基础知识，并定义一些将在这篇博文中重复使用的术语。</p><p id="8e38" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">自我关注是一种特殊类型的关注。常规注意和自我注意的区别在于，自我注意关注的是一个单一的序列，而不是将输入与输出序列联系起来。它允许模型让序列学习关于它自己的信息。举个例子，就拿“那个人走到河边，他吃了一个三明治”这句话来说吧。与以前的嵌入方法(如TF-IDF和word2vec [4])相比，自我关注允许模型学习“河岸”不同于“金融银行”(上下文相关)。此外，它允许模型学习“他”指的是“那个人”(可以学习依赖性)。</p><p id="beb5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">自我关注可以解释如下。假设我们有一个长度为n的序列x，x中的每个元素都用一个d维向量表示。在NLP的情况下，x将是句子的单词嵌入。x通过三个(训练的)权重矩阵WQ、WK和WV被投影，输出三个矩阵:Q、K和V，所有维度都是n*d。自我关注可以被定义为下面的一般公式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/555409811fcd163fb50cf169e9fb53c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/0*SAJEUjJBD0ENjdGU"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">方程式1: </strong>广义注意力</p></figure><p id="2bdf" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最常用的得分函数是softmax。如[2]所述，取softmax并应用一个比例因子会导致比例点积注意(SDP):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/36a3f717e708c688418195a37002e990.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/0*409pgdw2jvhSHp4d"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">方程式2: </strong>缩放-点积注意</p></figure><p id="f9c6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在这里，输入x的关注度是通过将Q乘以KT(将每个项目与每个其他项目相关联)、应用缩放因子、获取逐行softmax(归一化每个行)、将V中的每个值乘以其计算出的关注度来计算的，因此我们的输出再次是n*d。因此，Q和K用于将每个元素与每个其他元素相关联，而V用于将softmax的输出分配回每个单独的元素。</p><p id="7cf0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">作者认为，对于较大的d_k值，点积变得非常大，这反过来将softmax函数推到梯度非常小的区域。因此，点积通过除以√(d_k)来缩放。请注意，这对计算的复杂性没有影响，因为这是由softmax(QK)计算决定的。由于这个原因，它将被排除在一般公式之外。</p><p id="380b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">正如你可能已经看到的，这个公式有一个问题:Q和K相乘得到一个n*n矩阵。取n*n矩阵的行方式softmax具有O(n)的复杂度。这对于运行时和内存使用都是有问题的，因为n可能非常大。对于多页文档，很快就变得无法计算完整输入的自我关注，这意味着输入必须被截断或分块。这两种方法都去除了自我关注的一个主要好处:长期背景。这种类型的注意力，其中每个项目都与其他项目相乘，被称为“整体注意力”，可以形象化如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/738037d396be157f2d2984e42b329ac5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s_petAre18gviQfbeq76yg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">图一:</strong>全球瞩目。图片作者。</p></figure><p id="f0f2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在这里，对角线中的每个项目(深蓝色)查看其行和列中的所有其他项目(以浅蓝色突出显示)。</p><p id="d02a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为简单起见，以下定义将在下文中使用:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/fcb890427a10293a0b52dcc02f7f9a76.png" data-original-src="https://miro.medium.com/v2/resize:fit:310/0*H0A0YfCb8tylhnwe"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">等式3: </strong> P和A的定义</p></figure><p id="194f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在这里，P指的是Q和K相乘的结果n*n矩阵，A(自我关注矩阵)指的是P的softmax，注意大多数论文使用自己的定义，这可能会有点混乱。</p><p id="8866" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">关于注意力的更详细的解释，我鼓励你去读读<a class="ae kv" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">插图变压器</a>。</p><h1 id="bc37" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">可供选择的事物</h1><p id="fa08" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">降低SDP复杂性的主要假设是，并非输入的所有部分都同等重要，并且一些记号不需要关注其他特定记号。</p><p id="4d0a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了避免计算全球注意力，有几种选择:</p><ul class=""><li id="0691" class="mu mv iq lq b lr mk lu ml lx mw mb mx mf my mj mz na nb nc bi translated"><strong class="lq ir">稀疏注意力:</strong>稀疏注意力方法稀疏化全局注意力矩阵，以减少必须相互关注的标记的数量</li><li id="d529" class="mu mv iq lq b lr nd lu ne lx nf mb ng mf nh mj mz na nb nc bi translated"><strong class="lq ir">矩阵分解</strong>:矩阵分解方法的工作原理是注意力矩阵是低秩的，可以用低秩矩阵进行分解和近似，而不会丢失太多信息。</li><li id="2864" class="mu mv iq lq b lr nd lu ne lx nf mb ng mf nh mj mz na nb nc bi translated"><strong class="lq ir">位置敏感哈希:</strong>位置敏感哈希提供了一种快速计算最近邻搜索的方法。这可以直接应用于关注矩阵，以选择哪些令牌应该相互关注。</li><li id="ced3" class="mu mv iq lq b lr nd lu ne lx nf mb ng mf nh mj mz na nb nc bi translated"><strong class="lq ir">内核关注:</strong>内核关注方法将softmax函数解释为内核，并使用它来更有效地计算自我关注矩阵。</li></ul><p id="57c5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">所有这些选择都以牺牲一些性能为代价，降低了计算复杂度。注意，所有这些方法都试图降低序列长度n的复杂度。为此，所有复杂度都降低到依赖于n的部分。</p><h2 id="1419" class="ni kx iq bd ky nj nk dn lc nl nm dp lg lx nn no li mb np nq lk mf nr ns lm nt bi translated">稀疏的注意力</h2><p id="3a51" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">稀疏注意力方法通过仅考虑n*n自我注意力矩阵p中的计算子集来降低复杂性。其思想是记号不需要注意每一个其他记号，而是可以关注更重要的记号而忽略其他记号。那么问题就变成了:我们如何挑选要关注的令牌？</p><p id="1026" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">局部注意</strong> O(n*W)</p><p id="12f2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">局部注意，也称为窗口式<strong class="lq ir"> / </strong>滑动注意，是一种简单而有效的稀疏化自我注意矩阵的方法。在局部注意中，标记只关注它们的局部邻域或窗口w。因此，不再计算全局注意。通过只考虑W中的令牌，它将复杂度从n*n降低到n*W，这可以如图2所示。</p><p id="b9ce" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">随机注意</strong> O(n*R)</p><p id="ea53" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在随机注意中，标记只注意随机的其他标记。复杂度取决于所选随机记号的数量(R)，它是所有记号的比率。这可以从图2中看到。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/4c6d1abdffb51107637089f509ddc2ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hJPmbM2i1JfO8ifw"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">图2 </strong>:局部注意(左)和随机注意(右)。图片作者。</p></figure><p id="eb66" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">稀疏变压器</strong> O(n√n)</p><p id="cc66" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">稀疏变压器[5]是减少自我关注复杂性的第一个尝试。作者提出了两种稀疏注意模式:步进注意和固定注意，这两种模式都将复杂度降低到O(n√n)。他们的两种注意力类型可以使用以下函数来定义:</p><ul class=""><li id="fd89" class="mu mv iq lq b lr mk lu ml lx mw mb mx mf my mj mz na nb nc bi translated">跨步注意:如果<br/> <em class="nv"> (i+s) &gt; j &gt; ( i-s)或(i-j) mod s = 0 </em>，则第I个位置可以关注第j个位置</li><li id="88a5" class="mu mv iq lq b lr nd lu ne lx nf mb ng mf nh mj mz na nb nc bi translated">固定注意:如果<em class="nv"> floor(j/s) = floor(i/s)或(j mod s) ≥ (s-c) </em>，第I个位置可以注意到第j个位置</li></ul><p id="ca78" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">其中s是步幅(设置为√n ), c是超参数。这些算法的复杂性为O(n*s)，当s设置为√n时，这导致O(n√n)。跨步注意力类似于具有跨步的局部注意力，作者认为这对于从具有周期性结构的数据(如图像或音乐)中学习非常重要。但是，对于没有周期性结构的数据(如文本)，这种模式可能无法将信息路由到远处的项目。固定注意力是解决这个问题的方法。它让一些项目关注整个列，并创建一个“摘要”传播给其他项目。两种不同模式的可视化如图3所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/f917ffb3e5d03536207b827848ed1f76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fvv9neksf-xlEeXR"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">图3 </strong>:跨步注意(左)和固定注意(右)。图片作者。</p></figure><p id="e87f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">龙前</strong> O(n)</p><p id="a99d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">Longformer [6]使用了滑动(或局部)、扩张滑动和全局注意的组合。扩张滑动注意是基于扩张CNN的想法。扩大滑动注意的目标是逐渐增加每一层的感受野。作者提出在较低层使用局部注意，窗口W较小(可以看作是间隙d为0的扩张滑动窗口注意)，在较高层增加W和d。</p><p id="b5c5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">仅针对特定令牌添加全局注意。将哪些令牌设为全局的选择由用户决定。分类的一个合理选择是使[CLS]标记全局化，而对于QA任务，所有问号标记都可以全局化。他们算法的复杂度是(n*W + s*n)，它与序列长度n成线性比例，因此简化为O(n)。</p><p id="5309" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">请注意，Longformer的实现需要一个定制的CUDA内核，因为现代GPU针对密集矩阵乘法进行了优化。作者提供了一个定制的CUDA内核，允许在PyTorch和Tensorflow中的GPU上有效计算他们提出的稀疏矩阵乘法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/10dba6f94e02b867b8a4ddd5654ad62d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eyK2NGNj5OJLS46m"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">图3 </strong>:局部注意(左)、扩张滑动注意(中)、全局注意(右)。图片作者。</p></figure><h2 id="3b28" class="ni kx iq bd ky nj nk dn lc nl nm dp lg lx nn no li mb np nq lk mf nr ns lm nt bi translated">矩阵分解</h2><p id="ecd7" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在矩阵分解(或分解)方法中，矩阵P被假定为低秩的，这意味着矩阵中的所有项并不是彼此独立的。所以可以用更小的矩阵来分解和近似。这样，n*n矩阵可以简化为n*k(其中k&lt; n), which allows us to compute A (the result of the softmax) much more efficiently.</p><p id="b04f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">l前一个</strong> O(n)</p><p id="4fe6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">Linformer [7]的作者提出使用注意力矩阵的低秩分解来达到O(n)的复杂度。作者首先从经验上表明，当应用奇异值分解(SVD)时，A可以从它的前几个最大奇异值中恢复，这表明它是低秩的。然后，他们使用约翰逊-林登斯特劳斯引理(JL)证明A可以近似为低秩矩阵γ，误差非常小，该引理表示:</p><blockquote class="nw nx ny"><p id="77c9" class="lo lp nv lq b lr mk jr lt lu ml ju lw nz mm lz ma oa mn md me ob mo mh mi mj ij bi translated">高维空间中的一组点可以被投影到低维空间中，同时(几乎)保持点之间的距离。</p></blockquote><p id="7215" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">作者指出，计算每个自我关注矩阵的奇异值分解增加了额外的复杂性。相反，作者在V和K之后添加了两个线性投影矩阵，这有效地将原始(n*d)矩阵投影到更低(k*d)维矩阵，其中K是降低的维度。这可以被形象化，如图4所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/7d085316d4423b03b38dd1f383751bb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*T4VgiIlY3VC46-xv"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">图4 </strong>:标准立正(上)和非标准立正(下)。图片作者。</p></figure><p id="7a8b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">他们提出的新的注意力公式如等式3所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/060439eb24fbf6e6bcca870370db1786.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/0*IEvusSRG6t1kb_7p"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">等式3: </strong>前注意函数</p></figure><p id="d90b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这里，Ei和Fi是两个线性投影矩阵。请注意，要将A从n*n减少到°( n * K ),只需将K投影到维度K。由于V仍然是n*d，因此V也被投影到维度K，以确保最终的输出矩阵是n*d(这是下一层的预期维度)。</p><p id="4e57" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这实际上是通过线性投影减少了序列长度n。这对于NLP是有意义的，因为一个句子中的所有单词并不是(同等地)相关的。</p><p id="8e70" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后一步是选择k的值。作者表明dlog(d)的值对于k是足够的，这导致O(nk)的复杂度。因为d相对于输入长度n不增加，所以自我注意机制的复杂度变为O(n)。</p><p id="5d21" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">纽约变压器</p><p id="e255" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">ny strm former[8]使用ny strm方法来近似自我注意矩阵。想法是将矩阵P重写为一个由四部分组成的矩阵:B是m*m，其中m是某个数字&lt; n), C, D, and E. This is shown on the left in Figure 5:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/fc847151ca646571c2a48650cc8106bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UrC70sdySHgRslYG"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">图5 </strong>:矩阵近似的Nystrom方法解释。图片作者。</p></figure><p id="5bd2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">根据nyströ方法，p可以近似为P̃，方法是用DB⁺C代替e(其中B⁺是b的摩尔-彭罗斯伪逆)，然后可以进一步简化，如图5所示。原始的n*n矩阵现在被分解为两个n*m矩阵和一个m*m矩阵的乘积。这极大地减少了计算量，因为只有选定的K行和Q列需要相乘来创建这个分解(而不是所有的行和列)。</p><p id="c974" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了更好地理解这是如何工作的，让我们使用nyströ方法近似子矩阵e中的单个元素eᵢ,ⱼ。假设P是一个5*5的矩阵，我们选择B作为我们的第一个单元格(1，1)，C作为第一行(2，1到5，1)，D作为第一列(1，2到1，5)。这可以被形象化，如图6所示。假设我们想知道e₃,₃的值，它是c₃,₃和d₃,₃的乘积(在SDP中，这将是q和k中单个值的乘积)。在P̃，我们不再有c₃,₃和d₃,₃的实际乘积，但是我们知道c₃,₁and d₁,₃(as的值，这些值在我们选择的行和列中。为了逼近e₃,₃，我们将c₃,₁and d₁,₃的值乘以b的倒数。正如您所看到的，我们可以用q中的一行和k中的一列的结果来逼近e中的任何值</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/a606b202f18c82f83627efdde1a6b3a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C2DDxq7Bx-EzH22fDd1y8A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">图6 </strong>:矩阵近似的Nyströ方法示例。图片作者。</p></figure><p id="f62e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">虽然本例中选择了Q和K的第一行和第一列，但也可以对多行和多列进行采样，称为“界标”，本文中就是这么做的。使用分段平均值选择这些界标，这类似于局部平均池(将输入分成段并取每个段的平均值)。</p><p id="9337" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然而，仍然存在一个问题:为了计算关注矩阵a，需要首先计算p，因为softmax运算通过取p中整行的内容来归一化a的元素。由于目标是避免计算p，所以作者提出了一个变通方法:他们对P̃的三个子矩阵进行softmax运算，并将它们相乘，如等式3所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/250e0af97a285e941dd1aa81734afc64.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/0*pNTkKtdb_UBUXdjp"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">方程3: </strong>纽斯特罗姆注意力函数</p></figure><p id="c30e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这里，Z*是B⁺.的近似值</p><p id="fc54" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">虽然这在技术上是不允许的，因为softmax是一个非线性操作，但作者表明，这种方法提供的近似仍然是足够的。</p><h2 id="366b" class="ni kx iq bd ky nj nk dn lc nl nm dp lg lx nn no li mb np nq lk mf nr ns lm nt bi translated">局部敏感散列法</h2><p id="14af" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">位置敏感哈希(LSH)是一种可用于高效近似最近邻搜索的技术。LSH的思想是可以选择哈希函数，使得对于高维空间p和q中的任意两点，如果p接近q，那么hash(p) == hash(q)。使用该属性，所有点都可以被划分到散列桶中。这使得更有效地找到任何点的最近邻居成为可能，因为只需要计算到相同散列桶中的点的距离。在自我关注的情况下，这可以用于通过对Q和K应用LSH来加速P的计算，并且在应用LSH之后仅将彼此接近的项相乘，而不是执行完整的计算QK。</p><p id="028a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">重整器</strong> O(nlog(n))</p><p id="edcd" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">《改革家》[9]的作者是第一个提出利用LSH进行有效的自我关注的人。他们注意到，由于softmax由最大的元素支配，所以对于Q中的每个查询qi，qi只需要关注K中最接近qi的键(或者在相同的散列桶中)。</p><p id="fd38" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了更好地理解这是如何工作的，我们来看一个例子。假设我们有一个包含许多点的二维空间，如图7左侧所示。在自我关注的情况下，这些点就是p中的项目。颜色代表靠近在一起的点。为了将项目划分到散列桶中，通过原点绘制了许多随机超平面，如图7中的右侧所示。在这种情况下，绘制了两个超平面，称为H1和H2。任何超平面都有一个正边(1)和一个负边(0)。然后，根据项目出现在每个超平面的哪一侧，将它们放入散列桶(在本例中为4个)。因此，哈希桶的数量由绘制的超平面的数量来定义。在这样做之后，项目只需要计算到它们自己的散列桶内的项目的距离(或者，在自我关注的上下文中，关注相同散列桶内的项目)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/3c80d82a20814546fce4082ab79e44de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4blZdlONQqFKbsZt"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">图7</strong>:LSH的例子。图片作者。</p></figure><p id="9584" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">正如在图7右侧的结果散列桶中可以看到的那样，很接近的项目可能仍然在不同的散列桶中结束。为了减轻这种情况，可以执行多轮散列，并将每个值分配给最常出现的散列。然而，这确实增加了算法的复杂性。作者表明，通过8轮散列，该模型达到了类似于全局注意力模型的性能。</p><p id="f77b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">作者使用了一种称为角度LSH [10]的变体，它使用余弦距离来计算任意两点之间的距离。它们表明，两个非常接近的点很有可能会在同一个桶中结束。</p><p id="3311" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">将点分成桶后，按桶对点进行排序。但是，有些桶可能比其他桶大。最大的存储桶仍将主导内存需求，这是一个问题。出于这个原因，作者将桶分成固定的块，因此内存需求取决于块的大小。请注意，项目可能不会与桶中的其他项目在同一个块中结束。这些项目可以处理它们应该结束的块中的所有项目，但是不能处理它们自己(这给复杂性增加了一个小的恒定成本)，如图7的最后一行所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/688f3d3c85ed106120dcd1ea333ec087.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IXXbLR5oJnjXPWP2"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">图7</strong>:LSH注意事项说明。图片来源:[9]。</p></figure><p id="c4cc" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这有效地将复杂度降低到O(n log n)。需要注意的重要一点是，由于不依赖于n而从复杂度中去除了8轮散列，因此引入了较大的12⁸常数，这实际上导致重整器仅在输入序列非常长(&gt; 2048)时变得更高效。</p><h2 id="8285" class="ni kx iq bd ky nj nk dn lc nl nm dp lg lx nn no li mb np nq lk mf nr ns lm nt bi translated">核心注意力</h2><p id="2a5a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">核是这样一种函数，它以某个低维空间中的两个向量x和y的点积作为输入，并返回某个高维空间中点积的结果。这可以概括为一个函数K(x，y) =φ(x)ᵀφ(y)，其中k是核函数，φ是从低维到高维空间的映射。在机器学习的背景下，支持向量机(SVM)是一个众所周知的例子。特别是对于有效的自我关注，内核方法的工作原理是Softmax可以被解释为内核并被重写，这样我们就可以避免显式计算关注矩阵a。</p><p id="28d0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">表演者</strong> O(n)</p><p id="030d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">执行者[11]是基于一种叫做<em class="nv">的机制，通过正正交随机特征</em>(或FAVOR+)。这个想法是，我们可以使用核方法来近似softmax函数。</p><p id="ca96" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">通常，当应用核方法时，我们希望在高维空间中计算点积。这可以通过使用适当的核函数K来实现(例如我们在核SVM中所做的)。然而，执行者做相反的事情:我们已经知道我们的函数k是什么(非线性的softmax)，我们想要找到φ，以便我们可以计算φ(x)ᵀφ(y(它是线性的)。我们可以将它形象化，如图8所示。在左手边，我们看到我们的L*L矩阵A乘以V(注意，这只是公式softmax(QKT)*V)，作者将序列长度称为L而不是n)。相反，所提出的方法使用φ来直接计算φ(Q)= Q’和φ(K)= K’，这允许我们首先将K和V相乘，并且避免了矩阵a的高成本计算</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/bd1af6491636a291f6ac4ff462e27a24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hTMxUZfBVV2DBpQi"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">图8 </strong>:好感+关注的说明。图片来源:[11]。</p></figure><p id="3514" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">主要的研究问题变成:我们如何找到φ？这个想法是基于随机傅立叶特征[12]。作者指出，大多数内核可以使用通用函数建模，如等式4所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/74bf46eee85b468828ba2d94ddaf4e84.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/0*d_hjg7ffwjCoWy_w"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">等式4: </strong>用于内核建模的通用函数</p></figure><p id="cded" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这里，h是x的某个函数，m是定义近似精度的参数，ω₁…ωₘ是从某个分布d中抽取的随机向量(因此随机部分更倾向于+)，而f₁…fₗ是确定性函数。m越高，近似值越好，因为它定义了绘制的ω数。</p><p id="56ce" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">作者证明了Softmax核可以通过选择等式5中所示的值来近似:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/54c0f2111ca5059fc4cedb150d733eca.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/0*ogc5Cauy8MHsVw4V"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">等式5: </strong>用于逼近Softmax的函数</p></figure><p id="d52b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，我们可以通过φ函数传递我们的Q和K，并得到结果矩阵的点积。这样做的结果就好像我们先将它们相乘，然后取softmax。由于我们可以通过φ独立地传递Q和K，我们现在可以先将K和V相乘。</p><p id="ce6c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然而，还有一个问题。与softmax不同，sin和cos可以具有负值，这导致当softmax的实际值接近0时，近似值的方差变大。由于很多自我关注值接近0，这是个问题。为此，作者建议使用不同的函数，即等式6中所示的函数，这些函数仅输出正值(因此有利于+的正部分)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/5af8f77374e6c32689055d5bf4fbb608.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/0*0WbD8xEqYL4etm4U"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">等式6: </strong>用于逼近Softmax (FAVOR+ version)的函数</p></figure><p id="2d0e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，作者解释说，确保ωs正交会导致更小的方差(因此正交部分更有利于+)。</p><h2 id="11be" class="ni kx iq bd ky nj nk dn lc nl nm dp lg lx nn no li mb np nq lk mf nr ns lm nt bi translated">自我关注的替代品</h2><p id="8d3e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">很明显，有很多研究致力于提高点产品注意力的效率。然而，还有另一种选择:完全不使用自我关注，而是使用一种更简单的方法在我们的令牌之间共享信息。最近有多篇论文提出了这个想法([13]、[14]、[15])。我们将讨论一个，因为所有这些论文的总体思路都非常相似。</p><p id="7133" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> FNet </strong> O(n)</p><p id="2bb2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">FNet [15]是一种替代的变换器架构，它用离散傅里叶变换(DFT)完全取代了自关注模块。因此，除了前馈层之外，不再有可学习的参数。DFT将信号分解成其组成频率。其定义如公式7所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/0cf5579d1c09779435163b35fa23c35a.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/0*cat-hedhjIdI9sKG"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">等式7: </strong> DFT函数</p></figure><p id="e2de" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">其中N是组件的数量。当N为无穷大时，我们可以精确地创建原始信号。在NLP的上下文中，我们的信号是一个令牌序列。实际上，每个组件n都包含一些关于输入序列中每个标记的信息。</p><p id="0660" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">他们方法的有趣之处不在于他们使用DFT，而在于他们应用线性变换来混合他们的令牌。他们还尝试了一种线性编码器，这与合成器模型的工作方式非常相似，甚至是一种完全随机的编码器。虽然线性编码器的性能稍高，但它有可学习的参数，因此比FNet慢。BERT-Base在GLUE上的平均分仍然高得多，但是他们报告说训练时间提高了大约7倍。因为有许多可能的线性变换，所以有一个有趣的开放式研究问题，即什么是最适合变压器的。</p><h1 id="fd5a" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">基准</h1><p id="0fb9" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">虽然本帖中讨论的所有论文都报告了它们关于输入序列长度n的理论复杂性，但在实践中，由于较大的常数(如Reformer)或低效的实现，一些论文可能仍然不切实际。为此，<a class="ae kv" href="https://pypi.org/project/xformers/" rel="noopener ugc nofollow" target="_blank"> Xformers </a>被用于计算许多方法的不同序列长度的内存使用和运行时间。注意，并不是所有讨论的方法都在Xformers中实现，不幸的是，BlockSparse在我的GPU(RTX 3090)上不工作。</p><p id="91db" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">长序列长度(512，1024，2048): </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/538cb0ad4ecb82b1d64f40801b891d85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gdP2YkwYo3gdzcxd"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">图9 </strong>:各种高效注意力方法的长序列长度的内存使用和运行时使用。图片作者。</p></figure><p id="2c12" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">短序列长度(128，256): </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/1e60044797442e5d008d0d13ed95c45c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qP0gIM3pAJ96pyfC"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mq">图10 </strong>:各种高效注意方法的短序列长度的内存使用和运行时使用。图片作者。</p></figure><p id="3384" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">显然，对于更长的序列，所有方法都明显比SDP更有效。虽然这里比较的所有注意机制(除了SDP)都与序列长度成线性比例关系，但有趣的是，由于常数和其他比例因子，这些机制之间仍然存在明显的差异。值得注意的是，Linformer的伸缩性不如其他方法。另一个有趣的结果是Nystromformer的内存使用和运行时间。虽然它的伸缩性很好，如序列长度(512、1024、2048)的图表所示，但对于短序列(128和256)，它实际上是最低效的方法。这可能是由于所选标志的数量，如[8]中所建议的，其值保持在64。</p><p id="18d6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">有趣的是，对于最长512的序列长度，SDP的性能与其他方法非常相似。唯一明显比其他方法更有效的方法是FNet(傅立叶混合注意)。它几乎完全独立于序列长度，同时没有重要的常数需要考虑。</p><h1 id="7026" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="9059" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">考虑到基于变压器的模型日益增长的相关性，有效的自我关注仍然是一个活跃的研究领域。虽然它们看起来令人望而生畏，但大多数技术实际上可以追溯到您可能已经熟悉的更一般的数学概念。希望这篇博文既是对大多数相关技术的介绍，也是对它们的解释，帮助你更深入地了解这个领域。</p><h1 id="6aef" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">参考</h1><p id="6196" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">[1] Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan N. Gomez、Lukasz Kaiser和Illia Polosukhin。你只需要关注，2017。</p><p id="a60a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[2] Devlin，j .，Chang，m .，Lee，k .，&amp; Toutanova，K. BERT:用于语言理解的深度双向转换器的预训练，2018年。</p><p id="f7ee" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[3] Brown，T. B .，Mann，b .，Ryder，n .，Subbiah，m .，Kaplan，j .，Dhariwal，p .，Neelakantan，a .，Shyam，p .，Sastry，g .，Askell，a .，Agarwal，s .，Krueger，g .，Henighan，t .，Child，r .，Ramesh，a .，Ziegler，D. M .，Wu，j .，Winter，c .，Hesse，c .。。阿莫代伊博士。语言模型是很少出手的学习者，2020。</p><p id="be56" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[4]托马斯·米科洛夫、程凯、格雷戈·科拉多和杰弗里·迪恩。向量空间中单词表示的有效估计，2013。</p><p id="3d3c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[5]蔡尔德，r .，格雷，s .，拉德福德，a .，&amp; Sutskever，I .用稀疏变压器生成长序列，2019。</p><p id="e931" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[6] Iz Beltagy，Matthew E. Peters和Arman Cohan。Longformer:长文档转换器，2020</p><p id="f9a5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[7]王，李，B. Z，Khabsa，m .，方，h .，，马，h .林前:自我注意与线性复杂性，2020 .</p><p id="06ae" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[8] Xiong，y .，Zeng，z .，Chakraborty，r .，Tan，m .，Fung，g .，Li，y .，&amp; Singh，V. Nystromformer:一种基于Nystrom的自我注意近似算法.2021.</p><p id="3491" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[9]尼基塔·基塔耶夫、祖卡斯·凯泽和安塞尔姆·列夫斯卡娅。改革者:高效的变压器，2020。</p><p id="113b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[10]亚历山大·巴甫洛夫·安多尼、彼得·因迪克、蒂伊斯·拉霍文、伊利亚·拉赞施泰因和路德维希·施密特。角距离的实用和最佳lsh，2015。</p><p id="391f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[11] Choromanski，k .，Likhosherstov，v .，Dohan，d .，Song，x .，Gane，a .，Sarlos，t .，Hawkins，p .，Davis，j .，Mohiuddin，a .，Kaiser，l .，Belanger，d .，Colwell，l .，&amp; Weller，a .，重新思考表演者的注意力，2020年。</p><p id="c8df" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[12]阿里·拉希米，本杰明·雷希特。大规模内核机器的随机特性，2007。</p><p id="c4ed" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[13] Tay，y .，Bahri，d .，Metzler，d .，Juan，d .，Zhao，z .，&amp; Zheng，c .合成器:在变形金刚模型中反思自我注意。2020.</p><p id="f889" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[14]托尔斯提欣、霍尔斯比、科列斯尼科夫、拜尔、李、翟、安特辛纳、杨、施泰纳、基泽斯、乌兹科雷特、卢契奇、多索维茨基、《混合建筑:全建筑展望》，2021年。</p><p id="9fe4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[15] Ainslie，j .，Eckstein，I .，&amp; Ontanon，S. FNet:用傅立叶变换混合令牌，2021年。</p></div></div>    
</body>
</html>