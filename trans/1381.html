<html>
<head>
<title>From raw text to model prediction in under 30 lines of Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从原始文本到模型预测，不到30行Python代码</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-raw-text-to-model-prediction-in-under-30-lines-of-python-32133d853407#2022-04-06">https://towardsdatascience.com/from-raw-text-to-model-prediction-in-under-30-lines-of-python-32133d853407#2022-04-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f71f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">快速探索NLP管道的快速指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/19961ea55146c350680c774b2847f9c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ipx75P6SOupMDS2z"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@alpridephoto?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">андрейсизов</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><h2 id="4d9a" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">介绍</h2><p id="02ee" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">自然语言处理(NLP)是处理人类语言数据的机器学习的子领域。在能够用机器学习模型进行预测之前，处理人类文本通常涉及标准的预处理步骤，如数据清理和将文本转换为数字向量。</p><p id="965c" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">在这个故事中，我们将带您浏览一个示例，解释如何使用<a class="ae kv" href="https://github.com/tvdboom/ATOM" rel="noopener ugc nofollow" target="_blank"> ATOM </a>库快速处理大量文本文档，并将它们分类到预先确定的主题中。ATOM是一个开源的Python包，旨在帮助数据科学家加快机器学习管道的探索。如果你想对图书馆有一个温和的介绍，请阅读这个故事。</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h2 id="e456" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">加载数据集</h2><p id="afca" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">我们开始获取文本文档并初始化atom实例。我们将要使用的数据是<a class="ae kv" href="https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset" rel="noopener ugc nofollow" target="_blank"> 20个新闻组数据集</a>，可以使用sklearn轻松下载。该数据集包含20个主题的大约18000篇新闻文章，为了加快计算速度，我们只使用了其中的5篇。目标是预测每篇文章的主题。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="b920" class="kw kx iq my b gy nc nd l ne nf">import numpy as np<br/>from atom import ATOMClassifier<br/>from sklearn.datasets import fetch_20newsgroups</span><span id="2729" class="kw kx iq my b gy ng nd l ne nf"># Load the dataset (get only 5 topics)<br/>X, y = fetch_20newsgroups(<br/>    return_X_y=True,<br/>    categories=[<br/>        'alt.atheism',<br/>        'sci.med',<br/>        'comp.windows.x',<br/>        'misc.forsale',<br/>        'rec.autos',<br/>    ],<br/>)</span><span id="4bcd" class="kw kx iq my b gy ng nd l ne nf"># atom expects a 2-dimensional array, so reshape to (n_articles, 1)<br/>X = np.array(X).reshape(-1, 1)</span><span id="e52b" class="kw kx iq my b gy ng nd l ne nf"># Initialize atom<br/>atom = ATOMClassifier(X, y, test_size=0.2, verbose=2)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/172e07fb95a8358ae57355217bc4a9cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*pY24N1ux0AEEsuEplETZUA.png"/></div></figure><p id="96a1" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">atom中的数据存储在熊猫数据帧中。注意，我们拥有的唯一特性(包含文章)被自动调用<code class="fe ni nj nk my b">corpus</code>。这很重要，因为atom的所有<a class="ae kv" href="https://tvdboom.github.io/ATOM/latest/user_guide/nlp/" rel="noopener ugc nofollow" target="_blank"> NLP方法</a>只应用于数据集中具有特定名称的列。这种机制允许atom将包含文本语料库的数据集与其他非文本特征结合起来(我们不会在本文中使用)。使用<code class="fe ni nj nk my b">dataset</code>属性检查数据。</p><p id="32f8" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.dataset</code></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/7973bca4af3243ff7f08b0cb727c6bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gGJBqaBOUrv5uLPiYtGsXw.png"/></div></div></figure></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h2 id="988f" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">文本清理</h2><p id="302f" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">让我们看看第一篇文章。</p><p id="251a" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.corpus[0]</code></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/9f00e737ab588c9c351cf687fa82c0e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*owgaE9C234GnHD51e-uJfw.png"/></div></div></figure><p id="2d42" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">注意它的混乱。文本数据很少干净。无论是从网站上搜集来的还是从纸质文档中推断出来的，通常都填充了与模型无关的信息，比如电子邮件地址、数字、html标签等..我们可以使用atom的<a class="ae kv" href="https://tvdboom.github.io/ATOM/latest/API/ATOM/atomclassifier/#textclean" rel="noopener ugc nofollow" target="_blank"><em class="nn">text clean</em></a><em class="nn"/>方法从新闻文章中去除这种“噪音”。该方法对语料库应用以下变换:</p><ul class=""><li id="5277" class="no np iq lu b lv ml ly mm lf nq lj nr ln ns mk nt nu nv nw bi translated">将unicode字符解码为ascii表示形式</li><li id="6a85" class="no np iq lu b lv nx ly ny lf nz lj oa ln ob mk nt nu nv nw bi translated">将所有字符转换为小写</li><li id="4229" class="no np iq lu b lv nx ly ny lf nz lj oa ln ob mk nt nu nv nw bi translated">从文本中删除电子邮件地址</li><li id="6c76" class="no np iq lu b lv nx ly ny lf nz lj oa ln ob mk nt nu nv nw bi translated">从文本中删除URL链接</li><li id="2827" class="no np iq lu b lv nx ly ny lf nz lj oa ln ob mk nt nu nv nw bi translated">从文本中删除HTML标签</li><li id="abd8" class="no np iq lu b lv nx ly ny lf nz lj oa ln ob mk nt nu nv nw bi translated">从文本中删除表情符号</li><li id="edc1" class="no np iq lu b lv nx ly ny lf nz lj oa ln ob mk nt nu nv nw bi translated">从文本中删除数字</li><li id="45d4" class="no np iq lu b lv nx ly ny lf nz lj oa ln ob mk nt nu nv nw bi translated">删除文本中的标点符号</li></ul><p id="42ab" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.textclean()</code></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/91d06fa8e0eb331ee8176c531838b382.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*hvbAHe5528apJ6SVeTv2ZA.png"/></div></figure><p id="65d4" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">使用<code class="fe ni nj nk my b">drops</code>属性来检查每篇文章删除了什么。</p><p id="f514" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.drops</code></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/c890894a7c3b2854cdd77a39fa4b3cb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZL73L5_-PmHN4OoTDuE0bw.png"/></div></div></figure><p id="0c70" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">现在看看第一篇。</p><p id="860a" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.corpus[0]</code></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/cfc5fb9a7a7371c8cf702e619442a690.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7tKpW87yw5kHi91i_5gwNw.png"/></div></div></figure></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h2 id="b420" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">标记化</h2><p id="eebf" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">下一步是将文章转换成令牌序列。在这种情况下，单词。我们需要这样做有两个原因:能够使用像词干化或词汇化这样的算法，这些算法要求文档由标记组成，以便知道应该考虑哪些单独的单词；并且能够将文本映射成模型可以接受的数字。</p><p id="466a" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.tokenize()</code></p><p id="aa3f" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">现在每篇文章都由一系列单词组成。</p><p id="f72a" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.corpus[0][:7]</code></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/7d68a2413676d792d99168d5c81ad880.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*Yoqel37LEfBUG0-lSFkLMQ.png"/></div></figure><p id="5b7d" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">提示:</strong>使用atom的<a class="ae kv" href="https://tvdboom.github.io/ATOM/latest/API/plots/plot_wordcloud/" rel="noopener ugc nofollow" target="_blank"><em class="nn">plot _ word cloud</em></a>方法对语料库中最常见的术语进行可视化处理。</p><p id="55e9" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.plot_wordcloud()</code></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/3e4305693962bcd96a5c42c9c1796764.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*zGOxzg4kL2GPMv7rGokkEw.png"/></div></figure></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h2 id="8dd5" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">正常化</h2><p id="5dab" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">规范化是将单词列表转换为更统一的标准的过程。这有助于减少模型必须处理的不同信息量，从而提高效率。像变元化这样的规范化技术的目标是将一个单词的屈折形式和派生相关形式简化为一个共同的基本形式，例如universal → universe或running → run。</p><p id="f138" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">我们可以通过删除所有停用词来进一步降低文章的复杂性。停用词是任何自然语言中最常见的词，例如英语中的“the”、“a”、“is”。使用atom的<a class="ae kv" href="https://tvdboom.github.io/ATOM/latest/API/ATOM/atomclassifier/#textnormalize" rel="noopener ugc nofollow" target="_blank">文本<em class="nn">规格化</em> </a>方法可以实现规格化和停用词移除。</p><p id="09c7" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.textnormalize(stopwords="english", lemmatize=True)</code></p><p id="c9ad" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">还是那句话，我们来看看第一篇文章的前7个字。请注意,“从”和“在”不再存在。</p><p id="f57e" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.corpus[0][:7]</code></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/ade3e4b0b6c8e462fc23d7df7d04ecc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*njMCJgVrFcfKgyW5MS4M5Q.png"/></div></figure></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h2 id="6d75" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">N-grams</h2><p id="9cce" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">有时，单词本身的意思和与相邻单词组合在一起的意思不同。例如，当单词“york”直接跟在单词“new”后面时，它的意思与不跟在后面时完全不同。这两个词的组合被称为二元模型。使用<code class="fe ni nj nk my b">plot_ngrams</code>方法检查语料库中哪些两个单词的组合是最常见的。</p><p id="807b" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.plot_ngrams(ngram=2)</code></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/f4caf73fd647cec80bc29e82cdc77124.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tcgF9T0UwyQLp6Czgb6Wxw.png"/></div></div></figure><p id="7fb1" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">Atom的<a class="ae kv" href="https://tvdboom.github.io/ATOM/latest/API/ATOM/atomclassifier/#tokenize" rel="noopener ugc nofollow" target="_blank"/></p><p id="5143" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.tokenize(bigram_freq=200)</code></p><p id="4350" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">二元模型频率为200意味着如果二元模型在语料库中至少出现那么多次，就认为它是二元模型。使用<code class="fe ni nj nk my b">bigrams</code>属性查看创建了哪些二元模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/f4a0621b9affad3e67223de020724937.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*Y8F8vAG6YocToyWe2VrDrA.png"/></div></figure></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h2 id="55be" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">…向量化…</h2><p id="ff29" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">文本数据不能直接馈送给算法本身，因为它们中的大多数期望具有固定大小的数字特征向量，而不是具有可变长度的文本文档中的单词。矢量化是将一组文本文档转换成数字特征向量的一般过程。</p><p id="73d1" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">在大型语料库中，一些单词可能非常常见，但携带的关于文档实际内容的有意义信息非常少。如果我们将计数直接输入到分类器中，那些非常频繁的词将会掩盖更罕见、但更有趣的词的频率。使用<a class="ae kv" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> TF-IDF </a>策略将计数特征重新加权为浮点值。创建的列以它们正在计数的单词命名。</p><p id="5447" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.vectorize(strategy="tfidf")</code></p><p id="d6d5" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">由于在完整的语料库中有许多单词，并且每个单词都有自己的列，所以数据集的维度增加了很多。</p><p id="f32e" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.shape</code></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/3b7f15a98611e382877a492b1598504b.png" data-original-src="https://miro.medium.com/v2/resize:fit:210/format:webp/1*VHcGuT_QwJMHL11sc45skg.png"/></div></figure><p id="a57d" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">为了处理这个庞大的数据集，矢量化方法默认返回稀疏数组的数据帧。</p><p id="8a2c" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.dataset.dtypes</code></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/138b05db5d9dee2b379becd748cebe9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*hbOWe4hSHxQijP4vIHEj4w.png"/></div></figure><p id="117f" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">信息:</strong>您可以使用该方法的<code class="fe ni nj nk my b">return_sparse</code>参数来改变这种行为。请注意，当语料库包含许多单词时，使用return_sparse=False会使转换非常缓慢，并占用大量内存。</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h2 id="1ba9" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">建模和结果</h2><p id="2126" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">要为我们的数据选择正确的模型，我们需要检查哪些模型具有对稀疏数据帧的本地支持。</p><p id="1f3a" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.available_models()</code></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/607abefac39caa242edf9c26e936ce13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jdhe8DpBx3XlauYRxL3NSw.png"/></div></div></figure><p id="11dc" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">看看第17行的<code class="fe ni nj nk my b">accepts_sparse</code>栏。看起来<a class="ae kv" href="https://tvdboom.github.io/ATOM/latest/API/models/rf/" rel="noopener ugc nofollow" target="_blank">随机森林(RF) </a>很适合我们的任务。</p><p id="e8ab" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.run(models="RF", metric="f1_weighted")</code></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/11cef7da4a170a081d3db6021719aa4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*YXuI42curMr8ghIgadepIQ.png"/></div></figure><p id="eb28" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">就这样，我们训练了一个模型来对文本数据进行预测。使用以下工具快速评估各种指标的结果:</p><p id="b2c2" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.evaluate()</code></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/6bda227526fed75c213bbf10fa6314c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*7l7IjNFGCFO-Qq5TpOl1hg.png"/></div></div></figure><p id="1e1a" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">该模型似乎总体得分较高。使用atom的<a class="ae kv" href="https://tvdboom.github.io/ATOM/latest/user_guide/plots/" rel="noopener ugc nofollow" target="_blank">绘图方法</a>进一步分析结果。</p><p id="ba15" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.plot_confusion_matrix()</code></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/8e412de0386165b68dd683deedba51a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n30UOmmevJAY1aDftpAhlQ.png"/></div></div></figure><p id="f4bd" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">ATOM还集成了<a class="ae kv" href="https://shap.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> shap </a>库，可以根据Shapley值快速洞察图形。例如，在对第一篇文章进行预测时，查看哪些单词对算法的影响最大。</p><p id="8081" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.decision_plot(index=0, target=atom.predict(0), show=10)</code></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/266b92ddddf351ee0d88b2b423bcfac6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pOyQkIPrnQ02ywSFPvqYtA.png"/></div></div></figure><p id="5461" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">或者检查所有文章中哪些词对预测第一个主题最重要:无神论。</p><p id="a40f" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe ni nj nk my b">atom.beeswarm_plot(target=0, show=15)</code></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/e19a9acb82fe20b5d3971e45d43317b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*xB0xZ8ObGF2Sg_kapz7NVg.png"/></div></div></figure></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="3ed8" class="oq kx iq bd ky or os ot lb ou ov ow le jw ox jx li jz oy ka lm kc oz kd lq pa bi translated">结论</h1><p id="fe03" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">我们展示了如何使用ATOM包快速探索自然语言数据集。首先，我们将原始文本文档转换成向量，之后，我们训练并评估了一个分类器。以及不到30行代码中的所有内容！</p><p id="c38f" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">关于ATOM的更多信息，请看一下软件包的文档<a class="ae kv" href="https://tvdboom.github.io/ATOM/" rel="noopener ugc nofollow" target="_blank">。对于bug或功能需求，请不要犹豫，在</a><a class="ae kv" href="https://github.com/tvdboom/ATOM" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上提出问题或给我发邮件。</p><p id="fcee" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">相关故事:</p><ul class=""><li id="db1a" class="no np iq lu b lv ml ly mm lf nq lj nr ln ns mk nt nu nv nw bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/atom-a-python-package-for-fast-exploration-of-machine-learning-pipelines-653956a16e7b">https://towards data science . com/atom-a-python-package-for-fast-exploration-of-machine-learning-pipelines-653956 a16 e7b</a></li><li id="be44" class="no np iq lu b lv nx ly ny lf nz lj oa ln ob mk nt nu nv nw bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/how-to-test-multiple-machine-learning-pipelines-with-just-a-few-lines-of-python-1a16cb4686d">https://towards data science . com/how-to-test-multiple-machine-learning-pipelines-with-just-the-less-lines-of-python-1 a16 CB 4686d</a></li><li id="6c01" class="no np iq lu b lv nx ly ny lf nz lj oa ln ob mk nt nu nv nw bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/from-raw-data-to-web-app-deployment-with-atom-and-streamlit-d8df381aa19f">https://towards data science . com/from-raw-data-to-we B- app-deployment-with-atom-and-streamlit-d 8df 381 aa 19 f</a></li><li id="e659" class="no np iq lu b lv nx ly ny lf nz lj oa ln ob mk nt nu nv nw bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/exploration-of-deep-learning-pipelines-made-easy-e1cf649892bc">https://towards data science . com/exploration-of-deep-learning-pipelines-made-easy-e1cf 649892 BC</a></li><li id="0bdf" class="no np iq lu b lv nx ly ny lf nz lj oa ln ob mk nt nu nv nw bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/deep-feature-synthesis-vs-genetic-feature-generation-6ba4d05a6ca5">https://towards data science . com/deep-feature-synthesis-vs-genetic-feature-generation-6 ba 4d 05 a6 ca 5</a></li></ul><p id="bbd7" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">参考资料:</p><ul class=""><li id="4b98" class="no np iq lu b lv ml ly mm lf nq lj nr ln ns mk nt nu nv nw bi translated"><a class="ae kv" href="http://qwone.com/~jason/20Newsgroups/" rel="noopener ugc nofollow" target="_blank"> 20个新闻组数据集</a>是机器学习技术文本应用实验的一个流行数据集，最初由Ken Lang为他的<a class="ae kv" href="http://qwone.com/~jason/20Newsgroups/lang95.bib" rel="noopener ugc nofollow" target="_blank"><em class="nn">news weaver:Learning to filter net news</em></a>论文收集。</li><li id="c5e2" class="no np iq lu b lv nx ly ny lf nz lj oa ln ob mk nt nu nv nw bi translated">这个故事中所有没有说明的图片都是作者创作的。</li></ul></div></div>    
</body>
</html>