<html>
<head>
<title>Intro to PyTorch: Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch 简介:第 1 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/intro-to-pytorch-part-1-663574fb9675#2022-03-08">https://towardsdatascience.com/intro-to-pytorch-part-1-663574fb9675#2022-03-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="47d7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">PyTorch 图书馆简介</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/92801613a39771c131cf360df1915772.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nu_DrSKvJQcssg83Yd1kaQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">亨特·哈里特在<a class="ae ky" href="https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="4be2" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍介绍</h1><p id="8183" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">PyTorch 基于 Torch 库，是最受机器学习从业者欢迎的深度学习框架之一。PyTorch 受欢迎的一些原因是它的易用性、动态计算图，以及它比 Tensorflow 等其他框架更“Pythonic 化”的事实。</p><p id="968a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在本教程中，我们将检查 PyTorch 的基本组件，然后使用 CIFAR10 数据集完成一个图像分类任务。因为 PyTorch 加载了大量的特性，并且有大量的方法来应用它们，所以这显然不是全面的。这篇文章的目的是介绍这个包以及您将使用的一些组件，并提供一些资源以便您可以继续您的旅程。</p><h1 id="64bc" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">张量</h1><p id="7760" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">PyTorch 的核心组件是<strong class="lt iu">张量</strong>数据结构。如果您熟悉 NumPy(如果您不熟悉，请查看我在<a class="ae ky" rel="noopener" target="_blank" href="/intermediate-python-numpy-cec1c192b8e6">关于数据科学</a>中的文章), PyTorch tensors 类似于 NumPy ndarrays，主要区别在于它们支持 CUDA，并且构建为在硬件加速器上运行，如 GPU。张量拥有的另一个重要特征是，它们针对<a class="ae ky" href="https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html" rel="noopener ugc nofollow" target="_blank">自动微分</a>进行了优化，这是被称为<strong class="lt iu">反向传播</strong>的神经网络训练算法的基础。这两个优化对于深度学习至关重要:</p><ul class=""><li id="9592" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated">深度学习通常包含的大量数据、特征和训练迭代需要 GPU 的大规模并行架构在合理的时间内进行训练</li><li id="4922" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">通过反向传播的训练需要有效和精确的区分</li></ul><blockquote class="ng nh ni"><p id="a153" class="lr ls nj lt b lu mn ju lw lx mo jx lz nk mp mc md nl mq mg mh nm mr mk ml mm im bi translated">PyTorch 还支持分布式计算，将训练过程扩展到单台机器之外！</p></blockquote><p id="cd61" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">说了这么多，我们再来看看张量 API！</p><h1 id="6ec8" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">与张量一起动手</h1><blockquote class="ng nh ni"><p id="c08c" class="lr ls nj lt b lu mn ju lw lx mo jx lz nk mp mc md nl mq mg mh nm mr mk ml mm im bi translated"><strong class="lt iu">注意</strong>:如果您想继续这里的内容，请先跳到设置部分，这样您就可以用 Colab 编写代码了</p></blockquote><p id="559f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们可以从<strong class="lt iu"> Python 列表</strong>中自然地创建张量:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="6b7c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这也自然地适用于<strong class="lt iu"> Numpy ndArrays </strong>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="b134" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">就像在 NumPy(和 Tensorflow)中一样，我们可以用随机值、全 1 或全 0 初始化张量。只需提供<code class="fe np nq nr ns b">shape</code>(如果您想要指定数据类型，则提供<code class="fe np nq nr ns b">dtype</code>):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="a456" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">别忘了张量不一定是二维的！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="a574" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一个新的张量可以从现有的张量中产生。因此，如果我们愿意，我们可以创建一个新的零张量，它具有与我们创建的<code class="fe np nq nr ns b">A_tensor</code>相同的属性(形状和数据类型):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="c8d5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">或者您可能想要随机的浮点值:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="3516" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">想要张量的属性吗？</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="1eb5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">创建张量是好的，但是真正的乐趣开始于我们可以开始操作它们和应用数学运算的时候。已经内置了大量简洁的张量运算，所以我们肯定没有时间一一介绍。相反，我会给你一个<a class="ae ky" href="https://pytorch.org/docs/stable/torch.html" rel="noopener ugc nofollow" target="_blank">链接，让你更详细地查看它们</a>，并且只列出几个:</p><ul class=""><li id="2f37" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated">矩阵乘法</li><li id="3e91" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">计算特征向量和特征值</li><li id="c221" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">整理</li><li id="5be6" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">索引、切片、连接</li><li id="652a" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">海明窗(不确定这是什么，但是听起来很酷！！)</li></ul><h1 id="213b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">数据集和数据加载器模块</h1><h2 id="1665" class="nt la it bd lb nu nv dn lf nw nx dp lj ma ny nz ll me oa ob ln mi oc od lp oe bi translated">资料组</h2><p id="4ac0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">像 Tensorflow 一样，PyTorch 的包中包含了许多数据集(包括<a class="ae ky" href="https://pytorch.org/text/stable/datasets.html" rel="noopener ugc nofollow" target="_blank">文本</a>、<a class="ae ky" href="https://pytorch.org/vision/stable/datasets.html" rel="noopener ugc nofollow" target="_blank">图像</a>和<a class="ae ky" href="https://pytorch.org/audio/stable/datasets.html" rel="noopener ugc nofollow" target="_blank">音频</a>数据集)。本教程的深度学习部分将使用这些内置图像数据集之一:<code class="fe np nq nr ns b">CIFAR10</code>。这些数据集非常常见，并在 ML 社区中被广泛记录，因此它们非常适合于原型和基准模型，因为您可以将您的模型的性能与其他人用他们的模型实现的性能进行比较。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="5c0d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这样，如果数据集有标注或分类，您可以快速查看这些标注或分类的列表:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="c5e0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">显然，作为机器学习实践者，内置数据集并不是你需要的全部。尽管这个过程比仅仅导入一个数据集更复杂，但是用 PyTorch 创建自己的数据集是相当容易和灵活的。这已经超出了本文的范围，但是我将在不久的将来发布一个创建数据集的深入指南。</p><h1 id="f262" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">数据加载器</h1><p id="129e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">遍历数据集将逐个遍历每个样本，因此 PyTorch 为我们提供了 DataLoader 模块，可以轻松地在我们的数据集中创建迷你批处理。<code class="fe np nq nr ns b">DataLoader</code>允许我们指定<code class="fe np nq nr ns b">batch_size</code>以及混洗数据:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="775e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因此，在你的深度学习工作流程中，你会希望通过迷你批处理中的<code class="fe np nq nr ns b">DataLoader</code>将你的数据输入到你的模型中进行训练。</p><p id="87eb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">教程的深度学习部分将演示如何使用<code class="fe np nq nr ns b">DataLoader</code>并将其输入神经网络。</p><p id="2778" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在我们进入深度学习之前，最后一个重要的功能是设置设备。当您想要在 GPU 上训练时，您可以检查是否有 GPU 可供 PyTorch 使用:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="9de3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">PyTorch 默认为 CPU，因此即使手头有 GPU，您仍然需要指定要使用 GPU 进行训练。如果你确定你的 GPU 是可用的，你可以使用。to(“cuda”)在您的张量和模型上。否则，您可以考虑将设备变量设置为任何可用的设备:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="42e5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你使用 Google Colab，你将可以免费使用 GPU(除非你想订阅)。说到 Colab，让我们继续进行分类任务的设置！</p><h1 id="885f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">设置</h1><p id="8aae" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本教程中，我们将使用 Google Colab。Colab 一直是我所有机器学习项目的首选，因为在我看来，没有比它更简单的设置了。显然，一些项目将需要不同的设置，但对于较小的项目和教程，你真的不能击败 Colab 的免费 GPU 访问和环境，其中已经包括像 PyTorch，NumPy，Scikit-Learn 这样的软件包。</p><p id="3a27" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">首先，导航到<a class="ae ky" href="https://colab.research.google.com" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>页面，用你的 Google 账户登录。<code class="fe np nq nr ns b">File &gt; New notebook</code>。将顶部的笔记本名称更改为<code class="fe np nq nr ns b">pytorchIntro.ipynb</code>，或者您喜欢的其他名称。默认情况下，Colab 不会给你一个 GPU 访问的实例，所以你必须指定你想要使用的 GPU:在顶部，转到<code class="fe np nq nr ns b">Runtime &gt; Change runtime type &gt; Hardware accelerator &gt; Select "GPU" &gt; Save</code>。现在你有一个 GPU 来训练你的模型！</p><blockquote class="ng nh ni"><p id="25e5" class="lr ls nj lt b lu mn ju lw lx mo jx lz nk mp mc md nl mq mg mh nm mr mk ml mm im bi translated">如果您对将要使用的 GPU 感到好奇，请键入<code class="fe np nq nr ns b">!nvidia-smi</code>并通过点击该行左侧的 Play 按钮或按 Shift+Enter 来执行该行。如果您想要的只是 GPU 设备，也可以运行<code class="fe np nq nr ns b">!nvidia-smi -L</code>:</p></blockquote><pre class="kj kk kl km gt of ns og oh aw oi bi"><span id="9cad" class="nt la it ns b gy oj ok l ol om">GPU 0: Tesla T4 (UUID: GPU-7619bc40-f58c-a507-3911-58907fbd2721)</span></pre><p id="ad97" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在您已经有了 Colab，并且有一个 GPU 准备好训练您的模型，让我们来看看代码。</p><h1 id="3f8c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">进口</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="556f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">将这些导入添加到第一行，并执行该行。这些是我们将使用的主要 PyTorch 模块，以及一些支持导入。当我们使用它们的时候，我们会更详细地讨论它们。</p><h1 id="2fe4" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">资料组</h1><h1 id="5c3b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><a class="ae ky" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR-10 数据集</a></h1><p id="bd04" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">该数据集由 60，000 幅 32x32 彩色图像组成，所有图像都被标记为 10 个类别中的一个。训练集是 50，000 幅图像，而测试集是 10，000 幅图像。</p><p id="ffd2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是一个很好的来自家庭资源的数据集的可视化:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/67d7ffdab4a5700cfb124476baeb0ee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*wKT36GluUwBSHA9BnU3Beg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="oo">来源:</em><a class="ae ky" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"><em class="oo">https://www.cs.toronto.edu/~kriz/cifar.html</em></a></p></figure><p id="d231" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这个项目的目标是建立一个模型，可以准确地将图像分类为 10 种分类之一。</p><h1 id="9330" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">加载数据集</h1><p id="aecf" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">所以我们从 torchvision 导入了 CIFAR10，现在我们需要下载实际的数据集，并准备将其加载到神经网络中。</p><p id="f64e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">首先，由于我们应该在将图像提供给模型之前对它们进行归一化，所以我们将定义一个<code class="fe np nq nr ns b">transform</code>函数，并在创建训练和测试数据变量时使用<code class="fe np nq nr ns b">torchvision.transforms.Normalize</code>到<a class="ae ky" href="https://en.wikipedia.org/wiki/Normalization_(image_processing)" rel="noopener ugc nofollow" target="_blank">归一化</a>我们所有的图像。<code class="fe np nq nr ns b">Normalize</code>方法将期望的<strong class="lt iu">平均值</strong>和<strong class="lt iu">标准偏差</strong>作为参数，由于这些是彩色图像，因此应该为每个(R、G、B)颜色通道提供一个值。</p><p id="8776" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们将这里的值设置为 0.5，因为我们希望图像数据的值接近 0，但是还有其他更精确的归一化方法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="0747" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在我们可以在 transform 参数中使用我们的 transform 函数，这样 PyTorch 就会将它应用到整个数据集。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="2436" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">既然我们已经下载并规范化了数据集，我们可以使用 PyTorch DataLoader 准备好将它提供给神经网络，在这里我们可以定义<code class="fe np nq nr ns b">batch_size</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="8ca9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><code class="fe np nq nr ns b">DataLoader</code>是可迭代的，所以让我们通过检查一次迭代的维度来看看<code class="fe np nq nr ns b">train_dataloader</code>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="8612" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里 X 是图像，y 是标签。我们设置了<code class="fe np nq nr ns b">batch_size = 4</code>,所以通过我们的<code class="fe np nq nr ns b">train_dataloader</code>的每一次迭代都是 4 个 32×32 图像和它们的 4 个相应标签的小批量。</p><p id="689c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在让我们看看数据集中的一些例子。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/cc4619738d2d86f85e0f24e9efb87982.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*wJnTORrJGo1kmB-UrOntrg.jpeg"/></div></figure><p id="1929" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在我们可以看到一些图像和它们相应的标签。通常，您会希望在继续进行模型构建之前进行更彻底的数据探索和分析，但是由于这只是 PyTorch 的一个介绍，我们将继续构建和训练模型。</p><h1 id="f43e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">定义基础模型</h1><p id="2f9c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">让我们建立一个神经网络。</p><p id="3f87" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">首先，我们将定义我们的模型类，并将其命名为<code class="fe np nq nr ns b">NeuralNetwork</code>。我们的模型将是 PyTorch <a class="ae ky" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html" rel="noopener ugc nofollow" target="_blank"> nn 的子类。模块</a>，它是 PyTorch 中所有神经网络模块的基类。</p><p id="bf23" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因为我们的数据集中有彩色图像，所以每个图像的形状是<code class="fe np nq nr ns b">(3, 32, 32)</code>，3 个 RGB 颜色通道中的每一个都是 32×32 的张量。由于我们的初始模型将由完全连接的层组成，我们将需要<a class="ae ky" href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html" rel="noopener ugc nofollow" target="_blank"> nn。Flatten() </a>我们输入图像数据。我们的展平方法将输出一个具有 3072 (32 x 32 x 3)个节点的线性层。<code class="fe np nq nr ns b">nn.Linear()</code>分别以输入神经元的数量和输出神经元的数量作为自变量(<code class="fe np nq nr ns b">nn.Linear(1024 in, 512 out)</code>)。从这里您可以添加<code class="fe np nq nr ns b">Linear</code>层和<code class="fe np nq nr ns b">ReLU</code>层到您的心满意足！我们模型的输出是 10 个 logits，对应于我们数据集中的 10 个类。</p><p id="9784" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在我们定义了模型的结构之后，我们将定义向前传递的顺序。由于我们的模型是一个简单的序列模型，我们的<code class="fe np nq nr ns b">forward</code>方法将非常简单。<code class="fe np nq nr ns b">forward</code>方法将从输入<code class="fe np nq nr ns b">Tensors</code>计算输出<code class="fe np nq nr ns b">Tensor</code>。</p><p id="5377" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你愿意，你可以简单地打印出定义好的<code class="fe np nq nr ns b">model</code>，这样你就可以得到一个结构的概要。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><h1 id="845b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">损失函数和优化器</h1><p id="87f3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">由于这是一个分类问题，我们将使用交叉熵损失函数。提醒一下，当模型输出介于 0 和 1 之间的预测概率值时，交叉熵计算对数损失。因此，当预测的概率偏离真实值时，损失会迅速增加(如果预测更有把握，错误的预测会受到更多惩罚)。下图显示了预测值越来越接近真实值时损失函数的行为。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/fea53aa4bcd658ed9e3635ec2585e477.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*P4R6AETKK_fkdEULaRyYPg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="oo">图:y 轴代表损失，x 轴是真值为 1 时的预测值。可以看到，随着预测值趋近于 1，损耗趋近于 0。预测值越接近 0，损失值越高。</em></p></figure><p id="8f5d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">有了 PyTorch，我们只需使用<code class="fe np nq nr ns b">CrossEntropyLoss()</code>。对于其他 ML 任务，如果更合适，可以使用不同的<a class="ae ky" href="https://pytorch.org/docs/stable/nn.html#loss-functions" rel="noopener ugc nofollow" target="_blank">损失函数</a>。对于我们的优化算法，我们将使用随机梯度下降，这是在<a class="ae ky" href="https://pytorch.org/docs/stable/optim.html" rel="noopener ugc nofollow" target="_blank"> torch.optim 包</a>中实现的，以及其他优化器，如 Adam 和 RMSprop。我们只需要传递我们模型的参数，以及学习率<code class="fe np nq nr ns b">lr</code>。如果您想在模型优化中使用动量或重量衰减，您可以将其传递给<code class="fe np nq nr ns b">SGD()</code>优化器以及<code class="fe np nq nr ns b">momentum</code>和<code class="fe np nq nr ns b">weight_decay</code>参数(都默认为 0)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><h1 id="e41a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">定义训练循环</h1><p id="0129" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这里我们定义了我们的<code class="fe np nq nr ns b">train()</code>函数，在训练过程中我们将传递<code class="fe np nq nr ns b">train_dataloader</code>、<code class="fe np nq nr ns b">model</code>、<code class="fe np nq nr ns b">loss_fn</code>和<code class="fe np nq nr ns b">optimizer</code>作为参数。<code class="fe np nq nr ns b">size</code>变量是整个训练数据集的长度(50k)。在下一行，<code class="fe np nq nr ns b">model.train()</code>是一个 PyTorch <code class="fe np nq nr ns b">nn.Module</code>方法，它将模型设置为训练模式，启用您在训练期间想要的某些行为(例如，退出、批量规范等。).相比之下(当我们定义测试函数时你会看到)，如果你想测试你的模型性能，你可以使用<code class="fe np nq nr ns b">model.eval()</code>。接下来，我们将遍历每个小批量，指定我们希望使用带有<code class="fe np nq nr ns b">to(device)</code>的 GPU。我们将小批量输入到模型中，计算损耗，然后反向传播。</p><h1 id="77c9" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">反向传播和训练进度输出</h1><p id="678c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">对于反向投影步骤，我们需要首先运行<code class="fe np nq nr ns b">optimizer.zero_grad()</code>。这将在开始反向投影之前将梯度设置为零，因为我们不想在后续过程中累积梯度(这种行为在某些情况下可能是需要的，例如 RNNs，您需要累积梯度)。<code class="fe np nq nr ns b">loss.backward()</code>使用损失来计算梯度，然后我们使用<code class="fe np nq nr ns b">optimizer.step()</code>来更新权重。最后，我们可以打印出训练过程的更新，输出每 2000 个训练样本后计算出的损失。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><h1 id="f3ed" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">定义测试方法</h1><p id="dad6" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在训练模型之前，让我们实现测试函数，这样我们可以在每个历元之后评估我们的模型，并在测试集上输出准确度。测试方法的最大区别是，我们使用<code class="fe np nq nr ns b">model.eval()</code>将模型设置为测试模式，而<code class="fe np nq nr ns b">torch.no_grad()</code>将禁用梯度计算，因为我们在测试期间不使用反向传播。最后，我们计算了测试集的平均损失和总体精度。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><h1 id="6482" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">训练我们的模型</h1><p id="3e0f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在我们已经加载并预处理了数据集，构建了神经网络，定义了损失函数/优化器/训练循环…我们准备好训练了！指定您想要训练模型的<code class="fe np nq nr ns b">epochs</code>的数量。每个历元将经历一个<code class="fe np nq nr ns b">train</code>循环，每 2000 个样本输出一次进度，然后在测试集上<code class="fe np nq nr ns b">test</code>模型，在每个历元后输出测试集上的精度和损耗。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/90cc5f0d59bcbe3a8d43554fafd34bf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*wb0uwnIPqpqPaNWakLqM2Q.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="oo">这是每个训练时段的输出结果</em></p></figure><h1 id="e08e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">保存和加载模型</h1><p id="4ace" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">训练完成后，如果您想保存您的模型用于推理，请使用<code class="fe np nq nr ns b">torch.save()</code>。将<code class="fe np nq nr ns b">model.state_dict()</code>作为第一个参数传递；这只是一个 Python 字典对象，它将层映射到它们各自的学习参数(权重和偏差)。对于第二个参数，命名您保存的模型(使用<code class="fe np nq nr ns b">.pth</code>或<code class="fe np nq nr ns b">.pt</code>扩展保存 PyTorch 模型是常见的约定)。如果您希望将此参数保存在特定位置，也可以为其指定完整路径。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="c95e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">当你想加载你的模型进行推理时，用<code class="fe np nq nr ns b">torch.load()</code>抓取你保存的模型，用<code class="fe np nq nr ns b">load_state_dict</code>映射学习到的参数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><h1 id="521a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">评估模型</h1><p id="00ad" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">您可以遍历<code class="fe np nq nr ns b">test_dataloader</code>来检查带有标签的图像样本。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/567486bf9717423151b5800f353f1b89.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*VtEYWHjGDn_r44j-9uXrUg.jpeg"/></div></figure><p id="4a88" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后将其与我们模型的预测标签进行比较，以预览其性能:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="ec2e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">所以我们可以看到，我们的模型似乎在学习分类！让我们看看我们模型的性能数字。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="66a9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> 53% </strong>的准确率不是最先进的，但是比随机猜测或者只预测一类要好得多，所以我们的模型肯定学到了一些！:)</p><p id="1612" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">接下来，我们可以快速检查它在分类每个类时的表现:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="9378" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">所以现在我们对模型的性能有了更好的了解:猫和鸟的图像对网络来说更难分类。</p><h1 id="9c4c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">未完待续…</h1><p id="0ff8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">显然，像我们在本教程中构建的全连接网络通常不用于图像分类。在本教程的第 2 部分，我们将更多地关注 PyTorch 中的性能优化:</p><ul class=""><li id="16ac" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated">使用细胞神经网络进行图像分类</li><li id="e706" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">超参数调谐</li><li id="53ca" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">数据扩充</li><li id="93ef" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">迁移学习</li></ul><p id="e181" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我希望你喜欢，并学到了一点。如果你想了解更多，PyTorch 有一些优秀的<a class="ae ky" href="https://pytorch.org/docs/stable/index.html" rel="noopener ugc nofollow" target="_blank">文档</a>，所以我鼓励你去看看！感谢阅读！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/af46451a816f9f4f565ac60cfd0360c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*LG6Eebzdawmcm6YQ4YbuEg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="oo"> PyTorch 挺牛逼的:)</em></p></figure></div></div>    
</body>
</html>