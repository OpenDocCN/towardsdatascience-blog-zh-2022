<html>
<head>
<title>DeepAR: Mastering Time-Series Forecasting with Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DeepAR:通过深度学习掌握时间序列预测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deepar-mastering-time-series-forecasting-with-deep-learning-bc717771ce85#2022-11-14">https://towardsdatascience.com/deepar-mastering-time-series-forecasting-with-deep-learning-bc717771ce85#2022-11-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d38c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">亚马逊的自回归深度网络</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a9e9c2d40968caac7ec700bd66e2cacf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wsu-W7QAk1NV738pJAuxsQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">创造了稳定的扩散[1]</p></figure><p id="0ae4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">几年前，时间序列模型只能处理单一序列。</strong></p><p id="2656" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，如果我们有多个时间序列，一种选择是为每个序列创建一个模型。或者，如果我们可以将我们的数据“列表化”,我们可以应用梯度增强的树模型——即使在今天也非常有效。</p><p id="fa3f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第一个可以本地处理多个时间序列的模型是<strong class="la iu">DeepAR【2】</strong>，这是由<strong class="la iu">亚马逊</strong>开发的<em class="lu">自回归递归网络</em>。</p><p id="cfc4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将深入了解<em class="lu"> DeepAR </em>如何工作，以及为什么它是时序社区的一个里程碑。</p><blockquote class="lv lw lx"><p id="7049" class="ky kz lu la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">如果你想了解受<strong class="la iu"> DeepAR </strong>启发的其他深度学习模型，请查看这篇文章:</p></blockquote><div class="mb mc gp gr md me"><a rel="noopener follow" target="_blank" href="/the-best-deep-learning-models-for-time-series-forecasting-690767bc63f0"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd iu gy z fp mj fr fs mk fu fw is bi translated">时间序列预测的最佳深度学习模型</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">关于时间序列和深度学习你需要知道的一切</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">towardsdatascience.com</p></div></div><div class="mn l"><div class="mo l mp mq mr mn ms ks me"/></div></div></a></div><h1 id="b779" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">什么是 DeepAR</h1><blockquote class="nl"><p id="02cf" class="nm nn it bd no np nq nr ns nt nu lt dk translated">DeepAR 是第一个将深度学习与传统概率预测相结合的成功模型。</p></blockquote><p id="41c0" class="pw-post-body-paragraph ky kz it la b lb nv ju ld le nw jx lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">让我们看看为什么<em class="lu"> DeepAR </em>脱颖而出:</p><ul class=""><li id="6185" class="oa ob it la b lb lc le lf lh oc ll od lp oe lt of og oh oi bi translated"><strong class="la iu">多时间序列支持:</strong>模型针对多个时间序列进行训练，学习进一步提高预测准确性的全局特征。</li><li id="6e43" class="oa ob it la b lb oj le ok lh ol ll om lp on lt of og oh oi bi translated"><strong class="la iu">额外协变量:</strong> <em class="lu"> DeepAR </em>允许额外特性(协变量)。例如，如果你的任务是温度预报，你可以包括<code class="fe oo op oq or b">humidity-level</code>、<code class="fe oo op oq or b">air-pressure</code>等。</li><li id="851f" class="oa ob it la b lb oj le ok lh ol ll om lp on lt of og oh oi bi translated"><strong class="la iu">概率输出:</strong>该模型利用<strong class="la iu">分位数损失</strong>来输出预测区间，而不是进行单一预测。</li><li id="e5f3" class="oa ob it la b lb oj le ok lh ol ll om lp on lt of og oh oi bi translated"><strong class="la iu">“冷”预测:</strong>通过从数以千计潜在共享一些相似性的时间序列中学习，<em class="lu"> DeepAR </em>可以为很少或根本没有历史的时间序列提供预测。</li></ul><h1 id="bb43" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">DeepAR 的 LSTMs</h1><p id="f66f" class="pw-post-body-paragraph ky kz it la b lb os ju ld le ot jx lg lh ou lj lk ll ov ln lo lp ow lr ls lt im bi translated">DeepAR 使用 LSTM 网络来创建概率输出。</p><p id="f6c3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">长短期记忆网络</em> </strong> <em class="lu"> ( </em> LSTMs)在众多的时间序列预测模型架构中使用:例如，我们可以使用:</p><ul class=""><li id="4dc2" class="oa ob it la b lb lc le lf lh oc ll od lp oe lt of og oh oi bi translated">普通 LSTMs</li><li id="a69a" class="oa ob it la b lb oj le ok lh ol ll om lp on lt of og oh oi bi translated">多层 LSTMs</li><li id="3ffb" class="oa ob it la b lb oj le ok lh ol ll om lp on lt of og oh oi bi translated">带 CNN 的 LSTMs</li><li id="b4ab" class="oa ob it la b lb oj le ok lh ol ll om lp on lt of og oh oi bi translated">带<em class="lu">时间 2 秒</em>的 LSTMs</li><li id="52d2" class="oa ob it la b lb oj le ok lh ol ll om lp on lt of og oh oi bi translated">编码器-解码器拓扑中的 LSTMs</li><li id="ca59" class="oa ob it la b lb oj le ok lh ol ll om lp on lt of og oh oi bi translated">带<em class="lu">注意</em><strong class="la iu">【3】</strong>(<strong class="la iu">图 1 </strong>)的编码器-解码器拓扑中的 LSTMs</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/02b2216a2a242b4eb3dc4433859e7d39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WPci3VkVIfyxw53g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oy">图 1: </strong>谷歌神经机器翻译——GNMT 架构(<a class="ae oz" href="https://arxiv.org/pdf/1609.08144.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="42ba" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，虽然<em class="lu">变形金刚</em>确实在 NLP 领域占据主导地位，但在与时间序列相关的任务中，它们并没有决定性地胜过 LSTMs。主要原因是 LSTMs 更擅长处理本地时态数据。</p><p id="95de" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">关于<strong class="la iu">循环网络 vs 变压器的更多信息，</strong>查看这篇<a class="ae oz" href="/towards-data-science/deep-learning-no-lstms-are-not-dead-20217553b87a" rel="noopener ugc nofollow" target="_blank">文章</a>。</p><h1 id="9a9c" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">DeepAR —建筑</h1><p id="d160" class="pw-post-body-paragraph ky kz it la b lb os ju ld le ot jx lg lh ou lj lk ll ov ln lo lp ow lr ls lt im bi translated">与之前的模型相反，<em class="lu"> DeepAR </em>使用 LSTMs 的方式有点不同:</p><p id="9aee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">DeepAR 没有使用 lstm 直接计算预测，而是利用 lstm 来参数化高斯似然函数。即估计高斯函数的<code class="fe oo op oq or b"><em class="lu">θ = (μ, σ)</em></code>参数(<em class="lu">表示</em>和<em class="lu">标准</em>偏差)。</p><p id="7737" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">图 2 </strong>和<strong class="la iu">图 3 </strong>显示了<em class="lu"> DeepAR </em>在<em class="lu">训练</em>和<em class="lu">推理</em>模式下的架构概况:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/e1b61311931671bd14df886144b3da78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/0*n_Cjywheoon3LjtI.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oy">图 2: </strong>训练期间 DeepAR 中的数学运算(<a class="ae oz" href="https://arxiv.org/pdf/1704.04110.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="d1a2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">先说培训。假设我们在时间序列<code class="fe oo op oq or b">i</code>的时间步<code class="fe oo op oq or b">t</code>:</p><ol class=""><li id="f7db" class="oa ob it la b lb lc le lf lh oc ll od lp oe lt pb og oh oi bi translated">首先，LSTM 单元将当前时间步<code class="fe oo op oq or b">t</code>的协变量<code class="fe oo op oq or b">x_i,t</code>和先前时间步<code class="fe oo op oq or b">t-1</code>的目标变量<code class="fe oo op oq or b">z_i,t-1</code>作为输入。此外，LSTM 接收前一时间步的隐藏状态<code class="fe oo op oq or b">hi,t-1</code>。</li><li id="304f" class="oa ob it la b lb oj le ok lh ol ll om lp on lt pb og oh oi bi translated">然后，LSTM 单元输出其隐藏状态<code class="fe oo op oq or b">hi,t</code>，该隐藏状态被馈送到下一步骤。</li><li id="6381" class="oa ob it la b lb oj le ok lh ol ll om lp on lt pb og oh oi bi translated"><code class="fe oo op oq or b"><em class="lu">μ</em></code> <em class="lu"> </em>和<em class="lu"> </em> <code class="fe oo op oq or b"><em class="lu">σ</em></code> <em class="lu"> </em>值是从<code class="fe oo op oq or b">hi,t</code>间接计算出来的，并且“成为”高斯似然函数<code class="fe oo op oq or b"> <em class="lu">p(y_i|θ_i)= l(</em>z_i,t|Θι,t<em class="lu">)</em></code>的参数。论文用希腊字母 theta <code class="fe oo op oq or b"><em class="lu">θ = (μ, σ)</em></code> <em class="lu">定义了这些参数。如果你不理解这部分，不要担心，我们稍后会更详细地解释。</em></li><li id="efd2" class="oa ob it la b lb oj le ok lh ol ll om lp on lt pb og oh oi bi translated">换句话说，该模型试图回答这个问题:构造高斯分布的最佳参数<code class="fe oo op oq or b"><em class="lu">μ</em></code><em class="lu"/><em class="lu"/><code class="fe oo op oq or b"><em class="lu">σ</em></code><em class="lu"/>是什么，该高斯分布输出尽可能接近目标变量<code class="fe oo op oq or b">z_i,t</code>的预测？</li><li id="07c1" class="oa ob it la b lb oj le ok lh ol ll om lp on lt pb og oh oi bi translated">训练步骤<code class="fe oo op oq or b">t</code>到此结束。当前目标值<code class="fe oo op oq or b">z_i</code>和隐藏状态<code class="fe oo op oq or b">hi,t</code>被传递到下一个时间步骤，并且训练过程继续。由于<em class="lu"> DeepAR </em>每次训练(和预测)单个数据点，该模型被称为<strong class="la iu">自回归。</strong></li></ol></div><div class="ab cl pc pd hx pe" role="separator"><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/617402a86ca1eb78cac8aec2032df300.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/0*oIPtKoceP4L7NNBH.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oy">图 3:</strong>DeepAR 训练推理中的数学运算(<a class="ae oz" href="https://arxiv.org/pdf/1704.04110.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="0d08" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">推理的步骤几乎是一样的。</p><p id="da3e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不过有一点改变了:现在，在每个推理步骤<code class="fe oo op oq or b">t</code>，我们使用在之前的时间步骤<code class="fe oo op oq or b">t-1</code>中采样的预测变量<code class="fe oo op oq or b">ž_i,t-1</code>来计算新的预测<code class="fe oo op oq or b">ž_i,t</code>。</p><p id="b47b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">记住，<code class="fe oo op oq or b">ž_i,t</code>现在是从我们的模型在训练期间学习的高斯分布中采样的。但是，我们的模型并没有直接学习参数<code class="fe oo op oq or b"><em class="lu">μ</em></code> <em class="lu"> </em>和<em class="lu"> </em> <code class="fe oo op oq or b"><em class="lu">σ</em></code> <em class="lu"> </em>。</p><p id="9394" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将在下一节看到这些参数是如何计算的。</p><h1 id="c204" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">高斯似然</h1><p id="87b8" class="pw-post-body-paragraph ky kz it la b lb os ju ld le ot jx lg lh ou lj lk ll ov ln lo lp ow lr ls lt im bi translated">在深入研究 DeepAR 的自回归特性如何工作之前，理解似然函数如何工作是很重要的。如果你熟悉这个概念，你可以跳过这一节。</p><blockquote class="nl"><p id="c3ae" class="nm nn it bd no np nq nr ns nt nu lt dk translated">最大似然估计的目标是找到更好地解释样本数据的最佳分布参数。</p></blockquote><p id="d55b" class="pw-post-body-paragraph ky kz it la b lb nv ju ld le nw jx lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">让我们假设我们的数据遵循高斯(正态)分布。每个高斯分布由平均值<code class="fe oo op oq or b"><em class="lu">μ</em> </code>和标准偏差<code class="fe oo op oq or b"><em class="lu">σ</em></code> <em class="lu">、</em>即<code class="fe oo op oq or b"><em class="lu">θ = (μ, σ)</em></code> <em class="lu">参数化。</em>于是，高斯似然ℓ给出了<code class="fe oo op oq or b"><em class="lu">θ = (μ, σ)</em></code> <em class="lu"> </em>的定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/f4e58800bf7dcf5e0e9956dc25e5e4a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/0*RpUovvN5BT2_sbMF.png"/></div></figure><p id="0379" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，看一下<strong class="la iu">图 4 </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/0e07cede6d3a4caaf17d6bac79f1022a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/0*5xhNDTDE92dYMPEA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oy">图 4:</strong>2 个高斯分布的最大似然估计(图片由作者提供)</p></figure><p id="eaff" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们有绿色和橙色的数据点，每一个都遵循不同的高斯分布。让我们假设给你这些数据点，你的目标是估计它们的两个高斯分布。</p><p id="5fe6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">更正式地说，任务是在两个分布中找到最佳的<code class="fe oo op oq or b"><em class="lu">μ</em></code> <em class="lu">和</em> <code class="fe oo op oq or b"><em class="lu">σ</em></code>来最佳地拟合那些数据(<em class="lu"> DeepAR </em>假设只有一个分布)。在统计学中，这个任务也被称为最大化<strong class="la iu">高斯对数似然函数</strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/9d9d8330436877f50db7b43b2e1ee32b.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/0*rnlLzWQf2D-uf5cg.png"/></div></figure><p id="a90a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于所有时间步长<code class="fe oo op oq or b">t</code> ⋹ <code class="fe oo op oq or b">[t…τmax]</code>和<code class="fe oo op oq or b">i</code> ⋹ <code class="fe oo op oq or b">[1…N]</code>，函数被最大化，其中<code class="fe oo op oq or b">N</code>是我们数据集中时间序列的总数。</p><h1 id="fa9e" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">参数估计</h1><p id="1d7f" class="pw-post-body-paragraph ky kz it la b lb os ju ld le ot jx lg lh ou lj lk ll ov ln lo lp ow lr ls lt im bi translated">在统计学中，参数<code class="fe oo op oq or b"><em class="lu">μ</em></code> <em class="lu">和</em> <code class="fe oo op oq or b"><em class="lu">σ</em></code> <em class="lu"> </em>通常是使用<strong class="la iu">mle 公式</strong>(<strong class="la iu">m</strong>maximum<strong class="la iu">l</strong>og-likelihood<strong class="la iu">e</strong>stimulator s)来估计的，这些公式是通过对似然函数进行微分而得到的。</p><p id="0831" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们这里不这么做。</p><p id="1224" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">相反，我们让 LSTM 和 2 个<strong class="la iu">密集层</strong>基于模型的输入导出那些参数。这个过程如图<strong class="la iu">图 5: </strong>所示</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/5185c691ed7e74100f7c5874e696b9f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zsXjsr_Uba2mYqwk.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oy">图 5:<code class="fe oo op oq or b">μ </code>和<code class="fe oo op oq or b"> σ </code>的</strong>参数计算(图片由作者提供)</p></figure><p id="ce15" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">估算<code class="fe oo op oq or b"><em class="lu">μ</em></code> <em class="lu"> </em>和<em class="lu"> </em> <code class="fe oo op oq or b"><em class="lu">σ</em></code> <em class="lu"> </em>的过程很简单:</p><ul class=""><li id="0f2b" class="oa ob it la b lb lc le lf lh oc ll od lp oe lt of og oh oi bi translated">首先，<strong class="la iu"> LSTM </strong>计算其隐藏状态<code class="fe oo op oq or b">hi,t</code>。</li><li id="855e" class="oa ob it la b lb oj le ok lh ol ll om lp on lt of og oh oi bi translated">然后，<code class="fe oo op oq or b">hi,t</code>经过密集层<code class="fe oo op oq or b">W_μ</code>计算平均值<code class="fe oo op oq or b"><em class="lu">μ</em></code> <em class="lu">。</em></li><li id="9131" class="oa ob it la b lb oj le ok lh ol ll om lp on lt of og oh oi bi translated">同样，相同的<code class="fe oo op oq or b">hi,t</code>通过第二个密集层<code class="fe oo op oq or b">W_σ</code>并计算平均值<code class="fe oo op oq or b"><em class="lu">σ</em></code> <em class="lu">。</em></li><li id="1976" class="oa ob it la b lb oj le ok lh ol ll om lp on lt of og oh oi bi translated">现在我们有了<code class="fe oo op oq or b"><em class="lu">μ</em></code><em class="lu"/><code class="fe oo op oq or b"><em class="lu">σ</em></code><em class="lu">。</em>模型使用这些参数创建高斯分布，并采集样本。然后，模型检查该样本与实际观察值的接近程度<code class="fe oo op oq or b">z_i,t</code>。</li><li id="e580" class="oa ob it la b lb oj le ok lh ol ll om lp on lt of og oh oi bi translated">时间步长<code class="fe oo op oq or b">t</code>的训练到此结束。LSTM 权重和两个密集层<code class="fe oo op oq or b">W_μ</code>和<code class="fe oo op oq or b">W_σ</code>在反向传播期间被训练。</li></ul><p id="f157" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">换句话说，<em class="lu"> DeepAR </em>通过<code class="fe oo op oq or b">hi,t,</code> <code class="fe oo op oq or b">W_μ </code>和<code class="fe oo op oq or b">W_σ</code>间接计算<code class="fe oo op oq or b"><em class="lu">μ</em></code> <em class="lu">和</em> <code class="fe oo op oq or b"><em class="lu">σ</em></code> <em class="lu"> </em>。这样做是为了通过反向传播使他们的计算成为可能。</p><p id="cd41" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在推理过程中，我们没有目标变量<code class="fe oo op oq or b">z_i,t</code>进行比较。<em class="lu"> DeepAR </em>已经学习了所有的神经网络权重，并使用它们来创建预测<code class="fe oo op oq or b">ž_i,t</code>。</p><p id="8972" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">就是这样！我们现在已经看到了 DeepAR 如何端到端地工作。</p><p id="8af1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在接下来的章节中，我们将解释更多关于<em class="lu"> DeepAR </em>的机制。</p><blockquote class="lv lw lx"><p id="797c" class="ky kz lu la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated"><strong class="la iu">注:</strong>估计的均值和标准差参数在统计学中正式用<code class="fe oo op oq or b">μ hat</code>和<code class="fe oo op oq or b">σ hat</code>符号化。</p></blockquote><h1 id="1b7e" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">自动缩放</h1><p id="c5ba" class="pw-post-body-paragraph ky kz it la b lb os ju ld le ot jx lg lh ou lj lk ll ov ln lo lp ow lr ls lt im bi translated">处理多种不同的时间序列是棘手的。</p><p id="4c51" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">想象一个<strong class="la iu">产品销售预测场景</strong>:一个产品可能有几百个订单的销售额，而一个不同的产品可能有几百万个订单的销售额。</p><p id="0b96" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不同数量级的时间序列之间的这种巨大差异可能会混淆模型。为了克服这个问题，<em class="lu"> DeepAR </em>引入了一个<strong class="la iu">自动缩放机制。</strong>更具体地说，该模型计算一个依赖于项目的<code class="fe oo op oq or b">ν_ι</code>来重新调整自回归输入<code class="fe oo op oq or b">z_i,t</code>。这由以下公式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/746f4d30197be2a3330c829d9cb36965.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/0*qTB9-c4d66Xnxpas.png"/></div></figure><p id="886c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，在每个时间步<code class="fe oo op oq or b">t</code>，来自前一步的自回归输入<code class="fe oo op oq or b">z_i,t</code>首先被该因子缩放。</p><blockquote class="lv lw lx"><p id="8e97" class="ky kz lu la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated"><strong class="la iu">注意:</strong>DeepAR 的自动缩放机制非常好用。然而，在实践中，最好首先手动标准化我们的时间序列。这样做将增强我们模型的性能。</p></blockquote><h1 id="6450" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">时序景观中的 DeepAR</h1><p id="9f38" class="pw-post-body-paragraph ky kz it la b lb os ju ld le ot jx lg lh ou lj lk ll ov ln lo lp ow lr ls lt im bi translated">在本节中，我们将讨论<em class="lu"> DeepAR </em>如何与其他型号竞争，以及它的局限性。</p><h2 id="88a5" class="pn mu it bd mv po pp dn mz pq pr dp nd lh ps pt nf ll pu pv nh lp pw px nj py bi translated">统计模型</h2><p id="6b1d" class="pw-post-body-paragraph ky kz it la b lb os ju ld le ot jx lg lh ou lj lk ll ov ln lo lp ow lr ls lt im bi translated">作者表明<em class="lu"> DeepAR </em>优于传统的统计方法，如<strong class="la iu"> ARIMA </strong>。此外，<em class="lu"> DeepAR </em>相对于那些模型的巨大优势是它不需要额外的特征预处理(例如，首先使时间序列平稳)。</p><p id="248e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">亚马逊后来发布了一个更新版本，名为<strong class="la iu"> DeepVAR[4] </strong>，显著提高了性能。我们将在以后的文章中描述这个模型。</p><h2 id="4fd9" class="pn mu it bd mv po pp dn mz pq pr dp nd lh ps pt nf ll pu pv nh lp pw px nj py bi translated">深度学习模型</h2><p id="5cfb" class="pw-post-body-paragraph ky kz it la b lb os ju ld le ot jx lg lh ou lj lk ll ov ln lo lp ow lr ls lt im bi translated">自从<em class="lu"> DeepAR </em>发布以来，研究界已经发布了无数用于时间序列预测的深度学习模型。</p><p id="6239" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">并不是所有的都可以和<em class="lu"> DeepAR </em>直接比较，因为他们的工作方式不同。据我所知，我能想到的最接近的是<strong class="la iu">时间融合变压器(TFT) [5]。</strong></p><p id="11f2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们来讨论一下<em class="lu"> DeepAR </em>和 TFT 之间的两个显著区别:</p><p id="49e4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 1。多个时间序列<br/> </strong> <em class="lu"> DeepAR </em>为每个时间序列计算一个单独的嵌入。这种嵌入被用作 LSTM 的一个特征，帮助 DeepAR 区分不同的时间序列。</p><p id="ff72" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">TFT 也利用 LSTMs，工作方式类似。然而，TFT 使用这些嵌入来配置 LSTM 的初始隐藏状态<code class="fe oo op oq or b">h_0</code>。这种方法好得多，因为 TFT 在每个时间序列上适当地调节 LSTM 单元，而不改变时间动态。</p><p id="bfdf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 2。预测类型<br/> </strong> TFT 不是<em class="lu">自回归</em>模型——它被归类为<strong class="la iu"> <em class="lu">多时段预测模型</em> </strong>。两种类型的模型都可以输出多步预测。然而，多时段预测模型一次生成预测，而不是像自回归模型那样逐个提供预测。</p><p id="3570" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种方法的优势在于，多时段预测模型可以为其协变量没有任何值的时间步长创建预测。TFT 在这方面表现出色，因为它是功能多样性最丰富的型号之一。</p><h1 id="e81e" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">结束语</h1><p id="5074" class="pw-post-body-paragraph ky kz it la b lb os ju ld le ot jx lg lh ou lj lk ll ov ln lo lp ow lr ls lt im bi translated">DeepAR 是一个卓越的深度学习模型，是时间序列社区的一个里程碑。</p><p id="dd26" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，这种模型在生产中很普遍:它是亚马逊用于时间序列预测的<strong class="la iu"> GluonTS [6] </strong>工具包的一部分，可以在亚马逊 SageMaker 上进行训练。</p><p id="c357" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在下一篇文章中，我们将使用<em class="lu"> DeepAR </em>来创建一个端到端的项目。<br/>敬请期待！</p></div><div class="ab cl pc pd hx pe" role="separator"><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph"/></div><div class="im in io ip iq"><h1 id="4557" class="mt mu it bd mv mw pz my mz na qa nc nd jz qb ka nf kc qc kd nh kf qd kg nj nk bi translated">感谢您的阅读！</h1><p id="2c9a" class="pw-post-body-paragraph ky kz it la b lb os ju ld le ot jx lg lh ou lj lk ll ov ln lo lp ow lr ls lt im bi translated">我每个月写一篇有影响力的 AI 论文的深度分析。<br/> <strong class="la iu">保持连接！</strong></p><ul class=""><li id="0de9" class="oa ob it la b lb lc le lf lh oc ll od lp oe lt of og oh oi bi translated">订阅我的<a class="ae oz" href="/subscribe/@nikoskafritsas" rel="noopener ugc nofollow" target="_blank">简讯</a>！</li><li id="ed69" class="oa ob it la b lb oj le ok lh ol ll om lp on lt of og oh oi bi translated">在<a class="ae oz" href="https://www.linkedin.com/in/nikos-kafritsas-b3699180/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>上关注我！</li></ul><h1 id="c774" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">参考</h1><p id="5e50" class="pw-post-body-paragraph ky kz it la b lb os ju ld le ot jx lg lh ou lj lk ll ov ln lo lp ow lr ls lt im bi translated">[1]用稳定扩散创建，CreativeML Open RAIL-M license。文字提示:“穿越太空的星云，数码艺术，插画”</p><p id="b410" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2] D. Salinas 等<a class="ae oz" href="https://arxiv.org/pdf/1704.04110.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> DeepAR:用自回归递归网络进行概率预测</em> </a> <em class="lu">，</em>《国际预测杂志》(2019)。</p><p id="d2a3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[3]吴永辉等<a class="ae oz" href="https://arxiv.org/abs/1609.08144" rel="noopener ugc nofollow" target="_blank">谷歌的神经机器翻译系统:弥合人与机器翻译的鸿沟</a> (2016)</p><p id="a263" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[4] D. Salinas 等<a class="ae oz" href="https://arxiv.org/pdf/1910.03002.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lu">低秩高斯 Copula 过程的高维多元预测</em> </a> <em class="lu">，</em>国际预测杂志(2019)。</p><p id="45ca" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[5] Bryan Lim 等人<a class="ae oz" href="https://arxiv.org/pdf/1912.09363.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lu">用于可解释的多时间范围时间序列预测的时间融合变换器</em> </a>(国际预测杂志，2021 年 12 月)</p><p id="720e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[6]亚马逊的 GluonTS 包，<a class="ae oz" href="https://ts.gluon.ai/stable/api/gluonts/gluonts.model.deepar.html" rel="noopener ugc nofollow" target="_blank">https://ts . gluon . ai/stable/API/Glu onts/Glu onts . model . deepar . html</a></p></div></div>    
</body>
</html>