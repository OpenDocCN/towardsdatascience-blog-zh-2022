<html>
<head>
<title>Convolutional Neural Networks: An Introduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积神经网络:导论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-perceptron-to-densenet-an-introduction-to-convolutional-neural-networks-ab37e3b7872e#2022-10-21">https://towardsdatascience.com/from-perceptron-to-densenet-an-introduction-to-convolutional-neural-networks-ab37e3b7872e#2022-10-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b012" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从感知器到DenseNet的短暂旅程</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e15ad1f38504d74227026622195a0c4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QIbPET3MbOEuYwuIUyBGEg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">杰姆·萨哈冈在<a class="ae kv" href="https://unsplash.com" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片。</p></figure><p id="969f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">人们可能有理由怀疑，当网上有许多关于同一主题的介绍时，为什么有必要再介绍一次卷积神经网络。然而，本文将读者从最简单的神经网络感知机带到深度学习网络ResNet和DenseNet，(希望)以一种可理解的方式，但肯定是以一种简洁的方式，在几个步骤中涵盖了深度学习的许多基础知识。所以我们开始吧——如果你想的话。</p><h1 id="e2ca" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">介绍</h1><p id="2890" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">机器学习是人工智能的重要组成部分。它用于许多应用领域，如图像识别、语音识别、风力预测和药物设计。数据科学是一个与人工智能密切相关的新兴领域，专注于学习，预测专注于收集，可视化，推理和学习模型的预处理。特征是描述观察的变量。一个模式<em class="mp"/><strong class="ky ir"><em class="mp">x</em></strong><em class="mp">∈ℝᵈ</em>是一组特征。标签<em class="mp"> y ∈ ℝ </em>是我们感兴趣的观察结果。模式-标签对形成了基本事实。</p><p id="38f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">监督学习通过调整分类器的参数来适应训练模式，从而在训练阶段训练分类器。从形成<em class="mp"> d </em>维训练集的一组模式-标签对<em class="mp">(</em><strong class="ky ir"><em class="mp">x</em></strong><em class="mp">ᵢ，yᵢ) </em>与<em class="mp"> i = 1 …，n </em>与<strong class="ky ir"><em class="mp">x</em></strong><em class="mp">ᵢ∈ℝᵈ</em>中，训练一个机器学习模型<em class="mp"> f </em>，用于预测适当的标签信息在分类中，标签是离散的，例如<em class="mp"> {0，1} </em>并被称为类或类别，而在回归中，标签是连续的。</p><h1 id="67e5" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">感知器</h1><p id="5550" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">感知器[1]是一个简单的神经单元(<em class="mp">f:ℝᵈℝ</em>),汇总加权输入并将其输入激活功能</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/135225340105c43bc464b73f3f44505e.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*bQ2VaFTFDEVi7ekyLkAKSQ.png"/></div></figure><p id="e75d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，<strong class="ky ir"><em class="mp">x</em></strong><em class="mp">∈ℝᵈ</em>是感知器的输入，<strong class="ky ir"><em class="mp">w</em></strong><em class="mp">∈ℝᵈ</em>是权重，<em class="mp"> b ∈ ℝ </em>是偏差，<strong class="ky ir"> w </strong> <strong class="ky ir"> x </strong>是所有<strong class="ky ir"><em class="mp">x<em class="mp">的分量乘积之和</em></em></strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/f4ce36b2a59d85d71f8b66a67b788475.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*LA3RZyS4kAVvs3pq7VOXCw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">激活函数ReLU图</p></figure><p id="8787" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">功能σ<em class="mp">:ℝℝ</em>是一种激活功能，类似于整流线性单元，简称ReLU，见图1:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/e235e196c28409b00f55d2d7b26ded07.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*kLwHV2yjsA_b27hCNfJ8CA.png"/></div></figure><p id="e55b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ReLU可以计算得非常快，这解释了它在深度学习方面的成功。在众多进一步激活的功能中，乙状结肠功能是其中之一:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/7bbc9d7f9fac9f5a953aff4dc227d4c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*8aLId0ihyOSwr3SbhATslQ.png"/></div></div></figure><p id="4ac8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将输入映射到0和1之间的值和双曲正切值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/a806b3aba746c101e11a85e34cddec6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*ga0fWr0mqXaVppdajZ_hSQ.png"/></div></figure><p id="5df0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">映射到区间<em class="mp"> [-1，1] </em>。</p><p id="9b31" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将多个感知器分层允许分离更复杂的集合，也包括像XOR问题这样的非线性集合。密集层将众多的感知机放入一层。连续层的所有神经元相互连接，即密集连接。密集层也称为全连接(FC)层。输入层的神经元数量对应于模式的维度。一个多层感知器(MLP)由一个输入层和一个输出层以及一个或多个隐藏层组成，见图2。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/712307de4dde7ccd52945df22a02728b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*MKN0zmCDyM48WpzskMAKPw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:左:有一个隐藏层的MLP的插图。右图:多个图层允许非线性分类边界。</p></figure><p id="a06d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个以神经元为节点的MLP图中的每条边都配备有自己的权重<em class="mp"> w </em> ᵢ.如果<strong class="ky ir"> <em class="mp"> x </em> </strong> <em class="mp"> ∈ ℝᵈ </em>是一个层的输入并且<strong class="ky ir"><em class="mp">w</em></strong><em class="mp">∈ℝ^{k×d }</em>是一个层的权重矩阵，那么</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/1d71d669661c5e150e33ea67d7a476c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/1*pWVpTFpZjvu3nAGI4F57zA.png"/></div></figure><p id="98d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">是有偏差的加权和<strong class="ky ir"><em class="mp">b</em></strong><em class="mp">∈ℝᵏ</em>。输出是一个<em class="mp"> k </em>维向量，其元素被逐元素地馈送给激活函数σ，从而产生相应层的激活向量。如果需要处理通道信息(图像的RGB值),张量的扩展是合理的。信息从输入层传递到输出层。因此，与具有反向连接的循环网络相比，该网络体系结构被称为前馈网络。</p><p id="6b23" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">网络权重和偏差通常统一初始化为小值，如-0.01到0.01。或者，Glorot初始化旨在使一个层的输出方差等于其输入方差。Glorot从以0为中心的正态分布中抽取样本，标准偏差基于输入和输出的数量。</p><h1 id="ffac" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">损失函数</h1><p id="0962" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在分类中使用一个热编码，这意味着一个输出神经元对应于一个返回1的输出类，而所有其他神经元返回0。例如，在数字数据集的<em class="mp"> K=10 </em>类的情况下，使用一个热编码的MLP采用10个输出神经元，每个表示一个类，例如<strong class="ky ir">y</strong>=(0,0,1,0,0,0,0,0,0,0)^t用于<em class="mp">数字2 </em>)。</p><p id="1cb7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了实现对<em class="mp"> K </em>类的一个热编码，最后一层的<em class="mp"> k = K </em>神经元的最终激活y’被馈送到softmax:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/c6028f951c91be7b4c002b0e46b7bbf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*nOh0RPQf30eEg9RME5anCw.png"/></div></figure><p id="9f63" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它将所有输出调整为0到1之间的值，而所有输出的总和为1。这些值将被解释为类别概率。最后，提供softmax函数的最高输出的输出神经元确定MLP的分类决策，即:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/f1edd1dc0663abb59433f69e090c246c.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*3oKuFbl-orT7Mweq4W7hBg.png"/></div></figure><p id="2146" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">利用交叉熵(也称为对数损失)来计算整个训练集的误差，该交叉熵面向香农熵原理。在所有数据训练样本上，我们得到总体平均交叉熵损失如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/acbb7c9826de010e881b1f2703feb8bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*XUcmi1U2FnMp5TAkrQwm-A.png"/></div></figure><p id="67cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<em class="mp"> n </em>是训练集大小或批量大小，<em class="mp"> yᵢⱼ ∈ {0，1} </em>是来自训练集的标签的地面真实概率，<em class="mp"> y'ᵢⱼ ∈ [0，1] </em>是来自softmax的第I个示例中第j个类的预测概率。由于一个热编码，每个图案和每个维度都有一个<em class="mp"> yⱼ </em>。对于接近1的大差异，罚分产生大的分数，对于接近0的小差异，罚分产生小的分数。</p><p id="6bc7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在回归中，MSE最常用作损失函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/77b61bc3bec0b71285a4a8eaed20816c.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*jjW4itIyhYuXKR9ctCSRlw.png"/></div></figure><p id="7e8b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于标签<em class="mp"> y₁、…、yₙ ∈ ℝ </em>和输出<em class="mp"> y'₁、…、y'ₙ ∈ ℝ </em>。引入的损失函数用于调整神经网络的权重。</p><p id="73e7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">损失函数用于训练模型，见下一节。但是为了评估分类的准确性，像召回率和精确度这样的测量是有用的。在分类中，一个模式可以被正确分类(真)或不被正确分类(假)。在两类分类场景中，我们可以区分正确分类为正的真正(TP)模式和真正分类为负的真负(TN)模式。假阳性(FP)模式被分类为阳性，但应该是阴性，假阴性(FN)模式被错误地分类为阴性。对于两个以上的标签(阳性和阴性)，混淆矩阵是一个一般化。在矩阵的每个位置，它计算属于属于该行的类的模式的数量，并且被分类到属于该列的类。模型的精度定义为TPs / (TPs + FPs ),表示正面预测的准确度。同时，被定义为TPs/(TPs + FNs)的回忆表明实际阳性被发现的程度。</p><h1 id="27f0" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">培养</h1><p id="7eb6" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">学习就是体重适应。训练阶段的典型设置是调整网络的所有权重<strong class="ky ir"> <em class="mp"> w </em> </strong> <em class="mp"> ᵢ </em>和偏差<em class="mp"> bⱼ </em>，使得模式被MLP映射以校正训练集的标签。<br/>神经学习中主要的权重自适应算法是反向传播。反向传播对权重执行梯度下降，以最小化损失函数<em class="mp"> L </em>，该损失函数例如可以是交叉熵或MSE。梯度下降是一种优化方法，它将搜索移动到梯度的相反方向。</p><p id="f322" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">点<strong class="ky ir">处的梯度<em class="mp">δl(</em><strong class="ky ir"><em class="mp">w</em></strong><em class="mp">)</em>w</strong><em class="mp">∈ℝᵏ</em>是一个<em class="mp">k</em>-偏导数的维向量<em class="mp">∂l(</em><strong class="ky ir"><em class="mp">w</em></strong><em class="mp">)/∂w</em>ᵢ<em class="mp"/>w . r . t .各参数<em class="mp"> wᵢ、I</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/d473ba69230e52ad6e8185382d43ec78.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*QaMXBB6E4vsWmB_cfIO85Q.png"/></div></figure><p id="44b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">梯度下降通常比无向随机搜索更快。对于回归问题，损失定义为残差平方和，对于分类，常见的损失函数是交叉熵损失，参见上面的等式。</p><p id="81f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果损失函数<em class="mp"> L </em>是可微的，可以计算偏导数<em class="mp"> ∂L/∂w </em> w. r. t .权重，从而得到梯度<em class="mp">δl(</em><strong class="ky ir"><em class="mp">w</em></strong><em class="mp">)</em>。梯度下降通过以学习率<em class="mp"> η </em>进入梯度的相反方向来执行最小化:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/9a6e7fa577a87d73444fd1e958f3045f.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*896ETmixlagQa91y-RPM4A.png"/></div></div></figure><p id="ac69" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此更新也称为普通更新。</p><p id="3759" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于简单的感知器<em class="mp"> f(x) </em>与<em class="mp"> f(x) = </em> σ <em class="mp"> (wx + b) </em>，具有输入模式<em class="mp"> x </em>，目标标签<em class="mp"> y </em>，sigmoid激活函数，以及损耗<em class="mp"> L = 1/ 2 (y — </em> σ <em class="mp"> (z)，</em>我们示例性地导出反向传播:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/8e4a02eda702d225906e8800e0a1a626.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*YLIv_0WJgRVeIUeApw4Y_A.png"/></div></figure><p id="5bdc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用链式法则。常数消失，又是链式法则，利用sigmoid的导数，我们得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/51004398c077c2ba372a732559ced56d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lNzmj-u1W6wfizyXkbWSxQ.png"/></div></div></figure><p id="6de4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在一个时期中，完整训练集的所有模式被呈现给网络。权重更新的一个成功变体被称为随机梯度下降(SGD)。SGD在呈现一个模式后更新权重并计算梯度。因此，它试图通过一次只考虑一个训练样本来逼近真实梯度。如果训练集被打乱并分成不连续或重叠的批次，则训练可能是有效的。在小批量模式中，用训练样本子集的梯度来训练神经网络。SGD不如mini batch健壮，但它允许更快的步骤。此外，它可能更少陷入局部最优。局部最优采用了比其邻域更好的适应度，但可能不是全局最优，见图3。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/4a6ae0f40d7fe9b4360e9558ba9ecf9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*8q0cIypmOIJ3znYWRi4hMw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3:局部最优的图示。</p></figure><p id="e7c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果与整个训练集的真正梯度下降相比，小批量梯度下降是正常梯度下降和SGD之间的折衷。</p><h1 id="05cf" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">动力</h1><p id="a0b7" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">学习率<em class="mp"> η </em>在训练网络中起着重要的作用。动量有助于避免权重更新的振荡，并允许更快的收敛。以下是对权重为<em class="mp"> k </em>的多元变量的扩展，即<em class="mp">δ</em><strong class="ky ir"><em class="mp">w</em></strong><em class="mp">，δl(</em><strong class="ky ir"><em class="mp">w</em></strong><em class="mp">)∈ℝᵏ</em>。在经典动量中，旧的权重变化<em class="mp">δ</em><strong class="ky ir"><em class="mp">w</em></strong>再次应用递减因子<em class="mp">β∈【0，1】</em>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/69fa661b39bd356b5c45c27841789671.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*Rm8qnI63ZS_x2SkIZAOWkg.png"/></div></figure><p id="91d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">term<em class="mp">βδ</em><strong class="ky ir"><em class="mp">w</em></strong><em class="mp"/>在极端梯度的情况下减缓自适应过程，防止优化器超调。开始时，<em class="mp">δ</em><strong class="ky ir"><em class="mp">w</em></strong><em class="mp"/>被设置为零矢量。左边的图4说明了动量原理。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/d0e846e5c566a53ca2b2c5459141ae79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*-qXuDIFiB0eLd-Fx53sq6g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4:动量图(左)和内斯特罗夫动量图(右)。</p></figure><p id="0504" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">内斯特罗夫动量是动量更新的延伸。它为凸函数提供了更强的理论收敛保证。实际上，它比标准动量理论更有效。它将动量视为一种前瞻，即首先进入动量方向<em class="mp">βδ</em><strong class="ky ir"><em class="mp">w</em></strong>，并从那里计算现在略有不同的梯度∇<em class="mp">l(</em><strong class="ky ir"><em class="mp">w</em></strong><em class="mp">+βδ</em><strong class="ky ir"><em class="mp">w</em></strong><em class="mp"/>)在新的前瞻位置:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/0a915653a81730b6befe75e0243d5e34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*487ZrVZj-fVx0svB_QnaOA.png"/></div></figure><p id="8c8a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图4(右)展示了内斯特罗夫的势头。过去已经提出了不同的权重更新变体，其中一个突出的变体是Adam。</p><h1 id="4def" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">过度拟合</h1><p id="5a76" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">专注于适应训练模式的模型可能过度适应训练数据，并且可能学习复杂的适应，尽管期望的模型结构可能不太复杂。这导致模型的泛化能力不足。过度拟合可以通过正则化、交叉验证和退出来避免。</p><p id="ad3a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正则化基于权重的大小，例如所有权重的平方和，向误差分类或回归误差<em class="mp">L(</em><strong class="ky ir"><em class="mp">w</em></strong><em class="mp">)</em>添加惩罚:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/26e217d2a1b33956758fc68893005117.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*Q2Lm6ZtCZIkPEYU0RBHKAA.png"/></div></figure><p id="e751" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">带参数<a class="ae kv" href="https://www.key-shortcut.com/schriftsysteme/abg-griechisches-alphabet" rel="noopener ugc nofollow" target="_blank"> α </a>的情况下<em class="mp"> L </em>正则化，其中</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/f7c6d7856b12e4247283dd5434132189.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*PQbIdIPns9z0p2PEIRRp8w.png"/></div></figure><p id="b5b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">是权重向量<strong class="ky ir"> <em class="mp"> w </em> </strong>的<em class="mp"> L </em>范数。大的重量与过度配合有关，而小的重量被认为可以防止过度配合。对权重的惩罚强制小权重，从而防止过度拟合。</p><p id="d9c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">交叉验证(CV)使用一组训练样本进行训练，并在独立的验证集上评估模型质量。推广到不止一个这样的交叉验证过程，N重交叉验证将随机混洗的数据集分成<em class="mp"> N </em>个分离的子集，参见图5。每个子集都被排除一次作为验证集。对所有的<em class="mp"> N </em>个折叠重复该过程，并对误差进行平均。对独立测试集的最终评估可用于说明模型质量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/ef6131e3cf8917d3c4074e7f201450d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*q-KaT3piJefilCQFBW3oXQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5: CV重复地(这里是3次3折CV)留下一个(灰色)验证集，并在剩余的(蓝色)折叠上训练模型。</p></figure><p id="1ff8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个极端的例子是<em class="mp"> N=n </em>的留一交叉验证(LOO-CV)，即每个模式都是一个折叠。从统计学的角度来看，它是有用的，但是由于大量的训练过程，它是非常低效的，因此主要适用于小数据集。</p><p id="7d89" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">控制遗漏验证集上的误差允许早期停止，即，在验证误差增大而训练误差进一步减小的情况下退出训练过程。</p><p id="5f83" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在训练阶段，Dropout关闭每个神经元的概率为<em class="mp"> p ∈ [0，1] </em>，称为dropout rate。对于每个隐藏层、每个训练样本和每次迭代，随机分数<em class="mp"> p </em>激活被忽略，也称为清零。在测试阶段，所有激活都被使用，但按系数<em class="mp"> p </em>减少。这说明了在训练阶段缺少激活的原因。Dropout也用于卷积层，参见dropout部分，它也随机地将激活置零。</p><p id="30ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图6显示了辍学情况。灰色神经元不参与训练过程。它们的权重不会更新。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/9fd4558b1958b88232ac6f4988e64cfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*hVC4lKgE7vVQjCCVqXH8xw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图6:在退出期间，神经元以概率p被去激活，这里示出了训练期间的灰色神经元。</p></figure><p id="aa4b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Dropout迫使网络学习类的多个独立表示，这防止了过度拟合。也可以理解为多个子网的集成学习，将它们的决策结合起来。</p><h1 id="0484" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">卷积层</h1><p id="1efb" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">卷积网络很早就已经推出[2]，但Alex Krizhevsky在2012年的ImageNet大规模视觉识别挑战赛中使用AlexNet实现了图像识别的突破。图7示出了示例性的网络架构。卷积层充当平移不变特征学习器。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/233c9bb48feb0c5180371c69ff411cf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VoaXPBLlxY4KxP_BmnM0og.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图7:带滤波器和信道的卷积网络架构。</p></figure><p id="1367" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">设<strong class="ky ir"> X </strong>为<em class="mp"> H × W × C- </em>维度输入。一个卷积层由<em class="mp">C’</em>乘以<em class="mp"> C m × m </em>个滤波器核矩阵组成。对每个输入通道执行卷积运算，并对结果求和。该操作是针对每个过滤器内核执行的。卷积运算将一个<em class="mp"> m × m- </em>维内核矩阵<strong class="ky ir"> w </strong>从左上至右下移动到输入体积，参见图8，计算:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/29f9c8d3f7eaf3d8c9bfe2947d895cfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*eCVTovVDqX4kpi6gnda61w.png"/></div></figure><p id="6fb0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">滤波器内核<strong class="ky ir"> <em class="mp"> w </em> </strong>通过反向传播进行调整，并学习有用的特征检测器。输入<strong class="ky ir"> <em class="mp"> x </em> </strong> <em class="mp"> </em>和<strong class="ky ir"> <em class="mp"> w </em> </strong>之间的相似性产生高激活<strong class="ky ir"> <em class="mp"> a </em> </strong>。每个输入通道的激活被累加，因此由<em class="mp">C’</em>被加数组成。该过程被重复<em class="mp">C’</em>次，导致<em class="mp">C’</em>输出通道用于输出<strong class="ky ir"> <em class="mp"> A </em> </strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/e35a0d2efeb1cc073065f743244b7c0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nA6caLChN-nFEBt_wy2OSA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图8:2D卷积过程的例子。</p></figure><p id="f117" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，具有<em class="mp"> 32 × 32 </em>彩色图像的CIFAR-10网络的第一卷积层可以采用64×3滤波器核<em class="mp"> 3 × 3 </em>矩阵。该层将输出64个通道。</p><p id="7d6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">滤波器核在输入体矩阵上移动的步长称为步幅。它采用垂直轴和水平轴。两个轴的步幅都是一个常见的选择，见图9。较高的值降低了计算和存储的复杂性。为了避免维度收缩，输入体积的边界可以用零填充，例如，通过添加<em class="mp">W-m</em>零。这个过程称为零填充。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/16b663320934a3c5feb99e70f5b3b8df.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*euUeg6dP2qyHO4MB-ZMr9w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图9:1×1步的图示。</p></figure><p id="e250" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">偏差<em class="mp"> b </em>可被添加到输出体积，即所有激活。卷积层的输出应用激活函数<strong class="ky ir"><em class="mp">A’</em></strong><em class="mp">=σ(</em><strong class="ky ir"><em class="mp">A</em></strong><em class="mp">)</em>，例如ReLU。汇集层减少了集中于最大或平均激活的维度。滤波器核的数量通常随着网络的深度而增加。</p><h1 id="08a2" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">联营</h1><p id="06bb" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">卷积层导致激活显著增加。为了减少数量，使用了池层，参见图10。池化是一种基于渠道的操作。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/e872a5078d9c39b42fe6e4080f16f249.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*kATlazAO2tFAtYHsSyydkQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图10: Max pooling返回输入体积内的最大值，该最大值通常以对应于体积维度的步幅移动(这里是2x2)</p></figure><p id="2594" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">设<strong class="ky ir"> <em class="mp"> A </em> </strong>为激活的特征图，最大池在每个通道上移动一个<em class="mp"> m × m </em>方块，选择最大值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/7596e74cc00787a18e37234a75cb8af5.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*w8BeMYiSQIeF0bIbSur5kw.png"/></div></figure><p id="837c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">平均汇集是使用每个方块的平均值的相应过程。汇集层也可用于替换密集层头(激活的最后一层)。</p><p id="9a48" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">VGG-19是早期卷积网络的一个例子。它由19个权重层组成，对于224 × 224 RGB图像的输入具有以下配置:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/ca5f2610107b81717903264f5fd0ede8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IFzIrY3U837z_sQ_K-RBPg.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/b430e883b79cae569f5e2f9a414a590f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*w1OBGWv7-cB2J0dSmqr3MA.png"/></div></figure><p id="65cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">FC表示全连接层。这三个FC也被称为MLP头。VGG-19使用了1.44亿个参数，是深度架构的一个例子。</p><h1 id="fedf" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">雷斯内特</h1><p id="d7bc" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">一个网络使用的参数和权重越多，它就能代表越多。数百层是可能的，但是训练起来计算量很大。但是非常深的网络容易过度拟合。随着误差以倍增的方式通过网络向后传播，每层中的梯度更新变得更小。这种效应随着层数的增加而增加，称为消失梯度问题。ResNet [3]通过以恒等式<strong class="ky ir"> <em class="mp"> X </em> </strong>的形式提供快捷连接来解决这个问题，并将它们添加到一个模块的输出<strong class="ky ir"> <em class="mp"> R </em> </strong>中，这就变成了一个来自输入的残差。残差比恒等式更容易学习。ResNet模块提供身份，并学习与他们的偏差。层和身份的总和成为一个ResNet模块<em class="mp"> R </em>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/336fc9924c2a1053d3bbf2554fb75d9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/format:webp/1*gB4o_mqlivnQIzWy_5tyMw.png"/></div></figure><p id="372e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">而剩余的<strong class="ky ir"> <em class="mp"> R </em> </strong>就要学会了。为了匹配<strong class="ky ir"> R </strong>和<strong class="ky ir">t18】xt20】的尺寸，可以添加投影矩阵<strong class="ky ir"> <em class="mp"> W </em> </strong> <em class="mp"> ₚ </em>，从而得到:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/fab05c1cb5e85efcadabb3e7437fa2f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*R7b9y05YkIxbHAtJlWVlaA.png"/></div></figure><p id="4594" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图11示出了基于两个卷积层的示例性ResNet模块。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/cd7791fe6c5c397fd67e8222c8f03cfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*aGFmhkjS5KEcw6o63OYGXQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图11:ResNet模块的标识快捷方式连接。</p></figure><p id="70be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ResNet原理可以应用于由全连接或卷积层组成的所有类型的模块。甚至快捷连接也可以由覆盖卷积层模块的一个卷积层组成。</p><p id="58a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对最优模型<em class="mp"> f* </em>的搜索可以被视为在函数类<em class="mp"> F </em>中的搜索。在这个空间中，神经网络是由架构、超参数和权重定义的函数<em class="mp"> f ∈ F </em>。改变函数类不一定会使其更接近<em class="mp"> f* </em>。一个ResNet模块实现了添加剩余部分的恒等式，从而表示一个嵌套函数，即它持有<em class="mp">f’</em>⊂<em class="mp">f</em>，这强制向最优<em class="mp"> f* </em>移动，参见图12。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/9f61cb5bb33f1413770cd993afe8d1b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*1UFs44ljDKrjwGlD_p0pFQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图12:用ResNet模块实现的嵌套函数类(右)更接近最佳函数f *,非嵌套函数类(左)则不一定</p></figure><p id="9049" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">原始论文中的ResNet架构采用152层，一个7 × 7、64步距2卷积和一个3 × 3最大池，步距2后接以下卷积ResNet模块:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/58396c75b66f15f59daa070a4d59d23c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PUwlP3hVU6V0sMtrqNsXVg.png"/></div></div></figure><p id="4517" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由每三个回旋组成。在网络的末端，使用1000维全连接层和softmax进行平均池化。</p><h1 id="e8ac" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">DenseNet</h1><p id="105c" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在DenseNet [4]中，各层以前馈方式与后续各层相连，参见图13或DenseNet模块。像泰勒级数展开一样，将函数分解成越来越高的项，DenseNet用越来越深的层来表示函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/e8006b75bfe3ce75f876f1b2d0194d5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*_iavH8y25-FmWCT-jZXtkA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图13:在密集块中，卷积层的每个输出都被馈送到后续层的输入。密集块的最后一层是过渡层。</p></figure><p id="78a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<em class="mp"> l </em>层的密集块中<em class="mp"> l </em> ⋅ <em class="mp"> (l + 1) / 2 </em>连接被引入。每个卷积层接收所有先前层的输出作为输入，并向每个后续层产生自己的输出。先前层的输出被连接(不像在ResNet中那样被求和):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/bf4f872463cee3996e9a692fa7861800.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*CxMdbtpIPJ1Ox7PESUM-QA.png"/></div></figure><p id="6299" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每一个密集块增加通道的数量。过渡层限制了模型的复杂性，瓶颈1 × 1卷积层减少了信道的数量，平均池层，例如，步幅2将高度和宽度减半。这样，过渡层减少了输入维度。密集架构允许具有相同性能的不太深(即，更浅)的网络，并且还解决了消失梯度问题。</p><h1 id="8fa3" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="0c4c" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">卷积神经网络不仅是人工智能中最重要的方法之一，其基本方法和原理也用于许多其他深度学习算法。例如，反向传播是几乎所有深度学习方法的优化主干，卷积层是众多神经架构的一部分。它们的应用不仅限于图像识别，而且在许多其他领域也得到了证明。如果您想更深入地了解上述主题和实现，请参阅进一步阅读部分的一些参考资料。Python是深度学习的最佳编程语言，Keras以及PyTorch可以让你轻松访问卷积神经网络。</p><p id="adef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mp">除特别注明外，所有图片均为作者所有。</em></p><h1 id="b19f" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">延伸阅读</strong></h1><ul class=""><li id="583e" class="nz oa iq ky b kz mk lc ml lf ob lj oc ln od lr oe of og oh bi translated"><em class="mp">潜入深度学习</em>，张等人，<a class="ae kv" href="https://d2l.ai/d2l-en.pdf" rel="noopener ugc nofollow" target="_blank"/></li><li id="20f2" class="nz oa iq ky b kz oi lc oj lf ok lj ol ln om lr oe of og oh bi translated"><em class="mp">统计学习导论</em>，第2卷，詹姆士等人，<a class="ae kv" href="https://hastie.su.domains/ISLR2/ISLRv2_website.pdf" rel="noopener ugc nofollow" target="_blank">https://hastie.su.domains/ISLR2/ISLRv2_website.pdf</a></li><li id="526c" class="nz oa iq ky b kz oi lc oj lf ok lj ol ln om lr oe of og oh bi translated"><em class="mp">喀拉斯简介</em>、<a class="ae kv" href="https://keras.io/getting_started/" rel="noopener ugc nofollow" target="_blank">https://keras.io/getting_started/</a></li><li id="0f93" class="nz oa iq ky b kz oi lc oj lf ok lj ol ln om lr oe of og oh bi translated"><em class="mp">Keras Simple conv net</em>、<a class="ae kv" href="https://keras.io/examples/vision/mnist_convnet/" rel="noopener ugc nofollow" target="_blank">https://keras.io/examples/vision/mnist_convnet/</a></li></ul><h1 id="4161" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">一些(可能的)参考文献</h1><p id="b92a" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">[1] F. Rosenblatt，感知器——一种感知和识别的自动机，康奈尔航空实验室，报告№85-460-1(1957)</p><p id="4f43" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] Y. LeCun，B. Boser，J. S. Denker，D. Henderson，R. E. Howard，W. Hubbard，L. D. Jackel，反向传播应用于手写邮政编码识别，神经计算，1(4):541-551(1989)</p><p id="3797" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]何克明，张晓明，任树生，孙杰，图像识别的深度残差学习，，第770-778页(2016)</p><p id="4150" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4]黄，刘，范德马滕，温伯格，稠密连接卷积网络.CVPR，2261-2269(2017)</p></div></div>    
</body>
</html>