<html>
<head>
<title>Generalizing Your Model: An Example With EfficientNetV2 and Cats &amp; Dogs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">推广你的模型:EfficientNetV2和猫和狗的例子</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generalizing-your-model-an-example-with-efficientnetv2-and-cats-dogs-6903740dfe2c#2022-05-02">https://towardsdatascience.com/generalizing-your-model-an-example-with-efficientnetv2-and-cats-dogs-6903740dfe2c#2022-05-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><div class="ju jv jw jx gt ab cb"><figure class="jy jz ka kb kc kd ke paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><img src="../Images/47b3943b30f0e003b630187dbf925612.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*v9th1RWc_HhG9uz7e2_uaw.gif"/></div></figure><figure class="jy jz ka kb kc kd ke paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><img src="../Images/0d33cd7c16aee20fed58074fa4fabece.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*5P3EWZMJMPjA-hMoCkkRfQ.gif"/></div></figure><figure class="jy jz ka kb kc kd ke paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><img src="../Images/16b73e163f187b682a184d9a2692d67d.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*nHW2DaWpWySVyBzm__pfXA.gif"/></div><p class="kl km gj gh gi kn ko bd b be z dk kp di kq kr translated">作者图片</p></figure></div><p id="e78a" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">考虑一下这个场景。你正在使用最新的CNN网络架构EfficientNetV2来训练一个图像分类器。您已经取得了令人印象深刻的训练准确性(&gt; 95%)，但模型学习评估样本的效果远不如训练样本。</p><p id="ef8c" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">作为机器学习工程师，我们明白我们的模型只有在看不见的数据上表现良好时才是好的。这就引出了一个问题:</p><p id="b1e7" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><em class="lq">我们如何提高网络在不可见数据上的性能？</em></p><p id="23b3" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">当我们的模型过度拟合时，两个最常见的修正是:</p><ol class=""><li id="efa2" class="lr ls iq ku b kv kw kz la ld lt lh lu ll lv lp lw lx ly lz bi translated">在更多样本上训练我们的模型</li><li id="7502" class="lr ls iq ku b kv ma kz mb ld mc lh md ll me lp lw lx ly lz bi translated">改变我们模型的复杂性</li></ol><p id="f791" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">由于获取数据是昂贵的(时间+成本),在这篇博客中，我们将专注于通过增强管道和改变模型复杂性来转换数据。在专注于建筑安全和安保的建筑科技初创公司<a class="ae mf" href="https://forsight.ai/" rel="noopener ugc nofollow" target="_blank"> Forsight </a>，我们的机器学习团队使用这些策略来产生更好的通用模型。</p><p id="d28b" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><em class="lq">你可以在CNN解读</em> <a class="ae mf" href="https://medium.com/@daniel.reiff2/understand-your-algorithm-with-grad-cam-d3b62fce353" rel="noopener"> <em class="lq">这里</em> </a> <em class="lq">和PPE检测</em> <a class="ae mf" href="https://medium.com/swlh/construction-feat-tf2-object-detection-api-4465a3937c87" rel="noopener"> <em class="lq">这里</em> </a> <em class="lq">中了解我们的工作。</em></p><p id="c8c9" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在本文中，我们将使用一个猫和狗的数据集，向您展示如何微调您的模型，并提高看不见的数据的性能。您可以很容易地扩展这个示例，并使用它来改进您自己的模型！让我们跳进来。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="f68d" class="mg mh iq bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">数据集和模型</h1><p id="3523" class="pw-post-body-paragraph ks kt iq ku b kv ne kx ky kz nf lb lc ld ng lf lg lh nh lj lk ll ni ln lo lp ij bi translated">我们将使用来自<a class="ae mf" href="https://www.kaggle.com/c/dogs-vs-cats/data" rel="noopener ugc nofollow" target="_blank"> kaggle </a>的高质量数据集，用一个经典的图像分类问题,保持简单。让我们拍摄20，000张图片，并在其中的16，000张图片上进行训练。剩余的4，000幅图像将用于评估。</p><p id="d421" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们将从基线EfficientNetV2B0模型体系结构开始。基础模型将位于输入/标准化层和二进制分类头之间。二进制分类头将包括一个全局平均池层、一个35%下降的下降层和一个密集预测层。该模型将使用Adam优化器训练100个时期。为了更快地收敛，我们将使用1周期学习率计划，最大值为0.001。1周期学习率由两个阶段组成，以实现超收敛。在第一阶段，我们使用余弦退火将学习速率逐渐提高到最大。在第二阶段，我们使用余弦退火将学习率再次降低到0。你可以在这里阅读更多相关信息<a class="ae mf" href="https://www.avanwyk.com/tensorflow-2-super-convergence-with-the-1cycle-policy/" rel="noopener ugc nofollow" target="_blank">。</a></p><div class="ju jv jw jx gt ab cb"><figure class="jy jz nj kb kc kd ke paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><img src="../Images/7921c8b20e0b2fa92e9616faf76ab25b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*CGpoMk5UCngAh7-_A_TywQ.png"/></div></figure><figure class="jy jz nj kb kc kd ke paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><img src="../Images/cef5c6e0d51b38de49b99957205c65f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Z3QMV4j9s3PUjDUeomADAA.png"/></div><p class="kl km gj gh gi kn ko bd b be z dk nk di nl kr translated">结果(图片由作者提供)</p></figure></div><p id="871e" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在100个时期之后，该模型已经非常好地学习了训练数据，准确率接近100%。但是验证损失正在增加，这表明我们过度适应训练数据，验证准确率停留在85%左右，这还不够好！让我们开始微调模型，以提高图像增强管道在验证样本上的性能。</p><h1 id="f702" class="mg mh iq bd mi mj nm ml mm mn nn mp mq mr no mt mu mv np mx my mz nq nb nc nd bi translated">图像增强</h1><p id="209a" class="pw-post-body-paragraph ks kt iq ku b kv ne kx ky kz nf lb lc ld ng lf lg lh nh lj lk ll ni ln lo lp ij bi translated">数据扩充是一系列保留输出标签的输入转换。这是增加数据集大小和多样性的常用技术。对于影像数据集，常见的变换包括像素级操作，如更改颜色、亮度和添加噪声。像旋转和翻转这样的图像级变换也很常见。让我们在模型训练之前将图像增强管道插入到更大的数据管道中。在一些简单的二元分类问题中，降低模型复杂度也有助于防止过拟合。但在这个例子中，我们将使用图像增强管道来增加训练集的多样性，并有望减少过度拟合。</p><p id="8302" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">对于管道，我们将使用<a class="ae mf" href="https://albumentations.ai/" rel="noopener ugc nofollow" target="_blank">albuminations</a>，这是一个快速灵活的库，广泛用于工业、研究、竞赛和项目。我们将实现像素级的变换:模糊、随机亮度对比、rgb偏移和噪声。之后，我们将实现图像水平转换:水平翻转和随机旋转90度。最后，我们将添加图像压缩。您可以探索不同的增强，序列转换成管道，并用您自己的图像测试一切<a class="ae mf" href="https://albumentations-demo.herokuapp.com/" rel="noopener ugc nofollow" target="_blank">这里</a>！下面是我们的完整管道，带有示例输入&amp;输出图像:</p><div class="ju jv jw jx gt ab cb"><figure class="jy jz nr kb kc kd ke paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><img src="../Images/e5a210b6b0dbfd1596a715ae5137b4d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1572/format:webp/1*VUCySu8dW-xUfZ_u3bjzVQ.png"/></div></figure><figure class="jy jz ns kb kc kd ke paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><img src="../Images/e3ec28e510239dbce5462b714e59042f.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*Qu9UwAHDt5NF4Buml2PHTA.png"/></div><p class="kl km gj gh gi kn ko bd b be z dk nt di nu kr translated">增强管道(图片由作者提供)</p></figure></div><p id="22c9" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">让我们通过研究每个增强应用于每个训练样本的概率来试验管道。现在，我们将保持简单，并应用相同的概率，x，到每一个增加。但在未来，这种可能性可以微调。我们将把这个概率从0%逐渐增加到33%。我们将使用上面详述的相同b0模型，所有其他参数将保持不变。</p><div class="ju jv jw jx gt ab cb"><figure class="jy jz nj kb kc kd ke paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><img src="../Images/5f79c1cf1fdeccc5fa29da1ee5eaeb32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*sVnU-HoGzkb9KnS3cTs2Tg.png"/></div></figure><figure class="jy jz nj kb kc kd ke paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><img src="../Images/bab23c027ab4bde36de83f32f9b5bfb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*AzGSzGYtyqDWrSmYwDB7PA.png"/></div><p class="kl km gj gh gi kn ko bd b be z dk nk di nl kr translated">增强管道实验结果(图片由作者提供)</p></figure></div><p id="4759" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">增强管道提高了模型在不可见数据上的性能，每次应用增强的可能性只有5%!当我们训练模型时，验证损失不再增加，表明模型不再过度拟合。将概率提高到10%+会随着收益递减而进一步减少验证损失。另一方面，当我们增加增量%时，训练性能受到影响。这是因为当训练样本不断变化时，模型更难在训练样本中学习模式。但是模型对评价集的把握更好，这正是我们想要的！模型权重现在明显较少受训练集中的细节和噪声的影响。接下来，让我们将增强概率参数设置为33%。既然模型没有过度拟合，我们就可以通过试验模型的复杂性来进一步提高看不见的数据的性能。</p><h1 id="323f" class="mg mh iq bd mi mj nm ml mm mn nn mp mq mr no mt mu mv np mx my mz nq nb nc nd bi translated">模型复杂性</h1><p id="f902" class="pw-post-body-paragraph ks kt iq ku b kv ne kx ky kz nf lb lc ld ng lf lg lh nh lj lk ll ni ln lo lp ij bi translated">2021年，谭明星和郭怡广推出了更小、更高效的EfficientNet版本EfficientNetV2。在研究了EfficientNet中的瓶颈之后，他们设计了一个新的参数搜索空间，这产生了一个改进的模型架构。你可以阅读更多关于模型架构和参数搜索过程的信息<a class="ae mf" href="https://arxiv.org/pdf/2104.00298.pdf" rel="noopener ugc nofollow" target="_blank">点击</a> ✎编辑<!-- -->。他们还引入了一种新的非均匀缩放策略，其中层逐渐添加到后期阶段，并通过深度参数(层)和宽度参数(通道)按比例放大，以创建更复杂的模型。作者使用他们改进的参数搜索空间创建了一个基线模型EfficientNetV2B0，然后使用他们的缩放策略创建了更复杂的模型。我们将对更复杂的模型进行实验，但也会通过缩小深度参数来降低基线模型的复杂性。以下是我们将尝试的所有模型的概述:</p><figure class="ju jv jw jx gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="gh gi nv"><img src="../Images/8d162402c942a6d1021fd16dd4565242.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F8tTOOl0IJ8DYAHsGE7knA.png"/></div></div><p class="kl km gj gh gi kn ko bd b be z dk translated">作者图片</p></figure><p id="f425" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们感兴趣的是模型复杂性和模型在看不见的数据上的表现之间的关系。这使我们提出以下问题:</p><p id="14fb" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><em class="lq">基线模型对于手头的分类任务来说是否过于复杂？这会导致模型学习训练集中的噪声吗？</em></p><p id="2d6e" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><em class="lq">基线模型不够复杂吗？区分猫和狗需要更多的复杂性吗？</em></p><p id="2663" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们将尝试通过上表中详细列出的各种复杂性的训练模型来回答这些问题。我们将使用一个增强管道，每个转换有33%的机会被应用。所有其他模型和数据集参数将保持不变。</p><div class="ju jv jw jx gt ab cb"><figure class="jy jz nj kb kc kd ke paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><img src="../Images/a5880305647de262a873d43153519e4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*UJJ90gshmobOTjUPvDyJ3g.png"/></div></figure><figure class="jy jz nj kb kc kd ke paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><img src="../Images/48d59c3ec8f4be9fef2380df41162e53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*tFaUSf9CpZtb7ghfbSLkpQ.png"/></div><p class="kl km gj gh gi kn ko bd b be z dk nk di nl kr translated">作者图片</p></figure></div><p id="ce78" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">增加模型的复杂性会提高对不可见数据的处理能力！当将模型复杂性从b0基线模型中的5.92E+6个参数增加到L模型中的1.18E+8个参数时，我们能够将验证损失减少45%。从直觉上讲，区分猫和狗将受益于更复杂的模型。不同品种的猫和狗的外貌差异很大。有些品种的狗看起来很像猫，反之亦然。此外，该模型不能使用颜色和大小等基本特征来区分。需要考虑更复杂的特征，如面部结构和爪子。让我们用Grad-CAM来分析模型看不到的样本。红色区域是导致模型将图像分类为猫或狗的区别特征。你可以在这里阅读更多关于Grad-CAM <a class="ae mf" href="https://medium.com/@daniel.reiff2/understand-your-algorithm-with-grad-cam-d3b62fce353" rel="noopener">解释算法的内容。</a></p><div class="ju jv jw jx gt ab cb"><figure class="jy jz ka kb kc kd ke paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><img src="../Images/4de633fbb7baac76eeaf3607bfdee0d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*QoPMLMNOXmeg52428OQtfA.gif"/></div></figure><figure class="jy jz ka kb kc kd ke paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><img src="../Images/5a48738eba1df38111a2ac277ad7bc92.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*ZnSyeMJ2uoG-ePJBi0YmNQ.gif"/></div></figure><figure class="jy jz ka kb kc kd ke paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><img src="../Images/b54547b9bdf52ad620a5e807d3623a3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*f1M4ZtSw2s8n1RNIwwj80Q.gif"/></div><p class="kl km gj gh gi kn ko bd b be z dk kp di kq kr translated">作者图片</p></figure></div><p id="94a3" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在这两个狗样本中，随着我们增加模型的复杂性，红色区域更加精确地符合狗的身体。在低复杂度下，红色区域包括部分背景，表明这些模型没有真正学会狗的区别特征。特别令人吃惊的是，对于右边的狗来说，随着我们逐渐增加复杂度，红色区域从背景中转移出来，并以狗的脸和鼻子为目标。</p><p id="359a" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">对于cat示例，关注点无处不在，尤其是在不太复杂的模型中。B0–3模型强调图像的所有部分，除了猫的脸。但是，当我们继续添加更多的复杂性，红色区域在猫的脸上归零。最复杂的模型，L，强调猫的胡须，我们知道这是一个显著的特征！</p><h1 id="5f5c" class="mg mh iq bd mi mj nm ml mm mn nn mp mq mr no mt mu mv np mx my mz nq nb nc nd bi translated">结论</h1><p id="c567" class="pw-post-body-paragraph ks kt iq ku b kv ne kx ky kz nf lb lc ld ng lf lg lh nh lj lk ll ni ln lo lp ij bi translated">在这篇博客中，我们希望提供一些有用的见解和工具，来提高你的模型在看不见的数据上的性能。在猫和狗的帮助下，我们探索了一种图像增强管道来减少过度拟合。此外，我们还试验了不同的模型复杂性。在这个例子中，我们见证了更多的复杂性如何帮助我们的模型聚焦于更复杂的特性。<strong class="ku ir">最重要的是，我们使用这些工具将未知数据的模型准确率从大约85%提高到大约97%！</strong>即使有最好的模型和增强管道，我们的模型仍然会出现一些样本错误。正如你在下面看到的，一些狗的样本看起来像猫(左图)，反之亦然(右图)。</p><div class="ju jv jw jx gt ab cb"><figure class="jy jz nj kb kc kd ke paragraph-image"><img src="../Images/498c925d710b7a849aab09e1a1461e8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*3JtcPqJiAAW0gL19-dZCSw.png"/></figure><figure class="jy jz nj kb kc kd ke paragraph-image"><img src="../Images/26890cddc6361e069c35e9d95ace8be6.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*Ib5iDBoTEgncRkJv3aPDzQ.png"/><p class="kl km gj gh gi kn ko bd b be z dk nk di nl kr translated">狗(左)和猫(右)(图片由作者提供)</p></figure></div><p id="313e" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在机器学习中，我们只能通过在我们的架构和管道中试验参数来提高模型性能。我们希望这篇文章能为您提供关于如何充分利用数据的宝贵见解！</p><p id="6225" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><em class="lq">如果你对这个话题感兴趣，并且你愿意研究类似的问题，请联系我们</em><a class="ae mf" href="https://forsight.ai/contact/" rel="noopener ugc nofollow" target="_blank"><em class="lq"/></a><em class="lq">。</em></p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="7bc4" class="mg mh iq bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">参考</h1><ol class=""><li id="5c44" class="lr ls iq ku b kv ne kz nf ld nw lh nx ll ny lp lw lx ly lz bi translated">谭明星，郭诉乐，2021，“高效网络2:更小的模型和更快的训练”，<a class="ae mf" href="https://arxiv.org/abs/2104.00298" rel="noopener ugc nofollow" target="_blank"/></li><li id="2b72" class="lr ls iq ku b kv ma kz mb ld mc lh md ll me lp lw lx ly lz bi translated">Jason Browlnee，“使用机器学习算法的过度拟合和欠拟合”，<a class="ae mf" href="https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/" rel="noopener ugc nofollow" target="_blank">https://machineellingmastery . com/over fit-and-under fitting-With-Machine-Learning-Algorithms/</a></li><li id="d693" class="lr ls iq ku b kv ma kz mb ld mc lh md ll me lp lw lx ly lz bi translated">卡格尔猫狗数据集，<a class="ae mf" href="https://www.microsoft.com/en-us/download/details.aspx?id=54765" rel="noopener ugc nofollow" target="_blank">https://www.microsoft.com/en-us/download/details.aspx?id=54765 </a>。许可:社区数据许可协议-许可-版本2.0。本协议对结果的使用、修改或共享不施加任何限制或义务</li><li id="3ad0" class="lr ls iq ku b kv ma kz mb ld mc lh md ll me lp lw lx ly lz bi translated">“Tensorflow 2中具有1周期策略的超级收敛，<a class="ae mf" href="https://www.avanwyk.com/tensorflow-2-super-convergence-with-the-1cycle-policy/" rel="noopener ugc nofollow" target="_blank">https://www . avan wyk . com/tensor flow-2-Super-convergence-with-1 cycle-Policy/</a></li><li id="f091" class="lr ls iq ku b kv ma kz mb ld mc lh md ll me lp lw lx ly lz bi translated">Jason Brownlee，“如何避免深度学习神经网络中的过拟合”，<a class="ae mf" href="https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/" rel="noopener ugc nofollow" target="_blank">https://machineellingmastery . com/introduction-to-regulatory-to-reduce-over fitting-and-improve-generalization-error/</a></li></ol></div></div>    
</body>
</html>