<html>
<head>
<title>Create a K-Nearest Neighbors Algorithm from Scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python从头开始创建K-最近邻算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/create-your-own-k-nearest-neighbors-algorithm-in-python-eb7093fc6339#2022-04-09">https://towardsdatascience.com/create-your-own-k-nearest-neighbors-algorithm-in-python-eb7093fc6339#2022-04-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8516" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过亲身实践来巩固你对KNN的了解</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7dde8b448ebb5fc74c0c9cfbaa30c3b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BuernlLyuIvHAgb5"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">马库斯·斯皮斯克在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="30a2" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="ccbf" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">有一句名言说:“你是和你相处时间最长的五个人的平均值。”尽管我们不会为你的友谊质量建模(任何人的作品集)。)，本教程将教授一种简单直观的算法方法来根据数据的邻居对数据进行分类。k-最近邻(knn)算法是一种监督学习算法，具有优雅的执行和令人惊讶的简单实现。正因为如此，knn为机器学习初学者提供了一个很好的学习机会，用几行Python代码创建一个强大的分类或回归算法。</p><h1 id="7f12" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">算法</h1><p id="ea4f" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">Knn是一个<strong class="lq ir">监督的</strong>机器学习算法。监督模型既有目标变量又有独立变量的<strong class="lq ir">。<strong class="lq ir">目标变量</strong>或因变量，表示为y，取决于自变量，是您寻求预测的值。标为X(单值)或X(多值)的<strong class="lq ir">独立变量</strong>提前已知，用于预测y。</strong></p><p id="189d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">Knn既可以用于<strong class="lq ir">分类</strong>也可以用于<strong class="lq ir">回归</strong>。<strong class="lq ir">分类</strong>模型预测一个<em class="mp">分类</em>目标变量，而<strong class="lq ir">回归</strong>模型预测一个<em class="mp">数字</em>目标。</p><p id="ba9d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">假设您有一个标量属性和对应于这些属性的类的数据集。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/c784ae0c32e3e82e867275ec2138a924.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iM3GTYZdDhkLz8Ob9QkZrQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/fe6876f51940c415b7033c9e171b1a6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*_KWItDJn99blfo_KUtSN-Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="bc8e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这里，n是数据点的总数，m是类的总数。y不是一个类，它也可以是一个标量值，knn可以用作回归，但对于本教程，我们将重点放在分类上。</p><p id="814d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在这个二维例子中，我们可以很容易地在二维空间中看到这些点。假设类将倾向于与该空间中相同类的点聚集在一起，我们可以通过在其附近最频繁出现的类来对新点进行分类。因此，在给定点k被指定为所述点附近要考虑的邻居的数量，并且从这些邻居中，最频繁出现的类被预测为即将到来的点的类。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/094680f1879b222b059bd069f28e1f07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*3YDJqw62VAxkHjgQkmWm_g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="7514" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">图1: </strong> <em class="mp">当k=1时，此点归为第1组。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/e178564d20f5b3c8bbbe3485a53e86aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*IGJ9zJd_nLJcnlnT4_lHew.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="c87e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">图2: </strong> <em class="mp">当k=3时，该点被归为0组。</em></p><h1 id="c13c" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">数据</h1><p id="2758" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们将用<a class="ae kv" href="https://archive.ics.uci.edu/ml/datasets/iris" rel="noopener ugc nofollow" target="_blank"> UCI机器学习库虹膜数据集</a>来评估我们的算法。但是，任何由标量输入组成的分类数据集都可以。我们将对数据集进行解包，并对属性进行标准化，使其均值和单位方差为零。这样做是因为我们不想判断哪些特征对于预测类是最重要的(对于这个分析！).最后，我们将把数据集分成训练集和测试集，测试集由20%的原始数据集组成。</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="0b03" class="my kx iq mu b gy mz na l nb nc">from sklearn import datasets<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler</span><span id="4333" class="my kx iq mu b gy nd na l nb nc"># Unpack the iris dataset, from UCI Machine Learning Repository<br/>iris = datasets.load_iris()<br/>X = iris['data']<br/>y = iris['target']</span><span id="5d03" class="my kx iq mu b gy nd na l nb nc"># Preprocess data<br/>X = StandardScaler().fit_transform(X)</span><span id="7b76" class="my kx iq mu b gy nd na l nb nc"># Split data into train &amp; test sets<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span></pre><h1 id="ebc9" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">模型创建</h1><p id="908b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated"><strong class="lq ir">助手功能</strong></p><p id="c7f2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在这个算法中，我们需要找出给定组中多次出现的最常见的元素。下面的函数给出了一种直观的Pythonic式方法来查找列表中最常见的元素。</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="ee22" class="my kx iq mu b gy mz na l nb nc">def most_common(lst):<br/>    '''Returns the most common element in a list'''<br/>    return max(set(lst), key=lst.count)</span></pre><p id="5a42" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">接下来，我们需要计算数据集中一个点和每个点之间的距离。最常见和直观的方法是欧几里德距离，但是也可以使用其他距离方法。</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="1805" class="my kx iq mu b gy mz na l nb nc">def euclidean(point, data):<br/>    '''Euclidean distance between a point  &amp; data'''<br/>    return np.sqrt(np.sum((point - data)**2, axis=1))</span></pre><p id="1d0c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">实施</strong></p><p id="3e77" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，让我们开始构造一个knn类。对于给定的knn分类器，我们将指定k和一个距离度量。为了保持该算法的实现类似于广泛使用的scikit-learn套件，我们将初始化self。X_train和self.y_train，但是这可以在初始化时完成。</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="41d1" class="my kx iq mu b gy mz na l nb nc">class KNeighborsClassifier():<br/>    def __init__(self, k=5, dist_metric=euclidean):<br/>        self.k = k<br/>        self.dist_metric = dist_metric</span><span id="8671" class="my kx iq mu b gy nd na l nb nc">    def fit(self, X_train, y_train):<br/>        self.X_train = X_train<br/>        self.y_train = y_train</span></pre><p id="b2e5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">接下来，knn算法的大部分由预测方法执行。逐点迭代属性数据集(X_test)。对于每个数据点，执行以下步骤:</p><ol class=""><li id="aacd" class="ne nf iq lq b lr mk lu ml lx ng mb nh mf ni mj nj nk nl nm bi translated">计算到训练数据集中每个点的距离</li><li id="e90d" class="ne nf iq lq b lr nn lu no lx np mb nq mf nr mj nj nk nl nm bi translated">训练数据集类按到数据点的距离排序</li><li id="327e" class="ne nf iq lq b lr nn lu no lx np mb nq mf nr mj nj nk nl nm bi translated">前k个类保存并存储在邻居列表中。现在，我们只需将最近邻居列表映射到我们的most_common函数，返回X_test中传递的每个点的预测列表。</li></ol><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="d58b" class="my kx iq mu b gy mz na l nb nc">class KNeighborsClassifier():<br/>    def __init__(self, k=5, dist_metric=euclidean):<br/>        self.k = k<br/>        self.dist_metric = dist_metric</span><span id="0719" class="my kx iq mu b gy nd na l nb nc">    def fit(self, X_train, y_train):<br/>        self.X_train = X_train<br/>        self.y_train = y_train</span><span id="a57e" class="my kx iq mu b gy nd na l nb nc">    def predict(self, X_test):<br/>        neighbors = []<br/>        for x in X_test:<br/>            distances = self.dist_metric(x, self.X_train)<br/>            y_sorted = [y for _, y in sorted(zip(distances, self.y_train))]<br/>            neighbors.append(y_sorted[:self.k])</span><span id="b0b8" class="my kx iq mu b gy nd na l nb nc">        return list(map(most_common, neighbors))</span></pre><p id="0363" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，定义了一种评估方法来方便地评估模型的性能。属性及其类的数据集作为X_test和y_test进行传递，并且将属性的模型预测与实际类进行比较。</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="3b1f" class="my kx iq mu b gy mz na l nb nc">class KNeighborsClassifier():<br/>    def __init__(self, k=5, dist_metric=euclidean):<br/>        self.k = k<br/>        self.dist_metric = dist_metric</span><span id="5430" class="my kx iq mu b gy nd na l nb nc">    def fit(self, X_train, y_train):<br/>        self.X_train = X_train<br/>        self.y_train = y_train</span><span id="5432" class="my kx iq mu b gy nd na l nb nc">    def predict(self, X_test):<br/>        neighbors = []<br/>        for x in X_test:<br/>            distances = self.dist_metric(x, self.X_train)<br/>            y_sorted = [y for _, y in sorted(zip(distances, self.y_train))]<br/>            neighbors.append(y_sorted[:self.k])</span><span id="23a1" class="my kx iq mu b gy nd na l nb nc">        return list(map(most_common, neighbors))</span><span id="c32c" class="my kx iq mu b gy nd na l nb nc">    def evaluate(self, X_test, y_test):<br/>        y_pred = self.predict(X_test)<br/>        accuracy = sum(y_pred == y_test) / len(y_test)<br/>        return accuracy</span></pre><p id="1448" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">信不信由你，我们已经完成了——我们可以轻松地部署这个算法来建模分类问题。但是，为了完整性，我们应该为iris数据集优化k。我们可以通过迭代k的范围并绘制模型的性能来做到这一点。</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="9a5f" class="my kx iq mu b gy mz na l nb nc">accuracies = []<br/>ks = range(1, 30)<br/>for k in ks:<br/>    knn = KNeighborsClassifier(k=k)<br/>    knn.fit(X_train, y_train)<br/>    accuracy = knn.evaluate(X_test, y_test)<br/>    accuracies.append(accuracy)</span><span id="ff80" class="my kx iq mu b gy nd na l nb nc">fig, ax = plt.subplots()<br/>ax.plot(ks, accuracies)<br/>ax.set(xlabel="k",<br/>       ylabel="Accuracy",<br/>       title="Performance of knn")<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/c6595f06f894ef1a8e02146e96ce7af6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*HEx1LkdHYSLCM2qx"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="c526" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">图3: </strong> <em class="mp"> knn精度对比k </em></p><p id="03e1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">看来我们的knn模型在低k下表现最好。</p><h1 id="1afc" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="198b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">就这样，我们结束了。我们用不到100行python代码实现了一个简单直观的k近邻算法(不包括绘图和数据解包，不到50行)。下面包含了整个项目代码。</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="5f59" class="my kx iq mu b gy mz na l nb nc">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn import datasets<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler<br/></span><span id="9c6d" class="my kx iq mu b gy nd na l nb nc">def most_common(lst):<br/>    return max(set(lst), key=lst.count)<br/></span><span id="75d9" class="my kx iq mu b gy nd na l nb nc">def euclidean(point, data):<br/>    # Euclidean distance between points a &amp; data<br/>    return np.sqrt(np.sum((point - data)**2, axis=1))<br/></span><span id="2076" class="my kx iq mu b gy nd na l nb nc">class KNeighborsClassifier:<br/>    def __init__(self, k=5, dist_metric=euclidean):<br/>        self.k = k<br/>        self.dist_metric = dist_metric</span><span id="40b7" class="my kx iq mu b gy nd na l nb nc">    def fit(self, X_train, y_train):<br/>        self.X_train = X_train<br/>        self.y_train = y_train</span><span id="d42f" class="my kx iq mu b gy nd na l nb nc">    def predict(self, X_test):<br/>        neighbors = []<br/>        for x in X_test:<br/>            distances = self.dist_metric(x, self.X_train)<br/>            y_sorted = [y for _, y in sorted(zip(distances, self.y_train))]<br/>            neighbors.append(y_sorted[:self.k])</span><span id="b819" class="my kx iq mu b gy nd na l nb nc">        return list(map(most_common, neighbors))</span><span id="d1a1" class="my kx iq mu b gy nd na l nb nc">    def evaluate(self, X_test, y_test):<br/>        y_pred = self.predict(X_test)<br/>        accuracy = sum(y_pred == y_test) / len(y_test)<br/>        return accuracy<br/></span><span id="1af4" class="my kx iq mu b gy nd na l nb nc"># Unpack the iris dataset, from UCI Machine Learning Repository<br/>iris = datasets.load_iris()<br/>X = iris['data']<br/>y = iris['target']</span><span id="4f70" class="my kx iq mu b gy nd na l nb nc"># Split data into train &amp; test sets<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span><span id="8cc3" class="my kx iq mu b gy nd na l nb nc"># Preprocess data<br/>ss = StandardScaler().fit(X_train)<br/>X_train, X_test = ss.transform(X_train), ss.transform(X_test)</span><span id="7c93" class="my kx iq mu b gy nd na l nb nc"># Test knn model across varying ks<br/>accuracies = []<br/>ks = range(1, 30)<br/>for k in ks:<br/>    knn = KNeighborsClassifier(k=k)<br/>    knn.fit(X_train, y_train)<br/>    accuracy = knn.evaluate(X_test, y_test)<br/>    accuracies.append(accuracy)</span><span id="4bbd" class="my kx iq mu b gy nd na l nb nc"># Visualize accuracy vs. k<br/>fig, ax = plt.subplots()<br/>ax.plot(ks, accuracies)<br/>ax.set(xlabel="k",<br/>       ylabel="Accuracy",<br/>       title="Performance of knn")<br/>plt.show()</span></pre><p id="4f5a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">回到我们的名言“你是和你相处时间最长的五个人的平均值”，knn分类应该改为“你是和你相处时间最长的k个人中最频繁的。”对于一个独立的挑战，通过创建一个knn回归模型来修改这个代码以更好地适应原始代码，其中一个点被解释为它的k个最近邻居的平均标量目标值。</p><p id="ece7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">感谢阅读！<br/> <a class="ae kv" href="https://www.linkedin.com/in/turnermluke/" rel="noopener ugc nofollow" target="_blank">在LinkedIn上跟我联系</a> <br/> <a class="ae kv" href="https://github.com/turnerluke/ML-algos/blob/main/knn/KNeighborsClassifier.py" rel="noopener ugc nofollow" target="_blank">在GitHub中看到这个项目</a> <br/> <a class="ae kv" href="https://github.com/turnerluke/ML-algos/blob/main/knn/KNeighborsRegressor.py" rel="noopener ugc nofollow" target="_blank">看到一个knn回归实现</a></p></div></div>    
</body>
</html>