<html>
<head>
<title>Apache Spark for Data Science — Word Count With Spark and NLTK</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark for Data Science—Spark和NLTK的字数统计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/apache-spark-for-data-science-word-count-with-spark-and-nltk-ed312b3b667d#2022-04-16">https://towardsdatascience.com/apache-spark-for-data-science-word-count-with-spark-and-nltk-ed312b3b667d#2022-04-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="521c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><strong class="ak">学习计算一本书的字数并解决常见的停用词问题——在PySpark中实现</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c8135a859c02c88efc4d1588e1bf54a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7IajqC5EwWRzch9U"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@wflwong?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">沃伦·王</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="bf6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你知道Apache Spark最常见的初学者练习是什么吗？你猜对了——这是字数统计。这个想法是抓取一个文本文档，最好是一个长文档，并统计每个单词的出现次数。这是一个典型的MapReduce任务，你可以用Spark的RDDs来处理。今天你将学习如何解决它，以及如何很好地解决它。</p><p id="33c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将讨论三种解决方案:</p><ol class=""><li id="56bb" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">基本解法</strong> —用Spark的<code class="fe me mf mg mh b">countByValue()</code>法统计字数。对初学者来说还可以，但不是最优解。</li><li id="fff5" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated"><strong class="lb iu">带正则表达式的MapReduce</strong>—所有文本并不平等。“python”、“python”和“Python”这些词对你我来说是一样的，但对Spark来说不是。</li><li id="ab68" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated">MapReduce with NLTK  —通过过滤掉常见的英语停用词，将整个事情提升到一个新的水平。我们不在乎“the”出现多少次，它是无用的信息。</li></ol><p id="e9af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有许多工作要做，所以让我们马上开始吧。</p><p id="83aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不想看书？我曾以视频格式报道过相同的主题:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mn mo l"/></div></figure></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="7806" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">Spark中用于字数统计的数据集</h1><p id="5dab" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">我使用过古腾堡计划的这本免费电子书，名为《从废物中获取数百万》。我从没读过，但对我们想做的事来说，它似乎足够长了。下载<a class="ae ky" href="https://www.gutenberg.org/cache/epub/67837/pg67837.txt" rel="noopener ugc nofollow" target="_blank"> TXT版本</a> — <em class="nt">右键</em> — <em class="nt">另存为</em>—<em class="nt">millions _ from _ waste . TXT</em>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/8eef1d3ddae05267613d4a6d759faeed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Nllt8dswVKTZ6MXt051HQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片1 —来自废书的数百万(图片由作者提供)</p></figure><p id="55f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在继续之前，请确保您知道文本文件的绝对路径。</p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="2097" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">使用Spark和Python进行简单的字数统计</h1><p id="5a43" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">与任何Spark应用程序一样，您需要做的第一件事是创建一个新的Spark会话。使用以下代码创建一个名为<code class="fe me mf mg mh b">word-counts</code>的本地会话:</p><pre class="kj kk kl km gt nv mh nw nx aw ny bi"><span id="f62f" class="nz mx it mh b gy oa ob l oc od">from pyspark import SparkConf, SparkContext</span><span id="31ee" class="nz mx it mh b gy oe ob l oc od">conf = SparkConf().setMaster("local").setAppName("word-counts")<br/>sc = SparkContext(conf=conf)</span></pre><p id="3fad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，从文本文件加载数据集，并使用<code class="fe me mf mg mh b">textFile()</code>方法将其转换为RDD:</p><pre class="kj kk kl km gt nv mh nw nx aw ny bi"><span id="8fb8" class="nz mx it mh b gy oa ob l oc od">book = sc.textFile("file:///Users/dradecic/Desktop/millions_from_waste.txt")<br/>book.collect()[:10]</span></pre><p id="75c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe me mf mg mh b">collect()</code>方法打印整个RDD - <code class="fe me mf mg mh b">[:10]</code>确保只打印前十行。让我们看看我们在处理什么:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/2f133764cb5a36e11aa64965dd74e02e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*prBny8Y-iBGxTLeeKzdGQQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2 —书的前10行(作者图片)</p></figure><p id="ce83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">文本文档的每一行都是Spark RDD文件的一个专用元素。由于每一行都是一个字符串，我们可以将它拆分成单个单词，并调用<code class="fe me mf mg mh b">countByValue()</code>来获得单词计数:</p><pre class="kj kk kl km gt nv mh nw nx aw ny bi"><span id="8046" class="nz mx it mh b gy oa ob l oc od">word_counts = book.flatMap(lambda x: x.split()).countByValue()</span><span id="7424" class="nz mx it mh b gy oe ob l oc od">for i, (word, count) in enumerate(word_counts.items()):<br/>    if i == 15: break<br/>    print(word, count)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/55a70dc0e28ee034f12a6b7d807efa9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*W6zSKymZpoFl8G4jEE_U9A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图PySpark中的简单字数统计(图片由作者提供)</p></figure><p id="79c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就这样吗？嗯，不。我们有字数统计，但截至目前，Spark区分小写和大写字母和标点符号。我们这里也有很多停用词，比如“The”、“of”、“A”、“is”等等。接下来我们将讨论大写字母和标点符号，把停用词留到以后。</p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="366b" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">PySpark中正则表达式的字数统计</h1><p id="8366" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">正则表达式允许我们指定一个可搜索的模式，并用其他内容替换字符串中的任何内容。我们的目标是小写每个单词，并删除标点符号。这里有两个功能可以帮助我们:</p><ol class=""><li id="1019" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><code class="fe me mf mg mh b">preprocess_word()</code> -对单个单词应用正则表达式(删除标点符号)并使其小写。</li><li id="def5" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated"><code class="fe me mf mg mh b">preprocess_words()</code> -将<code class="fe me mf mg mh b">preprocess_word()</code>应用于作为字符串传递的单词序列。</li></ol><p id="76db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些功能的定义和使用示例如下所示:</p><pre class="kj kk kl km gt nv mh nw nx aw ny bi"><span id="acd4" class="nz mx it mh b gy oa ob l oc od">import re<br/></span><span id="8843" class="nz mx it mh b gy oe ob l oc od">def preprocess_word(word: str):<br/>    return re.sub("[^A-Za-z0-9]+", "", word.lower())</span><span id="6266" class="nz mx it mh b gy oe ob l oc od">def preprocess_words(words: str):<br/>    return [preprocess_word(word) for word in words.split()]<br/>    <br/>    <br/>preprocess_words("The Project Gutenberg eBook of Millions from Waste, by Frederick A.")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/612e3fb62b3659b6ec2ec9a0345c161d.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*DGIf0jVtEAXrWjRZ9lJMaA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片4 —书的预处理句子(图片由作者提供)</p></figure><p id="fab7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这正是我们想要的——所有的单词都是小写，标点符号被删除。让我们用同样的方法计算字数:</p><pre class="kj kk kl km gt nv mh nw nx aw ny bi"><span id="64b0" class="nz mx it mh b gy oa ob l oc od">book = sc.textFile("file:///Users/dradecic/Desktop/millions_from_waste.txt")</span><span id="eb57" class="nz mx it mh b gy oe ob l oc od">word_counts = book.flatMap(preprocess_words).countByValue()<br/>for i, (word, count) in enumerate(word_counts.items()):<br/>    if i == 15: break<br/>    print(word, count)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/9213717dfb8d9191535d50760eab79d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*4PmnKNUalhEhdl1Nn1gfEg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5 —预处理输入的字数(作者提供的图片)</p></figure><p id="fe44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有另一种统计单词的方法，这将使排序变得容易得多。你必须:</p><ul class=""><li id="287a" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu oj mb mc md bi translated">对输入的单词进行计数——首先从<code class="fe me mf mg mh b">(x)</code>到<code class="fe me mf mg mh b">(x, 1)</code>。</li><li id="c3c3" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu oj mb mc md bi translated">应用reducer方法对计数求和:</li></ul><pre class="kj kk kl km gt nv mh nw nx aw ny bi"><span id="79cc" class="nz mx it mh b gy oa ob l oc od">book = sc.textFile("file:///Users/dradecic/Desktop/millions_from_waste.txt")</span><span id="4d5b" class="nz mx it mh b gy oe ob l oc od">words = book.flatMap(preprocess_words)<br/>word_counts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)<br/>word_counts.collect()[:10]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/fe6843fd1f7f04128bab70f31f91ce4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*T0z7SvvaAtGY-FUBBwp7bg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6 —重新格式化的字数(作者图片)</p></figure><p id="17f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Spark附带了一个叫做<code class="fe me mf mg mh b">sortByKey()</code>的方法，用于排序。唯一的问题是，目前的关键是单词而不是计数。使用以下代码反转单词和计数:</p><pre class="kj kk kl km gt nv mh nw nx aw ny bi"><span id="2126" class="nz mx it mh b gy oa ob l oc od">word_counts_sorted = word_counts.map(lambda x: (x[1], x[0]))<br/>word_counts_sorted.collect()[:10]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/103254f3f72c5431c19251fc9a4307d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*17cYDoGoUFDQUpl-TJLJig.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7 —排序字数(1)(按作者排序)</p></figure><p id="68bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，最后，按关键字对RDD进行排序:</p><pre class="kj kk kl km gt nv mh nw nx aw ny bi"><span id="f246" class="nz mx it mh b gy oa ob l oc od">word_counts_sorted = word_counts.map(lambda x: (x[1], x[0])).sortByKey()<br/>word_counts_sorted.collect()[:10]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/b6ed17a169735ae5525bfc6a4fa43ec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*5EtZDdX8c3AEVfvbXlVeeQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图8-排序字数(2)(作者图片)</p></figure><p id="696b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">单词按升序排列，这不是我们想要的。<code class="fe me mf mg mh b">sortByKey()</code>的第一个参数是<code class="fe me mf mg mh b">ascending</code>，默认设置为<code class="fe me mf mg mh b">True</code>。将它翻转到<code class="fe me mf mg mh b">False</code>以获得按计数降序排列的单词:</p><pre class="kj kk kl km gt nv mh nw nx aw ny bi"><span id="d9c1" class="nz mx it mh b gy oa ob l oc od">word_counts_sorted = word_counts.map(lambda x: (x[1], x[0])).sortByKey(False)<br/>word_counts_sorted.collect()[:10]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/bc0cda5dc12468e3c730556b2da501fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*kFuYLan-jVa7AA_XqrndHA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图9-排序字数(3)(作者图片)</p></figure><p id="9329" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是可行的，但我们还有另一个问题。所有出现频率最高的单词都是停用词，从这些词中，我们无法猜测这本书的内容。在下一节中，我们将使用NLTK模块来消除它们。</p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="f5b9" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">用NLTK从字数中删除停用词</h1><p id="75e1" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">如果您还没有使用过NLTK，那么您必须安装这个库并下载停用词。直接从笔记本中逐行运行以下代码:</p><pre class="kj kk kl km gt nv mh nw nx aw ny bi"><span id="2293" class="nz mx it mh b gy oa ob l oc od">!pip install nltk</span><span id="7c39" class="nz mx it mh b gy oe ob l oc od">import nltk<br/>nltk.download("stopwords")</span></pre><p id="e4bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完成后，导入<code class="fe me mf mg mh b">stopwords</code>模块，用英语打印前几个:</p><pre class="kj kk kl km gt nv mh nw nx aw ny bi"><span id="b28f" class="nz mx it mh b gy oa ob l oc od">from nltk.corpus import stopwords</span><span id="e1de" class="nz mx it mh b gy oe ob l oc od">stop_words = stopwords.words("english")<br/>stop_words[:10]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/6cba808cb4b810b4d0d43080a4baab2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GelBgWNSX6ei83ZGLELYBg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片10 —前10个英文停用词(图片由作者提供)</p></figure><p id="f191" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，下一步是什么？我们想要修改<code class="fe me mf mg mh b">preprocess_words()</code>函数来排除停用词。向其中添加另一行，以便只保留不是空字符串和停用词的单词:</p><pre class="kj kk kl km gt nv mh nw nx aw ny bi"><span id="2659" class="nz mx it mh b gy oa ob l oc od">def preprocess_word(word: str):<br/>    return re.sub("[^A-Za-z0-9]+", "", word.lower())</span><span id="8367" class="nz mx it mh b gy oe ob l oc od">def preprocess_words(words: str):<br/>    preprocessed = [preprocess_word(word) for word in words.split()]<br/>    return [word for word in preprocessed if word not in stop_words and word != ""]<br/>    </span><span id="0659" class="nz mx it mh b gy oe ob l oc od">preprocess_words("The Project Gutenberg eBook of Millions from Waste, by Frederick A.")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/e57677c5e608a2e9550667433a0fb942.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jaVZZ18h5ULGE_FWjjBZcQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图11 —第一个没有停用词的句子(作者图片)</p></figure><p id="99d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这次输出更干净。我们没有像上次一样听到任何胡言乱语。剩下的唯一步骤是再次计算和排序字数。代码和以前一样:</p><pre class="kj kk kl km gt nv mh nw nx aw ny bi"><span id="392c" class="nz mx it mh b gy oa ob l oc od">book = sc.textFile("file:///Users/dradecic/Desktop/millions_from_waste.txt")</span><span id="e7fa" class="nz mx it mh b gy oe ob l oc od">words = book.flatMap(preprocess_words)<br/>word_counts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)<br/>word_counts_sorted = word_counts.map(lambda x: (x[1], x[0])).sortByKey(False)<br/>word_counts_sorted.collect()[:10]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/60c8e0811278148b10c253d0d1065e19.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*whewNPEtSM_tBqvVj77zTQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图12—Spark中没有停用词的字数(图片由作者提供)</p></figure><p id="1cc6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们只打印了前10个单词，但是您已经明白了这一点——这次的输出更加清晰。随意探索其他单词，甚至可以在图表上显示它们的频率。</p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="2228" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">Apache Spark中的字数汇总</h1><p id="615f" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">今天，您已经学习了如何使用Python和Spark，仅使用RDD语法来解决最流行的MapReduce问题。整个事情简单得令人难以置信，但是您可以(也应该)通过预处理每个单词并排除最常见的单词来丰富解决方案。</p><p id="319d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将深入SparkSQL，敬请关注！</p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><p id="ceb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nt">喜欢这篇文章吗？成为</em> <a class="ae ky" href="https://medium.com/@radecicdario/membership" rel="noopener"> <em class="nt">中等会员</em> </a> <em class="nt">继续无限制学习。如果你使用下面的链接，我会收到你的一部分会员费，不需要你额外付费。</em></p><div class="or os gp gr ot ou"><a href="https://medium.com/@radecicdario/membership" rel="noopener follow" target="_blank"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">通过我的推荐链接加入Medium-Dario rade ci</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">medium.com</p></div></div><div class="pd l"><div class="pe l pf pg ph pd pi ks ou"/></div></div></a></div></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h2 id="c20c" class="nz mx it bd my pj pk dn nc pl pm dp ng li pn po ni lm pp pq nk lq pr ps nm pt bi translated">推荐阅读</h2><ul class=""><li id="8aab" class="lv lw it lb b lc no lf np li pu lm pv lq pw lu oj mb mc md bi translated"><a class="ae ky" href="https://betterdatascience.com/best-data-science-prerequisite-books/" rel="noopener ugc nofollow" target="_blank">学习数据科学先决条件(数学、统计和编程)的5本最佳书籍</a></li><li id="a5a8" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu oj mb mc md bi translated"><a class="ae ky" href="https://betterdatascience.com/top-books-to-learn-data-science/" rel="noopener ugc nofollow" target="_blank">2022年学习数据科学的前5本书</a></li><li id="d677" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu oj mb mc md bi translated"><a class="ae ky" href="https://betterdatascience.com/python-list-print/" rel="noopener ugc nofollow" target="_blank">用Python打印列表的7种方法</a></li></ul><h2 id="d2bf" class="nz mx it bd my pj pk dn nc pl pm dp ng li pn po ni lm pp pq nk lq pr ps nm pt bi translated">保持联系</h2><ul class=""><li id="93b4" class="lv lw it lb b lc no lf np li pu lm pv lq pw lu oj mb mc md bi translated">雇用我作为一名技术作家</li><li id="100c" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu oj mb mc md bi translated">订阅<a class="ae ky" href="https://www.youtube.com/c/BetterDataScience" rel="noopener ugc nofollow" target="_blank"> YouTube </a></li><li id="1c1d" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu oj mb mc md bi translated">在LinkedIn 上连接</li></ul></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><p id="47b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nt">原载于2022年4月16日</em><a class="ae ky" href="https://betterdatascience.com/apache-spark-word-count/" rel="noopener ugc nofollow" target="_blank"><em class="nt">【https://betterdatascience.com】</em></a><em class="nt">。</em></p></div></div>    
</body>
</html>