<html>
<head>
<title>Image Segmentation, UNet, and Deep Supervision Loss Using Keras Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Keras模型的图像分割、UNet和深度监督损失</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/image-segmentation-unet-and-deep-supervision-loss-using-keras-model-f21a9856750a#2022-09-28">https://towardsdatascience.com/image-segmentation-unet-and-deep-supervision-loss-using-keras-model-f21a9856750a#2022-09-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="03d1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用于分割的深度CNN经常遭受消失梯度。我们能通过计算不同输出水平的损耗来解决这个问题吗？</h2></div><p id="4e67" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图像分割需要将图像像素分成不同的类别。一些应用包括识别医学图像中的肿瘤区域，在无人机图像中分离陆地和水域等。与CNN输出类别概率得分向量的分类不同，分割需要CNN输出图像。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/5113f9c73d7ecc3068257c2217dabf6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6MPQK-THSl-22nJ41sC_1g.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">网球运动员的图像分割(<a class="ae lr" href="https://cocodataset.org/#explore?id=47780" rel="noopener ugc nofollow" target="_blank">来源</a> - <a class="ae lr" href="https://cocodataset.org/#termsofuse" rel="noopener ugc nofollow" target="_blank">知识共享署名4.0许可</a></p></figure><p id="ca48" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，传统的CNN架构被调整以产生期望的结果。包括变压器在内的一系列架构可用于分割图像。但除了改进网络设计，研究人员还在不断尝试其他方法来提高分段性能。最近，我偶然看到一部<a class="ae lr" href="https://arxiv.org/pdf/1505.04597.pdf" rel="noopener ugc nofollow" target="_blank">作品</a>，它简要描述了在多个输出级别计算损耗(深度监督损耗)的想法。在这个帖子里，我也想分享一下。</p><p id="c76b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将实现一个类似于UNet的模型，这是一个常用的分段架构，并使用Keras模型子类训练它的监督损失。代码可以参考附带的<a class="ae lr" href="https://github.com/shashank14k/Medical-Imaging/blob/main/Deep%20Supervision/unet-supervision-loss.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a> / <a class="ae lr" href="https://www.kaggle.com/code/shashank069/unet-supervision-loss/notebook" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>链接。我假设你熟悉Keras的基础知识。由于我们有很多内容要介绍，我将链接所有资源并跳过一些事情，如<a class="ae lr" href="https://medium.com/ai-salon/understanding-dice-loss-for-crisp-boundary-detection-bb30c2e5f62b" rel="noopener">骰子损失</a>，使用<a class="ae lr" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="noopener ugc nofollow" target="_blank"> model.fit </a>、<a class="ae lr" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator" rel="noopener ugc nofollow" target="_blank">图像生成器</a>等的keras训练。</p><p id="3067" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们先从了解图像分割开始。</p><h1 id="0af4" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">1.图象分割法</h1><p id="35da" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">通俗地说，分割就是像素分类。如果一幅图像有一只猫和一只狗，我们希望机器识别猫和狗的像素，并在输出中将它们标记为1(猫)或2(狗)。每隔一个像素(背景、噪声等)为0。为了训练这样的模型，我们使用成对的图像和遮罩。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mp"><img src="../Images/27b422a1f2d58ed40f76d487e60554f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MwPz1EKz9edBfuiLsN-0bA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated"><a class="ae lr" href="https://cocodataset.org/#explore?id=47780" rel="noopener ugc nofollow" target="_blank">来源</a> ( <a class="ae lr" href="https://cocodataset.org/#termsofuse" rel="noopener ugc nofollow" target="_blank">知识共享署名4.0许可</a>)</p></figure><p id="d632" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设您想在MRI扫描中识别脑瘤。您将首先创建一组阳性(肿瘤)和阴性(非肿瘤)图像的训练集。然后，您将为每个对象创建相应的遮罩。怎么做到的？进行MRI扫描，定位肿瘤区域，将该区域中的所有像素值转换为0，并将所有其他像素设置为1。自然地，非肿瘤口罩将是全黑的。在这些对(输入=MRI，输出=掩模)上训练的模型将在MRI扫描中识别肿瘤。有用，不是吗？</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mq"><img src="../Images/ee9319a69c992fe488013344f90e50ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wzuh24neAAVV-vtpCAS1fg.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated"><a class="ae lr" href="https://www.kaggle.com/code/shashank069/brainmri-image-segmentation-attentionunet" rel="noopener ugc nofollow" target="_blank">来源</a>——来自我建立的一个肿瘤识别模型</p></figure><p id="0aab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们深入研究分割图像所需的神经网络架构。</p><h1 id="f602" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">2.Unet</h1><p id="21aa" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">传统上，CNN善于识别图像中的内容。对于分割，CNN还需要学习精确定位图像成分。UNet有能力做到这一点。最初的<a class="ae lr" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank"> UNet </a>论文将其描述为一个分为两部分的网络——收缩(编码器)和扩张(解码器)。让我们从编码器部分开始(注意，我对<a class="ae lr" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank"> UNet </a>论文中提出的架构做了一些小的修改)。</p><pre class="lc ld le lf gt mr ms mt mu aw mv bi"><span id="a743" class="mw lt iq ms b gy mx my l mz na"># Important Libraries to import<br/>import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import os<br/>import cv2<br/>import tensorflow as tf<br/>from tensorflow.keras import Input<br/>from tensorflow.keras.models import Model, load_model, save_model<br/>from tensorflow.keras.layers import Input, Activation, BatchNormalization, Lambda, Conv2D, Conv2DTranspose,MaxPooling2D, concatenate,UpSampling2D,Dropout<br/>from tensorflow.keras.optimizers import Adam<br/>from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau<br/>from tensorflow.keras import backend as K<br/>from tensorflow.keras.preprocessing.image import ImageDataGenerator<br/>from sklearn.model_selection import KFold<br/>from tensorflow.keras.losses import BinaryCrossentropy<br/>import random</span></pre><h2 id="7368" class="mw lt iq bd lu nb nc dn ly nd ne dp mc ko nf ng me ks nh ni mg kw nj nk mi nl bi translated">2.1编码器</h2><p id="eeda" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">编码器的功能类似于普通的CNN。它不断地分解输入，以辨别与图像中的对象相关联的特征。该过程在多个块中重复进行(<strong class="kh ir">编码器块</strong>)。每个块由以下部分组成:</p><ol class=""><li id="2a6e" class="nm nn iq kh b ki kj kl km ko no ks np kw nq la nr ns nt nu bi translated">具有填充和(3，3)内核的两个卷积层相继出现(我们称之为<strong class="kh ir">卷积块</strong>)。必要时还可以包括批量标准化/删除层[原始文件使用无填充卷积]。我们将使用relu作为激活函数。</li><li id="fa9d" class="nm nn iq kh b ki nv kl nw ko nx ks ny kw nz la nr ns nt nu bi translated">跨距为2的最大池层，用于压缩图像</li></ol><pre class="lc ld le lf gt mr ms mt mu aw mv bi"><span id="cd67" class="mw lt iq ms b gy mx my l mz na"># Functions to build the encoder path<br/>def conv_block(inp, filters, padding='same', activation='relu'):<br/>    <em class="oa">"""<br/>    Convolution block of a UNet encoder<br/>    """<br/>    </em>x = Conv2D(filters, (3, 3), padding=padding, activation=activation)(inp)<br/>    x = Conv2D(filters, (3, 3), padding=padding)(x)<br/>    x = BatchNormalization(axis=3)(x)<br/>    x = Activation(activation)(x)<br/>    return x<br/><br/><br/>def encoder_block(inp, filters, padding='same', pool_stride=2,<br/>                  activation='relu'):<br/>    <em class="oa">"""<br/>    Encoder block of a UNet passes the result from the convolution block<br/>    above to a max pooling layer<br/>    """<br/>    </em>x = conv_block(inp, filters, padding, activation)<br/>    p = MaxPooling2D(pool_size=(2, 2), strides=pool_stride)(x)<br/>    return x, p</span></pre><p id="6de2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您注意到，encoder_block返回两个值-max-pooling前后的图像。UNet将后者(p)传递给下一个块，并将前者(x)存储在内存中。为什么？我们稍后将对此进行阐述。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/6ee20d89e5a08375eb7644684affbfd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*ru8YI-YCZXYOJIhatCWe4g.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图片<a class="ae lr" href="https://github.com/HarisIqbal88/PlotNeuralNet/blob/master/examples/Unet_Ushape/Unet_ushape.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>(麻省理工学院<a class="ae lr" href="https://github.com/HarisIqbal88/PlotNeuralNet/blob/master/LICENSE" rel="noopener ugc nofollow" target="_blank">授权</a>)</p></figure><p id="cbf5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以上是在<a class="ae lr" href="https://arxiv.org/pdf/1505.04597.pdf" rel="noopener ugc nofollow" target="_blank"> UNet </a>论文中描述的第一个编码器模块的副本。它包括两个卷积层，顺序应用64个滤波器，随后是最大池层(由向下的绿色箭头表示)。我们将用上面讨论的修改来复制它。对于像UNet这样不包含密集(平坦)图层的网络，输入形状规范不是强制性的，但我们将输入形状定义为(256，256，1)。</p><pre class="lc ld le lf gt mr ms mt mu aw mv bi"><span id="08b1" class="mw lt iq ms b gy mx my l mz na"># Building the first block<br/>inputs = Input((256,256,1))<br/>d1,p1 = encoder_block(inputs,64)</span></pre><p id="a669" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一个块之后是三个类似的块，具有滤波器= 128，256，512。这四个模块一起构成收缩路径/编码器。</p><pre class="lc ld le lf gt mr ms mt mu aw mv bi"><span id="5123" class="mw lt iq ms b gy mx my l mz na">#Building the other four blocks<br/>d2,p2 = encoder_block(p1,128)<br/>d3,p3 = encoder_block(p2,256)<br/>d4,p4 = encoder_block(p3,512)</span></pre><h2 id="20df" class="mw lt iq bd lu nb nc dn ly nd ne dp mc ko nf ng me ks nh ni mg kw nj nk mi nl bi translated">1.2中部</h2><p id="88cc" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">是的，我们讨论过网络有两个部分，但我喜欢分开处理中间部分。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/c598071bffca79054a6806df635e2180.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*JWNZs9PAUYQDrGTImhcV7Q.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图片<a class="ae lr" href="https://github.com/HarisIqbal88/PlotNeuralNet/blob/master/examples/Unet/Unet.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>(麻省理工<a class="ae lr" href="https://github.com/HarisIqbal88/PlotNeuralNet/blob/master/LICENSE" rel="noopener ugc nofollow" target="_blank">授权</a>)</p></figure><p id="bb4b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它从先前的编码器模块中获取最大汇集输出，并使其通过1024个滤波器的两个连续(3，3)卷积。就像卷积块一样，你问？是的，只是这一次，输出没有通过最大池。因此，我们将使用conv块，而不是编码器块，来创建中间部分。</p><pre class="lc ld le lf gt mr ms mt mu aw mv bi"><span id="bf29" class="mw lt iq ms b gy mx my l mz na"># Middle convolution block (no max pooling)<br/>mid = conv_block(p4,1024) #Midsection</span></pre><p id="35e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个最终输出现在将被<strong class="kh ir">上采样</strong>。</p><h2 id="7989" class="mw lt iq bd lu nb nc dn ly nd ne dp mc ko nf ng me ks nh ni mg kw nj nk mi nl bi translated">1.3解码器</h2><p id="4600" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">UNet已经将图像塑造成许多特征地图，对输入图像中的内容有了一个公平的想法。它知道图像包含的不同类别(对象)。现在，它需要预测所有这些类的正确位置，并在最终输出中相应地标记它们的像素。为此，UNet利用了两个关键理念——跳过连接和上采样。</p><h2 id="93bc" class="mw lt iq bd lu nb nc dn ly nd ne dp mc ko nf ng me ks nh ni mg kw nj nk mi nl bi translated">1.3.1置换卷积</h2><p id="88a3" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">在遍历编码器并通过中间块之后，输入转换为shape (16，16，1024)[您可以使用keras的model.summary( ) api检查这一点]。UNet然后应用转置卷积对输出进行上采样。那么什么是转置卷积呢？看看，如果下图回答了你的问题。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi od"><img src="../Images/a4c5e6b864c4c1da7d081e3cc39b58ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W8LknzSuq2Io8oVnyco7vg.jpeg"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">转置卷积</p></figure><p id="5c0c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本质上，我们将输入中的每个条目乘以内核权重，并将所有(2，2)输出缝合在一起，得到最终输出。在共同指数中，这些数字相加。像卷积核一样，转置卷积核中的权重也是可学习的。网络在反向传播期间学习它们，以精确地对特征图进行上采样。参考这篇<a class="ae lr" rel="noopener" target="_blank" href="/understand-transposed-convolutions-and-build-your-own-transposed-convolution-layer-from-scratch-4f5d97b2967">文章</a>以了解转置卷积的更多信息。</p><h2 id="f91f" class="mw lt iq bd lu nb nc dn ly nd ne dp mc ko nf ng me ks nh ni mg kw nj nk mi nl bi translated">跳过连接</h2><p id="24a7" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">对于每个编码器模块，UNet还有一个共轭解码器模块。我们已经讨论过，解码器模块学习对图像进行上采样。为了增强他们的学习能力，并确保像素在最终输出中的位置正确，解码器会向相应的编码器寻求帮助。他们以<strong class="kh ir">跳过连接</strong>的形式利用这一点。</p><p id="48de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">跳过连接是UNet的本质。如果您以前使用过resnets，您应该对这个概念很熟悉。在1.1中，我们讨论了UNet将卷积模块的输出(x)存储在内存中。这些输出与来自每个解码器模块的上采样图像连接在一起。下图中的水平箭头表示跳过连接。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi oe"><img src="../Images/12f77e0e8105dbdaf106f727b4e7a83a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sY2khmPiPlUaKw83BtpjQA.jpeg"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图片<a class="ae lr" href="https://github.com/HarisIqbal88/PlotNeuralNet/blob/master/examples/Unet_Ushape/Unet_ushape.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>(麻省理工<a class="ae lr" href="https://github.com/HarisIqbal88/PlotNeuralNet/blob/master/LICENSE" rel="noopener ugc nofollow" target="_blank">授权</a>)</p></figure><p id="93c7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">研究人员认为，随着输入图像在网络中滚动得越来越深，更精细的细节，如图像中不同对象/类别的位置，会丢失。跳过连接从初始层转移放错位置的信息，使UNet能够创建更好的分段图。</p><p id="0c2c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，连接的上采样图像流入卷积块(2个连续的卷积层)。因此，我们使用以下函数来创建解码器模块。</p><pre class="lc ld le lf gt mr ms mt mu aw mv bi"><span id="6981" class="mw lt iq ms b gy mx my l mz na"># Functions to build the decoder block<br/>def decoder_block(inp,filters,concat_layer,padding='same'):<br/>    #Upsample the feature maps<br/>    x=Conv2DTranspose(filters,(2,2),strides=(2,2),padding=padding)(inp)<br/>    x=concatenate([x,concat_layer])#Concatenation/Skip conncetion with conjuagte encoder<br/>    x=conv_block(x,filters)#Passed into the convolution block above<br/>    return x</span></pre><h2 id="fd8f" class="mw lt iq bd lu nb nc dn ly nd ne dp mc ko nf ng me ks nh ni mg kw nj nk mi nl bi translated">1.4最终的联合国电子信息网网络</h2><p id="651e" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">下面是我们最后的UNet网络。e5之后的输出具有形状(256，256，64)。为了将其与输入(256，256，1)匹配，我们将使用一个带有1个过滤器的(1，1)卷积层。如果你对1，1卷积感兴趣，一定要看这个吴恩达的视频。</p><pre class="lc ld le lf gt mr ms mt mu aw mv bi"><span id="ab3d" class="mw lt iq ms b gy mx my l mz na"># Bulding the Unet model using the above functions<br/>inputs=Input((256,256,1))<br/>d1,p1=encoder_block(inputs,64)<br/>d2,p2=encoder_block(p1,128)<br/>d3,p3=encoder_block(p2,256)<br/>d4,p4=encoder_block(p3,512)<br/>mid=conv_block(p4,1024) #Midsection<br/>e2=decoder_block(mid,512,d4) #Conjugate of encoder 4<br/>e3=decoder_block(e2,256,d3) #Conjugate of encoder 3<br/>e4=decoder_block(e3,128,d2) #Conjugate of encoder 2 <br/>e5=decoder_block(e4,64,d1) #Conjugate of encoder 1<br/>outputs = Conv2D(1, (1,1),activation=None)(e5) #Final Output<br/>ml=Model(inputs=[inputs],outputs=[outputs,o1],name='Unet')</span></pre><h2 id="25f2" class="mw lt iq bd lu nb nc dn ly nd ne dp mc ko nf ng me ks nh ni mg kw nj nk mi nl bi translated">2.图像分割和深度监控</h2><p id="75ac" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">好了，该实施我们所学的内容了。</p><p id="ef57" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将在这个<a class="ae lr" href="https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database" rel="noopener ugc nofollow" target="_blank">新冠肺炎胸部x光</a> ( <a class="ae lr" href="https://www.kaggle.com/datasets/anasmohammedtahir/covidqu" rel="noopener ugc nofollow" target="_blank">主数据集</a>)数据库上执行图像分割。它包括四个图像类别— Covid、正常、肺部阴影和病毒性肺炎。通过这篇文章，我只想分享如何使用监督损失和Keras模型子类来分割图像。性能在这里不是一个因素。</p><p id="2afa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我将构建一个简单的问题。我们将从Covid类获取图像，并将它们的像素分割成肺和非肺。(注意，我们并不试图训练模型来识别covid受影响的区域，而是绘制肺部所占据的空间)。然而，典型地，医学成像涉及极其复杂的情况，如发现肿瘤影响的器官等。虽然我们不会在这里讨论它们，但是您可以使用/修改所附的代码来实现这些引人注目的应用程序。</p><p id="5cec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们使用以下代码块从目录中检索图像/遮罩路径</p><pre class="lc ld le lf gt mr ms mt mu aw mv bi"><span id="08e1" class="mw lt iq ms b gy mx my l mz na"># Block to read image paths, will be used in image data generator<br/>df = pd.DataFrame(columns=['img_path','msk_path','img_shape','msk_shape','class'])<br/>for cat in ['COVID']:<br/>    dir_ = f"../input/covid19-radiography-database/COVID-19_Radiography_Dataset/{cat}"<br/>    for f in os.listdir(f"{dir_}/images"):<br/>        s1 = cv2.imread(f"{dir_}/images/{f}",config.img_type_num).shape<br/>        s2 = cv2.imread(f"{dir_}/masks/{f}",config.msk_type_num).shape<br/>        dic={'img_path':f"{dir_}/images/{f}",'msk_path':f"{dir_}/masks/{f}",'img_shape':s1,<br/>            'msk_shape':s2}<br/>        df = df.append(dic,ignore_index=True)</span></pre><p id="0437" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下是数据集中的一些<strong class="kh ir"> </strong>图像及其对应的遮罩。如上所述，掩码有两类:</p><p id="6ce9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">0:肺</p><p id="69b2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1:非肺</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi of"><img src="../Images/039a81ea268c4cc2414a5de1c7147533.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FV6zCmy21mhOXbWUoPfoeQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">作者图片</p></figure><p id="59fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 2.1功能缺失和深度监管缺失</strong></p><p id="d270" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">训练掩码只有两个值，0和1。因此，我们可以使用<a class="ae lr" rel="noopener" target="_blank" href="/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a">二元交叉熵</a>来计算它们和我们最终输出之间的损耗。现在，让我们解决房间里的大象——监督缺失。</p><p id="0ffe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">深度神经结构的一个问题是梯度损失。一些架构如此之深，以至于梯度在反向传播到初始层时消失，导致初始层中最小的权重偏移，从而导致学习不足。此外，这使得模型不成比例地依赖于更深层的性能。</p><p id="aeb9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了提高梯度流，这篇论文建议计算不同解码器级别的损耗。你会问，具体是怎样的？首先，如下所示，我们将从网络中提取一个额外的输出‘O1’。我们将从倒数第二个解码器(e4)获得形状为(128，128，128)的结果，并使用(1，1)卷积滤波器将其缩小到(128，128，1)。</p><pre class="lc ld le lf gt mr ms mt mu aw mv bi"><span id="34c8" class="mw lt iq ms b gy mx my l mz na"># Adding output from 2nd last decoder block<br/>inputs=Input((256,256,1))<br/>d1,p1=encoder_block(inputs,64)<br/>d2,p2=encoder_block(p1,128)<br/>d3,p3=encoder_block(p2,256)<br/>d4,p4=encoder_block(p3,512)<br/>mid=conv_block(p4,1024) #Midsection<br/>e2=decoder_block(mid,512,d4) #Conjugate of encoder 4<br/>e3=decoder_block(e2,256,d3) #Conjugate of encoder 3<br/>e4=decoder_block(e3,128,d2) #Conjugate of encoder 2 <br/>o1 = Conv2D(1,(1,1),activation=None)(e4) # Output from 2nd last decoder<br/>e5=decoder_block(e4,64,d1) #Conjugate of encoder 1<br/>outputs = Conv2D(1, (1,1),activation=None)(e5) #Final Output</span></pre><p id="65cf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我们将o1作为模型输出添加到Keras的模型API的输出列表中。</p><pre class="lc ld le lf gt mr ms mt mu aw mv bi"><span id="4ef5" class="mw lt iq ms b gy mx my l mz na"># Adding output to output list in keras model API<br/>ml=Model(inputs=[inputs],outputs=[outputs,o1],name='Unet')</span></pre><p id="9318" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，为了计算这个级别的损失，我们还需要将输入的副本调整为(128，128，1)。现在，最后的损失将是:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi og"><img src="../Images/a78f79f36ffc0717024ef676f9a3f5d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/0*ZRwm_hGhOpgV5BNo"/></div></figure><p id="c61d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们也可以对这两个损失进行加权组合。计算不同层的损耗也使它们能够更好地逼近最终输出。</p><h2 id="0e3d" class="mw lt iq bd lu nb nc dn ly nd ne dp mc ko nf ng me ks nh ni mg kw nj nk mi nl bi translated">2.2使用Keras模型子类进行培训</h2><p id="e617" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">好的，现在剩下的就是训练了。虽然从技术上讲，您可以在著名的“ml.fit”中传递输入列表和调整大小的输入，但我更喜欢使用keras model子类。它允许我们更多地使用损失函数。</p><p id="38e8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们创建一个继承tf.keras.Model的类网络。在初始化类网络的对象时，我们将传递要使用的模型(ml)、损失函数(二进制交叉熵)、度量(<a class="ae lr" href="https://medium.com/ai-salon/understanding-dice-loss-for-crisp-boundary-detection-bb30c2e5f62b" rel="noopener">骰子损失</a>)和损失权重(对两个解码器级别的损失进行加权)。</p><pre class="lc ld le lf gt mr ms mt mu aw mv bi"><span id="7c69" class="mw lt iq ms b gy mx my l mz na"># Defining network class which inherits keras model class<br/>class network(tf.keras.Model):<br/>    <br/>    def __init__(self,model,loss,metric,loss_weights):<br/>        super().__init__()<br/>        self.loss = loss<br/>        self.metric = metric<br/>        self.model = model<br/>        self.loss_weights = loss_weights</span></pre><p id="3a64" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我们将使用函数train_step覆盖“ml.fit”中发生的情况。输入图像通过网络的整个流程和损失计算都是在'<a class="ae lr" href="https://www.tensorflow.org/api_docs/python/tf/GradientTape" rel="noopener ugc nofollow" target="_blank"> tf范围内完成的。GradientTape' </a>,仅用两条线表示梯度。</p><pre class="lc ld le lf gt mr ms mt mu aw mv bi"><span id="73f1" class="mw lt iq ms b gy mx my l mz na"># Overriding model.fit using def train_step<br/>def call(self,inputs,training):<br/>        out = self.model(inputs)<br/>        if training==True:<br/>            return out<br/>        else:<br/>            if type(out) == list:<br/>                return out[0]<br/>            else:<br/>                return out<br/>    <br/>    def calc_supervision_loss(self,y_true,y_preds):<br/>        loss = 0<br/>        for i,pred in enumerate(y_preds):<br/>            y_resized = tf.image.resize(y_true,[*pred.shape[1:3]])<br/>            loss+= self.loss_weights[i+1] * self.loss(y_resized,pred)<br/>            return loss<br/>    <br/>    def train_step(self,data):<br/>        x,y = data<br/>        with tf.GradientTape() as tape:<br/>            y_preds = self(x,training=True)<br/>            if type(y_preds) == list:<br/>                loss = self.loss_weights[0] * self.loss(y,y_preds[0])<br/>                acc = self.metric(y,y_preds[0])<br/>                loss += self.calc_supervision_loss(y,y_preds[1:])<br/>            else:<br/>                loss = self.loss(y,y_preds)<br/>                acc = self.metric(y,y_preds)<br/>        trainable_vars = self.trainable_variables #Network trainable parameters<br/>        gradients = tape.gradient(loss, trainable_vars) #Calculating gradients <br/>        #Applying gradients to optimizer<br/>        self.optimizer.apply_gradients(zip(gradients, trainable_vars)) <br/>        return loss,acc</span></pre><p id="57f5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们操作监督损失时，网络返回列表中的输出，我们调用函数calc_supervision_loss来计算最终损失。</p><p id="6461" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">类似地，我们可以覆盖验证步骤</p><pre class="lc ld le lf gt mr ms mt mu aw mv bi"><span id="1d32" class="mw lt iq ms b gy mx my l mz na"># Overriding validation step<br/>def test_step(self,data):<br/>        x,y=data<br/>        y_pred = self(x,training=False)<br/>        loss = self.loss(y,y_pred)<br/>        acc = self.metric(y,y_pred)<br/>        return loss,acc</span></pre><p id="245b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从现在开始，一切都是惯例。我们将使用<a class="ae lr" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator" rel="noopener ugc nofollow" target="_blank"> Keras ImageDataGenerator </a>来传递用于训练的图像-遮罩对。</p><pre class="lc ld le lf gt mr ms mt mu aw mv bi"><span id="18b2" class="mw lt iq ms b gy mx my l mz na"># Keras Image data generator<br/>def img_dataset(df_inp,path_img,path_mask,batch):<br/>    img_gen=ImageDataGenerator(rescale=1./255.)<br/>    df_img = img_gen.flow_from_dataframe(dataframe=df_inp,<br/>                                     x_col=path_img,<br/>                                     class_mode=None,<br/>                                     batch_size=batch,<br/>                                    color_mode=config.img_mode,<br/>                                         seed=config.seed,<br/>                                     target_size=config.img_size)<br/>    df_mask=img_gen.flow_from_dataframe(dataframe=df_inp,<br/>                                     x_col=path_mask,<br/>                                     class_mode=None,<br/>                                     batch_size=batch,<br/>                                    color_mode=config.msk_mode,<br/>                                        seed=config.seed,<br/>                                     target_size=config.img_size)<br/>    data_gen = zip(df_img,df_mask)<br/>    return data_gen</span></pre><p id="e462" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们创建训练集和验证集，设置优化器，实例化我们上面创建的网络类并编译它。(正如我们从keras继承类网络一样，我们可以使用。直接编译功能)</p><pre class="lc ld le lf gt mr ms mt mu aw mv bi"><span id="3f3a" class="mw lt iq ms b gy mx my l mz na">train=img_dataset(train_ds,'img_path','msk_path',config.batch_size)<br/>        val=img_dataset(val_ds,'img_path','msk_path',config.batch_size)<br/>        opt = Adam(learning_rate=config.lr, epsilon=None, amsgrad=False,beta_1=0.9,beta_2=0.99)<br/>        <br/>        model = network(ml,BinaryCrossentropy(),dice_coef,[1,0.5])<br/>        model.compile(optimizer=opt,loss=BinaryCrossentropy(),metrics=[dice_coef])</span></pre><p id="5696" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">进入训练循环</p><pre class="lc ld le lf gt mr ms mt mu aw mv bi"><span id="211f" class="mw lt iq ms b gy mx my l mz na"># Custom training loop<br/>best_val = np.inf<br/>        for epoch in range(config.train_epochs):<br/>            epoch_train_loss = 0.0<br/>            epoch_train_acc=0.0<br/>            epoch_val_acc=0.0<br/>            epoch_val_loss=0.0<br/>            num_batches = 0<br/>            for x in train:<br/>                if num_batches &gt; (len(train_ds)//config.batch_size):<br/>                    break<br/>                a,b = model.train_step(x)<br/>                epoch_train_loss+=a<br/>                epoch_train_acc+=b<br/>                num_batches+=1<br/>            epoch_train_loss = epoch_train_loss/num_batches<br/>            epoch_train_acc = epoch_train_acc/num_batches<br/>            num_batches_v=0<br/>            for x in val:<br/>                if num_batches_v &gt; (len(val_ds)//config.batch_size):<br/>                    break<br/>                a,b = model.test_step(x)<br/>                epoch_val_loss+=a<br/>                epoch_val_acc+=b<br/>                num_batches_v+=1<br/>            epoch_val_loss=epoch_val_loss/num_batches_v<br/>            if epoch_val_loss &lt; best_val:<br/>                best_val = epoch_val_loss<br/>                print('---Validation Loss improved,saving model---')<br/>                model.model.save('./weights',save_format='tf')<br/>            epoch_val_acc=epoch_val_acc/num_batches_v<br/>            template = ("Epoch: {}, TrainLoss: {}, TainAcc: {}, ValLoss: {}, ValAcc {}")<br/>            print(template.format(epoch,epoch_train_loss,epoch_train_acc,<br/>                                  epoch_val_loss,epoch_val_acc))</span></pre><h2 id="407c" class="mw lt iq bd lu nb nc dn ly nd ne dp mc ko nf ng me ks nh ni mg kw nj nk mi nl bi translated">2.3结果</h2><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi oh"><img src="../Images/e70f1af100dc49f616cd051cd8a5c8d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FAT6nMejnyo2pZ3OB4lVEg.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">预测(图片由作者提供)</p></figure><p id="e872" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">预测的掩模相当准确。该模型的验证骰子得分为0.96，验证损失为0.55。然而，正如我们所讨论的，我们不应该过多地解读这些值，因为手头的问题是粗糙的。目的是展示如何使用监督损失。在上面提到的<a class="ae lr" href="https://arxiv.org/abs/2110.03352v2" rel="noopener ugc nofollow" target="_blank">论文</a>中，作者使用三个解码器的输出来计算最终损耗。</p></div><div class="ab cl oi oj hu ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ij ik il im in"><p id="cd4d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">谢谢你一直读到最后。我希望这能使你在将来使用监督损失。如果你喜欢这份工作，一定要检查我的Github。</p><p id="df9a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">参考资料:</p><p id="40f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lr" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1505.04597</a></p><div class="op oq gp gr or os"><a href="https://arxiv.org/abs/2110.03352v2" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd ir gy z fp ox fr fs oy fu fw ip bi translated">优化的U-Net用于脑肿瘤分割</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">我们提出了一个优化的U-Net架构，用于BraTS21挑战赛中的脑肿瘤分割任务。找到…</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">arxiv.org</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg ll os"/></div></div></a></div><div class="op oq gp gr or os"><a href="https://www.jeremyjordan.me/semantic-segmentation/" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd ir gy z fp ox fr fs oy fu fw ip bi translated">语义图像分割综述。</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">在这篇文章中，我将讨论如何使用卷积神经网络来完成语义图像分割的任务。图像…</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">www.jeremyjordan.me</p></div></div><div class="pb l"><div class="ph l pd pe pf pb pg ll os"/></div></div></a></div><div class="op oq gp gr or os"><a href="https://github.com/shashank14k/Medical-Imaging/tree/main/BrainMRI_Image%20Segmentation" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd ir gy z fp ox fr fs oy fu fw ip bi translated">医学成像/大脑main主沙山14k图像分割/医学成像</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">这个项目的目标是使用卷积神经网络(CNN)的医学图像分割…</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">github.com</p></div></div><div class="pb l"><div class="pi l pd pe pf pb pg ll os"/></div></div></a></div><div class="op oq gp gr or os"><a href="https://ieeexplore.ieee.org/document/9144185" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd ir gy z fp ox fr fs oy fu fw ip bi translated">人工智能能帮助筛查病毒性和新冠肺炎肺炎吗？</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">冠状病毒病(新冠肺炎)是一种疫情疾病，已经造成数千人死亡和感染…</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">ieeexplore.ieee.org</p></div></div><div class="pb l"><div class="pj l pd pe pf pb pg ll os"/></div></div></a></div><p id="7ae7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lr" href="https://linkinghub.elsevier.com/retrieve/pii/S001048252100113X" rel="noopener ugc nofollow" target="_blank">https://linking hub . Elsevier . com/retrieve/pii/s 001048252100113 x</a></p></div></div>    
</body>
</html>