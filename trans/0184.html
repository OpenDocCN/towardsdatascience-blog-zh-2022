<html>
<head>
<title>Uncertainty in Deep Learning — Aleatoric Uncertainty and Maximum Likelihood Estimation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的不确定性——随机不确定性和最大似然估计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/uncertainty-in-deep-learning-aleatoric-uncertainty-and-maximum-likelihood-estimation-c7449ee13712#2022-02-08">https://towardsdatascience.com/uncertainty-in-deep-learning-aleatoric-uncertainty-and-maximum-likelihood-estimation-c7449ee13712#2022-02-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/36092c42757644250b94d45382f3d9a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gZlLB1CKKwn52LAP"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">纳赛尔·塔米米在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="333a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<a class="ae kc" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-brief-introduction-1f9a5de3ae04">之前的文章</a>中，我们讨论了深度学习中的softmax输出和不确定性。现在，我们用张量流概率来扩展这些。</p><p id="b7ed" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我将会有一系列的文章来解释这些术语并展示TFP的实现。</p><ul class=""><li id="b7d6" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated"><a class="ae kc" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-brief-introduction-1f9a5de3ae04">第1部分——简介</a></li><li id="22d8" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><strong class="kf ir">第2部分——随机不确定性和最大似然估计</strong></li><li id="78ed" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><a class="ae kc" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-epistemic-uncertainty-and-bayes-by-backprop-e6353eeadebb">第3部分——认知不确定性和贝叶斯反推</a></li><li id="759d" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><a class="ae kc" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-bayesian-cnn-tensorflow-probability-758d7482bef6">第4部分—实现完全概率贝叶斯CNN </a></li><li id="a93d" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><a class="ae kc" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-experiments-with-bayesian-cnn-1ca37ddb6954">第五部分——贝叶斯CNN实验</a></li><li id="24ce" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">第6部分—贝叶斯推理和转换器</li></ul><h1 id="a5dd" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">简而言之张量流概率..</h1><p id="8ad3" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">TensorFlow Probability (TFP)是一个用于概率编程和统计推断的库。它提供了一个高级API来构建和操作概率分布，并对它们进行后验推断。<strong class="kf ir"> TFP与TensorFlow生态系统</strong>集成，允许您在TensorFlow中构建和训练概率模型，然后在文本分类、图像识别等应用中使用这些模型进行推理。</p><p id="cfbb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在接下来的部分中，我们将使用张量流概率来表示<strong class="kf ir">随机不确定性</strong>:</p><h2 id="f5d1" class="ms lq iq bd lr mt mu dn lv mv mw dp lz ko mx my md ks mz na mh kw nb nc ml nd bi translated">本文组织如下:</h2><ul class=""><li id="2b41" class="lb lc iq kf b kg mn kk mo ko ne ks nf kw ng la lg lh li lj bi translated">什么是任意不确定性？</li><li id="8c90" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><strong class="kf ir">未能捕捉任意不确定性的模型</strong></li><li id="e021" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><strong class="kf ir">张量流概率层简介</strong></li><li id="d5d6" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><strong class="kf ir">最大似然估计和负对数似然</strong></li><li id="2e14" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><strong class="kf ir">捕捉随机不确定性的概率模型</strong></li></ul><p id="c1db" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在开始任何事情之前，让我们看看这些任务的导入:</p><figure class="nh ni nj nk gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><h1 id="3e7c" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">什么是随机不确定性？</h1><p id="d0f6" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">Aleatoric来自alea ，拉丁词“骰子”。随机不确定性是由事件的随机性引入的不确定性。例如，抛硬币的结果是一个任意事件。</p><p id="9bb3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">在深度学习中，</strong>随机不确定性通常是指输入数据的随机性，这可能是由多种因素造成的，包括传感器噪声、像素噪声等。因此，这意味着我们的训练数据在表示输入和目标的真实关系方面并不完美，这反过来会导致我们的模型不太准确。</p><p id="b220" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">还有一件事，增加更多的数据不会减少任意的不确定性。</strong></p><h1 id="0ee2" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated"><strong class="ak">未能捕捉任意不确定性的模型</strong></h1><figure class="nh ni nj nk gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><ol class=""><li id="c14e" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la nn lh li lj bi translated">我们的训练数据只是一个线性关系，可以表示为:<code class="fe no np nq nr b">y=2x+1</code>，然而数据并不完美，存在一些正常的噪声。</li><li id="0224" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la nn lh li lj bi translated">模型有<strong class="kf ir"> 2个线性层，</strong>因为训练数据是线性的。<strong class="kf ir">还要注意，别忘了我们用</strong> <code class="fe no np nq nr b"><strong class="kf ir">mse</strong></code> <strong class="kf ir">作为损失函数</strong>。我们将在MLE部分回顾这一点。</li><li id="d473" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la nn lh li lj bi translated">我们拟合我们的模型，得到了对<code class="fe no np nq nr b">2</code>的预测。</li></ol><p id="e3be" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">运行上面的代码，我们将得到以下结果:</p><figure class="nh ni nj nk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/d0a50d4b4679b0c61af0acad5d4410be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VyDK4iBmU3iURfxBct95wA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片作者。</p></figure><p id="d81f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们在这里讨论一下是哪里出了问题。首先，正如我们在<a class="ae kc" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-brief-introduction-1f9a5de3ae04">上一篇文章</a>中所讨论的，该模型具有点估计权重。除此之外，我们还有一个点估计预测。</p><p id="f158" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">模型也不能捕捉数据的范围。如果我们可以建立一个模型，使用标准差和平均值绘制以下数据的95%区间，会怎么样？</p><h1 id="ca05" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">张量流-概率层简介</h1><figure class="nh ni nj nk gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="35d6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在进入TFP之前，让我们从一个普通的模型开始:</p><figure class="nh ni nj nk gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="c8ef" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们运行上面的代码时，我们将得到以下结果:</p><figure class="nh ni nj nk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/45051c311a18fac7cb90098480670759.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9pF2Jn5xZM1IvjcnofkVUg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片作者。</p></figure><p id="a455" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是意料之中的，因为我们的<code class="fe no np nq nr b">Dense</code>层是由一个<code class="fe no np nq nr b">tanh</code>函数激活的，权重是由1初始化的。</p><p id="54d1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，该模型获得输入并将它们乘以1，然后将这些值传递给<code class="fe no np nq nr b">tanh</code>函数。回想一下我们的模型是:</p><figure class="nh ni nj nk gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="f7ef" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">带线路:</p><pre class="nh ni nj nk gt nu nr nv nw aw nx bi"><span id="2959" class="ms lq iq nr b gy ny nz l oa ob">model_predictions = np.maximum(0, model.predict(space))</span></pre><p id="a29c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">负值被消除，因此生成了上面的图。综上所述，我们创建了一个模型<em class="oc">模型</em>模型<code class="fe no np nq nr b">np.maximum(0, tanh)</code>。</p><h2 id="980d" class="ms lq iq bd lr mt mu dn lv mv mw dp lz ko mx my md ks mz na mh kw nb nc ml nd bi translated">探索前馈对象类型</h2><figure class="nh ni nj nk gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><pre class="nh ni nj nk gt nu nr nv nw aw nx bi"><span id="61c0" class="ms lq iq nr b gy ny nz l oa ob"># Output:<br/>Feedforward returns: tf.Tensor([[0.19737528]], <br/>                     shape=(1, 1), dtype=float32)</span><span id="f5f1" class="ms lq iq nr b gy od nz l oa ob">Allclose!</span></pre><p id="3735" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">重要的一点是前馈(<strong class="kf ir">模型(x) </strong>)返回一个<code class="fe no np nq nr b">tf.Tensor</code>。我们也刚刚确认了模型准确输出<code class="fe no np nq nr b">np.maximum(0, tanh)</code>。</p><h2 id="c2bd" class="ms lq iq bd lr mt mu dn lv mv mw dp lz ko mx my md ks mz na mh kw nb nc ml nd bi translated">预测同一个样本五次</h2><pre class="nh ni nj nk gt nu nr nv nw aw nx bi"><span id="4934" class="ms lq iq nr b gy ny nz l oa ob">sample = tf.convert_to_tensor([0.2])<br/>for _ in range(5):<br/>    print(model(sample).numpy()[0])</span><span id="7146" class="ms lq iq nr b gy od nz l oa ob">--&gt; [0.19737528]<br/>--&gt; [0.19737528]<br/>--&gt; [0.19737528]<br/>--&gt; [0.19737528]<br/>--&gt; [0.19737528]</span></pre><p id="9698" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">他们都是一样的，现在也记住这一点。<strong class="kf ir">这意味着模型是确定性的。</strong></p><p id="8e33" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">添加TFP层很容易，因为TFP与TensorFlow本身集成在一起。</p><h1 id="8e08" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">输出分布的模型</h1><figure class="nh ni nj nk gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><pre class="nh ni nj nk gt nu nr nv nw aw nx bi"><span id="b66f" class="ms lq iq nr b gy ny nz l oa ob">Feedforward returns: tfp.distributions._TensorCoercible("tensor_coercible",<br/>                                   batch_shape= [1, 1],<br/>                                   event_shape=[], dtype=int32)</span></pre><p id="1f2f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们一行一行地解开这个模型。该模型具有如前一示例的密集层，但在此之后它具有<code class="fe no np nq nr b">tfpl.DistributionLambda</code>。这允许模型输出一个分布而不是一个固定值。所以这就是为什么前馈型现在不一样了。</p><p id="3ce7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> TFP </strong>在<code class="fe no np nq nr b">tfp.distributions (tfd)</code>下有许多分布，但为了简单起见，使用伯努利分布。</p><p id="2f76" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简单地说，<strong class="kf ir">伯努利</strong>分布是一种离散概率分布。它用于模拟二元事件的概率，即一个事件有两种可能的结果<strong class="kf ir">。这些</strong>结果可以标记为“成功-1”和“失败-0”。</p><pre class="nh ni nj nk gt nu nr nv nw aw nx bi"><span id="403f" class="ms lq iq nr b gy ny nz l oa ob">tfpl.DistributionLambda(lambda x: tfd.Bernoulli(probs = x),<br/>                    convert_to_tensor_fn = tfd.Distribution.sample)</span></pre><p id="b36a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">来自密集层的输出被传递到伯努利分布中，该分布创建了成功事件的概率。而且我们需要从这个分布中提取数值(在训练时)，这是通过<code class="fe no np nq nr b">convert_to_tensor_fn</code>论证来完成的。使用<code class="fe no np nq nr b">tfd.Distribution.sample</code>，输出将是来自该分布的样本。典型的选择有:</p><pre class="nh ni nj nk gt nu nr nv nw aw nx bi"><span id="dc59" class="ms lq iq nr b gy ny nz l oa ob">1) tfd.Distribution.sample (default value)<br/>2) tfd.Distribution.mean<br/>3) tfd.Distribution.mode</span></pre><figure class="nh ni nj nk gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="ec6c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们从模型中取样时，我们得到了不同的结果。现在我们的模型<strong class="kf ir">不再产生确定性的</strong>输出。</p><figure class="nh ni nj nk gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><figure class="nh ni nj nk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/a1d9cfd13e8d67916e276e6af3899eff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4DFaDDsrB0Ok4Uvbuns-9Q.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="006c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们所讨论的，伯努利样本将是0和1。这张图显示，当<code class="fe no np nq nr b">x</code>值越高，成为<code class="fe no np nq nr b">1</code>的概率也越高。</p><p id="dfa9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在有了所有的片段，我们看到了如何在我们的模型中包括分布对象，<strong class="kf ir">唯一缺少的是在训练时我们应该使用什么作为损失函数？</strong></p><h1 id="515e" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated"><strong class="ak">最大似然估计和负对数似然</strong></h1><p id="c4d9" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">回忆我们的数据:</p><figure class="nh ni nj nk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/ac388d7733ce977a47068984048aedf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EZX2SN5W5qRnMpDpdHG86Q.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片作者。</p></figure><p id="f0e5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe no np nq nr b">x</code>和<code class="fe no np nq nr b">y </code>值之间没有完美的线性关系。换句话说，我们不能通过每个点画一条直线，所以我们得出结论，数据有一些噪音。然而，我们可以通过使用线性回归模型来找到线性关系的良好近似，我们首先已经做到了这一点。回想一下我们的第一个模型:</p><figure class="nh ni nj nk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/d0a50d4b4679b0c61af0acad5d4410be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VyDK4iBmU3iURfxBct95wA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片作者。</p></figure><p id="e39a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这条直线似乎是一个合理的近似值，我们使用<code class="fe no np nq nr b">mse</code>作为损失函数。但是，这个<code class="fe no np nq nr b">mse</code>从何而来？</p><h2 id="a174" class="ms lq iq bd lr mt mu dn lv mv mw dp lz ko mx my md ks mz na mh kw nb nc ml nd bi translated">最大似然估计</h2><p id="f18e" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">神经网络是深度学习的基本工具。它们将数据点映射到参数值，确定看到给定输入的概率，并通过找到最小化训练数据的负对数似然<strong class="kf ir">的权重来帮助训练自己。在<strong class="kf ir">可能性</strong>中，<strong class="kf ir">数据是固定的</strong>，并且是θ或模型权重</strong>的<strong class="kf ir">函数。</strong></p><p id="eecf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这听起来可能令人困惑，所以让我们一步一步来，我们已经给出:</p><figure class="nh ni nj nk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oe"><img src="../Images/d4d3bd9bb1955444334012b3e568d469.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QRrhb9CbB3xvtu-1RsTZMg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片作者。</p></figure><p id="8bbc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在优化过程中，<strong class="kf ir">我们更喜欢最小化</strong>而不是最大化，所以:</p><figure class="nh ni nj nk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi of"><img src="../Images/a52bbc16d25216f7a3f28617fec959b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*7MFfoheWQixrHepLX6vRHg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片作者。</p></figure><p id="3150" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们已经看到了负对数似然性的来源。但是<code class="fe no np nq nr b">mse?</code>怎么样呢？嗯，如果你假设，数据有高斯误差(噪声)和固定方差(误差是独立的),你会以<strong class="kf ir">最小化误差平方和</strong>结束。</p><h2 id="38fb" class="ms lq iq bd lr mt mu dn lv mv mw dp lz ko mx my md ks mz na mh kw nb nc ml nd bi translated">将NLL定义为自定义损耗</h2><figure class="nh ni nj nk gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="c319" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于我们处理的是分布和分布对象(其中有<code class="fe no np nq nr b">.log_prob()</code> ( log-PDF)方法)，我们可以直接最小化<strong class="kf ir"> NLL </strong>作为自定义损失函数。</p><h1 id="702d" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated"><strong class="ak">捕捉随机不确定性的概率模型</strong></h1><figure class="nh ni nj nk gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="3fe2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以输出方差固定的独立正态分布。这里，我们只学习目标分布的均值。</p><p id="d441" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">换句话说，我们拟合了一个方差固定的正态分布！</p><p id="4c3a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还可以用这个模型绘制±2sd，因为输出是一个分布对象。我不会在这里包括效用函数，因为它是一个简单的绘图和从分布中获得标准差。</p><pre class="nh ni nj nk gt nu nr nv nw aw nx bi"><span id="dc75" class="ms lq iq nr b gy ny nz l oa ob">Loss: 1.6406861543655396<br/>Model mean: 1.0017924<br/>Mean of the data: 1.0012743054317013</span></pre><figure class="nh ni nj nk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi og"><img src="../Images/33bf8c6c64f1c8ba5c312bbcd4c15f65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aolj3b2A7lBEXoIflXbkxg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片作者。</p></figure><p id="eb1d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">学习的平均值似乎非常接近原始平均值！但是我们这里处理的是深度学习，所以也可以学习标准差。</p><p id="f813" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种模式需要一些改变:</p><figure class="nh ni nj nk gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="6883" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">模型摘要:</p><pre class="nh ni nj nk gt nu nr nv nw aw nx bi"><span id="4d7b" class="ms lq iq nr b gy ny nz l oa ob">Layer (type)                Output Shape              Param #   <br/>=================================================================<br/> dense_8 (Dense)             (None, 32)                64        <br/>                                                                 <br/> dense_9 (Dense)             (None, 2)                 66        <br/>                                                                 <br/> distribution_lambda_3 (Dist  ((None, 1),              0         <br/> ributionLambda)              (None, 1))                         <br/>                                                                 <br/>=================================================================</span></pre><p id="c4bc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于涉及到性病，我们可能需要更长时间的训练。现在有<strong class="kf ir"> 3 </strong>主要变化:</p><ol class=""><li id="44b7" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la nn lh li lj bi translated">最后一个密是<code class="fe no np nq nr b">Dense(1 + 1)</code>，一个是mean，一个是std。</li><li id="33a2" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la nn lh li lj bi translated">我们将<code class="fe no np nq nr b">tf.nn.softplus</code>应用于<code class="fe no np nq nr b">scale</code>论证，因为<strong class="kf ir">不能为负</strong>。</li><li id="8846" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la nn lh li lj bi translated">我们传递第一个指数作为平均值，传递第二个指数作为标准差。</li></ol><p id="ea19" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该模型将产生以下结果:</p><pre class="nh ni nj nk gt nu nr nv nw aw nx bi"><span id="4f86" class="ms lq iq nr b gy ny nz l oa ob">Loss: 0.68198561668396<br/>Model mean: 0.98936194<br/>Mean of the data: 1.0012743054317013</span></pre><figure class="nh ni nj nk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oh"><img src="../Images/990cf49c385e6ce931689a3672339ba5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EKYTM_t_ZqjgDi4nKNJC9A.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片作者。</p></figure><p id="5675" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，这些绿色条代表95%区间内的数据分布，带有学习平均值和标准差。在进入另一个复杂的例子之前，我想展示一些关于那些<code class="fe no np nq nr b">tfp.layers.</code>的东西</p><p id="7ee0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你看到我们需要<code class="fe no np nq nr b">Dense(1 + 1)</code>用于新的模型。然而，TFP中有一个很好的包装器来处理这些参数。我们可以编写相同的模型:</p><figure class="nh ni nj nk gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="efc1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe no np nq nr b">params_size</code>确保最后一个密集层有正确的单元数。</p><p id="587f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，我们可以使用<code class="fe no np nq nr b">tfpl.IndependentNormal</code>来代替<code class="fe no np nq nr b">DistributionLambda</code>，它是lambda层的包装器。这次通过事件形状就够了。<code class="fe no np nq nr b">event_shape</code>为1，因为数据只有一维。</p><p id="4e02" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，返回的模型摘要是相同的。</p><h2 id="53ae" class="ms lq iq bd lr mt mu dn lv mv mw dp lz ko mx my md ks mz na mh kw nb nc ml nd bi translated">任意不确定性非线性数据</h2><p id="6c46" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">现在，由于数据不再是线性的，所以模型被激活了。</p><figure class="nh ni nj nk gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><figure class="nh ni nj nk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oi"><img src="../Images/73e0ce4c83115826349965c614e06159.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F2bqdcegdSXSqeEvDX_uCg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片作者。</p></figure><p id="3ea2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以写一个模型来捕捉任意的不确定性，想法是一样的:</p><figure class="nh ni nj nk gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="12c9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该模型将给出以下结果:</p><figure class="nh ni nj nk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oj"><img src="../Images/68cc3812e7c74dbb2584e86878b1d4cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*enxE-aqeUmIBhyo_kw8S6Q.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片作者。</p></figure><p id="fa0c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们讨论一下任意不确定性的高低。</p><figure class="nh ni nj nk gt jr gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/f31515a78d521c2d3d966703c072e1d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*CO4tclsNqnAnSQb18FudlA.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片作者。</p></figure><h2 id="b374" class="ms lq iq bd lr mt mu dn lv mv mw dp lz ko mx my md ks mz na mh kw nb nc ml nd bi translated">分类示例</h2><p id="a8a1" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">分类也遵循同样的逻辑。我们可以为<code class="fe no np nq nr b">imdb_reviews</code>数据集写一个模型。</p><p id="9a16" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我将只为它添加代码:</p><figure class="nh ni nj nk gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="bb6d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们已经用<code class="fe no np nq nr b">IndependentBernoulli</code>代替了Lambda层。如果我们做多类分类，我们会使用<code class="fe no np nq nr b">tfpl.OneHotCategorical()</code>。</p><h1 id="eaf1" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">结论</h1><p id="4405" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">我们</p><ul class=""><li id="bfe6" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated">所讨论的随机不确定性不能通过增加新的样本来减少，</li><li id="92ff" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">简要了解了TFP分布对象以及如何将它们集成到我们的Keras模型中，</li><li id="d3b3" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">看过MLE和NLL，</li><li id="a0ec" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">创建了Keras模型来表示任意的不确定性。</li></ul><p id="9836" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所有的代码都可以在<a class="ae kc" href="https://github.com/Frightera/Medium_Notebooks_English/blob/main/Uncertainty%20in%20DL%20-%20Aleatoric%20Uncertainty/Aleatoric%20Uncertainty.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="edd5" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">后续步骤</h1><p id="2700" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">在接下来的文章中，我们将讨论一些使用TFP来表示认知不确定性，也就是模型不确定性的方法。</p><p id="80fb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，我们还将看到如何编写一个完全概率模型来表示噪声项不是正态或高斯的数据中的不确定性。</p></div></div>    
</body>
</html>