<html>
<head>
<title>Implementation and Limitations of Imputation Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">插补方法的实施和局限性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementation-and-limitations-of-imputation-methods-b6576bf31a6c#2022-05-04">https://towardsdatascience.com/implementation-and-limitations-of-imputation-methods-b6576bf31a6c#2022-05-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/950b64bf85ca9b449200a783118f8fe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PovaJ2Ka7PdlJinqjHAwDQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者图片</p></figure><p id="d242" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">不幸的是，对于多少是人为的，多少是适当的，并没有硬性规定。这在很大程度上取决于用户偏好。研究文章常常甚至忽略了对其插补方法的详细解释，甚至忽略了需要插补的数据比例。缺乏对这些方法的解释，尤其是在涉及到所需的插补种类和数量时，应该是一种失礼。对于进行假设、插值等的医疗保健数据来说尤其如此。关于病人身份。</p><p id="d857" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">您必须将丢失的数据(在结构化数据的情况下)视为2D矩阵，在该矩阵中，您可以考虑行和列的连续性。例如，如果某个变量(列)系统地丢失，会给模型带来更多的干扰，而不是帮助，那么您可能要考虑将它从模型中全部删除。或者，如果患者或观察缺少大部分数据，您可能希望对观察执行按行删除。</p><p id="50f0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">许多模型无法处理输入数据中的缺失值。支持向量机、glmnet和神经网络不能容忍任何数量的缺失值。能够容忍缺失值的少数模型是朴素贝叶斯和CART方法下的一些基于树的模型[1]。</p><p id="9d38" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在本文中，我们将讨论最流行/最普遍的插补方法的数学、实现和局限性。</p><h1 id="d8fb" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">KNN</h1><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mb"><img src="../Images/529a6647efd2ef2396cda2914757e9b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X87BpgaCcr4IYZqP8lxH0Q.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">KNN可视化，作者图片</p></figure><p id="3e3f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">k-最近邻(KNN)插补的工作方式非常类似于分类算法。我们基于n维空间中最近的点来近似该值。需要定义KNN算法的超参数，包括:邻居数量和权重。</p><p id="1382" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">选择<strong class="kh iu">数量的邻居</strong> ( <strong class="kh iu"> n_neighbors </strong>)将是噪声和因此的概化和计算复杂性之间的折衷。<strong class="kh iu">小K </strong> =更多噪声/更快，<strong class="kh iu">大K </strong> =稳健我们的结果将会面对噪声/计算复杂。在二进制[0，1]插补的情况下，通常建议选择K的奇数<strong class="kh iu">值作为平局决胜值。</strong></p><p id="f94d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在KNN插补之前，我们需要<strong class="kh iu">标准化</strong>我们的数据。像逻辑回归一样，如果不进行标准化，KNN也受到大值的影响。</p><p id="2740" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">权重</strong>:<strong class="kh iu">统一</strong>表示每个邻域中的所有点将被同等加权。相反，'<strong class="kh iu">距离</strong>'意味着每个点的权重将是邻域内距离的倒数</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mg"><img src="../Images/9717ceb550f7702445e8ce0722c283c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yJCaKD8UMjTdk7sFgHnERg.png"/></div></div></figure><h2 id="7e6d" class="mh le it bd lf mi mj dn lj mk ml dp ln kq mm mn lr ku mo mp lv ky mq mr lz ms bi translated">优势:</h2><ul class=""><li id="2388" class="mt mu it kh b ki mv km mw kq mx ku my ky mz lc na nb nc nd bi translated">易于实施</li><li id="14b3" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">小型数据集和数值数据类型的最佳选择</li></ul><h2 id="8056" class="mh le it bd lf mi mj dn lj mk ml dp ln kq mm mn lr ku mo mp lv ky mq mr lz ms bi translated">局限性:</h2><ul class=""><li id="a03e" class="mt mu it kh b ki mv km mw kq mx ku my ky mz lc na nb nc nd bi translated">随着预测变量和实例的增多，计算会变得更加困难(扩展性不好)</li><li id="223c" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">可以用于分类变量，但在名义分类数据的情况下需要转换为虚拟变量，在顺序数据的情况下需要进行数值转换。</li></ul><pre class="mc md me mf gt nj nk nl nm aw nn bi"><span id="8afa" class="mh le it nk b gy no np l nq nr">from sklearn.impute import KNNImputer</span><span id="a2b2" class="mh le it nk b gy ns np l nq nr"># Instantiate KNN imputer from sklearn<br/>knn_imputer = KNNImputer(n_neighbors=5, weights='uniform')</span><span id="0457" class="mh le it nk b gy ns np l nq nr"># imputing the missing value with knn imputer<br/>array_imputed = knn_imputer.fit_transform(df)</span><span id="30e2" class="mh le it nk b gy ns np l nq nr">#convert to dataframe:<br/>df_imputed = pd.DataFrame(array_imputed, index = DF_INDX, columns=column_names).reset_index()</span></pre><h1 id="130b" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">平均</h1><p id="7eba" class="pw-post-body-paragraph kf kg it kh b ki mv kk kl km mw ko kp kq nt ks kt ku nu kw kx ky nv la lb lc im bi translated">使用平均值的插补计算简单、快速[2]。由于这个特性，它可以很好地适应大型数据集。计算复杂度随着这种插补方法线性增加——O(n)。但是，如果您的数据存在异常值，您可能希望选择'<strong class="kh iu">中值</strong> ' <strong class="kh iu">策略</strong>，而不是使用平均值。:)</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/e2a4578896f7d6e00d7469f446da6dae.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*NeB_shq-cVBFP_0NuCjLcA.png"/></div></figure><h2 id="68b8" class="mh le it bd lf mi mj dn lj mk ml dp ln kq mm mn lr ku mo mp lv ky mq mr lz ms bi translated">优势:</h2><ul class=""><li id="0f88" class="mt mu it kh b ki mv km mw kq mx ku my ky mz lc na nb nc nd bi translated">简单/容易实施</li><li id="605d" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">计算速度快</li><li id="ebed" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">适用于大型数据集</li></ul><p id="843a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">缺点:</strong></p><ul class=""><li id="a9aa" class="mt mu it kh b ki kj km kn kq nx ku ny ky nz lc na nb nc nd bi translated">易受偏态分布/异常值的影响</li><li id="e1f6" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated"><strong class="kh iu">不应</strong>用于名义分类数据</li><li id="5130" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">理论上可以处理有序分类数据(需要数字转换和舍入)</li><li id="a638" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">可能导致结果的偏差，因为它改变了潜在的分布(峰度)</li></ul><pre class="mc md me mf gt nj nk nl nm aw nn bi"><span id="3d3f" class="mh le it nk b gy no np l nq nr">from sklearn.impute import SimpleImputer</span><span id="d735" class="mh le it nk b gy ns np l nq nr"># calling the Simple Imputer 'mean' class<br/>imp = SimpleImputer(missing_values=np.nan, strategy='mean')</span><span id="0b91" class="mh le it nk b gy ns np l nq nr">#imput dataframe (will return an array)<br/>array_imputed = imp.fit_transform(df)</span><span id="8a91" class="mh le it nk b gy ns np l nq nr">#convert from array to dataframe:<br/>df_imputed = pd.DataFrame(array_imputed, index = DF_INDX, columns=column_names).reset_index(‘id’)</span></pre><h1 id="97a3" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">最频繁</h1><p id="7761" class="pw-post-body-paragraph kf kg it kh b ki mv kk kl km mw ko kp kq nt ks kt ku nu kw kx ky nv la lb lc im bi translated">非常类似于上面描述的“均值”选项。我们简单地用参数中的<strong class="kh iu">策略</strong>替换掉“<strong class="kh iu">最频繁</strong>”。</p><h2 id="82fc" class="mh le it bd lf mi mj dn lj mk ml dp ln kq mm mn lr ku mo mp lv ky mq mr lz ms bi translated">优势:</h2><ul class=""><li id="3acf" class="mt mu it kh b ki mv km mw kq mx ku my ky mz lc na nb nc nd bi translated">适用于分类数据</li><li id="fa6a" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">相当于对数值数据类型使用“mode ”!</li><li id="394a" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">计算简单且计算快速。由于这个特性，它可以很好地适应大型数据集</li></ul><h2 id="dca3" class="mh le it bd lf mi mj dn lj mk ml dp ln kq mm mn lr ku mo mp lv ky mq mr lz ms bi translated">缺点:</h2><ul class=""><li id="be9a" class="mt mu it kh b ki mv km mw kq mx ku my ky mz lc na nb nc nd bi translated">可能会导致结果偏差，因为它会像“均值”(峰度)一样改变分布</li><li id="c5da" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">因为如果只有几个实例丢失，则偏置是最好的</li></ul><pre class="mc md me mf gt nj nk nl nm aw nn bi"><span id="3978" class="mh le it nk b gy no np l nq nr">from sklearn.impute import SimpleImputer</span><span id="e7b3" class="mh le it nk b gy ns np l nq nr"># calling the most frequent class<br/>imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')</span><span id="9d47" class="mh le it nk b gy ns np l nq nr">#impute the dataframe<br/>array_imputed = imp.fit_transform(df)</span><span id="4c89" class="mh le it nk b gy ns np l nq nr">#convert from array to dataframe:<br/>df_imputed = pd.DataFrame(array_imputed, index = DF_INDX, columns=column_names).reset_index(‘ID’)<br/></span></pre><h1 id="2111" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">移动平均数</h1><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oa"><img src="../Images/1bf9744f5108ca6590e343c922e61d9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kWuLUhfwNDKmaLk9zfhrbg.png"/></div></div></figure><p id="e027" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">移动平均线需要一个确定的数据窗口。等式如下所示，其中“I”表示总值减去窗口大小加1，“k”是窗口大小，“n”是观察值的总数，“p”是单个观察值。我们可以使用<strong class="kh iu">在Python中实现一个简单的移动平均。</strong>卷()【熊猫法】。</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/b897c4e42f47f183bd87798433660fdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*Hs_PIkhJKCcvDPyuhQ7H6w.png"/></div></figure><h2 id="2973" class="mh le it bd lf mi mj dn lj mk ml dp ln kq mm mn lr ku mo mp lv ky mq mr lz ms bi translated">优势:</h2><ul class=""><li id="ef09" class="mt mu it kh b ki mv km mw kq mx ku my ky mz lc na nb nc nd bi translated">适用于时间序列数据</li><li id="3030" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">也可以使用简单移动平均的其他变体，如加权移动平均</li><li id="c77a" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">保持时间序列的总体趋势</li></ul><h2 id="0836" class="mh le it bd lf mi mj dn lj mk ml dp ln kq mm mn lr ku mo mp lv ky mq mr lz ms bi translated">不足之处</h2><ul class=""><li id="0a54" class="mt mu it kh b ki mv km mw kq mx ku my ky mz lc na nb nc nd bi translated">如果窗口案例中有太多的缺失值会给插补带来问题。例如，如果窗口大小为10，并且一行中有12个缺失值</li><li id="d496" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">如果时间序列有很大的方差，可能会极大地影响计算的平均值</li></ul><pre class="mc md me mf gt nj nk nl nm aw nn bi"><span id="e313" class="mh le it nk b gy no np l nq nr"># window size = 50</span><span id="70e6" class="mh le it nk b gy ns np l nq nr">df['SMA50'] = df['col1'].rolling(50).mean()</span></pre><h1 id="dc8d" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">多重输入链式方程(小鼠)</h1><p id="84fe" class="pw-post-body-paragraph kf kg it kh b ki mv kk kl km mw ko kp kq nt ks kt ku nu kw kx ky nv la lb lc im bi translated">MICE是目前最流行的插补方法之一。MICE也称为序列回归插补、完全条件规范或吉布斯抽样，是由Rubin等人开发的。铝[3]。在MICE算法中，使用一系列(链)回归方程来获得插补。这意味着我们实际上使用简单的插补方法，如平均值，但在数据的不同部分重复该过程几次，并对这些变量进行回归，选择一个最终与我们的分布最相似的方法。</p><p id="a259" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">步骤是:</p><ul class=""><li id="b4d6" class="mt mu it kh b ki kj km kn kq nx ku ny ky nz lc na nb nc nd bi translated">1.计算数据集中每个缺失值的平均值。这些插补的值可以被认为是占位符</li><li id="af99" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">2.根据插补模型中的其他变量对未缺失变量的观察值进行回归</li><li id="8fd0" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">3.然后用回归模型的预测值(插补值)替换变量的缺失值</li><li id="a1e6" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">4.对每个丢失数据的变量重复这个过程。单次迭代被视为每个变量的循环。在每个周期结束时，缺失值已被预测值(来自回归)所取代，反映了观察到的(未受影响的)数据之间的关系。这最终保持了原始分布的形状。这一过程循环重复，每次都更新插补值，然后汇集估计值，得出最终结果</li></ul><h2 id="214d" class="mh le it bd lf mi mj dn lj mk ml dp ln kq mm mn lr ku mo mp lv ky mq mr lz ms bi translated">优势:</h2><ul class=""><li id="df3f" class="mt mu it kh b ki mv km mw kq mx ku my ky mz lc na nb nc nd bi translated">在插补前后保持相对分布相似</li><li id="e93b" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">适用于有序分类数据</li><li id="2069" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">用于名义分类数据需要转换成虚拟变量</li></ul><h2 id="497d" class="mh le it bd lf mi mj dn lj mk ml dp ln kq mm mn lr ku mo mp lv ky mq mr lz ms bi translated">不足之处</h2><ul class=""><li id="f310" class="mt mu it kh b ki mv km mw kq mx ku my ky mz lc na nb nc nd bi translated">用于有序分类数据将需要<strong class="kh iu">。round() </strong>方法，因为结果将是一个浮点</li></ul><pre class="mc md me mf gt nj nk nl nm aw nn bi"><span id="4137" class="mh le it nk b gy no np l nq nr"># import fancyimpute library<br/>from fancyimpute import IterativeImputer</span><span id="887b" class="mh le it nk b gy ns np l nq nr"># calling the  MICE class<br/>mice_imputer = IterativeImputer()</span><span id="d2cc" class="mh le it nk b gy ns np l nq nr"># imputing the missing value with mice imputer<br/>array_imputed = mice_imputer.fit_transform(df)</span><span id="39d7" class="mh le it nk b gy ns np l nq nr">#convert to dataframe:<br/>df_imputed = pd.DataFrame(array_imputed, index = DF_INDX, columns=column_names).reset_index('ID')<br/><br/>imputed_final['col1'] = imputed_final['col1'].round().astype(int)</span></pre><h1 id="2c30" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated"><strong class="ak">数据假发</strong></h1><p id="7417" class="pw-post-body-paragraph kf kg it kh b ki mv kk kl km mw ko kp kq nt ks kt ku nu kw kx ky nv la lb lc im bi translated">Datawig是一种深度学习插补方法，采用长短期记忆(LSTM)网络进行插补。它可以执行分类插补和数字插补。与大多数深度学习方法一样，Datawig经常利用您在数据中感兴趣的任何结果(目标)列。</p><p id="a141" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">它需要一个特定版本的<strong class="kh iu"> mxnet </strong>，目前只有Python 3.7支持，后续版本不支持。它是一种非常强大的插补方法，但是您需要创建一个单独的环境，以便将其作为插补方法使用。描述该方法的论文可以在<a class="ae oc" href="https://jmlr.org/papers/v20/18-753.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到，文档<a class="ae oc" href="https://datawig.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">这里</a>找到。使用<strong class="kh iu">simple inputr</strong>类将自动检测必要的<strong class="kh iu">列编码器</strong> (SequentialEncoder、BowEncoder、CategoricalEncoder、NumericalEncoder)和<strong class="kh iu">特征器</strong>(lstmfeaturezer、BowFeaturizer、EmbedingFeaturizer、NumericalFeaturizer)。</p><h2 id="235e" class="mh le it bd lf mi mj dn lj mk ml dp ln kq mm mn lr ku mo mp lv ky mq mr lz ms bi translated">优势:</h2><ul class=""><li id="c4c0" class="mt mu it kh b ki mv km mw kq mx ku my ky mz lc na nb nc nd bi translated">适用于数字数据</li><li id="b99b" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">适用于分类序数/名义插补</li><li id="e617" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">使用SimpleImputer类自动检测列中的数据类型，并对它们进行正确编码</li></ul><h2 id="9411" class="mh le it bd lf mi mj dn lj mk ml dp ln kq mm mn lr ku mo mp lv ky mq mr lz ms bi translated">缺点:</h2><ul class=""><li id="e3fe" class="mt mu it kh b ki mv km mw kq mx ku my ky mz lc na nb nc nd bi translated">需要大量数据才能准确</li><li id="ab3f" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">可能需要目标(结果)变量</li><li id="90bf" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">需要将插补数据分为训练和测试(这可能很困难，取决于您缺失数据的数量和结构)</li><li id="1a88" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">在执行有序分类数据时，仍需要执行舍入</li></ul><pre class="mc md me mf gt nj nk nl nm aw nn bi"><span id="b455" class="mh le it nk b gy no np l nq nr"># importing datawig library<br/>import datawig</span><span id="d421" class="mh le it nk b gy ns np l nq nr"># calling the datawig class<br/>imputer = datawig.simple_imputer.SimpleImputer(<br/>        input_columns=['col1','col2','col3','col4'], output_column='col5', output_path = 'imputer_model2')<br/>    <br/>#split data without any nans and use as trainging data</span><span id="5159" class="mh le it nk b gy ns np l nq nr">imputer.fit(train_df = df_nona_train)</span><span id="486e" class="mh le it nk b gy ns np l nq nr">imputed = imputer.predict(df_na_test)    <br/><br/>df_imputed_copy['col5'].round().astype(int)</span></pre><h1 id="000e" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">时间序列的双向递归插补</h1><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi od"><img src="../Images/d7601ba9e76a3872d9999e669c2f285f.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*xAaPQcMlJSM63WoOkGWs-w.png"/></div></figure><p id="dc86" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">顾名思义，时间序列双向递归插补(BRITS)是针对时间序列数据的数值插补。特别是多个相关时间序列。它采用双向递归神经网络(RNN)进行插补。它的代码/GitHub repo可以在<a class="ae oc" href="https://github.com/NIPS-BRITS/BRITS" rel="noopener ugc nofollow" target="_blank">这里</a>找到。还有这里的学术论文<a class="ae oc" href="http://papers.nips.cc/paper/7911-brits-bidirectional-recurrent-imputation-for-time-series" rel="noopener ugc nofollow" target="_blank"/>【4】。该算法有两个后续部分，一个递归部分(RNN)和一个回归部分(全连接神经网络)。</p><h2 id="ee6b" class="mh le it bd lf mi mj dn lj mk ml dp ln kq mm mn lr ku mo mp lv ky mq mr lz ms bi translated">优势:</h2><ul class=""><li id="ea09" class="mt mu it kh b ki mv km mw kq mx ku my ky mz lc na nb nc nd bi translated">适用于数字时间序列数据</li><li id="25e2" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">不是为分类插补设计的</li></ul><h2 id="9d85" class="mh le it bd lf mi mj dn lj mk ml dp ln kq mm mn lr ku mo mp lv ky mq mr lz ms bi translated">缺点:</h2><ul class=""><li id="f4ba" class="mt mu it kh b ki mv km mw kq mx ku my ky mz lc na nb nc nd bi translated">仅适用于时间序列数据</li><li id="5d1d" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">不适用于分类(序数或名义)时间序列数据</li></ul><h1 id="7153" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">插补方法汇总表</h1><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oe"><img src="../Images/a57f87908c444b41d53c5c0d45686c6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u1b9F6aF08DvwKpA1DS-3w.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">汇总表，按作者分类的图像</p></figure><h1 id="b566" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">评估插补的标准</h1><p id="3129" class="pw-post-body-paragraph kf kg it kh b ki mv kk kl km mw ko kp kq nt ks kt ku nu kw kx ky nv la lb lc im bi translated">如果你有原始数据(稀有)，但如果你正在开发一种新的插补方法，那么你会想要完整的数据集，并以MCAR和马尔的方式在数据中制造“缺失”。那么，我们希望用什么指标来衡量我们的估算是否充分呢？</p><p id="e4f4" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在连续(区间数据)的情况下，我们通常使用均方根误差(RMSE)来评估拟合优度，以确定估算值与原始值的距离。</p><p id="e44c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在分类数据的情况下，我们通常使用F1来评估拟合优度，以确定估算值与原始值的距离。</p><ul class=""><li id="3e94" class="mt mu it kh b ki kj km kn kq nx ku ny ky nz lc na nb nc nd bi translated"><strong class="kh iu">连续数据= RMSE </strong></li><li id="d090" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated"><strong class="kh iu">分类数据= F1 </strong></li></ul><p id="37be" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">下面是在scikit learn [5]中执行RMSE错误的代码片段:</p><pre class="mc md me mf gt nj nk nl nm aw nn bi"><span id="6afb" class="mh le it nk b gy no np l nq nr"># import necessary library for MSE<br/>from sklearn.metrics import mean_squared_error</span><span id="a60c" class="mh le it nk b gy ns np l nq nr">MSE = mean_squared_error(df_orginal['col1'], df_imputed['col1'])</span><span id="ad8a" class="mh le it nk b gy ns np l nq nr">RMSE = math.sqrt(MSE)</span></pre><p id="e430" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">和F1错误:</p><pre class="mc md me mf gt nj nk nl nm aw nn bi"><span id="ebd1" class="mh le it nk b gy no np l nq nr"># import necessary library for F1 score<br/>from sklearn.metrics import f1_score</span><span id="0112" class="mh le it nk b gy ns np l nq nr">F1 = f1_score(df_original['col1'], df_imputed['col1], average='micro')</span></pre><p id="fd01" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">请注意，F1<strong class="kh iu">{ '微观'，'宏观'，'加权'，'二进制' }[T5[6]]有不同的指标:</strong></p><ul class=""><li id="feac" class="mt mu it kh b ki kj km kn kq nx ku ny ky nz lc na nb nc nd bi translated">微观指标是针对真阳性、真阴性、假阳性和假阴性进行系统计数的(跨整个列)</li><li id="abe3" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">宏-计算每个标签的指标。注意标签的不平衡是很重要的，因为它们的存在没有被考虑在内</li><li id="d097" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">加权-与宏非常相似，只是现在考虑了标签不平衡</li><li id="5473" class="mt mu it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated">二元-仅报告单一类别的结果，此选项仅适用于二元变量的插补。</li></ul><p id="87d3" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如果你喜欢读这篇文章，并且想支持像我一样的作家和其他人，考虑注册Medium。这是5美元一个月，如果你使用我的链接，我赚一小笔佣金，这反过来有助于燃料更多的内容！:)<a class="ae oc" href="https://medium.com/@askline1/membership" rel="noopener">https://medium.com/@askline1/membership</a></p><h1 id="aada" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">参考</h1><p id="ec60" class="pw-post-body-paragraph kf kg it kh b ki mv kk kl km mw ko kp kq nt ks kt ku nu kw kx ky nv la lb lc im bi translated">[1]布莱曼、弗里德曼、奥尔申和斯通。1984.<em class="of">分类和回归树</em>。纽约:查普曼；halls<br/>【2】sci kit Learn，<a class="ae oc" href="https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-Learn . org/stable/modules/generated/sk Learn . impute . simple imputr . html</a>，访问时间:2022年5月3日<br/>【3】Rubin DB(1987)。调查中无应答的多重插补。纽约约翰·威利父子公司。<br/> [4]曹等。al，<em class="of"> BRITS:时间序列的双向递归插补</em>，第32届神经信息处理系统会议(NeurIPS 2018)，蒙特利尔，加拿大。2018<br/>【5】Scikit Learn，<a class="ae oc" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html" rel="noopener ugc nofollow" target="_blank">https://Scikit-Learn . org/stable/modules/generated/sk Learn . metrics . mean _ squared _ error . html</a>，访问时间:2022年5月3日<br/>【6】Scikit Learn，<a class="ae oc" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html" rel="noopener ugc nofollow" target="_blank">https://Scikit-Learn . org/stable/modules/generated/sk Learn . metrics . f1 _ score . html</a>，访问时间:2022年5月3日</p></div></div>    
</body>
</html>