<html>
<head>
<title>Transformers for Tabular Data (Part 2): Linear Numerical Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">表格数据的转换(第二部分):线性数字嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/improving-tabtransformer-part-1-linear-numerical-embeddings-dbc3be3b5bb5#2022-10-22">https://towardsdatascience.com/improving-tabtransformer-part-1-linear-numerical-embeddings-dbc3be3b5bb5#2022-10-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cbe2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">基于FT-Transformer的表格数据深度学习</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/470cec345b6e99b6329aab439e69715a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Cv_chyVnBSV392HM"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">尼克·希利尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="3d7f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="3362" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/transformers-for-tabular-data-tabtransformer-deep-dive-5fb2438da820">在上一篇关于TabTransformer </a>的文章中，我描述了这个模型是如何工作的，以及如何应用到你的数据中。这篇文章将建立在它的基础上，所以如果你还没有阅读它，我强烈建议你从那里开始，然后再回到这篇文章。</p><p id="5ced" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在一些数据集上，TabTransformer的性能优于传统的多层感知器(MLPs ),接近梯度提升树(GBTs)的性能。然而，该架构有一个明显的缺点——在构建上下文嵌入时，它没有考虑数字特征。这篇文章深入探讨了Gorishniy等人(2021)的<a class="ae ky" href="https://arxiv.org/pdf/2106.11959v2.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>，该论文通过引入<strong class="lt iu"> FT-Transformer(特征标记器+ Transformer) </strong>解决了这个问题。</p><h1 id="b07d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">TabTransformer与FT-Transformer</h1><p id="0dcf" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">两个模型都使用变压器(<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> Vaswani等人，2017 </a>)作为其模型主干，但有两个主要区别:</p><ul class=""><li id="da7b" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated">数字嵌入的使用</li><li id="9de5" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">使用CLS令牌进行输出</li></ul><h2 id="f3ae" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">数字嵌入</h2><p id="0007" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">传统的TabTransformer接受分类嵌入，并通过Transformer块将它们转换成上下文相关的嵌入。然后，数字特征与这些上下文嵌入连接，并通过MLP得到预测。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/8fc3531f0a4b3fff0984af6d79a414e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qUiaXv7d3mEU2mWI.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TabTransformer图。图片作者。</p></figure><p id="f79d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">大部分的魔法都发生在变形金刚内部，所以很遗憾数字特征被忽略了，只在模型的最后一层使用。Gorishniy等人(2021)提出通过嵌入数字特征来解决这个问题。</p><p id="8350" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">FT-Transformer使用的嵌入是线性的，这意味着每个特征在通过一个简单的完全连接的层后被转换为密集矢量。需要注意的是，这些密集图层不共享权重，因此每个数字要素有一个单独的嵌入图层。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/5f8f55879872e429cbd96c1ef16af9ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aZ3VFJWwTyvaMoK_uHrPqA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线性数字嵌入。图片作者。</p></figure><p id="0cf5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">您可能会问自己—如果这些特征已经是数字了，为什么还要这样做？主要原因是数字嵌入可以与分类嵌入一起通过转换器块。这增加了更多可以学习的上下文，从而提高了表示质量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/119bc400e0bb41f6d90f98d25037364a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uoT0DkYXW85CRCJlh0M3gw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数字嵌入变压器。图片作者。</p></figure><p id="e359" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">有趣的是，已经证明(例如<a class="ae ky" href="https://arxiv.org/abs/2203.05556" rel="noopener ugc nofollow" target="_blank">这里</a>)这些数字嵌入的添加可以提高各种深度学习模型的性能(不仅仅是TabTransformer)，因此它们甚至可以应用于简单的MLP。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/442d615f6752e6dc8aa95a175ed8d89e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mto-CSy3s4sUyw1P8VJhQQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数字嵌入的MLP。图片作者。</p></figure><h2 id="9856" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">CLS代币</h2><p id="79fb" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">CLS令牌的用法来自NLP领域，但它可以很好地转换成表格任务。基本思想是，在我们嵌入了我们的特征之后，我们向它们附加另一个“嵌入”,它代表一个CLS令牌。通过这种方式，分类、数字和CLS嵌入通过转换程序块得到语境化。之后，上下文化的CLS令牌嵌入充当简单MLP分类器的输入，该分类器产生期望的输出。</p><h2 id="c4e6" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">傅立叶变换变压器</h2><p id="d485" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">通过用数字嵌入和CLS令牌扩充TabTransformer，我们得到了最终提出的架构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/d4d107cb2d373425002bc0184e734909.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cRWwJ9NgmMnLJU3ncihuMA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">FT-变压器。图片作者。</p></figure><h1 id="50ab" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">报告的结果</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/4c163ab9c04ca7d5273a80c7436f1465.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CUQv-EmfXj5maAv1EgqyqQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">FT-Transformer的报告结果。来源:<a class="ae ky" href="https://arxiv.org/pdf/2106.11959v2.pdf" rel="noopener ugc nofollow" target="_blank">戈里什尼等人(2021年)</a></p></figure><p id="a594" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从结果中我们可以看出，FT-Transformer在各种数据集上都优于梯度提升模型。此外，它优于ResNet，ResNet是一个强大的表格数据深度学习基线。有趣的是，超参数调整并没有改变FT-Transformer结果太多，这可能表明它对超参数不是那么敏感。</p><h1 id="a9c9" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">验证结果</h1><p id="c5b2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">本节将向您展示如何通过验证<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/adult" rel="noopener ugc nofollow" target="_blank">成人收入数据集</a>的结果来使用FT-Transformer。我将使用一个名为<code class="fe ny nz oa ob b">tabtransformertf</code>的包，它可以使用<code class="fe ny nz oa ob b">pip install tabtransformertf</code>来安装。它允许我们使用表格变压器模型，而无需大量的预处理。下面您可以看到分析的主要步骤和结果，但请务必查看<a class="ae ky" href="https://github.com/aruberts/TabTransformerTF/blob/main/notebooks/fttransformer-demo.ipynb" rel="noopener ugc nofollow" target="_blank">补充笔记本</a>了解更多详情。</p><h2 id="98f0" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated"><strong class="ak">数据预处理</strong></h2><p id="19ea" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">数据可以从<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/adult" rel="noopener ugc nofollow" target="_blank">这里</a>下载或者使用一些API。数据预处理步骤与本文无关，因此您可以在<a class="ae ky" href="https://github.com/aruberts/TabTransformerTF/blob/main/notebooks/fttransformer-demo.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到完整的工作示例。特定于FT-Transformer的预处理类似于<a class="ae ky" rel="noopener" target="_blank" href="/transformers-for-tabular-data-tabtransformer-deep-dive-5fb2438da820"> TabTransformer </a>，因为我们需要创建分类预处理层并将数据转换成TF数据集。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><h2 id="069c" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">FT-变压器初始化</h2><p id="8920" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">模型的初始化相对简单，每个参数都有注释。三个FT-变压器特定参数是— <code class="fe ny nz oa ob b">numerical_embeddings</code>、<code class="fe ny nz oa ob b">numerical_embedding_type</code>和<code class="fe ny nz oa ob b">explainable</code></p><ul class=""><li id="e21f" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated"><code class="fe ny nz oa ob b">numerical_embeddings</code> —类似于<code class="fe ny nz oa ob b">category_lookup</code>，这些是预处理层。对于FT-Transformer来说是<code class="fe ny nz oa ob b">None</code>，因为我们没有对数字特征进行预处理。</li><li id="ff83" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated"><code class="fe ny nz oa ob b">numerical_embedding_type</code> — <code class="fe ny nz oa ob b">linear</code>用于线性嵌入。更多类型将在下一篇文章中介绍。</li><li id="00a1" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated"><code class="fe ny nz oa ob b">explainable</code>-如果设置为<code class="fe ny nz oa ob b">True</code>，模型将输出每行的特征重要性。它们是从注意力权重中推断出来的。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><h2 id="657e" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated"><strong class="ak">模特培训</strong></h2><p id="6844" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">培训程序类似于任何Keras模型。唯一需要注意的是，如果您已经将<code class="fe ny nz oa ob b">explainable</code>指定为<code class="fe ny nz oa ob b">True</code>，那么您需要两个损失和指标，而不是一个。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="9279" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">训练需要大约70个时期，下面你可以看到损失和度量值的进展。您可以减少提前停止的回合数，或者进一步简化模型(例如，减少注意力头数)，以加快训练速度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/b4b17b13ad32f6d8c994d2584e765939.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ri85qpwhAolpCki5wCVTGw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">培训/验证损失和指标。作者的情节。</p></figure><h2 id="8216" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">估价</h2><p id="652e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">测试数据集使用ROC AUC和PR AUC进行评估，因为这是一个不平衡的二元分类问题。为了验证报告的结果，我还包括了假设阈值为0.5的准确性指标。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="eae8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">得到的准确度分数为0.8576，略低于报告的分数0.86。这种差异可能是由于训练过程中的随机变化或不同的超参数造成的。尽管如此,<strong class="lt iu">结果与报道的足够接近,</strong>,所以这是一个好迹象，表明这项研究是可重复的。</p><h2 id="7a8b" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">可解释性</h2><p id="9177" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">FT-Transformer最大的优点之一是内置的可解释性。由于所有的特征都通过一个转换器，我们可以得到它们的注意力图，并推断出特征的重要性。这些重要性是使用以下公式计算的</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/cfe18e9b0856889efae56644b41a3a61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*52d2BCS55BWd_h5c4sOFBw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">特征重要性公式。来源:<a class="ae ky" href="https://arxiv.org/pdf/2106.11959v2.pdf" rel="noopener ugc nofollow" target="_blank">戈里什尼等人(2021年)</a></p></figure><p id="f158" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">其中<em class="og"> p_ihl </em>是从第<em class="og">I</em>个样本的第<em class="og"> l </em>层向前传递的【CLS】令牌的第<em class="og"> h </em>个头部注意图。该公式基本上总结了跨越不同注意头(<code class="fe ny nz oa ob b">heads</code>参数)和变压器层(<code class="fe ny nz oa ob b">depth</code>参数)的【CLS】令牌的所有注意分数，然后将它们除以<code class="fe ny nz oa ob b">heads x depth</code>。局部重要性(<em class="og"> p_i </em>)可以跨所有行进行平均，以获得全局重要性(<em class="og"> p </em>)。</p><p id="1e6a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，让我们看看成人收入数据集的重要性。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="9245" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从上面的代码中，您可以看到模型已经输出了我们需要的大部分信息。对其进行处理和绘图会得到以下结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/c1443ac2ca40ca2c1b737037795478f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HftAl-trdMG0VYBIp-Fe2w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">特征重要性。由作者策划。</p></figure><p id="6900" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">前五个特征确实有道理，因为收入较高的人往往年龄较大，已婚，受教育程度较高。我们也可以通过查看最大预测和最小预测的重要性来检测局部重要性。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><div class="kj kk kl km gt ab cb"><figure class="oi kn oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/861048b5a66722a8b9c4362960094eb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*ntPXcXSvcvzCJ-84bNR-jQ.png"/></div></figure><figure class="oi kn oo ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/e6d30ec9867563e6b787f5b9159104d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*-AUSSyo2KzOGNPTATb2yHA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk op di oq or translated">前3名贡献。由作者创建。</p></figure></div><p id="abd9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">同样，重要性有直观的意义。最大概率赚50K以上的人，资本收益大，受教育15年，年龄大。可能性最低的人只有18岁，完成了10年的教育，每周工作15个小时。</p><h1 id="db41" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">结论</strong></h1><p id="f6a4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这篇文章中，你看到了FT-Transformer是什么，它与TabTransformer有何不同，以及如何使用<code class="fe ny nz oa ob b">tabtransformertf</code>包来训练它。</p><p id="d7ff" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">总的来说，FT-Transformer是深度表格学习领域的一个有前途的补充。与TabTransformer相比，该模型不仅嵌入了分类特征，还嵌入了数字特征，因此能够显著提高其性能，并进一步缩小深度模型与XGBoost等梯度增强模型之间的差距。此外，该模型是可解释的，这有利于许多领域。</p><p id="7ec3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我的下一篇文章将讨论不同的数字嵌入类型(不仅仅是线性的),这将进一步提高性能。敬请期待！</p><h1 id="7142" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">文献学</h1><ul class=""><li id="9fd3" class="ms mt it lt b lu lv lx ly ma os me ot mi ou mm mx my mz na bi translated">成人收入数据集(知识共享署名4.0国际许可(CC BY 4.0)) — Dua，d .和Graff，C. (2019)。UCI机器学习知识库[http://archive . ics . UCI . edu/ml]。加州欧文:加州大学信息与计算机科学学院。</li><li id="3114" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">Yury Gorishniy等人，2021年，<a class="ae ky" href="https://arxiv.org/abs/2106.11959" rel="noopener ugc nofollow" target="_blank">重新审视表格数据的深度学习模型</a></li><li id="161e" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">瓦斯瓦尼等人，2017年，【https://arxiv.org/abs/2106.11959 T2】</li></ul></div></div>    
</body>
</html>