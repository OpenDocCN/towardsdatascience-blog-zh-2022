<html>
<head>
<title>A New Era of Massively Parallel Simulation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大规模并行模拟的新时代</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-new-era-of-massively-parallel-simulation-a-practical-tutorial-using-elegantrl-5ebc483c3385#2022-11-22">https://towardsdatascience.com/a-new-era-of-massively-parallel-simulation-a-practical-tutorial-using-elegantrl-5ebc483c3385#2022-11-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1bef" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用 ElegantRL 的实用教程</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6b79050e75780603c6b74d81c5123e35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GbkU1KrUvx3_gq35"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@fanfandyuen?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Jason Yuen </a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><blockquote class="kw kx ky"><p id="8b51" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="iq">最近强化学习的一个突破是，GPU 加速的模拟器，如 NVIDIA 的</em> Isaac Gym <em class="iq">实现了</em> <strong class="lc ir"> <em class="iq">大规模并行模拟</em> </strong> <em class="iq">。它在一个工作站 GPU 上运行数千个并行环境，并将数据收集过程加快了 2~3 个数量级。</em></p></blockquote><p id="ddc1" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">Steven Li 和的这篇文章解释了大规模并行模拟的最新突破。它还通过使用云原生开源强化学习(RL)库<em class="lb"> ElegantRL </em>的实用教程，讲述了<strong class="lc ir">如何训练机器人在 10 分钟内解决 Isaac Gym 基准任务</strong>和<strong class="lc ir">如何从头开始构建自己的并行模拟器</strong>。</p><div class="lz ma gp gr mb mc"><a href="https://github.com/AI4Finance-Foundation/ElegantRL" rel="noopener  ugc nofollow" target="_blank"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd ir gy z fp mh fr fs mi fu fw ip bi translated">GitHub-ai 4 finance-Foundation/ElegantRL:Cloud-native 深度强化学习。🔥</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">ElegantRL(网站)是为从业者开发的，具有以下优势:云原生:遵循云原生…</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">github.com</p></div></div><div class="ml l"><div class="mm l mn mo mp ml mq kp mc"/></div></div></a></div></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="4131" class="my mz iq bd na nb nc nd ne nf ng nh ni jw nj jx nk jz nl ka nm kc nn kd no np bi translated">什么是 GPU 加速模拟？</h1><p id="e6f6" class="pw-post-body-paragraph kz la iq lc b ld nq jr lf lg nr ju li lw ns ll lm lx nt lp lq ly nu lt lu lv ij bi translated">与大多数数据驱动的方法类似，强化学习(RL)是数据饥渴的——一个相对简单的任务可能需要数百万次转换，而学习复杂的行为可能需要更多。</p><p id="5de8" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">加速数据收集过程的一种自然而直接的方法是拥有多个环境，并让代理与它们并行交互。在 GPU 加速模拟器之前，使用基于 CPU 的模拟器(如 MuJoCo 和 PyBullet)的人通常需要一个 CPU 集群来实现这一点。例如，OpenAI 使用了几乎 30，000 个 CPU 核心(920 个工作机，每个 32 个核心)来训练一个机器人解魔方[1]。如此巨大的计算需求对于大多数研究人员和从业者来说是不可接受的！</p><p id="c50b" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">幸运的是，多核 GPU 天然适合高度并行仿真，最近的一个突破是 NVIDIA 发布的<em class="lb">is AAC Gym</em>【2】，这是一个端到端 GPU 加速的机器人仿真平台。在 GPU 上运行模拟有几个优点:</p><ol class=""><li id="835a" class="nv nw iq lc b ld le lg lh lw nx lx ny ly nz lv oa ob oc od bi translated">允许使用单个 GPU 同时运行<strong class="lc ir">数万个</strong>环境，</li><li id="cefc" class="nv nw iq lc b ld oe lg of lw og lx oh ly oi lv oa ob oc od bi translated">加速每个环境前进一步，包括物理模拟，状态和奖励计算等。,</li><li id="340f" class="nv nw iq lc b ld oe lg of lw og lx oh ly oi lv oa ob oc od bi translated">避免在 CPU 和 GPU 之间来回传输数据，因为神经网络推理和训练位于 GPU 上。</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/b0f47f18378aa3b3b2f2555d15fc1d24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QtXog_7hNXLUItlGEcGuBw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图 1:传统 RL 训练流水线和基于 GPU 的端到端训练流水线之间的比较。[图片来自作者]</p></figure><h1 id="8037" class="my mz iq bd na nb ok nd ne nf ol nh ni jw om jx nk jz on ka nm kc oo kd no np bi translated">艾萨克体育馆机器人基准环境</h1><p id="498e" class="pw-post-body-paragraph kz la iq lc b ld nq jr lf lg nr ju li lw ns ll lm lx nt lp lq ly nu lt lu lv ij bi translated">Isaac Gym 提供了从运动到操作的一系列不同的机器人基准任务。为了使用 RL 成功训练机器人，我们展示了如何使用大规模并行库<em class="lb"> ElegantRL </em>。</p><p id="dbf4" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">现在，ElegantRL 完全支持 Isaac 健身房环境。在下面的六个机器人任务中，我们展示了在 ElegantRL 中实现的三个常用深度 RL 算法 PPO [3]、DDPG [4]和 SAC [5]的性能。请注意，我们在任务中使用不同数量的并行环境，从 4，096 到 16，384 个环境不等。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/ca7afaabe5b77c762cf88ba687a75581.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gwAQuaMHFDeDYLcK2ru1JA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图 2:三个艾萨克健身房任务:人形，弗兰卡立方体堆叠，快板手(从左到右)。[图片来自作者]</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/1073755fb72832cd10cb63286b6656f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6EibnCmbfnWnik2Qr-NunQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图 3:六项艾萨克健身任务的表现。[图片来自作者]</p></figure><p id="6ee5" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">对比之前的魔方例子需要一个 CPU 集群，需要几个月的训练，我们可以在 30 分钟内解决一个类似的影手重定位任务！</p><h1 id="07f1" class="my mz iq bd na nb ok nd ne nf ol nh ni jw om jx nk jz on ka nm kc oo kd no np bi translated">从头开始构建自己的模拟器</h1><p id="b0d5" class="pw-post-body-paragraph kz la iq lc b ld nq jr lf lg nr ju li lw ns ll lm lx nt lp lq ly nu lt lu lv ij bi translated">有没有可能像艾萨克健身房一样，自己搭建一个基于 GPU 的模拟器？答案是肯定的！在本教程中，我们提供两个组合优化问题的例子:<a class="ae kv" href="https://en.wikipedia.org/wiki/Maximum_cut" rel="noopener ugc nofollow" target="_blank">图最大割</a>和<a class="ae kv" href="https://en.wikipedia.org/wiki/Travelling_salesman_problem" rel="noopener ugc nofollow" target="_blank">旅行商问题</a> (TSP)。</p><p id="bb13" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">传统的 RL 环境主要由三个功能组成:</p><ul class=""><li id="f348" class="nv nw iq lc b ld le lg lh lw nx lx ny ly nz lv or ob oc od bi translated">init():定义环境的关键变量，比如状态空间和动作空间。</li><li id="89e8" class="nv nw iq lc b ld oe lg of lw og lx oh ly oi lv or ob oc od bi translated">step():接受一个动作作为输入，运行环境动力学的一个时间步长，并返回下一个状态、奖励和完成信号。</li><li id="df63" class="nv nw iq lc b ld oe lg of lw og lx oh ly oi lv or ob oc od bi translated">reset():重置环境并返回初始状态。</li></ul><p id="8bcd" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">大规模并行环境具有类似的功能，但是接收和返回一批状态、动作和奖励。考虑最大割问题:给定一个图<em class="lb"> G </em> = ( <em class="lb"> V </em>，<em class="lb"> E </em>)，其中<em class="lb"> V </em>是节点的集合，<em class="lb"> E </em>是边的集合，找出使割集的权重最大化的子集<em class="lb"> S </em> ⊆ <em class="lb"> V </em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/60440402bf6f67648c293d33fe70fced.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*jk0bW9O5ETQn5I-JUnScLQ.png"/></div></figure><p id="7e8d" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">其中<em class="lb"> w </em>是存储每个节点对之间权重的<em class="lb">T23】邻接对称矩阵。因此，有了<em class="lb"> N 个</em>节点，</em></p><ul class=""><li id="9537" class="nv nw iq lc b ld le lg lh lw nx lx ny ly nz lv or ob oc od bi translated">状态空间:大小为<em class="lb"> N </em> × <em class="lb"> N </em>的邻接对称矩阵和大小为<em class="lb"> N </em>的当前割集</li><li id="6c85" class="nv nw iq lc b ld oe lg of lw og lx oh ly oi lv or ob oc od bi translated">动作空间:尺寸为<em class="lb"> N </em>的割集</li><li id="ce44" class="nv nw iq lc b ld oe lg of lw og lx oh ly oi lv or ob oc od bi translated">奖励函数:割集的权重之和</li></ul><h2 id="d43e" class="ot mz iq bd na ou ov dn ne ow ox dp ni lw oy oz nk lx pa pb nm ly pc pd no pe bi translated"><strong class="ak">第一步:生成邻接对称矩阵并计算奖励<em class="pf"> : </em> </strong></h2><pre class="kg kh ki kj gt pg ph pi bn pj pk bi"><span id="4db2" class="pl mz iq ph b be pm pn l po pp">def generate_adjacency_symmetric_matrix(self, sparsity): # sparsity for binary<br/>  upper_triangle = torch.mul(torch.rand(self.N, self.N).triu(diagonal=1), (torch.rand(self.N, self.N) &lt; sparsity).int().triu(diagonal=1))<br/>  adjacency_matrix = upper_triangle + upper_triangle.transpose(-1, -2)<br/>  return adjacency_matrix # num_env x self.N x self.N<br/>        <br/>def get_cut_value(self,  adjacency_matrix, configuration):<br/>  return torch.mul(torch.matmul(configuration.reshape(self.N, 1), (1 - configuration.reshape(-1, self.N, 1)).transpose(-1, -2)), adjacency_matrix).flatten().sum(dim=-1)</span></pre><h2 id="d15a" class="ot mz iq bd na ou ov dn ne ow ox dp ni lw oy oz nk lx pa pb nm ly pc pd no pe bi translated"><strong class="ak">第二步:使用 vmap 批量执行函数</strong></h2><p id="43aa" class="pw-post-body-paragraph kz la iq lc b ld nq jr lf lg nr ju li lw ns ll lm lx nt lp lq ly nu lt lu lv ij bi translated">在本教程中，我们使用 PyTorch 的<a class="ae kv" href="https://pytorch.org/functorch/stable/generated/functorch.vmap.html" rel="noopener ugc nofollow" target="_blank"> vmap </a>函数在 GPU 上实现并行计算。vmap 函数是一个矢量化地图，它将函数作为输入并返回其矢量化版本。因此，我们基于 GPU 的 max cut 环境可以实现如下:</p><pre class="kg kh ki kj gt pg ph pi bn pj pk bi"><span id="f211" class="pl mz iq ph b be pm pn l po pp">import torch<br/>import functorch<br/>import numpy as np<br/><br/><br/>class MaxcutEnv():<br/>  def __init__(self, N = 20, num_env=4096, device=torch.device("cuda:0"), episode_length=6):<br/>      self.N = N<br/>      self.state_dim = self.N * self.N + self.N # adjacency mat + configuration<br/>      self.basis_vectors, _ = torch.linalg.qr(torch.randn(self.N * self.N, self.N * self.N, dtype=torch.float))<br/>      self.num_env = num_env<br/>      self.device = device<br/>      self.sparsity = 0.005<br/>      self.episode_length = episode_length<br/>      self.get_cut_value_tensor = functorch.vmap(self.get_cut_value, in_dims=(0, 0))<br/>      self.generate_adjacency_symmetric_matrix_tensor = functorch.vmap(self.generate_adjacency_symmetric_matrix, in_dims=0)<br/>  <br/>  def reset(self, if_test=False, test_adjacency_matrix=None):<br/>      if if_test:<br/>          self.adjacency_matrix = test_adjacency_matrix.to(self.device)<br/>      else:<br/>          self.adjacency_matrix = self.generate_adjacency_symmetric_matrix_batch(if_binary=False, sparsity=self.sparsity).to(self.device)<br/>      self.configuration = torch.rand(self.adjacency_matrix.shape[0], self.N).to(self.device).to(self.device)<br/>      self.num_steps = 0<br/>      return self.adjacency_matrix, self.configuration<br/>  <br/>  def step(self, configuration):<br/>      self.configuration = configuration   # num_env x N x 1<br/>      self.reward = self.get_cut_value_tensor(self.adjacency_matrix, self.configuration)<br/>      self.num_steps +=1<br/>      self.done = True if self.num_steps &gt;= self.episode_length else False<br/>      return (self.adjacency_matrix, self.configuration.detach()), self.reward, self.done</span></pre><p id="ab04" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">我们也可以类似地实现 TSP 问题。如下所示，我们在一个 A100 GPU 上测试了基于 GPU 的环境的每秒帧数(FPS)。首先，在这两个任务中，随着使用更多的并行环境，FPS 线性增加。然而，<strong class="lc ir"> GPU 利用率实际上限制了并行环境的数量</strong>。一旦 GPU 利用率达到最大，更多并行环境带来的加速会明显下降。在 max cut 中大约有 8，192 个环境会发生这种情况，在 TSP 中大约有 16，384 个环境会发生这种情况。因此，基于 GPU 的环境的最佳性能在很大程度上取决于 GPU 类型和任务的复杂性。</p><div class="kg kh ki kj gt ab cb"><figure class="pq kk pr ps pt pu pv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/2a3f7c164fae0e96f5a1912cb8f4fd2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*yCxKjJC-8o_MO6KHNm1qVw.jpeg"/></div></figure><figure class="pq kk pr ps pt pu pv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/e49c0bcb7c25a3421f847038f5a92672.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*lxcvXtKyCDhTwCi6H9G3eA.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk pw di px py translated">图 4:图 Maxcut 和 TSP 的每秒帧数。[图片来自作者]</p></figure></div><p id="ddfd" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">最后，我们给出了<a class="ae kv" href="https://github.com/AI4Finance-Foundation/RLSolver/blob/main/helloworld/graph_maxcut/vecenv_matcut.py" rel="noopener ugc nofollow" target="_blank">最大割</a>问题和<a class="ae kv" href="https://github.com/AI4Finance-Foundation/RLSolver/blob/main/helloworld/tsp/env_tsp.py" rel="noopener ugc nofollow" target="_blank"> TSP </a>问题的源代码。</p><h1 id="7c89" class="my mz iq bd na nb ok nd ne nf ol nh ni jw om jx nk jz on ka nm kc oo kd no np bi translated">结论</h1><p id="57c2" class="pw-post-body-paragraph kz la iq lc b ld nq jr lf lg nr ju li lw ns ll lm lx nt lp lq ly nu lt lu lv ij bi translated">大规模并行模拟在数据驱动方法中具有巨大的潜力。它不仅可以加快数据收集过程和加速工作流程，而且为研究概括和探索问题提供了新的机会。例如，一个智能代理<strong class="lc ir">可以简单地与数千个环境交互，其中每个环境包含不同的对象，以学习健壮的策略，</strong>或<strong class="lc ir">可以针对不同的环境利用不同的探索策略，以获得不同的数据</strong>。因此，如何有效地利用这个奇妙的工具仍然是一个挑战！</p><p id="2ee4" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">希望这篇文章能为你提供一些见解。如果你对更多感兴趣，请关注我们的开源社区<a class="ae kv" href="https://github.com/AI4Finance-Foundation" rel="noopener ugc nofollow" target="_blank">和</a><a class="ae kv" href="https://github.com/AI4Finance-Foundation/ElegantRL" rel="noopener ugc nofollow" target="_blank">回购</a>并加入我们的<a class="ae kv" href="https://ai4financeworkspace.slack.com/join/shared_invite/zt-v670l1jm-dzTgIT9fHZIjjrqprrY0kg#/shared-invite/email" rel="noopener ugc nofollow" target="_blank"> slack </a>！</p><h1 id="7e6b" class="my mz iq bd na nb ok nd ne nf ol nh ni jw om jx nk jz on ka nm kc oo kd no np bi translated">参考</h1><p id="88fb" class="pw-post-body-paragraph kz la iq lc b ld nq jr lf lg nr ju li lw ns ll lm lx nt lp lq ly nu lt lu lv ij bi translated">[1] Akkaya，Ilge，Marcin Andrychowicz，Maciek Chociej，Mateusz Litwin，Bob McGrew，Arthur Petron，Alex Paino 等人<em class="lb">用机器手解魔方</em>。<em class="lb"> arXiv 预印本 arXiv:1910.07113，</em> 2019。</p><p id="382f" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">[2]维克多·马科维奇丘克，卢卡什·，·郭，米歇尔·卢，基尔·斯托里，迈尔斯·麦克林，大卫·赫勒，尼基塔·鲁丁，阿瑟·奥尔希尔，安库尔·汉达，等.<em class="lb">艾萨克健身房:基于高性能 GPU 的机器人学习物理仿真</em>。NeurIPS，数据集和基准专题，2021 年。</p><p id="a4b8" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">[3] J .舒尔曼、f .沃尔斯基、普拉富拉·德里瓦尔、亚历克·拉德福德和奥列格·克里莫夫。<em class="lb">近似策略优化算法</em>。ArXiv，abs/1707.06347，2017。</p><p id="2d41" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">[4]斯科特·藤本、赫克·霍夫和大卫·梅格。<em class="lb">解决 actor-critic 方法中的函数近似错误</em>。2018 年机器学习国际会议。</p><p id="d7c6" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">[5] Tuomas Haarnoja、Aurick Zhou、P. Abbeel 和 Sergey Levine。<em class="lb">软行动者-批评家:具有随机行动者的非策略最大熵深度强化学习</em>。2018 年机器学习国际会议。</p></div></div>    
</body>
</html>