<html>
<head>
<title>Graph Machine Learning @ ICML 2022</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图形机器学习@ ICML 2022</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/graph-machine-learning-icml-2022-252f39865c70#2022-07-25">https://towardsdatascience.com/graph-machine-learning-icml-2022-252f39865c70#2022-07-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="889c" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">GraphML有什么新特性？</h2><div class=""/><div class=""><h2 id="ba23" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">最新进展和热点趋势，2022年7月版</h2></div><p id="3cc7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">国际机器学习大会(ICML) 是研究人员发表最佳作品的主要场所之一。ICML 2022挤满了数百份论文和<a class="ae lk" href="https://icml.cc/Conferences/2022/Schedule?type=Workshop" rel="noopener ugc nofollow" target="_blank">众多致力于图表的工作室</a>。我们分享最热门的研究领域的概况🔥在图表m1中。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/d3330f73e125b0af570c245851ff1f68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WM79_JkL45YRuDGiJ4S7pQ.png"/></div></div></figure></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><p id="5200" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="me">本帖由</em> <a class="ae lk" href="https://twitter.com/michael_galkin" rel="noopener ugc nofollow" target="_blank"> <em class="me">迈克尔·高尔金</em> </a> <em class="me">(米拉)和</em> <a class="ae lk" href="https://twitter.com/zhu_zhaocheng" rel="noopener ugc nofollow" target="_blank"> <em class="me">朱兆承</em> </a> <em class="me">(米拉)撰写。</em></p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><p id="f97d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们尽最大努力强调了ICML会议上图形ML的主要进展，每个主题包含2-4篇论文。尽管如此，由于被接受的论文数量庞大，我们可能会错过一些作品——请在评论或社交媒体上让我们知道。</p><h1 id="ac82" class="mf mg iq bd mh mi mj mk ml mm mn mo mp kf mq kg mr ki ms kj mt kl mu km mv mw bi translated">目录(可点击):</h1><ol class=""><li id="ed1a" class="mx my iq kq b kr mz ku na kx nb lb nc lf nd lj ne nf ng nh bi translated"><a class="ae lk" href="#7cf5" rel="noopener ugc nofollow">生成:去噪扩散就是你所需要的</a></li><li id="f89d" class="mx my iq kq b kr ni ku nj kx nk lb nl lf nm lj ne nf ng nh bi translated"><a class="ae lk" href="#b96e" rel="noopener ugc nofollow">图形变形金刚</a></li><li id="5f11" class="mx my iq kq b kr ni ku nj kx nk lb nl lf nm lj ne nf ng nh bi translated"><a class="ae lk" href="#165d" rel="noopener ugc nofollow">理论和表达性GNNs </a></li><li id="79ae" class="mx my iq kq b kr ni ku nj kx nk lb nl lf nm lj ne nf ng nh bi translated"><a class="ae lk" href="#5145" rel="noopener ugc nofollow">光谱GNNs </a></li><li id="6a03" class="mx my iq kq b kr ni ku nj kx nk lb nl lf nm lj ne nf ng nh bi translated"><a class="ae lk" href="#be73" rel="noopener ugc nofollow">可解释的GNNs </a></li><li id="1139" class="mx my iq kq b kr ni ku nj kx nk lb nl lf nm lj ne nf ng nh bi translated"><a class="ae lk" href="#fe10" rel="noopener ugc nofollow">图形增强:超出边缘丢失</a></li><li id="a350" class="mx my iq kq b kr ni ku nj kx nk lb nl lf nm lj ne nf ng nh bi translated"><a class="ae lk" href="#2f59" rel="noopener ugc nofollow">算法推理和图形算法</a></li><li id="55ed" class="mx my iq kq b kr ni ku nj kx nk lb nl lf nm lj ne nf ng nh bi translated"><a class="ae lk" href="#bbee" rel="noopener ugc nofollow">知识图推理</a></li><li id="6f00" class="mx my iq kq b kr ni ku nj kx nk lb nl lf nm lj ne nf ng nh bi translated"><a class="ae lk" href="#774c" rel="noopener ugc nofollow">计算生物学:分子连接、蛋白质结合、性质预测</a></li><li id="d171" class="mx my iq kq b kr ni ku nj kx nk lb nl lf nm lj ne nf ng nh bi translated"><a class="ae lk" href="#f4b7" rel="noopener ugc nofollow">酷图应用</a></li></ol></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="7cf5" class="mf mg iq bd mh mi nn mk ml mm no mo mp kf np kg mr ki nq kj mt kl nr km mv mw bi translated">代:去噪扩散是你所需要的</h1><p id="fd8b" class="pw-post-body-paragraph ko kp iq kq b kr mz ka kt ku na kd kw kx ns kz la lb nt ld le lf nu lh li lj ij bi translated"><strong class="kq ja">去噪扩散概率模型</strong> ( <a class="ae lk" href="https://arxiv.org/abs/2006.11239" rel="noopener ugc nofollow" target="_blank"> DDPMs </a>)将在2022年接管深度学习领域，几乎所有领域都具有令人惊叹的生成质量和比GANs和VAEs更好的理论属性，例如，图像生成(<a class="ae lk" href="https://arxiv.org/abs/2112.10741" rel="noopener ugc nofollow" target="_blank"> GLIDE </a>、<a class="ae lk" href="https://openai.com/dall-e-2/" rel="noopener ugc nofollow" target="_blank"> DALL-E 2 </a>、<a class="ae lk" href="https://gweb-research-imagen.appspot.com/paper.pdf" rel="noopener ugc nofollow" target="_blank"> Imagen </a>)、<a class="ae lk" href="https://arxiv.org/pdf/2205.09853.pdf" rel="noopener ugc nofollow" target="_blank">视频生成</a>、文本生成(<a class="ae lk" href="https://arxiv.org/pdf/2205.14217.pdf" rel="noopener ugc nofollow" target="_blank"> Diffusion-LM </a>)，甚至<a class="ae lk" href="https://arxiv.org/pdf/2205.09991.pdf" rel="noopener ugc nofollow" target="_blank">从概念上讲，扩散模型逐渐向输入对象添加噪声(直到它是高斯噪声)，并学习预测添加的噪声水平，以便我们可以从对象中减去它(去噪)。</a></p><p id="7ee1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">扩散可能是GraphML在2022年的最大趋势——特别是当应用于药物发现、分子和构象异构体生成以及一般的量子化学时。通常，它们与等变GNNs的最新进展配对。ICML为图形生成提供了几个很酷的去噪扩散实现。</p><p id="4195" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在由<strong class="kq ja"> Hoogeboom、Satorras、Vignac和Welling </strong>撰写的《<a class="ae lk" href="https://arxiv.org/pdf/2203.17003.pdf" rel="noopener ugc nofollow" target="_blank"><em class="me">3d</em></a><em class="me"/>中用于分子生成的等变扩散中，作者定义了用于分子生成的等变扩散模型(<strong class="kq ja"> EDM </strong>，其必须在原子坐标<em class="me"> x </em>上保持E(3)等变(关于<em class="me">旋转</em>、<em class="me">平移</em>、重要的是，分子具有不同的特征形态:原子电荷是一个有序整数，原子类型是一个热点分类特征，原子坐标是连续特征，因此，例如，你不能只添加高斯噪声到一个热点特征，并期望模型工作。相反，作者设计了特定特征的噪声处理和损失函数，并缩放输入特征以训练稳定性。</p><p id="ec15" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">EDM采用<a class="ae lk" href="https://arxiv.org/pdf/2102.09844.pdf" rel="noopener ugc nofollow" target="_blank">最先进的E(n) GNN </a>作为神经网络，根据输入特征和时间步长预测噪声。在推理时，我们首先对期望数量的原子<em class="me"> M </em>进行采样，然后我们可以根据期望的属性<em class="me"> c </em>对EDM进行调节，并要求EDM生成分子(由特征<em class="me"> x </em>和<em class="me"> h </em>定义)作为<em class="me"> x，h ~ p(x，h | c，M) </em>。</p><p id="483c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在实验上，EDM在实现负对数似然性、分子稳定性和独特性方面远远优于基于归一化流量和VAE的方法。烧蚀表明，等变GNN编码器至关重要，因为用标准MPNN代替它会导致性能显著下降。GitHub 上已经有<a class="ae lk" href="https://github.com/ehoogeboom/e3_diffusion_for_molecules" rel="noopener ugc nofollow" target="_blank">的代码了，试试吧！</a></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi nv"><img src="../Images/ba0f38725794783094d72615fe62f672.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9boZOQmoy-tN-hBI"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">向前和向后扩散。来源:<a class="ae lk" href="https://arxiv.org/pdf/2203.17003.pdf" rel="noopener ugc nofollow" target="_blank"> Hoogeboom、Satorras、Vignac和Welling </a>。</p></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/6e479221fe6a49ba09bdac54d51e5af7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*OXFrmspWBs0EBJRRuUg8GQ.gif"/></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">基于扩散的生成可视化。来源:<a class="ae lk" href="https://twitter.com/emiel_hoogeboom/status/1509838163375706112" rel="noopener ugc nofollow" target="_blank">推特</a></p></figure><p id="4c6a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于2D图形，<a class="ae lk" href="https://arxiv.org/pdf/2202.02514.pdf" rel="noopener ugc nofollow" target="_blank"> Jo、Lee和Hwang </a>提出了通过随机微分方程系统 ( <strong class="kq ja"> GDSS </strong>)的<strong class="kq ja">图形扩散。之前的EDM是去噪扩散概率模型(DDPM)的一个实例，<strong class="kq ja"> GDSS </strong>属于DDPMs的姐妹分支，即<strong class="kq ja">基于分数的模型</strong>。事实上，最近的<a class="ae lk" href="https://openreview.net/pdf?id=PxTIG12RRHS" rel="noopener ugc nofollow" target="_blank">(ICLR ' 21)</a>表明，如果我们用随机微分方程(SDEs)描述前向扩散过程，DDPMs和基于分数的模型可以统一到同一个框架中。</strong></p><p id="5f30" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">SDE允许将连续时间内的扩散建模为<a class="ae lk" href="https://en.wikipedia.org/wiki/Wiener_process" rel="noopener ugc nofollow" target="_blank">维纳过程</a>(为简单起见，假设它是添加噪声过程的一个奇特术语)，而DDPMs通常以1000步将其离散化(具有可学习的时间嵌入)，尽管SDE需要使用特定的解算器。与之前基于分数的图生成器相比，<strong class="kq ja"> GDSS </strong>将相邻关系<em class="me"> A </em>和节点特征<em class="me"> X </em>作为输入(并预测)。表示为SDE的向前和向后扩散需要计算<em class="me">分数</em>——这里是联合对数密度(X，A)的梯度。为了获得这些密度，我们需要一个<em class="me">基于分数的模型</em>，这里作者使用了一个<a class="ae lk" href="https://openreview.net/pdf?id=JHcqXGaqiGn" rel="noopener ugc nofollow" target="_blank">具有注意力集中</a>的GNN(图形多头注意力)。</p><p id="215e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在训练时，我们求解一个<strong class="kq ja">正向SDE </strong>并训练一个得分模型，而在推理时，我们使用训练好的得分模型并求解<strong class="kq ja">反向时间SDE </strong>。通常，你会在这里使用类似于<a class="ae lk" href="https://en.wikipedia.org/wiki/Langevin_dynamics" rel="noopener ugc nofollow" target="_blank">朗之万动力学</a>的东西，例如，朗之万MCMC，但是高阶<a class="ae lk" href="https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods" rel="noopener ugc nofollow" target="_blank">龙格-库塔</a>解算器原则上也应该在这里工作。实验表明，在2D图生成任务中，GDSS远远优于自回归生成模型和一次性VAEs，尽管由于集成了逆时SDE，采样速度可能仍然是一个瓶颈。<a class="ae lk" href="https://github.com/harryjo97/GDSS" rel="noopener ugc nofollow" target="_blank"> GDSS码</a>已经有了！</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ob"><img src="../Images/275f3c13f76aed8a4a3516756449f518.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4xi6x7S6bDoQz4UV"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">GDSS直觉。资料来源:<a class="ae lk" href="https://arxiv.org/pdf/2202.02514.pdf" rel="noopener ugc nofollow" target="_blank">乔、李和黄</a></p></figure><p id="dfee" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">👀现在看看arxiv，我们期待今年会有更多的扩散模型发布——图中的DDPMs应该有他们自己的大博客，敬请关注！</p><p id="8f61" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">➡️最后，非扩散生成的一个例子是Martinkus等人的工作，他们设计了用于一次性图形生成的GAN。除了经常立即生成邻接矩阵的其他GANs之外，<strong class="kq ja"> SPECTRE </strong>的思想是根据拉普拉斯算子的前k个(最低)特征值和特征向量来生成图形，这些特征值和特征向量已经给出了集群和连通性的一些概念。1️⃣ <strong class="kq ja">幽灵</strong>生成<em class="me"> k </em>个特征值。2️⃣作者使用了一个聪明的技巧，从top-k特征向量诱导的<a class="ae lk" href="https://en.wikipedia.org/wiki/Stiefel_manifold" rel="noopener ugc nofollow" target="_blank">斯蒂费尔流形</a>中抽取特征向量。Stiefel流形提供了一组标准正交矩阵，从中我们可以采样一个<em class="me"> n x k </em>矩阵。3️⃣Finally，获得一个拉普拉斯算子，作者使用一个<a class="ae lk" href="https://papers.nips.cc/paper/2019/file/bb04af0f7ecaee4aae62035497da1387-Paper.pdf" rel="noopener ugc nofollow" target="_blank">可证明强大的图网</a>来生成最终邻接。</p><p id="338e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">实验表明，<strong class="kq ja"> SPECTRE </strong>比其他GANs好几个数量级，比自回归图形生成器快30倍🚀。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oc"><img src="../Images/1492a80d92e22c8980d1d85a5a281e96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oT6-JIrrLCAoUu_-"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">SPECTRE生成特征值-&gt;特征向量-&gt;邻接的3步过程。来源:<a class="ae lk" href="https://arxiv.org/pdf/2204.01613.pdf" rel="noopener ugc nofollow" target="_blank">马丁库斯等人</a></p></figure></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="b96e" class="mf mg iq bd mh mi nn mk ml mm no mo mp kf np kg mr ki nq kj mt kl nr km mv mw bi translated">图形转换器</h1><p id="edd0" class="pw-post-body-paragraph ko kp iq kq b kr mz ka kt ku na kd kw kx ns kz la lb nt ld le lf nu lh li lj ij bi translated">在今年的ICML上，我们有两篇关于改进图形转换器的论文。</p><p id="e270" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">首先，<a class="ae lk" href="https://arxiv.org/pdf/2202.03036.pdf" rel="noopener ugc nofollow" target="_blank">陈，奥布雷，博格瓦德</a>提出<strong class="kq ja">结构感知变压器</strong>。他们注意到自我关注可以被重写为核平滑，其中查询键乘积是一个指数核。然后归结为找到一个更一般化的内核——作者建议使用节点和图的功能来添加结构意识，即<strong class="kq ja">k-子树</strong>和<strong class="kq ja">k-子图</strong>特征。<em class="me">K-子树</em>本质上是k-hop邻域，可以相对快速地挖掘，但最终受限于1-WL的表达能力。另一方面，<em class="me">k-子图</em>的计算成本更高(并且很难扩展)，但是提供了可证明的更好的区分能力。</p><p id="24f5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">无论您选择什么特征，这些子树或子图(为每个节点提取的)都将通过任何GNN编码器(例如，PNA)进行编码，汇集(总和/平均值/虚拟节点)，并在自我关注计算中用作查询和关键字(参见图示👇).</p><p id="08d0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">从实验上来说，k为3或4就足够了，k-子图特性在我们能够负担计算费用的图上工作得更好。有趣的是，像拉普拉斯特征向量和随机游走特征这样的位置特征只对<em class="me">k-子树SAT </em>有帮助，而对<em class="me">k-子图SAT </em>则毫无用处。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oc"><img src="../Images/f7cff0e99883483f60eda1bad8462db1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dyM1WMxaooNLzv58"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">资料来源:【陈、奥布雷、博格瓦德</p></figure><p id="40fa" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">第二，<a class="ae lk" href="https://arxiv.org/pdf/2107.07999.pdf" rel="noopener ugc nofollow" target="_blank"> Choromanski，林，陈等人</a>(该团队与著名<a class="ae lk" href="https://arxiv.org/abs/2009.14794" rel="noopener ugc nofollow" target="_blank">表演者</a>的作者有很多重叠)研究的是实现亚二次注意的原理性机制。特别是，他们考虑了相对位置编码(RPE)及其对不同数据形式(如图像、声音、视频和图形)的变化。考虑到图形，我们从<a class="ae lk" href="https://github.com/microsoft/Graphormer" rel="noopener ugc nofollow" target="_blank">graph former</a>得知，将最短路径距离注入注意力效果很好，但是需要完整注意力矩阵的具体化(因此，不可扩展)。我们能否在没有完全具体化的情况下近似softmax注意力，但仍然包含有用的图形归纳偏差？🤔</p><p id="03f3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">是啊！作者提出了两种这样的机制。(1)事实证明，我们可以使用<strong class="kq ja">图形扩散核(GDK)</strong>——也称为热核——模拟热传播的扩散过程，并作为最短路径的软版本。然而，扩散需要调用求解器来计算矩阵指数，因此作者设计了另一种方法。(2)随机行走图-节点核(RWGNK ),其值(对于两个节点)编码从这两个节点开始的随机行走获得的(这两个节点的)频率向量的点积。</p><p id="b967" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">随机漫步很棒，我们喜欢随机漫步😍查看下图，了解漫射和RW内核结果的可视化描述。具有RWGNK内核的最终转换器被称为<strong class="kq ja">图内核注意力转换器</strong><strong class="kq ja">【GKAT】</strong>，并且针对从er图中拓扑结构的综合识别到小型compbio和社交网络数据集的几个任务进行探索。GKAT 在合成任务上显示出更好的结果，并且在其他图形上的表现与GNNs相当。如果能看到真正的可伸缩性研究将转换器推向输入集大小的极限，那就太好了！</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oc"><img src="../Images/f27ae95ae90b3cfd1787b91aa706a77b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7NXxc31VcjKTc15V"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">资料来源:<a class="ae lk" href="https://arxiv.org/pdf/2107.07999.pdf" rel="noopener ugc nofollow" target="_blank">乔罗曼斯基、林、陈等</a></p></figure></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="165d" class="mf mg iq bd mh mi nn mk ml mm no mo mp kf np kg mr ki nq kj mt kl nr km mv mw bi translated">理论与表达性GNNs</h1><p id="184b" class="pw-post-body-paragraph ko kp iq kq b kr mz ka kt ku na kd kw kx ns kz la lb nt ld le lf nu lh li lj ij bi translated">GNN社区继续研究突破1-WL表达能力的上限并保持至少多项式时间复杂性的方法。</p><p id="a1b2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">➡️ <a class="ae lk" href="https://proceedings.mlr.press/v162/papp22a/papp22a.pdf" rel="noopener ugc nofollow" target="_blank">帕普和瓦腾霍夫</a>从当前理论研究的准确描述开始:</p><blockquote class="od oe of"><p id="4e12" class="ko kp me kq b kr ks ka kt ku kv kd kw og ky kz la oh lc ld le oi lg lh li lj ij bi translated">每当引入新的GNN变体时，相应的理论分析通常会显示它比1-WL更强大，有时还会将其与经典的k-WL层次结构进行比较……我们能否找到一种更有意义的方法来衡量GNN扩展的表现力？</p></blockquote><p id="5123" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">作者将表达性GNNs的文献分为4类:1️⃣ k-WL和近似；2️⃣底座计数<strong class="kq ja">(s)</strong>；3️⃣子图和邻域感知GNNs <strong class="kq ja"> (N) </strong>(在Michael Bronstein 最近的文章中广泛讨论了<a class="ae lk" rel="noopener" target="_blank" href="/using-subgraphs-for-more-expressive-gnns-8d06418d5ab">)；带有标记的4️⃣gnns——这些是节点/边扰动方法和节点/边标记方法<strong class="kq ja"> (M) </strong>。然后，作者提出了所有这些<strong class="kq ja"> k-WL、S、N和M </strong>家族如何相关以及哪一个在何种程度上更强大的理论框架。该层次比k-WL更细粒度，有助于设计具有足够表现力的gnn，以覆盖特定的下游任务并节省计算。</a></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oj"><img src="../Images/a4e698a4c3efc1e688f2d0beb27cddc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qF6OqUtucFYKFZEw"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">不同的富有表现力的GNN家族的等级制度。n =子图GNNs，S =子结构计数，M =带标记的GNNs。资料来源:<a class="ae lk" href="https://proceedings.mlr.press/v162/papp22a/papp22a.pdf" rel="noopener ugc nofollow" target="_blank"> Papp和Wattenhofer </a></p></figure><p id="1c40" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">➡️也许是最美味的ICML'22作品是由厨师们用🥓<a class="ae lk" href="https://github.com/chrsmrrs/speqnets" rel="noopener ugc nofollow" target="_blank"> SpeqNets </a>🥓(<em class="me">斯帕克</em>在德语中是<em class="me">培根</em>)。已知的高阶k-WL gnn要么在k阶张量上操作，要么考虑所有的<em class="me"> k </em>节点子图，这意味着在存储器需求上对<em class="me"> k </em>的指数依赖，并且不适应图的稀疏性。<strong class="kq ja"> SpeqNets </strong>为图同构问题引入了一种新的启发式算法，即<strong class="kq ja"> (k，s)-WL </strong>，它在表达性和可伸缩性之间提供了更细粒度的控制。</p><p id="0735" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">本质上，该算法是<a class="ae lk" href="https://arxiv.org/abs/1904.01543" rel="noopener ugc nofollow" target="_blank">局部k-WL </a>的变体，但仅考虑特定元组以避免k-WL的指数存储复杂性。具体来说，该算法只考虑最多具有<strong class="kq ja"> s个连通</strong>分量的k节点上的<strong class="kq ja"> k元组</strong>或子图，有效地利用了底层图的潜在稀疏性——变化的<strong class="kq ja"> k </strong>和<strong class="kq ja"> s </strong>导致理论上的可扩展性和表达性之间的折衷。</p><p id="9f1e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">基于上述组合见解，作者导出了一种新的置换等变图神经网络，记为<strong class="kq ja"> SpeqNets </strong>，在极限情况下达到普适性。与受监督的节点和图形级分类和回归机制中的标准高阶图形网络相比，这些架构大大减少了计算时间，从而显著提高了标准图形神经网络和图形核架构的预测性能。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oc"><img src="../Images/92777ba6fb94aac83523a1fff272905c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GGXN-0JHu6jnkz5E"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">的等级制度🥓SpeqNets🥓。来源:<a class="ae lk" href="https://proceedings.mlr.press/v162/morris22a/morris22a.pdf" rel="noopener ugc nofollow" target="_blank">莫里斯等人</a></p></figure><p id="149c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">接下来，<a class="ae lk" href="https://proceedings.mlr.press/v162/huang22l/huang22l.pdf" rel="noopener ugc nofollow" target="_blank">黄等人</a>以一种非正统的眼光看待排列不变的gnn，认为精心设计的<strong class="kq ja">排列敏感的</strong>gnn实际上更有表现力。Janossy pooling的理论说，如果我们展示一组变换的所有可能的例子，那么一个模型对于这样一组变换是不变的，对于n个T21元素的排列，我们有一个难以处理的n！排列组合。相反，作者表明，只考虑节点的邻域的成对2元排列就足够了，并且可以证明比2-WL更强大，并且不比3-WL更弱。</p><p id="593e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">实际上，提出的<a class="ae lk" href="https://github.com/zhongyu1998/PG-GNN" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> PG-GNN </strong> </a>扩展了GraphSAGE的思想，通过两层LSTM对节点邻域的每个随机排列进行编码，而不是传统的<em class="me"> sum/mean/min/max </em>。此外，作者设计了一种基于哈密顿圈的线性置换采样方法。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oc"><img src="../Images/6bf7109c77e4c463f1be481929c19eda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Rh17chscpE5r5UbK"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">PG-GNN排列敏感聚集思想。来源:<a class="ae lk" href="https://proceedings.mlr.press/v162/huang22l/huang22l.pdf" rel="noopener ugc nofollow" target="_blank">黄等</a></p></figure><p id="b232" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">你可能想看看其他一些有趣的作品:</p><ul class=""><li id="e65d" class="mx my iq kq b kr ks ku kv kx ok lb ol lf om lj on nf ng nh bi translated"><a class="ae lk" href="https://proceedings.mlr.press/v162/cai22b/cai22b.pdf" rel="noopener ugc nofollow" target="_blank">蔡和王</a>研究<a class="ae lk" href="https://arxiv.org/abs/1812.09902" rel="noopener ugc nofollow" target="_blank">不变图网络</a>的收敛性质，与传统MPNNs的不同之处在于，它们对节点和边特征的操作如同对整体张量的等变操作。基于<a class="ae lk" href="https://en.wikipedia.org/wiki/Graphon" rel="noopener ugc nofollow" target="_blank"> graphon </a>理论，作者找到了一类可证明收敛的ign。更多技术细节在<a class="ae lk" href="https://twitter.com/ChenCaiUCSD/status/1550109192803045376" rel="noopener ugc nofollow" target="_blank">牛逼的Twitter帖子</a>！</li><li id="e99e" class="mx my iq kq b kr ni ku nj kx nk lb nl lf nm lj on nf ng nh bi translated"><a class="ae lk" href="https://proceedings.mlr.press/v162/gao22e/gao22e.pdf" rel="noopener ugc nofollow" target="_blank">高与里贝罗</a>研究⏳时间GNNs⏳设计两个系列:(1)<em class="me"/>——我们首先通过一些GNN嵌入图形快照，然后应用一些RNN；(2) <em class="me">先时间后图形</em>其中，我们首先通过RNN对所有节点和边特征(在所有快照的统一图形上)进行编码，然后仅应用单个GNN过程，例如，<a class="ae lk" href="https://arxiv.org/abs/2006.10637" rel="noopener ugc nofollow" target="_blank"> TGN </a>和<a class="ae lk" href="https://openreview.net/forum?id=rJeW1yHYwH" rel="noopener ugc nofollow" target="_blank"> TGAT </a>可以被认为是该家族的实例。理论上，作者发现当使用像GCN或GIN这样的标准1-WL GNN编码器时，<em class="me">时间-图形</em>比<em class="me">时间-图形</em>更有表现力，并提出了一个带有GRU时间编码器和GCN图形编码器的简单模型。该模型在时态节点分类和回归任务上表现出极具竞争力的性能，速度快3-10倍，GPU内存效率高。有趣的是，作者发现<strong class="kq ja">无论是</strong> <em class="me">时间和图形</em>还是<em class="me">时间和图形</em> <strong class="kq ja">都不足以表达时间链接预测的</strong>🤔。</li><li id="a578" class="mx my iq kq b kr ni ku nj kx nk lb nl lf nm lj on nf ng nh bi translated">最后，“<em class="me">魏斯费勒-雷曼遇上-瓦瑟斯坦</em>”作者【陈、林、梅莫利、万、王(5-第一作者联合论文👀)从WL核中导出一个多项式时间的<a class="ae lk" href="https://github.com/chens5/WL-distance" rel="noopener ugc nofollow" target="_blank"> WL距离</a>，这样我们可以测量两个图的相异度——WL距离为0当且仅当它们不能被WL测试区分，并且正的当且仅当它们可以被区分。作者进一步认识到，提出的WL距离与<a class="ae lk" href="https://arxiv.org/abs/1808.04337" rel="noopener ugc nofollow" target="_blank">格罗莫夫-瓦瑟斯坦距离</a>有着深刻的联系！</li></ul><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/6801d4b956de53780f2948f9a6d2d695.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/0*ybT4mgocmE8xHq5k"/></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">魏斯费勒-莱曼如何在实践中遇到格罗莫夫-沃瑟斯坦？本应在论文中由<a class="ae lk" href="https://proceedings.mlr.press/v162/chen22o/chen22o.pdf" rel="noopener ugc nofollow" target="_blank">陈、老林、莫文蔚、万、王</a>。来源:<a class="ae lk" href="https://tenor.com/view/predator-arnold-schwarzenegger-hand-shake-arms-gif-3468629" rel="noopener ugc nofollow" target="_blank">期限</a></p></figure></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="5145" class="mf mg iq bd mh mi nn mk ml mm no mo mp kf np kg mr ki nq kj mt kl nr km mv mw bi translated">光谱GNNs</h1><p id="57c1" class="pw-post-body-paragraph ko kp iq kq b kr mz ka kt ku na kd kw kx ns kz la lb nt ld le lf nu lh li lj ij bi translated">➡️光谱GNNs往往被忽视的空间GNNs的主流，但现在有一个理由让你看看光谱GNNs 🧐.在王<a class="ae lk" href="https://proceedings.mlr.press/v162/wang22am/wang22am.pdf" rel="noopener ugc nofollow" target="_blank">和张</a>的《<a class="ae lk" href="https://proceedings.mlr.press/v162/wang22am/wang22am.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="me">谱图神经网络</em> </a>有多强大》一文中，作者证明了在一些较弱的假设下，线性谱是图上任何函数的通用逼近子。更令人兴奋的是，根据经验，这些假设对于真实世界的图来说是正确的，这表明线性谱GNN对于节点分类任务来说足够强大。</p><p id="3d4d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">但是我们如何解释光谱GNNs实验结果的差异呢？作者证明了谱GNNs的不同参数化(特别是多项式滤波器)影响收敛速度。我们知道Hessian矩阵的条件数(等损线有多圆)与收敛速度高度相关。基于这种直觉，作者提出了一些有利于优化的正交多项式。被命名为<a class="ae lk" href="https://en.wikipedia.org/wiki/Jacobi_polynomials" rel="noopener ugc nofollow" target="_blank">雅可比基</a>的多项式是<a class="ae lk" href="https://proceedings.neurips.cc/paper/2016/file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf" rel="noopener ugc nofollow" target="_blank">切比雪夫基</a>中使用的<a class="ae lk" href="https://en.wikipedia.org/wiki/Chebyshev_polynomials" rel="noopener ugc nofollow" target="_blank">切比雪夫基</a>的推广。雅可比基由两个超参数定义，<em class="me"> a </em>和<em class="me"> b </em>。通过调整这些超参数，可以找到一组有利于输入图形信号的基。</p><p id="f14d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">实验上，<strong class="kq ja"> JacobiConv </strong>在嗜同性和嗜异性数据集上都表现良好，即使作为线性模型也是如此。或许是时候抛弃那些华而不实的gnn了，至少对于节点分类任务来说是这样😏。</p><p id="e7b2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">➡️:还有两篇关于光谱gnn的论文。一个是基于光谱浓度分析的<a class="ae lk" href="https://proceedings.mlr.press/v162/li22h/li22h.pdf" rel="noopener ugc nofollow" target="_blank">图高斯卷积网络</a> (G2CN)，在嗜异性数据集上显示出良好的结果。另一篇来自<a class="ae lk" href="https://proceedings.mlr.press/v162/yang22n/yang22n.pdf" rel="noopener ugc nofollow" target="_blank"> Yang等人</a>的文章分析了基于光谱平滑度的图形卷积中的相关性问题，显示了锌上的<strong class="kq ja"> 0.0698 </strong> MAE的非常好的结果。</p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="be73" class="mf mg iq bd mh mi nn mk ml mm no mo mp kf np kg mr ki nq kj mt kl nr km mv mw bi translated">可解释的GNNs</h1><p id="0570" class="pw-post-body-paragraph ko kp iq kq b kr mz ka kt ku na kd kw kx ns kz la lb nt ld le lf nu lh li lj ij bi translated">由于大多数GNN模型都是黑箱，所以解释GNNs在关键领域的应用预测是很重要的。今年我们在这个方向有两篇很棒的论文，一篇是熊等人<a class="ae lk" href="https://proceedings.mlr.press/v162/xiong22a/xiong22a.pdf" rel="noopener ugc nofollow" target="_blank">的高效而强大的事后模型</a>，另一篇是苗等人<a class="ae lk" href="https://proceedings.mlr.press/v162/miao22a/miao22a.pdf" rel="noopener ugc nofollow" target="_blank"/>的内在可解释模型。</p><p id="a8de" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">熊等人扩展了他们之前的GNN解释方法<a class="ae lk" href="https://arxiv.org/pdf/2006.03589.pdf" rel="noopener ugc nofollow" target="_blank"> GNN-LRP </a>，使之更具可扩展性。与其他方法(<a class="ae lk" href="https://arxiv.org/pdf/1903.03894.pdf" rel="noopener ugc nofollow" target="_blank"> GNNExplainer </a>、<a class="ae lk" href="https://arxiv.org/pdf/2011.04573.pdf" rel="noopener ugc nofollow" target="_blank"> PGExplainer </a>、<a class="ae lk" href="https://arxiv.org/pdf/2010.05788.pdf" rel="noopener ugc nofollow" target="_blank"> PGM-Explainer </a>)不同的是，<a class="ae lk" href="https://arxiv.org/pdf/2006.03589.pdf" rel="noopener ugc nofollow" target="_blank"> GNN-LRP </a>是一种考虑子图中节点联合贡献的高阶子图归属方法。对于子图不仅仅是一组节点的任务来说，这种性质是必要的。例如，在分子中，六个碳的子图(忽略氢)可以是苯(一个环)或己烷(一个链)。如下图所示，高阶方法可以计算出这样的子图(右图)，而低阶方法(左图)则不能。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi op"><img src="../Images/f82c08d45a8583625636d0a21653ce27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dR_GvltDxvRKpSCh"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">来源:<a class="ae lk" href="https://proceedings.mlr.press/v162/xiong22a/xiong22a.pdf" rel="noopener ugc nofollow" target="_blank">熊等人</a>。</p></figure><p id="3ed3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然而，GNN-LRP算法的缺点是，它需要计算子图中每一次随机行走的梯度w.r.t .对于一个子图<em class="me"> S </em>和<em class="me"> L </em>跳随机行走需要<em class="me"> O(|S|L) </em>。这里动态编程来拯救😎。请注意，梯度w.r.t. a随机游走是乘法的(链式法则)，不同的随机游走通过求和来聚合。这可以通过和积算法有效地计算出来。这个想法是利用乘法上的求和的分配性质(更普遍的是，<a class="ae lk" href="https://en.wikipedia.org/wiki/Semiring" rel="noopener ugc nofollow" target="_blank">半环</a>)，在每一步聚合部分随机游走。这就构成了模型，<a class="ae lk" href="https://github.com/xiong-ping/sgnn_lrp_via_mp" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">子图GNN-LRP (sGNN-LRP) </strong> </a>。</p><p id="dc9d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">sGNN-LRP 还通过广义子图属性对GNN-LRP进行了改进，该属性考虑了子图<em class="me"> S </em>及其补图<em class="me"> G\S </em>中的随机游动。虽然看起来很复杂，但广义子图属性可以通过两次和积算法来计算。实验上，<strong class="kq ja"> sGNN-LRP </strong>不仅比所有现有的解释方法更好地发现属性，而且运行速度与常规消息传递GNN一样快。可能是解释和可视化的有用工具！🔨</p><p id="a567" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">💡顺便说一句，基于随机游走的模型比简单的节点或边模型更有表现力，这并不新鲜。NeurIPS的21篇论文<a class="ae lk" href="https://papers.nips.cc/paper/2021/file/f6a673f09493afcd8b129a0bcf1cd5bc-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> NBFNet </a>用随机游走和动态规划解决知识图推理，在直推式和归纳式设置中都取得了惊人的结果。</p><p id="8f88" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">➡️ <a class="ae lk" href="https://proceedings.mlr.press/v162/miao22a/miao22a.pdf" rel="noopener ugc nofollow" target="_blank">苗等</a>从另一个角度研究内在可解释的GNN模型。他们表明，事后解释方法，如<a class="ae lk" href="https://arxiv.org/pdf/1903.03894.pdf" rel="noopener ugc nofollow" target="_blank"> GNNExplainer </a>，对于解释来说是不合格的，因为它们仅仅使用了一个固定的预训练GNN模型。相比之下，联合优化预测器和解释模块的内在可解释GNN是更好的解决方案。沿着这个思路，作者从图信息瓶颈(<strong class="kq ja"> GIB </strong>)原理中推导出<a class="ae lk" href="https://github.com/Graph-COM/GSAT" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">图随机注意(GSAT) </strong> </a>。<strong class="kq ja"> GSAT </strong>对输入图进行编码，从后验分布中随机抽取一个子图(解释)。它基于采样的子图进行预测。作为一个优势，<strong class="kq ja"> GSAT </strong>不需要限制采样子图的大小。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oq"><img src="../Images/f0084847dc6fce78a89c4b848edafc85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qBxaIzgzWvEs8UA7"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">来源:<a class="ae lk" href="https://proceedings.mlr.press/v162/miao22a/miao22a.pdf" rel="noopener ugc nofollow" target="_blank">苗等人</a></p></figure><p id="c37f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">实验上，<strong class="kq ja"> GSAT </strong>在解释和预测性能方面都比事后方法好得多。它也可以与预训练的GNN模型相结合。如果您正在为您的应用程序构建可解释的gnn，GSAT应该是一个不错的选择。</p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="fe10" class="mf mg iq bd mh mi nn mk ml mm no mo mp kf np kg mr ki nq kj mt kl nr km mv mw bi translated">图形扩充:超越边缘丢失</h1><p id="6e56" class="pw-post-body-paragraph ko kp iq kq b kr mz ka kt ku na kd kw kx ns kz la lb nt ld le lf nu lh li lj ij bi translated">今年带来了一些关于改进gnn的自我监督能力的工作，这些工作超越了像节点/边丢失这样的随机边索引扰动。</p><p id="8e3a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae lk" href="https://arxiv.org/pdf/2202.07179.pdf" rel="noopener ugc nofollow" target="_blank">韩等人</a>将2017年以来用于图像增强的<a class="ae lk" href="https://github.com/facebookresearch/mixup-cifar10" rel="noopener ugc nofollow" target="_blank">混合</a>的思想带到了带有<strong class="kq ja">g-mix</strong>的图中(ICML 2022优秀论文奖🏅).混合的想法是拍摄两幅图像，将它们的特征混合在一起，并将它们的标签混合在一起(根据预定义的加权因子)，并要求模型预测这个标签。这种混合提高了分类器的鲁棒性和泛化质量。</p><blockquote class="od oe of"><p id="ba45" class="ko kp me kq b kr ks ka kt ku kv kd kw og ky kz la oh lc ld le oi lg lh li lj ij bi translated">但是我们如何混合两个通常可能有不同数量的节点和边的图呢？</p></blockquote><p id="b7eb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">作者找到了优雅的答案——让我们不要混合图表，而是混合他们的<a class="ae lk" href="https://en.wikipedia.org/wiki/Graphon" rel="noopener ugc nofollow" target="_blank">图表</a>——简而言之，就是图表生成器。来自同一个生成器的图具有相同的底层graphon。因此算法变得相当简单(见下图)——对于一对图，我们1️⃣估计它们的图；2️⃣通过加权求和将两个文法混合成一个新文法；3️⃣从新graphon和它的新标号中抽取一个图样本；4️⃣把这个送到分类器。在说明性示例中，我们有两个分别具有2个和8个连通分量的图，并且在混合它们的图之后，我们得到两个主要社区的新图，每个主要社区具有4个次要社区。估计图可以用一个阶跃函数和几种计算复杂度不同的方法来完成(作者大多求助于<a class="ae lk" href="https://arxiv.org/abs/1110.6517" rel="noopener ugc nofollow" target="_blank">【最大间隙】</a>)。</p><p id="1d90" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">实验上，<strong class="kq ja"> G-Mixup </strong>稳定了模型训练，表现得比传统的节点/边扰动方法更好或相当，但是在具有标签噪声或许多添加/移除的边的鲁棒性场景中远远超过它们。一种众所周知的扩充方法对图形的冷静适应👏！如果你感兴趣，ICML 22提供了一些关于混合的更一般的作品:一项关于混合如何改进校准的研究和如何在生成模型中使用它们的研究。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oc"><img src="../Images/89fba4f792995aee27abbc851055a3d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pLo1sPxfRu50w2SO"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">G-Mixup。来源:<a class="ae lk" href="https://arxiv.org/pdf/2202.07179.pdf" rel="noopener ugc nofollow" target="_blank">韩等</a></p></figure><p id="3911" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">刘等人从另一个角度来看增强技术，特别是在节点的邻域很小的情况下。<a class="ae lk" href="https://github.com/SongtaoLiu0823/LAGNN" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">局部增强GNNs (LA-GNN) </strong> </a>的思想是训练一个生成模型，为每个节点产生一个附加的特征向量。生成模型是(在整个图形上)训练的条件VAE，以预测以中心节点为条件的相连邻居的特征。也就是说，一旦CVAE被训练，我们只需传递每个节点的一个特征向量，并获得另一个特征向量，该特征向量应该比普通邻居捕获更多的信息。</p><p id="d7f8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然后，我们将每个节点的两个特征向量连接起来，并将其发送给任何下游的GNN和任务。注意，CVAE是预先训练过的，不需要用GNN训练。有趣的是，CVAE可以为看不见的图形生成特征，即，局部增强也可以用于归纳任务！最初的假设通过实验得到了证实——增强方法对于小角度的节点特别有效。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oc"><img src="../Images/7baa2334f4507a7ed9a42639ce6d1974.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VLK_Q-Jz3bpO5HXy"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">局部增强的想法。来源:<a class="ae lk" href="https://arxiv.org/pdf/2109.03856.pdf" rel="noopener ugc nofollow" target="_blank">刘等</a></p></figure><p id="d8e7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">下一步，<a class="ae lk" href="https://arxiv.org/pdf/2206.07161.pdf" rel="noopener ugc nofollow" target="_blank">于，王，王等人</a>，处理GNN可扩展性任务，其中使用标准邻居采样器a-la GraphSAGE可能导致指数邻域大小扩展和陈旧的历史嵌入。作者提出了<a class="ae lk" href="https://github.com/divelab/DIG/tree/dig/dig/lsgraph" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> GraphFM </strong> </a>，一种特征动量方法，其中历史节点嵌入通过动量步骤从它们的1跳邻居获得更新。通常，动量更新经常出现在SSL方法中，如用于更新<em class="me">目标</em>网络的模型参数的<a class="ae lk" href="https://papers.nips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> BYOL </a>和<a class="ae lk" href="https://arxiv.org/abs/2102.06514" rel="noopener ugc nofollow" target="_blank"> BGRL </a>。这里，GraphFM使用动量来减轻不同小批量中历史再现的方差，并为不同大小的邻域提供特征更新的无偏估计。</p><p id="0193" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">一般来说，GraphFM有两种选择<strong class="kq ja">graph FM-<em class="me">in batch</em>T17】和<strong class="kq ja">graph FM-<em class="me">out of batch</em>T21。(1) GraphFM-InBatch通过大幅减少必要邻居的数量来实现GraphSAGE风格的邻居采样，而GraphSAGE需要10–20个邻居，具体取决于级别，GraphFM只需要每层每个节点1个随机邻居。唯一👌！(2) GraphFM Out-of-batch构建在<a class="ae lk" href="https://arxiv.org/pdf/2106.05609.pdf" rel="noopener ugc nofollow" target="_blank"> GNNAutoScale </a>之上，我们首先应用图分割将图分割成k个小批。</strong></strong></p><p id="b6fc" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">从实验上看，特征动量看起来对SAGE风格的采样(批量版本)特别有用——对于所有基于邻居采样的方法来说，这似乎是一个很好的默认选择！</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oc"><img src="../Images/7729c48fe6ebfbc2631a0f434185bb73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*721I25AT_kQq-dhI"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">与<a class="ae lk" href="https://arxiv.org/pdf/2106.05609.pdf" rel="noopener ugc nofollow" target="_blank"> GNNAutoScale (GAS) </a>相比，历史节点状态也根据新的嵌入和特征动量(移动平均)进行更新。来源:【于，王，王，等</p></figure><p id="3064" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最后，<a class="ae lk" href="https://arxiv.org/pdf/2106.02172.pdf" rel="noopener ugc nofollow" target="_blank">赵等人</a>提出了一个巧妙的基于反事实链接的链接预测增强技巧。本质上，作者问:</p><blockquote class="od oe of"><p id="042b" class="ko kp me kq b kr ks ka kt ku kv kd kw og ky kz la oh lc ld le oi lg lh li lj ij bi translated">"如果图形结构变得与观察不同，这种联系还会存在吗？"</p></blockquote><p id="7508" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这意味着我们想找到链接，在结构上类似于根据一些给定的链接💊<em class="me">处理</em>(这里，那些是经典的度量标准，如SBM聚类、k-core分解、Louvain等等)但是给出了相反的结果。通过<a class="ae lk" href="https://github.com/DM2-ND/CFLP" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> CFLP </strong> </a>，作者假设训练GNN正确预测真实和反事实的联系有助于模型摆脱虚假的相关性，并只捕捉对推断两个节点之间的联系有意义的特征。</p><p id="8221" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在获得一组反事实链路之后(基于所选的<em class="me">处理函数</em>的预处理步骤)<strong class="kq ja"> CFLP </strong>首先在事实链路和反事实链路上被训练，然后链路预测解码器利用一些平衡和正则化项被微调。从某种意义上来说，这种方法类似于挖掘硬性否定来增加真实肯定联系的集合🤔实验上，<strong class="kq ja"> CFLP </strong>与GNN编码器配对大大优于单个GNN编码器在Cora/Citeseer/Pubmed上的结果，并且仍然在OGB-DDI 链接预测任务的<a class="ae lk" href="https://ogb.stanford.edu/docs/leader_linkprop/" rel="noopener ugc nofollow" target="_blank"> top-3中！</a></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oc"><img src="../Images/8fb02d0f2d18ff01ca732d42e111dab1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1Ww02qcZ0CKcq-fM"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">反事实链接(右)。来源:<a class="ae lk" href="https://arxiv.org/pdf/2106.02172.pdf" rel="noopener ugc nofollow" target="_blank">赵等人</a></p></figure></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="2f59" class="mf mg iq bd mh mi nn mk ml mm no mo mp kf np kg mr ki nq kj mt kl nr km mv mw bi translated">算法推理和图形算法</h1><p id="0d0e" class="pw-post-body-paragraph ko kp iq kq b kr mz ka kt ku na kd kw kx ns kz la lb nt ld le lf nu lh li lj ij bi translated">🎆算法推理社区的一个巨大里程碑——CLRS基准测试<a class="ae lk" href="https://github.com/deepmind/clrs" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">的出现</strong> </a>(以一本经典教科书<a class="ae lk" href="https://en.wikipedia.org/wiki/Introduction_to_Algorithms" rel="noopener ugc nofollow" target="_blank">命名，由Cormen、Leiserson、Rivest和Stein编写</a>，Velič ković等人编写<a class="ae lk" href="https://arxiv.org/pdf/2205.15659.pdf" rel="noopener ugc nofollow" target="_blank"/>！现在，没有必要发明玩具评估任务——CLRS包含30种经典算法(排序、搜索、MST、最短路径、图形、动态规划等等),将一个<a class="ae lk" href="https://icpc.global/" rel="noopener ugc nofollow" target="_blank"> ICPC </a>数据生成器转换成一个ML数据集😎。</p><p id="8226" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在<strong class="kq ja"> CLRS </strong>中，每个数据集元素是一个<em class="me">轨迹</em>，即输入、输出和中间步骤的集合。底层的表示格式是一组节点(通常不是一个图，因为边可能不是必需的)，例如，对5个元素的列表进行排序被构造为对一组5个节点的操作。轨迹由<em class="me">探测器</em>组成——格式元组(阶段、位置、类型、值)，用其状态编码算法的当前执行步骤。输出解码器取决于预期的类型—在示例图中👇排序是用指针建模的。</p><p id="497a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">分开来看，训练和验证轨迹具有16个节点(例如，长度为16的排序列表)，但是测试集在具有64个节点的任务上探测模型的分布外(OOD)能力。有趣的是，普通GNNs和MPNNs非常适合训练数据，但在OOD设置中表现不佳，其中<a class="ae lk" href="https://proceedings.neurips.cc//paper/2020/file/176bf6219855a6eb1f3a30903e34b6fb-Paper.pdf" rel="noopener ugc nofollow" target="_blank">指针图网络</a>显示更好的数字。这是GNNs无法推广到更大的推理图的观察集合的又一个数据点——如何解决这个问题仍然是一个开放的问题🤔。代码<a class="ae lk" href="https://github.com/deepmind/clrs" rel="noopener ugc nofollow" target="_blank">已经可用</a>并且可以用更多的定制算法任务来扩展。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi or"><img src="../Images/452a1a5c2ad716ee130b96c9c6ad0b1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Qc3fqqryPRbinpFl"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">clr中提示的表示。来源:<a class="ae lk" href="https://arxiv.org/pdf/2205.15659.pdf" rel="noopener ugc nofollow" target="_blank">veli kovi等人</a></p></figure><p id="c61e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">➡️从更理论的角度来看，<a class="ae lk" href="https://proceedings.mlr.press/v162/sanmarti-n22a/sanmarti-n22a.pdf" rel="noopener ugc nofollow" target="_blank"> Sanmartín等人</a>通过<a class="ae lk" href="https://www.youtube.com/watch?v=ZzBWh6orSHk" rel="noopener ugc nofollow" target="_blank">代数路径问题</a> (APP)概括了图度量的概念。APP是一个更高级的框架(在范畴理论中有<a class="ae lk" href="https://arxiv.org/abs/2005.06682" rel="noopener ugc nofollow" target="_blank">的一些根源</a>)，它通过半环的概念统一了许多现有的图形度量，如最短路径、<a class="ae lk" href="https://en.wikipedia.org/wiki/Cost_distance_analysis" rel="noopener ugc nofollow" target="_blank">通勤成本距离</a>和极大极小距离，半环是具有特定运算符和属性的集合上的代数结构。例如，最短路径可以描述为一个半环，带有"<em class="me"> min </em>"和"<em class="me"> + </em>"带有中性元素的运算符"<em class="me"> +inf </em>"和"<em class="me"> 0 </em>"。</p><p id="c680" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这里，作者创建了一个单一的应用程序框架<strong class="kq ja">对数范数距离</strong>，允许仅使用两个参数在最短路径、通勤成本和minimax之间进行插值。本质上，您可以改变和混合边权重和周围图结构(其他路径)对最终距离的影响。虽然没有实验，但这是一个坚实的理论贡献——如果你在“吃你的蔬菜”时学习范畴理论🥦，这篇论文非常值得一读——并且肯定会在GNNs中找到应用。👏</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi os"><img src="../Images/3ed0d8be18755cce384d1e460089600e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/0*Rxb2p7BVGojtW4Dj"/></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">对数范数距离。来源:<a class="ae lk" href="https://proceedings.mlr.press/v162/sanmarti-n22a/sanmarti-n22a.pdf" rel="noopener ugc nofollow" target="_blank">桑马丁等人</a></p></figure><p id="1217" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">➡️:最后，我们将在这一类别中添加一部作品<a class="ae lk" href="https://arxiv.org/pdf/2206.08119.pdf" rel="noopener ugc nofollow" target="_blank"><em class="me"/></a>，作者是<strong class="kq ja">罗西等人</strong>，他们将图论与博弈论结合在一起。博弈论在经济学和其他多学科研究中被大量使用，你可能听说过定义非合作博弈解决方案的<a class="ae lk" href="https://en.wikipedia.org/wiki/Nash_equilibrium" rel="noopener ugc nofollow" target="_blank">纳什均衡</a>。在这项工作中，作者考虑了3种游戏类型:<em class="me">线性二次型</em>、<em class="me">线性影响</em>和<em class="me">巴里克-奥诺里奥图形游戏</em>。游戏通常是通过他们的效用函数来定义的，但是在这篇文章中，我们假设我们对游戏的效用函数一无所知。</p><p id="7d7f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">游戏被定义为采取特定行动的n个玩家(图中的节点)(为了简单起见，假设我们可以用某个数字特征来描述它，查看🖼️).下面的插图行动可以影响邻近的玩家——这个任务的框架是根据玩家的行动推断他们的图形。本质上，这是一个图生成任务——给定节点特征X，预测a(归一化)邻接矩阵a，通常一个游戏玩K次，那些是独立游戏，所以编码器模型应该对游戏的排列不变(对每个游戏中节点的排列等变)。作者提出了<strong class="kq ja">金块</strong>🍗编码器-解码器模型，其中变换器编码器通过N个玩家处理K个游戏，产生潜在的表示，并且解码器是潜在的成对玩家特征的哈达玛乘积之和上的MLP，使得解码器对于K个游戏的顺序是置换不变的。</p><p id="f7ae" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">实验表明，该模型在合成数据集和真实数据集上都运行良好。这篇论文绝对是“开阔你的眼界”🔭你可能没想到会在ICML看到这些作品，但后来会发现这是一种引人入胜的阅读，并学到了许多新概念👏。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ot"><img src="../Images/03b033fac71f5bdb67b8e081f03616b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QdpAB57Qa1a0744B"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">来源:<a class="ae lk" href="https://arxiv.org/pdf/2206.08119.pdf" rel="noopener ugc nofollow" target="_blank">罗西等人</a></p></figure></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="bbee" class="mf mg iq bd mh mi nn mk ml mm no mo mp kf np kg mr ki nq kj mt kl nr km mv mw bi translated">知识图推理</h1><p id="534c" class="pw-post-body-paragraph ko kp iq kq b kr mz ka kt ku na kd kw kx ns kz la lb nt ld le lf nu lh li lj ij bi translated">知识图推理一直是GraphML方法的乐园。在今年的《ICML》上，有不少关于这个话题的有趣论文。作为今年的一个趋势，我们看到从嵌入方法(<a class="ae lk" href="https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> TransE </a>、<a class="ae lk" href="http://proceedings.mlr.press/v48/trouillon16.pdf" rel="noopener ugc nofollow" target="_blank"> ComplEx </a>、<a class="ae lk" href="https://arxiv.org/pdf/1902.10197.pdf" rel="noopener ugc nofollow" target="_blank"> RotatE </a>、<a class="ae lk" href="https://ojs.aaai.org/index.php/AAAI/article/view/5701/5557" rel="noopener ugc nofollow" target="_blank"> HAKE </a>)到GNNs和逻辑规则(其实GNNs也是<a class="ae lk" href="https://openreview.net/pdf?id=r1lZ7AEKvB" rel="noopener ugc nofollow" target="_blank">和逻辑规则</a>相关)的一个显著漂移。有四篇论文基于GNNs或逻辑规则，两篇论文扩展了传统的嵌入方法。</p><p id="4ac5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">先从颜等人<a class="ae lk" href="https://proceedings.mlr.press/v162/yan22a/yan22a.pdf" rel="noopener ugc nofollow" target="_blank"/>提出的<a class="ae lk" href="https://github.com/pkuyzy/CBGNN" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">循环基GNN (CBGNN) </strong> </a>说起。作者在逻辑规则和循环之间画了一个有趣的联系。对于任何一个链式逻辑规则，逻辑规则的头和体在知识图中总是形成一个循环。例如，下图的右图显示了<code class="fe ou ov ow ox b">(X, part of Y) ∧ (X, lives in, Z) → (Y, located in Z)</code>的周期。换句话说，逻辑规则的推理可以被视为预测循环的合理性，这归结为学习循环的表示。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oy"><img src="../Images/4f51da4a53be8646f2e707dd1455de2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Vc0atVKc1jvi5cPc"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">蓝色和红色三角形是更大的绿色循环中的循环。来源:<a class="ae lk" href="https://proceedings.mlr.press/v162/yan22a/yan22a.pdf" rel="noopener ugc nofollow" target="_blank">颜等人</a></p></figure><p id="72db" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">一个有趣的观察是，循环在<em class="me">模2 </em>加法和乘法下形成一个线性空间。在上面的例子中，红色❤️和蓝色的总和💙自行车，抵消了他们的共同优势，导致绿色💚循环。因此，我们不需要学习所有圈的表示，而是只需要学习线性空间的<strong class="kq ja">几个圈基</strong>。作者通过挑选与最短路径树有很大重叠的环来生成环基。为了学习循环的表示，他们创建了一个循环图，其中每个节点都是原始图中的一个循环，每个边都表示两个循环之间的重叠。应用GNN来学习循环图中的节点(原始图的循环)表示。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oc"><img src="../Images/5c0cd115364cb0ce0b2a0cd58f2a461a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*G3ePyiK1NED60ZU3"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">CBGNN编码。来源:<a class="ae lk" href="https://proceedings.mlr.press/v162/yan22a/yan22a.pdf" rel="noopener ugc nofollow" target="_blank">严等</a></p></figure><p id="3795" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了将<strong class="kq ja"> CBGNN </strong>应用于归纳关系预测，作者通过用LSTM对循环中的关系进行编码来为每个循环构建归纳输入表示。实验上，CBGNN在FB15k-237/WN18RR/NELL-995的感应版本上实现了SotA结果。</p><p id="87d8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">接下来，<a class="ae lk" href="https://proceedings.mlr.press/v162/das22a/das22a.pdf" rel="noopener ugc nofollow" target="_blank"> Das和godbole等人</a>提出了用于KBQA的基于案例推理(CBR)方法<a class="ae lk" href="https://github.com/rajarshd/CBR-SUBG" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> CBR-SUBG </strong> </a>。核心思想是在解决查询时从训练集中检索相似的查询-答案对。我们知道检索的想法在OpenQA任务中很流行(<a class="ae lk" href="https://arxiv.org/pdf/2106.05346.pdf" rel="noopener ugc nofollow" target="_blank"> EMDR </a>、<a class="ae lk" href="https://arxiv.org/abs/2005.11401" rel="noopener ugc nofollow" target="_blank"> RAG </a>、<a class="ae lk" href="https://arxiv.org/pdf/2010.12688.pdf" rel="noopener ugc nofollow" target="_blank"> KELM </a>、<a class="ae lk" href="https://openreview.net/forum?id=OY1A8ejQgEX" rel="noopener ugc nofollow" target="_blank">提内存LMs </a>)，但这是第一次看到这样的想法在图上被采用。</p><p id="947a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">给定一个自然语言查询，CBR首先基于由预先训练的语言模型编码的查询表示检索相似的k-最近邻(kNN)查询。所有检索到的查询都来自训练集，因此它们的答案是可访问的。然后，我们为每个查询-答案对生成一个局部子图，它被认为是答案的推理模式(尽管不一定是精确的)。当前查询的局部子图(我们无法访问其答案)是通过遵循其kNN查询的子图中的关系路径来生成的。<strong class="kq ja"> CBR-SUBG </strong>然后对每个子图应用GNN，并通过将节点表示与KNN查询中的答案进行比较来预测答案。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oz"><img src="../Images/1edfd257b30bfa88582b87c7ac33fcbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EWoo_QevY7Ohfyt3"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">基于案例的推理直觉。来源:<a class="ae lk" href="https://proceedings.mlr.press/v162/das22a/das22a.pdf" rel="noopener ugc nofollow" target="_blank"> Das和Godbole等人</a></p></figure><p id="1242" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">➡️今年有两种神经符号推理方法。第一个是<a class="ae lk" href="https://github.com/claireaoi/hierarchical-rule-induction" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">层次规则归纳(HRI) </strong> </a>出自<a class="ae lk" href="https://proceedings.mlr.press/v162/glanois22a/glanois22a.pdf" rel="noopener ugc nofollow" target="_blank"> Glanois等人</a>。HRI扩展了以前的一个工作，<a class="ae lk" href="https://arxiv.org/pdf/1809.02193.pdf" rel="noopener ugc nofollow" target="_blank">逻辑规则归纳(LRI) </a>关于归纳逻辑编程。规则归纳的想法是学习一堆规则，并应用它们来推断事实，如<a class="ae lk" href="https://en.wikipedia.org/wiki/Forward_chaining" rel="noopener ugc nofollow" target="_blank">正向链接</a>。</p><p id="dc43" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在<strong class="kq ja"> LRI </strong>和<strong class="kq ja"> HRI </strong>中，每个事实<code class="fe ou ov ow ox b">P(s,o)</code>都由一个嵌入<em class="me"> 𝜃p </em>的谓词和一个赋值<code class="fe ou ov ow ox b">vp</code>(即事实为真的概率)来表示。每个规则<code class="fe ou ov ow ox b">P(X,Y) ← P1(X,Z) ∧ P2(Z,Y)</code>都由其谓词的嵌入来表示。目标是迭代地应用规则来推导新的事实。在每次迭代过程中，通过软统一来匹配规则和事实，软统一衡量两个事实是否满足嵌入空间中的某些规则。一旦选择了一个规则，就会生成一个新的事实并将其添加到事实集中。所有嵌入和软统一操作被端到端地训练，以最大化观察到的事实的可能性。</p><p id="15cd" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">HRI 模型在三个方面比LRI模型有所改进:1)使用分层的先验知识，将每个迭代步骤中使用的规则分开。2)使用gumbel-softmax来归纳用于软统一的稀疏且可解释的解决方案。3)证明HRI能够表达的逻辑规则集。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi pa"><img src="../Images/4b5617a19a46c0dad23c756aa24cbd07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iJAW14fQ4A7H9wHI"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">分层规则归纳。来源:<a class="ae lk" href="https://proceedings.mlr.press/v162/glanois22a/glanois22a.pdf" rel="noopener ugc nofollow" target="_blank"> Glanois等人</a></p></figure><p id="0e21" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">第二篇是<a class="ae lk" href="https://proceedings.mlr.press/v162/zhu22c/zhu22c.pdf" rel="noopener ugc nofollow" target="_blank">朱等人</a>的GNN-QE论文(<strong class="kq ja">免责声明</strong>:本文作者的论文)。GNN-QE用广义神经网络和模糊集解决知识图上复杂的逻辑查询。它兼具神经方法(例如强大的性能)和符号方法(例如可解释性)的优点。因为在GNN-QE有很多有趣的东西，我们很快会有一个单独的博客帖子。敬请期待！🤗</p><p id="ff48" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最后，<a class="ae lk" href="https://proceedings.mlr.press/v162/kamigaito22a/kamigaito22a.pdf" rel="noopener ugc nofollow" target="_blank"> Kamigaito和Hayashi </a>研究了<strong class="kq ja">负抽样</strong>在知识图嵌入中的理论和实证效果。从<a class="ae lk" href="https://arxiv.org/pdf/1902.10197.pdf" rel="noopener ugc nofollow" target="_blank">旋转</a>开始，知识图嵌入方法使用一个归一化的负采样损失，加上一个边际二进制交叉熵损失。这不同于原来<a class="ae lk" href="https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> word2vec </a>中使用的负采样。在本文中，作者证明了基于距离的模型(<a class="ae lk" href="https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf" rel="noopener ugc nofollow" target="_blank">trans</a>，<a class="ae lk" href="https://arxiv.org/pdf/1902.10197.pdf" rel="noopener ugc nofollow" target="_blank"> RotatE </a>)要达到最优解，归一化负采样损失是必要的。边距在基于距离的模型中也起着重要的作用。只有当<em class="me"> 𝛾 ≥ log|V| </em>时，才能达到<strong class="kq ja">最优解</strong>，这与经验结果一致。基于这个结论，现在我们可以确定最佳的保证金没有超参数调整！😄</p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="774c" class="mf mg iq bd mh mi nn mk ml mm no mo mp kf np kg mr ki nq kj mt kl nr km mv mw bi translated">计算生物学:分子连接，蛋白质结合，性质预测</h1><p id="f96c" class="pw-post-body-paragraph ko kp iq kq b kr mz ka kt ku na kd kw kx ns kz la lb nt ld le lf nu lh li lj ij bi translated">总的来说，comp bio在ICML表现很好。在这里，我们将看看<strong class="kq ja">分子连接</strong>、<strong class="kq ja">蛋白质结合</strong>、构象异构体生成和分子性质预测的新方法。</p><p id="f388" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">分子连接</strong>是设计<a class="ae lk" href="https://en.wikipedia.org/wiki/Proteolysis_targeting_chimera" rel="noopener ugc nofollow" target="_blank">蛋白水解靶向嵌合体(PROTAC) </a>药物的关键部分。对我们来说，仅仅是GNN的研究者🤓在没有生物学背景的情况下，这意味着给定两个分子，我们想要生成一个有效的<em class="me">接头</em>分子，它将两个<em class="me">片段</em>分子连接在一个分子中，同时保留原始片段分子的所有属性(查看下面的插图以获得一个很好的示例)</p><p id="96aa" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了生成分子链，<a class="ae lk" href="https://arxiv.org/pdf/2205.07309.pdf" rel="noopener ugc nofollow" target="_blank">黄等人</a>创造了<strong class="kq ja"> 3DLinker </strong>，这是一个e(3)-等变生成模型(VAE)，它以<strong class="kq ja">绝对</strong>坐标顺序生成原子(和连接键)。通常，等变模型会生成相对坐标或相对距离矩阵，但在这里，作者旨在生成绝对<em class="me"> (x，y，z) </em>坐标。为了允许模型从等变(到坐标)和不变(到节点特征)变换中生成精确的坐标，作者应用了一个巧妙的<a class="ae lk" href="https://arxiv.org/pdf/2104.12229.pdf" rel="noopener ugc nofollow" target="_blank">向量神经元</a>的想法，它本质上是一个类似ReLU的非线性，通过巧妙的正交投影技巧来保持特征等变。</p><p id="ad27" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">富含<strong class="kq ja">向量神经元</strong>的e(3)-等变编码器对特征和坐标进行编码，而解码器以3个步骤顺序地生成链接(也在下面示出):1️⃣预测链接将被附加到的锚节点；2️⃣预测链接器节点节点类型；3️⃣预测边缘及其绝对坐标；4️⃣重复，直到我们在第二个片段中到达停止节点。<strong class="kq ja"> 3DLinker </strong>是(到目前为止)第一个生成具有<strong class="kq ja">精确3D坐标</strong>的连接分子并预测片段分子中锚点的等变模型——以前的模型在生成之前需要已知锚点——并显示最佳实验结果。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oc"><img src="../Images/a55e06369b530f9952a98ce480d6b6f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5sGzmdVs6oG1JtJ1"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">3d链接器直觉。来源:<a class="ae lk" href="https://arxiv.org/pdf/2205.07309.pdf" rel="noopener ugc nofollow" target="_blank">黄等</a></p></figure><p id="4313" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">➡️ <strong class="kq ja">蛋白质-配体结合</strong>是另一项至关重要的药物发现任务——预测一个小分子可能在哪里附着到一个更大蛋白质的某个区域。先是，<a class="ae lk" href="https://arxiv.org/pdf/2202.05146.pdf" rel="noopener ugc nofollow" target="_blank">史塔克，加内亚等人</a>创作<a class="ae lk" href="https://github.com/HannesStark/EquiBind" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">对等</strong> </a> (ICML聚焦💡)以蛋白质和配体图的随机RDKit构象作为输入，并输出结合相互作用的精确3D位置。EquiBind已经在<a class="ae lk" href="https://news.mit.edu/2022/ai-model-finds-potentially-life-saving-drug-molecules-thousand-times-faster-0712" rel="noopener ugc nofollow" target="_blank">麻省理工学院新闻</a>和<a class="ae lk" href="https://www.youtube.com/watch?v=706KjyR-wyQ&amp;list=PLoVkjhDgBOt11Q3wu8lr6fwWHn5Vh3cHJ&amp;index=14" rel="noopener ugc nofollow" target="_blank">阅读小组</a>中获得了非常热烈的欢迎和宣传，因此我们鼓励您详细了解技术细节！<strong class="kq ja"> EquiBind </strong>比商业软件快几个数量级，同时保持较高的预测精度。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oc"><img src="../Images/61e9c89ead2efd5f4ea92b413552be56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2kndRG2X4H-MdRWm"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">EquiBind。资料来源:<a class="ae lk" href="https://arxiv.org/pdf/2202.05146.pdf" rel="noopener ugc nofollow" target="_blank">斯特尔克、加内亚等人</a></p></figure><p id="ac5d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果结合分子是未知的，并且我们想要生成这样的分子，<a class="ae lk" href="https://proceedings.mlr.press/v162/liu22m/liu22m.pdf" rel="noopener ugc nofollow" target="_blank">刘等人</a>创建了<a class="ae lk" href="https://github.com/divelab/GraphBP" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> GraphBP </strong> </a>，这是一种自回归分子生成方法，其将目标蛋白质位点(表示为初始上下文)作为输入。使用任何3D GNN对上下文进行编码(此处为<a class="ae lk" href="https://proceedings.neurips.cc/paper/2017/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> SchNet </a>，GraphBP生成原子类型和球坐标，直到不再有接触原子可用或达到所需的原子数量。一旦原子生成，作者就求助于<a class="ae lk" href="https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-3-33" rel="noopener ugc nofollow" target="_blank">开放巴别塔</a>来创造化学键。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oc"><img src="../Images/e48d49cf419dbda88172335c4b49aae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*y7Or_o3IBX2kkw4m"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">用GraphBP生成结合分子。来源:<a class="ae lk" href="https://proceedings.mlr.press/v162/liu22m/liu22m.pdf" rel="noopener ugc nofollow" target="_blank">刘等</a></p></figure><p id="01f3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在<strong class="kq ja">分子性质预测中，</strong> <a class="ae lk" href="https://proceedings.mlr.press/v162/yu22a/yu22a.pdf" rel="noopener ugc nofollow" target="_blank">于和高</a>提出了一个简单而惊人强大的想法，用一包基序来丰富分子表征。也就是说，他们首先在训练数据集中挖掘出一个模体词汇表，并根据<a class="ae lk" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> TF-IDF </a>分数对它们进行排序(你好，来自NLP😉).然后，每个分子可以被表示为一个模体包(多热编码),并且整个分子数据集被转换为一个具有关系“模体-分子”(如果任何分子包含该模体)和“模体-模体”(如果任何两个模体在任何分子中共享一条边)的异质图。边缘特征是之前挖掘的那些TF-IDF得分。</p><p id="fafc" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">分子的最终嵌入是通过分子上的任何香草GNN和来自基序图的采样子图上的另一个异质GNN的串联获得的。这样一个<a class="ae lk" href="https://github.com/ZhaoningYu1996/HM-GNN" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">异构基元GNN (HM-GNN) </strong> </a>始终优于<a class="ae lk" href="https://arxiv.org/abs/2006.09252" rel="noopener ugc nofollow" target="_blank">图子结构网络(GSN) </a>，这是最早提出在社交网络中计算三角形和在分子中计算k-循环的GNN架构之一，甚至优于<a class="ae lk" href="https://arxiv.org/pdf/2106.12575.pdf" rel="noopener ugc nofollow" target="_blank">细胞同构网络(CIN) </a>，这是一个顶级的高阶消息传递模型。HM-GNNs可以作为高阶GNNs领域的后续研究的简单有力的基线💪。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oc"><img src="../Images/a45ef1c042efd6bbddc5a7d3c4cd7b2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nu5xOxemZSCuJZmp"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">在HM-GNN建立主题词汇。来源:<a class="ae lk" href="https://proceedings.mlr.press/v162/yu22a/yu22a.pdf" rel="noopener ugc nofollow" target="_blank">于、高</a></p></figure><p id="9462" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">➡️最后，<a class="ae lk" href="https://proceedings.mlr.press/v162/stark22a/stark22a.pdf" rel="noopener ugc nofollow" target="_blank">strk等人</a>的一项工作展示了使用<a class="ae lk" href="https://github.com/HannesStark/3DInfomax" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> 3D Infomax </strong> </a>方法对2D分子图及其3D构象图进行预训练的好处。<strong class="kq ja"> 3D Infomax </strong>的想法是最大化2D和3D表示之间的互信息，使得在2D图上的推断时间，当没有给出3D结构时，模型仍然可以受益于3D结构的隐含知识。</p><p id="ff7a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为此，2D分子用<a class="ae lk" href="https://arxiv.org/abs/2004.05718" rel="noopener ugc nofollow" target="_blank">主邻域聚集(PNA) </a>网编码，3D构象异构体用<a class="ae lk" href="https://openreview.net/forum?id=givsRXsOt9r" rel="noopener ugc nofollow" target="_blank">球形信息传递(SMP) </a>网编码，我们取它们表示的余弦相似性，并通过对比损失使一个分子与其真实3D构象异构体的相似性最大化，并将其他样品视为阴性。有了预训练的2D和3D网络，我们可以在下游任务中微调2D网络的权重——在这种情况下是QM9属性预测——结果明确显示预训练有效。顺便说一句，如果你对预训练进一步感兴趣，可以看看ICLR 2022上发布的<a class="ae lk" href="https://openreview.net/forum?id=xQUe1pOKPam" rel="noopener ugc nofollow" target="_blank"> GraphMVP </a>作为另一种2D/3D预训练方法。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oc"><img src="../Images/bb1653778c1df90e556865c66786dbc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3UpGqTxkVbAGVQGm"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">在3D Informax中，我们首先预训练2D和3D网络，并在推理时使用经过训练的2D网络。资料来源:<a class="ae lk" href="https://proceedings.mlr.press/v162/stark22a/stark22a.pdf" rel="noopener ugc nofollow" target="_blank">strk等人</a></p></figure></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="f4b7" class="mf mg iq bd mh mi nn mk ml mm no mo mp kf np kg mr ki nq kj mt kl nr km mv mw bi translated">酷图应用</h1><p id="6358" class="pw-post-body-paragraph ko kp iq kq b kr mz ka kt ku na kd kw kx ns kz la lb nt ld le lf nu lh li lj ij bi translated">GNNs极大地推动了物理模拟和分子动力学的发展。物理模拟的标准设置是一个粒子系统，其中节点特征是几个最近的速度，边缘特征是相对位移，任务是预测粒子在下一个时间步移动到哪里。</p><p id="656d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">⚛️今年，<a class="ae lk" href="https://proceedings.mlr.press/v162/rubanova22a/rubanova22a.pdf" rel="noopener ugc nofollow" target="_blank"> Rubanova，Sanchez-Gonzalez等人</a>通过在<strong class="kq ja"> C-GNS </strong> <strong class="kq ja">(基于约束的图网络模拟器)</strong>中加入明确的标量约束，进一步改进了物理模拟。从概念上讲，MPNN编码器的输出通过解算器进一步细化，该解算器最小化一些学习到的(或在推理时指定的)约束。求解器本身是一个可微分函数(在这种情况下是5次迭代梯度下降),因此我们也可以通过求解器进行反向投影。C-GNS与<a class="ae lk" href="http://implicit-layers-tutorial.org/" rel="noopener ugc nofollow" target="_blank">深层隐含层</a>有着内在的联系，这些深层隐含层的可见性越来越强，包括<a class="ae lk" href="https://fabianfuchsml.github.io/equilibriumaggregation/" rel="noopener ugc nofollow" target="_blank">GNN应用</a>。</p><p id="6473" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">物理模拟作品通常是花哨的模拟可视化的来源——查看带有视频演示的<a class="ae lk" href="https://sites.google.com/view/constraint-based-simulator" rel="noopener ugc nofollow" target="_blank">网站</a>！</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi pb"><img src="../Images/2b334146032df6104dd930971bdfa75f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0zUhtBrLrdGLJS7z"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">基于约束的图形网络模拟器。来源:<a class="ae lk" href="https://proceedings.mlr.press/v162/rubanova22a/rubanova22a.pdf" rel="noopener ugc nofollow" target="_blank">鲁巴诺瓦、桑切斯-冈萨雷斯等人</a></p></figure><p id="893a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">您可能想看看其他一些很酷的应用程序:</p><ul class=""><li id="b2ca" class="mx my iq kq b kr ks ku kv kx ok lb ol lf om lj on nf ng nh bi translated"><strong class="kq ja">交通预测</strong> : <a class="ae lk" href="https://proceedings.mlr.press/v162/lan22a/lan22a.pdf" rel="noopener ugc nofollow" target="_blank">兰、马等</a>创建了<a class="ae lk" href="https://github.com/SYLan2019/DSTAGNN" rel="noopener ugc nofollow" target="_blank"><strong class="kq ja">【DSTA-GNN】</strong></a>(动态时空感知图神经网络)用于交通预测🚥在繁忙的加州道路的真实世界数据集上进行评估——去年，在谷歌和DeepMind对改进谷歌地图ETA <a class="ae lk" rel="noopener" target="_blank" href="/graph-ml-in-2022-where-are-we-now-f7f8242599e0#2ddd">进行大量工作后，用图表预测交通得到了推动，我们在2021年的结果</a>中报道了这些工作。</li><li id="40e3" class="mx my iq kq b kr ni ku nj kx nk lb nl lf nm lj on nf ng nh bi translated"><strong class="kq ja">神经网络剪枝</strong> : <a class="ae lk" href="https://proceedings.mlr.press/v162/yu22e/yu22e.pdf" rel="noopener ugc nofollow" target="_blank">于等</a>设计<a class="ae lk" href="https://github.com/yusx-swapp/GNN-RL-Model-Compression" rel="noopener ugc nofollow" target="_blank"><strong class="kq ja">【GNN-RL】</strong></a>在给定期望的FLOPs缩减率的情况下，迭代地剪枝深度神经网络的权重。为此，作者将神经网络的计算图视为块的分层图，并将其发送到分层GNN(通过中间可学习池来粗粒度化NN架构)。编码的表示被发送到RL代理，该代理决定删除哪个块。</li><li id="9b31" class="mx my iq kq b kr ni ku nj kx nk lb nl lf nm lj on nf ng nh bi translated"><strong class="kq ja">排名</strong> : <a class="ae lk" href="https://arxiv.org/pdf/2202.00211.pdf" rel="noopener ugc nofollow" target="_blank">何等人</a>处理一个有趣的任务——给定一个两两互动的矩阵，例如，在足球联赛中的球队之间，其中<em class="me"> Aij &gt; 0 </em>表示球队<em class="me"> i </em>比球队<em class="me"> j </em>得分更高，找出得分最高的节点(球队)的最终排名。换句话说，我们希望在看到所有比赛的配对结果后，预测谁是联赛的冠军。作者提出<a class="ae lk" href="https://github.com/SherylHYX/GNNRank" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> GNNRank </strong> </a>将成对结果表示为有向图，并应用有向GNN来获得潜在节点状态，并计算图拉普拉斯的<a class="ae lk" href="https://en.wikipedia.org/wiki/Algebraic_connectivity" rel="noopener ugc nofollow" target="_blank">费德勒向量</a>。然后，他们将该任务设计为一个约束优化问题，具有<em class="me">近似</em>梯度步骤，因为我们无法通过费德勒向量的计算轻松地反向推进。</li></ul></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><p id="0409" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这就是ICML 2022的最终目标！😅</p><p id="f804" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">期待看到NeurIPS 2022论文以及全新<a class="ae lk" href="https://logconference.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">图学(LoG) </strong> </a> <strong class="kq ja"> </strong>会议的投稿！</p></div></div>    
</body>
</html>