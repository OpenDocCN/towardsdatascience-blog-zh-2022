<html>
<head>
<title>A Simple Guide to Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降的简单指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-descent-f7458de38365#2022-02-02">https://towardsdatascience.com/gradient-descent-f7458de38365#2022-02-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="eca9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用 Python 演示了多元线性回归示例中的算法</h2></div><p id="9891" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当研究一本机器学习书籍时，很有可能在第一页就遇到臭名昭著的梯度下降。虽然这种算法背后的想法需要一点数学直觉，但梯度下降的应用程序的有用性和通用性令人难以置信。这篇文章为你提供了所有你需要了解的实用知识。</p><p id="c613" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">无论你是否深入研究<strong class="kk iu">深度学习</strong>(反向传播)或者只是对如何导出<strong class="kk iu">线性回归</strong>(普通最小二乘法)中的系数感兴趣，梯度下降(GD)都是这些方法学不可或缺的一部分，不应该对用户保持黑盒模型。本解释旨在<strong class="kk iu">将几个简单的数学表达式与相关代码</strong>联系起来。</p><blockquote class="le lf lg"><p id="2f4e" class="ki kj lh kk b kl km ju kn ko kp jx kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated"><strong class="kk iu">本文介绍了一个简单的四步算法来实现梯度下降。你需要的一切！</strong></p></blockquote><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/3abaadec89848abcc72ff6e7c651eefe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Ej7RPQUh0mG6ngc33fc6w.jpeg"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">渐变的日落——作者图片</p></figure><h2 id="2036" class="mb mc it bd md me mf dn mg mh mi dp mj kr mk ml mm kv mn mo mp kz mq mr ms mt bi translated">为什么探索梯度下降？</h2><p id="58d2" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">当我们将算法直接放在线性回归的上下文中时，GD 可能是最容易理解的——因此我将使用回归作为主要参考。</p><p id="d941" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">目标是找到最小化直线/超平面到所有数据点的距离的系数。这可以通过矩阵运算(和求逆)来实现，但是计算量非常大。由于这个原因<strong class="kk iu">使用 GD 是一个很好的方法来获得解决方案</strong>。网上有各种指南展示如何一步一步地应用梯度下降(即更新一个系数&amp;迭代，更新下一个系数&amp;迭代，..)，然而，这使得很难掌握通过矩阵运算一次更新几个权重/系数的过程。这是至关重要的，因为更新权重/系数矩阵是深度学习文献中经常使用和讨论的主题。</p><p id="8891" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了使 GD 的整体计算概念更加具体，我将详细说明 GD 如何实际应用于导出矩阵符号<strong class="kk iu">中的线性回归系数</strong>。</p><blockquote class="le lf lg"><p id="cc63" class="ki kj lh kk b kl km ju kn ko kp jx kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated">我将举例说明几个简单的数学表达式，如果你觉得不太舒服，就继续进行，我相信代码部分最终会清除烟雾。下面的等式解释了“为什么”，而代码部分是“如何”的问题。</p></blockquote><h2 id="dc33" class="mb mc it bd md me mf dn mg mh mi dp mj kr mk ml mm kv mn mo mp kz mq mr ms mt bi translated">什么是梯度下降？</h2><p id="97b5" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">你可能还记得在学校的时候，找到一个函数的最小值/最大值直接导致了<strong class="kk iu">导出函数</strong>的任务。如果我们使用这个导数，一点一点地降低函数值，我们最终会收敛到最小值。关于 GD，我们试图最小化一个损失函数，这完全符合我们的学校数学工具包。</p><p id="81d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">换句话说，GD 是一个一阶优化算法，它允许我们找到一个函数的最小值(希望是全局的)。下面的视频展示了我们如何从一个随机点(红色区域)开始，迭代下降到这个函数的最小值(蓝色区域)。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mz na l"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">Gif 由<a class="ae nb" href="https://gfycat.com/angryinconsequentialdiplodocus" rel="noopener ugc nofollow" target="_blank">angryinconsequentialdiplodocus</a></p></figure><p id="2b7d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">起初看起来有点挑战性的是，几个系数需要几个<strong class="kk iu">偏导数。</strong>然后，每个导出值被存储在一个向量中，即梯度。在论文中，你可能会遇到“nabla”的符号，这是一个倒置的三角形:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/5020ab70b9ac989420e8e379a22ebae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:174/format:webp/1*XEXHyHDDA7e6ansiUA8MKA.png"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">最小化损失函数[1]</p></figure><p id="04dd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但在我们实际做 GD(这有助于我们最小化损失函数，通常表示为"<em class="lh"> J </em>或"<em class="lh"> L </em>")之前，让我们首先确定我们在看什么损失函数。</p><h2 id="245b" class="mb mc it bd md me mf dn mg mh mi dp mj kr mk ml mm kv mn mo mp kz mq mr ms mt bi translated">基本思想——线性回归</h2><p id="72a0" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">首先，理解我们想要解决的的<strong class="kk iu">潜在问题是很重要的——在这种情况下，这是一个回归问题，我们需要最小化普通最小二乘方程。真实值<em class="lh"> y </em>的<strong class="kk iu">估计值(" y hat") </strong>表示如下:</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/720751c31bdb1ec39bde7888eaf8aa96.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/format:webp/1*7P1fdgQRPron__Cp5_shlA.png"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">给定一个线性模型对 Y 的估计[2]</p></figure><p id="c2ce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了使用 GD，一个函数必须在所有点上都是可微的，并且是<strong class="kk iu"> <em class="lh">凸的</em> </strong> —这对于普通的最小二乘(OLS)问题是已知的，其中我们计算<strong class="kk iu">估计值和真实的<em class="lh"> y </em>值</strong>之间的偏差平方和——这是我们的<strong class="kk iu">损失函数</strong>:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/def0a3d927031939388ad0db828c02b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:232/format:webp/1*gZVCH56rSe2lE15WcDC39A.png"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">最小化 RSS —剩余平方和(凸函数)[3]</p></figure><p id="2838" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果残差平方和最小化，我们将获得一条直线，其特征是到所有数据点的距离最短，如下图所示:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/755b386580b1e937169aceb05215499a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*M0c_EK5cH6MJPab26miIeg.png"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">真实易值(蓝点)与回归线(y _ hats)-作者图片</p></figure><p id="4335" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">RSS 简单地表示真实 y 值和 X 值之间的平方差之和乘以它们的系数(β)。误差项被视为<strong class="kk iu">随机噪声</strong>(高斯分布[0，1])，因此，如果数据点不完全位于直线或超平面上，该等式将产生非零值——这是一般情况。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/9f4f64a4ac3773847c571c4d51620168.png" data-original-src="https://miro.medium.com/v2/resize:fit:286/format:webp/1*AQsxyDuBISFIW4HGYsYbRg.png"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">贝塔系数的估计值[4]</p></figure><p id="b752" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了估计β，我们需要对 X 矩阵(值<em class="lh"> xij) </em>求逆，这将直接引导我们估计系数(“hat”)。搞定了，那么<strong class="kk iu">又何必纠结于梯度下降呢？</strong></p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h2 id="6ab5" class="mb mc it bd md me mf dn mg mh mi dp mj kr mk ml mm kv mn mo mp kz mq mr ms mt bi translated">开始梯度下降旅程</h2><p id="136b" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">对一个矩阵求逆可能是<strong class="kk iu">计算上的挑战(即昂贵)，这是 GD 算法真正闪光的地方</strong>。作为矩阵求逆的替代，我们可以简单地取偏导数，使用这些值和学习率(α)来更新当前权重(β)。然后，我们重复这个步骤，以便<strong class="kk iu">迭代地逼近函数</strong>的最小值(再次查看上面的 GIF)<strong class="kk iu">。</strong></p><h2 id="d3af" class="mb mc it bd md me mf dn mg mh mi dp mj kr mk ml mm kv mn mo mp kz mq mr ms mt bi translated"><strong class="ak">最小化损失函数</strong></h2><p id="eb54" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">我们使用梯度来调整我们的初始权重/系数，这最终使我们达到函数的最小值(假设函数是<strong class="kk iu">凸</strong>)。定义我们多快接近这个最小值的<strong class="kk iu">“速度”参数</strong>被称为<strong class="kk iu">学习率</strong>。</p><p id="2eae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lh">我在这里不展开关于学习率的想法，但如果你打算这么做，也可以考虑考察一下“</em><strong class="kk iu"><em class="lh"/></strong><em class="lh">”在这种背景下，作为动量对于克服</em><strong class="kk iu"><em class="lh"/></strong><em class="lh">局部极小值是非常有用的。</em></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi no"><img src="../Images/aa3fe229f6ffc46fb974c57558130508.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*_Ie7ex8quX6gMuZ2aT0RBw.png"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">最小化损失函数—作者图片</p></figure><p id="6513" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如前所述，我们可以使用 GD 来代替昂贵的矩阵求逆。首先，我们需要我们的函数关于 beta 的偏导数(在我们简单的回归例子中，<em class="lh"> b0 </em>和<em class="lh"> b1 </em>)。</p><p id="0852" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们推导损失函数。偏导数的矢量由下列截距和斜率系数的单独计算组成(分别为<em class="lh"> b0 和 b1 </em>)。我们最小化所有贝塔系数(在多元线性回归的情况下，可能有<em class="lh"> p </em>系数):</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi np"><img src="../Images/8b54dd8127c3255db0881fa1566c461a.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*q07EHVCEr47moBG3XKutvQ.png"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">最小化所有 betas 上的 L[5]</p></figure><p id="2326" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将这两个贝塔系数分解开来，我们就有了两个等式，稍后可以轻松实现:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/53d37878e9542eb83931ab78efad2353.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*2aYULSOOfaCW81oNGQjlkA.png"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">注意区别:(1)！= (x) [6][7]</p></figure><p id="d8e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑到定义的 <strong class="kk iu"> <em class="lh">学习率，我们进一步使用导出值通过减去<em class="lh">导出值来<strong class="kk iu">减少初始权重</strong>/系数。</em>T47】</em></strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/51cfb8a8b37de372694aa8c94edce31e.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*d3LfWcZx9iPsWMnptZYofg.png"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">使用导数的广义视图迭代更新贝塔系数[8]</p></figure><p id="e9b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面概括的符号表明我们<strong class="kk iu">需要使用 1 的堆栈向量作为<em class="lh">X</em>T3】的第一列。这是必要的，以确保在将<em class="lh"> X </em>矩阵乘以系数矩阵时添加截距(<em class="lh"> b0 </em>是截距)。<em class="lh">可以跳转到下面的第 15 行和第 16 行代码来观察堆叠。</em></strong></p><p id="43af" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我发现下面的链接很好地解释了如何通过迭代更新两个值<em class="lh"> b0 </em>和<em class="lh"> b1 </em>来计算和更新权重(回归意义上的系数)——一次一个。同样，为了理解我们如何以及何时更新权重，链接给出了一个很好的解释。</p><p id="2c3b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了更加通用，同时也允许<strong class="kk iu">同时处理多个权重</strong>，我将用矩阵表示法解决这个问题，并将结果与下面的<strong class="kk iu"> Scikit-Learn 的实现进行比较。</strong></p><div class="ns nt gp gr nu nv"><a href="https://machinelearningmastery.com/linear-regression-tutorial-using-gradient-descent-for-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd iu gy z fp oa fr fs ob fu fw is bi translated">使用梯度下降进行机器学习的线性回归教程-机器学习掌握</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">随机梯度下降是机器学习中一种重要且广泛使用的算法。在这篇文章中，你将…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">machinelearningmastery.com</p></div></div><div class="oe l"><div class="of l og oh oi oe oj lv nv"/></div></div></a></div><h2 id="504a" class="mb mc it bd md me mf dn mg mh mi dp mj kr mk ml mm kv mn mo mp kz mq mr ms mt bi translated">从头开始实现梯度下降</h2><p id="50ad" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">以下步骤概述了如何继续这个 GD 回归示例:</p><pre class="lm ln lo lp gt ok ol om on aw oo bi"><span id="5a30" class="mb mc it ol b gy op oq l or os">1. Setting up the data<br/>2. Defining the learning rate (alpha)<br/>3. Defining the initial values for b0 and b1 (initialization)<br/>4. Start iterating # for i in 1000<br/><strong class="ol iu">4.1 Taking partial derivatives<br/>4.1.1 Calculate the error for the intercept (b0)<br/>4.1.2 Calculate the error for the coefficient(s) (b1)<br/>4.2 Update the parameters</strong></span><span id="21e0" class="mb mc it ol b gy ot oq l or os">Done.</span></pre><p id="0e54" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lh"> X </em>和<em class="lh"> y </em>变量定义如下:</p><pre class="lm ln lo lp gt ok ol om on aw oo bi"><span id="7298" class="mb mc it ol b gy op oq l or os">X = array([[1., 1.],<br/>           [1., 2.],<br/>           [1., 4.],<br/>           [1., 3.],<br/>           [1., 5.]])</span><span id="4826" class="mb mc it ol b gy ot oq l or os">y = array([1, 3, 3, 2, 5])</span><span id="c021" class="mb mc it ol b gy ot oq l or os">b = array([0., 0.]) # Initialized b =&gt; coefficients/weights</span></pre><p id="953a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据点可以显示为简单的散点图。回归的目标是在点之间画一条线，使到真实点的距离最小。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/dfdbac2d716859746e189fb0450291c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*bX7gBmYIyayI7LFKv0k51Q.png"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">带有我们目标回归线的散点图—图片由作者提供</p></figure><p id="047e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这正是我们所说的 OLS(普通最小二乘)问题。</p><p id="ffb6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">显然，橙色线设法非常接近所有数据点。此外，这条线有一个截距。</p><p id="582f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了再次具体说明<strong class="kk iu">偏导数</strong>，我举例说明了<strong class="kk iu">的两个代码行</strong>。相关的数学表达式分别是 6、7 或 8，可以在上述文本中找到:</p><pre class="lm ln lo lp gt ok ol om on aw oo bi"><span id="a64a" class="mb mc it ol b gy op oq l or os"># d/db0: (b0+b1.dot(x) — y ) * (1) <br/># d/db1: (b0+b1.dot(x) — y ) * (x) </span></pre><h2 id="423f" class="mb mc it bd md me mf dn mg mh mi dp mj kr mk ml mm kv mn mo mp kz mq mr ms mt bi translated">GD 实施</h2><p id="dafd" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">下面的代码初始化变量并开始迭代。前两行计算我们存储为梯度的值。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="ov na l"/></div></figure><p id="5cf3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种迭代算法为我们提供了截距为 0.39996588 和系数为 0.80000945 的结果，将其与从<strong class="kk iu"> <em class="lh"> sklearn </em> </strong>实现中获得的 0.399999 进行比较，结果似乎非常匹配。</p><h2 id="9214" class="mb mc it bd md me mf dn mg mh mi dp mj kr mk ml mm kv mn mo mp kz mq mr ms mt bi translated">Scikit Learn 的实现</h2><p id="4091" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">这段代码使用线性回归的 sklearn 实现来验证前面获得的结果:</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="ov na l"/></div></figure></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h2 id="f926" class="mb mc it bd md me mf dn mg mh mi dp mj kr mk ml mm kv mn mo mp kz mq mr ms mt bi translated">概述</h2><p id="fd92" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated"><em class="lh">我总是通过在例子中直接应用学习概念来接受它们，这在机器学习领域尤其如此</em>。对梯度下降的简要介绍旨在提供一个易于理解和实现的算法，让你找到凸函数的最小值。通过使用这种迭代算法来连续更新权重/系数，GD 允许我们克服诸如矩阵求逆(如线性回归示例中)之类的昂贵过程的计算工作。</p><p id="c60e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">GD 是几乎任何机器学习和深度学习程序的一个组成部分，这就是为什么它经常在相关大学课程中作为“先决条件”教授的原因。</p><p id="f49d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你觉得这篇文章有帮助，我会很感激“关注”🫀，直到那时:</p><p id="4c42" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lh">{照顾好自己，如果可以的话，也照顾好别人} </em></p><p id="bd1f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lh"> —借鉴史蒂芬·都伯纳</em></p></div></div>    
</body>
</html>