<html>
<head>
<title>How Computers See Depth: Recent Advances in Deep Learning-Based Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">计算机如何看深度:基于深度学习的方法的最新进展</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dl-for-depth-estimation-p2-7cb2c9ff325d#2022-02-10">https://towardsdatascience.com/dl-for-depth-estimation-p2-7cb2c9ff325d#2022-02-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1e47" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">第2部分:基于图像的立体视觉</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b7162e00ee0e821d50822ce33b460009.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PfmJhUY--XO2qjE8xV4PEA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">立体</strong> <strong class="bd ky">视觉</strong> <strong class="bd ky">跟</strong> <strong class="bd ky">深度</strong> <strong class="bd ky">学习。</strong> <em class="kz">输入</em>是<em class="kz">一个立体图像对(即从左右摄像机捕捉的图像)；输出是左图和两张照片中所有可见像素的深度图。</em>因此，现代端到端解决方案学会将两张RGB照片映射到深度图。目标是使用监督来最小化预测与实际之间的距离(或最大化相似性)。立体对(上面最左侧)是深度网络(中间)的输入，该深度网络将图像转换为相应的深度预测(右侧)。<em class="kz">注意，物体离相机越近，视差越显著(即，深度越小)。输出是显示在右侧的密集视差图，暖色代表较大的深度值(和较小的视差)。作者创造了可视化。</em></p></figure><p id="a707" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi lw translated">我们对深度的感知对于创造我们周围的3D世界至关重要。这种知识已经流行了几个世纪，其中一个人就是列奥纳多·达·芬奇。他利用自己的专业知识帮助自己创作了一些艺术作品，这些作品将会名闻遐迩，如《最后的晚餐》或《萨尔瓦托勒·希泽拉》。从技术上讲，对双筒望远镜的理解可以追溯到公元280年，当时欧几里德意识到我们的深度知觉是人类用两只眼睛聚焦于同一物体。尽管如此，今天，立体视觉仍然是一个非常有趣的问题。随着我对话题的熟悉，一路上做的笔记，现在正在转化为一系列的博客，供其他人参考。</p><p id="7b52" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">立体视觉是其他信息和多模态知识可以获得的基本任务。因此，立体视觉技术在现实世界的应用中有着广泛的实际用途，如机器人、自动驾驶汽车和大多数依赖于感知深度作为先验知识的任务！不足为奇的是，在过去的几十年里，这个话题已经被许多人所激发。有了现代的监督深度学习，问题的高度复杂性与高度复杂的网络更好地匹配。现在的需求很大，标记数据以适应这些受监督的深层网络的容量，这在获取和缓解条件以满足对数据的大量需求方面仍然是一个挑战。总之，我们正处于或接近深度感知可以在实践中自信地部署的边缘。但是，由于需要如此多的数据来训练深度网络，部署的模型需要大量的数据来学习将左/右对转换为其视差图的映射函数，如上图所示。</p><p id="a89b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在本系列的第2部分中，我们将介绍几种基于图像的立体视觉监控方法。提供了动机、策略、结果(如在原始论文中发表的)、摘要、文档链接和源代码。这里的目的不是提供传统意义上的文献综述，而是充当监督立体视觉的深层方法的百科全书。该系列的未来部分将涵盖视频数据、自我监督和非监督、多任务学习，以及数据集和基准的更多细节和分析(即，比上一篇文章<a class="ae mg" rel="noopener" target="_blank" href="/depth-from-disparity-via-deep-learning-part-0-458827141b23"> <em class="mf">第1部分</em> </a>提供的更深入)。现在让我们回顾一下从2015年到2020年的几个SOA方法。未来的博客将涵盖最新的方法。然而，这里介绍的大部分材料都是初步的。</p><p id="c79f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">享受:)</p><h1 id="632d" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">目录</h1><p id="8a7e" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated"><a class="ae mg" href="#80f6" rel="noopener ugc nofollow">简介</a> <br/> <a class="ae mg" href="#c45f" rel="noopener ugc nofollow">背景信息</a> <br/> ∘ <a class="ae mg" href="#dcb0" rel="noopener ugc nofollow">立体视觉</a> <br/> ∘ <a class="ae mg" href="#8c29" rel="noopener ugc nofollow">深度监督学习</a> <br/> ∘ <a class="ae mg" href="#1fe4" rel="noopener ugc nofollow">深度监督学习用于立体视觉</a><br/><a class="ae mg" href="#ce08" rel="noopener ugc nofollow">MC-CNN</a><br/>∘<a class="ae mg" href="#6ff4" rel="noopener ugc nofollow">简介</a> <br/> ∘ <a class="ae mg" href="#4c42" rel="noopener ugc nofollow">方法</a> <br/> ∘ <a class="ae mg" href="#fdf8" rel="noopener ugc nofollow">结果</a> <br/> ∘ <a class="ae mg" href="#6e7a" rel="noopener ugc nofollow">总之【T28 </a> <br/> ∘ <a class="ae mg" href="#e1ee" rel="noopener ugc nofollow">总结</a> <br/> ∘ <a class="ae mg" href="#805e" rel="noopener ugc nofollow">论文与代码</a><br/><a class="ae mg" href="#89f5" rel="noopener ugc nofollow">iresnet</a><br/>∘<a class="ae mg" href="#65c0" rel="noopener ugc nofollow">动机</a> <br/> ∘ <a class="ae mg" href="#99c8" rel="noopener ugc nofollow">方法</a> <br/> ∘ <a class="ae mg" href="#8bf2" rel="noopener ugc nofollow">结果</a> <br/> ∘ <a class="ae mg" href="#d48a" rel="noopener ugc nofollow">总结</a> <br/> ∘ <a class="ae mg" href="#121c" rel="noopener ugc nofollow">论文与代码</a><br/><br/> <a class="ae mg" href="#9c59" rel="noopener ugc nofollow">金字塔立体匹配网络(PSMN) </a> <br/> ∘ <a class="ae mg" href="#6ede" rel="noopener ugc nofollow">动机</a> <br/> ∘ <a class="ae mg" href="#2db6" rel="noopener ugc nofollow">方法</a> <br/> ∘ <a class="ae mg" href="#03a7" rel="noopener ugc nofollow">结果</a> <br/> ∘ <a class="ae mg" href="#0041" rel="noopener ugc nofollow">结果</a> <br/> ∘ <a class="ae mg" href="#a6fe" rel="noopener ugc nofollow">总结</a> <br/> ∘ <a class="ae mg" href="#3ce9" rel="noopener ugc nofollow">论文及代码</a><br/><a class="ae mg" href="#53b3" rel="noopener ugc nofollow">modulenet</a><br/>∘<a class="ae mg" href="#d4db" rel="noopener ugc nofollow">动机</a> <br/> <a class="ae mg" href="#2c0c" rel="noopener ugc nofollow">结果</a> <br/> ∘ <a class="ae mg" href="#d06b" rel="noopener ugc nofollow">总结</a> <br/> ∘ <a class="ae mg" href="#42a4" rel="noopener ugc nofollow">论文与代码</a> <br/> <a class="ae mg" href="#139c" rel="noopener ugc nofollow">讨论</a> <br/> ∘ <a class="ae mg" href="#787d" rel="noopener ugc nofollow">总结</a> <br/> ∘ <a class="ae mg" href="#fcb0" rel="noopener ugc nofollow">未来工作</a> <br/> ∘ <a class="ae mg" href="#006f" rel="noopener ugc nofollow">结论</a> <br/> <a class="ae mg" href="#5a8d" rel="noopener ugc nofollow">附加资源</a> <br/></p><h1 id="80f6" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">介绍</h1><p id="32e5" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi lw translated">在立体视觉中，人类的大脑有一种奇妙的能力去看深度。我们使用我们的两只眼睛，它们彼此分开，位于头部的两侧，这使我们能够以三维方式感知世界:高度、宽度和深度(即，相对于周围环境的前后位置)。这项技能并不是人类独有的——正如在<a class="ae mg" rel="noopener" target="_blank" href="/depth-from-disparity-via-deep-learning-part-0-458827141b23"> <em class="mf">第一部分</em> </a>中强调和举例说明的那样；许多动物以各种方式使用立体声！尽管如此，建模立体计算已被证明是一个具有挑战性的壮举。从监督建模的角度来看，困难在于所需的数据集，这些数据集必须包含表示目标场景和组成深度网络的各种对象的信息。我们探讨了这些挑战以及研究人员旨在克服的基于深度学习的方法的最新进展。</p><p id="4eb8" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">从<a class="ae mg" href="http://QR&amp;YG&amp;6@FHi7$A6$1" rel="noopener ugc nofollow" target="_blank">第1部分</a>，让我们回忆一下极线假设:场景包含两个已知内在属性(即镜头的焦距<em class="mf"> f </em>和外在属性(即左右摄像机的光学中心之间的距离称为基线<em class="mf"> b </em>)的摄像机。提供了预测的视差<em class="mf"> D </em>，简单的几何图形被用来重建在捕获图像时丢失的深度尺寸(即<em class="mf"> z </em>)。</p><p id="9a02" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在这一部分(即<em class="mf">第二部分</em>)中，我们回顾了2015-2020年间基于深度学习的深度估计方法的进展。未来的部分(即3、4和5)将涵盖最新和最大的(即2021–2022)，基于视频的深度立体方法(即多视图立体(MVS))，并在子像素级别产生相应的置信度。</p><h1 id="c45f" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">背景资料</h1><h2 id="dcb0" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">立体视觉</h2><p id="5149" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated">立体的基础——极线几何中更深的深度、图像校正和其他对初学者来说不太重要的数学细节(即，主题本身，如果需要，稍后再返回)将在本系列的第1部分中讨论。如果不理解下面的陈述，鼓励读者阅读前面的部分:</p><p id="3af9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="mf">深度知觉的重要性及其古老起源</em>；<em class="mf">视差的定义</em>；<em class="mf">深度线索的概念以及人类如何自然感知</em> ( <strong class="lc iu"> <em class="mf">提示:</em> </strong>两只【立体】眼睛)；<em class="mf">视差和深度之间的关系，以及如何从一个空间变换到另一个空间在两个</em>(即，左和右)<em class="mf">图像中提供了相应的像素；在</em>之前假设了什么参数和条件(即，获得内部和外部参数，以及校正的立体图像对)。</p><div class="nq nr gp gr ns nt"><a rel="noopener follow" target="_blank" href="/how-computers-see-depth-deep-learning-based-methods-368581b244ed"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd iu gy z fp ny fr fs nz fu fw is bi translated">计算机如何看深度:基于深度学习的方法的最新进展</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">第1部分:立体视觉的动机和基础</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">towardsdatascience.com</p></div></div><div class="oc l"><div class="od l oe of og oc oh ks nt"/></div></div></a></div><h2 id="8c29" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">深度监督学习</h2><p id="a0a4" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated"><em class="mf">端到端</em>有监督的深度学习在很多年前就已经成为主流<em class="mf">。它以之前无法想象的方式登上排行榜榜首。问题类型(即任务评估)的增长源于深度学习的成功，因此也是<em class="mf">大数据时代</em>的启动。也就是说，基于梯度的端到端深度学习概念可以追溯到20多年前[1]。然后，在2012年，辍学的概念遇到了大数据，缓解了过度拟合的问题，<em class="mf">通用GPU </em> (GPGPU)的计算能力，允许矩阵数学在数千个内核上并行执行，为我们许多人在2012年提到的开创性工作铺平了道路[2]。然而，deep nets又花了三年时间才在2015年提出用于补丁级别的深度重建[3]，又花了一年时间提出图像级别的端到端系统[4]。</em></p><p id="ef15" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在开始有监督的深度学习方法的旅程之前，让我们先定义一些重要的概念。<em class="mf">非端到端</em>和<em class="mf">端到端</em>描述了最高级别的学习方法:<em class="mf">端到端</em>意味着输入直接映射到最终输出，这是一个通过反向传播从一个信号学习的权重函数，该信号从<em class="mf">端到端</em>回溯。<em class="mf">端到端</em>是当今首选的培训方式，因为目标指标中最令人兴奋的特性是通过优化获得的。相反，<em class="mf">非端到端</em>不学习从输入空间到输出的映射，而是依赖于中间步骤，这些中间步骤通常由人工设计来调整，因此可能不是最佳的。下图列出了两者的特征。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/781cd066e9c66f269855a64ad16840dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cr-KFzq9g-K8gNWmfJKk4g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">传统非端到端方法和现代端到端学习方法的特点。作者创造了可视化。</p></figure><p id="2804" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">除了我们复习的第一种方法(即<em class="mf"> MC CNN </em>)之外，这部分涵盖的方法都是监督式<em class="mf">端到端</em>深度学习方法。尽管<em class="mf"> MC CNN </em>是一个深度神经网络，我们很快就会知道，该模型将面片级输入对映射到视差空间，这意味着将视差面片估计融合到图像所需的额外步骤是不可学习的。因此，<em class="mf"> MC CNN </em>不是一个<em class="mf">端到端</em>方法。</p><p id="c278" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">注意。</strong>假设读者对深度学习概念有基本了解，主要是监督方法。最后的<a class="ae mg" href="#39db" rel="noopener ugc nofollow">补充</a>部分提供了由三部分组成的介绍性系列的链接。</p><h2 id="1fe4" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">立体视觉的深度监督学习</h2><p id="ad39" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated">当设计用于深度估计的深度网络时，遵循两种一般的拓扑监督方法:编码器-解码器架构(<strong class="lc iu">图1 </strong>)和在连体CNN之上使用成本体积正则化的方法(<strong class="lc iu">图2 </strong>)。无论如何，输入空间是立体对(即，左和右RGB图像)，并且输出是由最小和最大视差限定的相应视差<strong class="lc iu"> <em class="mf"> D </em> </strong>:除非另有说明，否则视差等级的数量是离散的并且是已知的，因为假设图像对被校正。目标是最小化实际和预测视差映射之间的差异。</p><p id="dffd" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">上面列出的假设应该是有意义的:视差<strong class="lc iu"><em class="mf">D</em></strong><em class="mf"/>serves是在其<strong class="lc iu"> <em class="mf"> D </em> </strong>的空间中学习的(即，封闭形式的分类)，而离散特征代表点从左到右沿水平方向移动的像素数量(即，整数个像素)。如何在亚像素级别获得鲁棒性:敬请关注，在本博客结束之前你将会有答案:)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/5a9ca2850c9d1b449ca4b504a5a4f6f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cw--Ia948BxfYC8GIcrJuA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图一。</strong>编码器-解码器拓扑的高级示意图，其中模型学习将立体对映射到相应的视差图。<em class="kz">作者创造的人物。</em></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/3c9ab5ca3c83036b3095bbcfeb6a7cba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WOh-0OooeTSSff4P5TM4Lg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图二。</strong>模型的典型视图的高级表示，通过融合左右图像的特征来调整以产生最佳视差图，从而形成成本体。<em class="kz">作者创造的人物。</em></p></figure></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><p id="3a45" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在让我们开始具体方法的回顾。从<em class="mf"> Zbontar和Lecun </em> (2016)开始，我们将涵盖<strong class="lc iu"> </strong>方法。</p><h1 id="ce08" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">MC-CNN</h1><blockquote class="os ot ou"><p id="8014" class="la lb mf lc b ld le ju lf lg lh jx li ov lk ll lm ow lo lp lq ox ls lt lu lv im bi translated">Jure Zbontar和Yann LeCun。<em class="it">“通过训练卷积神经网络来比较图像块的立体匹配。”</em> J .马赫。学习。第17.1号决议<em class="it"> (2016年)</em>。</p></blockquote><h2 id="6ff4" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">介绍</h2><p id="5b6f" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated">第一个为深度估计提出的监督深度学习框架是<em class="mf"> MC CNN </em>。由于学习从立体对到视差空间的逐像素(即密集)映射的巨大复杂性，作者使用了图像补片。尽管如此，MC CNN<em class="mf">的概念对于后面介绍的端到端方法是必不可少的。换句话说，<em class="mf"> Jure Zbontar和Yann LeCun </em>推出了<em class="mf"> MC CNN </em>，开始了我们即将走的路。很明显，MC CNN </em>的部分内容启发了我们接下来的工作。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/e5436a992e6d1ec8935b13e9a4114bae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X9sp8D6F_2kjOZqj7E6aJw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">学习从立体对到视差图的映射。请注意，最靠近相机的物体具有更大的差异:暖色的差异更大，因此深度更小——来源:<a class="ae mg" href="https://arxiv.org/abs/1510.05970" rel="noopener ugc nofollow" target="_blank">原文</a>。</p></figure><h2 id="4c42" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">方法</h2><p id="f7a5" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated"><em class="mf"> MC CNN </em>是将立体图像对映射到相应视差的面片级方案。如下图所示，该系统模拟了传统立体视觉技术的典型步骤。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/01612fcbc2db99977289758ba5fb9b44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZB0rYA3X1Qkab5_EDuaeQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用于生成视差图的管道。来自左和右图像的标记为红色和蓝色输入(最左侧)的小块被馈送到CNN，为此产生具有<strong class="bd ky"><em class="kz"/></strong><em class="kz">通道(每个视差值一个)的特征，表示在小块的相应位置处</em> <strong class="bd ky"> <em class="kz"> d </em> </strong> <em class="kz">的成本。</em>来源:<a class="ae mg" href="https://arxiv.org/abs/1510.05970" rel="noopener ugc nofollow" target="_blank">原创论文</a>。</p></figure><p id="9bd5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">目标是为考虑中的所有差异<em class="mf"> d </em>计算每个位置<strong class="lc iu">pT37】的匹配成本。这样的成本可以简单地确定为关于点<strong class="lc iu"> <em class="mf"> p </em> </strong>的补片之间的绝对差。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/698cb1d4ddb4fd37afd7cc286c6843e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wfrdVQXfmvEBDEBBVamdow.png"/></div></div></figure><p id="4250" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">𝐼ᴸ和𝐼ᴿ是位置<strong class="lc iu"> <em class="mf"> p </em> </strong>处的图像强度。𝓝 <strong class="lc iu"> ₚ </strong>是固定窗口中以<strong class="lc iu"><em class="mf">p</em></strong>—也就是<strong class="lc iu"><em class="mf">p</em></strong>=(<em class="mf">x</em>，<em class="mf"> y </em>)，然后<strong class="lc iu"><em class="mf">p</em></strong><em class="mf">d</em>=(<em class="mf">x</em>-【t70)因此，下图描述了修补程序级别的连体网络。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/6ec7b1a7a3f77cdd14b1d0628e8aa997.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*ku-6yJaIowMLtE9EvdoG8g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MC CNN的连体架构(快)。注意，对于每个具有共享权重的相似性分数，处理一对9x9的面片(即，暹罗网络)。作者创造了可视化。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/48e33f480203d627545d3b38e9bad52a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a5iILj9vW7xmqRVJzJVRmA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者提出了快速<strong class="bd ky"> </strong>(左)和精确(右)的变体。</p></figure><p id="20e5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="mf"> MC CNN </em>的作者提出了两个变量，一个是速度(即<strong class="lc iu">快</strong>)，另一个是准确性(即<strong class="lc iu">准</strong>):对于任何一个，网络拓扑都会产生相似性得分。<strong class="lc iu">快速</strong>变体完全独立地从左侧和右侧提取特征，允许一个面片的表示被转换一次，然后与其他面片比较几次以生成分数。同时，<strong class="lc iu">精确</strong>模型在共享分支之前融合特征，通过一系列附加层从连接的特征中学习度量。因此，<strong class="lc iu">精确</strong>方法必须在每次生成分数时处理两个补丁，而更快的版本可以存储补丁的特征，例如只需在补丁对之间进行点积。</p><p id="f476" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">作者还表明，像传统的立体匹配方案一样，最大化相关性是最佳的。<strong class="lc iu">此外，相关性计算(即层)在模型的存储和速度方面花费较少。</strong>下图从不同的角度显示了管道。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/2b74c5fde433828ea733d6fcfb1bd966.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*asApg663vMLgQAjBqzWCXA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">他们采用了一个连体网络，提取每个像素所有可能差异的边缘分布，以学习信息丰富的图像补丁表示。</p></figure><h2 id="fdf8" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">结果</h2><p id="0f72" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated">在KITTI 2012、KITTI 2015和Middlebury数据集上，精确变体比其所有前辈产生了更低的错误率；快速版本的运行速度比精确架构快90倍，误差增加很少。</p><p id="dce2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这些结果表明，CNN在确定立体声对的匹配成本方面工作良好，甚至在需要实时性能的应用中也是如此。</p><p id="903f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">因此，这个相对简单的CNN声称是最先进的(SOA)，展示了现代深度学习方法解决经典立体视觉计算机视觉问题的巨大潜力。</p><h2 id="6e7a" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">概括起来</h2><p id="c1c3" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated">为了完整起见，<em class="mf"> MC CNN </em>被包含在这一部分中，关于用于立体视觉的第一批深度学习方法，因为它是一个开创性的解决方案。但是，它不是一个端到端的解决方案，因为它处理补丁，而不是在映像级别。因此，这种方法的覆盖面被设计得很小。更深入地讨论了处理方法。</p><p id="11cf" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">赞成的意见</p><ul class=""><li id="2e8a" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">首次尝试使用深度特征学习来解决立体深度估计</li><li id="7c7a" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">一项广泛的研究显示了一种<strong class="lc iu">快速</strong>和一种<strong class="lc iu">精确</strong>变体</li><li id="4787" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">补丁提供了大量的训练数据</li></ul><p id="b0c0" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">骗局</p><ul class=""><li id="04e7" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">不是端到端的解决方案</li><li id="9596" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">许多手工设计的步骤</li><li id="7f26" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">无法捕捉全局上下文，因此仍然无法捕捉不均匀和高度纹理化的区域。</li></ul><h2 id="cc03" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">纸张和代码</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ps pt l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae mg" href="https://arxiv.org/abs/1510.05970" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1510.05970</a></p></figure><div class="nq nr gp gr ns nt"><a href="https://github.com/jzbontar/mc-cnn" rel="noopener  ugc nofollow" target="_blank"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd iu gy z fp ny fr fs nz fu fw is bi translated">GitHub - jzbontar/mc-CNN:通过训练一个卷积神经网络来比较的立体匹配…</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">储存库包含用卷积神经网络计算立体匹配成本的过程；方法…</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">github.com。</p></div></div><div class="oc l"><div class="pu l oe of og oc oh ks nt"/></div></div></a></div><h1 id="f4e2" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">流动网络</h1><blockquote class="os ot ou"><p id="fdd8" class="la lb mf lc b ld le ju lf lg lh jx li ov lk ll lm ow lo lp lq ox ls lt lu lv im bi translated"><em class="it"> Dosovitskiy，Alexey，等人《流网:用卷积网络学习光流》</em>IEEE计算机视觉国际会议<em class="it"> (2015)。</em></p></blockquote><h2 id="363c" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">动机</h2><p id="21a8" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated">当以端到端的方式训练时，深度神经网络学习感兴趣的任务知识的能力处于其最高潜力。虽然最初提出FlowNet是为了解决光流(即，本质上类似于根据视差进行深度估计的任务)，但它是第一个端到端解决方案。因此，这里的许多概念将在以后直接应用于视差估计。</p><p id="d964" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">注意:</strong>光流是一个视频相邻帧中对应点之间的差值。正如我们所了解的，立体视觉中经常做出的一个假设是输入图像是经过校正的。差异是水平位移的函数。另一方面，光流考虑水平和垂直方向上像素位置的差异。因此，可以在描述由一对立体摄像机提供的场景流的两种类型的知识之间推断几何相似性。关于光流的细节超出了本博客的范围。感兴趣的读者可以在[5]中看到更多关于光流的内容，在[6]中可以看到更具描述性的场景流。最近，一项关于这两个主题的调查发表了[7]。</p><p id="fda2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">FlowNet在空间上和网络的收缩部分压缩信息(即特征)，然后在扩展部分细化这些特征。因此，将图像对直接映射到目标输出空间。论文中的下图描述了FlowNet的高级概念。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pv"><img src="../Images/b635dab15fc683495e18e1a3fdf329f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9zdeQMDxsr-msXghzYUXsg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">FlowNet的结构类似于编码器-解码器网络。U-Net [4]计算大小为(1，<em class="kz"> nrows </em>，<em class="kz"> ncolumns </em>)的正则化视差d⋆。这种方法的主要缺点是归一化互相关层的计算成本。而且还会产生模糊的视差图:<strong class="bd ky">来源— </strong> <a class="ae mg" href="https://arxiv.org/pdf/1504.06852.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</p></figure><h2 id="8cf2" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">方法</h2><p id="f96e" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated">如下图所示，Flownet包括两种网络架构:FlowNetSimple (FlowNetS)和FlowNetCorr (FlowNetC)。</p><p id="c018" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu"> FlowNetS </strong>将两幅图像连接起来，并将其输入网络。</p><p id="006e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu"> FlowNetC </strong>是一种不太通用的网络结构，其中每个图像都被送入一个连体网络，其工作方式如下:</p><ul class=""><li id="eb85" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">分别学习有意义的表示(如边)。</li><li id="c3f4" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">然后，各个分量在相关层中经历乘法面片比较(即，类似于矩阵乘法)。</li><li id="71e4" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">两个<strong class="lc iu"> </strong>都是乘法面片比较和类似的细化过程的结果。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pw"><img src="../Images/b6e35215e80d17709c1a97e7147a1a58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mgnxniyqO6mYjnauBt7tlQ.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi px"><img src="../Images/56c42c77b5d0890d83f34af6e5877221.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1g58p5n3hD5XxX1-Li5-oQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">两种网络架构:FlowNetSimple(顶部)和FlowNetCorr(底部)。绿色漏斗是图3中显示的扩展细化部分的占位符。网络包括细化部分都是端到端训练的—来源<strong class="bd ky"> <em class="kz"> : </em> </strong> <em class="kz">原论文</em>并由作者修改。</p></figure><p id="27a4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">两个特征图之间的乘法块比较由相关层进行。给定𝖿₂𝖿₁的两幅多通道特征图，宽度<em class="mf"> W </em>，高度<em class="mf"> H </em>，通道数<em class="mf"> C，</em>第一幅图中以x₁为中心，第二幅图中以x₂为中心的两个面片的<em class="mf">相关性</em>数学表达式如下。⋆</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi py"><img src="../Images/b8950e827ca42da9bc675030bfa4c23d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AhWStvBIOuP26MAV5A32kA.png"/></div></div></figure><p id="f9e7" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">正方形空间补片的大小为<code class="fe pz qa qb qc b"><em class="mf">K</em> = 2<em class="mf">k</em>+1</code>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qd"><img src="../Images/f0abeae3629ffb7e6aeb4c133d2a0482.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JHK4wTiun3pR7izFcwaAeA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">它仍然是许多系统的基本部分:形成成本体积的最佳熔丝信号。</p></figure><p id="f4d9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">使用输出的卷积图层连接并提取f₁的要素地图。</p><ul class=""><li id="1215" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">他们通过一系列<em class="mf"> conv </em>和合并图层降低分辨率。</li><li id="9ae0" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">作者通过<em class="mf">上进化</em>层的<em class="mf">解池</em>和<em class="mf">上进化</em>来细化粗池表示。</li><li id="5b45" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">将<em class="mf">上变换的</em>和相应的特征图连接起来，并进行上采样的粗流量预测。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qe"><img src="../Images/5e850472a8de8124af14de83ba8a0498.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4AcOZlyW-V8Ds0XKm75a9Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">细化粗特征映射到高分辨率预测—来源<strong class="bd ky"> <em class="kz"> : </em> </strong> <em class="kz">原始论文。</em></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qf"><img src="../Images/fe2bc274d7e023c84c81f37c6996355c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XdUTZzZOu9DS8fS5"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="kz"> FlowNet架构。</em></p></figure><h2 id="d82f" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">结果</h2><h2 id="e1ee" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">摘要</h2><p id="b0fb" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated">FlowNet是第一个用于立体深度估计的端到端深度特征学习解决方案(虽然最初是针对光流提出的，但过渡是微不足道的，因为视差是光流的子集)。换句话说，因为视差表示像素沿水平方向的移动，所以光流是帧<em class="mf"> t </em>和<em class="mf"> t+ </em> 1之间在所有方向上的位置变化(即，实数的2D矢量表示，与整数值1D视差张量相反)。因此，在立体视觉系统中，假设提供了校正的立体对，找到视差的问题需要在相邻对的对中找到光流的子集解决方案。事实上，给定带有摄像机的立体视觉设置，视差张量和光流张量描述了场景流[5]，这是对前方场景的更多了解。</p><p id="54c6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">FlowNet的缺点包括:</p><ul class=""><li id="a44f" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">归一化互相关层的高计算成本。</li><li id="297c" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">产生模糊的视差图。</li></ul><p id="b2fd" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">尽管如此，深度学习、端到端、监督学习范式中差异模型的演变，FlowNet引入了将启发其他人的概念(如本博客结尾所示)。</p><h2 id="805e" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">纸张和代码</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ps pt l"/></div></figure><div class="nq nr gp gr ns nt"><a href="https://github.com/ClementPinard/FlowNetTorch" rel="noopener  ugc nofollow" target="_blank"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd iu gy z fp ny fr fs nz fu fw is bi translated">GitHub-ClementPinard/FlowNetTorch:Fischer等人的FlowNet培训代码的Torch实现</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">这个库是FlowNet的一个torch实现，由Alexey Dosovitskiy等人在Torch。这段代码主要是…</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">github.com</p></div></div><div class="oc l"><div class="qg l oe of og oc oh ks nt"/></div></div></a></div><h1 id="89f5" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">iResNet</h1><blockquote class="os ot ou"><p id="05e5" class="la lb mf lc b ld le ju lf lg lh jx li ov lk ll lm ow lo lp lq ox ls lt lu lv im bi translated">通过特征恒常性学习视差估计。<em class="it"> IEEE计算机视觉和模式识别会议论文集</em>。2018.</p></blockquote><h2 id="65c0" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">动机</h2><p id="27ee" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated">梁等人考虑立体视觉系统的传统步骤，如本系列的第0部分中所介绍，并在上面列出(<a class="ae mg" href="#31ca" rel="noopener ugc nofollow">立体方法学</a>)。网络架构包括对应于立体匹配的每个步骤的模块。下图显示<em class="mf">特征提取</em>、<em class="mf">视差估计</em>和<em class="mf">细化</em>模仿传统方法。</p><h2 id="99c8" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">方法</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qh"><img src="../Images/e6fdf1c3a14594b80df348803d4fbb13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*is-09NottRCvG8sK"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="kz">建议网络的架构将立体匹配的所有四个步骤整合到一个网络中。为了更好的可视化，他们省略了编码器和解码器在不同尺度上的跳跃连接——来源</em> <strong class="bd ky"> <em class="kz"> : </em> </strong> <em class="kz">原文。</em></p></figure><p id="0428" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="mf">特征恒常性</em>是指特征空间中两个像素的对应关系。</p><p id="9672" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">跳过连接允许推断视差图的约束解。</p><ul class=""><li id="4691" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">当对匹配成本执行视差估计时，子网捕获低级语义信息。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/a90d8f5c4740b95f226e71f69dd36a6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/0*0Uw2PGAQ7EQZW6CP"/></div></figure><p id="f849" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="mf"> f </em> ꜀充当特征间的相关性，<em class="mf"> re </em>重建误差，产生第一视差<em class="mf"> disp </em>，而<em class="mf"> disp </em> ᵣ <em class="mf"> </em>为细化视差。</p><p id="1b29" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">迭代网络。</strong>视差细化阶段的输出可以是第一视差预测，并且因此直接回到视差细化模块中。作者发现2-3次迭代可以改善结果，这会影响最后的地图。</p><p id="293d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">它是如何工作的？</strong>如上面的数学公式所表达的，精确的视差<em class="mf">显示</em> ᵣ.</p><p id="0774" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">关系左侧三项的结果。具体来说，第一个<em class="mf">特征恒定性</em>项、<em class="mf"> f </em>、<em class="mf"> </em>是左右特征图之间的相关性:度量两个特征图在所有视差级别上的对应性。因此，第一项越大，特征越相似。第二项<em class="mf"> re </em>是多尺度融合特征之间的绝对差值。他们发现了特征空间中的重建误差，而不是像素，这是这项工作之前的惯例(例如，CRL)。最后，我们有第三项，初始视差估计，<em class="mf">显示</em> ᵢ或者仅仅是<em class="mf">显示</em>。</p><h2 id="8bf2" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">结果</h2><p id="9a10" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated">系统和子系统变体的现场流量结果如下表所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qj"><img src="../Images/62a4c2afc5b424492e887c45022c7250.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rBg7CkP_NqpM26SNIHx7tA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">来源:</strong>原论文。</p></figure><p id="efef" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">接下来是单尺度和多尺度的比较。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qk"><img src="../Images/aa33bb9aa344a5ecddc782873ba78fd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*OCeO8S_LgkNJZMH65kxs0Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">来源:</strong>原文。</p></figure><p id="de0c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">最后，在场景流的错误、大小和运行时方面与SOA进行比较。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ql"><img src="../Images/460e8de2dff69069983eb29c8075a37c.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*eEnOJGvDd454Yyu95CCMLw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">来源:</strong>原论文。</p></figure><p id="497d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们可以在从称为视差估计子网络(DES-Net)的第一模块提取的特征中看到<em class="mf"> re </em>的有效性，与像素相反。DES-Net学习一种表示以传递给第二模块，称为差异细化子网(DRS-Net)。因此，可以假设从DES-net(向下)传播的误差是由于重建误差和初始差异(见上面绿色和红色矩形中间并排的绿色和红色矩形，分隔DES-Net和DRS-Net)。他们将<em class="mf"> disp </em>输入到DRS-net，通过细化进行改进:<em class="mf"> disp </em>是第一个首字母，然后通过DRS-net创建<em class="mf"> disp' </em>，通过以下方式确定残差</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qm"><img src="../Images/9949aed731aa5d59b7e03401246b9086.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/0*OYnPCsQ5AmuR1anY"/></div></figure><p id="08aa" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">其中迭代次数(即细化步骤):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qn"><img src="../Images/27ec5adac67798297ead3ef02ca4070c.png" data-original-src="https://miro.medium.com/v2/resize:fit:198/0*2iZ-hiEyklWwotPB"/></div></figure><p id="51df" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">作者发现，2–3会挤压所有潜在的性能(即最适合设计的性能)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qo"><img src="../Images/54ef52b4287d0823473501a27c95f567.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/0*Olrp9BVrOVGrMaiT"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="kz">在不同迭代的场景流测试集上进行视差细化。第一行是输入图像，第二行示出了没有改善的初始视差，第三和第四行示出了在1次和2次迭代之后的精细视差。第5行给出了地面实况的悬殊——来源</em> <strong class="bd ky"> <em class="kz"> : </em> </strong> <em class="kz">原创论文。</em></p></figure><p id="2d20" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">下表中使用KITTI 2012和2015数据集进行的比较说明了KITTI的定性比较。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qp"><img src="../Images/51ee362cb3753c25fa11eb91a14569a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*guWj90DkDhvE-BWpD8av3g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">来源:</strong>原论文。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qq"><img src="../Images/4e1ca356ce0f0afaeb961eb6e111057a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*ukvq7OnmH-40jYHC9aQnsA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">来源:</strong>原文。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qr"><img src="../Images/6680e780e94e397cda82a474f77f16d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9svbwrktAwgV1SqelL7Qbg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">来源:</strong>原论文。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qs"><img src="../Images/bc9ce0e21e86d4d8f8e9eab4fce2beb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/0*ZfkhGVdd764EMS4i"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="kz">KITTI 2015上与SOA的比较。第一行中的图像是来自KITTI 2015的输入图像。我们的iResNet-i2细化结果(第3行)可以显著改善初始视差(第2行)，并给出比其他方法更好的可视化效果，尤其是在图像的上部，在该区域没有地面真实—来源</em> <strong class="bd ky"> <em class="kz"> : </em> </strong> <em class="kz">原文。</em></p></figure><h2 id="d48a" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">摘要</h2><h2 id="121c" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">纸张和代码</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ps pt l"/></div></figure><div class="nq nr gp gr ns nt"><a href="https://github.com/leonzfa/iResNet" rel="noopener  ugc nofollow" target="_blank"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd iu gy z fp ny fr fs nz fu fw is bi translated">GitHub - leonzfa/iResNet</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">这个库包含“通过特征恒定性学习视差估计”论文的代码(在CAFFE中)</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">github.com。</p></div></div><div class="oc l"><div class="qt l oe of og oc oh ks nt"/></div></div></a></div></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><h1 id="9c66" class="mh mi it bd mj mk qu mm mn mo qv mq mr jz qw ka mt kc qx kd mv kf qy kg mx my bi translated">几何和上下文网络</h1><p id="9efd" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated">深度立体回归的几何与情境的端对端学习。IEEE计算机视觉国际会议论文集。 2017。​</p><h2 id="cb16" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">动机</h2><ul class=""><li id="f925" class="pe pf it lc b ld mz lg na lj qz ln ra lr rb lv pj pk pl pm bi translated">据观察，立体算法的几个挑战将受益于知道全局语义上下文而不是局部几何。</li><li id="cf58" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">目的是学习端到端的立体回归模型，以理解更广泛的上下文信息。​</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rc"><img src="../Images/76e5e74f1531935a576f3554fe822b33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MsMWZXgVn7BZC3p1"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">理解全局语义和几何将减轻问题，因为更广泛的上下文将受益于像反射这样的障碍。</p></figure><h2 id="b862" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">方法</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rd"><img src="../Images/327e9cb0c461b606ff4744fe9d74070e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G1xDTk5hr34eiDxxzfhzTA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">来源:</strong>原创论文。</p></figure><ul class=""><li id="0af8" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">从校正的立体对进行端到端学习以估计亚像素视差。</li><li id="0d44" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">它形成了利用问题几何的可区分成本量。</li><li id="5ab6" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">通过跨成本体的多尺度三维卷积学习上下文。</li><li id="adde" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">评价视差曲线的可微软ArgMax。</li></ul><p id="5e56" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">接下来，我们将逐步完成构成GC-Net的模块，上图与stereo methods的原始算法(即工作流)相关。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rd"><img src="../Images/1bab9499d52c266c12a38a661e31960f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Ib2oxzJVszjccf4JIQigg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用突出显示的模块描绘原始图[图来自作者修改的论文]。</p></figure><p id="a0a8" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">特征提取:</strong></p><p id="8214" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如今，我们使用特征表示，而不是从原始像素强度计算成本。然后，比较对光度外观中的模糊性稳健的描述符，并捕获局部上下文。作者将这些特征称为一元特征。利用的优势</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi re"><img src="../Images/c4b3168b0c7d2ee77a461e77f9fe07f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xnShGBnisbHf539y0QBBhA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">原始论文中的表格。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi rf"><img src="../Images/7150d5cbf07603f303574713f7ba5144.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*lE6b8lwnxLKI0B-TGttoxw.png"/></div></figure><ul class=""><li id="8e5f" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">步长为2的卷积滤波器对输入进行二次采样，以减少计算需求。</li><li id="de88" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">之后，八个残差块由两个<code class="fe pz qa qb qc b">3×3</code>卷积滤波器串联而成。</li><li id="5f76" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">在左右网络之间共享参数以学习相应的特征。</li></ul><p id="5cea" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">成本量</strong></p><ul class=""><li id="ec4d" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">成本体积的大小<code class="fe pz qa qb qc b">H×W×(Dmax + 1)×F</code></li><li id="98a8" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">这是通过将每个一元特征与它们在每个视差级别上来自相对立体图像的对应一元特征连接起来并将其打包到4D体积中来实现的</li><li id="7d1d" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">它赋予网络学习语义的能力。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rg"><img src="../Images/82dc99bef6d6142e16b7b66a4d7576a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RwlRM5YFn74gluaz16X4Jg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">场景流数据集包含来自一系列合成场景的960 × 540px的35，454幅训练图像和4，370幅测试图像。我们比较不同的架构变体来证明我们的设计选择。结果强调了原始论文的3-D卷积架构来源的重要性。</p></figure><p id="e0bf" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">软Argmin </strong></p><p id="e727" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">立体算法从一元的匹配成本中产生最终成本量。体积通过最小化成本来估计差异，例如在成本体积差异维度上的argmin运算。但是，使用标准的argmin操作有两个问题。</p><ol class=""><li id="c243" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv rh pk pl pm bi translated">它是离散的，并且不能产生子像素视差估计。</li><li id="1cef" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv rh pk pl pm bi translated">因此，该模型是不可微的，并且不能使用反向传播来训练。</li></ol><p id="d2a3" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">作者引入了操作的软版本来缓解这些问题。我们称之为<em class="mf">软argmin </em>，数学定义如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ri"><img src="../Images/30808fb3ff9a441f89fecf7718af7b1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IvdO3Whjy1AvTPWO9i1xmw.png"/></div></div></figure><p id="6a60" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">其中<em class="mf">d</em>ₘₐₓ<em class="mf">t14】是最大差异<em class="mf"> d，</em>和期望值(即sigma)是</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rj"><img src="../Images/74fbf80cd0e573c09c4e41fa379a1524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*opxjaWvvt4ZU7fJyrAir7Q.png"/></div></div></figure><p id="6e04" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这种损失类似于bahda nau<em class="mf">et al</em><em class="mf">soft attention</em>【1】，一种<em class="mf">soft arg max</em>【2】的否定变体。后者扩展了[3]中使用的操作，通过高阶统计扩展损失(即，最小化预测和预期之间的差异，并优化置信度)。从统计学上讲，基于一阶矩(即预期值)的损失通过二阶矩(即方差)获得信息。旁注:对上述内容的深入探究在未来博客想法的列表中，所以请继续关注；)</p><ul class=""><li id="f897" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">为了克服这些限制，我们定义了一个软argmin，它是完全可微的，并且能够回归一个平滑的视差估计。</li><li id="5cfa" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">首先，我们通过取每个值的负值，将预测成本cd(对于每个差异<em class="mf"> d </em>)从成本量转换为概率量。我们使用softmax运算σ()对视差维上的概率体进行归一化。然后，我们对<em class="mf"> D </em>进行求和，并按其归一化概率进行加权。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rg"><img src="../Images/f9c436878cfd68379a49af45a947eb85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6vYrHTUZjjd-ZZgeyf7BhA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">软参数操作的图形描述。</strong>它沿着每条视差线获取一条成本曲线，并通过对每个视差的softmax概率与其视差指数的乘积求和来输出argmin的估计值。(<strong class="bd ky"> a </strong>)表明，当曲线为单峰时，这准确地捕捉了真实的argmin。(<strong class="bd ky"> b </strong>)示出了当数据是具有一个峰值和一个平坦区域的双峰时的故障情况。(<strong class="bd ky"> c </strong>)如果网络学会预缩放成本曲线，则可以避免这种失败，因为softmax概率会更极端，产生单峰结果。</p></figure><ul class=""><li id="6bb8" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">然而，与argmin操作相比，它的输出受所有值的影响，这意味着它容易受到多峰分布的影响，因为最不可能得到支持。相反，它将估计所有模式的加权平均值。为了克服这个限制，网络进行调整以产生单峰差异概率分布。</li><li id="1984" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">网络还可以预先调整匹配成本，以控制归一化后的softmax峰值(有时称为温度)。</li></ul><p id="ad31" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">损失函数</strong></p><ul class=""><li id="4472" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">使用GT视差dₙ和像素<em class="mf"> n </em>的预测视差<em class="mf"> dHat </em> ₙ之间的L₁训练模型</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rk"><img src="../Images/e4e86caec4e0216cf9b6905134ce92ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MWqJM3V9lD9HyaylvU06xA.png"/></div></div></figure><ul class=""><li id="32e3" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">回归模型允许基于光度重投影误差的无监督学习损失。</li></ul><h2 id="5bd7" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">结果</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rl"><img src="../Images/2085ea2105a5a78a542d4395c0d4264f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-_zY3UhqdyY1tiqcmGiDwg.png"/></div></div></figure><p id="aa3d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">定量结果</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rm"><img src="../Images/46bc1425e3aaadb72672f073444d1664.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KEjbOCGZ5dws5nicvq9pAQ.png"/></div></div></figure><p id="b870" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">定性结果</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rn"><img src="../Images/fd04a0b6d63be9dcea40626f298513be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bg4cXLDs3ajVW0CDi_49jA.png"/></div></div></figure><h2 id="56ea" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">摘要</h2><ul class=""><li id="b534" class="pe pf it lc b ld mz lg na lj qz ln ra lr rb lv pj pk pl pm bi translated">根据合成数据进行预训练(即SceneFlow [5])。</li><li id="ae52" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">回归通过分类差异获得SOA。</li><li id="a9e6" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">学习的端到端优于学习的一元特征+半全局匹配(即MC-CNN [1])。</li><li id="3a85" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">使用几何图形学习成本卷明显优于其他端到端(即DispNetC)。</li><li id="3138" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">该模型的推理速度相对较快。</li></ul><h2 id="a0d9" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">纸张和代码</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ps pt l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">链接到论文[ <a class="ae mg" href="https://arxiv.org/abs/1703.04309" rel="noopener ugc nofollow" target="_blank"> arXiv </a> ]。</p></figure><div class="nq nr gp gr ns nt"><a href="https://github.com/zyf12389/GC-Net" rel="noopener  ugc nofollow" target="_blank"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd iu gy z fp ny fr fs nz fu fw is bi translated">GitHub — zyf12389/GC-Net:使用PyTorch进行立体匹配的GC-Net</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">使用PyTorch的GC-net进行立体匹配</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">github.com</p></div></div><div class="oc l"><div class="ro l oe of og oc oh ks nt"/></div></div></a></div></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><h1 id="9c59" class="mh mi it bd mj mk qu mm mn mo qv mq mr jz qw ka mt kc qx kd mv kf qy kg mx my bi translated">金字塔立体匹配网络(PSMN)</h1><p id="9e11" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated">常、贾仁和。"金字塔立体匹配网络."IEEE计算机视觉和模式识别会议录。2018.</p><h2 id="6ede" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">动机</h2><p id="db55" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated">深层神经网络中的经验感受野远小于理论感受野。其他方法依赖于基于补丁的连体网络，缺乏利用上下文信息在不适定区域中寻找对应的手段(见下图)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rp"><img src="../Images/34fa4bdd865f171f89f4a107c2beaea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hZWQzTEUFBSttX-ZymVifg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">红色十字表示图像中心像素的感受野。基线:没有扩张的conv，没有SPP，没有堆积的沙漏。完整设置:扩张conv，SPP，堆叠沙漏。该图是从原始论文中复制的，并由作者进行了修改。</p></figure><h2 id="2db6" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">方法</h2><ul class=""><li id="d809" class="pe pf it lc b ld mz lg na lj qz ln ra lr rb lv pj pk pl pm bi translated">引入了金字塔池模块，用于将全局上下文信息合并到图像特征中。</li><li id="3d9d" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">堆叠沙漏3D CNN扩展了成本体积中上下文信息的区域支持。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rq"><img src="../Images/5d8db29f5b3380a124010b318a0c703d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6iWsQvVymMZ2-ylyXBlD9g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">提议的PSMN框架。这张图是从原图上复制和修改的。</p></figure><p id="6988" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">左和右输入立体图像是由四部分组成的两个权重共享管道的输入:(1)特征映射函数(即CNN)；(SPP模块，用于通过连接不同大小的子区域来获取特征；(3)用于特征融合的卷积层；以及(4)左和右图像特征用于形成4D成本体，该成本体传递到3D CNN用于成本体规则化和视差回归。最终输出是预测的视差图。下图描述了完整的系统。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rr"><img src="../Images/cd83fd9f9d887137024667bab40f3b78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*guKlqC048310sqJl"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图来自原论文。</p></figure><p id="230b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">因此，有两个主要模块:空间金字塔池和3D CNN:</p><ol class=""><li id="5f3d" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv rh pk pl pm bi translated">空间金字塔汇集模块通过聚合不同比例和位置的环境来形成成本量，从而利用全球环境信息的容量。</li><li id="72f1" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv rh pk pl pm bi translated">3D CNN学习使用有中间监督的堆叠沙漏网络来调整成本量。</li></ol><p id="e613" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在，让我们仔细看看这两个。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rs"><img src="../Images/104b6c259edd5c936290e02775be2a1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*raNKrrQ_UpdveUNkxRrKnQ.png"/></div></div></figure><p id="d53c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">步骤1:从左/右图像计算特征</strong></p><ul class=""><li id="25e8" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">使用多尺度池不再9✕9补丁。</li><li id="554e" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">每个级别的不同滤镜以原始分辨率工作。</li><li id="e790" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">然后，每一个都被汇集成不同的大小。</li><li id="8292" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">另一个<em class="mf"> conv </em>然后向上采样到原始分辨率。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rt"><img src="../Images/49fed5179becd1d8a5009cdd88c441cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZPEB2rEmRJzZUsvLFrLTAQ.png"/></div></div></figure><p id="ec6a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">特征向量结合了具有不同感受野的过滤器，但是增加感受野的空间汇集减少了可学习的参数。</p><p id="eefb" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">获得左右图像的每个像素的特征向量。</p><p id="616e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">第二步:构建成本量</strong></p><ul class=""><li id="7673" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">然后，我们对每个左右图像都有大小为<code class="fe pz qa qb qc b"><em class="mf">H</em>✕<em class="mf">W</em>✕<em class="mf">F</em></code>的特征张量。</li><li id="4ff9" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">构建一个大小为<code class="fe pz qa qb qc b"><em class="mf">H</em>✕<em class="mf">W</em>✕<em class="mf">D</em>✕2<em class="mf">F</em></code>的成本卷</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ru"><img src="../Images/a02f2f1d8a8fb2b5353731f2b7c87d4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*Dzl3miMXD4qgMPnmtLB6_A.png"/></div></figure><ul class=""><li id="e717" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">在这一点上，这个网络与MC-CNN的不同之处仅在于它如何获得这些功能。</li><li id="f0b2" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">他们的方法相当于为每个成本体积应用一堆完全连接的层，从特征到单个分数，即<code class="fe pz qa qb qc b"><em class="mf">H</em>✕<em class="mf">W</em>✕<em class="mf">D</em>✕2<em class="mf">F H</em>✕<em class="mf">W</em>✕<em class="mf">D</em>✕1</code>。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi rv"><img src="../Images/ab041fd8e7e46c0ad17eff156fcabc15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/0*Q6p12MPfuH7Hl3w_"/></div></figure><p id="c7d2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">第三步:流程成本量</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rw"><img src="../Images/360be20846a141512fe5b82818dbb187.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vmuwtC2DZHWgMGjLiIEKEQ.png"/></div></div></figure><p id="ba29" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">3D卷积层与剩余连接串联。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rg"><img src="../Images/81922bd73fa0b57e07d31f76570f523e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*su8WeA3hy1NA_kdipAXmSA.png"/></div></div></figure><ul class=""><li id="7a05" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">5D核而不是4D喜欢正则卷积。视差空间中的平移不变量:<em class="mf"> d </em>、<em class="mf"> d </em> +1之间的关系与<code class="fe pz qa qb qc b"><em class="mf">ď</em>, <em class="mf">ď</em>+1</code>之间的关系相同。</li></ul><p id="83fd" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">第四步:成本量对差异</strong></p><ul class=""><li id="049a" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">他们将最终的<code class="fe pz qa qb qc b"><em class="mf">H</em>✕<em class="mf">W</em>✕<em class="mf">D</em>(×1)</code>张量视为前softmax分数。</li><li id="39f0" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">因此，进行软最大值给出了可能的视差值的概率分布。</li><li id="2528" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">但是他们不会用分类损失来训练这个:因为一些错误分类比其他的更糟糕！(d=5而不是4比d=25而不是4要好)。</li></ul><p id="66ee" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">所以，他们对这个分布有期望！</p><p id="c6eb" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">损失函数</strong></p><ul class=""><li id="0105" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">使用<em class="mf"> soft argmin </em>获得GC-Net中提到的所有好处</li><li id="2b5a" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">GC-Net使用L₁作为惩罚指标</li></ul><p id="94a6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">○ L₁损失对异常值不太敏感</p><p id="0f17" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">○通过回归学习无界目标，L₂必须仔细调整</p><ul class=""><li id="c067" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">类似于快速R-CNN(即BB回归)，SPP使用以下平滑L₁变量</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rx"><img src="../Images/f44cad62246dcc31ffe4de366d356b6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d3ecwXDNUcvCz7U9FNRhPQ.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ry"><img src="../Images/138715ec57ada6084ff5e2e4bc7d0a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7bOZGqb6Gctu1MJofDLIKg.png"/></div></div></figure><ul class=""><li id="d013" class="pe pf it lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">使用类似GC-Net的视差回归来估计连续视差图。</li></ul><p id="433c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">○通过softmax运算σ()从预测成本<em class="mf"> c </em> ₔ计算每个差异<em class="mf"> d </em>的概率。</p><p id="3388" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">预测视差ᶺ <em class="mf"> d-hat </em>计算为每个视差<em class="mf"> d </em>的概率加权总和(2)</p><p id="3789" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">○上述视差回归比基于分类的立体匹配方法更鲁棒。</p><p id="d2f1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">●与R-CNN和SPPnet中使用的L₂损失相比，稳健的L₁损失对异常值不太敏感。当回归目标无界时，使用L₂损失的训练可能需要小心的学习率以防止爆炸梯度。情商。3结束这种敏感性。</p><h2 id="03a7" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated"><strong class="ak">结果</strong></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rg"><img src="../Images/cdd7c9c28400074a94685e3158ad106d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ord704RYBvBFjv8bcm7ZGw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">“KITTI 2012测试图像”的视差估计结果左图显示了立体图像对的左输入图像。PSMNet、GC-Net和MC-CNN获得的每个输入图像的视差显示在其误差图上方。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi rl"><img src="../Images/64563ef3797ea209e77006a35691e00d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DXBlxXXGiMEyeU0zRDo_NQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">为GC-Net及其前身显示的KITTI 2012测试图像的视差估计结果。左侧面板显示立体图像对的左侧输入图像。以下是PSMNet、GC-Net和MC-CNN针对每个输入图像获得的视差以及相应的误差图。</p></figure><h2 id="0041" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">结果</h2><h2 id="a6fe" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">摘要</h2><p id="9c9e" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated"><strong class="lc iu">优点</strong></p><p id="498b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">先前的DL立体匹配方法的一个问题是上下文信息。GC-Net是一个用于立体匹配的端到端学习框架，无需后处理，它采用编码器-解码器来合并多尺度特征以实现成本体积正则化。</p><p id="1a49" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">引入了金字塔池模块，用于将全局上下文信息合并到图像特征中</p><p id="99cd" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">一个堆叠沙漏3D有线电视新闻网，用于成本卷中上下文信息的扩展区域支持。</p><p id="240e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">缺点</strong></p><p id="b8d4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">大量足迹和耗时的推断</p><h2 id="3ce9" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">纸张和代码</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ps pt l"/></div></figure><div class="nq nr gp gr ns nt"><a href="https://github.com/JiaRenChang/PSMNet" rel="noopener  ugc nofollow" target="_blank"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd iu gy z fp ny fr fs nz fu fw is bi translated">GitHub - JiaRenChang/PSMNet:金字塔立体匹配网络(CVPR2018)</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">此库包含贾仁的“金字塔立体匹配网络”论文(CVPR 2018)的代码(PyTorch格式)</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">github.com</p></div></div><div class="oc l"><div class="rz l oe of og oc oh ks nt"/></div></div></a></div></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><h1 id="53b3" class="mh mi it bd mj mk qu mm mn mo qv mq mr jz qw ka mt kc qx kd mv kf qy kg mx my bi translated">ModuleNet</h1><p id="8d5e" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated">伦特里亚，奥克塔维奥&amp;奎瓦斯-泰洛，胡安-卡洛斯&amp;雷耶斯-菲格罗亚，阿兰&amp;里维拉，马里亚诺。ModuleNet:用于立体视觉的卷积神经网络。10.1007/978–3–030–49076–8_21.(2020).</p><h2 id="d4db" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">动机</h2><ol class=""><li id="687a" class="pe pf it lc b ld mz lg na lj qz ln ra lr rb lv rh pk pl pm bi translated">受FlowNet &amp; U-Net的启发，一个衡量任何范围内差异的模型。</li><li id="e722" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv rh pk pl pm bi translated">使用低计算时间算法来测量成本图。</li><li id="cb63" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv rh pk pl pm bi translated">架构很简单，因为它不需要任何额外的专门网络作为不同的FlowNet的变体进行优化。</li><li id="2635" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv rh pk pl pm bi translated">它改进了基线模型ELAS [3]和FlowNet c(<a class="ae mg" href="https://docs.google.com/document/d/1JcVa8AdmkE4U_07HQSr4R7K8uZ7NBVOJwz2F2ofQVIc/edit#heading=h.j2jcyz2h0mcs" rel="noopener ugc nofollow" target="_blank">FlowNet</a>的相关版本)，具有大约80%的非偏倚误差。</li></ol><p id="9e50" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">U-Net块由函数F₁: F₁表示，对噪声普查距离图进行正则化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sa"><img src="../Images/c9ed4de9652432e6508e809240084eae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lm9oF5d2xzbkAY4h5si30w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="kz">一般块(U-Net)捐赠为F </em> ₁ <em class="kz">接受距离张量</em> <strong class="bd ky"> <em class="kz"> D </em> </strong> <em class="kz">映射自</em> <strong class="bd ky"> <em class="kz"> </em> </strong> <em class="kz">图像对</em> <strong class="bd ky"> <em class="kz"> I </em> </strong> <em class="kz">然后转换为概率</em> <strong class="bd ky"> <em class="kz"> P </em> </strong> <em class="kz">。</em></p></figure><h2 id="2434" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">方法</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qf"><img src="../Images/35b76d2265859ca7775c47affb955bed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*e4i2jSrBt1l9K9gD"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="kz">普查转换</em></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi sb"><img src="../Images/186b28903d523fc8358ea6d7be211407.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/0*DzDtkc07-XSQrq7L"/></div></figure><p id="0c6e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">对于左图<em class="mf">我</em>ₗ；类似的，<em class="mf"> C </em> ᵣ为右像<em class="mf"> I </em> ᵣ.汉明距离(H)计算两个普查签名之间的不同位数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi sc"><img src="../Images/17ff68b74cfd054f5a5bcdee07233f85.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/0*C4yvc0mLee3VVvK3"/></div></figure><p id="63ae" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们举例说明具有代表性的₁. u-net</p><p id="8366" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">数学上:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi sd"><img src="../Images/4fd2fb0984c2bd3d6198db513992ccb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/0*-1LVHdscxcC3QSdC"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">距离张量中的信息。</p></figure><p id="7419" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu"> <em class="mf"> D </em> </strong>通过<em class="mf"> F </em> ₁.映射到概率<strong class="lc iu"> <em class="mf"> P </em> </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi se"><img src="../Images/5c9f0410128d013c8fd7f119d955dba9.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/0*yfdfk91-0-FFh8In"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">概率张量(模型输出)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi sf"><img src="../Images/5d76956b25ac7033795f143a8c96bd82.png" data-original-src="https://miro.medium.com/v2/resize:fit:202/0*v_e6i8tolOB10Mw5"/></div></figure><p id="2036" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">𝜃₁代表训练期间学到的重量。</p><p id="1c39" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">第二个U-Net可以改进主要(训练的)模块的输出。函数<em class="mf">f</em>₂<em class="mf">t53】也使用距离张量<em class="mf"> D </em>作为输入来提炼概率张量p:</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi sg"><img src="../Images/fcdfd18f75cb4b5d952607a5c926bf4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/0*7jWCSwUuNEegoBKu"/></div></figure><p id="cad5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">𝜃₂学会了举重。</p><p id="b895" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">因此，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi sh"><img src="../Images/611352502583e3ffaaed4a1a00b22eea.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/0*fjybqF198eZOdzi4"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qf"><img src="../Images/0de69d4eaad43f9f654e2d8916b9cd13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sQzwIBQVul6VTGNY"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="kz">基本块由两个U型网级联而成。</em></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi sg"><img src="../Images/1b7fd977f4a542db358a71b9c4e1eba9.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/0*2TROjIQp-hnDRrx2"/></div></figure><p id="f655" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">请注意，网络<em class="mf"> F i </em> s重新用于处理<em class="mf"> K </em>模块。</p><p id="993a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">最后是视差估计。</p><p id="3aab" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">通过在差异图<strong class="lc iu">ŷ</strong>中应用WTA程序来计算d⋆。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi si"><img src="../Images/cccfcee4d35f9822f5bb9460f6526295.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/0*pEvdepymoSvGsrDc"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qf"><img src="../Images/140b33b90406f9af90eb11b87f25e81f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8vB9B-qt3aDj8qZU"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="kz"> ModuleNet:模块化CNN模型。</em> <strong class="bd ky"> <em class="kz">来源:</em> </strong> <em class="kz">原创论文。</em></p></figure><h2 id="ac89" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">结果</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sj"><img src="../Images/ef9b271dcc454deedaf1f5d9fbf5fc47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aR043fZfjCywwdOd"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">所选场景的MPI Sintel数据集的结果。<strong class="bd ky"> <em class="kz">来源:</em> </strong> <em class="kz">原创论文。</em></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sk"><img src="../Images/2dc95be2c471dbebdfad5c20bf974e56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0P_AvFKf26Qmybtt"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">所选立体像对上的Middlebury数据集的结果。<strong class="bd ky"> <em class="kz">来源:</em> </strong> <em class="kz">原创论文。</em></p></figure><p id="ca1c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">注意:</strong>用于检测范围外差异的额外层有助于对添加噪声的像素进行分类，因为这些像素在工作范围之外或者是被遮挡的区域。</p><h2 id="4f88" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">摘要</h2><h2 id="af29" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">纸张和代码</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ps pt l"/></div></figure><p id="6567" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">代码不可用！:(</strong></p><p id="e7e7" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如果有，请分享！</p><h1 id="33d4" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">GA网</h1><h2 id="7461" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">动机</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sl"><img src="../Images/cb0882e79547c6de157839ec3619944f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OHdYIdm3XlY1E1AZ2Z1_DQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">性能插图。(a)具有挑战性的输入图像。(b) GC-Net [13]有19个3D卷积层用于匹配成本聚集。(c)GA-Net的结果仅使用两个建议的GA层和两个3D卷积层。它将匹配信息聚集到相当大的无纹理区域，并且比GC-Net更快。(d)基本事实。<strong class="bd ky"> <em class="kz">来源:</em> </strong> <em class="kz">原创论文。</em></p></figure><h2 id="2d65" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">方法</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sm"><img src="../Images/043e078593c5ed0b250252c31bcbaa3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nvr7CqrbW1dWlbFvI2ocxQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(a)架构概述。左和右图像输入通过权重共享特征提取管道。它由层叠的沙漏CNN通过级联连接而成。所提取的左和右图像特征形成了4D成本体，该成本体被馈送到成本聚集块，用于正则化、细化和视差回归。制导子网(绿色)为制导成本汇总(SGA和LGA)生成权重矩阵。(b) SGA层在四个方向上半全局地合计成本量。在视差回归之前使用LGA层，并且局部细化4D成本量若干次。<strong class="bd ky"> <em class="kz">来源:</em> </strong> <em class="kz">原创论文。</em></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sn"><img src="../Images/6785dd1ed6d39a68067b1c37c0e41759.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hf8FpbQlq3aRcpfQtqd8WA.png"/></div></div></figure><h2 id="2c0c" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">结果</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qj"><img src="../Images/f0d47779b64fa252fccf44c10d5691b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ybn8knVIE6XTCKOE810yfw.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi so"><img src="../Images/98a2238be646fb17eac57b169341c3a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*LZ3aPw4OULoVb6szxBmdvw.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi sp"><img src="../Images/a33ca2b29e66dacffa11bd3b9f83ee32.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*rtBRwuyntmA-n0pKEJjAlA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi sq"><img src="../Images/5f7aa54de95a092d67e19651d6d9c393.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*uMnxal9LNOJ_YdKWTiRY8g.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sr"><img src="../Images/c93a7680b6e2b01247f5d515438343d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GJW4chDYvdli7kyYXddl3g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">城市景观(上)和米德尔伯里(下)的样本结果。<strong class="bd ky">来源:</strong><a class="ae mg" href="https://github.com/feihuzhang/GANet/" rel="noopener ugc nofollow" target="_blank">https://github.com/feihuzhang/GANet/</a>。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi sp"><img src="../Images/c719f4b61ae98e01a8b2be1cfff09ad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*NqcFlbbVgxr3PBnk95UD0Q.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ss"><img src="../Images/63ee84f32b4b1a22c74783975377e576.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*xxYLH2xjcD9_Cl4CWjhizQ.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi st"><img src="../Images/bf8eb2d3467b3763d4fa8b942a15fd08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YeQ4YYTLSKzqVI4eHC-4eg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">结果可视化和比较。第一行:输入图像。第二行:GC-Net的结果[13]。第三行:PSMNet [3]的结果。最后一行:我们遗传网络的结果。蓝色箭头指出了显著的改进。引导聚合可以有效地将视差信息聚合到大的无纹理区域(例如，汽车和窗户)并给出精确的估计。它还可以聚合对象知识并保留深度结构(最后一列)。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi sp"><img src="../Images/5e9e48f53df17b026a427e8521aacf72.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*YkHTW_13kY6s-JUFkIelKA.png"/></div></figure><h2 id="d06b" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">摘要</h2><h2 id="42a4" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">纸张和代码</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ps pt l"/></div></figure><div class="nq nr gp gr ns nt"><a href="https://github.com/feihuzhang/GANet" rel="noopener  ugc nofollow" target="_blank"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd iu gy z fp ny fr fs nz fu fw is bi translated">GitHub - feihuzhang/GANet: GA-Net:端到端立体匹配的引导聚合网</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">GA-Net:端到端立体匹配的引导聚合网我们正在制定传统的几何和优化…</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">github.com</p></div></div><div class="oc l"><div class="su l oe of og oc oh ks nt"/></div></div></a></div><h1 id="139c" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">讨论</h1><h2 id="787d" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">摘要</h2><p id="374d" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated">我们看了深度学习时代立体视觉进步的几种基本方法。每种技术都有动机，技术上的审查，以及原始论文和官方源代码。</p><p id="389c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">不同于传统的调查，在传统的调查中，专家正确地写下了一个特定的主题，这里提供的是来自学习立体视觉的人的基于方法的概述。换句话说，这并不是要以任何方式取代同行评审调查，而是在一个不太正式的会议上提供一个更容易阅读的、针对具体方法的概述。如果你想查看我们发表的关于另一个计算机视觉主题的文献，请查看我们的<a class="ae mg" href="https://www.computer.org/csdl/journal/tp" rel="noopener ugc nofollow" target="_blank"> IEEE PAMI </a>调查。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi sv"><img src="../Images/a26c8d64bcf0786589f380e21a87f800.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fi077aqWvsZ6-PCgCV4aRQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">资料来源:<a class="ae mg" href="https://arxiv.org/pdf/2006.16033.pdf" rel="noopener ugc nofollow" target="_blank">视觉亲属关系分析和建模调查:酝酿中的十年</a>。由作者创作。</p></figure><h2 id="fcb0" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">未来的工作</h2><p id="ff65" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated">我们讨论了使用监控的基于图像的立体方法。本系列的后续部分将涵盖基于视频的(即多视图立体视觉)、多任务、自我监督、公共数据集、基准和基准，以及关于生成相应置信度映射的最后一部分。敬请期待！</p><h2 id="006f" class="ne mi it bd mj nf ng dn mn nh ni dp mr lj nj nk mt ln nl nm mv lr nn no mx np bi translated">结论</h2><p id="0ef6" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated">我们讨论的深度学习方法只是解决计算机视觉问题的许多新方法中的几个。如果你不了解深度学习基础，这篇博文可能不适合你。另一方面，如果您正在寻找将立体图像映射到深度图的端到端解决方案，无论是为了工作、个人项目还是研究，本博客系列可能是您的起点。无论如何，考虑在你的产品或服务中采用深度学习技术可能是值得的。上面讨论的哪个模型对你最有意义，或者是你的首选？您希望我在未来的部分中包含哪些方法？你喜欢什么，不喜欢什么？任何反馈、问题、请求等。，将不胜感激。</p><h1 id="5a8d" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">补充资源</h1><p id="63a0" class="pw-post-body-paragraph la lb it lc b ld mz ju lf lg na jx li lj nb ll lm ln nc lp lq lr nd lt lu lv im bi translated">PSM网络是使用PyTorch Lightning实现的。</p><div class="nq nr gp gr ns nt"><a href="https://pythonrepo.com/repo/xtliu97-PSMNet_PytorchLightning-python-deep-learning" rel="noopener  ugc nofollow" target="_blank"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd iu gy z fp ny fr fs nz fu fw is bi translated">py torch PSM-Net的重新实施:</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">这是一个基于嘉人昌/PSMNet的Pytorch闪电版PSMNet。使用python main.py开始训练…</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">pythonrepo.com</p></div></div><div class="oc l"><div class="sw l oe of og oc oh ks nt"/></div></div></a></div><p id="964b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">查看FlowNet上的深度博客:</p><div class="nq nr gp gr ns nt"><a rel="noopener follow" target="_blank" href="/a-brief-review-of-flownet-dca6bd574de0"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd iu gy z fp ny fr fs nz fu fw is bi translated">FlowNet简介</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">最近，细胞神经网络已经成功地用于估计光流。与传统方法相比，这些方法具有明显的优势</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">towardsdatascience.com。</p></div></div><div class="oc l"><div class="sx l oe of og oc oh ks nt"/></div></div></a></div><p id="33e1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">GA-Net:Youtube上端到端立体匹配的引导聚合网络。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="sy pt l"/></div></figure><h1 id="39db" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">补充的</h1><ol class=""><li id="66df" class="pe pf it lc b ld mz lg na lj qz ln ra lr rb lv rh pk pl pm bi translated"><a class="sz ta ep" href="https://medium.com/u/9d9e487d186?source=post_page-----7cb2c9ff325d--------------------------------" rel="noopener" target="_blank"> Dhanoop Karunakaran </a> aran编写了一个由3部分组成的博客系列，内容是使用深度学习(<a class="ae mg" href="https://medium.com/intro-to-artificial-intelligence/deep-learning-series-1-intro-to-deep-learning-abb1780ee20" rel="noopener">第1部分</a>)进行图像分类(<a class="ae mg" href="https://medium.com/intro-to-artificial-intelligence/simple-image-classification-using-deep-learning-deep-learning-series-2-5e5b89e97926?source=user_profile---------30-------------------------------" rel="noopener">第2部分</a>)和检测/定位图像中的交通标志(<a class="ae mg" href="https://medium.com/intro-to-artificial-intelligence/traffic-sign-detection-selefdriving-car-deep-learning-series-3-1db4eda67979" rel="noopener">第3部分</a>)。有大量具有足够深度和广度的资源，即使是忙于学习SOA的专家也能深入理解。因此，博客系列在相对较短的系列中提供了动机、视觉、数学和概念观点的良好平衡。张量流用于在证明和概念之间架起理解的桥梁。</li></ol><h1 id="d514" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">参考</h1><ol class=""><li id="c1a8" class="pe pf it lc b ld mz lg na lj qz ln ra lr rb lv rh pk pl pm bi translated">Y.LeCun、L. Bottou、Y. Bengio和P. Haffner。"基于梯度的学习应用于文档识别."IEEE 会议录，86(11):2278–2324，1998年11月。</li><li id="46d8" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv rh pk pl pm bi translated">Alex Krizhevsky，Ilya Sutskever和Geoffrey E. Hinton“使用深度卷积神经网络的图像网络分类”在<em class="mf">神经信息处理系统进展</em> (2012)。</li><li id="5c19" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv rh pk pl pm bi translated">Bahdanau、Dzmitry、Kyunghyun Cho和Yoshua Bengio。"通过联合学习对齐和翻译的神经机器翻译."在<em class="mf"> ICLR </em> (2015)。</li><li id="5b8a" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv rh pk pl pm bi translated">盖革、安德烈亚斯、马丁·罗瑟和拉克尔·乌尔塔森。“高效的大规模立体匹配。”亚洲计算机视觉会议。施普林格，柏林，海德堡，2010。</li><li id="41ca" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv rh pk pl pm bi translated">将立体视差和光流结合起来用于基本场景流。<em class="mf">商用车技术2018 </em>。斯普林格，2018。90–101.</li><li id="047e" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv rh pk pl pm bi translated">门兹、莫里茨和安德烈亚斯·盖格。"自动驾驶车辆的物体场景流."<em class="mf">IEEE计算机视觉和模式识别会议论文集</em>。2015.</li><li id="50eb" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv rh pk pl pm bi translated">翟，，等，“光流和场景流估计:综述”模式识别 114 (2021): 107861。</li><li id="66ce" class="pe pf it lc b ld pn lg po lj pp ln pq lr pr lv rh pk pl pm bi translated">罗恩伯格，奥拉夫，菲利普费舍尔和托马斯布罗克斯。" U-net:生物医学图像分割的卷积网络."<em class="mf">医学图像计算和计算机辅助介入国际会议</em>。施普林格，查姆，2015。</li></ol></div></div>    
</body>
</html>