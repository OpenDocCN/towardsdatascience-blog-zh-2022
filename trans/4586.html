<html>
<head>
<title>Trust Region Policy Optimization (TRPO) Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释了信任区域策略优化(TRPO)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2#2022-10-12">https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2#2022-10-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d710" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">强化学习算法TRPO建立在自然策略梯度算法的基础上，确保更新保持在“可信”的策略区域内</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a37dc19c468657d3dbfa19328ffd739c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RTWEO3zVJ1RP_zii"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@thirtyspoke?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Ronda Dorsey </a>拍摄的照片</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="bf31" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">描述OpenAI的信任区域策略优化(TRPO)算法的论文，作者是舒尔曼<em class="mc">等人</em>。(2015)，是现代强化学习的基础。它植根于Amari (1998)和Kakade (2001)关于自然政策梯度的早期工作，以及此后的改进。</p><p id="7e1a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">TRPO很快获得了欢迎和主流认可，因为根据经验，它比自然政策梯度算法表现得更好、更稳定。虽然它已经被最近的政策优化(PPO)超越，但它的贡献仍然很重要。</p><p id="4ffa" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">尽管我将简要地<strong class="li iu">重述一些核心概念</strong>，但我假设您已经了解了政策梯度和自然政策梯度方法。本文将阐述TRPO背后的单调改进定理(关注直觉)以及区别于自然政策梯度的三个实施变化。</p><h1 id="8775" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">政策梯度(一阶优化)</h1><p id="c873" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">政策梯度算法的主要思想相当简单。我们有一个参数化的策略<em class="mc"> π_θ </em>，它以一定的概率返回动作。如果我们观察到一个行为产生高于平均水平的回报，我们<strong class="li iu">会增加这个行为的可能性</strong>。</p><p id="c36d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为了指导更新，我们定义依赖于<em class="mc"> π_θ </em>的目标函数<em class="mc"> J(θ) </em>，并且<strong class="li iu">计算梯度</strong> <em class="mc"> ∇_θJ(θ) </em>(偏导数的向量)w.r.t .参数<em class="mc"> θ </em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/42cce7f5fe086e13f717a00de7f63635.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VChcmTuoomeMMacw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">目标函数，产生与随机政策成比例的国家行动轨迹。</p></figure><p id="81a5" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">通过对多个动作进行采样，我们可以将回报轨迹R(τ)相互对比——<strong class="li iu">导数告诉我们在哪个方向更新<em class="mc"> θ </em> </strong>。目标函数的斜率越陡，我们的更新就越大。</p><p id="bd61" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">概率分布<em class="mc">P(τ；θ) </em>既反映了随机政策，也反映了环境中嵌入的外生不确定性。幸运的是，我们可以推导出一个只依赖于策略<em class="mc">π_θ</em>T33】的<strong class="li iu">表达式。如果这个策略对于w.r.t. <em class="mc"> θ </em>是可微的，我们可以计算梯度<em class="mc"> ∇_θ </em>并优化目标函数。这产生了以下更新函数:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/0f452ca259da52230dadd302edcc7084.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*JGlxC5Spfo04JlYDxGtgPg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">传统的策略梯度更新函数，基于目标函数梯度∇_θJ(θ和步长α更新策略权重θ</p></figure><p id="0016" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">关键的挑战是<strong class="li iu">选择合适的学习速率<em class="mc">α</em>T3】。一阶导数告诉我们朝哪个方向走，但不是走多少。事实上，在一次更新中，<em class="mc"> α </em>的好值对于下一次更新可能是糟糕的。过冲和欠冲(见下图)在一阶优化中很常见；梯度只是不提供步长应该有多大的信息。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/576989a87d3afa52b579cfeebca59be4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jrxOdTjTTjzLoha_.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一阶梯度算法不提供关于适当步长的信息，并且经常过冲或失速。[图片由作者提供]</p></figure><p id="7559" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">有关策略梯度的更多详细信息，请查看以下文章:</p><div class="nd ne gp gr nf ng"><a rel="noopener follow" target="_blank" href="/policy-gradients-in-reinforcement-learning-explained-ecec7df94245"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd iu gy z fp nl fr fs nm fu fw is bi translated">强化学习中的策略梯度解释</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">了解所有关于基于似然比的政策梯度算法(加强):直觉，推导，和…</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">towardsdatascience.com</p></div></div><div class="np l"><div class="nq l nr ns nt np nu ks ng"/></div></div></a></div><h1 id="f92e" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">自然政策梯度(二阶优化)</h1><p id="6337" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">为了解决一阶优化的步长问题，自然策略梯度<strong class="li iu">还包括二阶导数</strong>，用于捕捉梯度相对于权重变化的灵敏度。本质上，它衡量的是政策<em class="mc">πθ</em>(回想一下，这是一个概率分布，在统计流形上移动)变化的程度。为此，我们计算更新前后策略之间的差异，称为KL-divergence:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/58a8826acd8b0d68a69de60fbb184062.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sN3aLK4PCCwz-FUM.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">KL-divergence衡量更新前后策略之间的距离。</p></figure><p id="1928" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">通过在散度上加上上限<em class="mc"> ϵ </em>，我们得到以下方案:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/f463378e18581e665762007c7575faad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*nVUptuu-22EpC88Y.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">权重更新方案，最大KL-散度为上限。</p></figure><p id="7c52" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">由于强化学习依赖于样本，我们在实践中执行了许多近似以使用自然梯度。最值得注意的是，我们对修改后的目标函数执行<strong class="li iu">泰勒展开</strong>，对目标函数使用一阶展开，对KL散度使用二阶展开。推导过程非常繁琐，所以我将直接跳到最终的权重更新公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/f9485fa86a08a552ecbed96fda5b1a31.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/0*BZmi0nmVr0LTCdDv.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自然政策梯度的权重更新公式</p></figure><p id="89f2" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们用一个依赖于目标函数的局部灵敏度的动态函数代替了<em class="mc"> α </em>，由费希尔信息矩阵<em class="mc"> F(θ) </em>表示。直观上，我们的目标是发散阈值<em class="mc">ϵ</em>t21】内的<strong class="li iu">最大更新。在平坦的区域，我们可以安全地执行较大的更新，在弯曲的区域，我们更加谨慎。因此，这种行为通常与一阶优化相反，小心翼翼地逼近奖励峰值。</strong></p><p id="8f0d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">有关自然政策梯度的详细解释，请查看以下文章:</p><div class="nd ne gp gr nf ng"><a rel="noopener follow" target="_blank" href="/natural-policy-gradients-in-reinforcement-learning-explained-2265864cf43c"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd iu gy z fp nl fr fs nm fu fw is bi translated">解释强化学习中的自然策略梯度</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">传统的政策梯度方法存在根本缺陷。自然梯度收敛得更快更好，形成…</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">towardsdatascience.com</p></div></div><div class="np l"><div class="ny l nr ns nt np nu ks ng"/></div></div></a></div><h1 id="4844" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">自然政策梯度的问题</h1><p id="eabd" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">通过限制政策变化并考虑当地的敏感性，自然梯度往往比普通梯度收敛得更好更快。然而，这种方法也有许多缺点。</p><p id="3770" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">首先，虽然提供了一个迷人的理论框架，自然梯度需要大量的近似。二阶泰勒展开可能会错误地表示政策之间的实际距离，导致步长过大。因此，自然政策梯度<strong class="li iu">不能保证满足KL-divergence的约束！</strong>在实践中，人们仍可能采取过大的步骤并违反约束。</p><p id="cf62" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">第二，费希尔信息矩阵f是一个<em class="mc"> |θ|⋅|θ| </em>矩阵，它可能需要大量内存来存储。然而，更具限制性的是计算逆矩阵<em class="mc"> F^-1 </em>，这是一个<em class="mc"> O(N ) </em>复杂度的<strong class="li iu">运算。对于包含数千甚至数百万个参数的神经网络，这在计算上变得非常困难。</strong></p><p id="1d49" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">第三，尽管偏离理论框架很远，自然梯度算法并不检查更新是否实际上改善了目标。由于近似值，建议的权重更新实际上可能会恶化性能。</p><p id="f4a7" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">总而言之，自然政策梯度面临以下关键问题:</p><ul class=""><li id="2928" class="nz oa it li b lj lk lm ln lp ob lt oc lx od mb oe of og oh bi translated">由于近似，可能违反<strong class="li iu"> KL约束</strong>，导致分析得出的步长过大。</li><li id="41d2" class="nz oa it li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">逆矩阵<strong class="li iu"> <em class="mc"> F^-1 </em>计算</strong>耗时太长，是一个<em class="mc"> O(N ) </em>复杂度的运算。</li><li id="25e2" class="nz oa it li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">我们没有检查<strong class="li iu">更新是否真的改善了政策</strong>。由于所有的近似值，情况可能并非如此。</li></ul><p id="21e0" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">TRPO通过一些有针对性的调整，在一定程度上解决了这三个问题。在处理实现更改之前，有必要讨论一些理论。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/4599f4a4483dec7b49a7f8d6bc346754.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mNPeOwe6iXw_601lRLql0Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梯度搜索(一阶优化)与信赖域搜索(二阶优化)的对比图。前者遵循最陡的斜率(有过冲或失速的风险)，而后者根据统计流形的曲率调整其搜索空间[图片由作者改编自<a class="ae ky" href="https://unsplash.com/@davealmine?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Dawid za wia</a>on<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a>]</p></figure><h1 id="b813" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">信赖域策略优化(TRPO)——理论</h1><p id="375e" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">如果你理解自然政策梯度，实际变化应该是全面的。为了充分理解它们，最好理解一下<strong class="li iu">单调改进定理</strong>。</p><p id="0522" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们从夺回<strong class="li iu">优势</strong> <em class="mc"> A(s，a) </em>开始，这是一个重塑的奖励信号。强化学习中的样板材料。优势表示——对于给定的策略<em class="mc"> π </em>和给定的状态<em class="mc">s</em>——特定行为<em class="mc"> a </em>的预期累积回报减去整体期望值。因此，优势描述了该行为的相对吸引力。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/881dc76f31c1ddc4e1328ff8516154aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pbhUlYWNamKq-2CE_Z8rUA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">优势功能。直观地说，优势描述了与状态s中特定动作A相关的Q值与s中所有动作的Q值之间的差异。正优势意味着该动作的“优于预期”性能。</p></figure><p id="66c8" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">随机政策影响轨迹中的行动选择<em class="mc"> τ </em>，环境决定状态的出现。因此，我们可以根据当前政策下生成的轨迹来计算预期回报。目标函数<em class="mc"> J(π_θ) </em>描述了这些折扣奖励，期望源于政策<em class="mc"> π_θ </em>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/90ea994aa6c752ba9f1590829e966d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*9jycHmnbsnX2CjFIV0kIzg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">目标函数，写为无限时间范围内的预期贴现回报。</p></figure><p id="c98b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为了描述两种政策的预期报酬的差异，我们可以取原政策的预期报酬并<strong class="li iu">加上新政策的预期优势</strong>【Kakade&amp;Langford(2002)或Schulman <em class="mc"> et al. </em> (2015)的证明】。该表达式采用新策略下调整后的行动概率，但采用旧策略下计算的优势(即，我们不必重新采样):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/f84ea7899f774ba95ea8b6e0c02e46fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*zda0M76OtTJgTE-mcmcsXw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">更新政策的目标函数，用当前政策和预期优势表示</p></figure><p id="ae01" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">由于时间范围是无限的，上述可以用状态分布<em class="mc">ρ_(π_θ+δθ)</em>来重新表示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/94064fd6e32884d85528f13fe17b078e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*eESRibE-taoK0CcwUQRwVQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表示保单质量的变化，将更新保单的预期优势加到当前保单的预期价值上。</p></figure><p id="7491" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">术语<em class="mc">ρ_(π_θ+δθ)</em>是有问题的——如果不采样，我们如何知道对应于更新策略的状态分布？为了解决这个问题，我们利用状态分布，就像在当前策略下发生的一样，代价是<strong class="li iu">引入近似误差</strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/86322cc2a3c5f6a6f3844051d3770c8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*gOE9KKMr-TblrZtAeJZvtg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">策略质量变化的近似值，使用当前策略π_θ下的状态分布，而不是更新后的策略<em class="ot">π_θ+δθ</em>。近似可能会引入误差。</p></figure><p id="1209" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">接下来，我们必须让事情适合模拟，因为我们没有完整的状态和动作集。我们用一个期望符号来代替<em class="mc">ρ</em>——可以使用蒙特卡罗模拟来采样——并使用<strong class="li iu">重要性采样来代替动作之和</strong>。通过重要性抽样，我们有效地利用了当前政策的行动预期，并对新政策下的概率进行了修正:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/5d84b2fbdcccaa8d7639a8f211edc774.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*evTq8MYRU9AOgbPwl8Y-3A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用状态样本和重要性抽样，为蒙特卡罗模拟调整目标函数。</p></figure><p id="25dd" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">描述更新策略相对于旧策略的质量的预期优势被称为替代优势<em class="mc">𝓛_π_θ(π_θ+δθ】</em>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/5489bc47895d565d1ddb8d42ff8cdb48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QKwO8f035cDWDDoRw_f0CA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">替代优势𝓛的定义，使用来自策略π_θ的样本描述更新策略π_θ+δθ的预期优势</p></figure><p id="41ed" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">近似误差可以用两种策略之间最坏情况下的KL-散度来表示。因此，我们可以表示以下不等式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/ae6017902b55e87be569fc8b551f189a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*Rx-CzmscLPPExhDdSCn4rA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">描述更新后目标值最小变化的不等式。右手边表示下限，考虑到对发散误差进行了修正的替代优势。</p></figure><p id="c6cf" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">舒尔曼<em class="mc">等人</em>推导出<em class="mc"> C </em>的值，并以此作为目标函数改进的下限。因此，如果我们改进了右侧，我们保证也会改进左侧。实质上，如果替代优势<em class="mc">𝓛_π_θ(π_θ+δθ</em>超过最坏情况近似误差<em class="mc"> C⋅D_KL </em>，我们肯定会改进目标。</p><p id="9e7b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这就是简而言之的单调改进定理。相应的程序是<strong class="li iu">最小化最大化(MM)算法</strong>。如果我们提高下限<em class="mc"> M=𝓛_π_θ-C⋅D_KL </em>，我们也提高目标至少相同的数量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/39ac7cbd36a1966b8c22c3613495f3da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jEJ55nyHSIuyy-9UwicC2Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MM算法的说明。当改进下限L(θ)-C⋅KL时，我们保证也改进目标函数η(θ)[来源:<a class="ae ky" href="https://drive.google.com/file/d/0BxXI_RttTZAhMVhsNk5VSXU0U3c/view?resourcekey=0-6NrgDm29IIPlXsPESX2w4w" rel="noopener ugc nofollow" target="_blank">舒尔曼，OpenAI，2017 </a> ]</p></figure><p id="8183" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这一理论结果令人放心，但<strong class="li iu">并不过分实用</strong>。首先，<em class="mc"> D_KL^max </em>(即最大散度)很难计算，对于模拟实现，我们用预期(例如平均)散度来代替它。第二，所提出的算法严厉地惩罚策略偏差，因此产生非常小的保守更新。</p><p id="87df" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">尽管有实际的缺点，单调改进定理为充分理解TRPO的机制提供了重要的背景信息。</p><h1 id="b836" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">信任区域策略优化(TRPO) —实践</h1><p id="2587" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">在实际实现方面，TRPO与早期的自然政策梯度算法并没有太大的不同。有三个主要的改进<strong class="li iu">，每个都解决了原始算法中的一个问题。请注意，TRPO旨在验证提议的更新是否实际上改善了我们的算法，符合单调改善定理。</strong></p><h2 id="1c37" class="oy me it bd mf oz pa dn mj pb pc dp mn lp pd pe mp lt pf pg mr lx ph pi mt pj bi translated">一、共轭梯度法[<em class="ot">x=f^-1∇logπ_θ(x】</em></h2><p id="7f82" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">从自然梯度，我们已经导出了以下等价关系，允许将费希尔信息矩阵表示为我们已经拥有的梯度的外积(即，我们不必计算包含所有二阶导数的海森矩阵):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/6e551fb4ea6807e7f5d560a294fe82ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0XIcdsrIkiJ1KXpI.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">费希尔信息矩阵可以计算为梯度的外积。</p></figure><p id="5e36" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">不幸的是，计算费希尔逆矩阵<em class="mc">f^-1(θ</em>是一个耗时且数值不稳定的过程。特别是对于神经网络，一个<em class="mc"> |θ|⋅|θ| </em>矩阵可以变得非常大，一个<em class="mc"> O(θ ) </em>矩阵是不可行计算的。</p><p id="72f2" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">好消息是:我们对逆矩阵本身不感兴趣。如果你检查方程式，我们只需要乘积<em class="mc"> F^-1∇logπ_θ(x) </em>。如果我们可以计算<em class="mc">x=f^-1∇logπ_θ(x)</em>——或者<em class="mc">f⋅x=∇logπ_θ(x)</em>——我们实际上不需要求逆。</p><p id="3093" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">进入<strong class="li iu">共轭梯度法</strong>。解释整个过程远远超出了本文的范围，但可以说这是一个既定的数值过程来近似<em class="mc"> x </em>。通过这种方式，我们可以避开计算倒数的需要。共轭梯度一般在<em class="mc"> |θ| </em>步内收敛(通常要小得多)，这样我们就可以处理大型矩阵。</p><p id="dafd" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">共轭梯度的使用并不是TRPO所独有的——例如，截断的自然政策梯度使用了相同的方法——但仍然是算法的一个组成部分。</p><h2 id="2c67" class="oy me it bd mf oz pa dn mj pb pc dp mn lp pd pe mp lt pf pg mr lx ph pi mt pj bi translated">二。线搜索[D _ KL(π_θ| |π_θ+δθ)≤δ]</h2><p id="895e" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">尽管自然梯度提供了给定KL散度约束的最佳步长，但是由于所有的近似，我们实际上可能不满足该约束。</p><p id="e0a8" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">TRPO通过执行线搜索——与典型的梯度搜索不同— <strong class="li iu">,迭代地减小更新的大小</strong>,直到第一次更新不违反约束，从而解决了这个性能问题。该过程可以被视为收缩信任区域，即，我们信任更新实际上改进目标的区域。</p><p id="395b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对于减少，使用指数衰减率<em class="mc"> α^j </em>，其中<em class="mc"> 0 &lt; α &lt; 1 </em>和<em class="mc"> j∈N </em>。如果第一次更新(<em class="mc"> α⁰=1 </em>)满足条件，我们保留原始的自然梯度步长。如果没有，我们继续缩小信任区域，直到更新令人满意。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/28f6cef19fd88b240cc13c047da6383a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RU2cHrMG4feOQnFd.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线搜索算法。该算法迭代地减小更新的大小(使用指数衰减α^j)，直到散度落在阈值内(II)并且代理优势是非负的(iii)[来源:<a class="ae ky" href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf" rel="noopener ugc nofollow" target="_blank"> Joshua Achuam，UC Berkeley </a></p></figure><p id="3edf" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">因此，与自然政策不同的是，<em class="mc">假定满足</em>发散约束，而在<strong class="li iu"> TRPO中执行的线搜索实际上实施了它</strong>。还要注意，单调改进理论利用了<em class="mc">惩罚</em>，而实现使用了<em class="mc">约束</em>(即信任区域)。</p><p id="b58c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">从伪代码中我们看到<em class="mc"> D_KL(θ||θ_k)≤δ </em>并不是唯一必须满足的约束。它还断言<em class="mc"> 𝓛_θ ≥0 </em>。这是改进检查，我们接下来将对此进行解释。</p><h2 id="f4f4" class="oy me it bd mf oz pa dn mj pb pc dp mn lp pd pe mp lt pf pg mr lx ph pi mt pj bi translated">三。改进检查[𝓛(θ)≥0]</h2><p id="04a7" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">最后的区别很有意义。与其说<em class="mc">假设</em>更新会提高代理优势<em class="mc">𝓛(θ</em>，我们实际上<em class="mc">检查</em>它。回想一下，我们基于旧策略计算优势，使用重要性抽样来调整概率。当然，检查需要一些时间，但是在接受它之前，验证我们的更新是否确实改进了策略是令人放心的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/8449427246f809bba5fcb7396ebe355c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3bOuhvM8261eKtFGxuj20w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TRPO算法的伪代码。TRPO执行一个共轭梯度算法，一个约束样本KL-divergence的线搜索和一个关于提高代理优势的检查[来源:<a class="ae ky" href="https://spinningup.openai.com/en/latest/algorithms/trpo.html" rel="noopener ugc nofollow" target="_blank"> OpenAI </a>，麻省理工学院许可]</p></figure><h1 id="3cea" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">结束语</h1><p id="e25c" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">与传统的梯度算法相比，自然策略梯度提供了相当大的改进，传统的梯度算法难以确定适当的学习速率，并且在学习时经常停滞或超调。TRPO再次向前迈出了重要的一步，主要是通过简单的<strong class="li iu">检查一个更新是否是一个改进</strong>。共轭梯度近似使得TRPO适合于具有大量参数的策略，尽管这种方法在早期的工作中已经被提出。</p><p id="65a2" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">根据经验，TRPO在许多任务上优于早期的自然梯度算法，但它本身也不是没有缺陷:</p><ul class=""><li id="e8b0" class="nz oa it li b lj lk lm ln lp ob lt oc lx od mb oe of og oh bi translated">虽然不再需要计算<em class="mc"> F^-1 </em>，但是估计<em class="mc"> F </em>仍然是一项繁琐的任务(需要内存和大量的样本批次)。对于深度神经网络，TRPO不具有伸缩性；</li><li id="753b" class="nz oa it li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">共轭梯度算法是容易得到的，但是它们的结合使得RL算法的实现更加复杂；</li><li id="2403" class="nz oa it li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">TRPO在利用卷积或循环网络的任务中表现不佳(CNN和RNN)；</li><li id="05d5" class="nz oa it li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">作为二阶算法，它明显比一阶算法慢，并且没有充分利用ADAM等优秀的优化器；</li><li id="a463" class="nz oa it li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">当架构有多个输出时——神经网络输出策略和价值函数的情况很常见——没有一致的度量标准可供使用；</li><li id="6f0b" class="nz oa it li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">总的来说，TRPO很难解释和说明，使得实现、调试和培训都很困难。</li></ul><div class="nd ne gp gr nf ng"><a rel="noopener follow" target="_blank" href="/proximal-policy-optimization-ppo-explained-abed1952457b"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd iu gy z fp nl fr fs nm fu fw is bi translated">解释了最近策略优化(PPO)</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">从强化到连续控制中的go-to算法</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">towardsdatascience.com</p></div></div><div class="np l"><div class="pn l nr ns nt np nu ks ng"/></div></div></a></div><p id="2a5f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae ky" href="https://medium.com/p/abed1952457b" rel="noopener">近似策略近似法</a>——也是OpenAI在2017年推出的——在经验上与TRPO等方法具有竞争力(甚至优于TRPO ),而且实现起来也简单得多。因此，<em class="mc">事实上已经</em>取代了PPO。尽管如此，<strong class="li iu"> TRPO代表了自然政策梯度发展中的一个重要里程碑</strong><strong class="li iu"/>，值得强化学习领域的学生关注。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="14f3" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="mc">喜欢这篇文章？你可能也会喜欢下面的RL文章:</em></p><div class="nd ne gp gr nf ng"><a rel="noopener follow" target="_blank" href="/the-five-building-blocks-of-markov-decision-processes-997dc1ab48a7"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd iu gy z fp nl fr fs nm fu fw is bi translated">马尔可夫决策过程的五个组成部分</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">通过掌握马尔可夫决策的基本原则来定义和交流您的强化学习模型…</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">towardsdatascience.com</p></div></div><div class="np l"><div class="po l nr ns nt np nu ks ng"/></div></div></a></div><div class="nd ne gp gr nf ng"><a rel="noopener follow" target="_blank" href="/why-reinforcement-learning-doesnt-need-bellman-s-equation-c9c2e51a0b7"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd iu gy z fp nl fr fs nm fu fw is bi translated">为什么强化学习不需要贝尔曼方程</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">强化学习中著名的贝尔曼方程和MDP公式的再评价</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">towardsdatascience.com</p></div></div><div class="np l"><div class="pp l nr ns nt np nu ks ng"/></div></div></a></div><div class="nd ne gp gr nf ng"><a rel="noopener follow" target="_blank" href="/cliff-walking-problem-with-the-discrete-policy-gradient-algorithm-59d1900d80d8"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd iu gy z fp nl fr fs nm fu fw is bi translated">基于离散策略梯度算法的悬崖行走问题</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">用Python实现了一个完整的增强算法。手动执行这些步骤来说明内部…</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">towardsdatascience.com</p></div></div><div class="np l"><div class="pq l nr ns nt np nu ks ng"/></div></div></a></div><h1 id="c200" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">进一步阅读</h1><p id="c635" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">论文:</p><ul class=""><li id="1b28" class="nz oa it li b lj lk lm ln lp ob lt oc lx od mb oe of og oh bi translated">阿马里，S. I. (1998年)。<a class="ae ky" href="https://d1wqtxts1xzle7.cloudfront.net/32944066/Amari_-_Natural_Gradient_works_efficiently_in_learning-with-cover-page-v2.pdf?Expires=1662047457&amp;Signature=KiOeWpPHdAWivpd0vlFhsg4bbZt~AmV1GA5ZL9ZwUMdjDPzzLB7fMeFiaBOQ11wYGgW5MKXX-gGgGgqQms3aHjpGYivUeN~U0xeXBwceDoz6jh4~7i8G--7rdavlU0HVj1sxX1vrQUSk~qVXqbsjKm-wCtwuk9o6jXUfeJql1PGTv65T70akN6rWVPUb3mW7Xw8drbgBfDsUh1AWCrczpSuWUIZG4n~gCzmGzfqb~Jfj3zBy~WbzSLNFUgMoT3FsrI-fq0~gMkmyXKThnd~4gY00k445Y0HKQmHjNq5RO9~OCr9~kx1tTvks6Zd9BBjtUBmHPAywLiQzPHWPEAf0-A__&amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA" rel="noopener ugc nofollow" target="_blank">自然渐变在学习中很有效。</a> <em class="mc">神经计算</em>，<em class="mc"> 10 </em> (2)，251–276。</li><li id="cc60" class="nz oa it li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">Kakade，S. M. (2001年)。<a class="ae ky" href="https://proceedings.neurips.cc/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf" rel="noopener ugc nofollow" target="_blank">天然的政策梯度。</a> <em class="mc">神经信息处理系统进展</em>，<em class="mc"> 14 </em>。</li><li id="8ee6" class="nz oa it li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">s . m . kakade和j . John Langford(2002年)。<a class="ae ky" href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf" rel="noopener ugc nofollow" target="_blank">近似最优近似强化学习。</a>在<em class="mc">国际机器学习会议</em>。</li><li id="c22c" class="nz oa it li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">舒尔曼，j .，莱文，s .，阿贝耳，p .，乔丹，m .，莫里茨，P. (2015)。<a class="ae ky" href="http://proceedings.mlr.press/v37/schulman15.pdf" rel="noopener ugc nofollow" target="_blank">信托区域政策优化。</a>参加<em class="mc">机器学习国际会议。</em></li></ul><p id="b650" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">讲座幻灯片:</p><ul class=""><li id="c8a1" class="nz oa it li b lj lk lm ln lp ob lt oc lx od mb oe of og oh bi translated">高级政策梯度(CS 285)。加州大学伯克利分校。</li><li id="0423" class="nz oa it li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">Achiam，J. (2017年)。<a class="ae ky" href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf" rel="noopener ugc nofollow" target="_blank">高级政策梯度方法。</a>加州大学伯克利分校。</li><li id="f8c9" class="nz oa it li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">自然政策梯度，TRPO，PPO (CMU 10703)。卡内基梅隆。</li><li id="7da6" class="nz oa it li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">舒尔曼，J. (2017)。<a class="ae ky" href="https://drive.google.com/file/d/0BxXI_RttTZAhMVhsNk5VSXU0U3c/view?resourcekey=0-6NrgDm29IIPlXsPESX2w4w" rel="noopener ugc nofollow" target="_blank">高级政策梯度方法:<br/>自然梯度、TRPO等。</a> OpenAI。</li></ul><p id="75ed" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">博客帖子:</p><ul class=""><li id="4075" class="nz oa it li b lj lk lm ln lp ob lt oc lx od mb oe of og oh bi translated">自然渐变(TRPO，PPO)。[ <a class="ae ky" href="https://julien-vitay.net/course-deeprl/notes/3.6-PPO.html" rel="noopener ugc nofollow" target="_blank">链接</a></li><li id="1c9b" class="nz oa it li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">OpenAI (2018)。信任区域策略优化。[ <a class="ae ky" href="https://spinningup.openai.com/en/latest/algorithms/trpo.html#id2" rel="noopener ugc nofollow" target="_blank">链接</a></li></ul></div></div>    
</body>
</html>