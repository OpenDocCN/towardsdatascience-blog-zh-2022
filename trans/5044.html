<html>
<head>
<title>Efficient K-means Clustering Algorithm with Optimum Iteration and Execution Time</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有最优迭代和执行时间的高效K-均值聚类算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/efficient-k-means-clustering-algorithm-with-optimum-iteration-and-execution-time-9358a794406c#2022-11-09">https://towardsdatascience.com/efficient-k-means-clustering-algorithm-with-optimum-iteration-and-execution-time-9358a794406c#2022-11-09</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="eb12" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">高效K-means聚类算法的实现:研究成果</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/44e7db18a8653224b279be28d0ba771f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b6JNsq5BlnF_4rrXNcqCrQ.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">改进的K-means聚类算法与其他现有方法的比较[1]</p></figure><h2 id="f9cd" class="kz la iu bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">动机</strong></h2><p id="d8a1" class="pw-post-body-paragraph lv lw iu lx b ly lz jv ma mb mc jy md li me mf mg lm mh mi mj lq mk ml mm mn in bi translated">回到2019年，我在读研究生，教授K-means聚类算法，作为机器学习课程的一部分。我很快抓住了算法的基本主题。该算法很有趣，因为它为每个聚类随机假设质心，并迭代直到它收敛到最终的聚类。不管初始质心是什么！它总是落在同一个星团里。在常规过程中，迭代次数随机变化。从那时起，我开始思考，如果我能修正初始质心，算法的迭代和执行时间将大大减少。我做了很多探索来寻找一种方法来选择最佳的初始质心。我分析过很多类似的作品。并找到了一个名为<strong class="lx iv"> <em class="mo"> kmeans++的标准方法。不幸的是，n </em>方法之一提供了最佳常数迭代。我和两个朋友以及我的主管一起，开始想出不同的点子来寻找可能的解决方案。我们还用现有的方法测试了我们产生的想法。经过一年的研究，我们提出了一个想法，即<strong class="lx iv">在执行时间和迭代次数方面优于其他方法。</strong>我们刚刚使用了<strong class="lx iv">百分位和PCA(主成分分析)</strong>来确定最佳初始质心。在接下来的部分中，我将通过实际操作来讨论所提出的方法。</strong></p><p id="c16c" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated"><em class="mo"/><strong class="lx iv"><em class="mo">n . b .—</em></strong><em class="mo">它已经由<strong class="lx iv"> Springer Nature </strong>发表在</em> <strong class="lx iv"> <em class="mo">【数据科学年鉴】</em> </strong>期刊上。<a class="ae mu" href="https://link.springer.com/article/10.1007/s40745-022-00428-2" rel="noopener ugc nofollow" target="_blank"> <em class="mo">全文在此处。</em> </a> ]</p></div><div class="ab cl mv mw hy mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="in io ip iq ir"><h2 id="191c" class="kz la iu bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">目录</h2><ol class=""><li id="ca80" class="nc nd iu lx b ly lz mb mc li ne lm nf lq ng mn nh ni nj nk bi translated"><code class="fe nl nm nn no b"><a class="ae mu" href="#26c5" rel="noopener ugc nofollow"><strong class="lx iv">Clustering Algorithm at a Glance</strong></a></code></li><li id="fc13" class="nc nd iu lx b ly np mb nq li nr lm ns lq nt mn nh ni nj nk bi translated"><code class="fe nl nm nn no b"><a class="ae mu" href="#7bb5" rel="noopener ugc nofollow"><strong class="lx iv">K-means Clustering Algorithm Overview</strong></a></code></li><li id="16a6" class="nc nd iu lx b ly np mb nq li nr lm ns lq nt mn nh ni nj nk bi translated"><code class="fe nl nm nn no b"><a class="ae mu" href="#7382" rel="noopener ugc nofollow"><strong class="lx iv">How the Proposed Method Works</strong></a></code></li><li id="9d57" class="nc nd iu lx b ly np mb nq li nr lm ns lq nt mn nh ni nj nk bi translated"><code class="fe nl nm nn no b"><a class="ae mu" href="#603e" rel="noopener ugc nofollow"><strong class="lx iv">Step By Step Implementation with Python</strong></a></code></li><li id="89e9" class="nc nd iu lx b ly np mb nq li nr lm ns lq nt mn nh ni nj nk bi translated"><code class="fe nl nm nn no b"><a class="ae mu" href="#3e5b" rel="noopener ugc nofollow"><strong class="lx iv">Comparison with the Traditional Method</strong></a></code></li><li id="c45f" class="nc nd iu lx b ly np mb nq li nr lm ns lq nt mn nh ni nj nk bi translated"><code class="fe nl nm nn no b"><a class="ae mu" href="#d0ac" rel="noopener ugc nofollow"><strong class="lx iv">Multidimensional Cluster Visualization with Parallel Coordinates</strong></a></code></li></ol></div><div class="ab cl mv mw hy mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="in io ip iq ir"><h2 id="26c5" class="kz la iu bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">聚类算法一览</h2><p id="6ad3" class="pw-post-body-paragraph lv lw iu lx b ly lz jv ma mb mc jy md li me mf mg lm mh mi mj lq mk ml mm mn in bi translated">当“集群”这个词来的时候，让我想起了一件简单的事情:<strong class="lx iv">“把<em class="mo">相似类型的物体组合在一起</em>”</strong>它是无监督学习的一部分。聚类问题是无监督的学习问题，因为这些算法对未标记的数据(无目标值)起作用。算法的主要任务是“根据相似的特性或特征对数据进行分组”</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nu"><img src="../Images/97b8256ea0f2d564acc612f3c3d71f4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*YAbD04utHs_iqIV736zS1w.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="c1c5" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">看看上图就知道了。这是一个二维图像。假设虚线表示具有两个特征的每个实例(<em class="mo"> X，Y </em>)。它清楚地显示了聚类是基于最小距离(相似类型的要素)形成的。存在许多聚类技术，包括K-means聚类、DBSCAN、凝聚层次聚类、高斯混合模型算法等。其中，K-means聚类被广泛应用。</p><h2 id="7bb5" class="kz la iu bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">k-均值聚类算法概述</h2><p id="6ac7" class="pw-post-body-paragraph lv lw iu lx b ly lz jv ma mb mc jy md li me mf mg lm mh mi mj lq mk ml mm mn in bi translated">首先，k-means聚类算法为每个聚类随机选择质心。然后计算每个点到质心的距离。基于最小距离创建聚类。然后计算每个聚类的平均点，并将其作为新的质心。重复相同的过程，直到质心收敛到一个固定点。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nv"><img src="../Images/1249f64f7f96db5d8689e4c84f520240.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*gqP3q7QOg9jOq7MFbQGLNw.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">k均值聚类算法[1]</p></figure><p id="0a41" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">我们来看看论文[1]中提到的算法。它非常精确地描述了整个过程。</p><h2 id="7382" class="kz la iu bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">提议的方法如何工作</h2><p id="32d7" class="pw-post-body-paragraph lv lw iu lx b ly lz jv ma mb mc jy md li me mf mg lm mh mi mj lq mk ml mm mn in bi translated">主算法我们什么都没修改。我们所做的只是提出了一种有效选择初始质心的方法。让我们看看下面给出的流程图。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nw"><img src="../Images/9b1ac9e89bec95bc9f21ea46a9d0ce70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*ecz19X5CWiJ_DcqU3E1WsA.png"/></div></figure><p id="bde0" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">让我一步一步地解释方法。</p><ol class=""><li id="599b" class="nc nd iu lx b ly mp mb mq li nx lm ny lq nz mn nh ni nj nk bi translated"><strong class="lx iv">数据集读取:</strong>对于每个机器学习模型，我们都需要读取数据集。所提出的方法需要相同的过程。当我们在下一步中应用PCA时，我们必须将所有的值转换成数字。</li><li id="a3c5" class="nc nd iu lx b ly np mb nq li nr lm ns lq nt mn nh ni nj nk bi translated"><strong class="lx iv">主成分分析(PCA)</strong>步骤都是关于降维的。这些特征应该归纳为两个部分。将使用PCA来完成这项工作。有了这两个特性，我们可以轻松地对数据进行水平或垂直分段。这就是为什么我们使用两种成分的PCA。</li><li id="c2e3" class="nc nd iu lx b ly np mb nq li nr lm ns lq nt mn nh ni nj nk bi translated"><strong class="lx iv">应用百分位数概念分离数据:</strong>百分位数是一种统计技术，通过它我们可以将数据分成100个不同的部分。每个部分保存1%的数据。要了解更多关于百分位的细节，你可以阅读以下文章— ( <a class="ae mu" rel="noopener" target="_blank" href="/ultimate-guide-to-statistics-for-data-science-a3d8f1fd69a7"> <strong class="lx iv"> <em class="mo">数据科学统计终极指南</em> </strong> </a>)</li><li id="3f95" class="nc nd iu lx b ly np mb nq li nr lm ns lq nt mn nh ni nj nk bi translated"><strong class="lx iv">数据集拆分和均值计算:</strong>这一步是最重要的一步。在上一步中，我们只是应用了百分位数。现在，我们需要使用百分位数分割数据集。例如，我们希望使用K-means聚类算法创建4个聚类，因此K=4。根据该方法，我们将根据第一部分<strong class="lx iv"> <em class="mo">将数据集分成4等份(第一部分0% — 25%，第二部分25% — 50%，第三部分50% — 75%，第四部分75% — 100%)。</em> </strong>接下来我们将<strong class="lx iv"> <em class="mo"> </em> </strong>通过映射指标提取各部分的主要数据。之后，将计算每个部分的平均值。</li></ol><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oa"><img src="../Images/486290e83caa88ceee7c83a242573d92.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*Th2dAiuR1oPU6xOaeAXahQ.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="6539" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">上图显示了一个spit零件的演示数据集。其中，<strong class="lx iv"> <em class="mo"> X1，X2…代表特性，D11，D21…表示实例值。</em> </strong>意思是将计算如下——</p><blockquote class="ob oc od"><p id="a299" class="lv lw mo lx b ly mp jv ma mb mq jy md oe mr mf mg of ms mi mj og mt ml mm mn in bi translated">平均值X1=(D11+D12+D13+D14)/4</p><p id="e221" class="lv lw mo lx b ly mp jv ma mb mq jy md oe mr mf mg of ms mi mj og mt ml mm mn in bi translated">平均X2=(D21+D22+D23+D24)/4</p><p id="cdf2" class="lv lw mo lx b ly mp jv ma mb mq jy md oe mr mf mg of ms mi mj og mt ml mm mn in bi translated">平均X3=(D31+D32+D33+D34)/4</p><p id="9c04" class="lv lw mo lx b ly mp jv ma mb mq jy md oe mr mf mg of ms mi mj og mt ml mm mn in bi translated">平均值X4=(D41+D42+D43+D44)/4</p><p id="3547" class="lv lw mo lx b ly mp jv ma mb mq jy md oe mr mf mg of ms mi mj og mt ml mm mn in bi translated">平均值X5=(D51+D52+D53+D54)/4</p></blockquote><p id="c7dd" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">聚类的初始质心将是<strong class="lx iv"> <em class="mo">的值(平均X1，平均X2，平均X3，平均X4，平均X5)。</em> </strong> <em class="mo">用同样的过程，将确定另外3个质心。</em></p><p id="57e9" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">5.<strong class="lx iv">质心确定并馈入K-means算法:</strong>我们在<strong class="lx iv"> <em class="mo">数据集分割和均值计算</em> </strong>步骤中获得了最佳初始质心。是时候将质心提供给主K-means聚类算法了。其余的过程与传统的K-means聚类算法相同。为了方便起见，我还包括了该方法的算法。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oh"><img src="../Images/da6e04120cf5537035f3c799873d37f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*HW0D1-JSiHWXqOxVkKRpTg.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">高效K均值聚类的建议算法[1]</p></figure><h2 id="603e" class="kz la iu bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">用Python逐步实现</h2><ul class=""><li id="fb2b" class="nc nd iu lx b ly lz mb mc li ne lm nf lq ng mn oi ni nj nk bi translated"><strong class="lx iv">导入必要的Python库。</strong></li></ul><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="oj ok l"/></div></figure><ul class=""><li id="8f9a" class="nc nd iu lx b ly mp mb mq li nx lm ny lq nz mn oi ni nj nk bi translated"><strong class="lx iv">读取数据集和一些预处理</strong></li></ul><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="ol ok l"/></div></figure><p id="8ee0" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">出于演示目的，我使用了<a class="ae mu" href="https://www.kaggle.com/datasets/abineshkumark/carsdata" rel="noopener ugc nofollow" target="_blank"> <strong class="lx iv"> cars.csv </strong> </a>数据集，可从<a class="ae mu" href="https://www.kaggle.com/datasets/abineshkumark/carsdata" rel="noopener ugc nofollow" target="_blank"><strong class="lx iv"><em class="mo">Kaggle</em></strong></a>获得。您也可以使用其他聚类数据集。</p><p id="e9da" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated"><em class="mo">让我们看看数据集的整体信息。</em></p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="om ok l"/></div></figure><p id="eb1e" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">上面的单元格清楚地显示了一些非数字列，如<strong class="lx iv"> <em class="mo"> cubicinches、heavy bs和brand。</em></strong><strong class="lx iv">品牌</strong>列保存分类值，如<em class="mo">美国、欧洲和日本。</em>我们将映射分类值以将其转换为数值。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="on ok l"/></div></figure><p id="82c3" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">现在，我们将删除包含分类值的<code class="fe nl nm nn no b"><strong class="lx iv">brand </strong></code>列。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="oo ok l"/></div></figure><p id="27e1" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">关于数据的当前信息。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="om ok l"/></div></figure><p id="e7cf" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">尽管如此，我们仍然有两个非数字的列<strong class="lx iv"><em class="mo">cubic inch和weightbs</em></strong>，它们是数值，但被指定为字符串。让我们把它转换成数值。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="op ok l"/></div></figure><p id="17c1" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">数据集当前状态的概述。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="oq ok l"/></div></figure><p id="526a" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">我们可以看到所有的值都是数字，但是有一些丢失的值需要小心处理。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="or ok l"/></div></figure><p id="c244" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">这段代码将处理丢失的值。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="on ok l"/></div></figure><p id="4aee" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">下图显示了数据的分布情况。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="os ok l"/></div></figure><p id="292f" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated"><code class="fe nl nm nn no b"><strong class="lx iv"><em class="mo">Output</em></strong></code></p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj ot"><img src="../Images/5aa405a7b0f16231c49de8ad566c6fbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5WUSjWxsU0tsmTw8u7ap3w.png"/></div></div></figure><ul class=""><li id="d12b" class="nc nd iu lx b ly mp mb mq li nx lm ny lq nz mn oi ni nj nk bi translated"><strong class="lx iv">为数据集寻找K值的最佳数量</strong></li></ul><p id="6e10" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">我已经用<strong class="lx iv"> <em class="mo">肘法</em> </strong>找到了最佳<strong class="lx iv"> K </strong>值。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="ou ok l"/></div></figure><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="ov ok l"/></div></figure><p id="9544" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">最佳簇的数量是3，因为肘部的弯曲从该点开始。</p><p id="6cd6" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated"><em class="mo">【注意——肘击法不是建议技术的一部分。我用它来寻找最佳聚类，因为我们正在处理一个随机数据集。而且我们不知道有多少个聚类适合这个数据集。】</em></p><ul class=""><li id="ea7f" class="nc nd iu lx b ly mp mb mq li nx lm ny lq nz mn oi ni nj nk bi translated"><strong class="lx iv">应用具有两种成分的主成分分析，并将数据可视化</strong></li></ul><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="ow ok l"/></div></figure><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="ox ok l"/></div></figure><ul class=""><li id="6bdc" class="nc nd iu lx b ly mp mb mq li nx lm ny lq nz mn oi ni nj nk bi translated"><strong class="lx iv">应用百分位数分离数据</strong></li></ul><p id="8e15" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">我们将创建3个集群。因此，我们需要将数据集分成3个相等的部分。百分位计算在100%以内。因此，每个群集中100/3=33.3%的数据可能是相等的。值得一提的是，百分点将应用于第一个PCA成分 的<strong class="lx iv"> <em class="mo">，因为它保存了最多的信息。</em></strong></p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="oy ok l"/></div></figure><p id="dffe" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">让我们看看我是如何分割数据集的。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="oz ok l"/></div></figure><ul class=""><li id="5908" class="nc nd iu lx b ly mp mb mq li nx lm ny lq nz mn oi ni nj nk bi translated"><strong class="lx iv">从主数据中提取分离的数据</strong></li></ul><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pa ok l"/></div></figure><p id="08f2" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">在上面的代码中，我根据PCA的第一个组成部分将数据分成3个相等的部分。但是我们不会从PCA分量中计算质心，因为可能会有巨大的信息丢失的可能性。因此，我们将数据合并到预处理的<code class="fe nl nm nn no b"><strong class="lx iv"><em class="mo">car </em></strong></code>数据框架中，以提取包含原始数据集所有特征的数值数据。上面代码的<code class="fe nl nm nn no b">first, second and third </code>变量包含了每个分割的数据。</p><ul class=""><li id="3145" class="nc nd iu lx b ly mp mb mq li nx lm ny lq nz mn oi ni nj nk bi translated"><strong class="lx iv">平均值计算和质心选择</strong></li></ul><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pa ok l"/></div></figure><p id="1b33" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">在这一步中，我们计算了每个数据分割部分的平均值，并将其视为质心。最后，我们在K-means聚类算法中加入了质心。出于测试目的，该过程已经执行了10次。我已经在<code class="fe nl nm nn no b"> iteration and time_execution</code>列表中记录了建议方法的迭代次数和执行时间。</p><h2 id="3e5b" class="kz la iu bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">与传统方法的比较</h2><p id="e279" class="pw-post-body-paragraph lv lw iu lx b ly lz jv ma mb mc jy md li me mf mg lm mh mi mj lq mk ml mm mn in bi translated">用传统的K-means聚类算法训练模型。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pb ok l"/></div></figure><p id="dfdd" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">我已经在<code class="fe nl nm nn no b">iteration_default </code>和<code class="fe nl nm nn no b">execution_time_default</code>列表中记录了执行时间和迭代。</p><p id="609a" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated"><em class="mo">我们来画个对比图进行迭代。</em></p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pc ok l"/></div></figure><p id="08f6" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">它清楚地表明，我们提出的方法工作于常数迭代，因为我们的质心是固定的；另一方面，传统的K-means聚类算法表现出任意的迭代。并且在每种情况下，该方法的迭代次数都是最小的。</p><p id="b3bc" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated"><em class="mo">执行时间方面的比较。</em></p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pc ok l"/></div></figure><p id="98e8" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">实验结果表明，在大多数情况下，该模型的性能优于传统方法。</p><h2 id="d0ac" class="kz la iu bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">具有平行坐标的多维集群可视化</h2><p id="7157" class="pw-post-body-paragraph lv lw iu lx b ly lz jv ma mb mc jy md li me mf mg lm mh mi mj lq mk ml mm mn in bi translated">我们的数据集中有超过3个维度。实际上我们无法想象它。这就是为什么我会用平行坐标来绘制这些簇。</p><p id="feb3" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated"><em class="mo">【解释平行坐标的概念不在本文讨论范围内。如果你想了解一下，点击</em> <a class="ae mu" href="https://www.analyticsvidhya.com/blog/2021/11/visualize-data-using-parallel-coordinates-plot/" rel="noopener ugc nofollow" target="_blank"> <em class="mo">这里</em> </a> <em class="mo">就知道了。】</em></p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pd ok l"/></div></figure><p id="16fb" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated">这里，<code class="fe nl nm nn no b"><strong class="lx iv"><em class="mo">0, 1, 2</em></strong></code>代表单个集群。</p><h2 id="c2e0" class="kz la iu bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">结论</h2><p id="4255" class="pw-post-body-paragraph lv lw iu lx b ly lz jv ma mb mc jy md li me mf mg lm mh mi mj lq mk ml mm mn in bi translated">机器学习模型很复杂，需要强大的计算能力。如果我们能找到任何方法来降低它的计算复杂性，它将发挥重要作用。由于所提出的方法优于其他方法，它也将节省计算成本。</p><p id="5cec" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated"><strong class="lx iv"> <em class="mo">【注意——你可以引用参考部分所示的方法。】</em> </strong></p></div><div class="ab cl mv mw hy mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="in io ip iq ir"><p id="0c80" class="pw-post-body-paragraph lv lw iu lx b ly mp jv ma mb mq jy md li mr mf mg lm ms mi mj lq mt ml mm mn in bi translated"><code class="fe nl nm nn no b"><strong class="lx iv"><em class="mo">Full notebook is available here.</em></strong></code></p><div class="pe pf gq gs pg ph"><a href="https://deepnote.com/@md-zubair/Efficient-K-means-Clustering-for-data-driven-modelling-a6ca9f6e-5f62-4914-a16d-6ead5c57ae36" rel="noopener  ugc nofollow" target="_blank"><div class="pi ab fp"><div class="pj ab pk cl cj pl"><h2 class="bd iv gz z fq pm fs ft pn fv fx it bi translated">用于数据驱动建模的高效K-均值聚类</h2><div class="po l"><h3 class="bd b gz z fq pm fs ft pn fv fx dk translated">用于数据驱动建模的高效K-means聚类。在这里，品牌栏包含…</h3></div><div class="pp l"><p class="bd b dl z fq pm fs ft pn fv fx dk translated">deepnote.com</p></div></div><div class="pq l"><div class="pr l ps pt pu pq pv kt ph"/></div></div></a></div><h2 id="a400" class="kz la iu bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">参考</h2><ol class=""><li id="4d8e" class="nc nd iu lx b ly lz mb mc li ne lm nf lq ng mn nh ni nj nk bi translated">祖拜尔，m .，伊克巴尔，M.A .，希尔，A. <em class="mo">等</em>一种面向高效数据驱动建模的改进K均值聚类算法。<em class="mo">安。数据。Sci。</em> (2022)。<a class="ae mu" href="https://doi.org/10.1007/s40745-022-00428-2" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1007/s40745-022-00428-2</a></li></ol></div><div class="ab cl mv mw hy mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="in io ip iq ir"><div class="kk kl km kn gu ph"><a href="https://mzh706.medium.com/subscribe" rel="noopener follow" target="_blank"><div class="pi ab fp"><div class="pj ab pk cl cj pl"><h2 class="bd iv gz z fq pm fs ft pn fv fx it bi translated">每当Md. Zubair发表文章时，就收到一封电子邮件。</h2><div class="po l"><h3 class="bd b gz z fq pm fs ft pn fv fx dk translated">每当Md. Zubair发表文章时，就收到一封电子邮件。通过注册，您将创建一个中型帐户，如果您还没有…</h3></div><div class="pp l"><p class="bd b dl z fq pm fs ft pn fv fx dk translated">mzh706.medium.com</p></div></div><div class="pq l"><div class="pw l ps pt pu pq pv kt ph"/></div></div></a></div><div class="pe pf gq gs pg ph"><a href="https://mzh706.medium.com/membership" rel="noopener follow" target="_blank"><div class="pi ab fp"><div class="pj ab pk cl cj pl"><h2 class="bd iv gz z fq pm fs ft pn fv fx it bi translated">通过我的推荐链接加入Medium-MD . Zubair</h2><div class="po l"><h3 class="bd b gz z fq pm fs ft pn fv fx dk translated">阅读Md. Zubair的每一个故事你的会员费直接支持Md. Zubair和你阅读的其他作家。你会…</h3></div><div class="pp l"><p class="bd b dl z fq pm fs ft pn fv fx dk translated">mzh706.medium.com</p></div></div><div class="pq l"><div class="px l ps pt pu pq pv kt ph"/></div></div></a></div></div></div>    
</body>
</html>