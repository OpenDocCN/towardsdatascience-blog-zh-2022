<html>
<head>
<title>Music Genre Classification Using a Divide &amp; Conquer CRNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于分而治之CRNN的音乐流派分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/music-genre-classification-using-a-divide-conquer-crnn-2ff1cf49859f#2022-09-12">https://towardsdatascience.com/music-genre-classification-using-a-divide-conquer-crnn-2ff1cf49859f#2022-09-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="eecb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一种适用于小数据集的有效方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f14a643629d68763c460c0310e4ce65e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G-YfNRTp4m1f-b_GcR-hjQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><h1 id="78a2" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">音乐流派分类中的C(R)NNs</h1><p id="7ac2" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">自从CNN在2012年开始在图像处理领域爆发以来，这些网络很快被应用于音乐流派分类，并取得了巨大的成功！今天，在所谓的<em class="mm">频谱图</em>上训练CNN已经成为最先进的技术，取代了几乎所有以前使用的基于手工制作的特征、MFCCs和/或支持向量机(SVM)的方法。</p><p id="51cd" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">最近几年，已经开始显示将LSTMs或GRUs之类的循环层添加到经典CNN架构中会产生更好的分类结果。对此的直观解释是，卷积层计算出什么和什么时候，而递归层发现什么和什么时候之间有意义的关系。这种架构被称为<strong class="ls iu">卷积递归神经网络(CRNN) </strong>。</p><h1 id="79c9" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">分治分类</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/27ca9dd00b86f77f2eac34c1a0d8c297.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*E_rmLlirzayUViA9QS8qyw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">AI生成的图像(稳定扩散1.5)。提示:“一群互相争斗的人，这样他们就不会联合起来对抗一个人”，分而治之的军事定义。图片作者。</p></figure><p id="2615" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">当我第一次了解《分而治之》时，我很困惑，因为我是在军事背景下听说的。军方的定义是</p><blockquote class="mt mu mv"><p id="b6f1" class="lq lr mm ls b lt mn ju lv lw mo jx ly mw mp mb mc mx mq mf mg my mr mj mk ml im bi translated">“使一群人意见相左，互相争斗，这样他们就不会联合起来反对一个人”(【www.merriam-webster.com】<a class="ae mz" href="http://www.merriam-webster.com" rel="noopener ugc nofollow" target="_blank">)</a></p></blockquote><p id="2619" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">然而，这几乎是<strong class="ls iu">与我们的目的</strong>相反的意思！参见<em class="mm">表1 </em>，其中列出了计算机科学和音乐流派分类中divide &amp;征服背后的三步过程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/68b5e40d091d2d2a9fbdbc314c578624.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1X_p02t9f2QFcL68giMorw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表1 —计算机科学和音乐流派分类中的分而治之。</p></figure><p id="0ef1" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">虽然定义不一样，但它们确实有很多重叠，而且我个人认为“分治”这个术语在这两种情况下都非常合适。在音乐流派分类中，该术语最早由董(2018)使用(据我所知)。然而，其他研究人员如Nasrullah和赵(2019)也使用了这种方法-只是没有使用相同的名称。</p><p id="9cd2" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">通常，流派分类仅仅基于一个30秒的片段。这部分是因为常用的音乐数据源如<a class="ae mz" href="https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification" rel="noopener ugc nofollow" target="_blank"> GTZAN </a>或<a class="ae mz" href="https://github.com/mdeff/fma" rel="noopener ugc nofollow" target="_blank"> FMA </a>数据集或Spotify Web API提供了这样长度的曲目。然而，应用分治法有三大优势:</p><h2 id="7bf1" class="nb kz it bd la nc nd dn le ne nf dp li lz ng nh lk md ni nj lm mh nk nl lo nm bi translated">1.更多数据</h2><p id="1fe1" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">假设您有完整长度的音轨，那么获取一个30秒的片段并丢弃剩余的3-4分钟的音轨是非常低效的。使用分治法，可以使用大多数音频信号。此外，通过允许切片之间的重叠，甚至可以绘制更多的片段。事实上，我认为这是一种自然的、相当无缝的数据扩充形式。</p><p id="bb19" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">例如，与每个音轨绘制一个30秒的片段相比，如果您绘制一个3秒的片段并与1秒重叠，您可以从3分钟的音轨中获得超过80倍的训练数据。即使你只有30秒的音轨，你也可以从每个音轨中获得14个3秒的片段和1秒的重叠。</p><h2 id="46c1" class="nb kz it bd la nc nd dn le ne nf dp li lz ng nh lk md ni nj lm mh nk nl lo nm bi translated">2.低维数据</h2><p id="bc61" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">一个30秒的片段产生了一个相当大的声谱图。在常用参数下，一个谱图可以有(1290 x 120)的形状，即<strong class="ls iu">超过15万个数据点</strong>。自然，一个3秒的片段，同样的参数会产生一个只有<strong class="ls iu">15k个数据点</strong>的~(129 x 120)声谱图。根据您使用的机器学习模型和架构，这可以显著降低模型的复杂性。</p><p id="41de" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">边注</strong>:如果你不知道什么是频谱图，或者它们为什么以及如何用于音频分类，我推荐<a class="ae mz" href="https://medium.com/x8-the-ai-community/audio-classification-using-cnn-coding-example-f9cbd272269e" rel="noopener">这篇由<a class="nn no ep" href="https://medium.com/u/27bf142e87f1?source=post_page-----2ff1cf49859f--------------------------------" rel="noopener" target="_blank">实验作家</a>撰写的文章</a>，它给出了一个漂亮而直观的解释。</p><h2 id="47de" class="nb kz it bd la nc nd dn le ne nf dp li lz ng nh lk md ni nj lm mh nk nl lo nm bi translated">3.与音频输入长度无关</h2><p id="076f" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">如果你想在现实世界中应用你训练过的模型，你会遇到各种长度的轨迹。你不必费力去寻找合适的30秒片段，而是开始画3秒的片段，直到覆盖整个音轨。如果输入的音轨长度不到30秒呢？你的模型是否足够健壮来处理10秒的广告词，这些广告词被零填充以达到30秒？分而治之，这不是问题。</p><h2 id="1463" class="nb kz it bd la nc nd dn le ne nf dp li lz ng nh lk md ni nj lm mh nk nl lo nm bi translated">不足之处</h2><p id="f565" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">使用分治法有三个主要缺点。首先，它增加了额外的处理步骤来分割音频文件并执行完整音轨的聚合预测。此外，基于片段的方法需要轨道式的训练-验证-测试分离，以避免相互关联的训练、验证和测试数据集。</p><p id="dacd" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">幸运的是，这两个步骤都已经包含在我的单标签音频处理管道<a class="ae mz" href="https://github.com/MaxHilsdorf/single_label_audio_processing_pipeline" rel="noopener ugc nofollow" target="_blank"> <em class="mm"> SLAPP </em> </a>中，在GitHub 上<strong class="ls iu">可以免费获得。</strong></p><p id="8046" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">最后，单个片段的预测通常使用某种多数投票进行汇总。这样，你的模型就完全忘记了任何超过3秒钟的音乐关系。除非您为聚合过程开发另一个复杂的元分类器。</p><h2 id="8ee1" class="nb kz it bd la nc nd dn le ne nf dp li lz ng nh lk md ni nj lm mh nk nl lo nm bi translated">何时使用分而治之</h2><p id="fd8a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">如果你有大量的数据，并且不需要更多的数据，或者如果你想分析在较长时间内展开的音乐结构，你可能不想使用分治法。但是，如果您的数据有限，并且希望用它来构建一个健壮而灵活的分类器，请考虑这种令人兴奋而有效的方法！</p><h1 id="68a9" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">利用SLAPP进行数据处理</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/3190da165120ba0b30b8dda6b2112baf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*xSoegOew7zjFS-Hq-9_GoA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">AI生成的图像(稳定扩散1.5)。提示:“功能强大的音乐处理机器的概念艺术”。图片作者。</p></figure><h2 id="bc7e" class="nb kz it bd la nc nd dn le ne nf dp li lz ng nh lk md ni nj lm mh nk nl lo nm bi translated">什么是SLAPP？</h2><p id="f9f1" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">本文也作为我新开发的<a class="ae mz" href="https://github.com/MaxHilsdorf/single_label_audio_processing_pipeline" rel="noopener ugc nofollow" target="_blank"> <strong class="ls iu">单标签音频处理流水线(SLAPP) </strong> </a> <strong class="ls iu">的展示。</strong>该工具自动化了单标签音频分类任务的整个数据处理工作流程。这包括将轨道分割成片段，计算光谱图，执行轨道方向的列车验证测试分割，等等。</p><p id="1ae7" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">由于在家用电脑上处理音频数据需要很长时间，SLAPP允许你在任何时候关闭电脑并重新加载你的进度，而不会有任何明显的时间或数据损失。在为我的学士论文开发了这个管道之后，我在几个分类任务上尝试了一下，从中获得了很大的乐趣和成功。</p><p id="4d20" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在<a class="ae mz" href="https://github.com/MaxHilsdorf/single_label_audio_processing_pipeline" rel="noopener ugc nofollow" target="_blank"> <strong class="ls iu"> GitHub </strong> </a> <strong class="ls iu"> </strong>上查看SLAPP，亲自试用<strong class="ls iu">。</strong></p><h2 id="e1ca" class="nb kz it bd la nc nd dn le ne nf dp li lz ng nh lk md ni nj lm mh nk nl lo nm bi translated">使用SLAPP处理GTZAN</h2><p id="b8ce" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><a class="ae mz" href="https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification" rel="noopener ugc nofollow" target="_blank"> GTZAN </a>是一个众所周知的公开可用的流派分类数据集，以<strong class="ls iu">来自10个不同流派的100首30秒长的曲目为特色</strong>。这个数据集非常适合我们的目的，因为它背后有大量的研究，而且它只有有限的数据。</p><p id="0002" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">为了使用SLAPP，你需要把你的MP3(不是WAV)文件放在一个文件夹结构中，就像图1中的<em class="mm">所示。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/23473547e68b107213d38600ff03e806.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GfBmy6OesxPL8EnHqLijlA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1 —使用SLAPP所需的文件夹结构示例。图片作者。</p></figure><p id="83ae" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">因为GTZAN就是这样的文件夹结构(耶！)，我们现在需要做的就是将WAV文件转换成MP3文件，我们就可以开始了。我建议您通过遍历目录，并使用</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="5ae6" class="nb kz it nr b gy nv nw l nx ny">from pydub import AudioSegment</span><span id="ddce" class="nb kz it nr b gy nz nw l nx ny">AudioSegment.from_wav("/input/file.wav").export("/output/file.mp3", format="mp3")</span></pre><p id="dcca" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">接下来，您需要做的就是通过导航到您想要的目录并使用以下命令来克隆SLAPP存储库:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="d53c" class="nb kz it nr b gy nv nw l nx ny"><strong class="nr iu">git clone </strong>https://github.com/MaxHilsdorf/single_label_audio_processing_pipeline</span></pre><p id="8861" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">确保您的系统满足使用SLAPP的所有要求；例如，Python库<a class="ae mz" href="https://github.com/jiaaro/pydub" rel="noopener ugc nofollow" target="_blank"> Pydub </a>和<a class="ae mz" href="https://librosa.org/doc/latest/index.html" rel="noopener ugc nofollow" target="_blank"> Librosa </a>以及音频编解码器<a class="ae mz" href="https://ffmpeg.org/download.html" rel="noopener ugc nofollow" target="_blank"> FFmpeg </a>(在资源库中有详细描述)。</p><p id="6ccf" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">现在，在SLAPP中，我们打开“pipeline_parameters.json”并像这样设置参数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="36f4" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这样，我们从GTZAN内的每个30秒的轨道中提取多达14个3秒的片段，允许1秒的重叠。通过运行“build_dataset.py”构建数据集，然后通过运行“process_dataset.py”对数据集进行归一化和混洗。这就是你要做的一切。SLAPP会帮你处理剩下的事情。</p><p id="98b8" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">验证和测试各占10%，您将获得<strong class="ls iu"> 11，186个训练谱图</strong>和1，386个验证和测试数据的谱图。每个谱图的形状为(130 x 100)。</p><p id="d11a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">旁注</strong>:如果你不只是阅读，而是实际编码，这个处理步骤会花一些时间，因为有很多计算步骤。在此过程中的任何时候，您都可以停止脚本并重新启动它们，从您停止的地方继续。如果你在SLAPP上遇到任何技术问题，请随时告诉我，因为这是我的第一个软件版本，可能会有很多错误，或者我的文档可能不够。</p><h1 id="0b01" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">构建和培训CRNN</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/7c88adc49c2bc8d521bda8a4e2ba25ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*zSBh1xaGSbVs7PV-6Sdbzg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">AI生成的图像(稳定扩散1.5)。提示:“音符的绘画建造了一个巨大的宗教寺庙”。图片作者。</p></figure><h2 id="0bf8" class="nb kz it bd la nc nd dn le ne nf dp li lz ng nh lk md ni nj lm mh nk nl lo nm bi translated">模型结构</h2><p id="c92c" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">使用Keras库，我构建了这样一个CRNN:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="8206" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这种CRNN架构背后的思想是使用卷积层+最大池将频率维度压缩为标量值，同时将时间维度保持为向量。这允许我们将门控递归单元(GRU)应用于由卷积块提取的特征。据我所知，这种类型的建筑是由Choi等人(2017)提出的，并已被Nasrullah和Zhao (2018)以及Bisharad和Laskar (2019)等多项研究采用。</p><p id="83cb" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">有关所用架构的详细概述，请参见图2 。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/186c1de83ae3e7a028f38246637ae794.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*wLyuC12be-AXiO-SEG7nLw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2 —所用CRNN模型的架构。</p></figure><p id="7c1b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在开发阶段，我尝试了该架构的不同变体，并比较了这些模型之间的验证损失。这种特殊的模型仅用大约40万个参数就取得了<strong class="ls iu">的最高结果。</strong></p><p id="7c31" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">边注:</strong>在这种情况下，SLAPP会给你一个形状的训练数据张量(11186，100，130)。然而，为了训练这个特定的CRNN，输入需要被整形为(11186，130，100，1)，有效地交换时间轴和频率轴(numpy.swapaxes)并增加另一个维度(numpy.expand_dims)。</p><h2 id="da35" class="nb kz it bd la nc nd dn le ne nf dp li lz ng nh lk md ni nj lm mh nk nl lo nm bi translated">模特培训</h2><p id="f6e6" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我用Adam optimizer训练模型，学习率为0.0001，分类交叉熵损失，批量为32。此外，我使用了Keras提供的EarlyStopping、ReduceLROnPlateau和ModelCheckpoint回调来确保训练顺利进行，并且只保存最好的模型。</p><p id="ff22" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">正如您在<em class="mm">图3 </em>中看到的，训练指标达到了接近90%的精度和召回分数，而验证精度和召回分数收敛在62%和73%之间。鉴于我们只处理3秒钟的音频和10个平衡的类(通过随机猜测→ 10%的准确性)，这已经相当令人印象深刻了。总体而言，本次训练运行中的最佳模型实现了67.32% 的<strong class="ls iu">验证准确率。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/65f407ca3c58e24036ff31010df7d9eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jt3BamX2YXhn9Hhk1F-JQA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3-CRNN训练运行的训练和验证指标。图片作者。</p></figure><p id="eb1c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">为了获得更稳定的结果，开始了10次训练运行并进行评估，以获得感兴趣的指标的平均分数和标准偏差。请记住，本文的目的不是提供复杂的统计评估，而是使用一个示例来展示分治分类的有效性。</p><h1 id="0096" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">片段与基于轨迹的分类</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/55520d5a487f706a1cafc13410dfb1c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*lxo4mTpZVCiA6xw4faVavA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">AI生成的图像(稳定扩散1.5)。提示:“陪审团严格审查一个有趣案件的照片，黑暗的气氛”。图片作者。</p></figure><h2 id="f1ff" class="nb kz it bd la nc nd dn le ne nf dp li lz ng nh lk md ni nj lm mh nk nl lo nm bi translated">如何计算基于轨迹的预测？</h2><p id="bf6f" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">对于基于音轨的预测，我将测试数据集中的每个音轨作为MP3文件加载。测试轨道名称由SLAPP自动写入您的数据目录中的“train_val_test_dict.json”。接下来，我使用的过程如下:</p><ol class=""><li id="0736" class="oe of it ls b lt mn lw mo lz og md oh mh oi ml oj ok ol om bi translated"><strong class="ls iu">将音轨</strong>分割成尽可能多的片段(在这种情况下，3秒的片段有2秒的重叠)。</li><li id="2a36" class="oe of it ls b lt on lw oo lz op md oq mh or ml oj ok ol om bi translated">将每个片段转换成适合训练模型的输入形状的<strong class="ls iu"> mel光谱图</strong>。</li><li id="9e77" class="oe of it ls b lt on lw oo lz op md oq mh or ml oj ok ol om bi translated">获取每个片段的原始类<strong class="ls iu">预测。</strong></li><li id="ac78" class="oe of it ls b lt on lw oo lz op md oq mh or ml oj ok ol om bi translated"><strong class="ls iu">对所有片段中每个类的原始分数</strong>求和。</li><li id="8d38" class="oe of it ls b lt on lw oo lz op md oq mh or ml oj ok ol om bi translated">您的全局预测是具有<strong class="ls iu">最高得分总和</strong>的班级。</li></ol><p id="101e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">由于这对于初学者来说可能会变得相当复杂，我将为您提供另一个模块来自动化这个基于轨迹的预测过程:请在这里找到该模块及其文档<a class="ae mz" href="https://github.com/MaxHilsdorf/divide_conquer_track_analyzer" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="a223" class="nb kz it bd la nc nd dn le ne nf dp li lz ng nh lk md ni nj lm mh nk nl lo nm bi translated">分类结果</h2><p id="bda1" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我对上面描述的模型进行了十次训练和评估，并对准确性得分进行了平均，以消除该过程的一些随机性。</p><p id="230c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">参见<em class="mm">图4 </em>获取准确度分数的直观概述。我们注意到的第一件事是，该模型在验证和测试数据上的表现几乎和<strong class="ls iu">一样好，这是一个好迹象。此外，所有的标准偏差都很小，这告诉我们，我们真的可以在平均精度的差异。</strong></p><p id="8306" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">很容易看出，基于音轨的分类优于基于片段的分类，平均相差约12.2个百分点。虽然为了简单起见，我们不打算做任何统计测试，但不可否认的是，在这种情况下，通过divide&amp;convert进行的<strong class="ls iu">基于轨迹的分类要优于</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/1f911a3a9e406320797da707df2f5662.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eHbEcRYc_uxpELwb_OVnFA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4 —基于片段和基于跟踪的分类的准确性和标准偏差(10次测试)。图片作者。</p></figure><h2 id="9029" class="nb kz it bd la nc nd dn le ne nf dp li lz ng nh lk md ni nj lm mh nk nl lo nm bi translated">如果我们使用完整的30秒片段会怎么样？</h2><p id="303b" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">到目前为止，我们已经表明(在这个例子中)基于音轨的分治分类优于3秒单片段分类。然而，如果我们在“原始”GTZAN中的1000个30秒音频片段上构建CRNN，我们可能会获得更高的准确性。</p><p id="0926" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这是可能的，所以我试了一下。我基于30秒的切片构建了另一个类似架构的CRNN(尽管由于数据较少，只有一半的参数),并对其进行了10次训练，以平均获得的准确度分数。这种方法在<strong class="ls iu">的平均准确率仅为55.8% </strong> (SD=0.043)，远远低于任何3秒模型<strong class="ls iu">。</strong></p><h1 id="3423" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">结论</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/1f761bf75b238516564ae52ca01cd8a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*Se_T7ssZzbKevKr8m63WrA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">AI生成的图像(稳定扩散1.5)。提示:“两幅真正深刻对话的音乐作品，抽象”。图片作者。</p></figure><h2 id="1f3e" class="nb kz it bd la nc nd dn le ne nf dp li lz ng nh lk md ni nj lm mh nk nl lo nm bi translated">分而治之确实征服了</h2><p id="bb5f" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">本文展示了分而治之的方法如何有助于为音频分类问题建立一个强大的分类器，即使没有太多的训练数据。数据处理管道SLAPP使得为单标签音频分类任务构建分而治之分类器所需的处理步骤变得非常容易。</p><h2 id="5d73" class="nb kz it bd la nc nd dn le ne nf dp li lz ng nh lk md ni nj lm mh nk nl lo nm bi translated">3秒比30秒好</h2><p id="ceea" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">为什么30秒的切片比3秒的切片更糟糕，尽管它们包含更多的信息？通过绘制3秒切片来增加数据集大小似乎有助于模型提取有意义的特征，而在处理由巨大光谱图组成的非常小的数据集时，它似乎会迷失方向。</p><p id="39ea" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">可以说，如果使用其他数据格式，如MFCCs或更小维度的手工制作的功能，该模型将更容易处理30秒的音频文件。然而，本文表明，如果使用分治法，几乎任何音频数据集都可以用声谱图数据进行分类。</p><h2 id="fb9d" class="nb kz it bd la nc nd dn le ne nf dp li lz ng nh lk md ni nj lm mh nk nl lo nm bi translated">SLAPP +分而治之的其他应用</h2><p id="851a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">如果你想在另一个类似的问题上尝试所提出的方法，这里有一些想法可能会启发你的下一个数据科学项目:</p><ol class=""><li id="3272" class="oe of it ls b lt mn lw mo lz og md oh mh oi ml oj ok ol om bi translated">基于语音数据的性别识别</li><li id="2a72" class="oe of it ls b lt on lw oo lz op md oq mh or ml oj ok ol om bi translated">基于语音数据的口音识别</li><li id="522b" class="oe of it ls b lt on lw oo lz op md oq mh or ml oj ok ol om bi translated">音乐情感分类器</li><li id="85e4" class="oe of it ls b lt on lw oo lz op md oq mh or ml oj ok ol om bi translated">使用来自机器传感器的音频数据构建分类器</li></ol><blockquote class="mt mu mv"><p id="cc04" class="lq lr mm ls b lt mn ju lv lw mo jx ly mw mp mb mc mx mq mf mg my mr mj mk ml im bi translated">非常感谢你对我的工作感兴趣！如果有什么对你不起作用，或者你在理解或应用这些概念时有困难，请告诉我。</p></blockquote><h1 id="8d27" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">参考</h1><p id="b87b" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu"> [1]比沙拉德，D. &amp;拉斯卡尔，R. H. (2019) </strong>。基于卷积递归神经网络结构的音乐类型识别。在:<em class="mm">专家系统</em> 36，4。</p><p id="1504" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">【2】崔、k；法泽卡斯；桑德勒，M. &amp;曹，K. (2017) </strong>。用于音乐分类的卷积递归神经网络。In: <em class="mm">国际声学、语音、信号处理会议2017 </em>。</p><p id="8b7a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">【3】董，M. (2018) </strong>。卷积神经网络在音乐流派分类中达到人类水平的准确性。参加:<em class="mm">认知计算神经科学会议，2018年9月5日至8日，宾夕法尼亚州费城</em>。</p><p id="513d" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">【4】纳斯鲁拉，Z. &amp;赵，Y. (2019) </strong>。音乐艺术家使用深度特征对音频、文本和图像进行分类。In: <em class="mm"> arXiv </em>，DOI:<a class="ae mz" href="https://doi.org/10.48550/arXiv.1707.04916" rel="noopener ugc nofollow" target="_blank">10.48550/arXiv . 1707.04916</a></p></div></div>    
</body>
</html>