<html>
<head>
<title>Let us Extract some Topics from Text Data — Part IV: BERTopic</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让我们从文本数据中提取一些主题——第四部分:主题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/let-us-extract-some-topics-from-text-data-part-iv-bertopic-46ddf3c91622#2022-12-19">https://towardsdatascience.com/let-us-extract-some-topics-from-text-data-part-iv-bertopic-46ddf3c91622#2022-12-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8b61" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解有关主题建模的 BERT 家族成员的更多信息</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f1d916a740f9ec741aa8c96b7484d232.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*P257qGMpL9Men4-h"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自<a class="ae ky" href="https://www.pexels.com/ko-kr/photo/4132326/" rel="noopener ugc nofollow" target="_blank">像素</a>的免费使用照片</p></figure><h1 id="f38a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="6d5a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">主题建模</strong>是一种自然语言处理(NLP)任务，它利用无监督学习方法从我们处理的一些文本数据中提取出主要主题。这里的“无监督”一词意味着没有与主题标签相关联的训练数据。相反，算法试图直接从数据本身发现潜在的模式，在这种情况下，是主题。</p><p id="6a7a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">有各种各样的算法广泛用于主题建模。在我之前的三篇文章中，我向您介绍了主题建模的三种算法:LDA、GSDMM 和 NMF。</p><div class="ms mt gp gr mu mv"><a rel="noopener follow" target="_blank" href="/let-us-extract-some-topics-from-text-data-part-i-latent-dirichlet-allocation-lda-e335ee3e5fa4"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">让我们从文本数据中提取一些主题—第一部分:潜在狄利克雷分配(LDA)</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">使用 Python 的 nltk、gensim、sklearn 和 pyLDAvis 包了解主题建模需要什么及其实现</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">towardsdatascience.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj ks mv"/></div></div></a></div><div class="ms mt gp gr mu mv"><a href="https://medium.com/geekculture/let-us-extract-some-topics-from-text-data-part-ii-gibbs-sampling-dirichlet-multinomial-mixture-9e82d51b0fab" rel="noopener follow" target="_blank"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">让我们从文本数据中提取一些主题—第二部分:吉布斯采样狄利克雷多项式混合…</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">了解如何使用 GSDMM 进行主题建模，以及它与 LDA 的比较</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">medium.com</p></div></div><div class="ne l"><div class="nk l ng nh ni ne nj ks mv"/></div></div></a></div><div class="ms mt gp gr mu mv"><a rel="noopener follow" target="_blank" href="/let-us-extract-some-topics-from-text-data-part-iii-non-negative-matrix-factorization-nmf-8eba8c8edada"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">让我们从文本数据中提取一些主题—第三部分:非负矩阵分解(NMF)</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">了解有关从线性代数派生的无监督算法的更多信息，该算法使用直观的方法来处理主题…</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">towardsdatascience.com</p></div></div><div class="ne l"><div class="nl l ng nh ni ne nj ks mv"/></div></div></a></div><p id="16c8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在本文中，我深入解释了什么是 BERTopic，以及如何将它用于您的主题建模项目！让我们直入主题吧！</p><h1 id="6d07" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">什么是 BERTopic</h1><p id="f25d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在我们弄清楚 BERTopic 是什么以及它做什么之前，我们需要知道 BERT 是什么，因为 BERTopic 是从 BERT 派生出来的。BERT 是 Transformers 双向编码器表示的缩写，它是 2018 年作为基于 transformer 的机器学习模型开发的。它已经在大量的语料库数据中预训练，因此在各种 NLP 任务中表现非常好。你可以在这里查看伯特<a class="ae ky" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">的原始论文。BERTopic 是作为 BERT 家族成员设计的，专门用于主题建模。</a></p><p id="f329" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">BERTopic 通过以下几个步骤运作。</p><p id="df61" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[步骤 1]使用句子转换器将文档表示为嵌入。你可以在这里了解更多句子变形金刚<a class="ae ky" href="https://github.com/UKPLab/sentence-transformers" rel="noopener ugc nofollow" target="_blank">。这一步使用的默认模型是 BERT(因此得名 BERTopic)。</a></p><p id="1a99" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">【第二步】第二步是降维过程。默认使用 UMAP(统一流形近似和投影)算法。当然，根据您的目标和数据，也可以使用包括主成分分析(PCA)在内的其他选项。</p><p id="d3dc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">【第三步】第三步是聚类过程。这一步实际计算不同文档之间的相似性，以确定它们是否属于同一主题。默认使用基于密度的分层空间聚类(HDBSCAN)算法。这是一种基于密度的聚类算法，因此通常比用于 topc 建模的 K-Means 聚类等算法性能更好。</p><p id="be72" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">【第四步】之后，c-TF-IDF 算法检索每个主题最相关的词。顾名思义，c-TF-IDF 类似于 TF-IDF，但不同之处在于它测量的是每个簇中的术语频率，而不是每个文档中的术语频率。c-TF-IDF 的数学公式如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/7efd457ff6f6172b0f57fd43ab8df2b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*WZtqPlqrgU_GdzQYc6hT1Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://github.com/MaartenGr/BERTopic/blob/master/docs/algorithm/c-TF-IDF.svg" rel="noopener ugc nofollow" target="_blank"> BERTopic Github 网站</a></p></figure><p id="1656" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[步骤 5]可选的最后一步是使用最大边际相关性(MMR)来优化术语。使用这种算法是有益的，因为它提高了相同主题和主题表示的术语之间的一致性。</p><p id="9d66" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下面是一个很好的描述上述步骤的信息图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/0adc85d5604898ae39e1b9914edf7e1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*q9p7XOt4ak9mFxvUWRGiyw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://github.com/MaartenGr/BERTopic/blob/master/docs/img/algorithm.png" rel="noopener ugc nofollow" target="_blank"> BERTopic Github 网站</a></p></figure><h1 id="d9ef" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">履行</h1><p id="f631" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们将使用 20 个新闻组数据，每个人都可以通过 sklearn 包获得这些数据。它使用<a class="ae ky" href="https://www.apache.org/licenses/LICENSE-2.0" rel="noopener ugc nofollow" target="_blank">Apache 2.0 版许可</a>。只是提醒一下，为了不同教程之间的一致性，我在关于主题建模的前三篇文章中使用了这个数据集。</p><p id="a5a8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们首先安装 BERTopic 包。</p><pre class="kj kk kl km gt no np nq bn nr ns bi"><span id="5b18" class="nt la it np b be nu nv l nw nx">!pip install bertopic</span></pre><p id="9005" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后我们导入我们需要的相关包。</p><pre class="kj kk kl km gt no np nq bn nr ns bi"><span id="c509" class="nt la it np b be nu nv l nw nx">import pandas as pd<br/>import numpy as np<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.datasets import fetch_20newsgroups<br/>from scipy import linalg<br/>import gensim<br/>from tqdm import tqdm<br/>import re<br/>import matplotlib.pyplot as plt<br/>from bertopic import BERTopic<br/><br/>%matplotlib inline<br/>np.set_printoptions(suppress=True)</span></pre><p id="b63b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">通常，清理和预处理文本的过程对于确保最佳性能是必要的，但是对于本教程，我们跳过这一部分，更多地关注 BERTopic 本身的实现。实际上，看到没有文本清理的主题建模输出的相对不错的质量，可以证明 BERTopic 的强大和有用。如果你想了解更多关于文本清理步骤的信息，请参考我以前的文章，这篇文章谈到了主题建模的 NMF。</p><p id="48ad" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从 skelarn 包中，我们读入了 20 个新闻组数据。然后，我们简单地实例化 BERTopic 对象，同时指定语言为英语。您可以通过 nr_topics 参数指定想要的主题数量。“Auto”意味着您希望模型自动确定要建模的主题的适当数量。</p><pre class="kj kk kl km gt no np nq bn nr ns bi"><span id="bbfb" class="nt la it np b be nu nv l nw nx">docs = fetch_20newsgroups(subset='all',  <br/>                        remove=('headers', 'footers', 'quotes'))['data']<br/><br/># Instantiate the model<br/>model = BERTopic(language="english", nr_topics = 'auto')<br/><br/># Fit and Transform<br/>topics, probs = model.fit_transform(docs)</span></pre><p id="dcd9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">但是，请注意，您可以根据自己的需要调整模型的各个部分。</p><h2 id="09ea" class="ny la it bd lb nz oa dn lf ob oc dp lj ma od oe ll me of og ln mi oh oi lp oj bi translated">语言</h2><p id="001b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">您可以指定语言，或者让模型使用 language 参数来推断语言。</p><pre class="kj kk kl km gt no np nq bn nr ns bi"><span id="d9a7" class="nt la it np b be nu nv l nw nx"># Specify the lanugage to be English expliclitly<br/>model = BERTopic(language="english")<br/><br/># Let the model infer<br/>model = BERTopic(language="multilingual")</span></pre><h2 id="fbf3" class="ny la it bd lb nz oa dn lf ob oc dp lj ma od oe ll me of og ln mi oh oi lp oj bi translated">嵌入</h2><p id="cc30" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">您还可以通过 embedding_model 参数调整 BERTopic 的嵌入部分。默认情况下，使用 BERT 基本模型，但是可以自由使用其他嵌入模型。选项列表可在本<a class="ae ky" href="https://www.sbert.net/docs/pretrained_models.html" rel="noopener ugc nofollow" target="_blank">文档</a>中找到。</p><pre class="kj kk kl km gt no np nq bn nr ns bi"><span id="0286" class="nt la it np b be nu nv l nw nx">BERTopic(embedding_model="xlm-r-bert-base-nli-stsb-mean-tokens")</span></pre><h2 id="1cab" class="ny la it bd lb nz oa dn lf ob oc dp lj ma od oe ll me of og ln mi oh oi lp oj bi translated">主题表征</h2><p id="6e1b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">除了使用默认的 TF-IDF 向量表示，您可以自己调整它，或者使用反向量表示。</p><pre class="kj kk kl km gt no np nq bn nr ns bi"><span id="3e4a" class="nt la it np b be nu nv l nw nx"># Update topic representation by increasing n-gram range and <br/># removing english stopwords<br/><br/>model.update_topics(docs, topics, n_gram_range=(1, 3), stop_words="english")<br/><br/># Use Custom CountVectorizer<br/><br/>from sklearn.feature_extraction.text import CountVectorizer<br/>cv = CountVectorizer(ngram_range=(1, 3), stop_words="english")<br/>model.update_topics(docs, topics, vectorizer=cv)</span></pre><h2 id="ca70" class="ny la it bd lb nz oa dn lf ob oc dp lj ma od oe ll me of og ln mi oh oi lp oj bi translated">降维</h2><p id="9379" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在将 UMAP 算法输入到 BERTopic 模型之前，您可以调整其超参数。此外，除了使用默认的 UMAP 算法进行降维，您还可以使用其他算法，如 PCA。</p><pre class="kj kk kl km gt no np nq bn nr ns bi"><span id="2756" class="nt la it np b be nu nv l nw nx">from umap import UMAP<br/>umap_model = UMAP(n_neighbors=10, <br/>                  n_components=7, <br/>                  min_dist=0.0, <br/>                  metric='cosine', <br/>                  random_state=42)<br/><br/>model = BERTopic(umap_model=umap_model, language="english")</span></pre><p id="8310" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">参考这篇很好的<a class="ae ky" href="https://medium.com/grabngoinfo/hyperparameter-tuning-for-bertopic-model-in-python-104445778347" rel="noopener">文章</a>，它实际上用 PCA 代替了 BERTopic 的 UMPA。</p><h2 id="2ddc" class="ny la it bd lb nz oa dn lf ob oc dp lj ma od oe ll me of og ln mi oh oi lp oj bi translated"><strong class="ak">话题数量</strong></h2><p id="d6d8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">您可以手动指定主题的数量，或者将 nr_topics 参数设置为“auto ”,让模型自动确定适当的主题数量。但是，您可能会遇到这样的情况:您已经训练了主题模型，但是发现主题的数量太多。在这种情况下，您可以在训练后使用 reduce_topics 函数减少主题的数量。</p><pre class="kj kk kl km gt no np nq bn nr ns bi"><span id="61e4" class="nt la it np b be nu nv l nw nx"># Specify number of topics manually<br/>model = BERTopic(nr_topics=20)<br/><br/># Automatic Topic Reduction<br/>model = BERTopic(nr_topics="auto")<br/><br/># Reduce the number of topics after training the model<br/>new_topics, new_probs =\<br/>model.reduce_topics(docs, topics, probs, nr_topics=5)</span></pre><h2 id="3f71" class="ny la it bd lb nz oa dn lf ob oc dp lj ma od oe ll me of og ln mi oh oi lp oj bi translated">获取主题的总体信息</h2><p id="f3c1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">您可以使用 get_topic_info 函数来获取主题的全部信息。</p><pre class="kj kk kl km gt no np nq bn nr ns bi"><span id="a1c7" class="nt la it np b be nu nv l nw nx">freq = model.get_topic_info() <br/>freq</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/da805f9f4f4930bb053096091241e111.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*VXVULMZmCizaNIWfTudn3g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:来自作者</p></figure><p id="691b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Topic -1 指的是一组不可能归类到其他主题中的文档，因此我们可以忽略它。我们可以看到文档在每个主题中的数量分布，还可以在 name 列中看到代表每个主题的一些关键字。</p><h2 id="866a" class="ny la it bd lb nz oa dn lf ob oc dp lj ma od oe ll me of og ln mi oh oi lp oj bi translated">特定主题的关键字和 c-TF-IDF 分数</h2><p id="8f8b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">您还可以使用 get_topic 函数访问指定主题的关键字及其 c-TF-IDF 分数。例如，我们在下面的代码中访问主题 3 的关键字及其分数。</p><pre class="kj kk kl km gt no np nq bn nr ns bi"><span id="3108" class="nt la it np b be nu nv l nw nx"># We access index 4 for topic 3 because we skip index 0 which is topic -1<br/>model.get_topic(freq.iloc[4]["Topic"])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/612a1fbeb92406be6b02ce8d5c9d26d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*KnmsVhFDhcBrfM2BWwpxQg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:来自作者</p></figure><p id="3ae4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从上面的关键词我们可以看出，话题 3 主要是关于枪击和枪支相关的犯罪。</p><h2 id="ded7" class="ny la it bd lb nz oa dn lf ob oc dp lj ma od oe ll me of og ln mi oh oi lp oj bi translated">查找与指定术语相似的主题</h2><pre class="kj kk kl km gt no np nq bn nr ns bi"><span id="b753" class="nt la it np b be nu nv l nw nx"># Return top3 topics that are semantically most similar <br/># to an input query term<br/><br/># 3 most similar topics to specified word<br/>similar_topics, similarity = \<br/>model.find_topics("religion", top_n = 3) <br/><br/><br/>print("Most Similar Topic Info: \n{}".format(model.get_topic(similar_topics[0])))<br/>print("Similarity Score: {}".format(similarity[0]))<br/><br/>print("\n Most Similar Topic Info: \n{}".format(model.get_topic(similar_topics[1])))<br/>print("Similarity Score: {}".format(similarity[1]))<br/><br/>print("\n Most Similar Topic Info: \n{}".format(model.get_topic(similar_topics[2])))<br/>print("Similarity Score: {}".format(similarity[2]))</span></pre><p id="fd73" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">输出是:</p><pre class="kj kk kl km gt no np nq bn nr ns bi"><span id="c289" class="nt la it np b be nu nv l nw nx">Most Similar Topic Info: <br/>[('prophecies', 0.035076938400189765), ('prophecy', 0.028848478747937348), ('that', 0.02108670502531178), ('god', 0.02051417591444672), ('lord', 0.020264581769842555), ('of', 0.016688896522909655), ('the', 0.016135781453880685), ('to', 0.015035130690705624), ('scripture', 0.014930014538798414), ('we', 0.014849027662146918)]<br/>Similarity Score: 0.5521519602007433<br/><br/> Most Similar Topic Info: <br/>[('bank', 0.09616363061888215), ('islamic', 0.08725362721875433), ('bcci', 0.07506873356081414), ('banks', 0.04599130160033494), ('islam', 0.03498368962676642), ('interest', 0.03153905791196487), ('an', 0.02707799288051472), ('risk', 0.026608617086657786), ('investor', 0.023625991363580155), ('loans', 0.023098071864865885)]<br/>Similarity Score: 0.5518991782725108<br/><br/> Most Similar Topic Info: <br/>[('moral', 0.04437134134615366), ('objective', 0.04058577723387244), ('morality', 0.03933015749038743), ('is', 0.023387671936210788), ('that', 0.021184421900981805), ('what', 0.017148832156794584), ('you', 0.017133130253694097), ('not', 0.01467957486207896), ('immoral', 0.014518771930711771), ('of', 0.014256652246875072)]<br/>Similarity Score: 0.5515153930656871</span></pre><p id="a8c9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从上面的例子可以看出，我们可以指定一个术语，并找到与该输入最相关的主题。在这种情况下，我们使用了“宗教”这个术语。与宗教一词语义最相似的前三个话题主要是关于预言、伊斯兰教和道德。</p><p id="f199" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">作为软件包的一部分，BERTopic 也有一些不错的可视化功能。</p><pre class="kj kk kl km gt no np nq bn nr ns bi"><span id="8fad" class="nt la it np b be nu nv l nw nx"># Intertopic Distance Map<br/>model.visualize_topics( )</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/3ee9ae97ed0188ca192c1cf47b644940.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g3Ck-C2CSorwK107oZ9aLw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:来自作者</p></figure><p id="7620" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">visualize_topics 函数显示我们之前在<a class="ae ky" href="https://medium.com/r?url=https%3A%2F%2Ftowardsdatascience.com%2Flet-us-extract-some-topics-from-text-data-part-i-latent-dirichlet-allocation-lda-e335ee3e5fa4" rel="noopener"> LDA 教程</a>中见过的主题间距离图。这基本上是一种可视化，显示不同主题的气泡以及它们彼此之间的相似程度。两个气泡越接近，两个主题的语义就越相似。例如，主题 2 右下角的红色大气泡与主题 10 相似，后者是主题 2 中较小的气泡。主题 10 包含维生素和感染等关键词，而主题 2 包含癌症和药物等关键词，我们看到这两个主题彼此密切相关。</p><pre class="kj kk kl km gt no np nq bn nr ns bi"><span id="93f6" class="nt la it np b be nu nv l nw nx"># Topic Word Scores in Bar Chart<br/>model.visualize_barchart()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/36b6bd6a409918fd33c159998819ddef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iG6mLrh4ufg1DC7Z84TV_Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:来自作者</p></figure><p id="a2fd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">visualize_barchart 允许我们创建条形图，显示每个主题中关键字的不同得分。唯一的缺点是它只显示前 8 个主题，</p><pre class="kj kk kl km gt no np nq bn nr ns bi"><span id="73c8" class="nt la it np b be nu nv l nw nx"># Similarity Matrix<br/>model.visualize_heatmap( )</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/11f8890c14a2298672288ae873e03b0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4b8-TM7y0NdlRDwUFiIV_Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:来自作者</p></figure><p id="09b7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">visualize_heatmap 函数返回每个成对主题相似性的相似矩阵。如果你有太多的主题，它可能不会提供太多的信息，因为在一个情节中会有过多的信息。</p><pre class="kj kk kl km gt no np nq bn nr ns bi"><span id="741e" class="nt la it np b be nu nv l nw nx"># Probability distribution of first document across different topics<br/>model.visualize_distribution(probs[5]) </span></pre><p id="4b00" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">visualize_distribution 函数返回水平条形图，指示某个文档属于每个主题的概率。在上面的例子中，它返回第六个文档属于每个主题的概率。请注意，您需要在训练之前将 BERTopic 对象中的 calculate_probabilities 参数指定为 True，因为默认值为 False，如果是这样，visualize_distribution 函数将返回一个错误。</p><pre class="kj kk kl km gt no np nq bn nr ns bi"><span id="c89d" class="nt la it np b be nu nv l nw nx"># Hierarchical Topics<br/>hierarchical_topics = model.hierarchical_topics(docs)<br/>model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/ff30da8569032a37beedb95a565c2fbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bm19Xp-zy70Uu23upC48-A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:来自作者</p></figure><p id="2493" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果您想了解不同主题之间的层次关系，visualize_hierarchy 函数将会帮您解决这个问题。</p><pre class="kj kk kl km gt no np nq bn nr ns bi"><span id="4999" class="nt la it np b be nu nv l nw nx">topics_over_time =\<br/>model.topics_over_time(docs, topics, timestamp, nr_bins=20)<br/><br/>model.visualize_topics_over_time(topics_over_time, top_n_topics=20)</span></pre><p id="07ac" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们使用的数据集不包含任何时间元素，但是如果您的数据集包含时间元素，您也可以使用 topics_over_time 函数以线形图的形式显示主题随时间的频率。</p><h1 id="7cb8" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="c16b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本文中，我向您介绍了 BERTopic 算法，该算法源自 BERT，由多个步骤组成，利用了从 c-TF-IDF、UMAP 到 HDBSCAN 和 MRR 的各种算法。为了充分理解这个模型是如何工作的，你需要理解其他机器学习和 NLP 相关的任务，比如文本数据的维数减少和嵌入。尽管如此，它为我们提供了一种强大而简单的方法来执行主题建模，即使没有文本清理或预处理。这并不意味着文本清理是不必要的。请注意，在大多数情况下，文本清理和预处理对于确保建模主题的质量至关重要。要了解更多关于 BERTopic 的信息，请参考这个主要的<a class="ae ky" href="https://maartengr.github.io/BERTopic/algorithm/algorithm.html#code-overview" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="c794" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你觉得这篇文章有帮助，请考虑通过以下链接注册 medium 来支持我: )</p><p id="c9ea" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">joshnjuny.medium.com<a class="ae ky" href="https://joshnjuny.medium.com/membership" rel="noopener"/></p><p id="c778" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">你不仅可以看到我，还可以看到其他作者写的这么多有用和有趣的文章和帖子！</p><h1 id="2ffd" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">关于作者</h1><p id="85a0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><em class="oq">数据科学家。加州大学欧文分校信息学专业一年级博士生。主要研究兴趣是将 SOTA ML/DL/NLP 方法应用于健康和医疗相关的大数据，以提取有趣的见解，为患者、医生和决策者提供信息。</em></p><p id="4509" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="oq">密歇根大学刑事司法行政记录系统(CJARS)经济学实验室的前研究领域专家，致力于统计报告生成、自动化数据质量审查、构建数据管道和数据标准化&amp;协调。Spotify 前数据科学实习生。Inc .(纽约市)。</em></p><p id="bc51" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">他喜欢运动、健身、烹饪美味的亚洲食物、看 kdramas 和制作/表演音乐，最重要的是崇拜我们的主耶稣基督。结账他的 <a class="ae ky" href="http://seungjun-data-science.github.io" rel="noopener ugc nofollow" target="_blank"> <em class="oq">网站</em> </a> <em class="oq">！</em></p></div></div>    
</body>
</html>