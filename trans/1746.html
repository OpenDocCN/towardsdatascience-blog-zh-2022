<html>
<head>
<title>Feature selection: A comprehensive list of strategies</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特性选择:一个全面的策略列表</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-a-comprehensive-list-of-strategies-3fecdf802b79#2022-04-25">https://towardsdatascience.com/feature-selection-a-comprehensive-list-of-strategies-3fecdf802b79#2022-04-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/6b8170c0a89365857ecb06b9184d28ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*a8NNPh_QKWYwHejN"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">里卡多·戈麦斯·安吉尔在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="f0ac" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">11种特征选择技术的清单</h2></div></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><p id="ea33" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">假设您有一个包含25列和10，000行的数据集。经过一些特征工程，最终你得到了45列。在这45个特征中，你能保留几个？过多的特征会增加模型的复杂性和过度拟合，过少的特征会使模型欠拟合。因此，您优化您的模型，使其足够复杂，以便其性能可以推广，但又足够简单，以便易于训练、维护和解释。</p><p id="ac18" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">“特性选择”意味着你可以保留一些特性，放弃一些其它的特性。问题是——如何决定保留哪些特征，去掉哪些特征？本文的目的是概述一些特性选择策略:</p><ul class=""><li id="0841" class="mb mc jj lh b li lj ll lm lo md ls me lw mf ma mg mh mi mj bi translated">删除未使用的列</li><li id="09f0" class="mb mc jj lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">删除缺少值的列</li><li id="fbbc" class="mb mc jj lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">不相关特征<br/> —数字<br/> —分类</li><li id="3b35" class="mb mc jj lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">低方差特征</li><li id="704f" class="mb mc jj lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">多重共线性<br/> —数字<br/> —分类</li><li id="db7e" class="mb mc jj lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">特征系数</li><li id="1ec2" class="mb mc jj lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">p值</li><li id="a4c9" class="mb mc jj lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">差异通货膨胀系数(VIF)</li><li id="14b5" class="mb mc jj lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">基于特征重要性/杂质的特征选择</li><li id="5d63" class="mb mc jj lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">利用sci-kit learn进行自动特征选择<br/> —基于卡方的技术<br/> —正则化<br/> —顺序选择</li><li id="dd41" class="mb mc jj lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">主成分分析</li></ul><p id="0c5a" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">你不太可能在一个项目中同时使用这些策略，但是，手边有这样一个清单可能会很方便。</p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h2 id="4e42" class="mp mq jj bd mr ms mt dn mu mv mw dp mx lo my mz na ls nb nc nd lw ne nf ng nh bi translated">数据描述</h2><p id="8b9c" class="pw-post-body-paragraph lf lg jj lh b li ni kk lk ll nj kn ln lo nk lq lr ls nl lu lv lw nm ly lz ma im bi translated">在<a class="ae jg" href="https://github.com/pycaret/pycaret/blob/master/LICENSE" rel="noopener ugc nofollow" target="_blank">麻省理工学院许可</a>下发布，本次演示的数据集来自<a class="ae jg" href="https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/automobile.csv" rel="noopener ugc nofollow" target="_blank">py caret</a>——一个开源的低代码机器学习库。</p><p id="8c1c" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">数据集相当干净，但我做了一些预处理，我在这里跳过了。请注意，我使用这个数据集来演示不同的特征选择策略是如何工作的，而不是构建一个最终的模型，因此模型性能是不相关的(但这将是一个有趣的练习！).</p><p id="9bfd" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">让我们加载数据集:</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="aa46" class="mp mq jj ns b gy nw nx l ny nz">import pandas as pd</span><span id="ef9d" class="mp mq jj ns b gy oa nx l ny nz">data = '<a class="ae jg" href="https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/automobile.csv'" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/automobile.csv'</a><br/>df = pd.read_csv(data)</span><span id="876c" class="mp mq jj ns b gy oa nx l ny nz">df.sample(5)</span></pre><figure class="nn no np nq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ob"><img src="../Images/cbe713e12c1bdcd1371f62d3bd2e0828.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vbPN6bCt2pxJOIIq9awASQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">数据集的前几行(图:作者)</p></figure><p id="44ff" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">该数据集包含202行和26列，每行代表一辆汽车的实例，每列代表其功能和相应的价格。这些列包括:</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="f746" class="mp mq jj ns b gy nw nx l ny nz">df.columns</span><span id="a615" class="mp mq jj ns b gy oa nx l ny nz">&gt;&gt; Index(['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration', 'num-of-doors', 'body-style', 'drive-wheels', 'engine-location','wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-type', 'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-ratio', 'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price'], dtype='object')</span></pre><p id="26a7" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">现在，让我们深入研究特性选择的11个策略。</p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="9938" class="oc mq jj bd mr od oe of mu og oh oi mx kp oj kq na ks ok kt nd kv ol kw ng om bi translated">移除未使用的列</h1><p id="92c8" class="pw-post-body-paragraph lf lg jj lh b li ni kk lk ll nj kn ln lo nk lq lr ls nl lu lv lw nm ly lz ma im bi translated">当然，最简单的策略是运用你的直觉。有时很明显，一些列在最终的模型中不会以任何形式使用(如“ID”、“FirstName”、“LastName”等列)。如果您知道某个特定的列不会被使用，那么可以直接删除它。在我们的数据中，没有一列如此突出，所以在这一步中我不会删除任何列。</p><h1 id="1c02" class="oc mq jj bd mr od on of mu og oo oi mx kp op kq na ks oq kt nd kv or kw ng om bi translated">删除缺少值的列</h1><p id="1ac5" class="pw-post-body-paragraph lf lg jj lh b li ni kk lk ll nj kn ln lo nk lq lr ls nl lu lv lw nm ly lz ma im bi translated">具有缺失值在机器学习中是不可接受的，因此人们应用不同的策略来清理缺失数据(例如，插补)。但是，如果一列中丢失了大量数据，一种策略是将其完全删除。</p><p id="2fc9" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">在我们的数据集中，有显著缺失值的列是<code class="fe os ot ou ns b">normalized-losses</code>，我将删除它。</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="40e1" class="mp mq jj ns b gy nw nx l ny nz"># total null values per column<br/>df.isnull().sum()</span><span id="8a96" class="mp mq jj ns b gy oa nx l ny nz">&gt;&gt;<br/>symboling             0<br/>normalized-losses    35<br/>make                  0<br/>fuel-type             0<br/>aspiration            0<br/>num-of-doors          2<br/>body-style            0<br/>drive-wheels          0<br/>engine-location       0<br/>wheel-base            0<br/>length                0<br/>width                 0<br/>height                0<br/>curb-weight           0<br/>engine-type           0<br/>num-of-cylinders      0<br/>engine-size           0<br/>fuel-system           0<br/>bore                  0<br/>stroke                0<br/>compression-ratio     0<br/>horsepower            0<br/>peak-rpm              0<br/>city-mpg              0<br/>highway-mpg           0<br/>price                 0<br/>dtype: int64</span></pre><h1 id="36fb" class="oc mq jj bd mr od on of mu og oo oi mx kp op kq na ks oq kt nd kv or kw ng om bi translated">不相关的特征</h1><p id="5fc2" class="pw-post-body-paragraph lf lg jj lh b li ni kk lk ll nj kn ln lo nk lq lr ls nl lu lv lw nm ly lz ma im bi translated">无论算法是回归(预测一个数)还是分类(预测一个类)，特征都必须与目标相关。如果某个特征没有表现出相关性，那么它就是一个主要的淘汰目标。您可以分别测试数值特征和分类特征的相关性。</p><h2 id="fb94" class="mp mq jj bd mr ms mt dn mu mv mw dp mx lo my mz na ls nb nc nd lw ne nf ng nh bi translated">a)数字特征</h2><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="8da8" class="mp mq jj ns b gy nw nx l ny nz"># correlation between target and features<br/>(df.corr().loc['price']<br/> .plot(kind='barh', figsize=(4,10)))</span></pre><figure class="nn no np nq gt iv gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/f4d935bd5d6802211c4841979880a52c.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*RZ1ZbYuBt8e0_xArJpTqlg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">数字特征和目标之间的相关性(图:作者)</p></figure><p id="7a53" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">在这个例子中，诸如<em class="ow">峰值转速</em>、<em class="ow">压缩比、冲程、缸径、高度</em>和象征的<em class="ow">等特征与<em class="ow">价格</em>的相关性很小，因此我们可以忽略它们。</em></p><p id="241d" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">您可以手动删除列，但是我更喜欢使用相关阈值(在本例中为0.2)以编程方式来完成:</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="05e1" class="mp mq jj ns b gy nw nx l ny nz"># drop uncorrelated numeric features (threshold &lt;0.2)<br/>corr = abs(df.corr().loc['price'])<br/>corr = corr[corr&lt;0.2]<br/>cols_to_drop = corr.index.to_list()<br/>df = df.drop(cols_to_drop, axis=1)</span></pre><h2 id="b133" class="mp mq jj bd mr ms mt dn mu mv mw dp mx lo my mz na ls nb nc nd lw ne nf ng nh bi translated">b)分类特征</h2><p id="0de3" class="pw-post-body-paragraph lf lg jj lh b li ni kk lk ll nj kn ln lo nk lq lr ls nl lu lv lw nm ly lz ma im bi translated">同样，您可以使用箱线图来查找目标特征和分类特征之间的相关性:</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="114d" class="mp mq jj ns b gy nw nx l ny nz">import seaborn as sns</span><span id="4c9b" class="mp mq jj ns b gy oa nx l ny nz">sns.boxplot(y = 'price', x = 'fuel-type', data=df)</span></pre><figure class="nn no np nq gt iv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/f8da3a5ce09e89a96a31ba743f199c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*ybkGrU8UBaHErfRS9-DgcA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">按燃料类型分类的汽车价格(图:作者)</p></figure><p id="7940" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><em class="ow">柴油</em>型车的中位价格高于<em class="ow">燃气</em>型车。这意味着这个分类变量可以解释汽车价格，所以我不会放弃它。您可以像这样单独检查每个分类列。</p><h1 id="ce6a" class="oc mq jj bd mr od on of mu og oo oi mx kp op kq na ks oq kt nd kv or kw ng om bi translated">低方差特征</h1><p id="e9ec" class="pw-post-body-paragraph lf lg jj lh b li ni kk lk ll nj kn ln lo nk lq lr ls nl lu lv lw nm ly lz ma im bi translated">举个极端的例子，我们假设所有的车都有相同的公路——mpg(mpg:英里每加仑)。你觉得这个功能有用吗？没有，因为它的方差正好为0。我们可以选择放弃这样的低方差特性。让我们检查一下我们的功能差异:</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="b11c" class="mp mq jj ns b gy nw nx l ny nz">import numpy as np</span><span id="e8b2" class="mp mq jj ns b gy oa nx l ny nz"># variance of numeric features<br/>(df<br/> .select_dtypes(include=np.number)<br/> .var()<br/> .astype('str'))</span></pre><figure class="nn no np nq gt iv gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/2ae05dcaf410a996099a457792bda1d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*-XwV15vGo-YnRbW4QhJDng.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">数字特征的方差(图:作者)</p></figure><p id="6d01" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">这里“bore”具有极低的方差，因此这是消除的理想候选。然而，在这种特殊情况下，我不愿意放弃它，因为它的值介于2.54和3.94之间，因此方差预计较低:</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="bc2b" class="mp mq jj ns b gy nw nx l ny nz">df['bore'].describe()</span></pre><figure class="nn no np nq gt iv gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/167b5a45a8d5b1ff4a008884c532925d.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*12c3uuHcCQdn1b-toRSqfg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">一个特性的统计概要(图:作者)</p></figure><h1 id="8ffc" class="oc mq jj bd mr od on of mu og oo oi mx kp op kq na ks oq kt nd kv or kw ng om bi translated">多重共线性</h1><p id="eca3" class="pw-post-body-paragraph lf lg jj lh b li ni kk lk ll nj kn ln lo nk lq lr ls nl lu lv lw nm ly lz ma im bi translated">当任何两个要素之间存在相关性时，就会出现多重共线性。在机器学习中，期望每个特征应该独立于其他特征，即它们之间没有共线性。正如你将在下面看到的，马力大的车辆往往有大的<em class="ow">发动机尺寸</em>并不奇怪。所以你可能想去掉其中一个，让另一个决定目标变量——<em class="ow">价格</em>。</p><p id="5e49" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">您可以分别测试数值要素和分类要素的多重共线性:</p><h2 id="eec2" class="mp mq jj bd mr ms mt dn mu mv mw dp mx lo my mz na ls nb nc nd lw ne nf ng nh bi translated">a)数字特征</h2><p id="5722" class="pw-post-body-paragraph lf lg jj lh b li ni kk lk ll nj kn ln lo nk lq lr ls nl lu lv lw nm ly lz ma im bi translated">热图是直观检查和寻找相关要素的最简单方法。</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="6328" class="mp mq jj ns b gy nw nx l ny nz">import matplotlib.pyplot as plt</span><span id="be1e" class="mp mq jj ns b gy oa nx l ny nz">sns.set(rc={'figure.figsize':(16,10)})<br/>sns.heatmap(df.corr(),<br/>            annot=True,<br/>            linewidths=.5,<br/>            center=0,<br/>            cbar=False,<br/>            cmap="PiYG")<br/>plt.show()</span></pre><figure class="nn no np nq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/71f7cbc843c031fe472760016b382a12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*96lcYhV69TtEHcz2YSdEwQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于检查数字要素间多重共线性的热图(图:作者)</p></figure><p id="7988" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">正如你所看到的，大多数特征在某种程度上是相互关联的，但有些特征具有非常高的相关性，例如<em class="ow">长度</em>对<em class="ow">轴距</em>和<em class="ow">发动机尺寸</em>对<em class="ow">马力</em>。</p><p id="ad27" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">您可以基于关联阈值手动或以编程方式删除这些功能。我将手动删除共线性阈值为0.80的要素。</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="62fe" class="mp mq jj ns b gy nw nx l ny nz"># drop correlated features<br/>df = df.drop(['length', 'width', 'curb-weight', 'engine-size', 'city-mpg'], axis=1)</span></pre><p id="9893" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">请注意，您还可以使用一种称为方差膨胀因子(VIF)的方法来确定多重共线性，并根据高VIF值删除要素。稍后我会展示这个例子。</p><h2 id="1a8c" class="mp mq jj bd mr ms mt dn mu mv mw dp mx lo my mz na ls nb nc nd lw ne nf ng nh bi translated">b)分类变量</h2><p id="5e40" class="pw-post-body-paragraph lf lg jj lh b li ni kk lk ll nj kn ln lo nk lq lr ls nl lu lv lw nm ly lz ma im bi translated">与数值要素类似，您也可以检查分类变量之间的共线性。像卡方独立性检验这样的统计检验对它来说是理想的。</p><p id="ed81" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">让我们检查一下数据集中的两个分类列— <em class="ow">燃料类型</em>和<em class="ow">车身样式</em> —是独立的还是相关的。</p><p id="6eaa" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">首先，我们将选择感兴趣的分类特征:</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="fdfa" class="mp mq jj ns b gy nw nx l ny nz">df_cat = df[['fuel-type', 'body-style']]<br/>df_cat.sample(5)</span></pre><figure class="nn no np nq gt iv gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/0d5e62ba263986d51ae99f4a49b90609.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*yKMi00WyRhh658p9LHsOYA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">为独立性测试选择的两个分类特征(图:作者)</p></figure><p id="83a7" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">然后，我们将在每一列中创建类别的交叉表/列联表。</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="9f37" class="mp mq jj ns b gy nw nx l ny nz">crosstab = pd.crosstab(df_cat['fuel-type'], df_cat['body-style'])<br/>crosstab</span></pre><figure class="nn no np nq gt iv gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/37b522373c96436f7316e3f5efa0fb97.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*6TC8dmXNLYqwrdAWOR3U8Q.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">两个分类特征的交叉表/列联表(图:作者)</p></figure><p id="a891" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">最后，我们将对列联表进行卡方检验，这将告诉我们这两个特征是否独立。</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="3684" class="mp mq jj ns b gy nw nx l ny nz">from scipy.stats import chi2_contingency</span><span id="2cea" class="mp mq jj ns b gy oa nx l ny nz">chi2_contingency(crosstab)</span></pre><figure class="nn no np nq gt iv gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/a9a6dbbc25d8565ae73497239cb9974c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*V2MZCHwIJOCS3Ge1cMi-hQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">独立性卡方检验的结果(图:作者)</p></figure><p id="ff35" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">按照出现的顺序，输出是卡方值、<em class="ow"> p值</em>、自由度和一组预期频率。</p><p id="a8df" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">p值是&lt;0.05, thus we can reject the null hypothesis that there’s no association between features, i.e., there’s a statistically significant relationship between the two features.</p><p id="ee5a" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">Since there’s an association between the two features, we can choose to drop one of them.</p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><p id="f522" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><strong class="lh jk">我现在将换档一点……</strong></p><p id="2837" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">到目前为止，我已经展示了在实现模型之前<em class="ow">应用的特性选择策略。这些策略在构建初始模型的第一轮特征选择中非常有用。但是，一旦构建了模型，您将获得关于模型性能中每个特征的适合度的进一步信息。根据这些新信息，您可以进一步确定要保留哪些功能。</em></p><p id="58cd" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">下面我将展示其中的一些方法。</p><p id="7288" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">但是首先，我们需要使模型适合数据集，因此需要一些数据预处理。我正在做最少的数据准备，只是为了演示特征选择方法。</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="f59b" class="mp mq jj ns b gy nw nx l ny nz"># drop columns with missing values<br/>df = df.dropna()</span><span id="c622" class="mp mq jj ns b gy oa nx l ny nz">from sklearn.model_selection import train_test_split</span><span id="55f8" class="mp mq jj ns b gy oa nx l ny nz"># get dummies for categorical features<br/>df = pd.get_dummies(df, drop_first=True)</span><span id="255f" class="mp mq jj ns b gy oa nx l ny nz"># X features<br/>X = df.drop('price', axis=1)</span><span id="e5d6" class="mp mq jj ns b gy oa nx l ny nz"># y target<br/>y = df['price']</span><span id="36b0" class="mp mq jj ns b gy oa nx l ny nz"># split data into training and testing set<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)</span><span id="177f" class="mp mq jj ns b gy oa nx l ny nz">from sklearn.linear_model import LinearRegression<br/></span><span id="7a0b" class="mp mq jj ns b gy oa nx l ny nz"># scaling</span><span id="46dc" class="mp mq jj ns b gy oa nx l ny nz">from sklearn.preprocessing import StandardScaler<br/>scaler = StandardScaler()<br/>X_train = scaler.fit_transform(X_train)<br/>X_test = scaler.fit_transform(X_test)</span><span id="a79e" class="mp mq jj ns b gy oa nx l ny nz"># convert back to dataframe<br/>X_train = pd.DataFrame(X_train, columns = X.columns.to_list())<br/>X_test = pd.DataFrame(X_test, columns = X.columns.to_list())</span><span id="2a82" class="mp mq jj ns b gy oa nx l ny nz"># instantiate model<br/>model = LinearRegression()</span><span id="7f3d" class="mp mq jj ns b gy oa nx l ny nz"># fit<br/>model.fit(X_train, y_train)</span></pre><p id="cc39" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">现在我们已经拟合了模型，让我们做另一轮的特征选择。</p><h1 id="0a30" class="oc mq jj bd mr od on of mu og oo oi mx kp op kq na ks oq kt nd kv or kw ng om bi translated">特征系数</h1><p id="46a0" class="pw-post-body-paragraph lf lg jj lh b li ni kk lk ll nj kn ln lo nk lq lr ls nl lu lv lw nm ly lz ma im bi translated">如果你正在运行一个回归任务，那么特征适合度的一个关键指标是回归系数(所谓的<em class="ow"> beta </em>系数)，它显示了特征在模型中的相对贡献。有了这些信息，您就可以删除贡献很小或没有贡献的功能。</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="bcde" class="mp mq jj ns b gy nw nx l ny nz"># feature coefficients<br/>coeffs = model.coef_</span><span id="c1ec" class="mp mq jj ns b gy oa nx l ny nz"># visualizing coefficients<br/>index = X_train.columns.tolist()</span><span id="169e" class="mp mq jj ns b gy oa nx l ny nz">(pd.DataFrame(coeffs, index = index, columns = ['coeff']).sort_values(by = 'coeff')<br/> .plot(kind='barh', figsize=(4,10)))</span></pre><figure class="nn no np nq gt iv gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/3d2b3d83d771938a322645c37395d41d.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*j6J3zfwEdAZH9b3u69R-AQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">每个特性的Beta值/系数(图:作者)</p></figure><p id="3906" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">如你所见，有些贝塔系数很小，对汽车价格的预测贡献不大。您可以过滤掉这些特征:</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="0715" class="mp mq jj ns b gy nw nx l ny nz"># filter variables near zero coefficient value<br/>temp = pd.DataFrame(coeffs, index = index, columns = ['coeff']).sort_values(by = 'coeff')<br/>temp = temp[(temp['coeff']&gt;1) | (temp['coeff']&lt; -1)]</span><span id="bd0c" class="mp mq jj ns b gy oa nx l ny nz"># drop those features<br/>cols_coeff = temp.index.to_list()<br/>X_train = X_train[cols_coeff]<br/>X_test = X_test[cols_coeff]</span></pre><h1 id="618e" class="oc mq jj bd mr od on of mu og oo oi mx kp op kq na ks oq kt nd kv or kw ng om bi translated">p值</h1><p id="3407" class="pw-post-body-paragraph lf lg jj lh b li ni kk lk ll nj kn ln lo nk lq lr ls nl lu lv lw nm ly lz ma im bi translated">在回归中，p值告诉我们预测值和目标值之间的关系是否具有统计显著性。<code class="fe os ot ou ns b">statsmodels</code>库用特征系数和相关的<em class="ow"> p </em>值给出了回归输出的漂亮摘要。</p><p id="c69a" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">如果一些特性不重要，您可以逐个删除它们，每次都重新运行模型，直到您找到一组具有重要的<em class="ow"> p </em>值并通过更高的调整R2提高了性能的特性。</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="3826" class="mp mq jj ns b gy nw nx l ny nz">import statsmodels.api as sm<br/>ols = sm.OLS(y, X).fit()<br/>print(ols.summary())</span></pre><figure class="nn no np nq gt iv gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/519b1d4664d640e385247f9ea3e2bf10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*ZkVROki0ZMFUfsblFLLZKw.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">具有beta值和统计显著性的回归结果的部分摘要(图:作者)</p></figure><h1 id="0d80" class="oc mq jj bd mr od on of mu og oo oi mx kp op kq na ks oq kt nd kv or kw ng om bi translated">差异通货膨胀系数(VIF)</h1><p id="7141" class="pw-post-body-paragraph lf lg jj lh b li ni kk lk ll nj kn ln lo nk lq lr ls nl lu lv lw nm ly lz ma im bi translated">正如我前面提到的，方差膨胀因子(VIF)是测量多重共线性的另一种方法。它被测量为总模型方差与每个独立特征方差的比率。某个功能的高VIF表明它与一个或多个其他功能相关。根据经验法则:</p><p id="edeb" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">VIF = 1表示无相关性，<br/>VIF = 1–5中度相关，<br/> VIF &gt; 5高度相关</p><p id="1bed" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">可以想象，VIF是一种消除多重共线性要素的有用技术。在我们的演示中，让我们大方一点，保留所有VIF低于10的特性。</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="47d3" class="mp mq jj ns b gy nw nx l ny nz">from statsmodels.stats.outliers_influence import variance_inflation_factor</span><span id="c6be" class="mp mq jj ns b gy oa nx l ny nz"># calculate VIF<br/>vif = pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)</span><span id="c43d" class="mp mq jj ns b gy oa nx l ny nz"># display VIFs in a table<br/>index = X_train.columns.tolist()<br/>vif_df = pd.DataFrame(vif, index = index, columns = ['vif']).sort_values(by = 'vif', ascending=False)<br/>vif_df[vif_df['vif']&lt;10]</span></pre><figure class="nn no np nq gt iv gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/6a4d9ec807dc86998c4e7bba72962a74.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*6OEJPXBfA1dD9JQWCn1uaA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">基于低VIF选择的特征(图:作者)</p></figure><h1 id="8d6c" class="oc mq jj bd mr od on of mu og oo oi mx kp op kq na ks oq kt nd kv or kw ng om bi translated">基于特征重要性/杂质的选择</h1><p id="a49a" class="pw-post-body-paragraph lf lg jj lh b li ni kk lk ll nj kn ln lo nk lq lr ls nl lu lv lw nm ly lz ma im bi translated">决策树/随机森林使用最大程度减少杂质的特征来分割数据(根据基尼杂质或信息增益来衡量)。这意味着，找到最佳特征是算法在分类任务中如何工作的关键部分。然后，我们可以通过<code class="fe os ot ou ns b">feature_importances_</code>属性访问最佳特性。</p><p id="511f" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">让我们在数据集上实现一个随机森林模型，并过滤一些要素。</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="f15e" class="mp mq jj ns b gy nw nx l ny nz">from sklearn.ensemble import RandomForestClassifier</span><span id="6a35" class="mp mq jj ns b gy oa nx l ny nz"># instantiate model<br/>model = RandomForestClassifier(n_estimators=200, random_state=0)<br/># fit model<br/>model.fit(X,y)</span></pre><p id="a03a" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">现在我们来看看特性的重要性:</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="d05b" class="mp mq jj ns b gy nw nx l ny nz">#  feature importance<br/>importances = model.feature_importances_</span><span id="a3d7" class="mp mq jj ns b gy oa nx l ny nz"># visualization<br/>cols = X.columns<br/>(pd.DataFrame(importances, cols, columns = ['importance'])<br/> .sort_values(by='importance', ascending=True)<br/> .plot(kind='barh', figsize=(4,10)))</span></pre><figure class="nn no np nq gt iv gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/43e4b4df19a7e86a226a536a3c1eccad.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*5033w1Q3uZzFmHWPIrUdxQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">随机森林实现中的特性重要性(图:作者)</p></figure><p id="7605" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">上面的输出显示了每个特性在减少每个节点/分裂处的杂质方面的重要性。</p><p id="f385" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">由于随机森林分类器有许多估计器(例如，在上面的例子中有200棵决策树)，我们可以用置信区间来计算相对重要性的估计。让我们想象一下。</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="0afc" class="mp mq jj ns b gy nw nx l ny nz"># calculate standard deviation of feature importances <br/>std = np.std([i.feature_importances_ for i in model.estimators_], axis=0)</span><span id="2b97" class="mp mq jj ns b gy oa nx l ny nz"># visualization<br/>feat_with_importance  = pd.Series(importances, X.columns)<br/>fig, ax = plt.subplots(figsize=(12,5))<br/>feat_with_importance.plot.bar(yerr=std, ax=ax)<br/>ax.set_title("Feature importances")<br/>ax.set_ylabel("Mean decrease in impurity")</span></pre><figure class="nn no np nq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pi"><img src="../Images/599ecbb7bc7f69b94e129efdcdbf2f15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fykLHbPy7hrgMGHQ3-M0Pg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">随机森林实现中具有置信区间的特征重要性(图:作者)</p></figure><p id="4dae" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">既然我们知道了每个特性的重要性，我们就可以手动地(或者编程地)决定保留哪些特性，放弃哪些特性。</p><h1 id="4ce4" class="oc mq jj bd mr od on of mu og oo oi mx kp op kq na ks oq kt nd kv or kw ng om bi translated">通过Scikit Learn实现自动特征选择</h1><p id="537e" class="pw-post-body-paragraph lf lg jj lh b li ni kk lk ll nj kn ln lo nk lq lr ls nl lu lv lw nm ly lz ma im bi translated">幸运的是，在<code class="fe os ot ou ns b">sklearn</code>库中有一个完整的模块，只用几行代码就可以处理特性选择。</p><p id="635d" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">在<code class="fe os ot ou ns b">sklearn</code>中有许多自动化的流程，但在这里我只演示几个:</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="0480" class="mp mq jj ns b gy nw nx l ny nz"># import modules<br/>from sklearn.feature_selection import (SelectKBest, chi2, SelectPercentile, SelectFromModel, SequentialFeatureSelector, SequentialFeatureSelector)</span></pre><h2 id="59ed" class="mp mq jj bd mr ms mt dn mu mv mw dp mx lo my mz na ls nb nc nd lw ne nf ng nh bi translated">a)基于卡方的技术</h2><p id="6995" class="pw-post-body-paragraph lf lg jj lh b li ni kk lk ll nj kn ln lo nk lq lr ls nl lu lv lw nm ly lz ma im bi translated">基于卡方的技术基于一些预定义的分数选择特定数量的用户定义的特征(k)。这些分数是通过计算X(自变量)和y(因变量)之间的卡方统计来确定的。在<code class="fe os ot ou ns b">sklearn</code>中，你需要做的就是确定你想要保留多少功能。如果您想保留10个特性，实现将如下所示:</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="dba4" class="mp mq jj ns b gy nw nx l ny nz"># select K best features<br/>X_best = SelectKBest(chi2, k=10).fit_transform(X,y)</span><span id="a67d" class="mp mq jj ns b gy oa nx l ny nz"># number of best features<br/>X_best.shape[1]</span><span id="16c6" class="mp mq jj ns b gy oa nx l ny nz">&gt;&gt; 10</span></pre><p id="d1c1" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">如果有大量的特性，你可以指定你想要保留的特性的百分比。假设我们想保留75%的特性，放弃剩下的25%:</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="a56f" class="mp mq jj ns b gy nw nx l ny nz"># keep 75% top features <br/>X_top = SelectPercentile(chi2, percentile = 75).fit_transform(X,y)</span><span id="c2c2" class="mp mq jj ns b gy oa nx l ny nz"># number of best features<br/>X_top.shape[1]</span><span id="63c7" class="mp mq jj ns b gy oa nx l ny nz">&gt;&gt; 36</span></pre><h2 id="7015" class="mp mq jj bd mr ms mt dn mu mv mw dp mx lo my mz na ls nb nc nd lw ne nf ng nh bi translated">b)正规化</h2><p id="832e" class="pw-post-body-paragraph lf lg jj lh b li ni kk lk ll nj kn ln lo nk lq lr ls nl lu lv lw nm ly lz ma im bi translated">正则化减少了过度拟合。如果有太多的特征，正则化会通过收缩特征系数(称为L2正则化)或将一些特征系数设置为零(称为L1正则化)来控制它们的效果。</p><p id="502e" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">一些模型内置了L1/L2正则化作为惩罚特征的超参数。使用元转换器<code class="fe os ot ou ns b">SelectFromModel</code>可以消除这些特征。</p><p id="cb3d" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">让我们用<em class="ow">惩罚=‘L1’</em>来实现一个<code class="fe os ot ou ns b">LinearSVC </code>算法。然后我们将使用<code class="fe os ot ou ns b">SelectFromModel </code>删除一些功能。</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="4077" class="mp mq jj ns b gy nw nx l ny nz"># implement algorithm<br/>from sklearn.svm import LinearSVC<br/>model = LinearSVC(penalty= 'l1', C = 0.002, dual=False)<br/>model.fit(X,y)</span><span id="86e9" class="mp mq jj ns b gy oa nx l ny nz"># select features using the meta transformer<br/>selector = SelectFromModel(estimator = model, prefit=True)</span><span id="8a53" class="mp mq jj ns b gy oa nx l ny nz">X_new = selector.transform(X)<br/>X_new.shape[1]</span><span id="de7c" class="mp mq jj ns b gy oa nx l ny nz">&gt;&gt; 2</span><span id="bbc3" class="mp mq jj ns b gy oa nx l ny nz"># names of selected features<br/>feature_names = np.array(X.columns)<br/>feature_names[selector.get_support()]</span><span id="25f3" class="mp mq jj ns b gy oa nx l ny nz">&gt;&gt; array(['wheel-base', 'horsepower'], dtype=object)</span></pre><h2 id="02ae" class="mp mq jj bd mr ms mt dn mu mv mw dp mx lo my mz na ls nb nc nd lw ne nf ng nh bi translated">c)顺序选择</h2><p id="13a8" class="pw-post-body-paragraph lf lg jj lh b li ni kk lk ll nj kn ln lo nk lq lr ls nl lu lv lw nm ly lz ma im bi translated">顺序特征选择是一种经典的统计技术。在这种情况下，您可以一次添加/删除一个特征，并检查模型性能，直到它根据您的需要进行了优化。</p><p id="cf7a" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">顺序选择有两种变体。正向选择技术从0个特征开始，然后添加一个最大程度地减小误差的特征；然后添加另一个特性，以此类推。</p><p id="da06" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">反向选择的工作方向相反。该模型从包括所有特征开始，并计算误差；然后，它消除了一个进一步减少误差的特征。重复该过程，直到剩余期望数量的特征。</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="e2ba" class="mp mq jj ns b gy nw nx l ny nz"># instantiate model<br/>model = RandomForestClassifier(n_estimators=100, random_state=0)</span><span id="27e3" class="mp mq jj ns b gy oa nx l ny nz"># select features<br/>selector = SequentialFeatureSelector(estimator=model, n_features_to_select=10, direction='backward', cv=2)<br/>selector.fit_transform(X,y)</span><span id="c98d" class="mp mq jj ns b gy oa nx l ny nz"># check names of features selected<br/>feature_names = np.array(X.columns)<br/>feature_names[selector.get_support()]</span><span id="f5ed" class="mp mq jj ns b gy oa nx l ny nz">&gt;&gt; array(['bore', 'make_mitsubishi', 'make_nissan', 'make_saab',<br/>       'aspiration_turbo', 'num-of-doors_two', 'body style_hatchback', 'engine-type_ohc', 'num-of-cylinders_twelve', 'fuel-system_spdi'], dtype=object)</span></pre><h1 id="24b6" class="oc mq jj bd mr od on of mu og oo oi mx kp op kq na ks oq kt nd kv or kw ng om bi translated">主成分分析</h1><p id="6d94" class="pw-post-body-paragraph lf lg jj lh b li ni kk lk ll nj kn ln lo nk lq lr ls nl lu lv lw nm ly lz ma im bi translated">主成分分析的主要目的是降低高维特征空间的维数。在这种情况下，原始特征被重新投影到新的维度(即主成分)。最终目标是找到最能解释数据方差的分量数。</p><pre class="nn no np nq gt nr ns nt nu aw nv bi"><span id="aa3a" class="mp mq jj ns b gy nw nx l ny nz"># import PCA module<br/>from sklearn.decomposition import PCA</span><span id="4cab" class="mp mq jj ns b gy oa nx l ny nz"># scaling data<br/>X_scaled = scaler.fit_transform(X)</span><span id="2856" class="mp mq jj ns b gy oa nx l ny nz"># fit PCA to data<br/>pca = PCA()<br/>pca.fit(X_scaled)<br/>evr = pca.explained_variance_ratio_</span><span id="7122" class="mp mq jj ns b gy oa nx l ny nz"># visualizing the variance explained by each principal components</span><span id="dd46" class="mp mq jj ns b gy oa nx l ny nz">plt.figure(figsize=(12, 5))<br/>plt.plot(range(0, len(evr)), evr.cumsum(), marker="o", linestyle="--")</span><span id="1f8e" class="mp mq jj ns b gy oa nx l ny nz">plt.xlabel("Number of components")<br/>plt.ylabel("Cumulative explained variance")</span></pre><figure class="nn no np nq gt iv gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/ac1f0010b380ad457683b8b5469d1860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*QJBln_8UtSny_euz-1BI3g.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">主成分分析解释的累积方差(图:作者)</p></figure><p id="f10e" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">如您所见，20个主成分解释了80%以上的方差，因此您可以将您的模型与这20个成分相匹配。您可以预先确定方差阈值，并选择所需的主成分数。</p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="5e73" class="oc mq jj bd mr od oe of mu og oh oi mx kp oj kq na ks ok kt nd kv ol kw ng om bi translated">一锤定音</h1><p id="20d4" class="pw-post-body-paragraph lf lg jj lh b li ni kk lk ll nj kn ln lo nk lq lr ls nl lu lv lw nm ly lz ma im bi translated">希望这是一个有用的指南，可以应用于特征选择的各种技术。一些技术在之前<em class="ow">应用于拟合模型，例如删除具有缺失值的列、不相关的列、具有多重共线性的列以及使用PCA进行降维，而其他技术在</em>基础模型实施之后<em class="ow">应用，例如特征系数、p值、VIF等。你可能永远不会在一个项目中使用所有的策略，但是，你可以把这个列表作为一个清单。</em></p><p id="204c" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">我已经在<a class="ae jg" href="https://github.com/mabalam/feature_selection" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上传了这里描述的所有技术的Jupyter笔记本。</p><p id="e6f6" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">感谢您的阅读。请随意<a class="ae jg" href="https://mab-datasc.medium.com/subscribe" rel="noopener">订阅</a>以获得我即将发表的文章的通知，或者通过<a class="ae jg" href="https://www.linkedin.com/in/mab-alam/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>与我联系。</p></div></div>    
</body>
</html>