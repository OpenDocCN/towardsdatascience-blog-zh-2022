<html>
<head>
<title>ML Basics (Part-1): REGRESSION — A Gateway Method to Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML 基础(第一部分):回归——机器学习的入门方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ml-basics-part-1-regression-a-gateway-method-to-machine-learning-36d54d233907#2022-12-04">https://towardsdatascience.com/ml-basics-part-1-regression-a-gateway-method-to-machine-learning-36d54d233907#2022-12-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="473d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><strong class="ak">简单而全面地介绍线性、非线性和逻辑回归</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/85e8e532fff172bf2a5c23a49aeb9b64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CAUWagKY_C5lARMXO5vqwA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 1:线性、非线性和逻辑回归的例子(来源:作者)</p></figure><p id="c486" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">前言:</strong></p><p id="4489" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">人们对机器学习中基本主题的介绍性帖子越来越感兴趣。因此，我将从这篇文章开始，在接下来的文章中讨论这些话题。这篇文章基本上是独立的，但是，它需要对<em class="lu">线性代数</em>和<em class="lu">微积分</em>有基本的理解。</p><p id="478b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">回归</strong></p><p id="e089" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">回归是估计因变量(<strong class="la iu"> Y </strong>)与一个或多个自变量(<strong class="la iu"> Xi </strong>)之间关系的过程。它主要用于在给定的一组数据样本中寻找模式，并在给定一组其他变量的值的情况下预测一个变量的值。它有广泛的应用，从预测天气预报到预测房价。这些变量的值通常是连续的，但是，也有回归方法的子集，其中的值可以是离散的(例如，名义值/序数)。一会儿我们会看到不同类型的回归方法。</p><p id="6aa6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">线性回归</strong></p><p id="1ac0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最简单的形式是，回归模型是一个线性函数，通过对自变量的值进行线性组合，可以获得因变量的值。例如，如果数据是二维的，这种模型可以写成直线方程(图 2)。一个直线方程有三个系数(<strong class="la iu"> a </strong>、<strong class="la iu"> b </strong>和<strong class="la iu"> c </strong>)。如果我们把因变量<strong class="la iu"> y </strong>写成<strong class="la iu"> y </strong> = f( <strong class="la iu"> x </strong>)，那么我们看到只需要一个斜率(<strong class="la iu"> m </strong>)和一个截距'<em class="lu"> c </em>'来表示任意一条线。这意味着，如果我们想找到一条最适合一组数据点的线，那么我们只需要找到这两个变量的最佳值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/8dcf094debbab8d59cb4b10f9b10aad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JoO0qq_a5wpDePN-jQIbSw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 2:直线方程的例子(来源:作者)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/56c42663939a590a3685ad884987205b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*KM-4gAHpBGrN14zq80me3g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3:一组随机的数据点(来源:作者)</p></figure><p id="fafe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，让我们看一组数据点的例子，如图 3 所示。我们看到，大多数点正在形成类似于直线的模式，因此，我们可以将该数据建模为线性回归问题。更具体地说，我们感兴趣的是找到最适合这些数据点的直线的斜率和截距。</p><p id="c9b6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">通过求解线性方程组进行直线拟合</strong></p><p id="fbfc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们把每个点写成一个线性组合(<strong class="la iu"> y = mx + c </strong>)，我们最终会得到一组线性方程组。为了找到'<em class="lu"> m' </em>和'<em class="lu"> c' </em> <strong class="la iu"> </strong>的值，我们至少需要两个方程。我们看到我们有两个以上的方程，因此，我们的问题是可解的。更一般地说，我们可以写出一组具有‘n’个样本和’<em class="lu">d’</em>维的线性方程，如图 4 所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/44ff98373430eb8fa48a78c588524fee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*2H9OTVoEcQBbokgqeX5ySA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 4:线性方程组的解(来源:作者)</p></figure><p id="b9e2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以把所有的斜率和截距<em class="lu"> </em>组织成一个组合向量，叫做'<strong class="la iu"> <em class="lu"> w' </em> </strong>。此外，我们可以在<strong class="la iu"> <em class="lu"> X </em> </strong>中添加一列 1，以使乘积成为可能，并获得值的适当线性组合。然后我们就可以通过矩阵求逆，求解权值'<strong class="la iu"> <em class="lu"> w' </em> </strong>来求解这个线性方程组(<strong class="la iu"> Y = X w </strong>)。该解决方案为给定的数据样本集提供了最佳直线拟合的斜率和截距，如图 5 所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/2c4bbfe06a8b30e80f1874b25d9a6cb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*al8nP5aZMLDhBIb4FVBU9w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 5:一组数据点上的直线拟合(来源:作者)</p></figure><p id="ca3e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">梯度下降直线拟合</strong></p><p id="f354" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">线性方程组的矩阵逆解适用于噪声最小且无奇点的问题。然而，在大多数大型真实数据中，这些约束可能不成立。所以，一种更稳健、更通用的方法常被用来求解线性方程组，称为<em class="lu">梯度下降</em>。这是一种迭代优化方法，通过逐渐向输出解移动来寻找目标函数的最佳点(最小值/最大值)。更具体地，计算目标函数的导数(例如，在这种情况下，d<strong class="la iu">y</strong>/d<strong class="la iu">w</strong>=<strong class="la iu">X</strong>)，并且在最佳点的方向上采取一小步(即，<strong class="la iu">J =</strong>|<strong class="la iu">y</strong>-<strong class="la iu">y’|</strong>-&gt;-<strong class="la iu">y’</strong>的 T42 每一步后的权重更新为<strong class="la iu"><em class="lu">w’</em></strong>=<strong class="la iu"><em class="lu">w</em>——<em class="lu">alpha</em>*<strong class="la iu">J</strong>其中<em class="lu"> alpha </em>为学习率。使用<em class="lu">梯度下降</em>的直线拟合的输出示例如图 6 所示。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/008976220a2981b21be5c1144f5dd3e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*k0LrFvu44kBScJF5tM09Ig.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 6:随机梯度下降法直线拟合(来源:作者)</p></figure><p id="b33f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">非线性回归</strong></p><p id="5b2d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">线性回归适用于变量之间的关系是线性的情况。然而，在许多现实世界的应用程序中，情况并非如此。当因变量和自变量之间的关系不能表示为线性组合时，我们需要使用非线性回归。在图 7 中可以看到一组非线性形式的数据点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/10cf123daa822f95133f87efc394bf52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*qgdmaBc-lxHoKJTy9rvKLw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 7:一组随机生成的数据点(来源:作者)</p></figure><p id="6b2c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">多项式回归</strong></p><p id="2f36" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">非线性回归可以是不同类型、曲线、闭合形状或任何其他形式。当变量之间的关系可以用曲线的形式表示时，它就被表示为一阶大于一的非线性方程。例如，我们在下面的等式中看到一个 k 次多项式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lz"><img src="../Images/bdd20e69785a4f7a384f09c77df3d420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wVkaU96Ffr0GMIIa9QyE3A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多项式方程(来源:作者)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ma"><img src="../Images/31e9fba146f93cbec238a02392bd3116.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fxFOg2yxLqTrE9DKa8VaUQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 8:一个示例多项式函数(来源:作者)</p></figure><p id="8607" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以用线性方程相乘来构造多项式。图 8 给出了一个四次多项式的特例。我们通过将截距分别为<em class="lu"> 1、5、6 </em>和<em class="lu"> 12 </em>的四个简单线性方程相乘，创建了一个复杂的多项式。乘数<em class="lu"> 0.01 </em>提供了一个比例项，而负号反映了整个曲线。这只是为了说明多项式可以有不同的形状。学习这种曲线的主要目的是发现可能存在于真实世界数据集中的复杂模式。例如，数据可能是来自传感器的具有多种模式的连续信号，您可能希望使用非线性回归模型对其进行预测。</p><p id="8944" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们对之前的非线性数据应用多项式，我们会发现它可以很好地表示数据，如图 9 所示。对于该数据，二阶多项式应该足够了。非线性回归的问题表述与线性回归建模相同。然而，在形成权重矩阵时，必须计算高阶项的系数。一般来说，基于多项式的回归最适合于数据分布不广且似乎是连续进行的应用。这种数据的示例类型可以是来自传感器或测量设备的时间序列数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/5a543666533529ddc84b1154378713db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*xKpSAXj1_TKRYvwz6FMzAg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 9:一组数据点的多项式曲线拟合(来源:作者)</p></figure><p id="96f4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">高斯过程回归</strong></p><p id="7e48" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在有些情况下，我们希望模型不仅预测高度精确的函数值，例如直线/多项式拟合，而且还应该考虑余量。在这种情况下，我们学习每个点的分布(即平均值和方差),而不是单一的权重值。它可以直接应用于信号有噪声或有少量湍流的情况。将回归问题建模为高斯过程可以准确预测感官数据。这种高斯过程回归的核心是一个核函数。可以有多种适用于不同类型的数据和应用的内核函数。最常见的核函数是指数函数(例如径向基核)，其提供自变量和因变量之间的非线性关系。然后，回归过程的目标是优化问题，由此每个输入数据点的最合适的<em class="lu">均值</em>和<em class="lu">方差</em>值将被优化。对于不同类型的内核，这种高斯过程回归的样本输出可以在图 10 中看到。曲线周围的条带预测模型在预测某一点的值时的置信度。范围越窄，输出值越严格。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/58cb547cedeac3f2696fa6452bfa6f83.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*kZgSguVJCsBIrYlRSGY0_g.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/1b82b98a0756bbe28b8edab9c06731b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*hu0xp3VNCOYGFSFM5Mq4bQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 10:不同核函数的基于高斯的回归(来源:作者)</p></figure><p id="9f76" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">正则化逻辑回归</strong></p><p id="5923" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">到目前为止，我们已经处理了连续回归问题，其中预测输出变量的值是一个连续数。然而，在许多实际应用中，情况并非如此。例如，在对物体/特征进行分类的情况下，我们想要学习每个类别的离散标签，而不是数值。这个问题可以用一种特殊的回归方法来处理，叫做逻辑回归。它实际上是一种回归方法；但是，它修改了输出值和目标函数，使得对于给定的类，输出是 0 到 1 之间的概率值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi md"><img src="../Images/db349418a96add02e4acf2dc589a95a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hB2jSX8TJf-JGJCM-W4eoQ.png"/></div></div></figure><p id="3901" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这可以通过在 Sigmoid 函数中包装输入变量值“x”来实现。Sigmoid 函数是一个非线性指数函数，它将“x<strong class="la iu">’</strong>的任何值映射到 0 和 1 之间的正值。回归问题的其余部分保持不变。逻辑回归和基于 Sigmoid 函数的输出计算的目标函数在上面的等式中给出。训练集和测试集的数据点可以在图 11 中看到。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi me"><img src="../Images/d14271cd2a6ff27273396734fd67e5e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*4Dwj6qp_h7W9AYWeZSl4Uw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 11:分别用于训练集和测试集的一组示例数据点(来源:作者)</p></figure><p id="e91f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">分类问题需要对一组数据进行训练，而模型需要预测另一组数据的值。因此，重要的是，该模型不要仅对于训练集具有太紧的界限，并且应该对于训练集中不存在的输入值工作良好。这个问题被称为“<em class="lu">过拟合</em>”，通过一种被称为“<em class="lu">规则</em>规则化”的方法来解决。正则化是优化过程中的松弛项，它给模型拟合一个喘息的空间。这是通过在目标函数中增加一个附加项来实现的，如上面的等式所示。这种<em class="lu">正则化</em>项的系数<em class="lu">λ</em>是一个加权因子，其提供了准确性和概化性之间的折衷。这种回归拟合的结果可以在图 12 中看到。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/92bdceecdfa3cebed0a6d23b72618883.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*vBAeHoP9ZjArqjzBU3WDBA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 12:二元分类问题的回归拟合结果(来源:作者)</p></figure><p id="88fd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">结束语</strong></p><p id="bac4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们已经讨论了回归拟合及其类型。线性、非线性和逻辑回归方法是您在实际应用中会遇到的三种主要的回归问题。回归是一种简单而有效的方法，可以解决许多应用领域中的一些常见问题。它也构成了更复杂的机器学习方法的基础，我们将在以后的文章中讨论这些方法。</p><p id="058d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">如需代码，请点击链接:</strong></p><p id="246a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae mg" href="https://www.github.com/azad-academy/MLBasics-Regression" rel="noopener ugc nofollow" target="_blank">https://www.github.com/azad-academy/MLBasics-Regression</a></p><p id="673f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">在 Patreon 上支持我:</strong></p><div class="mh mi gp gr mj mk"><a href="https://patreon.com/azadacademy" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd iu gy z fp mp fr fs mq fu fw is bi translated">J. Rafid Siddiqui 博士正在创建关于 AI/ML/DL(Medium/Substack/Youtube)|…</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">拉菲德博士是一位学术研究者、作家、思想领袖和创新者。他在…写各种主题的文章</h3></div><div class="ms l"><p class="bd b dl z fp mp fr fs mq fu fw dk translated">patreon.com</p></div></div><div class="mt l"><div class="mu l mv mw mx mt my ks mk"/></div></div></a></div><p id="4fb0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">在子栈上找到我:</strong></p><div class="mh mi gp gr mj mk"><a href="https://azadwolf.substack.com" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd iu gy z fp mp fr fs mq fu fw is bi translated">Azad 学院</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">深度学习、机器学习、计算机视觉和人工智能领域的学习场所…</h3></div><div class="ms l"><p class="bd b dl z fp mp fr fs mq fu fw dk translated">azadwolf.substack.com</p></div></div><div class="mt l"><div class="mz l mv mw mx mt my ks mk"/></div></div></a></div><p id="e990" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">关注 Twitter 更新:</strong></p><p id="e695" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae mg" href="https://www.twitter.com/@azaditech" rel="noopener ugc nofollow" target="_blank">https://www.twitter.com/@azaditech</a></p></div></div>    
</body>
</html>