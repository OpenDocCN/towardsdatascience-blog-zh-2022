<html>
<head>
<title>What are PCA loadings and how to effectively use Biplots?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是PCA加载，如何有效地使用双标图？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-are-pca-loadings-and-biplots-9a7897f2e559#2022-04-19">https://towardsdatascience.com/what-are-pca-loadings-and-biplots-9a7897f2e559#2022-04-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8282" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">充分利用主成分分析的实用指南。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/51541b3be845a711e360d233f97187e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oFPSKL-SUUSSvFYWsBopMQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="17c5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">主成分分析是最著名的(大)数据分析技术。然而，解释低维空间中的差异仍然具有挑战性。理解<em class="lu">载荷</em>和解释<em class="lu">双标图</em>是任何使用PCA的人必须知道的部分。<em class="lu">在这里，我将解释I)如何解释深入洞察的负载，以(直观地)解释数据中的差异，ii)如何选择最具信息性的特征，iii)如何创建有洞察力的图，以及最后如何检测异常值。</em>理论背景将由实际操作指南支持，帮助您利用<a class="ae lv" href="https://erdogant.github.io/pca/" rel="noopener ugc nofollow" target="_blank"> pca </a>充分利用数据。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="a01a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">如果你觉得这篇文章很有帮助，可以使用我的</em> <a class="ae lv" href="https://medium.com/@erdogant/membership" rel="noopener"> <em class="lu">推荐链接</em> </a> <em class="lu">继续无限制学习，并注册成为中级会员。另外，</em> <a class="ae lv" href="http://erdogant.medium.com" rel="noopener"> <em class="lu">关注我</em> </a> <em class="lu">关注我的最新内容！</em></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="2f4c" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">介绍</h1><p id="d698" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">在本博客的最后，你可以(直观地)解释数据中的差异，选择最有信息的特征，并创建有洞察力的图表。我们将讨论以下主题:</p><ul class=""><li id="cf55" class="na nb it la b lb lc le lf lh nc ll nd lp ne lt nf ng nh ni bi translated">特征<strong class="la iu">选择</strong>对<strong class="la iu">提取</strong>。</li><li id="1b3e" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated"><strong class="la iu">使用PCA进行尺寸缩减</strong>。</li><li id="d777" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated"><strong class="la iu">解释了</strong> <em class="lu">、</em>和<strong class="la iu">小块图</strong>。</li><li id="c135" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated"><strong class="la iu">加载</strong>和<strong class="la iu">双标图</strong>。</li><li id="7455" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated">提取<strong class="la iu">最具信息性的特征</strong>。</li><li id="5250" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated"><strong class="la iu">离群点检测</strong>。</li></ul></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="2575" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">PCA的温和介绍。</h1><p id="a805" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">主成分分析的主要目的是通过最小化信息损失来降低数据集中的维数。一般来说，降维有两种方式:<strong class="la iu"> <em class="lu">特征选择</em> </strong>和<strong class="la iu"> <em class="lu">特征提取</em> </strong>。其中，后者用于<em class="lu"> PCA，其中基于原始特征的(线性)组合构建一组新的维度或潜在变量。</em>在<strong class="la iu"> <em class="lu">特征选择</em> </strong>的情况下，选择一个特征子集，该子集将为前面的任务提供信息。无论您选择哪种技术，降维都是重要的一步，原因有几个，例如<em class="lu">降低复杂性、提高运行时间、确定特性重要性、可视化类信息，以及最后但同样重要的是防止</em> <strong class="la iu"> <em class="lu">降维的灾难。</em> </strong>这意味着，对于给定的样本大小，超过一定数量的特征，分类器的性能将会下降，而不是提高(图1)。在大多数情况下，低维空间会导致更精确的映射，并补偿信息的“损失”。</p><p id="59c8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在下一节中，我将解释如何在特征选择和特征提取技术之间进行选择，因为有理由在这两者之间进行选择。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/0aa7cdf8da7ed9690a8532bd81cf2dbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5WLkZJBt2bw0laY757LUdA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。作为维度函数的(分类)模型的性能。(图片由作者提供)</p></figure><h2 id="3f72" class="np me it bd mf nq nr dn mj ns nt dp mn lh nu nv mp ll nw nx mr lp ny nz mt oa bi translated">特征选择。</h2><p id="0de1" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated"><strong class="la iu">特征选择在许多情况下是必要的</strong>；<strong class="la iu"><em class="lu"/></strong><em class="lu">1</em><strong class="la iu"><em class="lu">。</em> </strong>如果特征不是数字(如字符串)。2.以防需要提取有意义的特征。3.保持测量值不变(转换会将测量值和要丢失的单位进行线性组合)。<strong class="la iu">的缺点</strong>是特征选择过程确实需要<em class="lu">搜索策略和/或</em> <em class="lu">目标函数</em>来评估和选择潜在的候选者。例如，它可能需要一种使用类别信息的监督方法来执行统计测试，或者需要一种交叉验证方法来选择信息最丰富的特征。然而，特征选择也可以在没有类别信息的情况下完成，例如通过选择方差上的前<em class="lu"> N </em>个特征(越高越好)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/45d0e3284cb75b11c78f4ef9a61cb6e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FijZiLug9EFgHMTtUB6ERQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。特性选择程序的示意图。(图片由作者提供)</p></figure><h2 id="ceb4" class="np me it bd mf nq nr dn mj ns nt dp mn lh nu nv mp ll nw nx mr lp ny nz mt oa bi translated">特征提取。</h2><p id="7cb9" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">特征提取方法可以减少维数，同时最大限度地减少信息损失。为此，我们需要一个转换<em class="lu">函数</em>；<strong class="la iu"> y=f(x) </strong> <em class="lu">。</em>在PCA的情况下，变换限于线性函数，我们可以将其重写为一组权重<em class="lu">，组成变换步骤；</em> <strong class="la iu"> <em class="lu"> y=Wx，</em> </strong> <em class="lu">其中W为权重，x为输入特征，y为最终变换后的特征空间</em>。请参见下面的示意图，演示转换步骤和数学步骤。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/0875df40bd1aa1604d17e35792ce8137.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MlPasi_e1weqxSuPMiZxaQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3。将输入数据线性转换为y=Wx形式的特征提取过程的示意图。(图片由作者提供)</p></figure><p id="1cae" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">PCA的线性变换也有一些缺点<strong class="la iu"/>。这将使特性更难解释，有时甚至对某些用例的后续工作毫无用处。例如，如果使用特征提取技术发现了潜在的癌症相关基因，则可以描述该基因与其他基因一起部分参与。实验室中的后续研究没有意义，例如，部分敲除/激活基因。</p><h1 id="fae4" class="md me it bd mf mg od mi mj mk oe mm mn jz of ka mp kc og kd mr kf oh kg mt mu bi translated">PCA是如何降维的？</h1><p id="62e6" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">我们可以将PCA分解成大致四个部分，我将举例说明。</p><h2 id="8cb7" class="np me it bd mf nq nr dn mj ns nt dp mn lh nu nv mp ll nw nx mr lp ny nz mt oa bi translated">第一部分。将数据围绕原点居中。</h2><p id="ef08" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">第一部分是计算数据的平均值(如图4所示)，这可以通过四个较小的步骤来完成。首先通过计算每个特征的平均值<strong class="la iu"> (1和2) </strong>，然后计算中心(<strong class="la iu"> 3 </strong>)。<strong class="la iu"> </strong>我们现在可以移动数据，使其以原点为中心(<strong class="la iu"> 4 </strong>)。请注意，此变换步骤不会改变点之间的相对距离，而只是将数据集中在原点周围。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/824723fbe4316281041da4c4b780537e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q1w-Ojbr383Y01ejwaxvjA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4:以零为中心的数据。(图片由作者提供)</p></figure><h2 id="f5ed" class="np me it bd mf nq nr dn mj ns nt dp mn lh nu nv mp ll nw nx mr lp ny nz mt oa bi translated">第二部分。通过原点和数据点拟合直线。</h2><p id="487b" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">下一步是拟合一条穿过原点和数据点(或样本)的线。这可以通过1。通过原点画一条随机线，2。将样本正交投影到直线上，然后3 .旋转直到通过最小化距离找到最佳匹配。<em class="lu">然而，最大化投影数据点到原点</em>的距离更实际，这将导致相同的结果。使用距离平方和(SS)计算拟合，因为它将消除线周围数据点的方向。在这一点上(图5)，我们在方差最大的方向上拟合了一条线。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/3f20269284ed05825e066220ee1f525f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a7lrW-VLNkxCfz1n7uouyA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5:找到最合适的。从一条随机线(顶部)开始，通过最小化从数据点到线(底部)的距离，旋转直到它最适合数据。(图片由作者提供)</p></figure><h2 id="bcc7" class="np me it bd mf nq nr dn mj ns nt dp mn lh nu nv mp ll nw nx mr lp ny nz mt oa bi translated">第三部分。计算主成分和载荷。</h2><p id="5ab2" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">我们在变化最大的方向上确定了最佳拟合线，现在是第<strong class="la iu">个主成分</strong>或<strong class="la iu"> PC1 </strong>。下一步是计算PC1的斜率，该斜率描述了每个特征对PC1的贡献。在这个例子中，我们可以<em class="lu">直观地</em>观察到数据点在特征1上比在特征2上分散得更多(图6)。红线的斜率代表我们的视觉观察；我们每穿过特征1(向右)2个单位，它就在特征2的轴上向下移动1个单位。换句话说，要制作PC1(红线)，我们需要特征1的2部分和特征2的-1部分。我们可以将这些"<em class="lu">部分"</em>描述为<em class="lu">向量b </em>和<em class="lu"> c </em>，然后我们可以用它们来计算<em class="lu">向量a</em>向量<em class="lu"> a </em>将得到值2.23(见图6)。这就是我们所说的这台特殊电脑的<strong class="la iu">特征向量。</strong></p><p id="9cde" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是，我们需要向所谓的“<em class="lu">单位向量</em>标准化，这是通过将所有向量除以<em class="lu"> a </em> =2.23得到的。这样向量<em class="lu"> b </em> =2/2.23=0.85，向量<em class="lu"> c </em> =1/2.23=0.44，向量a=1 ( <em class="lu">又名单位向量</em>)。因此，换句话说，这些向量的范围在-1和1之间。例如，如果<em class="lu">向量b </em>非常大，比如值趋向于1，则意味着特征1几乎全部贡献给PC1。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/9a508f9cc112885d4d637ba667283fac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wtvSxpwbdYz-Ji_6_Mx5RA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6:计算PC1和PC2并确定负载。(图片由作者提供)</p></figure><p id="7bef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下一步是确定PC2，它是一条穿过原点并垂直于第一个PC的线。在本例中，只有两个特征，但如果有更多特征，第三条PC将成为通过原点并垂直于PC1和PC2的最佳拟合线。如前所述:<em class="lu">新的潜在变量，又名PCs，是初始特征的线性组合。PC中使用的每个特征的比例称为系数。</em></p><h1 id="5aee" class="md me it bd mf mg od mi mj mk oe mm mn jz of ka mp kc og kd mr kf oh kg mt mu bi translated">装货</h1><p id="4f39" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated"><em class="lu">重要的是要认识到主成分的可解释性较差，并且没有任何实际意义，因为它们是由初始变量的线性组合构成的。但是我们可以分析描述独立变量重要性的载荷。载荷是从数值的角度来看的，等于变量的系数，并提供了哪些变量对构件的贡献最大的信息。</em></p><ul class=""><li id="9d6f" class="na nb it la b lb lc le lf lh nc ll nd lp ne lt nf ng nh ni bi translated">负载范围从-1到1。</li><li id="1b00" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated">高绝对值(接近1或-1)表示变量强烈影响组件。接近0的值表示变量对组件的影响很小。</li><li id="3c0d" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated">负荷的符号(+或-)表示变量和主成分是正相关还是负相关。</li></ul><h2 id="772c" class="np me it bd mf nq nr dn mj ns nt dp mn lh nu nv mp ll nw nx mr lp ny nz mt oa bi translated">第四部分。转换和解释方差。</h2><p id="63b0" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">我们计算了PCs，现在可以旋转(或变换)整个数据集，使x轴是方差最大的方向(也称为PC1)。注意，转换步骤将导致原始特征的值将丢失。相反，每个PC将包含总变化的一部分，但是用<strong class="la iu"> <em class="lu">解释的方差</em> </strong>我们可以描述每个PC包含多少方差。为了计算<em class="lu">解释的方差</em>，我们可以将每台电脑的距离平方和(SS)除以数据点数减1。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/ccfc88f14787a30d5b842015bc03db28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yu6QzKyDO50I91FtBU2pGg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7。整个数据集的转换和确定计算解释的方差。(图片由作者提供)。</p></figure><h2 id="5133" class="np me it bd mf nq nr dn mj ns nt dp mn lh nu nv mp ll nw nx mr lp ny nz mt oa bi translated">第0部分。标准化</h2><p id="dd2d" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">在我们做第1到第4部分之前，通过标准化获得正确的数据是至关重要的，因此这应该是第一部分。因为我们搜索具有最大方差的方向，所以PCA对具有不同值范围的变量或异常值的存在非常敏感。<em class="lu">如果初始变量的范围差异较大，则范围较大的变量将优于范围较小的变量。我将在下一节演示这一点。</em>为了防止这种情况，我们需要将初始变量的范围标准化，以便每个变量对分析的贡献相等。我们可以通过减去平均值并除以每个变量的每个值的标准偏差来实现。标准化包括重新缩放要素，使其具有均值为零、标准差为一的标准正态分布的属性。这也称为z分数标准化，Scikit-learn为此使用了StandardScaler()。一旦标准化完成，所有的变量都应该在同一尺度上。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="0c3f" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">PCA库。</h1><p id="dd7f" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">关于用于即将到来的分析的<a class="ae lv" href="https://erdogant.github.io/pca" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> p </em> ca </a>库的一些话。<a class="ae lv" href="https://erdogant.github.io/pca" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> p </em> ca </a>库旨在应对一些挑战，例如:</p><ul class=""><li id="823f" class="na nb it la b lb lc le lf lh nc ll nd lp ne lt nf ng nh ni bi translated"><strong class="la iu"> <em class="lu">分析不同类型的数据。</em> </strong>除了常规<strong class="la iu"> <em class="lu"> PCA </em> </strong>之外，该库还包括<strong class="la iu"> <em class="lu">稀疏PCA </em> </strong>，其稀疏度可以通过L1惩罚系数来控制。还有一种<strong class="la iu"> <em class="lu">截断SVD </em> </strong>可以有效地处理稀疏矩阵，因为它在计算奇异值分解之前不将数据居中。</li><li id="c7d3" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated"><strong class="la iu"> <em class="lu">计算并绘制被解释的方差。</em> </strong>拟合数据后，可以绘制出被解释的方差:<strong class="la iu"><em class="lu">scree</em></strong><strong class="la iu"><em class="lu">plot</em></strong>。</li><li id="841d" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated"><strong class="la iu"> <em class="lu">提取性能最好的特征。</em> </strong>模型返回性能最好的特征。</li><li id="cca3" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated"><strong class="la iu"> <em class="lu">洞察双标图的加载情况。</em> </strong>检索与PCs相关的类的特征和可分性的变化的更多见解。</li><li id="d746" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated"><strong class="la iu"> <em class="lu">离群点检测</em>。</strong>可以使用两种众所周知的方法来检测异常值:<strong class="la iu"> <em class="lu">【霍特林-T2】</em></strong>和<strong class="la iu"> <em class="lu"> SPE-Dmodx。</em> </strong></li><li id="43e1" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated"><strong class="la iu">去除</strong>不需要的(技术)<strong class="la iu">偏差</strong>。数据可以通过从原始数据集中去除(技术)偏差的方式进行标准化。</li></ul><p id="6d01" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu"/><a class="ae lv" href="https://erdogant.github.io/pca" rel="noopener ugc nofollow" target="_blank"><em class="lu">PCA</em></a><em class="lu">相比其他实现有什么好处？</em></p><ul class=""><li id="4aee" class="na nb it la b lb lc le lf lh nc ll nd lp ne lt nf ng nh ni bi translated">在<em class="lu"> PCA </em>库的核心，<em class="lu"> sklearn </em>库用于最大化兼容性及其在管道中的集成。</li><li id="5607" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated">标准化是内置的功能。</li><li id="3191" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated">包含最需要的输出和绘图。</li><li id="0c8d" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated">简单直观。</li><li id="52bb" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated"><a class="ae lv" href="https://erdogant.github.io/pca/" rel="noopener ugc nofollow" target="_blank">开源</a>。</li><li id="eba8" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated"><a class="ae lv" href="https://erdogant.github.io/pca/" rel="noopener ugc nofollow" target="_blank">文档页面有很多例子</a>。</li></ul></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="9a84" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">理解载荷的实际例子。</h1><p id="f67f" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">让我们从一个简单直观的例子开始，演示负载、解释的方差和最重要特征的提取。</p><p id="9695" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们需要安装<a class="ae lv" href="https://erdogant.github.io/pca/" rel="noopener ugc nofollow" target="_blank"><em class="lu">p</em>ca</a><em class="lu"/>库。</p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="be66" class="np me it on b gy or os l ot ou">pip install pca</span></pre><h2 id="b595" class="np me it bd mf nq nr dn mj ns nt dp mn lh nu nv mp ll nw nx mr lp ny nz mt oa bi translated">创建合成数据集。</h2><p id="9d63" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">出于演示目的，我将创建一个包含8个要素和250个样本的合成数据集。每个要素将包含随机整数，但方差不断增加。所有功能都是相互独立的。特性1将包含[0，100]范围内的整数(因此方差最大)，特性2将包含[0，50]范围内的整数，特性3包含[0，25]范围内的整数，依此类推(参见下面的代码块)。<em class="lu">为了举例，我将</em> <strong class="la iu"> <em class="lu">而不是</em> </strong> <em class="lu">将数据归一化来演示原理。</em> <strong class="la iu"> <em class="lu">这个数据集现在理想为1。演示PCA的原理，2。演示负载和解释的差异，以及3。标准化的重要性(或缺乏标准化)。</em> </strong>在我们继续之前，我想再重复一遍:在处理真实世界的数据集时，建议仔细查看您的数据并进行相应的归一化，以使每个要素达到相同的比例。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ov ow l"/></div></figure><p id="5c70" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们绘制8个特征的直方图，我们可以看到特征1(灰色)的方差最大，其次是粉红色(特征2)，然后是褐色(特征3)，等等。最小的方差出现在红色条中(特征8)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/3b8a1511acc6a9cdbb229c8e699a4ea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eIiLA04dMPILZyAX5Z01uA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图8。8个独立特征的直方图(图片由作者提供)。</p></figure><h2 id="a426" class="np me it bd mf nq nr dn mj ns nt dp mn lh nu nv mp ll nw nx mr lp ny nz mt oa bi translated">我们应该期待发现什么？</h2><p id="e3bd" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated"><em class="lu">在我们对这个受控示例数据集进行PCA分析之前，让我们想一想我们应该期望发现什么。</em>首先，通过PCA分析，我们旨在以这样一种方式转换空间，即具有最大方差的特征将显示在第一分量中。在这个数据集中，我们很容易知道<em class="lu">哪些</em>特征包含最大的方差。我们期望如下:</p><ul class=""><li id="33f9" class="na nb it la b lb lc le lf lh nc ll nd lp ne lt nf ng nh ni bi translated">高维空间的旋转将以这样的方式进行，即特征1将是第一主分量(PC1)的主要贡献者。</li><li id="17c2" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated">根据PCA方法，第二主成分(PC2)应该垂直于PC1，穿过原点，并且在具有第二大方差的方向上移动。最大的贡献来自特征2。</li><li id="e352" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated">第三个主成分(PC3)应垂直于PC1和PC2，并且方向的方差第三大。在我们的例子中，我们知道特性3将是主要的贡献者。</li></ul><p id="c199" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总而言之，我们预计功能1、2和3将分别成为PC1、PC2和PC3的主要贡献者。<em class="lu">我们应该能够通过使用载荷和双标图来确认我们的预期。</em></p><h2 id="50a7" class="np me it bd mf nq nr dn mj ns nt dp mn lh nu nv mp ll nw nx mr lp ny nz mt oa bi translated">我们如何将负载可视化？</h2><p id="6329" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated"><strong class="la iu"> <em class="lu">载荷用有一定角度和长度的箭头来表示。</em> </strong> <em class="lu">该角度代表特定特征在PCs贡献方向上的贡献。箭头的长度描述了该特征在该方向上的贡献强度。</em>在2D图中，<strong class="la iu">(近)水平箭头</strong>(沿x轴)描述了该特征对PC1的贡献。另一方面，<strong class="la iu">垂直箭头</strong>描述了一个特征对y轴的贡献最大，并且朝向PC2。<strong class="la iu">某个角度下的箭头</strong>描述了特定特征在该方向上对各种PCs的贡献。<strong class="la iu">箭头的长度</strong>描绘了特定PC中特征的<strong class="la iu"><em class="lu"/></strong>或<strong class="la iu"><em class="lu"/></strong>。在我们的示例中，我们预计<em class="lu">特征1 </em>对<em class="lu"> PC1、</em>的贡献最大，并且几乎是水平的，因此将获得接近绝对值1的值。<em class="lu">特征2 </em>对<em class="lu"> PC2 </em>的贡献最大，并且几乎垂直，其值接近绝对值1。<em class="lu">特征3 </em>对<em class="lu"> PC3 </em>的贡献最大，并将位于z轴方向。，并得到一个接近绝对1的值。</p><h2 id="b63b" class="np me it bd mf nq nr dn mj ns nt dp mn lh nu nv mp ll nw nx mr lp ny nz mt oa bi translated">PCA结果。</h2><p id="8610" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">我们从理论上解释了我们应该期待什么，现在我们可以计算主成分及其载荷和解释的方差。对于这个例子，我将用<code class="fe oy oz pa on b">n_components=None</code>初始化pca，因此不会删除任何PC。请注意，默认值为:<code class="fe oy oz pa on b">n_components=0.95</code>，它表示PCs的减少量高达解释方差的95%。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ov ow l"/></div></figure><p id="bd2e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">拟合和转换数据后，我们可以开始研究数据的特征和变化。首先，我们可以用所谓的<strong class="la iu"> <em class="lu"> scree plot </em> </strong>(图9)来检验被解释的(累积的)方差。我们可以看到，PC1和PC2覆盖了超过95%的变化，前3台PC覆盖了总变化的99.64%。pca库中的默认设置是只有PC保持高达95%的解释方差(因此在这种情况下是前3个)。<strong class="la iu"> </strong> <em class="lu">如果我们一开始就将数据标准化(我们使用的是随机数据)，就会出现这种情况。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/1d97dfa95e3f01b8ebb005b66cd1095f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4quRZyWaiXW1-DqBE_tdQg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图9。带有(累积)解释方差的Scree图(图片由作者提供)。</p></figure><p id="7577" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">样品的二维图如图10所示。这里我们看到样本是均匀分布的。请注意，这些值在x轴和y轴之间变化很大。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/57907e51b0c5b24cc30147bbe16091c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JWWjEdjmZ0l8d-DpV7U43g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图10。PC1和PC2的随机数据的二维图。左图:每个样品都有不同的标签和颜色，因为没有给出类别信息。右图:我们可以删除图例和标签，并设置最大装载数量(红色箭头)。(图片由作者提供)</p></figure><p id="a964" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们只关注载荷，并从图中移除散布(图11，通过设置参数<code class="fe oy oz pa on b">cmap=None</code>)。我们可以清楚地看到，第一主成分的最大贡献者是特征1，因为箭头几乎是水平的，并且具有几乎-1 (-0.99)的负载。我们在PC2的功能2和PC3的功能3中看到了类似的行为，但对于PC3，我们需要绘制一个3D图来查看前3个功能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/2039d33775eaf78963afec92776fff90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U22SIn4TzngnOuNJPxyWYg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图11。负荷的二维和三维图。(图片由作者提供)。</p></figure><p id="c7cd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意，所有这些结果也存储在模型的输出中，如上面的代码块所示。此时，我们现在可以用箭头、角度、长度和相关特征来解释载荷。你应该意识到我们使用了随机整数，并且仍然可以找到潜在的“有趣的”PCs贡献者。<em class="lu">如果不进行归一化，范围最大的特征将具有最大的方差，并且是最重要的特征。通过归一化，在随机数据的情况下，所有要素都将具有或多或少相似的贡献。</em> <strong class="la iu"> <em class="lu">分析数据的时候一定要深思熟虑。</em> </strong></p><p id="d976" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在下一节中，我们将分析一个真实的数据集，看看载荷是如何表现的以及如何解释的。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="1edd" class="np me it bd mf nq nr dn mj ns nt dp mn lh nu nv mp ll nw nx mr lp ny nz mt oa bi translated">葡萄酒数据集。</h2><p id="450c" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">到目前为止，我们已经使用了一个合成数据集来理解在一个受控的数据集中载荷是如何表现的。在这一节中，我们将分析一个更现实的数据集，并描述特征在高维空间中的行为和贡献。我们将导入<strong class="la iu"> <em class="lu">葡萄酒数据集</em> </strong>，并创建一个包含178个样本的数据框，包含13个特征和3个葡萄酒类别<em class="lu">。</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ov ow l"/></div></figure><p id="2da5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">特征的范围差别很大，因此标准化步骤很重要。标准化步骤是<a class="ae lv" href="https://erdogant.github.io/pca/" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> pca库</em> </a> <em class="lu"> </em>中的内置功能，可以由<code class="fe oy oz pa on b">normalize=True.</code>轻松设置。出于演示目的，我将设置<code class="fe oy oz pa on b">n_components=None</code>以将所有PC保留在模型中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ov ow l"/></div></figure><p id="0ae1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在拟合和转换之后，我们可以输出存储在关键字“<em class="lu"> topfeat </em>”中的最佳性能特征。这里我们可以看到哪些变量对哪些PCs的贡献最大。使用<em class="lu"> scree图，</em>我们可以检查在PCs中捕获的方差。在图12中，我们可以看到PC1和PC2一起覆盖了数据集中几乎60%的变化。那很好！我们还看到，我们需要10台电脑来捕捉&gt; 95%的变化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/d704c677eb547b6f25aad48c9f940739.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cq-yHzjYDU3tS-bt8dWYrw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图12。Scree plot葡萄酒数据集(图片由作者提供)。</p></figure><p id="4d16" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们制作双绘图并给三个类着色，我们可以直接注意到前两个PC很好地将三个类分开。第一台PC捕获了36.1%的变化，第二台PC捕获了19.2%。加载现在将帮助我们更深入地检查变量是如何对PCs起作用的，以及类是如何被分开的。对x轴贡献最大的特征，因此对于PC1是<strong class="la iu"> <em class="lu">类黄酮</em> </strong>，而对于PC2(y轴)，最大的贡献来自变量<strong class="la iu"> <em class="lu">颜色_强度</em> </strong>。角度较宽的特征，如<strong class="la iu"> <em class="lu">色相</em> </strong>和<strong class="la iu"> <em class="lu">苹果酸</em> </strong>对PC1和PC2有部分贡献。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/105fe95cab06abd5050b487f415de7e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ztXusSlO2DtRvtVBtjXMNQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图13。葡萄酒数据集的双标图。样本根据类别信息进行着色。(图片由作者提供)。</p></figure><p id="e625" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们开始更深入地研究促成PC1的<em class="lu">类黄酮</em>变量。<em class="lu">类黄酮的</em>箭头角度为正，几乎水平，得分为0.422。这表明x轴上的一些差异应该来自于<em class="lu">类黄酮</em>变量。或者换句话说，如果我们使用<em class="lu">黄酮类化合物</em>值给样品着色，我们应该会发现左边的值较低，右边的值较高。我们可以很容易地对此进行如下研究:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ov ow l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7e2a9737a70d77dbb87a6d25cdaad32a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UoTEbSdamXi3Z8JY08dmGw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图14。葡萄酒数据集的双标图。样品上有颜色<em class="pg">类黄酮值</em>(图片由作者提供)。</p></figure><p id="66fe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">事实上，很高兴看到样本的左边是蓝色(低值)，右边是红色(高值)。因此，加载准确地描述了沿x轴贡献最大的特征以及取值范围。</p><blockquote class="ph"><p id="cb6a" class="pi pj it bd pk pl pm pn po pp pq lt dk translated">正负荷表示变量和主成分正相关，而负负荷表示负相关。当载荷很大时，(+或-)，它表明一个变量对主分量有很大的影响。</p></blockquote><p id="438a" class="pw-post-body-paragraph ky kz it la b lb pr ju ld le ps jx lg lh pt lj lk ll pu ln lo lp pv lr ls lt im bi translated">我们再拍一张。对PC2贡献最大的是<em class="lu"> color_intensity。</em>这个箭头是向下的，我们应该预料到这些值与PC2是负相关的。或者换句话说，具有高<em class="lu">颜色强度</em>值的样本被期望在底部，而低值在顶部。如果我们现在使用<em class="lu"> color_intensity </em>给样本着色(图15)，很明显变化确实是沿着y轴的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ov ow l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pw"><img src="../Images/a56502cd509621e02f92937063e8b978.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sYGIO3p5uL6J6UoI-svJKQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图14。葡萄酒数据集的双标图。样本根据color_intensity <em class="pg">值</em>着色(图片由作者提供)。</p></figure><p id="529c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们也来看看<em class="lu">镁</em>装。该变量的加载角度更大，并且方向向下，这意味着它对PC1和PC2有部分贡献。创建3D图并根据镁值对样本进行着色，可以清楚地看出PC3沿线也捕捉到了一些变化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi px"><img src="../Images/35b046b653235134075762807349b087.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ex0RLzRpUd4iW7fOJbRh8g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图14。葡萄酒数据集的双标图。样品在镁<em class="pg">值</em>上着色(图片由作者提供)。</p></figure><p id="2d18" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">除了给样品着色之外，还可以在其他维度上进一步研究样品，例如第4、第5等PCs及其负载。通过这些分析，我们对变量的变化有了更多的直觉，我们甚至可以假设这些类是如何分开的。所有这些见解都是讨论可能的后续步骤和设定一些期望的极好起点。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="9ee0" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">异常值检测。</h1><p id="ac4a" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated"><a class="ae lv" href="https://erdogant.github.io/pca/pages/html/Outlier%20detection.html" rel="noopener ugc nofollow" target="_blank"> pca库</a>包含两种检测离群点的方法:<a class="ae lv" href="https://erdogant.github.io/pca/pages/html/Outlier%20detection.html#" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">霍特林的T2 </strong> </a>和<a class="ae lv" href="https://erdogant.github.io/pca/pages/html/Outlier%20detection.html#spe-dmodx" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> SPE/DmodX </strong> </a>。霍特林的T2 <strong class="la iu"> </strong>通过计算<strong class="la iu">卡方检验</strong>来工作，而<strong class="la iu"> SPE/DmodX方法</strong>基于前2个样本的平均值和协方差。这两种方法在方法上是互补的，因此计算重叠可以指出最不正常的观察结果。当我们分析葡萄酒数据集时，我们检测到方法之间有三个重叠的异常值。如果您需要调查异常值，这将是一个很好的开始。或者，对于霍特林T2法，离群值可以排列为<code class="fe oy oz pa on b">y_proba</code>(越低越好)，对于SPE/DmodX法，可以排列为<code class="fe oy oz pa on b">y_score_spe</code>(越大越好)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi py"><img src="../Images/ab9fadb7cc1baa59a3f1261df4d299ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jL7jBXJR19aEzy3hUiWFIw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图15。使用SPE/DmodX方法检测到的异常值用菱形表示。使用霍特林T2方法检测到的异常值用十字表示。(图片由作者提供)</p></figure></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="879a" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">结束了。</h1><p id="383a" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">每个PC是一个线性向量，包含原始特征的贡献比例。每个变量对主成分的贡献的解释可以使用加载进行检索，并用双标图进行可视化。这种分析将给出关于变量变化和类可分性的直觉。这将有助于形成良好的理解，并可以为讨论后续步骤提供一个极好的起点。例如，您可以质疑是否需要使用高级机器学习技术来预测特定类别，或者我们是否可以对特定变量设置单个阈值来进行类别预测。</p><p id="51c8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，<a class="ae lv" href="https://erdogant.github.io/pca/" rel="noopener ugc nofollow" target="_blank"> pca库</a>提供了使用霍特林T2测试和SPE/Dmodx方法移除<a class="ae lv" href="https://erdogant.github.io/pca/pages/html/Outlier%20detection.html" rel="noopener ugc nofollow" target="_blank">异常值</a>的功能。我没有去去除不需要的(技术)差异(或偏差)，但这可以通过使用<a class="ae lv" href="https://erdogant.github.io/pca/pages/html/Examples.html#normalizing-out-pcs" rel="noopener ugc nofollow" target="_blank">标准化策略</a>来完成。</p><p id="9af1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是了。如果有不清楚的地方或者有进一步完善<a class="ae lv" href="https://erdogant.github.io/pca/" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> pca </em> </a>库的建议请告诉我！</p><p id="8ca3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">注意安全。保持冷静。</em></p><p id="7581" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">欢呼，E. </em> </strong></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="9433" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">如果你觉得这篇文章很有帮助，可以使用我的</em> <a class="ae lv" href="https://medium.com/@erdogant/membership" rel="noopener"> <em class="lu">推荐链接</em> </a> <em class="lu">继续无限制学习，并注册成为中级会员。另外，</em> <a class="ae lv" href="http://erdogant.medium.com" rel="noopener"> <em class="lu">关注我</em> </a> <em class="lu">关注我的最新内容！</em></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="068c" class="np me it bd mf nq nr dn mj ns nt dp mn lh nu nv mp ll nw nx mr lp ny nz mt oa bi translated">软件</h2><ul class=""><li id="f904" class="na nb it la b lb mv le mw lh pz ll qa lp qb lt nf ng nh ni bi translated"><a class="ae lv" href="https://erdogant.github.io/pca/" rel="noopener ugc nofollow" target="_blank"> pca github </a></li><li id="3dba" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated"><a class="ae lv" href="https://erdogant.github.io/pca/pages/html/Documentation.html#colab-notebook" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a></li></ul><h2 id="5e1a" class="np me it bd mf nq nr dn mj ns nt dp mn lh nu nv mp ll nw nx mr lp ny nz mt oa bi translated">我们连线吧！</h2><ul class=""><li id="ff1c" class="na nb it la b lb mv le mw lh pz ll qa lp qb lt nf ng nh ni bi translated"><a class="ae lv" href="https://www.linkedin.com/in/erdogant/" rel="noopener ugc nofollow" target="_blank">我们在LinkedIn上连线</a></li><li id="73b8" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated"><a class="ae lv" href="https://github.com/erdogant" rel="noopener ugc nofollow" target="_blank">在Github上关注我</a></li><li id="226b" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated"><a class="ae lv" href="https://erdogant.medium.com/" rel="noopener">跟着我上媒</a></li></ul></div></div>    
</body>
</html>