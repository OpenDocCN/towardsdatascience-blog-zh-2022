<html>
<head>
<title>Optimize PyTorch Performance for Speed and Memory Efficiency (2022)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">优化PyTorch性能以提高速度和内存效率(2022)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/optimize-pytorch-performance-for-speed-and-memory-efficiency-2022-84f453916ea6#2022-04-25">https://towardsdatascience.com/optimize-pytorch-performance-for-speed-and-memory-efficiency-2022-84f453916ea6#2022-04-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0fa6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">你应该知道的18个PyTorch技巧:它们是如何工作的以及为什么工作</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3d08ccfd6679893676e89a5434371eb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0KZggH2fQMklDkP4"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">调整深度学习管道就像找到正确的齿轮组合(图片由<a class="ae ky" href="https://unsplash.com/@timmossholder" rel="noopener ugc nofollow" target="_blank"> Tim Mossholder </a>在<a class="ae ky" href="https://unsplash.com/photos/GmvH5v9l3K4" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上提供)</p></figure><h1 id="26b1" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">你为什么要读这篇文章？</h1><p id="4302" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">深度学习模型的训练/推理过程涉及许多步骤。在时间和资源有限的情况下，每次实验迭代越快，我们就越能优化整个模型的预测性能。我收集并整理了一些PyTorch技巧和提示，以最大化内存使用效率并最小化运行时间。为了更好地利用这些技巧，我们还需要了解它们是如何工作的以及为什么工作。</p><p id="2baf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我首先提供了一个完整的列表和一个组合代码片段，以防您想直接优化您的脚本。随后，我将逐一详细介绍它们。对于每个技巧，我还提供了代码片段，并注释了它是特定于设备类型(CPU/GPU)还是模型类型。</p><h1 id="77b6" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">清单:</h1><ul class=""><li id="175e" class="ms mt it lt b lu lv lx ly ma mu me mv mi mw mm mx my mz na bi translated"><strong class="lt iu">数据加载</strong> <br/> 1。将活动数据移动到SSD <br/> 2。<code class="fe nb nc nd ne b">Dataloader(dataset, <strong class="lt iu">num_workers</strong>=4*num_GPU)</code> <br/> 3。<code class="fe nb nc nd ne b">Dataloader(dataset, <strong class="lt iu">pin_memory=True</strong>)</code></li><li id="b97e" class="ms mt it lt b lu nf lx ng ma nh me ni mi nj mm mx my mz na bi translated"><strong class="lt iu">数据操作<br/> </strong> 4。直接创建矢量/矩阵/张量作为<code class="fe nb nc nd ne b">torch.Tensor</code>，并在设备上运行操作<br/> 5。避免CPU与GPU <strong class="lt iu"> </strong> <br/> 6之间不必要的数据传输。使用<code class="fe nb nc nd ne b">torch.<strong class="lt iu">from_numpy</strong>(numpy_array)</code>或<code class="fe nb nc nd ne b">torch.<strong class="lt iu">as_tensor</strong>(others)<br/></code> 7。当适用于重叠数据传输时使用<code class="fe nb nc nd ne b">tensor.to(<strong class="lt iu">non_blocking=True</strong>)</code>8。通过PyTorch JIT将点态(元素态)操作融合到一个内核中</li><li id="ed46" class="ms mt it lt b lu nf lx ng ma nh me ni mi nj mm mx my mz na bi translated"><strong class="lt iu">模型架构</strong> <br/> 9。将所有不同架构设计的尺寸设置为8的倍数(对于混合精度的FP16)</li><li id="3296" class="ms mt it lt b lu nf lx ng ma nh me ni mi nj mm mx my mz na bi translated"><strong class="lt iu">训练</strong> <br/> 10。将批处理大小设置为8的倍数，并最大化GPU内存使用率<br/> 11。使用混合精度进行正向传递(而不是反向传递)<br/> 12。在优化器更新权重<br/> 13之前，将梯度设置为<code class="fe nb nc nd ne b"><strong class="lt iu">None</strong></code>(例如<code class="fe nb nc nd ne b">model.zero_grad(<strong class="lt iu">set_to_none=True</strong>)</code>)。梯度累积:每隔一批<em class="nk"> x </em>更新重量，以模拟更大的批量</li><li id="01d9" class="ms mt it lt b lu nf lx ng ma nh me ni mi nj mm mx my mz na bi translated"><strong class="lt iu">推论/验证</strong> <br/> 14。关闭渐变计算</li><li id="d8b3" class="ms mt it lt b lu nf lx ng ma nh me ni mi nj mm mx my mz na bi translated"><strong class="lt iu"> CNN(卷积神经网络)具体</strong> <br/> 15。<code class="fe nb nc nd ne b">torch.backends.cudnn.benchmark = True</code> <br/> 16。使用4D NCHW张量<br/> 17的channels_last存储器格式。关闭批量规范化之前的卷积图层的偏差</li><li id="0474" class="ms mt it lt b lu nf lx ng ma nh me ni mi nj mm mx my mz na bi translated"><strong class="lt iu">分布式优化</strong> <br/> 18。用<code class="fe nb nc nd ne b">DistributedDataParallel</code>代替<code class="fe nb nc nd ne b">DataParallel</code></li></ul><h1 id="6224" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">结合第7、11、12、13条提示的代码片段:</strong></h1><pre class="kj kk kl km gt nl ne nm nn aw no bi"><span id="b495" class="np la it ne b gy nq nr l ns nt"><em class="nk"># Combining the tips No.7, 11, 12, 13: nonblocking, AMP, setting <br/># gradients as None, and larger effective batch size</em></span><span id="245a" class="np la it ne b gy nu nr l ns nt">model.train()<br/><em class="nk"># Reset the gradients to None<br/></em>optimizer.zero_grad<!-- -->(<strong class="ne iu">set_to_none=True</strong>)<br/>scaler = <strong class="ne iu">GradScaler</strong>()<br/>for i, (features, target) in enumerate(dataloader):<br/><em class="nk">    # these two calls are nonblocking and overlapping<br/></em>    features = features.to('cuda:0', <strong class="ne iu">non_blocking=True)</strong><br/>    target = target.to('cuda:0', <strong class="ne iu">non_blocking=True</strong>)</span><span id="73fb" class="np la it ne b gy nu nr l ns nt"><em class="nk">    # Forward pass with mixed precision<br/></em>    with torch.cuda.amp.<strong class="ne iu">autocast</strong>(): <em class="nk"># autocast as a context manager</em><br/>        output = model(features)<br/>        loss = criterion(output, target)</span><span id="0640" class="np la it ne b gy nu nr l ns nt"><em class="nk">    # Backward pass without mixed precision<br/>    # It's not recommended to use mixed precision for backward pass<br/>    # Because we need more precise loss</em><br/>    scaler.scale(loss).backward()</span><span id="d4fe" class="np la it ne b gy nu nr l ns nt"><em class="nk">    # Only update weights every other 2 iterations<br/>    # Effective batch size is doubled<br/></em>    <strong class="ne iu">if (i+1) % 2 == 0 or (i+1) == len(dataloader):<br/></strong><em class="nk">        # scaler.step() first unscales the gradients .<br/>        # If these gradients contain infs or NaNs, <br/>        # optimizer.step() is skipped.<br/></em>        scaler.step(optimizer)</span><span id="0b5d" class="np la it ne b gy nu nr l ns nt"><em class="nk">        # If optimizer.step() was skipped,<br/>        # scaling factor is reduced by the backoff_factor <br/>        # in GradScaler()<br/>    </em>    scaler.update()</span><span id="5ca6" class="np la it ne b gy nu nr l ns nt"><em class="nk">        # Reset the gradients to None<br/></em>        <!-- -->optimizer.zero_grad<!-- -->(<strong class="ne iu">set_to_none=True</strong>)</span></pre><h1 id="feef" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">高级概念</h1><p id="9a7f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">总体而言，您可以通过3个关键点来优化时间和内存使用。首先，尽可能减少i/o(输入/输出),以便模型管道被<em class="nk">绑定到计算</em>(数学受限或数学受限)，而不是绑定到i/o(带宽受限或内存受限)。通过这种方式，我们可以利用GPU及其专业化来加速这些计算。第二，尽量重叠流程，节省时间。第三，最大化内存使用效率，节省内存。那么节省内存可以实现更大的批量，从而节省更多的时间。有更多的时间有助于更快的模型开发周期，并带来更好的模型性能。</p><h1 id="5dab" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">1.将活动数据移动到SSD</h1><p id="b806" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">有些机器有不同的硬盘驱动器，如HHD和SSD。建议将活动项目中使用的数据移动到SSD(或具有更好i/o的硬盘)以获得更快的速度。</p><p id="568c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"># CPU # GPU #保存时间</p><h1 id="28c8" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">2.异步数据加载和扩充</h1><p id="8d67" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><code class="fe nb nc nd ne b">num_workers=0</code>只有在训练或之前的过程完成后，才会执行数据加载。设置<strong class="lt iu"> </strong> <code class="fe nb nc nd ne b"><strong class="lt iu">num_workers</strong></code> <strong class="lt iu"> &gt; 0 </strong>有望加速该过程，尤其是对于i/o和大型数据的扩充。具体到GPU，<a class="ae ky" href="https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/73" rel="noopener ugc nofollow" target="_blank">本次实验</a>发现<code class="fe nb nc nd ne b">num_workers = 4*num_GPU</code>性能最好。话虽如此，你也可以为你的机器测试最好的<code class="fe nb nc nd ne b">num_workers</code>。需要注意的是，高<code class="fe nb nc nd ne b">num_workers</code>会产生较大的内存消耗开销(<a class="ae ky" href="https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/2" rel="noopener ugc nofollow" target="_blank"> ref </a>)，这也是意料之中的，因为内存中同时会处理更多的数据副本。</p><pre class="kj kk kl km gt nl ne nm nn aw no bi"><span id="cd08" class="np la it ne b gy nq nr l ns nt">Dataloader(dataset, <strong class="ne iu">num_workers</strong>=4*num_GPU)</span></pre><p id="664b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"># CPU # GPU #保存时间</p><h1 id="3d65" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">3.使用固定内存减少数据传输</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/8081c9c75c17d6c8a9ccb3832921246b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M8mejDZ5WbnFl8h59UfjCg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">设置<strong class="bd nw"> pin_memory=True </strong>跳过从可分页内存到固定内存的传输(图片由作者提供，灵感来自<a class="ae ky" href="https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/" rel="noopener ugc nofollow" target="_blank">该图片</a></p></figure><p id="cef8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">GPU不能直接从CPU的可分页内存中访问数据。设置<code class="fe nb nc nd ne b">pin_memory=True</code>可以直接为CPU主机<em class="nk">上的数据分配暂存内存</em>，节省从可分页内存向暂存内存(即固定内存，即页锁内存)传输数据的时间。该设置可与<code class="fe nb nc nd ne b">num_workers = 4*num_GPU</code>结合使用。</p><pre class="kj kk kl km gt nl ne nm nn aw no bi"><span id="a924" class="np la it ne b gy nq nr l ns nt">Dataloader(dataset, <strong class="ne iu">pin_memory=True</strong>)</span></pre><p id="5872" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"># GPU #保存时间</p><h1 id="a40e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">4.直接创建矢量/矩阵/张量作为火炬。张量和它们将运行操作的设备</h1><p id="5c50" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">每当您需要PyTorch的<code class="fe nb nc nd ne b">torch.Tensor</code>数据时，首先尝试在您将使用它们的设备上创建它们。不要用原生Python或者NumPy创建数据然后转换成<code class="fe nb nc nd ne b">torch.Tensor</code>。在大多数情况下，如果你打算在GPU中使用它们，直接在GPU中创建它们。</p><pre class="kj kk kl km gt nl ne nm nn aw no bi"><span id="8e7b" class="np la it ne b gy nq nr l ns nt"><em class="nk"># Random numbers between 0 and 1<br/># Same as np.random.rand([10,5])</em><br/>tensor = torch.rand([10, 5], <strong class="ne iu">device=torch.device('cuda:0')</strong>)</span><span id="169b" class="np la it ne b gy nu nr l ns nt"><em class="nk"># Random numbers from normal distribution with mean 0 and variance 1<br/># Same as np.random.randn([10,5])</em><br/>tensor = torch.randn([10, 5], <strong class="ne iu">device=torch.device('cuda:0')</strong>)</span></pre><p id="d75b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">唯一的语法区别是NumPy中的随机数生成需要额外的<strong class="lt iu"> random </strong>，比如<code class="fe nb nc nd ne b">np.<strong class="lt iu">random</strong>.rand()</code> vs <code class="fe nb nc nd ne b">torch.rand()</code>。许多其他函数在NumPy中都有相应的函数:</p><p id="69fa" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><code class="fe nb nc nd ne b">torch.empty()</code>、<code class="fe nb nc nd ne b">torch.zeros()</code>、<code class="fe nb nc nd ne b">torch.full()</code>、<code class="fe nb nc nd ne b">torch.ones()</code>、<code class="fe nb nc nd ne b">torch.eye()</code>、<code class="fe nb nc nd ne b">torch.randint()</code>、<code class="fe nb nc nd ne b">torch.rand()</code>、<code class="fe nb nc nd ne b">torch.randn()</code></p><p id="ec47" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"># GPU #保存时间</p><h1 id="d3df" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">5.避免CPU和GPU之间不必要的数据传输</h1><p id="9801" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">正如我在高级概念中提到的，我们希望尽可能减少i/o。请注意以下这些命令:</p><pre class="kj kk kl km gt nl ne nm nn aw no bi"><span id="aef5" class="np la it ne b gy nq nr l ns nt"><em class="nk"># BAD! AVOID THEM IF UNNECESSARY!<br/></em>print(cuda_tensor)<br/>cuda_tensor.cpu()<br/>cuda_tensor.to_device('cpu')<br/>cpu_tensor.cuda()<br/>cpu_tensor.to_device('cuda')<br/>cuda_tensor.item()<br/>cuda_tensor.numpy()<br/>cuda_tensor.nonzero()<br/>cuda_tensor.tolist()</span><span id="8615" class="np la it ne b gy nu nr l ns nt"><em class="nk"># Python control flow which depends on operation results of CUDA tensors</em><br/>if (cuda_tensor != 0).all():<br/>    run_func()</span></pre><p id="0511" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"># GPU #保存时间</p><h1 id="4e9a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">6.使用<code class="fe nb nc nd ne b">torch.from_numpy(numpy_array) and torch.as_tensor(others) instead of torch.tensor</code></h1><blockquote class="nx ny nz"><p id="83b7" class="lr ls nk lt b lu mn ju lw lx mo jx lz oa mp mc md ob mq mg mh oc mr mk ml mm im bi translated"><code class="fe nb nc nd ne b">torch.tensor()</code>总是<em class="it">复制数据。</em></p></blockquote><p id="c17c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果源设备和目标设备都是CPU，<code class="fe nb nc nd ne b">torch.from_numpy</code>和<code class="fe nb nc nd ne b">torch.as_tensor</code>可能不会创建数据副本。如果源数据是一个NumPy数组，使用<code class="fe nb nc nd ne b">torch.from_numpy(numpy_array)</code>会更快。如果源数据是具有相同数据类型和设备类型的张量，那么<code class="fe nb nc nd ne b">torch.as_tensor(others)</code>可以避免复制数据(如果适用)。<code class="fe nb nc nd ne b">others</code>可以是Python <code class="fe nb nc nd ne b">list</code>、<code class="fe nb nc nd ne b">tuple</code>或者<code class="fe nb nc nd ne b">torch.tensor</code>。如果源设备和目标设备不同，那么我们可以使用下一个技巧。</p><pre class="kj kk kl km gt nl ne nm nn aw no bi"><span id="bce2" class="np la it ne b gy nq nr l ns nt">torch.from_numpy(numpy_array)<br/>torch.as_tensor(others)</span></pre><p id="6b79" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"># CPU #保存时间</p><h1 id="626a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">7.当适用于重叠数据传输和内核执行时，使用<code class="fe nb nc nd ne b">tensor.to(non_blocking=True)</code></h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/21ee0b81affe8882d2acaedcf955cf35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*no-gQHz8daJbmYhCfAGNOA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">重叠数据传输以减少运行时间。(图片由作者提供)</p></figure><p id="4050" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">本质上，<code class="fe nb nc nd ne b">non_blocking=True</code>允许异步数据传输以减少执行时间。</p><pre class="kj kk kl km gt nl ne nm nn aw no bi"><span id="f0bc" class="np la it ne b gy nq nr l ns nt">for features, target in loader:<br/><em class="nk">    # these two calls are nonblocking and overlapping<br/></em>    features = features.to('cuda:0', non_blocking=True)<br/>    target = target.to('cuda:0', non_blocking=True)</span><span id="5975" class="np la it ne b gy nu nr l ns nt"><em class="nk">    # This is a synchronization point<br/>    # It will wait for previous two lines</em><br/>    output = model(features)</span></pre><p id="e348" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"># GPU #保存时间</p><h1 id="c65c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">8.通过PyTorch JIT将点态(元素态)操作融合到一个内核中</h1><p id="9c34" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">逐点运算(见<a class="ae ky" href="https://pytorch.org/docs/stable/torch.html#pointwise-ops" rel="noopener ugc nofollow" target="_blank">示例列表</a>)包括常见的数学运算，通常是内存受限的。PyTorch JIT会<em class="nk">自动</em>将相邻的逐点操作融合到一个内核中，以节省多次内存读/写。(挺神奇的吧？)例如，<code class="fe nb nc nd ne b">gelu</code>函数可以通过将5个内核融合成1个来对一百万个矢量加速4倍(<a class="ae ky" href="https://tigress-web.princeton.edu/~jdh4/PyTorchPerformanceTuningGuide_GTC2021.pdf" rel="noopener ugc nofollow" target="_blank"> ref </a>)。更多PyTorch JIT优化的例子可以在<a class="ae ky" href="https://pytorch.org/docs/stable/jit.html" rel="noopener ugc nofollow" target="_blank">这里</a>和<a class="ae ky" href="https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><pre class="kj kk kl km gt nl ne nm nn aw no bi"><span id="35d7" class="np la it ne b gy nq nr l ns nt"><strong class="ne iu">@torch.jit.script</strong> <em class="nk"># JIT decorator</em><br/>def fused_gelu(x):<br/>    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))</span></pre><p id="8179" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"># CPU # GPU #保存时间</p><h1 id="e660" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">9 &amp; 10.将所有不同建筑设计的尺寸和批量设置为8的倍数</h1><p id="17b8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了最大化GPU的计算效率，最好确保不同的架构设计(包括神经网络的输入和输出大小/维度/通道数和批量大小)是8的倍数甚至更大的2的幂(例如64、128和高达256)。这是因为当矩阵维数对齐2的倍数时，Nvidia GPUs的<strong class="lt iu">张量核心可以实现最佳的矩阵乘法性能。</strong>矩阵乘法是最常用的运算，也可能是瓶颈，所以我们最好能确保张量/矩阵/向量的维数能被2的幂整除(例如，8、64、128和256)。</p><p id="0da7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html" rel="noopener ugc nofollow" target="_blank">这些实验</a>表明，将输出维度和批量大小设置为8的倍数(即33712、4088、4096)可以将计算速度提高1.3到4倍，而输出维度为33708，批量大小为4084和4095，它们不能被8整除。加速幅度取决于过程类型(例如正向传递或梯度计算)和cuBLAS版本。特别是，如果你在NLP上工作，记得检查你的输出维度，通常是词汇量。</p><p id="798a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用大于256的倍数不会增加更多的好处，但也没有坏处。这些设置取决于cuBLAS和cuDNN版本以及GPU架构。你可以在这里找到矩阵维数<a class="ae ky" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc" rel="noopener ugc nofollow" target="_blank">的具体张量核心要求</a>。由于目前PyTorch放大器主要使用FP16，而FP16需要8的倍数，因此通常建议使用8的倍数。如果你有A100这样更高级的GPU，那么你可能会选择64的倍数。如果你使用的是AMD GPU，你可能需要查看AMD的文档。</p><p id="0aa8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">除了将批处理大小设置为8的倍数，我们还最大化批处理大小，直到它达到GPU的内存限制。这样，我们可以花更少的时间来完成一个纪元。</p><p id="97d9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"># GPU #保存时间</p><h1 id="a2db" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">11。对正向传递使用混合精度，但不对反向传递使用混合精度</h1><p id="cbd6" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">有些操作不需要float64或float32的精度。因此，将操作设置为较低的精度可以节省内存和执行时间。对于各种应用，Nvidia报告称，将混合精度与带张量内核的GPU结合使用可以将速度提高3.5倍至25倍(<a class="ae ky" href="https://developer.nvidia.com/blog/tensor-cores-mixed-precision-scientific-computing/" rel="noopener ugc nofollow" target="_blank"> ref </a>)。</p><p id="0291" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">值得注意的是，通常矩阵越大，加速度混合精度越高(<a class="ae ky" href="https://developer.nvidia.com/blog/tensor-cores-mixed-precision-scientific-computing/" rel="noopener ugc nofollow" target="_blank"> ref1 </a>，<a class="ae ky" href="http://www.netlib.org/utk/people/JackDongarra/PAPERS/haidar_fp16_sc18.pdf" rel="noopener ugc nofollow" target="_blank"> ref2 </a>)。在较大的神经网络(例如BERT)中，一项实验表明，混合精度可以将训练加速2.75倍，并减少37%的内存使用(<a class="ae ky" href="https://spell.ml/blog/mixed-precision-training-with-pytorch-Xuk7YBEAACAASJam" rel="noopener ugc nofollow" target="_blank">参考</a>)。采用Volta、Turing、Ampere或Hopper架构的新型GPU设备(例如，T4、V100、RTX 2060、2070、2080、2080 Ti、A100、RTX 3090、RTX 3080和RTX 3070)可以从混合精度中受益更多，因为它们拥有<a class="ae ky" href="https://developer.nvidia.com/tensor-cores" rel="noopener ugc nofollow" target="_blank">张量内核</a>架构，该架构经过特殊优化，性能优于CUDA内核(<a class="ae ky" href="https://www.techcenturion.com/tensor-cores/#nbspnbspnbspnbspTensor_Cores_vs_CUDA_Cores" rel="noopener ugc nofollow" target="_blank"> ref </a>)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/38361736acbe5cc3f316c676152d9c25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nRBNncgQBkzF_w9VI84eBw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">张量核心的NVIDIA架构支持不同精度(图片由作者提供；<a class="ae ky" href="https://www.nvidia.com/en-us/data-center/tensor-cores/" rel="noopener ugc nofollow" target="_blank">数据来源</a></p></figure><p id="bba8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">需要注意的是，采用Hopper架构的H100预计将于2022年第三季度发布，支持FP8 (float8)。PyTorch AMP可能也会支持FP8(当前版本1.11.0还不支持FP8)。</p><blockquote class="of"><p id="14ff" class="og oh it bd oi oj ok ol om on oo mm dk translated">在实践中，您需要在模型精度性能和速度性能之间找到一个最佳平衡点。我以前确实发现混合精度可能会降低模型性能，这取决于算法、数据和问题。</p></blockquote><p id="955b" class="pw-post-body-paragraph lr ls it lt b lu op ju lw lx oq jx lz ma or mc md me os mg mh mi ot mk ml mm im bi translated">利用PyTorch中的自动混合精度(AMP)包很容易实现混合精度。PyTorch中默认的浮点类型是float32 ( <a class="ae ky" href="https://pytorch.org/docs/stable/generated/torch.set_default_dtype.html#torch-set-default-dtype" rel="noopener ugc nofollow" target="_blank"> ref </a>)。AMP通过对一组操作使用float16来节省内存和时间(如<code class="fe nb nc nd ne b">matmul</code>、<code class="fe nb nc nd ne b">linear</code>、<code class="fe nb nc nd ne b">conv2d</code>等，见<a class="ae ky" href="https://pytorch.org/docs/stable/amp.html#ops-that-can-autocast-to-float16" rel="noopener ugc nofollow" target="_blank">完整列表</a>)。对于某些操作，AMP会<code class="fe nb nc nd ne b">autocast</code>到float32(例如<code class="fe nb nc nd ne b">mse_loss</code>、<code class="fe nb nc nd ne b">softmax</code>等，参见<a class="ae ky" href="https://pytorch.org/docs/stable/amp.html#ops-that-can-autocast-to-float32" rel="noopener ugc nofollow" target="_blank">完整列表</a>)。一些操作(如<code class="fe nb nc nd ne b">add</code>，见<a class="ae ky" href="https://pytorch.org/docs/stable/amp.html#ops-that-promote-to-the-widest-input-type" rel="noopener ugc nofollow" target="_blank">完整列表</a>)将在最宽的输入类型上操作。例如，如果一个变量是float32，另一个变量是float16，相加的结果将是float32。</p><p id="c89f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><code class="fe nb nc nd ne b">autocast</code>自动将精度应用于不同的操作。因为损耗和梯度是以float16精度计算的，所以梯度太小时可能会“下溢”并变成零。<code class="fe nb nc nd ne b">GradScaler</code>通过将损失乘以一个比例因子，根据缩放后的损失计算梯度，然后在优化器更新权重之前取消梯度的缩放，来防止下溢。如果缩放因子太大或太小，并导致<code class="fe nb nc nd ne b">inf</code>或<code class="fe nb nc nd ne b">NaN</code> s，那么缩放器将为下一次迭代更新缩放因子。</p><pre class="kj kk kl km gt nl ne nm nn aw no bi"><span id="5a8e" class="np la it ne b gy nq nr l ns nt">scaler = <strong class="ne iu">GradScaler</strong>()<br/>for features, target in data:<br/><em class="nk">    # Forward pass with mixed precision<br/></em>    with torch.cuda.amp.autocast(): <em class="nk"># autocast as a context manager</em><br/>        output = model(features)<br/>        loss = criterion(output, target)</span><span id="cf22" class="np la it ne b gy nu nr l ns nt"><em class="nk">    # Backward pass without mixed precision<br/>    # It's not recommended to use mixed precision for backward pass<br/>    # Because we need more precise loss</em><br/>    scaler.scale(loss).backward()</span><span id="edd6" class="np la it ne b gy nu nr l ns nt"><em class="nk">    # scaler.step() first unscales the gradients .<br/>    # If these gradients contain infs or NaNs, <br/>    # optimizer.step() is skipped.<br/></em>    scaler.step(optimizer) </span><span id="81c2" class="np la it ne b gy nu nr l ns nt"><em class="nk">    # If optimizer.step() was skipped,<br/>    # scaling factor is reduced by the backoff_factor in GradScaler()<br/></em>    scaler.update()</span></pre><p id="a970" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">您也可以使用<code class="fe nb nc nd ne b">autocast</code>作为正向传递函数的装饰器。</p><pre class="kj kk kl km gt nl ne nm nn aw no bi"><span id="7755" class="np la it ne b gy nq nr l ns nt">class AutocastModel(nn.Module):<br/>    ...<br/>    @autocast() <em class="nk"># autocast as a decorator</em><br/>    def forward(self, input):<br/>        x = self.model(input)<br/>        return x</span></pre><p id="f0bd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"># CPU # GPU #保存时间#保存内存</p><h1 id="6521" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">12.在优化器更新权重之前，将梯度设置为<code class="fe nb nc nd ne b">None</code></h1><p id="2a68" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">通过<code class="fe nb nc nd ne b">model.zero_grad()</code>或<code class="fe nb nc nd ne b">optimizer.zero_grad()</code>将梯度设置为零将对所有参数执行<code class="fe nb nc nd ne b">memset</code>，并通过读写操作更新梯度。然而，将梯度设置为<code class="fe nb nc nd ne b">None</code>将不会执行<code class="fe nb nc nd ne b">memset</code>，而只会用<em class="nk">写</em>操作来更新梯度。因此，将渐变设置为<code class="fe nb nc nd ne b">None</code>会更快。</p><pre class="kj kk kl km gt nl ne nm nn aw no bi"><span id="070f" class="np la it ne b gy nq nr l ns nt"><em class="nk"># Reset gradients before each step of optimizer<br/></em>for param in model.parameters():<br/>    param.grad = <strong class="ne iu">None</strong></span><span id="1c48" class="np la it ne b gy nu nr l ns nt"><em class="nk"># or (PyTorch &gt;= 1.7)</em><br/>model.zero_grad(<strong class="ne iu">set_to_none=True</strong>)</span><span id="e4eb" class="np la it ne b gy nu nr l ns nt"><em class="nk"># or (PyTorch &gt;= 1.7)<br/></em>optimizer.zero_grad<!-- -->(<strong class="ne iu">set_to_none=True</strong>)</span></pre><p id="df6b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"># CPU # GPU #保存时间</p><h1 id="0a1b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">13.梯度累积:每隔一批<em class="ou"> x </em>更新重量，以模拟更大的批量</h1><p id="a1d1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这个技巧是关于从更多的数据样本中积累梯度，以便梯度的估计更准确，并且权重被更新得更接近局部/全局最小值。这在批量较小的情况下更有帮助(由于GPU内存限制较小或每个样本的数据量较大)。</p><pre class="kj kk kl km gt nl ne nm nn aw no bi"><span id="cccc" class="np la it ne b gy nq nr l ns nt">for i, (features, target) in enumerate(dataloader):<br/><em class="nk">    # Forward pass<br/></em>    output = model(features)<br/>    loss = criterion(output, target)</span><span id="cf84" class="np la it ne b gy nu nr l ns nt"><em class="nk">    # Backward pass<br/></em>    loss.backward()</span><span id="de06" class="np la it ne b gy nu nr l ns nt"><em class="nk">    # Only update weights every other 2 iterations<br/>    # Effective batch size is doubled<br/></em>    <strong class="ne iu">if (i+1) % 2 == 0 or (i+1) == len(dataloader):<br/></strong>        # Update weights<br/>        optimizer.step()</span><span id="e06e" class="np la it ne b gy nu nr l ns nt"><em class="nk">        # Reset the gradients to None<br/></em>        <!-- -->optimizer.zero_grad<!-- -->(set_to_none=True)</span></pre><p id="493c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"># CPU # GPU #保存时间</p><h1 id="2786" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">14.关闭推断/验证的梯度计算</h1><p id="e7df" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">本质上，如果您只计算模型的输出，那么推断和验证步骤就不需要梯度计算。PyTorch使用一个中间内存缓冲区来处理<code class="fe nb nc nd ne b">requires_grad=True</code>变量中的操作。因此，如果我们知道我们不需要任何涉及梯度的操作，我们可以通过禁用推断/验证的梯度计算来避免使用额外的资源。</p><pre class="kj kk kl km gt nl ne nm nn aw no bi"><span id="4200" class="np la it ne b gy nq nr l ns nt"># torch.no_grad() as a context manager:<br/>    with torch.no_grad():<br/>    output = model(input)</span><span id="3019" class="np la it ne b gy nu nr l ns nt"># torch.no_grad() as a function decorator:<br/>@torch.no_grad()<br/>def validation(model, input):<br/>    output = model(input)<br/>return output</span></pre><p id="2d94" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"># CPU # GPU #保存时间#保存内存</p><h1 id="2663" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">15.<code class="fe nb nc nd ne b">torch.backends.cudnn.benchmark = True</code></h1><p id="a9f6" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在训练循环之前设置<code class="fe nb nc nd ne b">torch.backends.cudnn.benchmark = True</code>可以加速计算。因为cuDNN算法计算不同内核大小的卷积的性能各不相同，所以自动调优器可以运行一个基准来找到最佳算法(当前的算法是<a class="ae ky" href="https://github.com/pytorch/pytorch/blob/dab5e2a23ed387046d99f825e0d9a45bd58fccaa/aten/src/ATen/native/cudnn/Conv_v7.cpp#L268-L275" rel="noopener ugc nofollow" target="_blank">这些</a>、<a class="ae ky" href="https://github.com/pytorch/pytorch/blob/dab5e2a23ed387046d99f825e0d9a45bd58fccaa/aten/src/ATen/native/cudnn/Conv_v7.cpp#L341-L346" rel="noopener ugc nofollow" target="_blank">这些</a>和<a class="ae ky" href="https://github.com/pytorch/pytorch/blob/dab5e2a23ed387046d99f825e0d9a45bd58fccaa/aten/src/ATen/native/cudnn/Conv_v7.cpp#L413-L418" rel="noopener ugc nofollow" target="_blank">这些</a>)。当您的输入大小不经常改变时，建议使用“打开”设置。<em class="nk">如果输入大小经常变化，自动调谐器需要过于频繁地进行基准测试，这可能会损害性能。</em>向前和向后传播可以加速1.27倍到1.70倍(<a class="ae ky" href="https://tigress-web.princeton.edu/~jdh4/PyTorchPerformanceTuningGuide_GTC2021.pdf" rel="noopener ugc nofollow" target="_blank">参考</a>)。</p><pre class="kj kk kl km gt nl ne nm nn aw no bi"><span id="5e82" class="np la it ne b gy nq nr l ns nt">torch.backends.cudnn.benchmark = True</span></pre><p id="4545" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">#GPU #CNN #SaveTime</p><h1 id="81cb" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">16.对4D NCHW张量使用通道_最后存储格式</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/0a420b3d23ee4e14aff2b69cec628bcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yZF37VL9xLoYs6EpwpnyqQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">4D NCHW被重组为NHWC格式(图片由作者从<a class="ae ky" href="https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html" rel="noopener ugc nofollow" target="_blank">参考</a>获得灵感)</p></figure><p id="acf9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用<code class="fe nb nc nd ne b"><strong class="lt iu">channels_last</strong></code>内存格式以逐个像素的方式保存图像，作为内存中最密集的格式。原始4D NCHW张量在内存中按每个通道(红/绿/蓝)进行聚类。转换后，<code class="fe nb nc nd ne b">x = x.to(<strong class="lt iu">memory_format=torch.channels_last</strong>)</code>，数据在内存中被重组为NHWC ( <code class="fe nb nc nd ne b">channels_last</code>格式)。你可以看到RGB层的每个像素都更近了。据报道，这种NHWC格式通过FP16的放大器获得了8%到35%的加速(<a class="ae ky" href="https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html" rel="noopener ugc nofollow" target="_blank">参考</a>)。</p><p id="6791" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">目前仍在测试阶段，仅支持4D NCHW张量和一组模型(如<code class="fe nb nc nd ne b">alexnet</code>、<code class="fe nb nc nd ne b">mnasnet</code>族、<code class="fe nb nc nd ne b">mobilenet_v2</code>、<code class="fe nb nc nd ne b">resnet</code>族、<code class="fe nb nc nd ne b">shufflenet_v2</code>、<code class="fe nb nc nd ne b">squeezenet1</code>、<code class="fe nb nc nd ne b">vgg</code>族，见<a class="ae ky" href="https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html#performance-gains" rel="noopener ugc nofollow" target="_blank">全列表</a>)。但我可以肯定地看到这将成为一个标准的优化。</p><pre class="kj kk kl km gt nl ne nm nn aw no bi"><span id="f3c9" class="np la it ne b gy nq nr l ns nt">N, C, H, W = 10, 3, 32, 32</span><span id="6a5f" class="np la it ne b gy nu nr l ns nt">x = torch.rand(N, C, H, W)</span><span id="9469" class="np la it ne b gy nu nr l ns nt"># Stride is the gap between one element to the next one <br/># in a dimension.<br/>print(x.stride()) <em class="nk"># (3072, 1024, 32, 1)</em></span><span id="c608" class="np la it ne b gy nu nr l ns nt"><strong class="ne iu"><em class="nk"># Convert the tensor to NHWC in memory<br/></em>x2 = x.to(memory_format=torch.channels_last)</strong></span><span id="470a" class="np la it ne b gy nu nr l ns nt">print(x2.shape)  <em class="nk"># (10, 3, 32, 32) as dimensions order preserved</em></span><span id="4cc5" class="np la it ne b gy nu nr l ns nt">print(x2.stride())  <em class="nk"># (3072, 1, 96, 3), which are smaller</em></span><span id="ce41" class="np la it ne b gy nu nr l ns nt">print((x==x2).all()) <em class="nk"># True because the values were not changed</em></span></pre><p id="f25d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">#GPU #CNN #SaveTime</p><h1 id="564c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">17.关闭批量规范化之前的卷积图层的偏差</h1><p id="fb50" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这是可行的，因为从数学上讲，偏差效应可以通过批量标准化的均值减法来抵消。我们可以保存模型参数、运行时间和内存。</p><pre class="kj kk kl km gt nl ne nm nn aw no bi"><span id="a5d1" class="np la it ne b gy nq nr l ns nt">nn.Conv2d(..., <strong class="ne iu">bias=False</strong>)</span></pre><p id="e53c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"># CPU # GPU # CNN #保存时间#保存内存</p><h1 id="4704" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">18.用<code class="fe nb nc nd ne b">DistributedDataParallel</code>代替<code class="fe nb nc nd ne b">DataParallel</code></h1><p id="84f7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">对于多GPU来说，即使只有一个节点，也总是首选<code class="fe nb nc nd ne b"><a class="ae ky" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel" rel="noopener ugc nofollow" target="_blank">DistributedDataParallel</a></code>而不是<code class="fe nb nc nd ne b"><a class="ae ky" href="https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel" rel="noopener ugc nofollow" target="_blank">DataParallel</a></code>，因为<code class="fe nb nc nd ne b"><a class="ae ky" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel" rel="noopener ugc nofollow" target="_blank">DistributedDataParallel</a></code>应用多处理，并为每个GPU创建一个进程来绕过Python全局解释器锁(GIL)并加速。</p><p id="776c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"># GPU #分布式优化#保存时间</p><h1 id="47f3" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">摘要</h1><p id="5160" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这篇文章中，我制作了一个清单，并提供了18个PyTorch技巧的代码片段。然后，我解释了它们如何以及为什么在各个方面逐一工作，包括数据加载、数据操作、模型架构<strong class="lt iu">、</strong>训练、推理、CNN特定的优化以及分布式计算。一旦你深刻理解了它们是如何工作的，你也许就能在任何深度学习框架中找到适用于深度学习建模的通用原则。</p><p id="c331" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">希望您喜欢更高效的PyTorch并学习新的东西！</p><p id="77d3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你有任何意见和建议，请在下面留下。谢谢你。</p><p id="b476" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我是Jack Lin，<a class="ae ky" href="https://c3.ai/" rel="noopener ugc nofollow" target="_blank"> C3.ai </a>的高级数据科学家，我对深度学习和机器学习充满热情。你可以看看<a class="ae ky" href="https://medium.com/@jacklindsai" rel="noopener">我在Medium </a>上的其他文章！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/95c2c26c98a22983d48cadeb4546fced.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ikrnChJmmCHzxxa_"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">希望我们的深度学习管道将是“火箭般快速”的:D(图片由<a class="ae ky" href="https://unsplash.com/@billjelen" rel="noopener ugc nofollow" target="_blank">比尔·杰伦</a>在<a class="ae ky" href="https://unsplash.com/photos/NVWyN8GamCk" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄)</p></figure></div></div>    
</body>
</html>