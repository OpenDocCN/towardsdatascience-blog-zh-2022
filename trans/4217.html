<html>
<head>
<title>What are Stable Diffusion Models and Why are they a Step Forward for Image Generation?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是稳定扩散模型，为什么它们是图像生成的一个进步？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-are-stable-diffusion-models-and-why-are-they-a-step-forward-for-image-generation-aa1182801d46#2022-09-20">https://towardsdatascience.com/what-are-stable-diffusion-models-and-why-are-they-a-step-forward-for-image-generation-aa1182801d46#2022-09-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bd3e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><strong class="ak">潜在扩散模型简易指南</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/dd3bab271ed8381c828670057f86a5e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WTe5olMSFC-T6No0Y_gKWg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:潜在扩散模型(基础图:[3]，概念图覆盖:作者)</p></figure><p id="6d71" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，您将了解图像生成领域的最新进展。更具体地说，你将了解<em class="lu">潜在扩散模型(LDM) </em>及其应用。这篇文章将建立在<em class="lu">甘斯</em>、<em class="lu">扩散模型</em>和<em class="lu">变形金刚</em>的概念之上。所以，如果你想更深入地挖掘这些概念，请随时查看我以前关于这些主题的帖子。</p><p id="540a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">也许过去十年在计算机视觉和机器学习方面的突破是GANs(生成对抗网络)的发明——一种引入了超越数据中已有内容的可能性的方法，这是一个全新领域的垫脚石，现在称为生成建模。然而，在经历了一个蓬勃发展的阶段后，GANs开始面临一个平台期，其中大多数方法都在努力解决一些对抗性方法所面临的瓶颈。这不是个别方法的问题，而是问题本身的对抗性。全球农业网络的一些主要瓶颈是:</p><ul class=""><li id="effc" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">图像生成缺乏多样性</li><li id="4e07" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">模式崩溃</li><li id="81f6" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">问题学习多模态分布</li><li id="6572" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">高训练时间</li><li id="3c05" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">由于问题公式化的对抗性，不容易训练</li></ul><p id="dc22" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">存在另一系列基于可能性的方法(例如，马尔可夫随机场),其已经存在了相当一段时间，但是由于对于每个问题来说实现和公式化是复杂的，所以未能获得重大影响。其中一种方法是“<em class="lu">扩散模型</em>”——这种方法从气体扩散的物理过程中获得灵感，并试图对多个科学领域中的相同现象进行建模。然而，在图像生成领域，它们的使用最近变得很明显。主要是因为我们现在有更多的计算能力来测试复杂的算法，否则在过去是不可行的。</p><p id="bdc6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">标准的<em class="lu">扩散模型</em>有两个主要的过程域:<em class="lu">正向扩散</em>和<em class="lu">反向扩散</em>。在前向扩散阶段，通过逐渐引入噪声直到图像变成完全随机噪声，图像被破坏。在相反的过程中，一系列马尔可夫链<em class="lu">用于通过在每个时间步长逐渐去除预测的噪声来从高斯噪声中恢复数据。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/e4dc9a9d6215663ab06b36d96578ffff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oUzGQwCnmbVPMrlhM0f1pQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:典型的扩散模型过程(来源:[1])</p></figure><p id="009e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">扩散模型</em>最近在图像生成任务中表现出了显著的性能，并在图像合成等几项任务中取代了GANs的性能。这些模型还能够产生更多样的图像，并被证明不会遭受模式崩溃。这是由于<em class="lu">扩散模型</em>能够保持数据的语义结构。然而，这些模型对计算要求很高，训练需要非常大的内存和碳足迹，这使得大多数研究人员甚至不可能尝试这种方法。这是因为所有的马尔可夫状态都需要在内存中进行预测，这意味着大型深网的多个实例一直存在于内存中。此外，这种方法的训练时间也变得太长(例如，几天到几个月)，因为这些模型往往会陷入图像数据中细微的<em class="lu">难以察觉的</em>错综复杂中。然而，需要注意的是，这种细粒度的图像生成也是<em class="lu">扩散模型</em>的主要优势之一，因此，使用它们是一种悖论。</p><p id="1d4c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一个非常著名的来自NLP领域的方法系列是<em class="lu"> Transformers </em>。他们在语言建模和构建对话式人工智能工具方面非常成功。在视觉应用中，<em class="lu">变形金刚</em>已经显示出泛化和自适应的优势，这使得它们适合于通用学习。它们比其他技术更好地捕捉文本甚至图像中的语义结构。然而，与其他方法相比，<em class="lu">变形金刚</em>需要大量的数据，并且在许多视觉领域面临着性能停滞。</p><p id="f1e7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">潜在扩散模型</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/dd3bab271ed8381c828670057f86a5e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WTe5olMSFC-T6No0Y_gKWg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:潜在扩散模型(基础图:[3]，概念图覆盖:作者)</p></figure><p id="9ce1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最近提出的一种方法，通过将GANs的感知能力、<em class="lu">扩散模型</em>的细节保留能力和变形金刚的语义能力三者融合在一起，充分利用了这三者。这种技术被作者称为'<em class="lu">潜在扩散模型</em> ' <em class="lu"> (LDM) </em>。事实证明，LDM比前面提到的所有模型都更加健壮和高效。与其他方法相比，它们不仅内存效率高，而且可以生成各种各样的、非常详细的图像，这些图像保留了数据的语义结构。简而言之，<em class="lu"> LDM </em>是扩散过程在潜在空间而不是像素空间中的应用，同时结合了来自<em class="lu">变换器</em>的语义反馈。</p><p id="52c4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">任何生成学习方法都有两个主要阶段:感知压缩和语义压缩。</p><p id="9e74" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">感知压缩</strong></p><p id="8a4d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在感知压缩学习阶段，学习方法必须通过去除高频细节将数据封装成抽象表示。这一步对于构建环境的不变且健壮的表示是必要的。<em class="lu"> GANs </em>擅长提供这样的感知压缩。他们通过将像素空间的高维冗余数据投影到一个叫做潜在空间的超空间来实现这一点。潜在空间中的潜在向量是原始像素图像的压缩形式，可以有效地用于代替原始图像。</p><p id="6e72" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">更具体地说，自动编码器(AE)结构是捕获感知压缩的结构。AE中的编码器将高维数据投影到潜在空间，解码器从潜在空间恢复图像。</p><p id="01ca" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">语义压缩</strong></p><p id="58ae" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在学习的第二阶段，图像生成方法必须能够捕获数据中存在的语义结构。这种概念和语义结构保存了图像中各种对象的上下文和相互关系。<em class="lu">变形金刚</em>擅长捕捉文字和图像中的语义结构。<em class="lu">变形器的</em>概括能力和<em class="lu">扩散模型的细节保持能力的组合</em>提供了两个世界的最佳，并给出了一种生成细粒度高度细节图像同时保持图像中语义结构的方法能力。</p><p id="89c1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">知觉丧失</strong></p><p id="b238" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">LDM中的自动编码器通过将数据投射到潜在空间来捕捉数据的感知结构。作者使用特殊损失函数来训练这种自动编码器，称为，'<em class="lu">感知损失</em>'[4–5]。该损失函数确保重建被限制在图像流形内，并降低了模糊度，否则当使用像素空间损失(例如，L1/L2损失)时会出现模糊度。</p><p id="fbc8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">扩散损失</strong></p><p id="4d2b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">扩散模型通过逐渐去除正态分布变量中的噪声来学习数据分布。换句话说，DMs采用长度为<em class="lu"> T </em>的反向<em class="lu">马尔可夫链</em>。这也意味着DMs可以建模为一系列时间步长的'<em class="lu"> T' </em>去噪自动编码器<em class="lu"> t </em> <em class="lu"> =1，…，T. </em>这由以下等式中的εθ表示。注意，损失函数取决于潜在向量，而不是像素空间。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mk"><img src="../Images/6bacbeabfb5f9f857176503330a72c74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RtR_HiTQOMeSVhaehGT4yg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4:潜在扩散模型损失函数解释(来源:作者)</p></figure><p id="7e29" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">条件扩散</strong></p><p id="7617" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">扩散模型是依赖于先验的条件模型。在图像生成任务的情况下，先验通常是文本、图像或语义图。为了获得该条件的潜在表示，使用了将文本/图像嵌入潜在向量“τ”中的变换器(例如剪辑)。因此，最终的损失函数不仅取决于原始图像的潜在空间，还取决于条件的潜在嵌入。</p><p id="6ce7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">注意机制</strong></p><p id="3457" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">LDM的主干是一个U-Net自动编码器，具有提供交叉注意机制的稀疏连接[6]。一个<em class="lu">变换器</em>网络将条件文本/图像编码成一个潜在嵌入，该潜在嵌入又通过一个交叉注意层映射到U-Net的中间层。这个交叉注意力层实现了注意力(<strong class="la iu"> Q，k，v</strong>)= soft max(<strong class="la iu">qk</strong>t/✔<strong class="la iu">d</strong>)<strong class="la iu">v</strong>。而<strong class="la iu"> Q，K </strong>和<strong class="la iu"> V </strong>是可学习的投影矩阵【6】。</p><p id="78d4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">文本到图像合成</strong></p><p id="f061" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们使用python中最新的官方实现<em class="lu"> LDM </em> v4来生成图像。在文本到图像合成中，<em class="lu"> LDM </em>使用预训练的<em class="lu">剪辑</em>模型【7】，为文本和图像等多种形式提供通用的基于转换器的嵌入。然后，变压器模型的输出被输入到<em class="lu"> LDM </em>的python API，名为“<em class="lu">扩散器”</em>。也可以调整一些参数(例如，扩散级数、种子、图像尺寸等。).</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ml"><img src="../Images/6fa0c03f654b0efaaaf7d8225d0b5377.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LX1Kjn6JiHAtzL0uPT3A5Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5:使用文本输入的LDM生成的图像(来源:作者)</p></figure><p id="c60b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">图像到图像的合成</strong></p><p id="2a95" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">相同的设置对于图像到图像的合成也是有效的，但是，需要输入样本图像作为参考图像。生成的图像在语义和视觉上与作为参考给出的图像相似。这个过程在概念上类似于基于风格的GAN模型，但是，它在保留图像的语义结构方面做得更好。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mm"><img src="../Images/4a19a068b1fca8207a1f58a42b2d2b4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1wDc-36idjdzEPBliBrIIQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6:图5:使用图像+文本输入的LDM生成的图像(来源:作者)</p></figure><p id="7a09" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">结论</strong></p><p id="58fe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们已经涵盖了图像生成领域的一个非常新的发展，称为潜在扩散模型。LDM在生成细节丰富的不同背景的高分辨率图像方面是健壮的，同时它们还保留了图像的语义结构。因此，LDM在图像生成和深度学习方面都向前迈进了一步。如果您仍然对“<strong class="la iu">稳定</strong>扩散模型”感到疑惑，那么它只是LDMs的一个更名，应用于高分辨率图像，同时使用<em class="lu">剪辑</em>作为文本编码器。</p><p id="7108" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您想亲自尝试这种方法，您可以使用以下链接中简单易用的笔记本:</p><p id="c910" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">代码:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://www.github.com/azad-academy/stable-diffusion-model-tutorial"><div class="gh gi mn"><img src="../Images/0e8311a32346755a3295a50b2b996c9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:186/format:webp/1*xZErdSl3-nLCgAluzWBFig.png"/></div></a></figure><p id="5f49" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">订阅更新内容:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://azad-wolf.medium.com/subscribe"><div class="gh gi mo"><img src="../Images/7e5e03d04fdbf376b419c449aad341f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/format:webp/1*QxdDc9BTNIKvgenK-ebhwA.png"/></div></a></figure><p id="5319" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">成为帕特里翁的支持者:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://www.patreon.com/azadlab"><div class="gh gi mp"><img src="../Images/78810e7ce1a0510e8cbcb93b7b1e3ad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:204/format:webp/1*UhLmWjwkPw4A-b1MPN2zrg.png"/></div></a></figure><p id="125a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">参考文献:</strong></p><p id="4d90" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[1] Jonathan Ho，Ajay Jain，Pieter Abbeel，“去噪扩散概率模型<a class="ae mq" href="https://arxiv.org/abs/2006.11239" rel="noopener ugc nofollow" target="_blank">”，2020</a></p><p id="7e7c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2]亚历克·拉德福德、琼·金旭、克里斯·哈拉西、阿迪蒂亚·拉梅什、加布里埃尔·戈、桑迪尼·阿加瓦尔、吉里什·萨斯特里、阿曼达·阿斯克尔、帕梅拉·米什金、杰克·克拉克、格雷琴·克鲁格、伊利亚·苏茨基弗，“<a class="ae mq" href="https://arxiv.org/abs/2103.00020" rel="noopener ugc nofollow" target="_blank">从自然语言监督中学习可转移的视觉模型</a>”，2021</p><p id="5280" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[3]罗宾·龙巴赫和安德里亚斯·布拉特曼和张秀坤·洛伦茨和帕特里克·埃塞和比约恩·奥姆，《利用潜在扩散模型的高分辨率图像合成<a class="ae mq" href="https://arxiv.org/abs/2112.10752" rel="noopener ugc nofollow" target="_blank">》，arXiv:2112.10752，2021，</a></p><p id="4921" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[4]<a class="ae mq" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang%2C+R" rel="noopener ugc nofollow" target="_blank"/>，<a class="ae mq" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Isola%2C+P" rel="noopener ugc nofollow" target="_blank">菲利普·伊索拉</a>，<a class="ae mq" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Efros%2C+A+A" rel="noopener ugc nofollow" target="_blank">阿列克谢·阿·埃夫罗斯</a>，<a class="ae mq" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shechtman%2C+E" rel="noopener ugc nofollow" target="_blank">埃利·谢赫曼</a>，<a class="ae mq" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+O" rel="noopener ugc nofollow" target="_blank">奥利弗·王</a>，<a class="ae mq" href="https://arxiv.org/abs/1801.03924" rel="noopener ugc nofollow" target="_blank">作为知觉度量的深度特征的不合理有效性</a>，2018</p><p id="b19a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[5] Patrick Esser、Robin Rombach、bjrn Ommer，“驯服变形金刚，实现高分辨率图像合成”T27，CVPR，2020年</p><p id="8f9c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[6]阿希什·瓦斯瓦尼、诺姆·沙泽尔、尼基·帕尔马、雅各布·乌兹科雷特、利永·琼斯、艾丹·戈麦斯、卢卡斯·凯泽、伊利亚·波洛舒欣，“<a class="ae mq" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>”，2017年</p><p id="395e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[7]亚历克·拉德福德、琼·金旭、克里斯·哈拉西、阿迪蒂亚·拉梅什、加布里埃尔·戈、桑迪尼·阿加瓦尔、吉里什·萨斯特里、阿曼达·阿斯克尔、帕梅拉·米什金、杰克·克拉克、格雷琴·克鲁格、伊利亚·苏茨基弗，“<a class="ae mq" href="https://arxiv.org/abs/2103.00020" rel="noopener ugc nofollow" target="_blank">从自然语言监督中学习可转移视觉模型</a>”，2021</p><p id="4ea5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">8布拉特曼等人。艾尔。，潜在扩散模型，【https://github.com/CompVis/latent-diffusion】T2，2022</p></div></div>    
</body>
</html>