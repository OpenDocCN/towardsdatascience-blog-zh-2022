<html>
<head>
<title>Significantly Increase Your Grid-Search Results With These Parameters</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用这些参数显著增加您的网格搜索结果</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/significantly-increase-your-grid-search-results-with-these-parameters-b096b3d158aa#2022-12-02">https://towardsdatascience.com/significantly-increase-your-grid-search-results-with-these-parameters-b096b3d158aa#2022-12-02</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="bad2" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">使用估计器开关对任何机器学习流水线步骤进行网格搜索</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/29fb0282871d7aa237c6ca3bd9a9b6ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TWPCbrtNpc9y-pwD"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">照片由<a class="ae kz" href="https://unsplash.com/es/@hjrc33?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">埃克托尔·j·里瓦斯</a>在<a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="f03a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">构建机器学习模型的一个非常常见的步骤是使用交叉验证在训练集上对分类器的参数进行网格搜索，以找到最佳参数。鲜为人知的是，您还可以对几乎任何管道步骤进行网格搜索，例如特征工程步骤。例如，哪种插补策略对数值最有效？均值、中值还是任意？使用哪种分类编码方法？一键编码，还是顺序编码？</p><p id="b0d9" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在本文中，我将指导您使用网格搜索在您自己的机器学习项目中回答这些问题。</p><p id="a258" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">要安装本文所需的所有 Python 包:</p><p id="3bfe" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><code class="fe lw lx ly lz b">pip install extra-datascience-tools feature-engine</code></p><h2 id="e298" class="ma mb iu bd mc md me dn mf mg mh dp mi lj mj mk ml ln mm mn mo lr mp mq mr ms bi translated">数据集</h2><p id="0627" class="pw-post-body-paragraph la lb iu lc b ld mt jv lf lg mu jy li lj mv ll lm ln mw lp lq lr mx lt lu lv in bi translated">让我们考虑下面这个我创建的非常简单的公共领域数据集，它有两列:<code class="fe lw lx ly lz b">last_grade</code>和<code class="fe lw lx ly lz b">passed_course</code>。“最后成绩”列包含学生在最后一次考试中取得的成绩，而“通过课程”列是一个布尔列，如果学生通过了课程，则为<code class="fe lw lx ly lz b">True</code>，如果学生未通过课程，则为<code class="fe lw lx ly lz b">False</code>。我们能否建立一个模型，根据学生的最后成绩来预测他们是否通过了课程？</p><p id="367d" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">让我们首先探索一下<a class="ae kz" href="https://gist.github.com/sTomerG/22f485026a29079810bc4c62ed4ef064" rel="noopener ugc nofollow" target="_blank">数据集</a>:</p><pre class="kk kl km kn gu my lz mz bn na nb bi"><span id="5050" class="nc mb iu lz b be nd ne l nf ng">import pandas as pd<br/><br/>df = pd.read_csv('last_grades.csv')<br/>df.isna().sum()</span></pre><pre class="nh my lz mz bn na nb bi"><span id="dcdc" class="nc mb iu lz b be nd ne l ni ng">OUTPUT<br/>last_grade       125<br/>course_passed      0<br/>dtype: int64</span></pre><p id="89bc" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我们的目标变量<code class="fe lw lx ly lz b">course_passed</code>没有<code class="fe lw lx ly lz b">nan</code>值，所以不需要在这里删除行。</p><p id="2292" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">当然，为了防止任何数据泄漏，我们应该在继续之前先将数据集分成训练集和测试集。</p><pre class="kk kl km kn gu my lz mz bn na nb bi"><span id="8055" class="nc mb iu lz b be nd ne l nf ng">from sklearn.model_selection import train_test_split<br/><br/>X_train, X_test, y_train, y_test = train_test_split(<br/>                                    df[['last_grade']],<br/>                                    df['course_passed'],<br/>                                    random_state=42)</span></pre><p id="2186" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">因为大多数机器学习模型不允许<code class="fe lw lx ly lz b">nan</code>值，所以我们必须考虑不同的插补策略。当然，一般来说，你会启动 EDA(探索性数据分析)来确定<code class="fe lw lx ly lz b">nan</code>值是<em class="nj"> MAR </em>(随机缺失)<em class="nj"> MCAR </em>(完全随机缺失)还是<em class="nj"> MNAR </em>(非随机缺失)。这里有一篇很好的文章解释了这两者之间的区别:</p><div class="nk nl gq gs nm nn"><a rel="noopener follow" target="_blank" href="/all-about-missing-data-handling-b94b8b5d2184"><div class="no ab fp"><div class="np ab nq cl cj nr"><h2 class="bd iv gz z fq ns fs ft nt fv fx it bi translated">关于丢失数据处理的所有内容</h2><div class="nu l"><h3 class="bd b gz z fq ns fs ft nt fv fx dk translated">丢失数据是数据专业人员每天都需要处理的问题。虽然有很多文章，博客…</h3></div><div class="nv l"><p class="bd b dl z fq ns fs ft nt fv fx dk translated">towardsdatascience.com</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob kt nn"/></div></div></a></div><p id="e908" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我们不去分析为什么有些学生的最后一次成绩丢失了，而是简单地尝试在不同的插补技术上进行网格搜索，以说明如何在任何流水线步骤上进行网格搜索，例如这个特征工程步骤。</p><p id="d730" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">让我们来探究自变量<code class="fe lw lx ly lz b">last_grade</code>的分布:</p><pre class="kk kl km kn gu my lz mz bn na nb bi"><span id="eb6e" class="nc mb iu lz b be nd ne l nf ng">import seaborn as sns<br/><br/>sns.histplot(data=X_train, x='last_grade')</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oc"><img src="../Images/314d10f5cf007ff5922bc69601aeae89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*moNK-Y2nI_JSiKOhVQZTrA.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">最后一级的分布(按作者分类的图片)</p></figure><p id="3b6e" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">看起来最后的成绩正态分布，平均值为~ <em class="nj"> 6.5 </em>，值在~ <em class="nj"> 3 </em>和~ <em class="nj"> 9.5 </em>之间。</p><p id="5b46" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">让我们看看目标变量的分布，以确定使用哪个评分标准:</p><pre class="kk kl km kn gu my lz mz bn na nb bi"><span id="0987" class="nc mb iu lz b be nd ne l nf ng">y_train.value_counts()</span></pre><pre class="nh my lz mz bn na nb bi"><span id="34b7" class="nc mb iu lz b be nd ne l ni ng">OUTPUT<br/>True     431<br/>False    412<br/>Name: course_passed, dtype: int64</span></pre><p id="e19c" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">目标变量大致平分，这意味着我们可以使用<em class="nj"> scikit-learn </em>的默认计分器进行分类任务，也就是准确度分数。如果目标变量划分不均，准确度分数不准确，则使用<a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html" rel="noopener ugc nofollow" target="_blank"> F1 </a>代替。</p><h2 id="ee30" class="ma mb iu bd mc md me dn mf mg mh dp mi lj mj mk ml ln mm mn mo lr mp mq mr ms bi translated">网格搜索</h2><p id="2e57" class="pw-post-body-paragraph la lb iu lc b ld mt jv lf lg mu jy li lj mv ll lm ln mw lp lq lr mx lt lu lv in bi translated">接下来，我们将建立模型和网格搜索，并通过优化分类器的参数来运行它，这是我看到的大多数数据科学家使用网格搜索的方式。我们现在将使用<a class="ae kz" href="https://feature-engine.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> <em class="nj">特征引擎</em> </a> <em class="nj">的</em> <code class="fe lw lx ly lz b"><a class="ae kz" href="https://feature-engine.readthedocs.io/en/latest/api_doc/imputation/MeanMedianImputer.html#feature_engine.imputation.MeanMedianImputer" rel="noopener ugc nofollow" target="_blank">MeanMedianImputer</a></code>来估算平均值，使用<em class="nj"> scikit-learn </em>的<code class="fe lw lx ly lz b"><a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank">DecisionTreeClassifier</a></code>来预测目标变量。</p><pre class="kk kl km kn gu my lz mz bn na nb bi"><span id="5517" class="nc mb iu lz b be nd ne l nf ng">from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.model_selection import GridSearchCV<br/><br/>from feature_engine.imputation import MeanMedianImputer<br/><br/>model = Pipeline(<br/>  [<br/>    ("meanmedianimputer", MeanMedianImputer(imputation_method="mean")),<br/>    ("tree", DecisionTreeClassifier())<br/>  ]<br/>)<br/><br/>param_grid = [<br/>  {"tree__max_depth": [None, 2, 5]}<br/>]<br/><br/>gridsearch = GridSearchCV(model, param_grid=param_grid)<br/>gridsearch.fit(X_train, y_train)<br/><br/>pd.DataFrame(gridsearch.cv_results_).loc[:,<br/>                                      ['rank_test_score', <br/>                                       'mean_test_score', <br/>                                       'param_tree__max_depth']<br/>                                      ].sort_values('rank_test_score')</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj od"><img src="../Images/084822a62c16b68dd841b8b7844f4918.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xmtvvd1DbuVkmP4Lw_MrmQ.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">以上代码的结果(图片由作者提供)</p></figure><p id="82d0" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">从上表中我们可以看到，使用<code class="fe lw lx ly lz b">GridsearchCV</code>我们了解到，只需将<code class="fe lw lx ly lz b">DecisionTreeClassifier</code>的<code class="fe lw lx ly lz b">max_depth</code>从默认值<em class="nj">无</em>更改为<em class="nj"> 5，就可以将模型的精度提高约 0.55。这清楚地说明了网格搜索的积极影响。</em></p><p id="a3c6" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">然而，我们不知道用平均值来估算缺失的<code class="fe lw lx ly lz b">last_grades</code>是否实际上是最佳估算策略。我们能做的实际上是使用<a class="ae kz" href="https://extra-datascience-tools.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"><em class="nj">extra-data science-tools</em></a>’<code class="fe lw lx ly lz b"><a class="ae kz" href="https://extra-datascience-tools.readthedocs.io/en/latest/notebooks/tutorial.html#EstimatorSwitch" rel="noopener ugc nofollow" target="_blank">EstimatorSwitch</a></code>对三种不同的插补策略进行网格搜索:</p><ul class=""><li id="4df9" class="oe of iu lc b ld le lg lh lj og ln oh lr oi lv oj ok ol om bi translated">平均插补</li><li id="8a35" class="oe of iu lc b ld on lg oo lj op ln oq lr or lv oj ok ol om bi translated">中位数插补</li><li id="db2d" class="oe of iu lc b ld on lg oo lj op ln oq lr or lv oj ok ol om bi translated">任意数字插补(默认为<em class="nj"> 999 </em>为<em class="nj">特征引擎</em>的<code class="fe lw lx ly lz b"><a class="ae kz" href="https://feature-engine.readthedocs.io/en/latest/api_doc/imputation/ArbitraryNumberImputer.html" rel="noopener ugc nofollow" target="_blank">ArbitraryNumberImputer</a></code>)。</li></ul><pre class="kk kl km kn gu my lz mz bn na nb bi"><span id="f580" class="nc mb iu lz b be nd ne l nf ng">from feature_engine.imputation import (<br/>                                  ArbitraryNumberImputer,<br/>                                  MeanMedianImputer,<br/>                               )<br/>from sklearn.model_selection import GridSearchCV<br/>from sklearn.tree import DecisionTreeClassifier<br/>from extra_ds_tools.ml.sklearn.meta_estimators import EstimatorSwitch<br/><br/># create a pipeline with two imputation techniques<br/>model = Pipeline(<br/>  [<br/>    ("meanmedianimputer", EstimatorSwitch(<br/>                            MeanMedianImputer()<br/>                          )),<br/>    ("arbitraryimputer", EstimatorSwitch(<br/>                            ArbitraryNumberImputer()<br/>                          )),<br/>    ("tree", DecisionTreeClassifier())<br/>  ]<br/>)<br/><br/># specify the parameter grid for the classifier<br/>classifier_param_grid = [{"tree__max_depth": [None, 2, 5]}]<br/><br/># specify the parameter grid for feature engineering<br/>feature_param_grid = [<br/>    {"meanmedianimputer__apply": [True],<br/>     "meanmedianimputer__estimator__imputation_method": ["mean", "median"],<br/>     "arbitraryimputer__apply": [False],<br/>    },<br/>     {"meanmedianimputer__apply": [False],<br/>     "arbitraryimputer__apply": [True],<br/>    },<br/>    <br/>]<br/><br/># join the parameter grids together<br/>model_param_grid = [<br/>    {<br/>        **classifier_params,<br/>        **feature_params<br/>    }<br/>    for feature_params in feature_param_grid<br/>    for classifier_params in classifier_param_grid<br/>]</span></pre><p id="1ffb" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这里需要注意一些重要的事情:</p><ul class=""><li id="0ddf" class="oe of iu lc b ld le lg lh lj og ln oh lr oi lv oj ok ol om bi translated">我们在管道中的<em class="nj"> extra-datascience-tools 的</em> <code class="fe lw lx ly lz b">EstimatorSwitch</code>中包含了两个估算器，因为我们不想同时使用两个估算器。这是因为在第一个估算器转换了<em class="nj"> X </em>后，将没有<code class="fe lw lx ly lz b">nan</code>值留给第二个估算器进行转换。</li><li id="1071" class="oe of iu lc b ld on lg oo lj op ln oq lr or lv oj ok ol om bi translated">我们在分类器参数网格和特征工程参数网格之间分割参数网格。在代码的底部，我们将这两个网格连接在一起，以便每个特征工程网格都与每个分类器网格相结合，因为我们想为<code class="fe lw lx ly lz b">ArbitraryNumberImputer</code>和<code class="fe lw lx ly lz b">MeanMedianImputer</code>尝试一个<em class="nj"> None </em>、<em class="nj"> 2 </em>和<em class="nj"> 5 </em>的<code class="fe lw lx ly lz b">max_tree_depth</code>。</li><li id="d463" class="oe of iu lc b ld on lg oo lj op ln oq lr or lv oj ok ol om bi translated">我们在特征参数网格中使用字典列表而不是字典，这样可以防止同时应用<code class="fe lw lx ly lz b">MeanMedianImputer</code>和<code class="fe lw lx ly lz b">ArbitraryNumberImputer</code>。使用<code class="fe lw lx ly lz b">EstimatorSwitch</code>的<code class="fe lw lx ly lz b">apply</code>参数，我们可以简单地打开或关闭两个估算器中的一个。当然，你也可以运行代码两次，第一次注释掉第一个估算器，第二次注释掉第二个估算器。但是，这将导致我们的参数网格中出现错误，因此我们也需要调整该网格，并且不同插补策略的结果在相同的网格搜索 cv 结果中不可用，这使得比较更加困难。</li></ul><p id="5c8e" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">让我们看看新的结果:</p><pre class="kk kl km kn gu my lz mz bn na nb bi"><span id="4963" class="nc mb iu lz b be nd ne l nf ng">gridsearch = GridSearchCV(model, param_grid=model_param_grid)<br/>gridsearch.fit(X_train, y_train)<br/><br/>pd.DataFrame(gridsearch.cv_results_).loc[:,<br/>                                         ['rank_test_score', <br/>                                          'mean_test_score', <br/>                                          'param_tree__max_depth',<br/>                     'param_meanmedianimputer__estimator__imputation_method']<br/>                                           ].sort_values('rank_test_score')</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj os"><img src="../Images/9ecc2133c62d7c392b352c4fc3ea4816.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q1OB4YWQzQL7eShvnir_lw.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">特征工程的网格搜索结果(图片由作者提供)</p></figure><p id="406e" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我们现在看到了一个新的最佳模型，这是一个决策树，它的<code class="fe lw lx ly lz b">max_depth</code>为<em class="nj"> 2 </em>，使用了<code class="fe lw lx ly lz b">ArbitraryNumberImputer</code>。我们通过实施不同的插补策略将精确度提高了 1.4%！作为一个受欢迎的奖励，我们的树深度已经缩减为 2，这使得模型更容易解释。</p><p id="b17c" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">当然，网格搜索可能已经花费了相当多的时间，并且不仅在分类器上网格搜索，而且在其他流水线步骤上网格搜索也可能花费更长的时间。有几种方法可以将额外花费的时间减到最少:</p><ul class=""><li id="01cb" class="oe of iu lc b ld le lg lh lj og ln oh lr oi lv oj ok ol om bi translated">首先对分类器的参数进行网格搜索，然后根据情况对其他步骤(如特征工程步骤)进行网格搜索，反之亦然。</li><li id="fcf8" class="oe of iu lc b ld on lg oo lj op ln oq lr or lv oj ok ol om bi translated">使用<em class="nj">extra-data science-tools</em>'<code class="fe lw lx ly lz b"><a class="ae kz" href="https://extra-datascience-tools.readthedocs.io/en/latest/notebooks/tutorial.html#filter_tried_params" rel="noopener ugc nofollow" target="_blank">filter_tried_params</a></code>防止网格搜索的重复参数设置。</li><li id="aaf1" class="oe of iu lc b ld on lg oo lj op ln oq lr or lv oj ok ol om bi translated">使用<em class="nj"> scikit-learn </em>的<code class="fe lw lx ly lz b"><a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html" rel="noopener ugc nofollow" target="_blank">HalvingGridSearch</a></code>或<code class="fe lw lx ly lz b"><a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html#sklearn.model_selection.HalvingRandomSearchCV" rel="noopener ugc nofollow" target="_blank">HalvingRandomSearch</a></code>代替<code class="fe lw lx ly lz b">GridSearchCV</code>(仍在实验阶段)。</li></ul><h1 id="ae6e" class="ot mb iu bd mc ou ov ow mf ox oy oz mi ka pa kb ml kd pb ke mo kg pc kh mr pd bi translated">最后</h1><p id="e040" class="pw-post-body-paragraph la lb iu lc b ld mt jv lf lg mu jy li lj mv ll lm ln mw lp lq lr mx lt lu lv in bi translated">除了使用网格搜索来优化决策树等分类器之外，我们还看到，您实际上可以使用<em class="nj">extra-data science-tools</em>'<code class="fe lw lx ly lz b">EstimatorSwitch</code>通过对插补策略进行网格搜索来优化机器学习管道中的任何步骤。除了插补策略和分类器本身之外，还有一些值得网格搜索的流水线步骤的例子:</p><ul class=""><li id="613c" class="oe of iu lc b ld le lg lh lj og ln oh lr oi lv oj ok ol om bi translated"><a class="ae kz" href="https://feature-engine.readthedocs.io/en/latest/api_doc/encoding/MeanEncoder.html" rel="noopener ugc nofollow" target="_blank">表示分类变量的</a> vs <a class="ae kz" href="https://feature-engine.readthedocs.io/en/latest/api_doc/encoding/OrdinalEncoder.html" rel="noopener ugc nofollow" target="_blank">序数</a> vs <a class="ae kz" href="https://feature-engine.readthedocs.io/en/latest/api_doc/encoding/OneHotEncoder.html" rel="noopener ugc nofollow" target="_blank">一键编码</a>。</li><li id="59d6" class="oe of iu lc b ld on lg oo lj op ln oq lr or lv oj ok ol om bi translated">一个<a class="ae kz" href="https://feature-engine.readthedocs.io/en/latest/api_doc/encoding/RareLabelEncoder.html" rel="noopener ugc nofollow" target="_blank">稀有标签编码器</a>的公差。</li><li id="ab93" class="oe of iu lc b ld on lg oo lj op ln oq lr or lv oj ok ol om bi translated">一个<a class="ae kz" href="https://feature-engine.readthedocs.io/en/latest/api_doc/outliers/ArbitraryOutlierCapper.html" rel="noopener ugc nofollow" target="_blank">任意异常封口机</a>对一个<a class="ae kz" href="https://feature-engine.readthedocs.io/en/latest/api_doc/outliers/OutlierTrimmer.html" rel="noopener ugc nofollow" target="_blank">异常修剪器</a>。</li><li id="303e" class="oe of iu lc b ld on lg oo lj op ln oq lr or lv oj ok ol om bi translated">是否使用<a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html" rel="noopener ugc nofollow" target="_blank">目标回归变压器</a>。</li></ul></div><div class="ab cl pe pf hy pg" role="separator"><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj"/></div><div class="in io ip iq ir"><p id="733a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">感谢阅读！有任何问题，请随时联系我。我经常写关于机器学习、数据科学和 Python 的文章，所以如果你感兴趣，可以考虑在<a class="ae kz" href="https://medium.com/@tomergabay" rel="noopener"> Medium </a>和/或<a class="ae kz" href="https://www.linkedin.com/in/tomer-gabay/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上关注我。</p></div></div>    
</body>
</html>