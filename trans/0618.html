<html>
<head>
<title>Forum Moderation and Filtering with BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 BERT 进行论坛审核和过滤</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-bert-for-forum-moderation-and-filtering-9913c915b78d#2022-02-24">https://towardsdatascience.com/using-bert-for-forum-moderation-and-filtering-9913c915b78d#2022-02-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/d59cabbeeb67c7acce9847fa388b0f30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rWYUD2T0soJyKG07DAaMvw.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由来自<a class="ae jg" href="https://www.pexels.com/photo/red-white-and-black-labeled-box-5437588/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pexels </a>的<a class="ae jg" href="https://www.pexels.com/@brettjordan?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">布雷特·乔丹</a>拍摄</p></figure><div class=""/><div class=""><h2 id="ae94" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">在线论坛仍然在联系人们方面发挥着重要作用，但人工审核是一个耗时的过程。NLP 能帮忙吗？</h2></div><p id="52ef" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">尽管社交媒体和微博在交流方面的增长不受限制，但在线论坛仍然在联系人们方面发挥着重要作用。这种说法的最大证据可能是 Reddit 的成功，这是一个受欢迎的网络收集器，每月有 4 亿多用户和超过 10 万个活跃社区。</p><p id="1b25" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些<strong class="la jk">社区需要版主</strong>来引导和过滤信息流，标记偏离主题/不合适的内容。但是<strong class="la jk">适度是一项耗时的活动，需要版主的强烈承诺</strong>，版主通常是来自社区本身的志愿者。</p><p id="34e0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那么我们如何帮助版主进行他们的版主活动呢？我们能使用 NLP 使他们的工作更容易吗？</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><blockquote class="mb mc md"><p id="2877" class="ky kz me la b lb lc kk ld le lf kn lg mf li lj lk mg lm ln lo mh lq lr ls lt im bi translated">声明:本文属于我的<strong class="la jk">免代码</strong> <strong class="la jk">文章系列</strong>。无代码文章不包括任何对特定软件基础设施或技术实现的引用:没有代码片段，没有库，没有框架。这些内容的唯一目的是提供方法管道和启发实际用例场景，而不是提供现成的代码库来导出。<br/>如果您在项目实施中需要帮助，请随时<a class="ae jg" href="https://www.linkedin.com/in/tbuonocore/" rel="noopener ugc nofollow" target="_blank">联系我</a>！</p></blockquote></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/329d09bc6b54cce386b67148088db4f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AZRFoZhtFMxXbDXgoE4U3A.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://www.pexels.com/photo/multi-colored-folders-piled-up-159519/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>在<a class="ae jg" href="https://www.pexels.com/" rel="noopener ugc nofollow" target="_blank">像素</a>上拍摄</p></figure><h1 id="7254" class="mm mn jj bd mo mp mq mr ms mt mu mv mw kp mx kq my ks mz kt na kv nb kw nc nd bi translated">任务</h1><p id="ae5a" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">首先，让我们试着正式确定我们的<strong class="la jk">目标</strong>。简单来说，让我们假设我们只是想帮助论坛版主<strong class="la jk">检测已经发布在特定版块的离题消息</strong>，例如，一个专门为所有英雄联盟玩家的版块。这是一个<strong class="la jk">二元主题分类任务</strong>，一个监督学习问题，其中我们试图预测一段文本或句子是否属于一个类别。因此，该模型将被训练来区分围绕感兴趣的主题的消息和不围绕感兴趣的主题的消息。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nj"><img src="../Images/c49e3354a971f2c7b48db4e9e435578d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sh_CLNevvs4STyTGzUicQw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><h1 id="295e" class="mm mn jj bd mo mp mq mr ms mt mu mv mw kp mx kq my ks mz kt na kv nb kw nc nd bi translated">数据</h1><p id="4f5c" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated"><strong class="la jk">论坛转储</strong>是我们实现这个任务需要的数据。如上所述，这篇博文旨在尽可能保持一般性和可移植性，所以我们不会依赖任何特定的来源。如果你想继续使用 Reddit，你可以在这里找到前 2500 个子编辑<a class="ae jg" href="https://github.com/umbrae/reddit-top-2.5-million" rel="noopener ugc nofollow" target="_blank">的前 1000 篇帖子的开源转储。</a></p><p id="7797" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，在我们的特定场景中，我们可能希望从感兴趣的子编辑“r/leagueoflegends”中收集样本，并从不同的子编辑(如“r/nba”或“r/funny”)中收集相同数量的随机样本，根据内容与“英雄联盟”主题的相关程度，将每条消息标记为“正题”或“离题”。换句话说，我们需要使用<strong class="la jk">标记的数据</strong>。</p><p id="ef71" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">论坛转储通常不提供关于每条消息中讨论的主题的任何信息，所以我们不知道每条帖子的<em class="me">实际</em>主题是什么。这是我们需要解决的问题，我们有两个选择:</p><ol class=""><li id="937f" class="nk nl jj la b lb lc le lf lh nm ll nn lp no lt np nq nr ns bi translated"><strong class="la jk">让领域专家</strong>浏览整个语料库，手动将每条消息与一个主题相关联。这种<strong class="la jk">人在回路</strong>的解决方案将是精英中的精英，但它非常耗时，而且当样本量很大时，几乎没有人负担得起。</li><li id="5f0c" class="nk nl jj la b lb nt le nu lh nv ll nw lp nx lt np nq nr ns bi translated"><strong class="la jk">假设</strong> <strong class="la jk">消息发布在更接近消息</strong>中讨论的主题的线程上。换句话说，我们假设线程=主题。这等于声明，如果在“蛋糕食谱”板上发布了一个消息，那么该消息的内容必然是一个蛋糕食谱。在这种情况下，我们可以<strong class="la jk">在几秒钟内自动</strong>分配主题标签，无需任何人工参与。</li></ol><h2 id="2612" class="ny mn jj bd mo nz oa dn ms ob oc dp mw lh od oe my ll of og na lp oh oi nc oj bi translated">嘈杂的标签</h2><p id="7492" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">第二种选择显然是对现实的过度简化:事实上，讨论总是倾向于在一段时间后偏离主题，许多消息将只是对其他用户的简短回复(例如，“谢谢”、“是的，我同意”等)。)不涉及任何话题相关内容。</p><p id="bfaa" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，自动标记引入了<strong class="la jk">噪音标签</strong>，这是一种破坏模型性能的现象<strong class="la jk">在本应是基本事实的地方引入了误导信息</strong>。根据特定的训练语料库，有噪声标签的影响可以忽略或不忽略(通常不是)。无论如何，请记住，我们是在用标签时间来换取准确性<strong class="la jk">。</strong></p><p id="9942" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以通过<strong class="la jk">将对话</strong>缩短到第一个<em class="me"> K </em>帖子来限制这种不良行为的程度，因为直觉告诉我们，讨论的负责人通常比其他人更关注这个话题。或者，如果论坛依赖于类似 Reddit 的向下投票/向上投票系统(如果我们的数据集中有这些信息)，我们可以根据排名<strong class="la jk">保留排名前<em class="me"> K </em>的帖子。</strong></p><p id="cddd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">传统的 NLP 预处理(即停用词去除、词干提取等。)也可以应用，尽管在使用最新的上下文模型(如 BERT)时并不特别推荐(甚至停用词也提供某种上下文)。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/ffc8a2574cca3421dd7bdf1e615ccde9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CZHRJbBKo-GQH-31bcn8wQ.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来自<a class="ae jg" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae jg" href="https://pixabay.com/users/scribblinggeek-1818314/" rel="noopener ugc nofollow" target="_blank"> ScribblingGeek </a>的照片</p></figure><h1 id="241a" class="mm mn jj bd mo mp mq mr ms mt mu mv mw kp mx kq my ks mz kt na kv nb kw nc nd bi translated">模型</h1><p id="ee0c" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">一旦我们定义并运行了我们的预处理管道，我们应该最终得到一个干净的、经过过滤的语料库，准备好输入我们最喜欢的 NLP 模型:BERT。</p><p id="96b3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">变压器的双向编码器表示</strong></p><p id="2650" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT </a>和其他 Transformer 编码器架构在自然语言处理的各种任务上取得了巨大成功。他们<strong class="la jk">计算自然语言的向量空间表示，其中每个组件可以被视为一个分类特征</strong>。BERT 系列模型使用<strong class="la jk"> Transformer </strong>编码器架构来<strong class="la jk">处理</strong>前后所有单词的完整上下文中的每个单词，因此得名:Transformers 的双向编码器表示[ <a class="ae jg" href="https://www.tensorflow.org/text/tutorials/classify_text_with_bert" rel="noopener ugc nofollow" target="_blank"> Tensorflow，2021 </a> ]。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ok"><img src="../Images/0416bd4cf2720fc96fe0c700764de4e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FyFce7BITIXmyokymO26JQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="77e2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">BERT 模型通常在大型文本语料库上进行预训练，然后针对特定任务进行微调。这正是我们打算如何使用它，提供来自我们论坛帖子语料库的额外内容，以便<strong class="la jk">通过重新加权其参数来反映论坛中使用的交流风格，从而改进我们的语言模型</strong>。</p><p id="4d56" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">讨论 BERT 的细节超出了这里的范围。你可以在这里找到对变形金刚家族<a class="ae jg" href="https://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">清晰全面的解释。</a></p><p id="98b4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">BERT 有多种版本和变体，在层大小、注意头数量、输入序列长度、预训练目标类型等方面有所不同。像<a class="ae jg" href="https://arxiv.org/abs/1901.08746" rel="noopener ugc nofollow" target="_blank"> BioBERT </a>这样的特定领域版本也已经发布。</p><p id="f75d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">许多成熟的类似 BERT 的模型都可以用<a class="ae jg" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> Huggingface </a>实现，这是 NLP 和 Transformers 的最大社区之一。我个人的建议是坚持使用 Delvin 等人最初的基于 BERT 的版本，然后如果你寻求性能提升或更快的计算，尝试不同的风格。</p><p id="cf70" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">如何使用 BERT 进行消息分类</strong></p><p id="0b6e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那么如何利用 BERT 来检测离题帖呢？使用 BERT 的二进制分类依赖于<strong class="la jk">【CLS】令牌嵌入的输出向量</strong>，其可以被视为整个 post 的<strong class="la jk">浓缩向量表示。如果我们将一个<strong class="la jk">线性层</strong>和一个<strong class="la jk">sigmoid</strong><strong class="la jk">activation</strong>堆叠在编码器堆的顶部，则输出将是一个介于 0 和 1 之间的概率，可用于确定相应的帖子是正题还是离题。</strong></p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ol"><img src="../Images/b79125b8007a26ece2134a18e534b4fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZUmCezCMt-7Gv6Dk42s-gA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="3cd2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是用 BERT 处理二进制文本分类的一般方法。您还可以将 BERT 单独用于<strong class="la jk">嵌入提取目的</strong>，然后使用这样的嵌入作为<strong class="la jk">不同分类器</strong>的输入，就像正则化逻辑回归一样。</p><p id="eff5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">我们可以和伯特一起处理的其他论坛相关任务</strong></p><p id="b3b4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">BERT 是最通用的 NLP 模型之一。例如，如果我们<strong class="la jk">用 linear+softmax 层</strong>替换模型的头部，我们也可以解决<strong class="la jk">多分类问题</strong>，使用分类交叉熵将在管道末端获得的概率与原始标签进行比较。以这种方式训练的模型将能够为每个新消息分配最合适的子线程/主题。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="7a55" class="mm mn jj bd mo mp om mr ms mt on mv mw kp oo kq my ks op kt na kv oq kw nc nd bi translated">结论</h1><p id="b211" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">最近提出的基于注意力的方法表明，许多 NLP 问题的显著改进可以通过微调变压器模型容易地获得，该变压器模型先前被训练用于通用语料库的语言建模，以工作于特定的下游任务。利用迁移学习，有可能<strong class="la jk">改进通用模型，以最少的努力和适量的数据执行小范围的操作</strong>，使我们更接近手动和时间密集型流程(如论坛审核)的<strong class="la jk">自动化。在这篇博文中，我们应用这些概念设计了一个基于<strong class="la jk"> BERT </strong>的论坛审核助手，这是当今最流行和最通用的变形金刚模型之一。</strong></p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><p id="82f0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你觉得有帮助，请在评论区留下你的想法并分享！如果你喜欢我做的事情，你现在可以给我多几个小时的自主权来表示你的支持🍺</p><div class="is it gp gr iu or"><a href="https://www.linkedin.com/in/tbuonocore/" rel="noopener  ugc nofollow" target="_blank"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd jk gy z fp ow fr fs ox fu fw ji bi translated">Tommaso Buonocore 博士生-大数据和生物医学信息学-帕维亚大学|…</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">生物医学工程师和人工智能爱好者，目前正在研究 NLP 解决方案，以改善基于医疗保健的预测任务…</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">www.linkedin.com</p></div></div><div class="pa l"><div class="pb l pc pd pe pa pf ja or"/></div></div></a></div></div></div>    
</body>
</html>