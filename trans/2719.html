<html>
<head>
<title>The Relationship between Interpretability and Fairness</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释性与公平性的关系</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-relationship-between-interpretability-and-fairness-627266bd6fda#2022-06-12">https://towardsdatascience.com/the-relationship-between-interpretability-and-fairness-627266bd6fda#2022-06-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cbc1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">为什么可解释的模型更可能是公平的3个原因</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/bb9bf1f2c019ab9b1ab96450213ed758.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p0CQDBWcDf92baLincQrUw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(来源:<a class="ae ky" href="https://www.flaticon.com/premium-icon/justice_3053751?term=fairness&amp;page=1&amp;position=15&amp;page=1&amp;position=15&amp;related_id=3053751&amp;origin=search" rel="noopener ugc nofollow" target="_blank"> flaticon </a>)</p></figure><p id="d74b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可解释性和公平性似乎是相辅相成的。<a class="ae ky" rel="noopener" target="_blank" href="/interpretability-in-machine-learning-ab0cf2e66e1">可解释性</a>包括理解模型如何做出预测。<a class="ae ky" rel="noopener" target="_blank" href="/what-is-algorithm-fairness-3182e161cf9f">公平</a>包括理解预测是否偏向某些群体。在负责任的人工智能框架和ML会议中，这些特征总是被一起提及。然而，可解释性并不一定意味着公平性。</p><p id="3b34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也就是说，一个可解释的模型更有可能是公平的。这种联系有三个主要原因。对于可解释的模型，更容易确定不公平的原因。在一个可解释的模型中也更容易纠正不公平。一些方法，比如证明解释，甚至依赖于可解释性。最后，这两个特征通过在人工智能系统中建立信任的目标联系在一起。我们将深入讨论这些原因。</p><h1 id="e339" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">用不可知的方法测量偏差</h1><p id="8da4" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在我们深入探讨这些之前，让我们先讨论一下如何分析公平性。这也将帮助我们理解为什么这不是可解释性和公平性相关的原因之一。通常，公平性的衡量标准是<a class="ae ky" rel="noopener" target="_blank" href="/what-are-model-agnostic-methods-387b0e8441ef">模型不可知的</a>。这意味着它们可以用于任何型号。这包括<strong class="lb iu">内在可解释的</strong>模型，如线性回归和决策树。它还包括<strong class="lb iu">更难解释的</strong>模型，如随机森林甚至神经网络。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/f0e8fe25a210672c3ab5ff326390d7be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rJypRcPEOK_s-madSvNl4g.png"/></div></div></figure><p id="c0a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">准确性是衡量公平性的一个简单例子。具体来说，我们将比较该模型在不同人群中的准确性。这里的子群体是由敏感的特征定义的，如种族或性别。如果其中一个小组的准确性明显较低，这表明该模型对该小组不公平。</p><p id="f3d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了计算这个公平的衡量标准，我们只需要模型的预测和实际的目标变量。我们不需要研究模型本身的内部工作原理。这意味着计算可解释模型和不太可解释模型的度量是一样容易的。换句话说，如果 一个模型不公平，可解释性并不会让理解<strong class="lb iu"> <em class="mt">变得更容易。问题是，像这样的衡量标准并没有告诉我们<strong class="lb iu"> <em class="mt">为什么</em> </strong>模型是不公平的。</em></strong></p><h1 id="ffcf" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">原因1:更容易确定不公平的原因</h1><p id="a42c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">不公平有不同的<a class="ae ky" rel="noopener" target="_blank" href="/algorithm-fairness-sources-of-bias-7082e5b78a2c">原因</a> <strong class="lb iu">。</strong>这包括用于训练模型的数据中的偏差。具体来说，<strong class="lb iu">历史偏差</strong>、<strong class="lb iu">代理变量</strong>和<strong class="lb iu">不平衡数据集</strong>会导致不公平的预测。我们围绕<strong class="lb iu">算法</strong>和<strong class="lb iu">用户如何与模型</strong>互动的选择也会引入偏差。对于可解释的模型，更容易识别这些来源。</p><p id="eaee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是因为，对于可解释的模型，更容易理解模型如何工作。这意味着我们清楚地了解哪些特性是重要的。我们也理解这些特征和目标变量之间的关系。通常，与神经网络等模型相比，算法和成本函数也简单得多。所有这些都使得找出导致模型不公平的原因变得更加容易。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/08023a89b5d55dc51349dca8aff027ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dHM_-NvGJXx2sywlkVldqQ.png"/></div></div></figure><p id="d1bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与这个原因相关的是模型是如何构建的。通常，当你建立一个可解释的模型(例如线性回归)时，你将最终选择8到12个模型特征。您还将花时间使用领域知识将特性趋势与预期进行比较。真的，更多的心思放在<a class="ae ky" rel="noopener" target="_blank" href="/characteristics-of-a-good-feature-4f1ac7a90a42">功能选择</a>上。只要我们考虑到公平，我们可以在模型开发过程的早期提出问题。这意味着您甚至可以在偏差被引入模型之前就识别出偏差的来源。</p><h1 id="9d4f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">原因2:更容易纠正偏差</h1><p id="73cd" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">对于可解释的模型，也更容易<a class="ae ky" rel="noopener" target="_blank" href="/approaches-for-addressing-unfairness-in-machine-learning-a31f9807cf31">纠正不公平</a> <strong class="lb iu"> <em class="mt">。</em> </strong>这部分源于上面讨论的第一个原因。我们采取的纠正不公平的方法可能取决于不公平的原因。因为更容易确定原因，所以更容易选择最佳方法来纠正它。例如，我们可以纠正不公平模型的方法之一是移除代理变量。这些特征与种族或性别等敏感特征密切相关。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/c799f0408e791457e1fc40be788a6da0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HDEep7bnzCP-7x6fLyaQJg.png"/></div></div></figure><p id="00ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像随机森林这样的模型可以有多达30-40个特征。模型结构还允许特性之间的<a class="ae ky" rel="noopener" target="_blank" href="/finding-and-visualising-interactions-14d54a69da7c#:~:text=What%20are%20interactions%3F">交互</a>。这种复杂性使得很难理解任何代理变量和目标变量之间的关系。更难理解的是，这种关系是否会导致不公平的决定。最终，决定删除哪些代理变量更加困难。</p><p id="2fa5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以对算法本身进行其他修正。比如我们可以调整成本函数来考虑公平性。这可能涉及添加惩罚不公平决策的惩罚参数。换句话说，模型的目标不仅是做出准确的预测，而且是公平的预测。可解释模型的算法和成本函数通常更简单。这使得调整它们更容易。</p><p id="ccbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">确保公平的方法也可以超越上面提到的量化方法。对抗不公平的一个重要方法是为用户提供预测的解释，然后给用户挑战这些预测的权力。我们越了解一个模型是如何工作的，就越容易用一种人类友好的方式来解释它。换句话说，可解释性更容易给出好的解释。</p><h1 id="687e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">原因3:它们都是关于建立信任的</h1><p id="2d5d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">最重要的原因是可解释性和公平性的共同目标。它们都是关于建立对人工智能系统的信任。这主要是通过减少这些系统可能造成的伤害来实现的。也就是说，我们不会相信我们认为会对我们造成伤害的东西。</p><p id="72a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们更容易相信我们理解的东西。此外，我们对某事了解得越多，就越能确定它不会造成伤害。从这个意义上说，可解释性可以建立信任。公平的目标更直接。它旨在减少不公平决定带来的伤害。最终，一个关心人工智能系统信任度的组织会同时关心可解释性和公平性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/1098604dbd93729a214be11c5c622183.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j9eSKrPOLQ8ilxH8cO1kPg.png"/></div></div></figure><p id="63ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是为什么你会经常在负责任的人工智能框架中看到公平性和可解释性被一起提及。这些治理框架旨在定义开发人工智能系统的道德准则和最佳实践。除了公平性和可解释性，这些框架还涵盖了安全、保密和隐私等主题。像<a class="ae ky" href="https://ai.google/responsibilities/responsible-ai-practices/" rel="noopener ugc nofollow" target="_blank">谷歌</a>、<a class="ae ky" href="https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6" rel="noopener ugc nofollow" target="_blank">微软</a>和<a class="ae ky" href="https://www.ibm.com/artificial-intelligence/ethics" rel="noopener ugc nofollow" target="_blank"> IBM </a>这样的公司都有自己的框架。他们明白客户不会使用他们不信任的系统。</p><blockquote class="mx my mz"><p id="04c5" class="kz la mt lb b lc ld ju le lf lg jx lh na lj lk ll nb ln lo lp nc lr ls lt lu im bi translated">人工智能的发展正在创造新的机会来改善世界各地人们的生活，从商业到医疗保健到教育。这也提出了新的问题，即在这些系统中构建<strong class="lb iu">公平性</strong>、<strong class="lb iu">可解释性</strong>、隐私和安全性的最佳方式。</p><p id="9def" class="kz la mt lb b lc ld ju le lf lg jx lh na lj lk ll nb ln lo lp nc lr ls lt lu im bi translated">—谷歌人工智能研究<a class="ae ky" href="https://ai.google/responsibilities/responsible-ai-practices/" rel="noopener ugc nofollow" target="_blank">负责任的人工智能实践</a></p></blockquote><p id="0b5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">出于类似的原因，公平性和可解释性是相关的研究课题。例如，在下面你可以看到<strong class="lb iu"> FAccT 2022 </strong>会议论文的一些感兴趣的领域。这是一次关于公平、问责和透明的会议。公平性和可解释性是算法开发中首先提到的话题。一个对减少人工智能系统的危害感兴趣的研究者会对这两个话题感兴趣。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/01e7e84b909d54ebb896589c616c4fd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*YUUlKjEHumziZes0_Qye1Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">FAccT 2022征集论文(来源:<a class="ae ky" href="https://facctconference.org/2022/cfp.html" rel="noopener ugc nofollow" target="_blank"> ACM FAccT </a>)</p></figure><p id="7fdf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对信任的需求是公平性和可解释性之间关系的真正驱动力。我们看到可解释模型的特征也使这种关系更加密切。一路上，我们谈到了一些与公平相关的话题。这就是<strong class="lb iu">不公平的原因</strong>、<strong class="lb iu">分析公平</strong>和<strong class="lb iu">保证公平的途径</strong>。下面的三篇文章深入探讨了这些主题。</p><div class="ne nf gp gr ng nh"><a rel="noopener follow" target="_blank" href="/algorithm-fairness-sources-of-bias-7082e5b78a2c"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">你的模型做出不公平预测的5个原因</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">偏见的常见来源——历史偏见、代理变量、不平衡数据集、算法选择和用户交互</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">towardsdatascience.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv ks nh"/></div></div></a></div><div class="ne nf gp gr ng nh"><a rel="noopener follow" target="_blank" href="/analysing-fairness-in-machine-learning-with-python-96a9ab0d0705"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">分析机器学习中的公平性(用Python)</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">做一个探索性的公平性分析，并使用平等机会、均等优势和完全不同来衡量公平性…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">towardsdatascience.com</p></div></div><div class="nq l"><div class="nw l ns nt nu nq nv ks nh"/></div></div></a></div><div class="ne nf gp gr ng nh"><a rel="noopener follow" target="_blank" href="/approaches-for-addressing-unfairness-in-machine-learning-a31f9807cf31"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">解决机器学习中的不公平问题</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">预处理、加工中和后处理定量方法。以及非定量方法…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">towardsdatascience.com</p></div></div><div class="nq l"><div class="nx l ns nt nu nq nv ks nh"/></div></div></a></div></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><p id="78de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望这篇文章对你有帮助！你可以成为我的<a class="ae ky" href="https://conorosullyds.medium.com/membership" rel="noopener"> <strong class="lb iu">推荐会员</strong> </a> <strong class="lb iu">来支持我。你可以访问medium上的所有文章，我可以得到你的部分费用。</strong></p><div class="ne nf gp gr ng nh"><a href="https://conorosullyds.medium.com/membership" rel="noopener follow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">通过我的推荐链接加入Medium康纳·奥沙利文</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">conorosullyds.medium.com</p></div></div><div class="nq l"><div class="of l ns nt nu nq nv ks nh"/></div></div></a></div><p id="1378" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在|<a class="ae ky" href="https://twitter.com/conorosullyDS" rel="noopener ugc nofollow" target="_blank">Twitter</a>|<a class="ae ky" href="https://www.youtube.com/channel/UChsoWqJbEjBwrn00Zvghi4w" rel="noopener ugc nofollow" target="_blank">YouTube</a>|<a class="ae ky" href="https://mailchi.mp/aa82a5ce1dc0/signup" rel="noopener ugc nofollow" target="_blank">时事通讯</a>上找到我——注册免费参加<a class="ae ky" href="https://adataodyssey.com/courses/shap-with-python/" rel="noopener ugc nofollow" target="_blank"> Python SHAP课程</a></p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><h2 id="101e" class="og lw it bd lx oh oi dn mb oj ok dp mf li ol om mh lm on oo mj lq op oq ml or bi translated">图像来源</h2><p id="4c46" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">所有图片都是我自己的或从<a class="ae ky" href="http://www.flaticon.com/" rel="noopener ugc nofollow" target="_blank">www.flaticon.com</a>获得的。在后者的情况下，我拥有他们的<a class="ae ky" href="https://support.flaticon.com/hc/en-us/articles/202798201-What-are-Flaticon-Premium-licenses-" rel="noopener ugc nofollow" target="_blank">保费计划</a>中定义的“完全许可”。</p><h2 id="b905" class="og lw it bd lx oh oi dn mb oj ok dp mf li ol om mh lm on oo mj lq op oq ml or bi translated">参考</h2><p id="4958" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">Birhane，a .(2021)<strong class="lb iu">算法的不公正:一种关系伦理方法</strong>。<a class="ae ky" href="https://www.sciencedirect.com/science/article/pii/S2666389921000155" rel="noopener ugc nofollow" target="_blank">https://www . science direct . com/science/article/pii/s 2666389921000155</a></p><p id="fa02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Pessach，d .和Shmueli，e .(2020)，<strong class="lb iu">算法公平性。</strong><a class="ae ky" href="https://arxiv.org/abs/2001.09784" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2001.09784</a></p><p id="74c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Mehrabi，n .、Morstatter，f .、Saxena，n .、Lerman，k .和Galstyan，A .，(2021)，<strong class="lb iu">关于机器学习中的偏见和公平的调查。</strong><a class="ae ky" href="https://arxiv.org/abs/1908.09635" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1908.09635</a></p></div></div>    
</body>
</html>