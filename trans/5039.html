<html>
<head>
<title>Elegant Text Pre-Processing with NLTK in sklearn Pipeline</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">sklearn管道中使用NLTK的优雅文本预处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/elegant-text-pre-processing-with-nltk-in-sklearn-pipeline-d6fe18b91eb8#2022-11-09">https://towardsdatascience.com/elegant-text-pre-processing-with-nltk-in-sklearn-pipeline-d6fe18b91eb8#2022-11-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0da9" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用组件架构快速启动您的NLP代码</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d7116236742c9ec6590f2c8b5a8bbaaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LlL5LnIRVNlw4wWb"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Max陈在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="2240" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">典型的NLP预测管道从摄取文本数据开始。来自不同来源的文本数据具有不同的特征，在对它们应用任何模型之前，需要进行一定量的预处理。</p><p id="fb1b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们将首先回顾预处理的原因，并讨论不同类型的预处理。然后，我们将通过python代码了解各种文本清理和预处理技术。出于教学目的，本文中提供的所有代码片段都按其对应的类别进行了分组。由于库的独特性，预处理步骤中存在固有的顺序依赖性，所以请不要忘记参考本文中关于建议执行顺序的部分，以避免大量的痛苦和错误。</p><p id="43b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然代码片段可以在Jupyter笔记本中执行和测试，但通过使用统一和定义良好的API将它们重构为python类(或模块),以便在类似生产的sklearn管道中轻松使用和重用，可以实现它们的全部好处。本着这种精神，本文以一个sklearn转换器作为结尾，它包含了本文中概述的所有文本预处理技术和一个管道调用示例。</p><h2 id="3729" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">为什么要预处理？</h2><p id="855c" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">NLP中的所有技术，从简单的单词包到花哨的BERT，都需要一个共同的东西来表示文本——单词向量。而BERT及其同类产品不需要文本预处理，尤其是在使用预训练模型时(例如，它们依赖于单词块算法或其变体，从而消除了对词干提取和词条化等的需要。)，更简单的NLP模型极大地受益于文本预处理。</p><p id="388c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从单词向量的角度来看，每个单词都只是一个数字向量。因此，足球这个词不同于橄榄球。所以踢，被踢和踢是完全不同的。在非常大的语料库上的训练理论上可以为足球/橄榄球以及踢/被踢/踢产生类似的向量表示。但是我们可以马上大大缩短词汇，因为我们知道从我们的角度来看，这些不仅仅是相似的单词，而是相同的单词。类似地，助动词(in，was是as等。)连接词(and since)几乎出现在每个句子中，从NLP的角度来看，它对句子没有任何意义。砍掉这些将减少句子/文档的向量维数。</p><h2 id="9bb4" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">文本预处理的类型</h2><p id="3794" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">下图列出了一长串(但不是全部)非正式的文本处理技术。本文只实现了带虚线边框的绿框中的那些。其中一些不需要介绍，而其他的如实体规范化、依赖解析都是高级的，它们本身就包含某些词法/ML算法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/7b58d31fe845e9518f04bf0dcb6a5e87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9OaVaolyG1HZe734haUxnQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图一。文本预处理技术(图片由作者提供)</p></figure><p id="35aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下表1总结了受这些预处理影响的变化。由特定处理引起的更改在“之前”和“之后”列中突出显示。“之后”列中没有突出显示意味着“之前”中突出显示的文本由于预处理而被移除。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/9f9ea4d591d33c65364f7878363d5dcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0LtsWOm6nokbvMdER7zaig.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表1。每种文本预处理的影响</p></figure><h2 id="29fe" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">亲眼目睹这一切</h2><p id="6941" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">好了，理论到此为止。让我们写一些代码让它做一些有用的事情。与其一次进行一类预处理，不如按顺序进行某些操作。例如，删除html标签(如果有的话)作为第一个预处理步骤，然后是与小写结合的词汇化，然后是其他清理</p><ol class=""><li id="18aa" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated"><strong class="ky ir">所需库</strong></li></ol><p id="be51" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在Python 3.5+上，除了numpy和pandas之外，还应该安装以下模块。</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="f80d" class="ls lt iq nc b gy ng nh l ni nj">pip install nltk<br/>pip install beautifulsoup4 <br/>pip install contractions<br/>pip install Unidecode<br/>pip install textblob<br/>pip install pyspellchecker</span></pre><p id="092f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 2。用熊猫加载数据帧</strong></p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="ec97" class="ls lt iq nc b gy ng nh l ni nj">import pandas as pd</span><span id="acca" class="ls lt iq nc b gy nk nh l ni nj">df = pd.read_csv(“…..”)<br/>text_col = df[“tweets”] #tweets is text column to pre-process</span></pre><p id="0d83" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 3。下壳体</strong></p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="4aab" class="ls lt iq nc b gy ng nh l ni nj">text_col = text_col.apply(lambda x: x.lower())</span></pre><p id="c0f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 4。扩张收缩</strong></p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="9062" class="ls lt iq nc b gy ng nh l ni nj">text_col = text_col.apply(<br/>    lambda x: " ".join([contractions.fix(expanded_word) for expanded_word in x.split()]))</span></pre><p id="c711" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 5。噪声消除</strong></p><p id="2876" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">5.1删除html标签</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="7432" class="ls lt iq nc b gy ng nh l ni nj">from bs4 import BeautifulSoup<br/>text_col = text_col.apply(<br/>    lambda x: BeautifulSoup(x, 'html.parser').get_text())</span></pre><p id="03f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">5.2删除号码</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="e66f" class="ls lt iq nc b gy ng nh l ni nj">text_col = text_col.apply(lambda x: re.sub(r'\d+', '', x))</span></pre><p id="cec6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">5.3用空格替换点</p><p id="3a0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有时文本包含IP，这样的句点应该替换为空格。在这种情况下，请使用下面的代码。然而，如果需要将句子拆分成标记，那么首先使用带有punkt的nltk标记化。它理解句号的出现并不总是意味着句子的结束(如Mr. Mrs. e.g .)，因此会标记化。</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="791d" class="ls lt iq nc b gy ng nh l ni nj">text_col = text_col.apply(lambda x: re.sub("[.]", " ", x))</span></pre><p id="a891" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">5.4删除标点符号</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="477a" class="ls lt iq nc b gy ng nh l ni nj"># '!"#$%&amp;'()*+,-./:;&lt;=&gt;?@[\]^_`{|}~' 32 punctuations in python string module<br/>text_col = text_col.apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))</span></pre><p id="0a50" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">5.5去掉双空格</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="a9a7" class="ls lt iq nc b gy ng nh l ni nj">text_col = text_col.apply(lambda x: re.sub(' +', ' ', x))</span></pre><p id="5068" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 6。替换音调符号(重音字符)</strong></p><p id="dc81" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">音调符号被替换为最接近的字符。如果找不到兼容的，默认情况下会在它的位置放一个空格。或者，如果指定了errors="preserve ",则字符将保持原样</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="bd45" class="ls lt iq nc b gy ng nh l ni nj">from unidecode import unidecode<br/>text_col = text_col.apply(lambda x: unidecode(x, errors="preserve"))</span></pre><p id="e5b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 7。错别字纠正</strong></p><p id="99d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它有助于在应用许多关键步骤之前纠正拼写，如停用词删除、词条化等。为此，我们将使用textblob和pyspellchecker。使用这些就像123一样简单</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="7b2f" class="ls lt iq nc b gy ng nh l ni nj">from textblob import TextBlob<br/>text_col = text_col.apply(lambda x: str(TextBlob(x).correct()))</span></pre><p id="1342" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 8。移除停用词</strong></p><p id="eafa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用NLTK来移除停用字词。NLTK停用词必须通过下载功能下载(与pip安装无关)。该下载功能在随后的时间内被忽略。这些文件在Linux上通常下载到/use/local/share/nltk_data或/ <user_home> /local/nltk_data的适当子文件夹中。在windows上一般下载到C:\ Users \<user_id>\ AppData \ Roaming \ nltk _ data。下载后，它们也会被解压缩</user_id></user_home></p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="5d4a" class="ls lt iq nc b gy ng nh l ni nj">import nltk<br/>nltk.download("stopwords")<br/>sw_nltk = stopwords.words('english')</span><span id="e630" class="ls lt iq nc b gy nk nh l ni nj"># stopwords customaization: Add custom stopwords<br/>new_stopwords = ['cowboy']<br/>sw_nltk.extend(new_stopwords)</span><span id="267d" class="ls lt iq nc b gy nk nh l ni nj"># stopwords customaization: Remove already existing stopwords<br/>sw_nltk.remove('not')</span><span id="5cf2" class="ls lt iq nc b gy nk nh l ni nj">text_col = text_col.apply(<br/>    lambda x: " ".join([ word for word in x.split() if word not in sw_nltk]) )</span></pre><p id="c3be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以看到停用字词资源必须下载一次。每个资源都有一个名称，并存储在子文件夹base nltk_data中。例如，特定于语言的停用词作为单独的文件存储在corpora子文件夹下。POS tagger资源存储在taggers子文件夹下，依此类推。本质上，每个资源都有一个资源名称和一个用于存储的子文件夹。我们只能用这个代码片段第一次自动下载</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="7ff2" class="ls lt iq nc b gy ng nh l ni nj">def download_if_non_existent(res_path, res_name):<br/>    try:<br/>        nltk.data.find(res_path)<br/>    except LookupError:<br/>        print(f'resource {res_name} not found in {res_path}")<br/>        print("Downloading now ...')<br/>        nltk.download(res_name)</span><span id="5a80" class="ls lt iq nc b gy nk nh l ni nj">download_if_non_existent('corpora/stopwords', 'stopwords')<br/>download_if_non_existent('taggers/averaged_perceptron_tagger', 'averaged_perceptron_tagger')<br/>download_if_non_existent('corpora/wordnet', 'wordnet')</span></pre><p id="cda6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当第一次查找某个资源时，它的缺失会触发一个错误，这个错误会被及时捕获并采取适当的措施。</p><p id="7c64" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 9。引理化</strong></p><p id="9095" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">词干化和词汇化都将word转换为其基本形式。词干是一种快速的基于规则的技术，有时会不准确地截断(词干不足和词干过多)。你可能没有注意到NLTK提供了PorterStemmer和稍微改进的雪球斯特梅尔。</p><p id="b725" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">词汇化是基于词典的技术，比词干法更精确，但稍慢。我们将使用NLTK的WordnetLemmatizer。为此，我们将下载wordnet资源。</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="cfa9" class="ls lt iq nc b gy ng nh l ni nj">import nltk<br/>nltk.download("wordnet")<br/>from nltk.stem import WordNetLemmatizer</span><span id="8797" class="ls lt iq nc b gy nk nh l ni nj">lemmatizer = WordNetLemmatizer()<br/>text_col = text_col.apply(lambda x: lemmatizer.lemmatize(x))</span></pre><p id="f360" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的代码是有效的。但是有一个小问题。WordnetLemmatizer假设每个单词都是名词。然而有时词性会改变引理。如果不考虑的话，我们会得到有趣的结果</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="f1d6" class="ls lt iq nc b gy ng nh l ni nj">lemmatizer.lemmatize("leaves") # outputs 'leaf'</span></pre><p id="cd9e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">“leaves”这个词在词性=名词时变成leaf，在词性=动词时变成leave。因此，我们必须为Lemmatizer指定词性。下面是我们的做法</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="dc20" class="ls lt iq nc b gy ng nh l ni nj">from nltk.corpus import wordnet<br/>lemmatizer.lemmatize("leaves", wordnet.VERB) # outputs 'leave'</span></pre><p id="3c68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">太好了！但是我们如何获得一个单词在句子中的词性呢？幸运的是，NLTK在pos_tag函数中内置了这个功能</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="4512" class="ls lt iq nc b gy ng nh l ni nj">nltk.pos_tag(["leaves"]) # outputs [('leaves', 'NNS')]</span></pre><p id="65f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">NLTK已经将POS输出为上面的名词。在NLTK行话中，NNS代表复数名词。然而，NLTK足够聪明，可以识别出这个单词在正确的上下文中作为句子的一部分出现时是动词，如下所示。</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="7748" class="ls lt iq nc b gy ng nh l ni nj">sentence = "He leaves for England"<br/>pos_list_of_tuples = nltk.pos_tag(nltk.word_tokenize(sentence))<br/>pos_list_of_tuples</span></pre><p id="3d73" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的代码输出适当的位置，如下所示。</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="0e23" class="ls lt iq nc b gy ng nh l ni nj">[('He', 'PRP'), ('leaves', 'VBZ'), ('for', 'IN'), ('England', 'NNP')]</span></pre><p id="15cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们可以制定一个策略来使lemmatize。我们首先使用NLTK pos_tag()函数来识别词性。然后，我们将POS作为参数显式提供给WordnetLemmatizer.lemmatize()函数。听起来不错。但是有一个小问题。</p><p id="11e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">NLTK pos标签由2-3个字母组成。例如，在NLTK中，NN、NNS、NNP分别代表单数、复数和专有名词，而在WordNet中，所有这些名词都用一个由变量nltk.corpus.wordnet.NOUN指定的总括词性“n”来表示</p><p id="84cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们通过创建一个查找字典来解决这个问题。一个查看来自NLTK的细粒度POS并映射到Wordnet的。实际上，查看NLTK POS的第一个字符就足以确定Wordnet的等价字符。我们的字典查出来的是这样一些东西:</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="a59c" class="ls lt iq nc b gy ng nh l ni nj">pos_tag_dict = {"J": wordnet.ADJ,<br/>                "N": wordnet.NOUN,<br/>                "V": wordnet.VERB,<br/>                "R": wordnet.ADV}</span></pre><p id="ec55" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们使用查找字典进行词汇化</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="3eba" class="ls lt iq nc b gy ng nh l ni nj">sentence = "He leaves for England"<br/>pos_list_of_tuples = nltk.pos_tag(nltk.word_tokenize(sentence))</span><span id="9a54" class="ls lt iq nc b gy nk nh l ni nj">new_sentence_words = []<br/>for word_idx, word in enumerate(nltk.word_tokenize(sentence)):<br/>    nltk_word_pos = pos_list_of_tuples[word_idx][1]<br/>    wordnet_word_pos = tag_dict.get(nltk_word_pos[0].upper(), None)<br/>    if wordnet_word_pos is not None:<br/>        new_word = lemmatizer.lemmatize(word, wordnet_word_pos)<br/>    else:<br/>        new_word = lemmatizer.lemmatize(word)<br/><br/>    new_sentence_words.append(new_word)</span><span id="bef8" class="ls lt iq nc b gy nk nh l ni nj">new_sentence = " ".join(new_sentence_words)<br/>print(new_sentence)</span></pre><p id="5c12" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您会注意到我们的pos标签查找字典非常简单。我们根本不提供到许多POS标签的映射。在这种情况下，如果键不存在，我们可能会得到一个KeyError。我们在dictionary上使用get方法，如果缺少pos键，则返回默认值(None)。然后，我们将相应的wordnet pos传递给lemmatize函数。最终输出将是:</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="f352" class="ls lt iq nc b gy ng nh l ni nj">He leave for England tomorrow</span></pre><p id="801c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那太好了。让我们试试另一个句子，比如“蝙蝠在夜间飞行”。惊喜惊喜！我们得到“<strong class="ky ir">蝙蝠</strong>正在夜间飞行”。显然，当单词以大写字母开头时，lemmatizer不会删除复数。如果我们将其小写为“bats”，那么我们将得到“bat”作为lemmatize过程的输出。有时需要这种功能。因为我们的例子很简单，所以在词汇化之前，让我们把每个句子都小写。</p><p id="23fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">关于引理化的一个注记</strong></p><p id="9e95" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">词汇化不是一个强制性的过程。我们可以使用这些经验法则来决定我们是否需要术语化</p><ol class=""><li id="037a" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated">TF-IDF、Word2Vec等简单的单词矢量化技术受益于词汇化。</li><li id="cb4c" class="ms mt iq ky b kz nl lc nm lf nn lj no ln np lr mx my mz na bi translated">主题建模受益于术语化</li><li id="d0dd" class="ms mt iq ky b kz nl lc nm lf nn lj no ln np lr mx my mz na bi translated">情感分析有时会受到词条泛化的影响，当然也会因为删除某些停用词而受到影响</li><li id="b091" class="ms mt iq ky b kz nl lc nm lf nn lj no ln np lr mx my mz na bi translated">经验表明，对句子进行词汇化会降低BERT等中预先训练的大型语言模型的准确性。</li></ol><h2 id="39e0" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">建议的执行顺序</h2><p id="a68d" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">预处理没有固定的顺序。但是对于许多简单的场景，这里有一个建议的执行顺序。这必须在语料库中的每个文档上完成。回想一下，一个语料库由许多文档组成。反过来，每个文档是一个或多个句子的集合。例如，df['tweets']是熊猫数据帧中的一列。每一行df['tweets']本身可以有很多句子。</p><ol class=""><li id="df73" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated">删除HTML标签</li><li id="2785" class="ms mt iq ky b kz nl lc nm lf nn lj no ln np lr mx my mz na bi translated">替换音调符号</li><li id="503e" class="ms mt iq ky b kz nl lc nm lf nn lj no ln np lr mx my mz na bi translated">扩大收缩</li><li id="5ddc" class="ms mt iq ky b kz nl lc nm lf nn lj no ln np lr mx my mz na bi translated">删除号码</li><li id="7607" class="ms mt iq ky b kz nl lc nm lf nn lj no ln np lr mx my mz na bi translated">错别字纠正</li><li id="a62a" class="ms mt iq ky b kz nl lc nm lf nn lj no ln np lr mx my mz na bi translated">复合词汇化过程</li><li id="36d8" class="ms mt iq ky b kz nl lc nm lf nn lj no ln np lr mx my mz na bi translated">停用词移除</li></ol><p id="2486" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">前&amp;后</strong>步骤的复合符号化过程</p><p id="336e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对每个文档中的每个句子执行以下步骤:</p><ol class=""><li id="c4b4" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated">删除除句点以外的特殊字符</li><li id="5aaf" class="ms mt iq ky b kz nl lc nm lf nn lj no ln np lr mx my mz na bi translated">小写字母盘</li><li id="a0a4" class="ms mt iq ky b kz nl lc nm lf nn lj no ln np lr mx my mz na bi translated">把…按屈折变化形式进行归类</li><li id="d5de" class="ms mt iq ky b kz nl lc nm lf nn lj no ln np lr mx my mz na bi translated">删除每个文档中的句点和双空格</li></ol><h2 id="f9d5" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">类中的完整代码</h2><p id="f09f" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">既然我们已经完整地介绍了所有部分，让我们把它们放在一起，为文本预处理创建一个单独的完整的类。</p><p id="74af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先是进口</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="6f42" class="ls lt iq nc b gy ng nh l ni nj">import string<br/>import re<br/>import contractions<br/><br/>import nltk<br/>from nltk.corpus import stopwords<br/>from nltk.tokenize import word_tokenize<br/>from nltk.corpus import wordnet<br/>from nltk.stem import WordNetLemmatizer<br/><br/>from bs4 import BeautifulSoup<br/>from textblob import TextBlob<br/>from unidecode import unidecode</span></pre><p id="3753" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，是对文本进行词汇化的独立函数</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="9e1d" class="ls lt iq nc b gy ng nh l ni nj">def lemmatize_pos_tagged_text(text, lemmatizer, post_tag_dict):<br/>  sentences = nltk.sent_tokenize(text)<br/>  new_sentences = []<br/><br/>  for sentence in sentences:<br/>    sentence = sentence.lower()<br/>    new_sentence_words = []</span><span id="2eec" class="ls lt iq nc b gy nk nh l ni nj">    #one pos_tuple for sentence<br/>    pos_tuples = nltk.pos_tag(nltk.word_tokenize(sentence)) <br/><br/>    for word_idx, word in enumerate(nltk.word_tokenize(sentence)):<br/>      nltk_word_pos = pos_tuples[word_idx][1]<br/>      wordnet_word_pos = pos_tag_dict.get(<br/>                          nltk_word_pos[0].upper(), None)<br/>      if wordnet_word_pos is not None:<br/>        new_word = lemmatizer.lemmatize(word, wordnet_word_pos)<br/>      else:<br/>        new_word = lemmatizer.lemmatize(word)<br/><br/>      new_sentence_words.append(new_word)<br/><br/>    new_sentence = " ".join(new_sentence_words)<br/>    new_sentences.append(new_sentence)<br/><br/>  return " ".join(new_sentences)</span></pre><p id="ec8d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后是一个做所有文本预处理的类</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="e08d" class="ls lt iq nc b gy ng nh l ni nj">def download_if_non_existent(res_path, res_name):<br/>  try:<br/>    nltk.data.find(res_path)<br/>  except LookupError:<br/>    print(f'resource {res_path} not found. Downloading now...')<br/>    nltk.download(res_name)</span><span id="221e" class="ls lt iq nc b gy nk nh l ni nj">class NltkPreprocessingSteps:<br/>  def __init__(self, X):<br/>    self.X = X<br/>    download_if_non_existent('corpora/stopwords', 'stopwords')<br/>    download_if_non_existent('tokenizers/punkt', 'punkt')<br/>    download_if_non_existent('taggers/averaged_perceptron_tagger',<br/>                             'averaged_perceptron_tagger')<br/>    download_if_non_existent('corpora/wordnet', 'wordnet')<br/>    download_if_non_existent('corpora/omw-1.4', 'omw-1.4')<br/><br/>    self.sw_nltk = stopwords.words('english')<br/>    new_stopwords = ['&lt;*&gt;']<br/>    self.sw_nltk.extend(new_stopwords)<br/>    self.sw_nltk.remove('not')<br/><br/>    self.pos_tag_dict = {"J": wordnet.ADJ,<br/>                    "N": wordnet.NOUN,<br/>                    "V": wordnet.VERB,<br/>                    "R": wordnet.ADV}<br/><br/>    # '!"#$%&amp;'()*+,-./:;&lt;=&gt;?@[\]^_`{|}~' 32 punctuations in python<br/>    # we dont want to replace . first time around<br/>    self.remove_punctuations = string.punctuation.replace('.','')<br/><br/>  def remove_html_tags(self):<br/>    self.X = self.X.apply(<br/>            lambda x: BeautifulSoup(x, 'html.parser').get_text())<br/>    return self<br/><br/>  def replace_diacritics(self):<br/>    self.X = self.X.apply(<br/>            lambda x: unidecode(x, errors="preserve"))<br/>    return self<br/><br/>  def to_lower(self):<br/>    self.X = np.apply_along_axis(lambda x: x.lower(), self.X)<br/>    return self<br/><br/>  def expand_contractions(self):<br/>    self.X = self.X.apply(<br/>            lambda x: " ".join([contractions.fix(expanded_word) <br/>                        for expanded_word in x.split()]))<br/>    return self<br/><br/>  def remove_numbers(self):<br/>    self.X = self.X.apply(lambda x: re.sub(r'\d+', '', x))<br/>    return self<br/><br/>  def replace_dots_with_spaces(self):<br/>    self.X = self.X.apply(lambda x: re.sub("[.]", " ", x))<br/>    return self<br/><br/>  def remove_punctuations_except_periods(self):<br/>    self.X = self.X.apply(<br/>                 lambda x: re.sub('[%s]' %<br/>                  re.escape(self.remove_punctuations), '' , x))<br/>    return self<br/><br/>  def remove_all_punctuations(self):<br/>    self.X = self.X.apply(lambda x: re.sub('[%s]' %<br/>                          re.escape(string.punctuation), '' , x))<br/>    return self<br/><br/>  def remove_double_spaces(self):<br/>    self.X = self.X.apply(lambda x: re.sub(' +', ' ', x))<br/>    return self<br/><br/>  def fix_typos(self):<br/>    self.X = self.X.apply(lambda x: str(TextBlob(x).correct()))<br/>    return self<br/><br/>  def remove_stopwords(self):<br/>    # remove stop words from token list in each column<br/>    self.X = self.X.apply(<br/>            lambda x: " ".join([ word for word in x.split() <br/>                     if word not in self.sw_nltk]) )<br/>    return self<br/><br/>  def lemmatize(self):<br/>    lemmatizer = WordNetLemmatizer()<br/>    self.X = self.X.apply(lambda x: lemmatize_pos_tagged_text(<br/>                           x, lemmatizer, self.post_tag_dict))<br/>    return self<br/><br/>  def get_processed_text(self):<br/>    return self.X</span></pre><p id="1e27" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的类只不过是以前编写的函数的集合。注意，每个方法都返回对self的引用。这是为了以流畅的方式实现方法的无缝链接。通过查看下面的用法，您将会很欣赏它:</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="5724" class="ls lt iq nc b gy ng nh l ni nj">txt_preproc = NltkPreprocessingSteps(df['tweets'])<br/>processed_text = \<br/>    txt_preproc \<br/>    .remove_html_tags()\<br/>    .replace_diacritics()\<br/>    .expand_contractions()\<br/>    .remove_numbers()\<br/>    .fix_typos()\<br/>    .remove_punctuations_except_periods()\<br/>    .lemmatize()\<br/>    .remove_double_spaces()\<br/>    .remove_all_punctuations()\<br/>    .remove_stopwords()\<br/>    .get_processed_text()</span></pre><h2 id="9a4c" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">将代码转换到sklearn转换器</h2><p id="8217" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">现在，我们已经看到了最常用的文本预处理步骤的代码片段，是时候将它们放入sklearn转换器中了。</p><p id="542e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">sklearn转换器旨在执行数据转换，无论是插补、操作还是其他处理，可选地(并且最好)作为复合ML管道框架的一部分，具有其熟悉的fit()、transform()和predict()生命周期范例，这是一种非常适合我们的文本预处理和精度生命周期的结构。但是在我们到达那里之前，一个快速的sklearn生命周期初级读本是合适的。开始了。</p><h2 id="9f76" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">世界上最短的sklearn生命周期入门</h2><p id="e558" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">在sklearn行话中，管道是一组连续的执行步骤。这些步骤可以属于两个类别之一——转换、ML预测。管道可以是纯转换管道或预测管道。</p><p id="3616" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个纯转换管道只有为顺序执行设置的转换器。预测管道可以在其末端包含可选的转换器和强制的单个预测器。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/0567a95b180deeb3218d902a9fe684e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*dpD9jnAilNxTAViVW1nQDw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图二。两种类型的管道(图片由作者提供)</p></figure><p id="a8fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个转换有三个主要操作——<em class="nr">拟合</em>、<em class="nr">转换</em>和<em class="nr">拟合_转换</em>。预测器拟合方法也有三个类似的操作——<em class="nr">拟合</em>、<em class="nr">预测</em>和<em class="nr">拟合_预测</em>。在这两种情况下，<em class="nr">适合</em>的方法就是学习发生的地方。学习是根据类属性来捕获的，这些类属性可以分别由变换器和预测器用于<em class="nr">变换</em>和<em class="nr">预测</em>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/be9f2953c2f43f4505d4ed6c30e69737.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*IchEjE-FEHXOm53sI7L8AA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3。转换器和预测器操作(图片由作者提供)</p></figure><p id="54fc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">典型的监督学习场景包括两个步骤——训练和推理。在训练阶段，用户在管道上调用fit()。在推断阶段，用户在管道上调用predict()。</p><blockquote class="nt nu nv"><p id="40cc" class="kw kx nr ky b kz la jr lb lc ld ju le nw lg lh li nx lk ll lm ny lo lp lq lr ij bi translated">sklearn在训练和推理阶段的两个最重要的翻译:</p><p id="61a6" class="kw kx nr ky b kz la jr lb lc ld ju le nw lg lh li nx lk ll lm ny lo lp lq lr ij bi translated">1.管道上的fit()调用被转换为管道中所有转换器组件上的顺序fit()和transform()调用，以及最终预测器组件(如果存在)上的单个fit()。</p><p id="a124" class="kw kx nr ky b kz la jr lb lc ld ju le nw lg lh li nx lk ll lm ny lo lp lq lr ij bi translated">2.predict()调用被转换为管道中所有转换器上的顺序transform()调用(当转换器存在时)，最后是管道中最后一个组件上的单个predict()，如果调用预测，该组件必须是预测器。</p></blockquote><p id="5875" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">呜哇！恭喜你不费吹灰之力走了这么远。我们已经准备好向成熟的文本预处理转换器迈出最后一步。它来了。</p><h2 id="55b2" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">使NLTK预处理步骤适应sklearn转换器API</h2><p id="363d" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">这最后一节将是小菜一碟，因为在前面关于sklearn生命周期的章节中有足够的关于sklearn生命周期的背景知识。我们现在要做的就是将NLTK预处理步骤包装成<em class="nr"> TransformerMixin </em>的sklearn的子类。</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="d72b" class="ls lt iq nc b gy ng nh l ni nj">from sklearn.base import BaseEstimator, TransformerMixin<br/>class NltkTextPreprocessor(TransformerMixin, BaseEstimator):<br/>  def __init__(self):<br/>    pass<br/><br/>  def fit(self, X):<br/>    return self<br/><br/>  def transform(self, X):<br/>    txt_preproc = NltkPreprocessingSteps(X.copy())<br/>    processed_text = \<br/>            txt_preproc \<br/>            .remove_html_tags()\<br/>            .replace_diacritics()\<br/>            .expand_contractions()\<br/>            .remove_numbers()\<br/>            .fix_typos()\<br/>            .remove_punctuations_except_periods()\<br/>            .lemmatize()\<br/>            .remove_double_spaces()\<br/>            .remove_all_punctuations()\<br/>            .remove_stopwords()\<br/>            .get_processed_text()<br/><br/>    return processed_text</span></pre><p id="540b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，您可以用熟悉的标准方式在任何sklearn管道中定制NLTK预处理转换器。下面是它在纯转换管道和预测管道中的两个用法示例</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="f671" class="ls lt iq nc b gy ng nh l ni nj">from sklearn.model_selection import train_test_split<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.feature_extraction.text import TfidfTransformer<br/>from sklearn.naive_bayes import BernoulliNB</span><span id="a45d" class="ls lt iq nc b gy nk nh l ni nj">X = pd.read_csv("....")</span><span id="2799" class="ls lt iq nc b gy nk nh l ni nj">X_train, X_test, y_train, y_test = train_test_split(X['tweets'], y, random_state=0)</span><span id="dafc" class="ls lt iq nc b gy nk nh l ni nj">pure_transformation_pipeline = Pipeline(steps=[<br/>           ('text_preproc', NltkTextPreprocessor()), <br/>           <!-- -->('tfidf', TfidfTransformer())<!-- -->])<br/>pure_transformation_pipeline.fit(X_train)</span><span id="2fe1" class="ls lt iq nc b gy nk nh l ni nj"># Call fit_transform if we only wanted to get transformed data<br/>tfidf_data = pure_transformation_pipeline.fit_transform(X_train)</span><span id="712d" class="ls lt iq nc b gy nk nh l ni nj">prediction_pipeline = Pipeline(steps=[<br/>           ('text_preproc', NltkTextPreprocessor()), <br/>           <!-- -->('tfidf', TfidfTransformer()),<br/>           ('bernoulli', BernoulliNB())<!-- -->])<br/>prediction_pipeline.fit(X_train)</span><span id="f1b4" class="ls lt iq nc b gy nk nh l ni nj">y_pred = prediction_pipeline.predict(X_test)</span></pre><p id="1e88" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样，伙计们！我们的sklearn集成已经完成。现在我们可以获得管道的全部好处——交叉验证、防止数据泄露、代码重用等等。</p><p id="de19" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们开始以通常的方式处理基于NLTK的文本预处理——通过编写通常在Jupyter Notebook单元中执行的小代码片段，将它们重构为函数，过渡到可重用的类，并使我们的代码适应行业标准的可重用API和组件。</p><p id="85cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为什么我们要做这一切？原因其实很简单；由数据科学家编写的代码终究是代码，并且在维护、修订生命周期中伴随着价格标签(拥有成本)。引用马丁·福勒的话——“任何人都可以编写计算机能够理解的代码。优秀的程序员编写人类能够理解的代码”。具有标准使用模式的好代码使数据科学家(人类)容易理解和维护ML管道，从而降低拥有成本。有了适用于我们代码的标准API适配器，即使没有ML背景的软件工程师也可以轻松地使用它，推动了我们刚刚编写的基于NLTK的文本预处理代码在公司范围内的采用，更不用说ROI了。欢迎来到架构优雅的文本预处理世界。</p></div></div>    
</body>
</html>