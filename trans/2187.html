<html>
<head>
<title>Introduction to Autoencoders</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自动编码器简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-autoencoders-b6fc3141f072#2022-05-16">https://towardsdatascience.com/introduction-to-autoencoders-b6fc3141f072#2022-05-16</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="f8ca" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/tag/autoencoder" rel="noopener">自动编码器</a></h2><div class=""/><div class=""><h2 id="5069" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">如何使用Tensorflow简化您的数据</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/71a3705da36bbcccb7f02cbab87e3fb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uHa6X3ieK7AoglYr"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">由<a class="ae li" href="https://unsplash.com/@timstief?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">蒂姆·斯蒂夫</a>在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="6d52" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi mf translated"><span class="l mg mh mi bm mj mk ml mm mn di">我</span>想象你有一个数据集(<em class="mo"> X </em>，<em class="mo"> y </em>)，其中包含了<strong class="ll je">大量的特征</strong>，你想用它们来解决一个任务，比如回归或分类。虽然拥有更多的<strong class="ll je">观察值</strong>总是更好，但是在一个模型中包含太多的<strong class="ll je">特性</strong>可能会适得其反，无论是对于可解释性还是甚至是模型的性能。考虑这个简单的例子:</p><pre class="kt ku kv kw gu mp mq mr bn ms mt bi"><span id="9161" class="mu mv iu mq b be mw mx l my mz">import numpy as np<br/><br/>np.random.seed(0)<br/><br/>X = np.random.randn(1000, 900)<br/>y = 2*X[:, 0] + 1 + 0.1*np.random.randn(1000)</span></pre><p id="57f1" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在这里，我们创建了一个数据集(<em class="mo"> X </em>，<em class="mo"> y </em>)，包含1000个样本，每个样本有900个特征<em class="mo"> x </em> ₀，…，<em class="mo"> x </em> ₈₉₉。我们想知道的事实是<em class="mo"> y </em> = 2 <em class="mo"> x </em> ₀ + 1，由于误差也很小，线性回归应该完全达到接近1的<em class="mo"> r </em>。然而，进行交叉验证会产生</p><pre class="kt ku kv kw gu mp mq mr bn ms mt bi"><span id="5681" class="mu mv iu mq b be mw mx l my mz">from sklearn.linear_model import LinearRegression<br/>from sklearn.model_selection import cross_val_score<br/><br/>print(cross_val_score(LinearRegression(), X, y))<br/><br/># Output:<br/># [0.88558154 0.87775961 0.87564832 0.86230888 0.8796105]</span></pre><p id="a1dc" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">看不见的数据上的<em class="mo"> r </em>约为0.87，离1相当远。当将模型限制为仅使用前10个变量时，通过进行相同的分析，我们可以更清楚地看到这有多糟糕:</p><pre class="kt ku kv kw gu mp mq mr bn ms mt bi"><span id="1fea" class="mu mv iu mq b be mw mx l my mz">print(cross_val_score(LinearRegression(), X[:, :10], y))<br/><br/># Output:<br/># [0.99701015 0.99746395 0.99780414 0.99752536 0.99745693]</span></pre><p id="db23" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们可以看到，丢弃特征将性能提高到大约0.997。这意味着即使像线性回归这样简单的算法也可能淹没在特性中。</p></div><div class="ab cl na nb hy nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="in io ip iq ir"><p id="21fe" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">有几种技术可以减少特性的数量，比较流行的有</p><ul class=""><li id="5a05" class="nh ni iu ll b lm ln lp lq ls nj lw nk ma nl me nm nn no np bi translated">仅仅根据诸如单变量分析(即与目标<em class="mo"> y </em>的相关性)或重要性分数(<em class="mo">p</em>-值、基于树的算法的特征重要性、Shapley值等)的规则来丢弃一些特征，以及</li><li id="6ca5" class="nh ni iu ll b lm nq lp nr ls ns lw nt ma nu me nm nn no np bi translated"><a class="ae li" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">主成分分析</a> (PCA)，使用特殊的线性映射将数据转换到低维空间(即，将数据矩阵<em class="mo"> X </em>乘以另一个矩阵<em class="mo"> W </em>)。注意这里我们<strong class="ll je">不需要<em class="mo">y</em>T49】。</strong></li></ul><p id="b844" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在本文中，我们将了解到<em class="mo">类</em> <strong class="ll je">对PCA </strong>的另一种推广方法:<a class="ae li" href="https://en.wikipedia.org/wiki/Kernel_principal_component_analysis" rel="noopener ugc nofollow" target="_blank">内核PCA </a>。</p></div><div class="ab cl na nb hy nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="in io ip iq ir"><p id="15c5" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">开玩笑，我指的当然是<strong class="ll je">自动编码器</strong>！<em class="mo">(不过内核PCA是一个有效的选择。)</em></p><h1 id="cbbd" class="nv mv iu bd nw nx ny nz oa ob oc od oe kj of kk og km oh kn oi kp oj kq ok ol bi translated">自动编码器</h1><p id="adc9" class="pw-post-body-paragraph lj lk iu ll b lm om ke lo lp on kh lr ls oo lu lv lw op ly lz ma oq mc md me in bi translated">那么，这些神秘的自动编码器是如何工作的，为什么我们首先要关心呢？让我们来找出答案。</p><h2 id="e65a" class="or mv iu bd nw os ot dn oa ou ov dp oe ls ow ox og lw oy oz oi ma pa pb ok ja bi translated">直觉</h2><p id="6fa0" class="pw-post-body-paragraph lj lk iu ll b lm om ke lo lp on kh lr ls oo lu lv lw op ly lz ma oq mc md me in bi translated">通常，手边的数据包含了T4冗余，可以很容易地压缩而不会丢失太多信息。想象一个数据集，它具有以厘米为单位的<strong class="ll je">高度和以米为单位的</strong>高度。这两个要素包含相同的信息，但是如果它们都有不同种类的噪声，可能会影响模型性能。所以，还是选这两个中的一个比较好。</p><p id="a29f" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">虽然这种情况很明显，但自动编码器可以找到更细微的冗余，并将其压缩掉。我们希望的是，自动编码器中间所谓的<strong class="ll je">信息瓶颈</strong>(潜在空间，见下文)不会过滤掉必要的信息，而只是冗余信息。一个简单的检查是通过查看所有的<em class="mo"> x </em>和<em class="mo"> x </em>之间的<strong class="ll je">重建误差</strong>。一会儿就变得更具体了。</p><h2 id="540d" class="or mv iu bd nw os ot dn oa ou ov dp oe ls ow ox og lw oy oz oi ma pa pb ok ja bi translated">定义</h2><p id="ff11" class="pw-post-body-paragraph lj lk iu ll b lm om ke lo lp on kh lr ls oo lu lv lw op ly lz ma oq mc md me in bi translated">对于什么是自动编码器，没有一个真正的数学定义，至少我找不到。不过，我可以给你一个足够好的直觉。</p><p id="8698" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">自动编码器由<strong class="ll je">两个功能<em class="mo"> e </em>(编码器的简称)和<em class="mo"> d </em>(解码器的简称)</strong>组成，其行为如下:</p><ol class=""><li id="c306" class="nh ni iu ll b lm ln lp lq ls nj lw nk ma nl me pc nn no np bi translated"><em class="mo"> e </em>将一个具有<em class="mo"> n </em>特征的观测值<em class="mo"> x </em>作为输入，并将其映射到一个维度为<em class="mo"> m </em> ≤ <em class="mo"> n </em>的<strong class="ll je">低维</strong>向量<em class="mo"> z </em>，称为<em class="mo"> x </em>的<em class="mo">潜在向量</em>。</li><li id="bad0" class="nh ni iu ll b lm nq lp nr ls ns lw nt ma nu me pc nn no np bi translated"><em class="mo"> d </em>取潜在向量<em class="mo"> z </em>并再次输出一个尺寸为<em class="mo"> n </em>的<em class="mo">x</em>’。</li></ol><p id="6a44" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">另外，我们希望<strong class="ll je"> <em class="mo"> x </em>和<em class="mo"> x </em>接近，即<em class="mo">x</em>≈<em class="mo">x</em>’，</strong>即<strong class="ll je">重建误差应该很小</strong>。<strong class="ll je"> </strong>用通俗的语言来说:</p><blockquote class="pd"><p id="5f17" class="pe pf iu bd pg ph pi pj pk pl pm me dk translated">编码器压缩原始观察值x <em class="pn">。解码器再次解包。这个打包和拆包的过程不要太扭曲x(平均)。</em></p></blockquote><figure class="pp pq pr ps pt kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj po"><img src="../Images/8297e1fd1722d7ba87c9ef5a7553ae0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i5orBqdUvImHl8GNJZ2V4w.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="f56f" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">像zip和rar这样的压缩工具也试图达到同样的效果，但是有一个更严格的条件:x = x’。 PNG是另一种<em class="mo">无损</em>格式，虽然jpeg或mp3等格式并非无损，但<em class="mo">足够好</em>让人类看不出区别，至少如果压缩不是太激进，即<em class="mo"> m </em>不是太小。</p><p id="7790" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在我们转向通用自动编码器之前，让我们重复一下PCA在高层次上做什么——以及它失败的时候。</p><h2 id="64d6" class="or mv iu bd nw os ot dn oa ou ov dp oe ls ow ox og lw oy oz oi ma pa pb ok ja bi translated">何必呢？</h2><p id="e64b" class="pw-post-body-paragraph lj lk iu ll b lm om ke lo lp on kh lr ls oo lu lv lw op ly lz ma oq mc md me in bi translated">好的，我们知道拥有太多的特性<em class="mo">可能是一件消极的事情。我们还谈到了PCA能够减少特征的数量。然而，PCA——作为线性变换——具有有限的表达能力。提醒一下PCA是如何工作的:</em></p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pu"><img src="../Images/909fe43844784c8ebc2cc611cfe945f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eFHbACcAVjCWgLoMKSLqQA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><blockquote class="pv pw px"><p id="c101" class="lj lk mo ll b lm ln ke lo lp lq kh lr py lt lu lv pz lx ly lz qa mb mc md me in bi translated"><em class="iu">在autoencoders的语言中，</em> <strong class="ll je"> <em class="iu">左下角的图像显示编码</em> </strong> <em class="iu">(它只是一维而不是二维)，右下角的</em> <strong class="ll je"> <em class="iu">图像显示解码</em> </strong> <em class="iu">(回到原来的二维)。两个顶部图像都包含原始输入数据。</em></p></blockquote><p id="22e2" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">如果数据接近于在一个维度为<em class="mo"> m的<em class="mo">线性子空间</em>(即直线、平面、超平面)<em class="mo"> </em>中，具有<em class="mo"> m </em>分量的</em> PCA工作得很好。但是，如果不这样做，我们可能会丢失许多关于原始数据的信息，正如我们在这里看到的:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pu"><img src="../Images/1524966fc782a0321752ccf8bf7d8830.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pm2Hs91awOm9K4LaO0lgew.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="3ee4" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">好吧，如果线性变换的简单性是瓶颈，让我们使用更复杂的编码器和解码器！实现这一点的一个简单方法是使用神经网络，所以让我们开始吧。</p><blockquote class="pv pw px"><p id="a7de" class="lj lk mo ll b lm ln ke lo lp lq kh lr py lt lu lv pz lx ly lz qa mb mc md me in bi translated"><strong class="ll je"> <em class="iu">注:</em> </strong> <em class="iu">通常，当人们谈论自动编码器时，他们指的是</em> <strong class="ll je"> <em class="iu">基于神经网络的自动编码器</em> </strong> <em class="iu">。但是这并不是唯一的选择:只要你有一个解码器和一个编码器，一切都是允许的。例如，你可以根据下面的决策树来检查一个奇特的自动编码器。然而，在本文中，我将坚持使用传统的<em class="iu">自动编码器。</em></em></p></blockquote><div class="qb qc gq gs qd qe"><a rel="noopener follow" target="_blank" href="/building-a-simple-auto-encoder-via-decision-trees-28ba9342a349"><div class="qf ab fp"><div class="qg ab qh cl cj qi"><h2 class="bd je gz z fq qj fs ft qk fv fx jd bi translated">通过决策树构建一个简单的自动编码器</h2><div class="ql l"><h3 class="bd b gz z fq qj fs ft qk fv fx dk translated">如何使用随机决策树构建自动编码器</h3></div><div class="qm l"><p class="bd b dl z fq qj fs ft qk fv fx dk translated">towardsdatascience.com</p></div></div><div class="qn l"><div class="qo l qp qq qr qn qs lc qe"/></div></div></a></div><h1 id="294d" class="nv mv iu bd nw nx ny nz oa ob oc od oe kj of kk og km oh kn oi kp oj kq ok ol bi translated">Tensorflow中的实现</h1><p id="56d0" class="pw-post-body-paragraph lj lk iu ll b lm om ke lo lp on kh lr ls oo lu lv lw op ly lz ma oq mc md me in bi translated">编码时间到了！首先，让我们做一些基本的事情，用以下方式定义一个具有4个特征的数据集<em class="mo"> X </em>:</p><pre class="kt ku kv kw gu mp mq mr bn ms mt bi"><span id="c325" class="mu mv iu mq b be mw mx l my mz">import tensorflow as tf<br/><br/>tf.random.set_seed(0) # keep things reproducible<br/><br/>Z = tf.random.uniform(shape=(10000, 1), minval=-5, maxval=5)<br/>X = tf.concat([Z**2, Z**3, tf.math.sin(Z), tf.math.exp(Z)], 1)</span></pre><p id="39ad" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">如果你想一想，你能如何压缩<em class="mo"> X </em>的维度？用更少的特征重构<em class="mo"> X </em>有什么必要？</p></div><div class="ab cl na nb hy nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="in io ip iq ir"><p id="9e8e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">嗯，我们只需要<em class="mo"> Z </em>，即<strong class="ll je">一个单一特征</strong>。<em class="mo"> X </em>并不包含比<em class="mo"> Z </em>更多的信息，我们只需对<em class="mo"> Z </em>应用某种确定性函数就可以得到<em class="mo"> X </em>的特征。</p><p id="ae77" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">编码器可以计算出如何找到函数的<em class="mo">逆</em>，即从<em class="mo"> x </em> = ( <em class="mo"> z </em>，<em class="mo"> z </em>，<em class="mo"> sin </em> ( <em class="mo"> z </em>)，<em class="mo"> exp </em> ( <em class="mo"> z </em>))到<em class="mo">z</em>，解码器可以学习取一个数<em class="mo"> z </em>并将其转化为</p><p id="ed3e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">你有没有注意到我在这里是如何使用<strong class="ll je"><em class="mo"/></strong>这个词的？对我们来说，这将是显而易见的解决方案，因为我们知道如何从<em class="mo"> Z </em>生成<em class="mo"> X </em>。然而，自动编码器不知道这一点，可能会找到另一种方法来压缩数据。比如它可以想办法把<em class="mo"> x </em> = ( <em class="mo"> z </em>，<em class="mo"> z </em>，<em class="mo"> sin </em> ( <em class="mo"> z </em>)，<em class="mo"> exp </em> ( <em class="mo"> z </em>))压缩成<em class="mo"> z - </em> 3，解码器可以重新学习解码这个，一样好。</p></div><div class="ab cl na nb hy nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="in io ip iq ir"><p id="de28" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">好的，所以我们应该能够建立一个只有一个潜在维度的性能良好的自动编码器。我们可以通过以下方式做到这一点</p><pre class="kt ku kv kw gu mp mq mr bn ms mt bi"><span id="5d73" class="mu mv iu mq b be mw mx l my mz">class AutoEncoder(tf.keras.Model):<br/>    def __init__(self):<br/>        super().__init__()<br/>        <br/>        self.encoder = tf.keras.Sequential([<br/>            tf.keras.layers.Dense(100, activation='relu'),<br/>            tf.keras.layers.Dense(100, activation='relu'),<br/>            tf.keras.layers.Dense(100, activation='relu'),<br/>            tf.keras.layers.Dense(1), # compress into 1 dimension<br/>        ])<br/>        self.decoder = tf.keras.Sequential([<br/>            tf.keras.layers.Dense(100, activation='relu'),<br/>            tf.keras.layers.Dense(100, activation='relu'),<br/>            tf.keras.layers.Dense(100, activation='relu'),<br/>            tf.keras.layers.Dense(4), # unpack into 4 dimensions<br/>        ])<br/>    def call(self, x):<br/>        latent_x = self.encoder(x) # compress<br/>        decoded_x = self.decoder(latent_x) # unpack<br/>        return decoded_x<br/><br/>tf.random.set_seed(0) # keep things reproducible<br/><br/>ae = AutoEncoder()<br/><br/>ae.compile(<br/>    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),<br/>    loss='mse'<br/>)<br/><br/>ae.fit(<br/>    X, # input<br/>    X, # equals output<br/>    validation_split=0.2, # prevent overfitting<br/>    epochs=1000,<br/>    callbacks=[<br/>        tf.keras.callbacks.EarlyStopping(patience=10) # early stop<br/>    ]<br/>)<br/><br/># Output:<br/># [...]<br/># Epoch 69/1000<br/># 250/250 [==============================] - 0s 1ms/step - loss:          # 0.0150 - val_loss: 0.0276</span></pre><p id="6211" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">看起来不错，模型可以学习底层结构！我们还可以通过以下方式检查自动编码器产生的最大误差</p><pre class="kt ku kv kw gu mp mq mr bn ms mt bi"><span id="9031" class="mu mv iu mq b be mw mx l my mz">print(tf.reduce_max(tf.math.abs(X-ae(X))))<br/><br/># Output:<br/># tf.Tensor(0.89660645, shape=(), dtype=float32)</span></pre><p id="f5b0" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">不错，考虑到有些功能要上百个。现在我们知道了自动编码器的工作原理，让我们做一些更有趣的事情。</p><h2 id="2c31" class="or mv iu bd nw os ot dn oa ou ov dp oe ls ow ox og lw oy oz oi ma pa pb ok ja bi translated">压缩数字</h2><p id="2cfc" class="pw-post-body-paragraph lj lk iu ll b lm om ke lo lp on kh lr ls oo lu lv lw op ly lz ma oq mc md me in bi translated">自动编码器的一个经典例子是使用手写数字的MNIST数据集。让我们通过以下方式获取数据集</p><pre class="kt ku kv kw gu mp mq mr bn ms mt bi"><span id="e58d" class="mu mv iu mq b be mw mx l my mz">(X_train, _), (X_test, _) = tf.keras.datasets.mnist.load_data()<br/>X_train = X_train.reshape(-1, 28, 28, 1) / 255. # value range=[0,1]<br/>X_test = X_test.reshape(-1, 28, 28, 1) / 255.   # shape=(28, 28, 1)</span></pre><p id="1297" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">画几个数字:</p><pre class="kt ku kv kw gu mp mq mr bn ms mt bi"><span id="4980" class="mu mv iu mq b be mw mx l my mz">import matplotlib.pyplot as plt<br/><br/>for i in range(3):<br/>    plt.imshow(X_train[i])<br/>    plt.show()</span></pre><div class="kt ku kv kw gu ab cb"><figure class="qt kx qu qv qw qx qy paragraph-image"><img src="../Images/65b82717fdcc604ea60b2ef94c58f9d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*NrwSEeEP18rFrlH0W5-tzA.png"/></figure><figure class="qt kx qu qv qw qx qy paragraph-image"><img src="../Images/6126516f0f5b67738a19de1b43e1ab30.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*kmvUPr3MVKvANdnx48tGhg.png"/></figure><figure class="qt kx qu qv qw qx qy paragraph-image"><img src="../Images/a9078a0afb6a7bc202efa469804ccd16.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*mvZGUCMax4sc25T2nWettw.png"/><p class="le lf gk gi gj lg lh bd b be z dk qz di ra rb translated">MNIST数据集中的数字。图片由作者提供。</p></figure></div><p id="9e80" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">正如我们所看到的，数据集由手写数字组成，确切地说是28x28像素。由于每个像素都是一个特征，所以我们面对的是一个28*28= <strong class="ll je"> 784维的</strong>数据集，对于一个图像数据集来说，这甚至<em class="mo">非常低</em>。</p><p id="6856" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们现在将为这个数据集构建一个自动编码器。但是我们为什么要成功呢？答案是手写数字是所有潜在28x28像素图片的<strong class="ll je">非常小的子集</strong>。它们有非常特殊的像素模式。举例来说，尽管以下图像是28x28像素的图像，但它们绝不会被视为数字:</p><div class="kt ku kv kw gu ab cb"><figure class="qt kx qu qv qw qx qy paragraph-image"><img src="../Images/7a97a2d9ff9eb6dd2f5a3c0aa29bd8ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*9TlkrpAv4Aw6xLLXecZF4g.png"/></figure><figure class="qt kx qu qv qw qx qy paragraph-image"><img src="../Images/27df9018cbfc5949e047b66402a2298f.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*w1Sv9VU2NHIols9zdPRAcQ.png"/></figure><figure class="qt kx qu qv qw qx qy paragraph-image"><img src="../Images/b022f0f23432d648215be7571978ac19.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*MO7Quf0gHeLLk2G6bnKBHg.png"/><p class="le lf gk gi gj lg lh bd b be z dk qz di ra rb translated">不是数字。图片由作者提供。</p></figure></div><p id="65f3" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">但是我们需要多少个维度来表达MNIST数据集呢？不幸的是，这个问题没有通用的答案。通常，我们试图使维度尽可能小，以使重建(解码)足够好。</p></div><div class="ab cl na nb hy nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="in io ip iq ir"><p id="7c1c" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们现在可以再次构建另一个简单的前馈自动编码器，我鼓励您这样做。我们要做的是使用<strong class="ll je">卷积层</strong>建立一个自动编码器，因为我们在这里处理图像数据。这些类型的自动编码器被方便地称为<strong class="ll je">卷积自动编码器</strong>。开门见山，对吧？</p><p id="1666" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">注意，图像的像素值在0和1之间，用于更快和稳定的训练，并且单个图像的形状为(<em class="mo">高</em> = 28，<em class="mo">宽</em> = 28，<em class="mo">通道</em> = 1)，用于卷积层工作。</p><pre class="kt ku kv kw gu mp mq mr bn ms mt bi"><span id="128d" class="mu mv iu mq b be mw mx l my mz">from tensorflow.keras import layers<br/><br/>class ConvolutionalAutoEncoder(tf.keras.Model):<br/>    def __init__(self):<br/>        super().__init__()<br/>        <br/>        self.encoder = tf.keras.Sequential([<br/>            layers.Conv2D(4, 5, activation='relu'),<br/>            layers.Conv2D(4, 5, activation='relu'),<br/>            layers.Conv2D(1, 5, activation='relu'),<br/>        ])<br/>        self.decoder = tf.keras.Sequential([<br/>            layers.Conv2DTranspose(4, 5, activation='relu'),<br/>            layers.Conv2DTranspose(4, 5, activation='relu'),<br/>            layers.Conv2DTranspose(1, 5, activation='sigmoid'),<br/>        ])<br/>    def call(self, x):<br/>        latent_x = self.encoder(x)<br/>        decoded_x = self.decoder(latent_x)<br/>        return decoded_x<br/><br/>tf.random.set_seed(0)<br/><br/>ae = ConvolutionalAutoEncoder()<br/><br/>ae.compile(optimizer='adam', loss='bce')<br/><br/>ae.fit(<br/>    X_train, <br/>    X_train,<br/>    batch_size=128,<br/>    epochs=1000,<br/>    validation_data=(X_test, X_test),<br/>    callbacks=[<br/>        tf.keras.callbacks.EarlyStopping(patience=1)<br/>    ]<br/>)<br/><br/># Output:<br/># [...]<br/># Epoch 12/1000<br/># 469/469 [==============================] - 91s 193ms/step - loss: # 0.0815 - val_loss: 0.0813</span></pre><p id="03a5" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们可以通过它来查看潜在空间有多大</p><pre class="kt ku kv kw gu mp mq mr bn ms mt bi"><span id="f256" class="mu mv iu mq b be mw mx l my mz">print(ae.summary())</span></pre><p id="cd7a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们看到第一层(即编码器)的输出形状为(None，16，16，1 ),这意味着输出是16 * 16 * 1 = 256维，也可以解释为单通道的16x16像素图像。</p></div><div class="ab cl na nb hy nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="in io ip iq ir"><p id="0d06" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在我们继续之前，我们还可以手动检查一些图像重建:</p><pre class="kt ku kv kw gu mp mq mr bn ms mt bi"><span id="aae0" class="mu mv iu mq b be mw mx l my mz">n = 10<br/><br/>decoded_images = ae.predict(X_test[:n])<br/>latent_images = ae.encoder(X_test[:n])<br/><br/>plt.figure(figsize=(20, 4))<br/><br/>for i in range(n):<br/>    ax = plt.subplot(3, n, i + 1)<br/>    plt.imshow(X_test[i])<br/>    ax.get_xaxis().set_visible(False)<br/>    ax.get_yaxis().set_visible(False)<br/>    <br/>    ax = plt.subplot(3, n, i + 1 + n)<br/>    plt.imshow(latent_images[i] / tf.reduce_max(latent_images[i]))<br/>    ax.get_xaxis().set_visible(False)<br/>    ax.get_yaxis().set_visible(False)<br/>    ax = plt.subplot(3, n, i + 1 + 2*n)<br/>    plt.imshow(decoded_images[i])<br/>    ax.get_xaxis().set_visible(False)<br/>    ax.get_yaxis().set_visible(False)<br/>    <br/>plt.show()</span></pre><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj rc"><img src="../Images/56d48f8ba4f48919ae436cfb7ef850f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yg_9Y4BhWpvIoYnWulOcqg.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">顶行:原始，中行:压缩，底行:重建。图片由作者提供。</p></figure><p id="af51" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">整体看起来还过得去！它到处都有一些问题，例如2，但建立一个更好的网络架构可能会使它变得更好。我们还可以看到，原始图像的潜在版本仍然非常类似于原始数字，只是分辨率更差，因为它是16x16而不是28x28像素。虽然情况并不总是这样，但是在这里，autoencoder碰巧发现这是一个很好的压缩。</p><p id="86ef" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我发现有趣的是，自动编码器了解到图像的边界不包含有用的信息。它只是放大数字，让它们填充完整的图像，这很有意义——我可能会像人类一样做同样的事情。然而，和神经网络一样，它不一定是这样的。在这里，这只是一个很好的小巧合。</p><h2 id="ca19" class="or mv iu bd nw os ot dn oa ou ov dp oe ls ow ox og lw oy oz oi ma pa pb ok ja bi translated">更多架构</h2><p id="cf90" class="pw-post-body-paragraph lj lk iu ll b lm om ke lo lp on kh lr ls oo lu lv lw op ly lz ma oq mc md me in bi translated">我们现在已经看到了普通的自动编码器，以及卷积自动编码器。你可能已经猜到，通过改变自动编码器的结构，我们也可以创建<strong class="ll je"> LSTM自动编码器或序列到序列自动编码器</strong>、<strong class="ll je">变压器自动编码器</strong>等等。</p><p id="55bd" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">还有<strong class="ll je">稀疏自动编码器</strong>，如果编码器一次使用太多的潜在维度，它会惩罚编码器。您可以通过<code class="fe rd re rf mq b">activity_regularizer</code>关键字将活动正则化添加到图层中。</p><p id="6b5f" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">尽管主要思想总是相同的:获取一个输入并使其变小。然后尽你所能把它放大一倍，基本上就像每个犯罪系列里一样(这是真的！):</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj rg"><img src="../Images/6ca58312986538e2b338ec84c1654b28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/1*85dy4ubnWcina-uux-P3qw.gif"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由<a class="ae li" href="https://imgur.com/user/trustno2" rel="noopener ugc nofollow" target="_blank">信任号</a>在<a class="ae li" href="https://imgur.com/gallery/zVq9W7u" rel="noopener ugc nofollow" target="_blank">https://imgur.com/gallery/zVq9W7u</a>上。</p></figure></div><div class="ab cl na nb hy nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="in io ip iq ir"><p id="567e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">另一种值得注意的我非常喜欢的自动编码器是<strong class="ll je">变体自动编码器。</strong>它不仅能够重建数据，甚至能够<strong class="ll je">创建新数据</strong>，这使其成为<strong class="ll je">创成式模型</strong>。使用它们，您可以创建催眠图片，如</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj rh"><img src="../Images/74c5d0f4d77d710754373159a0c2856f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*GOHZCt0vYIDciBNv8dkDoQ.jpeg"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated"><a class="ae li" href="https://www.tensorflow.org/tutorials/generative/cvae" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tutorials/generative/cvae</a></p></figure><h1 id="6fcb" class="nv mv iu bd nw nx ny nz oa ob oc od oe kj of kk og km oh kn oi kp oj kq ok ol bi translated">结论</h1><p id="438b" class="pw-post-body-paragraph lj lk iu ll b lm om ke lo lp on kh lr ls oo lu lv lw op ly lz ma oq mc md me in bi translated">我们已经看到，数据可能带有大量冗余。这增加了处理所述数据所需的内存和计算能力，甚至可能降低模型的性能。通常，我们可以通过删除一些功能或使用PCA等方法来摆脱困境。</p><p id="767d" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">然而，这些方法可能过于简单，无法从数据中恰当地提取相关信息——它们存在不足。为了解决这个问题，我们可以使用自动编码器，它可以按照我们需要的复杂程度来转换数据。大多数时候，我们使用神经网络构建自动编码器，但也存在其他方法，例如使用决策树的<a class="ae li" rel="noopener" target="_blank" href="/building-a-simple-auto-encoder-via-decision-trees-28ba9342a349"/>。</p><p id="4a34" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">然后，我们看到了如何用Tensorflow用几行代码构建一个普通的卷积自动编码器。您还看到了更多的自动编码器类型，这取决于您如何定义您的编码器和解码器。</p><p id="7813" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在，您可以开始自己探索自动编码器，使用它们，看看您是否能从它们中受益，解决您的问题。玩得开心！</p></div><div class="ab cl na nb hy nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="in io ip iq ir"><p id="b7de" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我希望你今天学到了新的、有趣的、有用的东西。感谢阅读！</p><p id="1b35" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je">作为最后一点，如果你</strong></p><ol class=""><li id="3538" class="nh ni iu ll b lm ln lp lq ls nj lw nk ma nl me pc nn no np bi translated"><strong class="ll je">想支持我多写点机器学习和</strong></li><li id="d023" class="nh ni iu ll b lm nq lp nr ls ns lw nt ma nu me pc nn no np bi translated"><strong class="ll je">无论如何都要计划获得中等订阅量，</strong></li></ol><p id="26bf" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je">为什么不做</strong> <a class="ae li" href="https://dr-robert-kuebler.medium.com/membership" rel="noopener"> <strong class="ll je">通过这个环节</strong> </a> <strong class="ll je">？这将对我帮助很大！😊</strong></p><p id="bdc6" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">透明地说，给你的价格不变，但大约一半的订阅费直接归我。</p><p id="433e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">非常感谢，如果你考虑支持我的话！</p><blockquote class="pd"><p id="f363" class="pe pf iu bd pg ph pi pj pk pl pm me dk translated"><em class="pn">有问题就在</em><a class="ae li" href="https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/" rel="noopener ugc nofollow" target="_blank"><em class="pn">LinkedIn</em></a><em class="pn">上写我！</em></p></blockquote></div></div>    
</body>
</html>