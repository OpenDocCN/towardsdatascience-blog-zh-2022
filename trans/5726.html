<html>
<head>
<title>How is Linear Algebra Applied in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性代数在机器学习中是如何应用的</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-is-linear-algebra-applied-for-machine-learning-d193bdeed268#2022-12-30">https://towardsdatascience.com/how-is-linear-algebra-applied-for-machine-learning-d193bdeed268#2022-12-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="503a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从使用矩阵和向量来表示数据开始</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fa5b0754bd7814537120c4104bdf0844.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TrKE4T_q795SlHZlsuT4Zw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用于机器学习的线性代数(图片来自作者的<a class="ae ky" href="https://www.visual-design.net/" rel="noopener ugc nofollow" target="_blank">网站</a></p></figure><p id="4d85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">说实话，线性代数在机器学习中的作用一直困扰着我，因为我们大多是在数学背景下学习这些概念(如向量、矩阵)，而放弃它们在机器学习环境中的应用。事实上，线性代数在机器学习中有几个基本的用例，包括数据表示、维数约简和向量嵌入。从介绍线性代数中的基本概念开始，本文将构建一个如何将这些概念应用于数据表示的基本视图，例如求解线性方程组、线性回归和神经网络。但是，如果您想了解更多关于主成分分析(PCA)的线性代数，您可能会发现这篇文章更有帮助。</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/a-visual-learners-guide-to-explain-implement-and-interpret-principal-component-analysis-cc9b345b75be"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">解释、实施和解释主成分分析的可视化学习指南</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">机器学习的线性代数——协方差矩阵、特征向量和主分量</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="9e13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mu">对于机器学习线性代数的视频演练，我在本文底部包含了我的 YouTube 视频。</em></p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="df15" class="mv mw it bd mx my mz na nb nc nd ne nf jz ng ka nh kc ni kd nj kf nk kg nl nm bi translated">标量、向量、矩阵和张量的定义</h1><p id="2739" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">首先，让我们解决线性代数的构建模块——标量、向量、矩阵和张量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/ea8970ac16439cfcf6f30f278a84a068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1ggIeMdOGZIbFRxgl5I4HA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">标量、矢量、矩阵、张量(图片由作者提供)</p></figure><ul class=""><li id="5f89" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu ny nz oa ob bi translated">标量:单个数字</li><li id="f3a1" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">Vector:一维数字数组</li><li id="d96c" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">矩阵:数字的二维数组</li><li id="1c1c" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">张量:多维数组的数字</li></ul><p id="ced0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了实现它们，我们可以在 python 中使用<a class="ae ky" href="https://numpy.org/doc/" rel="noopener ugc nofollow" target="_blank"> NumPy </a>数组<code class="fe oh oi oj ok b">np.array()</code>。</p><pre class="kj kk kl km gt ol ok om bn on oo bi"><span id="5f81" class="op mw it ok b be oq or l os ot">scalar = 1<br/>vector = np.array([1,2])<br/>matrix = np.array([[1,1],[2,2]])<br/>tensor = np.array([[[1,1],[2,2]], <br/>                   [[3,3],[4,4]]])</span></pre><p id="5da3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看上面生成的向量、矩阵和张量的形状。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/54d128381d04b4145c84fbe03337ec9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/0*o510EE1kXymoeg0t.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">向量、矩阵、张量形状(图片由作者提供)</p></figure><h1 id="32c5" class="mv mw it bd mx my ov na nb nc ow ne nf jz ox ka nh kc oy kd nj kf oz kg nl nm bi translated">矩阵和向量运算</h1><h2 id="742d" class="pa mw it bd mx pb pc dn nb pd pe dp nf li pf pg nh lm ph pi nj lq pj pk nl pl bi translated">1.加法、减法、乘法、除法</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/409672fd1bc74403fb9d5fed61fe3ad5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/0*Y0KN7VbvsvEFP8j7.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">矩阵运算中的加减乘除(图片由作者提供)</p></figure><p id="9d32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似于我们对数字的运算，同样的逻辑也适用于矩阵和向量。但是，请注意，对矩阵的这些操作对两个大小相同的矩阵有限制。这是因为它们是以元素方式执行的，这与矩阵点积不同。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/d12d6d5d3a2104b7c30d779360d91fa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/0*aA_YnqHE_DBlxT6Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">矩阵运算(图片作者提供)</p></figure><h2 id="0fb0" class="pa mw it bd mx pb pc dn nb pd pe dp nf li pf pg nh lm ph pi nj lq pj pk nl pl bi translated">2.点积</h2><p id="5c31" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">点积经常与矩阵元素乘法混淆(上面已经演示过)；其实是对矩阵和向量比较常用的运算。</p><p id="ce81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">点积通过将第一个矩阵的每一行与第二个矩阵的每一列一次一个元素地迭代相乘来操作，因此<code class="fe oh oi oj ok b">j x k</code>矩阵和<code class="fe oh oi oj ok b">k x i</code>矩阵之间的点积是一个<code class="fe oh oi oj ok b">j x i</code>矩阵。这是一个 3x2 矩阵和 2x3 矩阵之间的点积的例子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi po"><img src="../Images/654db085e204195d699d197f938fd7ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/0*gF2gBbXT1LBgaT72.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">矩阵点积(图片由作者提供)</p></figure><p id="45f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">点积运算需要第一个矩阵中的列数与第二个矩阵中的行数相匹配。我们用<code class="fe oh oi oj ok b">dot()</code> <em class="mu"> </em>来执行点积。运算中矩阵的顺序至关重要——如下所示，<code class="fe oh oi oj ok b">matrix2.dot(matrix1)</code>将产生与<code class="fe oh oi oj ok b">matrix1.dot(matrix2)</code>不同的结果。因此，与逐元素乘法相反，矩阵点积是不可交换的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/6da1a9d07b9b874e33c56963cdb83181.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/0*lVtGoRZ4EYG7pTN7.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">矩阵点积(图片由作者提供)</p></figure><h2 id="03e2" class="pa mw it bd mx pb pc dn nb pd pe dp nf li pf pg nh lm ph pi nj lq pj pk nl pl bi translated">3.使再成形</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/033f0fc6b11747cf21da11f8a8ce35b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/0*ZjIwtT9zpWgLzmMc.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">矩阵整形(图片由作者提供)</p></figure><p id="219b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个向量通常被视为一个只有一列的矩阵，通过使用<code class="fe oh oi oj ok b">reshape()</code>指定列数和行数，它可以被重新整形为矩阵格式。我们也可以将矩阵改变成不同的布局。例如，我们可以使用下面的代码将 2x2 矩阵转换为 4 行 1 列。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/b0d2a0d1146dfdd30dc015a2a2c012ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/0*hgyEVU0j_CVOj-s6.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">矩阵整形(图片由作者提供)</p></figure><p id="a71a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当矩阵的大小未知时，<code class="fe oh oi oj ok b">reshape(-1)</code>常用来降低矩阵维数，将数组“展平”成一行。整形矩阵可广泛应用于神经网络中，以使数据适合神经网络结构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/6995768bd3de7831488e50cbc3de26f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/0*xBtWe444nwR3W-tQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">矩阵整形(图片由作者提供)</p></figure><h2 id="c849" class="pa mw it bd mx pb pc dn nb pd pe dp nf li pf pg nh lm ph pi nj lq pj pk nl pl bi translated">4.移项</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/0903a04051b4bcdeb1b0ad5c3f61a57d.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/0*04ovjPpvsIYGrbs-.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">矩阵转置(作者图片)</p></figure><p id="b456" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">转置交换矩阵的行和列，这样一个<code class="fe oh oi oj ok b">j x k</code>矩阵变成了<code class="fe oh oi oj ok b">k x j</code>。为了转置一个矩阵，我们使用<code class="fe oh oi oj ok b">matrix.T</code>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/9042f42a0d9a79918f99d0a3d9039f40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/0*s6YfxaLCy6KUTE26.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">矩阵转置(作者图片)</p></figure><h2 id="0362" class="pa mw it bd mx pb pc dn nb pd pe dp nf li pf pg nh lm ph pi nj lq pj pk nl pl bi translated">5.单位矩阵和逆矩阵</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/c3dbcfe0d5c5bd07ffb11ff6fe1a5698.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/0*4FVpSGwoyE_5M9WG.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">矩阵求逆(图片作者提供)</p></figure><p id="5aa1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">逆矩阵是矩阵的一种重要变换，但是为了理解逆矩阵，我们首先需要解决什么是单位矩阵。单位矩阵要求列数和行数相同，并且所有对角元素都为 1。此外，矩阵或向量在乘以其对应的单位矩阵后保持不变。</p><p id="ed3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了用 Python 创建一个 3×3 的单位矩阵，我们使用了<code class="fe oh oi oj ok b">numpy.identity(3)</code>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/1bbd75ac492550a263632c0cc3abba5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/0*H4e-_7hMigWcpFBL.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">身份矩阵(图片由作者提供)</p></figure><p id="4204" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">矩阵本身(下面表示为 M)和矩阵的逆矩阵的点积是单位矩阵，它遵循以下等式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi px"><img src="../Images/53c4311c87443ca6071e4533d4a49109.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/0*Gze8-dq1q4s7X7Me.png"/></div></figure><p id="4633" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于矩阵求逆，有两件事需要考虑:1)矩阵和矩阵求逆的顺序无关紧要，即使当顺序改变时，大多数矩阵点积是不同的；2)并非所有矩阵都有逆矩阵。</p><p id="a416" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了计算矩阵的逆矩阵，我们可以使用<code class="fe oh oi oj ok b">np.linalg.inv()</code>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi py"><img src="../Images/a3e46a634e9879c32117e4ea183e38ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/0*dcdA8GUn_rtG7dE3.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">矩阵求逆(图片作者提供)</p></figure><p id="a7f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个阶段，我们只涉及了线性代数中支持数据表示应用的一些基本概念；如果你想更深入地了解更多的概念，我发现 Deisenroth，Faisal 和 Ong 的书《<a class="ae ky" href="https://mml-book.github.io/book/mml-book.pdf" rel="noopener ugc nofollow" target="_blank">机器学习的数学</a>》特别有帮助。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="5a15" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">感谢您远道而来。如果你想阅读我更多关于媒介的文章，我将非常感谢你的支持，注册成为</strong> <a class="ae ky" href="https://destingong.medium.com/membership" rel="noopener"> <strong class="lb iu">媒介会员</strong> </a> <strong class="lb iu">。</strong></p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="51ec" class="mv mw it bd mx my mz na nb nc nd ne nf jz ng ka nh kc ni kd nj kf nk kg nl nm bi translated">线性代数在 ML 中的应用</h1><p id="e208" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">我们将从向量和矩阵在解线性方程组中最直接的应用开始，并逐渐推广到线性回归，然后是神经网络。</p><h2 id="783c" class="pa mw it bd mx pb pc dn nb pd pe dp nf li pf pg nh lm ph pi nj lq pj pk nl pl bi translated">1.线性代数在线性方程组中的应用</h2><p id="5bec" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">假设我们有下面的线性方程组，计算 a 和 b 的值的典型方法是一次消除一个元素，这对于两个变量来说需要 3 到 4 个步骤。</p><blockquote class="pz qa qb"><p id="5860" class="kz la mu lb b lc ld ju le lf lg jx lh qc lj lk ll qd ln lo lp qe lr ls lt lu im bi translated"><em class="it"> 3a + 2b = 7 </em></p><p id="7eed" class="kz la mu lb b lc ld ju le lf lg jx lh qc lj lk ll qd ln lo lp qe lr ls lt lu im bi translated"><em class="it"> a — b = -1 </em></p></blockquote><p id="fcbd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一种解决方案是用矩阵和向量的点积来表示。我们可以将所有系数打包成一个矩阵，将所有变量打包成一个向量，因此我们得到如下结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/050803adf45fe8c4a7c7ac0a3aa0b5cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/0*h4bYnAFtyvCFtQNa.png"/></div></figure><p id="bf9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">矩阵表示给了我们一种不同的心态，一步解决方程。如下所示，我们将系数矩阵表示为<em class="mu"> M </em>，变量向量表示为<em class="mu"> x </em>和输出向量<em class="mu"> y，</em>然后将方程的两边乘以矩阵 M 的逆矩阵。由于矩阵的逆矩阵和矩阵本身之间的点积是单位矩阵，我们可以将线性方程组的解简化为系数矩阵 M 的逆矩阵和输出向量<em class="mu"> y </em>之间的点积。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qg"><img src="../Images/57672ed37638c10310d9b7abaea29aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/0*kkEFsiFWKO7yRmO6.png"/></div></figure><p id="5c09" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用下面的代码片段一步计算变量 a 和 b 的值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/220d7f8bfbb7e32782989a26ef33c283.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/0*jn_xazuUlhjw7Nrt.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">以矩阵形式求解线性方程组(图片由作者提供)</p></figure><p id="2bcf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过用矩阵表示线性方程组，这大大提高了计算速度。假设我们使用传统的方法，它需要使用几个 for 循环来一次消除一个元素。对于这样一个简单的系统来说，这似乎是一个小小的增强，但如果我们将其扩展到机器学习，甚至是由大量像这样的系统组成的深度学习，它会大大提高效率。</p><h2 id="6975" class="pa mw it bd mx pb pc dn nb pd pe dp nf li pf pg nh lm ph pi nj lq pj pk nl pl bi translated">2.线性代数在线性回归中的应用</h2><p id="9179" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">在求解线性方程系统中示出的相同原理可以被推广到机器学习中的线性回归模型。如果你想刷新你对线性回归的记忆，请查看我的文章“线性回归实用指南”。</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/a-practical-guide-to-linear-regression-3b1cb9e501a6"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">线性回归实用指南</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">从 EDA 到特征工程再到模型评估</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="qi l mj mk ml mh mm ks ly"/></div></div></a></div><p id="ce9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们有一个包含 n 个特征和 m 个实例的数据集，我们通常将线性回归表示为这些特征的加权和。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qj"><img src="../Images/6f5682cd5da4894d1578e52c9b8a7d0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/0*44xJIndkwRXpaiwZ.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qk"><img src="../Images/14fd5dd88cc86818276de957ce557e1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/0*Bolq5ydZrM9ec3KC.png"/></div></figure><p id="215f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们用矩阵的形式来表示一个实例的公式呢？我们可以将特征值存储在一个<code class="fe oh oi oj ok b">1 x (n+1)</code>矩阵中，权重存储在一个<code class="fe oh oi oj ok b">(n+1) x 1</code>向量中。然后我们将相同颜色的元素相乘，将它们相加得到加权和。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ql"><img src="../Images/31ced4a848519d70f0c70209dafc9a15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E1zezDvX0gCyfE6HGCoJSg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线性回归矩阵表—一个实例(图片由作者提供)</p></figure><p id="ab9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当实例数量增加时，我们自然会想到使用 for 循环一次迭代一个条目，这可能会很耗时。通过以矩阵格式表示算法，线性回归优化过程归结为通过线性代数运算求解系数向量[ <em class="mu"> w0，w1，w2 … wn] </em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qm"><img src="../Images/de6e68401c430c9492fe46eacf6bac86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ve9Y7shMklQDH7HeoEfN0Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线性回归矩阵表单—多个实例(图片由作者提供)</p></figure><p id="fab2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，流行的 Python 库(如 Numpy 和 Pandas)建立在矩阵表示的基础上，并利用“矢量化”来加快数据处理速度。找到了文章《<a class="ae ky" href="https://medium.com/codex/say-goodbye-to-loops-in-python-and-welcome-vectorization-e4df66615a52" rel="noopener">告别 Python 中的循环，欢迎矢量化！</a>“在 for-loop 和矢量化的计算时间比较方面很有帮助。</p><h2 id="abfa" class="pa mw it bd mx pb pc dn nb pd pe dp nf li pf pg nh lm ph pi nj lq pj pk nl pl bi translated">3.线性代数在神经网络中的应用</h2><p id="ce59" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">神经网络由多层互连的节点组成，其中来自前几层的节点的输出被加权，然后被聚合以形成后几层的输入。如果我们放大神经网络的互连层，我们可以看到回归模型的一些组件。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qn"><img src="../Images/3b98af04fda41480785070adedd618c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jxJsInuyDBGuoQ7AMydWmw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">神经网络中的隐藏层(图片由作者提供)</p></figure><p id="91ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">举一个简单的例子，我们从一个神经网络中可视化出隐藏层 I(具有节点<em class="mu"> i1，i2，i3 </em>)和隐藏层 j(具有节点<em class="mu"> j1，j2 </em>)的内部过程。<em class="mu"> w11 </em>代表馈入节点<em class="mu"> j1、</em>的输入节点<em class="mu"> i1 </em>的权重，<em class="mu"> w21 </em>代表馈入节点<em class="mu"> j1 </em>的输入节点<em class="mu"> i2 </em>的权重。在这种情况下，我们可以将权重打包成 3x2 矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qo"><img src="../Images/f1f508a917808ace65a812d6fba3c28e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zY15DPpdbZn4FhMaVpTJkA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">矩阵形式的神经网络—一个实例(图片由作者提供)</p></figure><p id="d465" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这可以推广到数千甚至数百万个实例，形成神经网络模型的大规模训练数据集。这个过程类似于我们如何表示线性回归模型，除了我们使用一个矩阵来存储权重，而不是一个向量，但是原理是一样的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qp"><img src="../Images/3b9247e175c843827eb150de47ceb6ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ozvTkLXaVwBDuMSNqbN12w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">矩阵形式的神经网络—多个实例(图片由作者提供)</p></figure><p id="d442" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更进一步，我们可以将此扩展到深度学习的深度神经网络。这就是张量在表示二维以上数据时发挥作用的地方。例如，在卷积神经网络中，我们对图像像素使用 3D 张量，因为它们通常通过三个不同的通道(即，红色、绿色、蓝色通道)来描述。</p><p id="ca97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你所看到的，线性代数在机器学习和深度学习算法中起着构建模块的作用，这只是线性代数在数据科学中的多个用例之一。希望在未来的文章中，我可以介绍更多的应用，如线性代数降维。要阅读更多我关于媒体的文章，我将非常感谢你的支持，注册成为媒体会员。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="0d51" class="mv mw it bd mx my mz na nb nc nd ne nf jz ng ka nh kc ni kd nj kf nk kg nl nm bi translated">带回家的信息</h1><p id="b1f0" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">线性代数在机器学习中的重要性似乎是不言而喻的，然而，它在数据表示等方面发挥着重要作用。在本文中，我们首先介绍一些基本概念，例如:</p><ul class=""><li id="e0d1" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu ny nz oa ob bi translated">标量、矢量、矩阵、张量</li><li id="5eff" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">加法、减法、乘法、除法、点积</li><li id="27bd" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">整形、转置、反转</li></ul><p id="f200" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，我们讨论了这些概念如何应用于数据科学和机器学习，包括</p><ul class=""><li id="9dc1" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu ny nz oa ob bi translated">线性方程组</li><li id="5e43" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">线性回归</li><li id="2893" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">神经网络</li></ul><h2 id="8822" class="pa mw it bd mx pb pc dn nb pd pe dp nf li pf pg nh lm ph pi nj lq pj pk nl pl bi translated">更多这样的文章</h2><div class="lv lw gp gr lx"><div role="button" tabindex="0" class="ab bv gv cb fp qq qr bn qs ks ex"><div class="qt l"><div class="ab q"><div class="l di"><img alt="Destin Gong" class="l de bw qu qv fe" src="../Images/dcd4375055f8aa7602b1433a60ad5ca3.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*_qYYfgLTcvZF3zhHO0yVdA@2x.jpeg"/><div class="fb bw l qu qv fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://destingong.medium.com/?source=post_page-----d193bdeed268--------------------------------" rel="noopener follow" target="_top">德斯坦贡</a></p></div></div><div class="qy qz gw l"><h2 class="bd iu wd nt fp we fr fs me fu fw is bi translated">机器学习实用指南</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wf au wg wh wi sv wj an eh ei wk wl wm el em eo de bk ep" href="https://destingong.medium.com/list/practical-guides-to-machine-learning-a877c2a39884?source=post_page-----d193bdeed268--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wn l fo"><span class="bd b dl z dk">10 stories</span></div></div></div><div class="rl dh rm fp ab rn fo di"><div class="di rd bv re rf"><div class="dh l"><img alt="Principal Component Analysis for ML" class="dh" src="../Images/1edea120a42bd7dc8ab4a4fcdd5b822d.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*swd_PY6vTCyPnsgBYoFZfA.png"/></div></div><div class="di rd bv rg rh ri"><div class="dh l"><img alt="Time Series Analysis" class="dh" src="../Images/fda8795039b423777fc8e9d8c0dc0d07.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*8sSAHftNwd_RNJ3k4VA0pA.png"/></div></div><div class="di bv rj rk ri"><div class="dh l"><img alt="deep learning cheatsheet for beginner" class="dh" src="../Images/b2a4e3806c454a795ddfae0b02828b30.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uNyD4yNMH-DnOel1wzxOOA.png"/></div></div></div></div></div><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/tensorflow-template-for-deep-learning-beginners-3b976d0ee084"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">深度学习初学者 TensorFlow 模板</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">如何建立你的第一个深度神经网络</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="rs l mj mk ml mh mm ks ly"/></div></div></a></div><div class="lv lw gp gr lx"><div role="button" tabindex="0" class="ab bv gv cb fp qq qr bn qs ks ex"><div class="qt l"><div class="ab q"><div class="l di"><img alt="Destin Gong" class="l de bw qu qv fe" src="../Images/dcd4375055f8aa7602b1433a60ad5ca3.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*_qYYfgLTcvZF3zhHO0yVdA@2x.jpeg"/><div class="fb bw l qu qv fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://destingong.medium.com/?source=post_page-----d193bdeed268--------------------------------" rel="noopener follow" target="_top">德斯坦贡</a></p></div></div><div class="qy qz gw l"><h2 class="bd iu wd nt fp we fr fs me fu fw is bi translated">开始学习数据科学</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wf au wg wh wi sv wj an eh ei wk wl wm el em eo de bk ep" href="https://destingong.medium.com/list/get-started-in-data-science-8006bb4ba3ad?source=post_page-----d193bdeed268--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wn l fo"><span class="bd b dl z dk">8 stories</span></div></div></div><div class="rl dh rm fp ab rn fo di"><div class="di rd bv re rf"><div class="dh l"><img alt="" class="dh" src="../Images/d302bbd526df8af0e847419971dc535a.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*__Lp9NvZvtLrZ00KKyoS_A.png"/></div></div><div class="di rd bv rg rh ri"><div class="dh l"><img alt="Statistical Tests in Python" class="dh" src="../Images/2ff8d4b6d8bd95fde596b31de22ef09e.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*2hGMrCjLtVKtOKD_QnyuWA.png"/></div></div><div class="di bv rj rk ri"><div class="dh l"><img alt="" class="dh" src="../Images/ae659430af3f4c100a2e11f1f558462c.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*wCpwHS7BWBe6JnHfIMSWrQ.png"/></div></div></div></div></div><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="rt ru l"/></div></figure></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="18da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mu">原载于 2022 年 12 月 28 日</em><a class="ae ky" href="https://www.visual-design.net/post/semi-automated-exploratory-data-analysis-process-in-python" rel="noopener ugc nofollow" target="_blank"><em class="mu">【https://www.visual-design.net】</em></a><em class="mu">。</em></p></div></div>    
</body>
</html>