<html>
<head>
<title>4 Effective Ways to Prevent Overfitting and Why They Work</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">防止过度拟合的4种有效方法及其工作原理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/4-effective-ways-to-prevent-overfitting-and-why-they-work-f6e3b98aefda#2022-10-27">https://towardsdatascience.com/4-effective-ways-to-prevent-overfitting-and-why-they-work-f6e3b98aefda#2022-10-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0160" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">构建有用的机器学习模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/75e501a6004d51b6bf6d28073052b52d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uvCrMimLIuysOKUI"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">过度拟合会导致你的模型错过目标，照片由<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@enginakyurt?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> engin akyurt </a>拍摄</p></figure><h2 id="2cf0" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">介绍</h2><p id="bcc6" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">在这篇文章中，我将分享在构建机器学习(ML)模型时可以避免过度拟合的四种实用方法，以及它们为什么有效。</p><p id="ed47" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">过度拟合是一种不希望出现的情况，当模型拟合得过于接近训练数据，以至于无法很好地推广到新的示例，即无法为以前未见过的数据集提供准确的预测。</p><blockquote class="mt"><p id="16f1" class="mu mv it bd mw mx my mz na nb nc mn dk translated">面对现实吧，过度拟合让ML模型无法使用。</p></blockquote><p id="a0be" class="pw-post-body-paragraph lv lw it lx b ly nd ju ma mb ne jx md li nf mf mg lm ng mi mj lq nh ml mm mn im bi translated">检测过度拟合的最常见方法是通过比较训练和测试数据集上的模型性能。与训练集相比，过度拟合的模型在测试集上将具有明显更差的性能。</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h2 id="a909" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">4防止过度拟合的有效方法</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/94442d3403bd0d0289a9c2fa946d6732.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*XPcpG-cQ-xXcTVsESvnWNg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="8f40" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">为了构建有用的ML模型，我们需要有效地解决过度拟合问题。以下是一些避免过度拟合模型的实用方法:</p><ol class=""><li id="4091" class="nq nr it lx b ly mo mb mp li ns lm nt lq nu mn nv nw nx ny bi translated"><strong class="lx iu">添加更多数据</strong></li></ol><p id="7756" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">是的，增加训练样本的数量是减少过度拟合的可靠方法。除非训练集正确地代表了整体数据中的真实模式，否则您的模型可能会过于接近有限的数据，从而使其无法通用化到未经训练的新数据集。</p><p id="7b27" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">这可以通过收集新样本(这在某些应用中可能很困难)或数据扩充(这将产生现有样本的变体)来实现。例如，在图像分类任务中，已经发现数据扩充可以显著提高模型性能。</p><p id="d8a9" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><em class="nz">工作原理:</em>随着数据量的增加，该模型倾向于更一般化，因为它适合训练样本的较小部分。</p><p id="13f7" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">2.<strong class="lx iu">调整超参数</strong></p><p id="65ed" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">复杂的ML模型通常比简单的模型更精确，但是前者更容易过度拟合。</p><p id="b461" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">在这种情况下，当超参数越多或训练时间越长时，ML模型变得越复杂。超参数的值控制学习过程，不能从数据中估计，并且在模型之外。因此，它们是在训练开始前设置的。</p><p id="c108" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">该解决方案取决于算法类型，可以通过设置最佳超参数值来实现。一些常见的例子包括:</p><ul class=""><li id="5b99" class="nq nr it lx b ly mo mb mp li ns lm nt lq nu mn oa nw nx ny bi translated"><em class="nz">树方法(决策树、随机森林、LGBM、XGBoost): </em>减少每棵树的最大深度(即每棵树的叶子数)。</li><li id="4da4" class="nq nr it lx b ly ob mb oc li od lm oe lq of mn oa nw nx ny bi translated"><em class="nz">神经网络:</em>早期停止(较早停止训练过程)和减少隐层数量(去掉部分神经元)。</li><li id="e307" class="nq nr it lx b ly ob mb oc li od lm oe lq of mn oa nw nx ny bi translated"><em class="nz"> K最近邻(KNN): </em>增加K的值</li></ul><p id="51d9" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><em class="nz">工作原理:</em>通过调整超参数可以降低模型的复杂性，从而减少过度拟合的机会。例如，越早停止训练过程，模型越简单。在KNN的情况下，增加K的值降低了模型对局部模式的敏感性，因为在训练过程中使用了更多的邻居。这使得模型对于新的数据集更具普适性，从而减少过度拟合。</p><p id="f815" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">3.<strong class="lx iu">减少特征数量</strong></p><p id="cab1" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">过度拟合可能是由具有大量特征的有限数据量造成的。但是，减少要素的数量可能会导致重要信息的丢失，因此必须谨慎操作。</p><p id="9667" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">一种想法是在探索性数据分析期间手动移除预测能力低的特征。与目标变量具有更强相关性的特征具有更强的预测能力。此外，可以使用<a class="ae ky" rel="noopener" target="_blank" href="/understanding-feature-importance-and-how-to-implement-it-in-python-ff0287b20285">特征重要性分数</a>和互信息来测量预测能力。</p><p id="6c97" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">此外，可以使用诸如主成分分析(PCA)的技术来降低数据的维度。使用这种方法，可以选择分量的数量，使得仅选择贡献重要信息(解释的方差)的特征。然而，PCA导致变换特征的可解释性的损失。</p><p id="b5fc" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><em class="nz">工作原理:</em>使用较少的特征降低了模型的复杂性，有助于防止过度拟合。</p><p id="7403" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">4.<strong class="lx iu">正规化</strong></p><p id="617f" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">该方法用于通过惩罚对学习过程贡献很小或没有贡献的特征来约束ML模型的复杂性。</p><p id="89d6" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">一些正则化类型包括L1(最小绝对收缩和选择算子——套索)、L2(脊)和专门用于神经网络的<a class="ae ky" href="https://programmathically.com/dropout-regularization-in-neural-networks-how-it-works-and-when-to-use-it/" rel="noopener ugc nofollow" target="_blank">下降</a>。</p><p id="39a5" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">LASSO将不太重要的要素的贡献缩小到零，从而消除了这些要素，而Ridge则减少了这些要素的贡献。关于正规化的更多细节可以在<a class="ae ky" href="https://www.geeksforgeeks.org/regularization-in-machine-learning/" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="8080" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><em class="nz">工作原理:</em>正则化限制了模型的复杂性，从而减少了过度拟合的机会。</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h2 id="4473" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">结论</h2><p id="3a0f" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">我们已经介绍了解决过度拟合的四种有效方法以及它们为什么有效。这些方法包括添加更多数据、调整超参数、减少特征数量和正则化。还提供了一些资源链接供进一步学习。</p><p id="4c00" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">我希望这篇文章有深刻的见解，直到下一次。干杯！</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><p id="11e8" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">你可以通过下面我的推荐链接订阅Medium，获得更多我和其他作者的启发性文章，这也支持我的写作。谢谢大家！</p><div class="og oh gp gr oi oj"><a href="https://aolaoye.medium.com/membership" rel="noopener follow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd iu gy z fp oo fr fs op fu fw is bi translated">通过我的推荐链接加入媒体</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">阅读Abiodun Olaoye(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接支持…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">aolaoye.medium.com</p></div></div><div class="os l"><div class="ot l ou ov ow os ox ks oj"/></div></div></a></div></div></div>    
</body>
</html>