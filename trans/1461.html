<html>
<head>
<title>Create a K-Means Clustering Algorithm from Scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python从头开始创建K-Means聚类算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/create-your-own-k-means-clustering-algorithm-in-python-d7d4c9077670#2022-04-11">https://towardsdatascience.com/create-your-own-k-means-clustering-algorithm-in-python-d7d4c9077670#2022-04-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8091" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过自己实现k-means聚类巩固你的知识</h2></div><h1 id="2bd8" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">介绍</h1><p id="56dc" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">k-means聚类是一种<strong class="kz ir">无监督的</strong>机器学习算法，旨在根据数据点的相似性将数据集分割成组。无监督模型有<strong class="kz ir">自变量</strong>，没有<strong class="kz ir">因变量</strong>。</p><p id="e970" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">假设您有一个二维标量属性的数据集:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ly"><img src="../Images/e3a7886f2f964862aa6dbe9cd161d475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H_aefoRr0TPnkQWD9hbgrQ.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图片作者。</p></figure><p id="17b1" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">如果该数据集中的点属于不同的组，且这些组之间的属性差异很大，但组内的属性差异不大，则绘制时这些点应形成聚类。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/dea011fcbb241be497030cec8f63c00b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*ea78Wc_MEd7esuzYkeoANg.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图片作者。</p></figure><p id="16fb" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated"><strong class="kz ir">图1: </strong> <em class="mp">具有不同属性组的点的数据集。</em></p><p id="0337" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">该数据集清楚地显示了3类不同的数据。如果我们试图将一个新的数据点分配给这三组中的一组，可以通过找到每组的中点(质心)并选择最近的质心作为未分配数据点的组来实现。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/479fe87b7db4a191fa0c0436e2c16aaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*5D4A2q1wNYlD2AMkO3GjUg.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图片作者。</p></figure><p id="a4f2" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated"><strong class="kz ir">图2: </strong> <em class="mp">数据点被分割成用不同颜色表示的组。</em></p><h1 id="e5ac" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">算法</h1><p id="297c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">对于给定的数据集，k被指定为点所属的不同组的数量。这k个质心首先被随机初始化，然后执行迭代以优化这k个质心的位置，如下所示:</p><ol class=""><li id="86e2" class="mq mr iq kz b la lt ld lu lg ms lk mt lo mu ls mv mw mx my bi translated">计算从每个点到每个质心的距离。</li><li id="8da1" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">点被分配给它们最近的质心。</li><li id="49cd" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">质心被移动为属于它的点的平均值。如果质心没有移动，算法结束，否则重复。</li></ol><h1 id="5770" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">数据</h1><p id="214a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了评估我们的算法，我们将首先在二维空间中生成一个组数据集。sklearn.datasets函数make_blobs创建二维正态分布的分组，并分配与所述点所属的组相对应的标签。</p><pre class="lz ma mb mc gt ne nf ng nh aw ni bi"><span id="2fd9" class="nj kg iq nf b gy nk nl l nm nn">import seaborn as sns<br/>from sklearn.datasets import make_blobs<br/>import matplotlib.pyplot as plt<br/>from sklearn.preprocessing import StandardScaler</span><span id="e6b2" class="nj kg iq nf b gy no nl l nm nn">centers = 5<br/>X_train, true_labels = make_blobs(n_samples=100, centers=centers, random_state=42)<br/>X_train = StandardScaler().fit_transform(X_train)</span><span id="7580" class="nj kg iq nf b gy no nl l nm nn">sns.scatterplot(x=[X[0] for X in X_train],<br/>                y=[X[1] for X in X_train],<br/>                hue=true_labels,<br/>                palette="deep",<br/>                legend=None<br/>                )</span><span id="3cee" class="nj kg iq nf b gy no nl l nm nn">plt.xlabel("x")<br/>plt.ylabel("y")<br/>plt.show()</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/b427ecfa23e5a13f330d5a9bc6a61fb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*dXBPArvY86zKzV71Z8Q6Bg.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图片作者。</p></figure><p id="c91c" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated"><strong class="kz ir">图3: </strong> <em class="mp">我们将用来评估我们的k均值聚类模型的数据集。</em></p><p id="1c7c" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">该数据集提供了k-means算法的独特演示。观察橙色点一反常态地远离其中心，并直接位于紫色数据点群中。这个点不能被准确地分类为属于正确的组，因此即使我们的算法工作得很好，它也应该被错误地表征为紫色组的成员。</p><h1 id="054b" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">模型创建</h1><h1 id="6155" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">助手功能</h1><p id="8067" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在这个算法中，我们需要多次计算一个点和一组点之间的距离。为此，让我们定义一个计算欧几里德距离的函数。</p><pre class="lz ma mb mc gt ne nf ng nh aw ni bi"><span id="ca2a" class="nj kg iq nf b gy nk nl l nm nn">def euclidean(point, data):<br/>    """<br/>    Euclidean distance between point &amp; data.<br/>    Point has dimensions (m,), data has dimensions (n,m), and output will be of size (n,).<br/>    """<br/>    return np.sqrt(np.sum((point - data)**2, axis=1))</span></pre><h1 id="facc" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">履行</h1><p id="1caa" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">首先，用k值和寻找最佳质心位置的最大迭代次数初始化k均值聚类算法。如果在优化质心位置时没有考虑最大迭代次数，则存在运行无限循环的风险。</p><pre class="lz ma mb mc gt ne nf ng nh aw ni bi"><span id="fbf0" class="nj kg iq nf b gy nk nl l nm nn">class KMeans:</span><span id="ef94" class="nj kg iq nf b gy no nl l nm nn">    def __init__(self, n_clusters=8, max_iter=300):<br/>        self.n_clusters = n_clusters<br/>        self.max_iter = max_iter</span></pre><p id="9567" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">现在，大部分算法是在将模型拟合到训练数据集时执行的。</p><p id="005a" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">首先，我们将在测试数据集的域中随机初始化质心，使其均匀分布。</p><pre class="lz ma mb mc gt ne nf ng nh aw ni bi"><span id="b6ba" class="nj kg iq nf b gy nk nl l nm nn"># Randomly select centroid start points, uniformly distributed across the domain of the dataset<br/>min_, max_ = np.min(X_train, axis=0), np.max(X_train, axis=0)<br/>self.centroids = [uniform(min_, max_) for _ in range(self.n_clusters)]</span></pre><p id="658b" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">接下来，我们执行优化质心位置的迭代过程。</p><p id="d30c" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">优化过程是将质心位置重新调整为属于它的点的平均值。重复这个过程，直到质心停止移动，或者经过了最大迭代次数。我们将使用while循环来说明这个过程没有固定的迭代次数。此外，您还可以使用for循环，该循环重复max_iter次，并在质心停止变化时中断。</p><p id="2156" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">在开始while循环之前，我们将初始化退出条件中使用的变量。</p><pre class="lz ma mb mc gt ne nf ng nh aw ni bi"><span id="5d21" class="nj kg iq nf b gy nk nl l nm nn">iteration = 0<br/>prev_centroids = None</span></pre><p id="00ae" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">现在，我们开始循环。我们将遍历训练集中的数据点，将它们分配给一个初始化的空列表。sorted_points列表为每个质心包含一个空列表，数据点一旦被赋值就被追加到这个列表中。</p><pre class="lz ma mb mc gt ne nf ng nh aw ni bi"><span id="6184" class="nj kg iq nf b gy nk nl l nm nn">while np.not_equal(self.centroids, prev_centroids).any() and iteration &lt; self.max_iter:<br/>    # Sort each data point, assigning to nearest centroid<br/>    sorted_points = [[] for _ in range(self.n_clusters)]<br/>    for x in X_train:<br/>        dists = euclidean(x, self.centroids)<br/>        centroid_idx = np.argmin(dists)<br/>        sorted_points[centroid_idx].append(x)</span></pre><p id="410d" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">既然我们已经将整个训练数据集分配给它们最近的质心，我们可以更新质心的位置并完成迭代。</p><pre class="lz ma mb mc gt ne nf ng nh aw ni bi"><span id="d21b" class="nj kg iq nf b gy nk nl l nm nn"># Push current centroids to previous, reassign centroids as mean of the points belonging to them<br/>    prev_centroids = self.centroids<br/>    self.centroids = [np.mean(cluster, axis=0) for cluster in sorted_points]<br/>    for i, centroid in enumerate(self.centroids):<br/>        if np.isnan(centroid).any():  # Catch any np.nans, resulting from a centroid having no points<br/>            self.centroids[i] = prev_centroids[i]<br/>    iteration += 1</span></pre><p id="577d" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">迭代完成后，再次检查while条件，算法将继续，直到质心被优化或最大迭代次数被通过。下面介绍了完全适合的方法。</p><pre class="lz ma mb mc gt ne nf ng nh aw ni bi"><span id="9a44" class="nj kg iq nf b gy nk nl l nm nn">class KMeans:</span><span id="3269" class="nj kg iq nf b gy no nl l nm nn">    def __init__(self, n_clusters=8, max_iter=300):<br/>        self.n_clusters = n_clusters<br/>        self.max_iter = max_iter</span><span id="9284" class="nj kg iq nf b gy no nl l nm nn">    def fit(self, X_train):</span><span id="0dc2" class="nj kg iq nf b gy no nl l nm nn">        # Randomly select centroid start points, uniformly distributed across the domain of the dataset<br/>        min_, max_ = np.min(X_train, axis=0), np.max(X_train, axis=0)<br/>        self.centroids = [uniform(min_, max_) for _ in range(self.n_clusters)]</span><span id="f6ab" class="nj kg iq nf b gy no nl l nm nn">        # Iterate, adjusting centroids until converged or until passed max_iter<br/>        iteration = 0<br/>        prev_centroids = None<br/>        while np.not_equal(self.centroids, prev_centroids).any() and iteration &lt; self.max_iter:<br/>            # Sort each datapoint, assigning to nearest centroid<br/>            sorted_points = [[] for _ in range(self.n_clusters)]<br/>            for x in X_train:<br/>                dists = euclidean(x, self.centroids)<br/>                centroid_idx = np.argmin(dists)<br/>                sorted_points[centroid_idx].append(x)</span><span id="ff3c" class="nj kg iq nf b gy no nl l nm nn">            # Push current centroids to previous, reassign centroids as mean of the points belonging to them<br/>            prev_centroids = self.centroids<br/>            self.centroids = [np.mean(cluster, axis=0) for cluster in sorted_points]<br/>            for i, centroid in enumerate(self.centroids):<br/>                if np.isnan(centroid).any():  # Catch any np.nans, resulting from a centroid having no points<br/>                    self.centroids[i] = prev_centroids[i]<br/>            iteration += 1</span></pre><p id="507a" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">最后，让我们创建一个方法来评估一组点的质心，我们已经优化了我们的训练集。该方法返回每个点的质心和所述质心的索引。</p><pre class="lz ma mb mc gt ne nf ng nh aw ni bi"><span id="4923" class="nj kg iq nf b gy nk nl l nm nn">def evaluate(self, X):<br/>        centroids = []<br/>        centroid_idxs = []<br/>        for x in X:<br/>            dists = euclidean(x, self.centroids)<br/>            centroid_idx = np.argmin(dists)<br/>            centroids.append(self.centroids[centroid_idx])<br/>            centroid_idxs.append(centroid_idx)</span><span id="c541" class="nj kg iq nf b gy no nl l nm nn">        return centroids, centroid_idx</span></pre><h1 id="97a7" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">首次模型评估</h1><p id="f52c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在我们终于可以部署我们的模型了。让我们在原始数据集上训练和测试它，看看结果。我们将保留绘制数据的原始方法，通过颜色分离真实标注，但现在我们将额外通过标记样式分离预测标注，以查看模型的执行情况。</p><pre class="lz ma mb mc gt ne nf ng nh aw ni bi"><span id="23ac" class="nj kg iq nf b gy nk nl l nm nn">kmeans = KMeans(n_clusters=centers)<br/>kmeans.fit(X_train)</span><span id="d70f" class="nj kg iq nf b gy no nl l nm nn"># View results<br/>class_centers, classification = kmeans.evaluate(X_train)<br/>sns.scatterplot(x=[X[0] for X in X_train],<br/>                y=[X[1] for X in X_train],<br/>                hue=true_labels,<br/>                style=classification,<br/>                palette="deep",<br/>                legend=None<br/>                )<br/>plt.plot([x for x, _ in kmeans.centroids],<br/>         [y for _, y in kmeans.centroids],<br/>         '+',<br/>         markersize=10,<br/>         )</span><span id="14a6" class="nj kg iq nf b gy no nl l nm nn">plt.show()</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/c4a3b2ef550c02d7291702db6b92a41e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*E3CJkzy-1sIMKi8KzSby5Q.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图片作者。</p></figure><p id="78ea" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated"><strong class="kz ir">图4: </strong> <em class="mp">一个失败的例子，一个质心没有点，一个包含两个集群。</em></p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/390365c195362b4205370c207d6374ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*maxReYnbwcDAM-DspFaCjg.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图片作者。</p></figure><p id="eece" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated"><strong class="kz ir">图5: </strong> <em class="mp">一个失败的例子，一个质心没有点，两个包含两个簇，两个分裂一个簇。</em></p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/e0780ac320819d121b926b3648e79b82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*5IYsuMbelEwhgt1dNnHglg.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图片作者。</p></figure><p id="aae9" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated"><strong class="kz ir">图6: </strong> <em class="mp">两个质心包含一个半簇，两个质心分裂一个簇的失败例子。</em></p><h1 id="7173" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">重新评估质心初始化</h1><p id="921c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">看来我们的模型表现不太好。我们可以从这三个失败的例子中推断出两个主要问题。</p><ol class=""><li id="e34d" class="mq mr iq kz b la lt ld lu lg ms lk mt lo mu ls mv mw mx my bi translated">如果质心被初始化为远离任何组，它不太可能移动。(例:<strong class="kz ir">图4 </strong>中的右下质心。)</li><li id="ceb7" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">如果质心被初始化得太近，它们不太可能彼此分开。(例如:<strong class="kz ir">图6 </strong>中绿色组的两个质心。)</li></ol><p id="fee7" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">我们将开始用初始化质心位置的新过程来解决这些问题。这种新方法被称为k-means++算法。</p><ol class=""><li id="f7e4" class="mq mr iq kz b la lt ld lu lg ms lk mt lo mu ls mv mw mx my bi translated">将第一质心初始化为数据点之一的随机选择。</li><li id="c91f" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">计算每个数据点和所有质心之间的距离总和。</li><li id="c245" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">随机选择下一个质心，其概率与到质心的总距离成比例。</li><li id="b276" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">返回步骤2。重复上述步骤，直到所有质心都已初始化。</li></ol><p id="35cb" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">该代码包含在下面。</p><pre class="lz ma mb mc gt ne nf ng nh aw ni bi"><span id="0b86" class="nj kg iq nf b gy nk nl l nm nn"># Initialize the centroids, using the "k-means++" method, where a random datapoint is selected as the first,<br/># then the rest are initialized w/ probabilities proportional to their distances to the first<br/># Pick a random point from train data for first centroid<br/>self.centroids = [random.choice(X_train)]</span><span id="fbe9" class="nj kg iq nf b gy no nl l nm nn">for _ in range(self.n_clusters-1):<br/>    # Calculate distances from points to the centroids<br/>    dists = np.sum([euclidean(centroid, X_train) for centroid in self.centroids], axis=0)<br/>    # Normalize the distances<br/>    dists /= np.sum(dists)<br/>    # Choose remaining points based on their distances<br/>    new_centroid_idx, = np.random.choice(range(len(X_train)), size=1, p=dists)<br/>    self.centroids += [X_train[new_centroid_idx]]</span></pre><p id="f188" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">如果我们运行这个新模型几次，我们会看到它表现得更好，但仍然不总是完美的。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/0aac412f418c6300a4a892b416da9b1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*-6SIYlCcluy72uewowcypA.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图片作者。</p></figure><p id="303d" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated"><strong class="kz ir">图7: </strong> <em class="mp">实现k-means++初始化方法后的理想收敛。</em></p><h1 id="0971" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">结论</h1><p id="56d3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">就这样，我们结束了。我们学习了一个简单而优雅的无监督机器学习模型的实现。完整的项目代码包含在下面。</p><pre class="lz ma mb mc gt ne nf ng nh aw ni bi"><span id="8b33" class="nj kg iq nf b gy nk nl l nm nn">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.preprocessing import StandardScaler<br/>from numpy.random import uniform<br/>from sklearn.datasets import make_blobs<br/>import seaborn as sns<br/>import random<br/></span><span id="3577" class="nj kg iq nf b gy no nl l nm nn">def euclidean(point, data):<br/>    """<br/>    Euclidean distance between point &amp; data.<br/>    Point has dimensions (m,), data has dimensions (n,m), and output will be of size (n,).<br/>    """<br/>    return np.sqrt(np.sum((point - data)**2, axis=1))<br/></span><span id="6966" class="nj kg iq nf b gy no nl l nm nn">class KMeans:</span><span id="03c5" class="nj kg iq nf b gy no nl l nm nn">    def __init__(self, n_clusters=8, max_iter=300):<br/>        self.n_clusters = n_clusters<br/>        self.max_iter = max_iter</span><span id="5869" class="nj kg iq nf b gy no nl l nm nn">    def fit(self, X_train):</span><span id="3436" class="nj kg iq nf b gy no nl l nm nn">        # Initialize the centroids, using the "k-means++" method, where a random datapoint is selected as the first,<br/>        # then the rest are initialized w/ probabilities proportional to their distances to the first<br/>        # Pick a random point from train data for first centroid<br/>        self.centroids = [random.choice(X_train)]</span><span id="ac99" class="nj kg iq nf b gy no nl l nm nn">        for _ in range(self.n_clusters-1):<br/>            # Calculate distances from points to the centroids<br/>            dists = np.sum([euclidean(centroid, X_train) for centroid in self.centroids], axis=0)<br/>            # Normalize the distances<br/>            dists /= np.sum(dists)<br/>            # Choose remaining points based on their distances<br/>            new_centroid_idx, = np.random.choice(range(len(X_train)), size=1, p=dists)<br/>            self.centroids += [X_train[new_centroid_idx]]</span><span id="dde7" class="nj kg iq nf b gy no nl l nm nn">        # This initial method of randomly selecting centroid starts is less effective<br/>        # min_, max_ = np.min(X_train, axis=0), np.max(X_train, axis=0)<br/>        # self.centroids = [uniform(min_, max_) for _ in range(self.n_clusters)]</span><span id="34da" class="nj kg iq nf b gy no nl l nm nn">        # Iterate, adjusting centroids until converged or until passed max_iter<br/>        iteration = 0<br/>        prev_centroids = None<br/>        while np.not_equal(self.centroids, prev_centroids).any() and iteration &lt; self.max_iter:<br/>            # Sort each datapoint, assigning to nearest centroid<br/>            sorted_points = [[] for _ in range(self.n_clusters)]<br/>            for x in X_train:<br/>                dists = euclidean(x, self.centroids)<br/>                centroid_idx = np.argmin(dists)<br/>                sorted_points[centroid_idx].append(x)</span><span id="d9ff" class="nj kg iq nf b gy no nl l nm nn">            # Push current centroids to previous, reassign centroids as mean of the points belonging to them<br/>            prev_centroids = self.centroids<br/>            self.centroids = [np.mean(cluster, axis=0) for cluster in sorted_points]<br/>            for i, centroid in enumerate(self.centroids):<br/>                if np.isnan(centroid).any():  # Catch any np.nans, resulting from a centroid having no points<br/>                    self.centroids[i] = prev_centroids[i]<br/>            iteration += 1</span><span id="3982" class="nj kg iq nf b gy no nl l nm nn">    def evaluate(self, X):<br/>        centroids = []<br/>        centroid_idxs = []<br/>        for x in X:<br/>            dists = euclidean(x, self.centroids)<br/>            centroid_idx = np.argmin(dists)<br/>            centroids.append(self.centroids[centroid_idx])<br/>            centroid_idxs.append(centroid_idx)</span><span id="b736" class="nj kg iq nf b gy no nl l nm nn">        return centroids, centroid_idxs<br/></span><span id="02e7" class="nj kg iq nf b gy no nl l nm nn"># Create a dataset of 2D distributions<br/>centers = 5<br/>X_train, true_labels = make_blobs(n_samples=100, centers=centers, random_state=42)<br/>X_train = StandardScaler().fit_transform(X_train)</span><span id="2dec" class="nj kg iq nf b gy no nl l nm nn"># Fit centroids to dataset<br/>kmeans = KMeans(n_clusters=centers)<br/>kmeans.fit(X_train)</span><span id="66c7" class="nj kg iq nf b gy no nl l nm nn"># View results<br/>class_centers, classification = kmeans.evaluate(X_train)<br/>sns.scatterplot(x=[X[0] for X in X_train],<br/>                y=[X[1] for X in X_train],<br/>                hue=true_labels,<br/>                style=classification,<br/>                palette="deep",<br/>                legend=None<br/>                )<br/>plt.plot([x for x, _ in kmeans.centroids],<br/>         [y for _, y in kmeans.centroids],<br/>         'k+',<br/>         markersize=10,<br/>         )</span><span id="d9c8" class="nj kg iq nf b gy no nl l nm nn">plt.show()</span></pre><p id="65a9" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">感谢阅读！<br/> <a class="ae np" href="https://www.linkedin.com/in/turnermluke/" rel="noopener ugc nofollow" target="_blank">在LinkedIn上和我联系</a> <br/> <a class="ae np" href="https://github.com/turnerluke/ML-algos/blob/main/k_means/k_means.py" rel="noopener ugc nofollow" target="_blank">在GitHub上看到这个项目</a></p></div></div>    
</body>
</html>