<html>
<head>
<title>Linear Regression with OLS: Unbiased, Consistent, BLUE, Best (Efficient) Estimator</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OLS线性回归:无偏、一致、蓝色、最佳(有效)估计量</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-with-ols-unbiased-consistent-blue-best-efficient-estimator-359a859f757e#2022-05-25">https://towardsdatascience.com/linear-regression-with-ols-unbiased-consistent-blue-best-efficient-estimator-359a859f757e#2022-05-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="065a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用一点数学知识理解OLS线性回归</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/d5213c721d3f256f4b3c25b654e29b32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/0*Aa3xwnRpL_sQTF5y.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">作者图片</p></figure><p id="3b88" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">已知OLS估计量是<strong class="kw iu">无偏</strong>、<strong class="kw iu">一致</strong>和<strong class="kw iu">蓝</strong>(最佳线性无偏估计量)。但是这些属性是什么意思呢？为什么它们对线性回归模型很重要？在本文中，我们将讨论这些属性。</p></div><div class="ab cl lq lr hx ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="im in io ip iq"><p id="2da1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">典型的线性回归如下所示。响应变量(即Y)被解释为解释变量(如截距、X1、X2、X3……)的线性组合，而<strong class="kw iu"> ε </strong>是误差项(即<strong class="kw iu">随机变量</strong>)，代表拟合响应值和实际响应值之间的差异。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi lx"><img src="../Images/bc6cf0739eaf3d1d10fc1d2f44a136cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xAkErqXfQMntG7Ks.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图1(作者图片)</p></figure><p id="4842" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了让OLS发挥作用，我们需要确保一些假设是正确的。</p><p id="c76a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">假设1- <strong class="kw iu">参数的线性度</strong>:线性模型中的参数是线性的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mc"><img src="../Images/1a6f459ecd247cfcd9e49030cf4c79a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WHiUexj2YZ0yVyLdFiIhXg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图2(作者图片)</p></figure><p id="4458" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">此外，OLS估计量也是线性的，我们可以将OLS闭合解改写如下(将图1中的Y代入图3)。矩阵代数只在线性存在的情况下有效。因此，线性假设是成立的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi md"><img src="../Images/9ed2d10f26b00133435c7b84c303e68d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e_fly4aybl6PpqI1o3kGVw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图3(作者图片)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi me"><img src="../Images/2e2ce04ced6478f05a6e2aa79798f13d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6iUuDv2cZ3Vieo_HdtmaUA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图4(作者图片)</p></figure><p id="b98c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">假设2- <strong class="kw iu">随机抽样</strong>:观测数据代表<strong class="kw iu"> iid </strong>独立同分布)随机样本，遵循总体模型(见图1)。如果数据是跨部门收集的，我们需要确保它们是随机抽样的。底线是观察到的数据应该代表总体数据</p><p id="4f0b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">假设3- <strong class="kw iu">无完美共线性</strong>:任何解释变量都不能表示为其他解释变量的线性组合。原因是逆矩阵(在图3中)仅在X具有满秩时存在，这意味着如果存在完美的共线性，它将没有封闭形式的解决方案。</p><p id="77bd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">假设4- <strong class="kw iu">零条件均值</strong>:误差项的期望值对解释变量的所有值都是零条件的(即E[ <strong class="kw iu"> ε </strong> |X] = 0)</p><p id="5033" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">假设5- <strong class="kw iu">同方差</strong>和<strong class="kw iu">无自相关</strong>:误差项应具有恒定方差和iid。换句话说，误差项的方差-协方差矩阵中的对角线值应该是常数，非对角线值应该都是0。</p><p id="d4b3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">假设6- <strong class="kw iu">误差的正态性</strong>:误差项呈正态分布。这个假设对于OLS方法的有效性来说并不是必需的，但这允许我们有一个可靠的估计标准误差，并做出有意义的统计推断。</p></div><div class="ab cl lq lr hx ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="im in io ip iq"><h2 id="c0f9" class="mf mg it bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">β vs β^ vs E(β^)</h2><p id="2be9" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">你可能在统计学教科书上看到过<strong class="kw iu"> β </strong>(如β，β^，E(β^))的一些变体。我们来讨论一下它们的定义和区别。</p><p id="8800" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> β </strong>是一个概念值——解释<strong class="kw iu">总体数据</strong>中解释变量和因变量之间关系的<strong class="kw iu">真</strong>(通常还有<strong class="kw iu">未知</strong>)参数值(即<strong class="kw iu">常数值</strong>)。</p><p id="55ab" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在大多数情况下，我们不会使用人口数据，因为它不可用或太大而无法处理。因此，我们将使用<strong class="kw iu">样本数据</strong>(具有<strong class="kw iu">有限</strong>个观察值)来开发我们的线性回归模型。</p><p id="da99" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在<strong class="kw iu">随机抽样</strong>的假设下，观测样本数据代表大小为n的同分布随机样本，遵循<strong class="kw iu">总体模型</strong>。假设我们有多组样本数据(通过从总体中反复抽取样本)，并在每个数据集中分别运行模型。</p><p id="afe1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在给定的样本数据集中，我们将有一个OLS估计量，<strong class="kw iu"> β^ </strong>，它可以用封闭形式的解决方案来解决(图3)。</p><p id="8605" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">很有可能我们会在不同的数据集中得到<strong class="kw iu">一组不同的估计量(即β^) </strong>。因此，<strong class="kw iu"> β^是一个</strong> <strong class="kw iu">随机变量。</strong>基于<strong class="kw iu"> </strong>中心极限定理，<strong class="kw iu"> β^ </strong>的<strong class="kw iu">抽样分布</strong>具有均值，随着样本量的增加，均值收敛于<strong class="kw iu"> β </strong>。</p><p id="06d9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> E(β^) </strong>是这个随机变量β^.的期望值通俗地说，如果我们在多组样本中运行线性模型，不断记录估计量的值，取一个平均值。平均值就是期望值，E(β^).</p></div><div class="ab cl lq lr hx ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="im in io ip iq"><h2 id="0f8e" class="mf mg it bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">OLS估计量是无偏的</h2><p id="82ae" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">在<strong class="kw iu">有限样本</strong>属性下，我们说OLS估计量是无偏的，这意味着OLS估计量的<strong class="kw iu">期望值</strong>、<strong class="kw iu"> E(β^) </strong>将等于<strong class="kw iu">真实总体参数β </strong>。</p><blockquote class="nd ne nf"><p id="629b" class="ku kv ng kw b kx ky ju kz la lb jx lc nh le lf lg ni li lj lk nj lm ln lo lp im bi translated"><strong class="kw iu">无偏性</strong>难道<strong class="kw iu">而非</strong>意味着我们从<strong class="kw iu">观测数据</strong>(即一组随机样本)中得到的OLS估计量将等于<strong class="kw iu">精确总体</strong>参数值，因为由于<strong class="kw iu">不可约误差项ε </strong>，线性模型仍然不能完全解释这种关系。</p><p id="b5ba" class="ku kv ng kw b kx ky ju kz la lb jx lc nh le lf lg ni li lj lk nj lm ln lo lp im bi translated">相反，无偏性意味着，如果我们对来自同一总体的不同随机样本集重复运行线性回归模型<strong class="kw iu"/>，那么估计量的<strong class="kw iu">预期值</strong>将等于<strong class="kw iu">真实值</strong>总体参数，如下所示。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nk"><img src="../Images/9a8be6c8d647334fabed62f9bff9d8de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dS3AdPfgJwmtr6tw9I3_WA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图5(作者图片)</p></figure><p id="3d4f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">虽然我们从观测数据中得到的OLS估计量不等于<strong class="kw iu">精确总体</strong>参数值，但是只要<strong class="kw iu">观测数据是总体数据的良好代表，并且线性模型在假设下被正确指定，</strong>那么我们从观测数据中得到的系数估计量应该非常接近真实总体参数值。</p><p id="4911" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">否则，如果观察数据<strong class="kw iu">不是</strong>总体数据的良好代表，模型将遭受<strong class="kw iu">测量误差、</strong>或<strong class="kw iu">线性模型</strong>由于常见问题<strong class="kw iu">没有</strong>正确指定(例如，省略变量或内生性)，那么我们从观察数据得到的系数估计值将<strong class="kw iu">有偏差</strong>。</p></div><div class="ab cl lq lr hx ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="im in io ip iq"><h2 id="661e" class="mf mg it bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">OLS估计量是一致的</h2><blockquote class="nd ne nf"><p id="870b" class="ku kv ng kw b kx ky ju kz la lb jx lc nh le lf lg ni li lj lk nj lm ln lo lp im bi translated">在<strong class="kw iu">渐近</strong>性质下，我们说OLS估计量是一致的，这意味着随着样本量变大并趋向于<strong class="kw iu">无穷大，OLS估计量将<strong class="kw iu">收敛</strong>到<strong class="kw iu">真</strong>总体参数<strong class="kw iu">。</strong></strong></p></blockquote><p id="17e1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从Jeffrey Wooldridge的教科书，<em class="ng">计量经济学导论，C.3，</em>我们可以证明，如果假设成立，随着样本量变大，OLS估计量的概率极限将等于真实总体参数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nl"><img src="../Images/d36de55e0aca940981b258405b638313.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S36uqoSNEO9J6A6qbLXn4A.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图6(作者图片)</p></figure><ul class=""><li id="972e" class="nm nn it kw b kx ky la lb ld no lh np ll nq lp nr ns nt nu bi translated">当E[ <strong class="kw iu"> ε </strong> |X] = 0成立时，暗示Cov(X，u) = 0，那么图6中的第二项等于0。我们已经证明，随着样本容量的增大，OLS估计量会收敛到真实的总体参数。因此OLS估计量是一致的。</li><li id="fe18" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated">如果Cov(X，u) ≠ 0，那么我们有一个不一致的估计量。不一致的问题不会随着样本量的增加而消失。<a class="ae oa" rel="noopener" target="_blank" href="/causal-inference-with-linear-regression-endogeneity-9d9492663bac">同时，OLS估计量也有偏差。</a></li><li id="0ef5" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated">如果Cov(X，u) &gt; 0表示X与误差项正相关，则<strong class="kw iu">渐近偏差</strong>向上。</li><li id="350f" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated">如果Cov(X，u) &lt; 0 meaning x is negatively correlated with the error term, then <strong class="kw iu">渐近偏差</strong>向下。</li></ul><p id="6381" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你可能想知道为什么我们对大样本的属性感兴趣，比如一致性，而实际上我们的样本是有限的。</p><blockquote class="nd ne nf"><p id="d6a2" class="ku kv ng kw b kx ky ju kz la lb jx lc nh le lf lg ni li lj lk nj lm ln lo lp im bi translated">答案是，如果我们可以证明当样本量变大时，估计量是一致的，那么我们可能会对有限样本中的估计量更加自信和乐观。另一方面，<strong class="kw iu">如果一个估计量是不一致的，我们知道这个估计量在有限样本中是有偏的</strong>。</p></blockquote></div><div class="ab cl lq lr hx ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="im in io ip iq"><h2 id="4dff" class="mf mg it bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">OLS估计量是<strong class="ak">有效的</strong></h2><p id="923a" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">为了评估线性回归模型的估计量，我们根据其偏差和方差来使用其效率。</p><ul class=""><li id="6449" class="nm nn it kw b kx ky la lb ld no lh np ll nq lp nr ns nt nu bi translated">无偏但没有最小方差的估计量不是最好的。</li><li id="79da" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated">方差最小但有偏差的估计量不是最好的</li><li id="2b15" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated">无偏且方差最小的估计量是最好的(有效的)。</li><li id="0477" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated">OLS估计量是最好的(有效的)估计量，因为在所有的<strong class="kw iu">线性</strong>和<strong class="kw iu">无偏</strong>估计量中，OLS估计量具有<strong class="kw iu">最小方差</strong>。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ob"><img src="../Images/0d7b2ef87e9ece503000288f127b0276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iqt64_1zVW1D8Yk8Jko9sQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图7(作者图片)</p></figure><p id="4b3f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以用一点矩阵运算来证明高斯-马尔可夫定理。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi oc"><img src="../Images/3f726837014c9b080249605434a474ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yoXoqlONtlN34TpDg-1PVQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图8(作者图片)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi od"><img src="../Images/6a3bb8ed538a2b393c3dab082809f6fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dFGCR_CCHDD8aKFab6gp9w.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图9(作者图片)</p></figure><p id="b3eb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们已经证明了OLS估计量的方差比任何其他线性无偏估计量都小。因此，OLS是最好的(有效的)线性估计。</p></div><div class="ab cl lq lr hx ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="im in io ip iq"><h2 id="955d" class="mf mg it bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">最终注释</h2><ul class=""><li id="5fc8" class="nm nn it kw b kx my la mz ld oe lh of ll og lp nr ns nt nu bi translated">如果估计量的抽样分布的期望值等于真实总体参数值，则估计量是无偏的。</li><li id="4764" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated">如果随着样本量的增加，估计值趋于无穷大，那么估计值是一致的。换句话说——一致性意味着，随着样本量的增加，估计量的抽样分布在总体参数值处变得更加集中，方差变得更小。</li><li id="78f7" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated">在OLS假设下，OLS估计量是蓝色的(所有线性无偏估计量中方差最小的)。因此，它是<strong class="kw iu">最好的</strong> ( <strong class="kw iu">高效的</strong>)估计器。</li></ul><p id="835d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果你对<strong class="kw iu">线性回归</strong>和<strong class="kw iu">因果推断</strong>感兴趣，这里有一些相关的帖子可以浏览。</p><ul class=""><li id="fd00" class="nm nn it kw b kx ky la lb ld no lh np ll nq lp nr ns nt nu bi translated"><a class="ae oa" rel="noopener" target="_blank" href="/causal-inference-econometric-models-vs-a-b-testing-190781fe82c5"> <strong class="kw iu">因果推断:计量经济模型vs. A/B检验</strong> </a></li><li id="4a7d" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated"><a class="ae oa" rel="noopener" target="_blank" href="/linear-regression-vs-logistic-regression-ols-maximum-likelihood-estimation-gradient-descent-bcfac2c7b8e4"> <strong class="kw iu">线性回归与逻辑回归:OLS、最大似然估计、梯度下降</strong> </a></li><li id="8d14" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated"><a class="ae oa" rel="noopener" target="_blank" href="/linear-regression-with-ols-unbiased-consistent-blue-best-efficient-estimator-359a859f757e"><strong class="kw iu">OLS线性回归:无偏、一致、蓝色、最佳(有效)估计量</strong> </a></li><li id="4909" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated"><a class="ae oa" rel="noopener" target="_blank" href="/understand-bias-and-variance-in-causal-inference-with-linear-regression-a02e0a9622bc"> <strong class="kw iu">线性回归因果推断:省略变量和无关变量</strong> </a></li><li id="6084" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated"><a class="ae oa" rel="noopener" target="_blank" href="/causal-inference-with-linear-regression-endogeneity-9d9492663bac"> <strong class="kw iu">线性回归因果推断:内生性</strong> </a></li><li id="b05b" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated"><a class="ae oa" rel="noopener" target="_blank" href="/linear-regression-with-ols-heteroskedasticity-and-autocorrelation-c12f1f65c13"><strong class="kw iu">OLS线性回归:异方差和自相关</strong> </a></li></ul><h1 id="8c24" class="oh mg it bd mh oi oj ok mk ol om on mn jz oo ka mq kc op kd mt kf oq kg mw or bi translated">感谢您的阅读！！！</h1><p id="e7cc" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">如果你喜欢这篇文章，并且想<strong class="kw iu">请我喝杯咖啡，请</strong>点击这里。</p><p id="8eb9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">您可以注册一个<a class="ae oa" href="https://aaron-zhu.medium.com/membership" rel="noopener"> <strong class="kw iu">会员</strong> </a>来解锁我的文章的全部访问权限，并且可以无限制地访问介质上的所有内容。如果你想在我发表新文章时收到电子邮件通知，请订阅。</p></div></div>    
</body>
</html>