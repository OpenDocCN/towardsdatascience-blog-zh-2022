<html>
<head>
<title>GraphGPS: Navigating Graph Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GraphGPS:导航图形转换器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/graphgps-navigating-graph-transformers-c2cc223a051c#2022-06-14">https://towardsdatascience.com/graphgps-navigating-graph-transformers-c2cc223a051c#2022-06-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="2c42" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">图ML的最新进展</h2><div class=""/><div class=""><h2 id="da1f" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">烹饪最佳图形转换器的食谱</h2></div><p id="c877" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">2021年，图形转换器(GT)赢得了最近的分子性质预测挑战，这要归功于缓解了许多与普通消息传递GNNs有关的问题。在这里，我们试图将众多新开发的GT模型组织到一个单一的GraphGPS框架中，为所有类型的Graph ML任务提供通用的、强大的、可扩展的线性复杂度的图形转换器。事实证明，仅仅一辆调校良好的GT就足以展示SOTA在许多实际任务中的成果！</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/950df76f458e9b78ddf3e1dc75cb8ba0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gOUffNZRkXJB5utkz6hlWg.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">消息传递gnn、全连接图形转换器和位置编码。作者图片</p></figure><p id="5027" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="ma">这篇文章是与</em> <a class="ae mb" href="https://rampasek.github.io/" rel="noopener ugc nofollow" target="_blank"> <em class="ma">拉迪斯拉夫·拉帕塞克</em></a><em class="ma"/><a class="ae mb" href="https://twitter.com/dom_beaini?lang=en" rel="noopener ugc nofollow" target="_blank"><em class="ma">多米尼克·比艾尼</em> </a> <em class="ma">和</em> <a class="ae mb" href="https://vijaydwivedi.com.np/" rel="noopener ugc nofollow" target="_blank"> <em class="ma">维贾伊·普拉卡什·德威迪</em> </a> <em class="ma">共同撰写的，是基于论文</em> <a class="ae mb" href="https://arxiv.org/abs/2205.12454" rel="noopener ugc nofollow" target="_blank"> <em class="ma">的一个通用、强大、可扩展的图形转换器(2022) </em> </a> 【T24)</p><p id="f311" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">大纲:</p><ol class=""><li id="3d10" class="mc md iq kq b kr ks ku kv kx me lb mf lf mg lj mh mi mj mk bi translated"><a class="ae mb" href="#675f" rel="noopener ugc nofollow">消息传递GNNs vs图形转换器</a></li><li id="5163" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj mh mi mj mk bi translated"><a class="ae mb" href="#3975" rel="noopener ugc nofollow">图形转换器的优缺点和种类</a></li><li id="e7d7" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj mh mi mj mk bi translated"><a class="ae mb" href="#40a9" rel="noopener ugc nofollow">graph GPS框架</a></li><li id="10eb" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj mh mi mj mk bi translated"><a class="ae mb" href="#e373" rel="noopener ugc nofollow">概要:蓝图</a></li><li id="849d" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj mh mi mj mk bi translated"><a class="ae mb" href="#cf65" rel="noopener ugc nofollow">功能强大；结构和位置特点</a></li><li id="9882" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj mh mi mj mk bi translated"><a class="ae mb" href="#6be7" rel="noopener ugc nofollow">可扩展:线性变压器</a></li><li id="fa2e" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj mh mi mj mk bi translated"><a class="ae mb" href="#beae" rel="noopener ugc nofollow">烹饪时间——如何充分利用你的GT </a></li></ol></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="675f" class="mx my iq bd mz na nb nc nd ne nf ng nh kf ni kg nj ki nk kj nl kl nm km nn no bi translated"><strong class="ak">消息传递GNNs vs图形转换器</strong></h1><p id="cd2f" class="pw-post-body-paragraph ko kp iq kq b kr np ka kt ku nq kd kw kx nr kz la lb ns ld le lf nt lh li lj ij bi translated">消息传递GNN(通常从<a class="ae mb" rel="noopener" target="_blank" href="/graph-neural-networks-beyond-weisfeiler-lehman-and-vanilla-message-passing-bc8605fa59a"> Weisfeiler-Leman的角度</a>进行分析)众所周知地遭受<a class="ae mb" href="https://openreview.net/forum?id=S1ldO2EFPr" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">过度平滑</strong> </a>(增加GNN层数，特征倾向于收敛到相同的值)<a class="ae mb" href="https://openreview.net/forum?id=i80OPhOCVH2" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">过度挤压</strong> </a>(当试图将来自许多邻居的消息聚集到单个向量时丢失信息)，并且可能最重要的是，在小而稀疏的分子图上已经明显的长程相关性捕捉不佳。</p><p id="eae9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">今天，我们知道了许多打破消息传递玻璃天花板的方法——包括<a class="ae mb" rel="noopener" target="_blank" href="/using-subgraphs-for-more-expressive-gnns-8d06418d5ab">高阶GNNs </a>，更好地<a class="ae mb" href="https://openreview.net/forum?id=7UmjRGzp-A" rel="noopener ugc nofollow" target="_blank">理解图拓扑</a>，<a class="ae mb" rel="noopener" target="_blank" href="/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774">扩散模型</a>，<a class="ae mb" href="https://arxiv.org/abs/2110.09443" rel="noopener ugc nofollow" target="_blank">图重布线</a>，以及<a class="ae mb" href="https://arxiv.org/abs/2012.09699" rel="noopener ugc nofollow" target="_blank">图转换器</a>！</p><p id="b8a5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在消息传递方案中，一个节点的更新是其<strong class="kq ja">邻居</strong>的函数，而在GTs中，一个节点的更新是图中所有<strong class="kq ja">节点的函数(由于Transformer层中的自关注机制)。也就是说，GT实例的输入就是整个图形。</strong></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi nu"><img src="../Images/af1df1919d00483b21db2c92b84287ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1OG1LhGV9klzHkOCCUYF3Q.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">更新目标节点表示(红色)，局部消息传递仅聚集直接邻居，而全局注意力是图中所有节点的函数。在GraphGPS中，我们将两者结合起来！作者图片</p></figure></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="3975" class="mx my iq bd mz na nb nc nd ne nf ng nh kf ni kg nj ki nk kj nl kl nm km nn no bi translated"><strong class="ak">图形转换器的利与弊</strong></h1><p id="beb2" class="pw-post-body-paragraph ko kp iq kq b kr np ka kt ku nq kd kw kx nr kz la lb ns ld le lf nt lh li lj ij bi translated">将整个图形输入到Transformer层会带来一些直接的好处和缺点。</p><p id="2e56" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">✅优点:</p><ul class=""><li id="7772" class="mc md iq kq b kr ks ku kv kx me lb mf lf mg lj nv mi mj mk bi translated">类似于图的重新布线，我们现在将节点更新过程从图结构中分离出来。</li><li id="8915" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj nv mi mj mk bi translated">处理远程连接没有问题，因为所有节点现在都相互连接在一起(我们经常分离来自原始图的<em class="ma">真实</em>边和在计算关注矩阵时添加的<em class="ma">虚拟</em>边——查看上面的插图，其中实线表示真实边，虚线表示虚拟边)。</li><li id="e0a6" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj nv mi mj mk bi translated">用“在迷宫中导航”来类比，我们可以使用地图，摧毁迷宫的墙壁，并使用魔法翅膀，而不是四处走动和寻找🦋。不过，我们必须事先学习地图，稍后我们将看到如何使导航更加精确和高效。</li></ul><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi nw"><img src="../Images/f79495a71a6a1266ff7c6d20c44b467a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fD-jDP5ewyvC7AlMq_2pPw.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">图表变形金刚给你翅膀🦋。来源:<a class="ae mb" href="https://twitter.com/dom_beaini/status/1499019741234704385" rel="noopener ugc nofollow" target="_blank">多米尼克·比艾尼@推特</a></p></figure><p id="1ab2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">🛑缺点类似于在NLP中使用变压器产生的缺点:</p><ul class=""><li id="6c1d" class="mc md iq kq b kr ks ku kv kx me lb mf lf mg lj nv mi mj mk bi translated">语言输入是连续的，而图对于节点排序是排列不变的。我们需要图中节点更好的<strong class="kq ja">可识别性</strong>——这通常通过某种形式的位置特征来实现。例如，在NLP中，<a class="ae mb" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">原始变换器</a>使用正弦位置特征作为序列中令牌的绝对位置，而最近的<a class="ae mb" href="https://openreview.net/forum?id=R8sQPpGCv0" rel="noopener ugc nofollow" target="_blank"> AliBi </a>引入了相对位置编码方案。</li><li id="7f95" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj nv mi mj mk bi translated">失去使GNNs在具有<strong class="kq ja">明显局部性</strong>的图形上工作良好的归纳偏差，这是许多真实世界图形的情况。特别是在那些边代表相关/接近的地方。通过重新连接图形使其完全连接，我们必须以某种方式将结构放回原处，否则，我们很可能会“将婴儿与水一起扔掉”。</li><li id="7d4b" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj nv mi mj mk bi translated">最后但并非最不重要的是，限制可以是节点数量的<strong class="kq ja">平方</strong>计算<strong class="kq ja">复杂度</strong> O(N ),而消息传递gnn是边数量O(E)的线性。图形通常是稀疏的，即N ≈ E，因此计算负担随着较大的图形而增加。我们能做点什么吗？</li></ul><p id="d1c8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">2021年为GTs带来了各种各样的位置和结构特征，使节点更容易区分。</p><p id="42cf" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在graph transformers中，Dwivedi &amp; Bresson 的<a class="ae mb" href="https://arxiv.org/abs/2012.09699" rel="noopener ugc nofollow" target="_blank">第一个GT架构使用拉普拉斯<strong class="kq ja">特征向量</strong>作为位置编码，等人的</a>的【SAN】也增加了拉普拉斯<strong class="kq ja">特征值</strong>相应地重新加权注意力，Ying等人的<a class="ae mb" href="https://arxiv.org/pdf/2106.05234.pdf" rel="noopener ugc nofollow" target="_blank">graph former</a>增加了<strong class="kq ja">最短路径距离</strong>作为注意力偏差，Wu、Jain等人的graph trans在一个图通过后运行GT</p><p id="4a68" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在图的位置特征领域，除了拉普拉斯导出的特征，最近的一批工作包括Dwivedi等人的<a class="ae mb" href="https://arxiv.org/abs/2110.07875" rel="noopener ugc nofollow" target="_blank">随机行走结构编码(RWSE)采用m次幂随机行走矩阵的对角，Lim，Robinson等人的</a><a class="ae mb" href="https://arxiv.org/abs/2202.13013" rel="noopener ugc nofollow" target="_blank">sign net</a>确保拉普拉斯特征向量的符号不变性，以及Wang等人的<a class="ae mb" href="https://openreview.net/pdf?id=e95i1IHcWj" rel="noopener ugc nofollow" target="_blank">等变和稳定PEs</a>分别确保节点和位置特征的排列和旋转等变。</p><p id="b530" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">嗯，他们太多了🤯我如何知道什么最适合我的任务？</p><blockquote class="nx"><p id="10ac" class="ny nz iq bd oa ob oc od oe of og lj dk translated">是否有一种原则性的方法来组织和使用所有这些图形转换器图层和位置要素？</p></blockquote><p id="b3b2" class="pw-post-body-paragraph ko kp iq kq b kr oh ka kt ku oi kd kw kx oj kz la lb ok ld le lf ol lh li lj ij bi translated">是啊！这就是我们在最近的论文<a class="ae mb" href="https://arxiv.org/abs/2205.12454" rel="noopener ugc nofollow" target="_blank">中提出的通用、强大、可扩展的图形转换器</a>的配方。</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="40a9" class="mx my iq bd mz na nb nc nd ne nf ng nh kf ni kg nj ki nk kj nl kl nm km nn no bi translated"><strong class="ak">graph GPS框架</strong></h1><p id="76ca" class="pw-post-body-paragraph ko kp iq kq b kr np ka kt ku nq kd kw kx nr kz la lb ns ld le lf nt lh li lj ij bi translated">在GraphGPS中，GPS代表:</p><p id="5efd" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> 🧩综合医院</strong>——我们提出了一个构建图形转换器的蓝图，将特征(预)处理、本地消息传递和全局注意力的模块合并到一个管道中</p><p id="35d5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">🏆强大的</strong> —当与适当的位置和结构特征配对时，GPS图形转换器被证明比1-WL测试更强大</p><p id="b696" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">📈可扩展的</strong> —我们引入了线性全局注意力模块，突破了长期以来只在分子图上运行图转换器的问题(平均少于100个节点)。现在我们可以在每张有数千个节点的图上做这件事！</p><p id="60ae" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">或者可能是指有位置和结构的图形？😉</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="e37e" class="mx my iq bd mz na nb nc nd ne nf ng nh kf ni kg nj ki nk kj nl kl nm km nn no bi translated"><strong class="ak">将军:蓝图</strong></h1><p id="b112" class="pw-post-body-paragraph ko kp iq kq b kr np ka kt ku nq kd kw kx nr kz la lb ns ld le lf nt lh li lj ij bi translated">如果我们可以利用这两个世界的优点，为什么我们需要修补消息传递gnn和图形转换器来实现某个特性呢？让模型决定对于一组给定的任务和图表什么是重要的。一般来说，蓝图可以用一张图来描述:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi om"><img src="../Images/68135fc535f6e04763db406ea628d8d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QKN2j0vBNS8fF-W2EuW5NQ.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">GraphGPS蓝图提出了一种模块化架构，用于构建具有各种位置、结构特征以及局部和全局关注的图形转换器。来源:<a class="ae mb" href="https://arxiv.org/abs/2205.12454" rel="noopener ugc nofollow" target="_blank"> arxiv </a>。点击放大</p></figure><p id="4387" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">它看起来有点庞大，所以让我们把它一部分一部分地分解，看看那里发生了什么。</p><p id="c0d2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">总的来说，蓝图由三个主要部分组成:</p><ol class=""><li id="b4a4" class="mc md iq kq b kr ks ku kv kx me lb mf lf mg lj mh mi mj mk bi translated">通过位置和结构编码的节点标识。在分析了最近发表的许多增加图中位置性的方法后，我们发现它们可以大致分为三类:<strong class="kq ja">局部</strong>、<strong class="kq ja">全局</strong>和<strong class="kq ja">相对</strong>。这些特性非常强大，有助于克服臭名昭著的1-WL限制。更多信息请见下文！</li><li id="96b2" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj mh mi mj mk bi translated">具有原始图表要素的结点标识的聚合-即输入结点、边和图表要素。</li><li id="7839" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj mh mi mj mk bi translated">处理层(GPS层)-我们如何实际处理具有构造要素的图表，这里结合了本地消息传递(任何MPNNs)和全局注意力模型(任何图表转换器)</li><li id="abcb" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj mh mi mj mk bi translated">(奖金🎁)您可以将任何位置和结构特征与我们基于<a class="ae mb" href="https://www.pyg.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch-Geometric </a>的新<a class="ae mb" href="https://github.com/rampasek/GraphGPS" rel="noopener ugc nofollow" target="_blank"> GraphGPS </a>库中的任何处理层相结合！</li></ol></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="cf65" class="mx my iq bd mz na nb nc nd ne nf ng nh kf ni kg nj ki nk kj nl kl nm km nn no bi translated"><strong class="ak">功能强大；结构和位置特征</strong></h1><p id="31b7" class="pw-post-body-paragraph ko kp iq kq b kr np ka kt ku nq kd kw kx nr kz la lb ns ld le lf nt lh li lj ij bi translated">结构和位置特征旨在对每个节点或边的独特特征进行编码。在最基本的情况下(如下图所示)，当所有节点都具有相同的初始特征或根本没有特征时，应用位置和结构特征有助于区分图中的节点，为它们分配不同的特征，并至少提供某种意义上的图结构。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi on"><img src="../Images/c5b1e2fcd469af675de556d26a6ab94c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uc094aOMW4zd0Ycng0H2Xw.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">结构和位置特征有助于区分图中的节点。作者图片。</p></figure><p id="7f56" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们通常将<em class="ma">位置</em>与<em class="ma">结构</em>特征分开(尽管在理论上也有类似于<a class="ae mb" href="https://arxiv.org/abs/1910.00452" rel="noopener ugc nofollow" target="_blank">斯里尼瓦桑&amp;里贝罗</a>的作品)。</p><blockquote class="nx"><p id="145c" class="ny nz iq bd oa ob oc od oe of og lj dk translated">直观地，位置特征帮助节点回答问题<strong class="ak">“我在哪里？”当结构特征回答“我的邻居看起来像什么？”</strong></p></blockquote><p id="d720" class="pw-post-body-paragraph ko kp iq kq b kr oh ka kt ku oi kd kw kx oj kz la lb ok ld le lf ol lh li lj ij bi translated"><strong class="kq ja">位置编码(PEs) </strong>提供了图形中给定节点的空间位置的一些概念。它们帮助一个节点回答问题<strong class="kq ja">“我在哪里？”</strong>。理想情况下，我们希望每个节点都有某种笛卡尔坐标，但是由于图是拓扑结构，并且在2D平面上有无限多种定位图的方法，我们必须考虑一些不同的方法。谈到PEs，我们将现有的(和理论上可能的)方法分为3个分支——局部PEs、全局PEs和相对PEs。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi oo"><img src="../Images/1b38681538f2542776b74a3353088fe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lfGeiGoxi1Yyf2BzdYrKWQ.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">位置编码(PE)的分类。点击放大。作者图片。</p></figure><p id="d9c6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">👉局部PE(作为节点特征)—在一个<strong class="kq ja">簇</strong>内，两个节点彼此越接近，它们的局部PE就越接近，比如一个单词在句子中的位置(而不是在文本中)。例子:(1)节点和包含该节点的簇的质心之间的距离；(2)m步随机游走矩阵的非对角元素之和(m次方)。</p><p id="7d4d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">👉全局PEs(作为节点特征)—在一个<strong class="kq ja">图</strong>中，两个节点越接近，它们的全局PEs就越接近，例如一个单词在文本中的位置。例子:(1)原<a class="ae mb" href="https://arxiv.org/abs/2012.09699" rel="noopener ugc nofollow" target="_blank">图变换器</a>和<a class="ae mb" href="https://arxiv.org/abs/2106.03893" rel="noopener ugc nofollow" target="_blank">三</a>中使用的邻接或拉普拉斯的特征向量；(2)到整个图形的质心的距离；(3)每个连接组件的唯一标识符</p><p id="3594" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">👉相对PE(作为边要素)-与任何局部或全局PE给出的距离相关的边表示，例如两个世界之间的距离。例子:(1)从热核、随机游走、<a class="ae mb" href="https://en.wikipedia.org/wiki/Green%27s_function" rel="noopener ugc nofollow" target="_blank">格林函数</a>、图测地线得到的成对距离；(2)邻接或拉普拉斯的特征向量的梯度，或者任何局部/全局PEs的梯度。</p><p id="353c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们来看一个关于这个著名分子☕️的各种PE的例子(<a class="ae mb" href="https://youtu.be/w6Pw4MOzMuo?t=387" rel="noopener ugc nofollow" target="_blank">根据他的iclr’21主题演讲</a>，这是迈克尔布朗斯坦最喜欢的分子)。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi op"><img src="../Images/c92d38581739b1afbfeea1f03c0efe25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PvFUVl8Avp6PuPEV8jhGGA.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">咖啡因分子☕️.的局部、全局和相对位置编码的图示作者图片</p></figure><p id="24cd" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">结构编码</strong> <strong class="kq ja"> (SEs) </strong>提供了图和子图的结构表示。它们帮助一个节点回答问题<strong class="kq ja">“我的邻居看起来像什么？”</strong>。类似地，我们将可能的SEs分为局部、全局和相对，尽管是在不同的酱料下。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi or"><img src="../Images/fc33154e9dbc44a796d5c1e67d259634.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L-xk6SYPVpPMiz5Fjxt0Sw.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">结构编码的分类。点击放大。作者图片。</p></figure><p id="b52c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">👉本地SEs(作为节点特征)允许节点理解它是什么子结构的一部分。也就是说，给定两个节点和半径为m的se，这些节点周围的m跳子图越相似，它们的局部SE就越接近。例子:(1)节点度(用于<a class="ae mb" href="https://arxiv.org/pdf/2106.05234.pdf" rel="noopener ugc nofollow" target="_blank">graphformer</a>)；(2)m步随机行走矩阵的对角线<a class="ae mb" href="https://arxiv.org/abs/2110.07875" rel="noopener ugc nofollow" target="_blank">(RWSE)</a>；③<a class="ae mb" href="https://openreview.net/forum?id=7UmjRGzp-A" rel="noopener ugc nofollow" target="_blank">瑞西曲率</a>；(4)枚举或统计三角形和环形等子结构(<a class="ae mb" href="https://arxiv.org/abs/2006.09252" rel="noopener ugc nofollow" target="_blank">图子结构网络</a>，<a class="ae mb" href="https://openreview.net/forum?id=Mspk_WYKoEH" rel="noopener ugc nofollow" target="_blank"> GNN为核</a>)。</p><p id="8b4a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">👉全局SEs(作为图形特征)向网络提供关于图形的全局结构的信息。如果我们比较两个图，如果它们的结构相似，它们的全局SEs将是接近的。例子:(1)邻接或拉普拉斯算子的特征值(用于<a class="ae mb" href="https://arxiv.org/abs/2106.03893" rel="noopener ugc nofollow" target="_blank">三</a>)；(2)众所周知的图的性质，如直径、连通分支数、围长、平均度。</p><p id="21ea" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">👉相对SEs(作为边特征)允许两个节点了解它们的结构有多少不同。当两个节点在同一个子结构中时，这些可以是任何局部se的梯度或布尔指示器(例如，在<a class="ae mb" href="https://arxiv.org/abs/2006.09252" rel="noopener ugc nofollow" target="_blank"> GSN </a>中)。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi os"><img src="../Images/d2ee9a51a41d67e90d5384297ace0375.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A5ZCFTBEmKsdf4aKKIKhNg.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">咖啡因分子☕️.的局部、整体和相对结构编码的图解作者图片</p></figure><p id="4238" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">根据图的结构，位置和结构特征可能会带来许多超越1-WL极限的表达能力。例如，在高度规则的<a class="ae mb" href="https://github.com/PurdueMINDS/RelationalPooling/tree/master/" rel="noopener ugc nofollow" target="_blank">循环跳跃链接(CSL) </a>图中，拉普拉斯的特征向量(我们框架中的全局PEs)为CSL (11，2)和CSL (11，3)图分配唯一且不同的节点特征，使它们明显可区分(其中1-WL失败)。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi ot"><img src="../Images/4d9bd142d24e692e612c2594e05fd66d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ztohsfwxp2rB9naHmoAj_A.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">位置(PEs)和结构(SEs)特征比1-WL更强大，但它们的有效性可能取决于图形的性质。作者图片。</p></figure><p id="91c1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">PEs和SEs的集合</strong></p><p id="5b2e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">考虑到PEs和SEs在不同的场景中可能是有益的，为什么你要用一个位置/结构特征来限制模型呢？</p><p id="3224" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在GraphGPS中，我们允许组合任意数量的PE和SE，例如，16个拉普拉斯特征向量+特征值+ 8d RWSE。在预处理中，每个节点可能有许多向量，我们使用集合聚合函数将它们映射到要添加到节点特征的单个向量。</p><p id="5a3e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">该机制允许使用<a class="ae mb" rel="noopener" target="_blank" href="/using-subgraphs-for-more-expressive-gnns-8d06418d5ab">子图GNNs </a>、<a class="ae mb" href="https://arxiv.org/abs/2202.13013" rel="noopener ugc nofollow" target="_blank">符号网</a>、<a class="ae mb" href="https://openreview.net/pdf?id=e95i1IHcWj" rel="noopener ugc nofollow" target="_blank">等变和稳定PEs </a>、<a class="ae mb" href="https://arxiv.org/abs/2202.03036" rel="noopener ugc nofollow" target="_blank">k-子树SATs </a>以及其他将节点特征构建为复杂聚合函数的模型。</p><p id="709b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，有了表达性的位置和结构特性，我们可以应对最后的挑战——可伸缩性。</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="6be7" class="mx my iq bd mz na nb nc nd ne nf ng nh kf ni kg nj ki nk kj nl kl nm km nn no bi translated"><strong class="ak">可扩展:线性变压器🚀</strong></h1><p id="8247" class="pw-post-body-paragraph ko kp iq kq b kr np ka kt ku nq kd kw kx nr kz la lb ns ld le lf nt lh li lj ij bi translated">几乎所有现有的图转换器都采用标准的自关注机制，将整个N矩阵具体化为N个节点的图(因此假设该图是完全连通的)。一方面，它允许向GTs注入边缘特征(像在<a class="ae mb" href="https://arxiv.org/pdf/2106.05234.pdf" rel="noopener ugc nofollow" target="_blank">图形成器</a>中一样，该图形成器使用边缘特征作为注意力偏差)并将真实边缘与虚拟边缘分开(像在<a class="ae mb" href="https://arxiv.org/abs/2106.03893" rel="noopener ugc nofollow" target="_blank"> SAN </a>中一样)。另一方面，具体化注意力矩阵的平方复杂度为O(N ),这使得GTs很难扩展到超过50-100个节点的分子图。</p><p id="338a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">幸运的是，NLP中围绕变压器的大量研究最近提出了许多线性变压器架构，如<a class="ae mb" href="https://arxiv.org/abs/2006.04768" rel="noopener ugc nofollow" target="_blank"> Linformer </a>、<a class="ae mb" href="https://openreview.net/forum?id=Ua6zuk0WRH" rel="noopener ugc nofollow" target="_blank"> Performer </a>、<a class="ae mb" href="https://arxiv.org/abs/2007.14062" rel="noopener ugc nofollow" target="_blank"> BigBird </a>，以线性地将注意力扩展到输入序列O(N)。整个<a class="ae mb" href="https://arxiv.org/abs/2011.04006" rel="noopener ugc nofollow" target="_blank">远程竞技场</a>基准测试旨在评估超长序列的线性变压器。线性转换器的本质是绕过全注意力矩阵的计算，而是通过各种数学“技巧”来近似其结果，如Linformer中的低秩分解或Performer中的softmax内核近似。一般来说，这是一个非常活跃的研究领域🔥我们预计不久将会有越来越多有效的方法出现。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi ou"><img src="../Images/394e7e7035f3a308b971142e2974c9a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tcPpbRskq_tr7DlknezIYA.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">普通的N变压器(左)实现了完整的注意力矩阵，而线性变压器(右)通过各种近似绕过了这一阶段，而没有明显的性能损失。作者图片。</p></figure><p id="c536" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">有趣的是，关于图转换器的线性注意力模型的研究并不多——到目前为止，我们只知道Choromanski等人最近的<a class="ae mb" href="https://arxiv.org/abs/2107.07999" rel="noopener ugc nofollow" target="_blank"> ICML 2022工作，不幸的是，这些工作没有在相当大的图上进行实验。</a></p><p id="89ff" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在GraphGPS中，我们建议用几乎任何可用的线性注意力模块来代替全局注意力模块(普通的变压器)。运用线性注意力会引出两个重要的问题，这两个问题需要我们做大量的实验来回答:</p><ol class=""><li id="f1e9" class="mc md iq kq b kr ks ku kv kx me lb mf lf mg lj mh mi mj mk bi translated">既然没有显式的注意矩阵计算，如何融入边缘特征？我们在GTs中到底需要不需要边缘特性？<br/> <strong class="kq ja">回答</strong>:根据经验，在我们基准测试的数据集上，我们发现GraphGPS中的线性<em class="ma">全局</em>注意力即使在没有边缘特征的情况下也能很好地工作(假设边缘特征是由一些经过GNN的<em class="ma">局部</em>消息处理的)。此外，我们从理论上证明了当输入节点特征已经编码了边缘特征时，线性全局注意不会丢失边缘信息。</li><li id="bbc5" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj mh mi mj mk bi translated">线性注意力模型的速度和性能之间的权衡是什么？<br/> <strong class="kq ja">回答</strong>:权衡是相当有利的——我们在从方形注意力模型切换到线性注意力模型时没有发现重大的性能下降，但发现了巨大的内存提升。也就是说，至少在当前的基准测试中，您可以简单地将全部注意力转移到大得多的图上的线性和训练模型，而不会有巨大的性能损失。尽管如此，如果我们想对线性整体注意力表现更有把握，就需要有更大的图表和长期依赖性的更大的基准。</li></ol></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="beae" class="mx my iq bd mz na nb nc nd ne nf ng nh kf ni kg nj ki nk kj nl kl nm km nn no bi translated"><strong class="ak">👨‍🍳烹饪时间——如何让你的GT发挥最大功效</strong></h1><p id="8f6c" class="pw-post-body-paragraph ko kp iq kq b kr np ka kt ku nq kd kw kx nr kz la lb ns ld le lf nt lh li lj ij bi translated">长话短说——一个经过调整的GraphGPS，结合了本地和全球的关注，与更复杂和计算更昂贵的模型相比表现非常有竞争力，并在许多基准上树立了一个新的SOTA！</p><p id="f907" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">例如，在分子回归基准锌中，GraphGPS达到历史新低0.07 MAE。该领域的进步非常快—去年的SAN确实设定了0.139的SOTA，因此我们将错误率提高了50%！📉</p><p id="5253" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">此外，由于高效的实施，我们显著提高了图形转换器的速度——大约快了400%🚀—ogbg-molp CBA上的时间为196 s/epoch，而之前的SOTA图形转换器型号SAN的时间为883 s/epoch。</p><p id="2265" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们使用<a class="ae mb" href="https://openreview.net/forum?id=Ua6zuk0WRH" rel="noopener ugc nofollow" target="_blank"> Performer </a>和<a class="ae mb" href="https://arxiv.org/abs/2007.14062" rel="noopener ugc nofollow" target="_blank"> BigBird </a>作为线性全局注意力模型进行实验，并将GraphGPS扩展到多达10，000个节点的图形，适合标准32 GB GPU，这是以前任何图形转换器都无法实现的。</p><p id="8a95" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最后，我们开源了<a class="ae mb" href="https://github.com/rampasek/GraphGPS" rel="noopener ugc nofollow" target="_blank"> GraphGPS库</a>(类似于<a class="ae mb" href="https://arxiv.org/abs/2011.08843" rel="noopener ugc nofollow" target="_blank"> GraphGym </a>环境)，在这里您可以轻松地插入、组合和配置:</p><ul class=""><li id="cff0" class="mc md iq kq b kr ks ku kv kx me lb mf lf mg lj nv mi mj mk bi translated">具有和不具有边缘特征的任何本地消息传递模型</li><li id="6c1c" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj nv mi mj mk bi translated">任何全局注意力模型，例如，全变压器或任何线性架构</li><li id="8018" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj nv mi mj mk bi translated">任何结构(SE)和位置(PE)编码方法</li><li id="eaf8" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj nv mi mj mk bi translated">SEs和PEs的任意组合，例如拉普拉斯PEs和随机游走RWSE！</li><li id="d6a0" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj nv mi mj mk bi translated">聚合se和PE的任何方法，如SignNet或DeepSets</li><li id="9d75" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj nv mi mj mk bi translated">在PyG支持的任何图形数据集上运行它，或者使用自定义包装器运行它</li><li id="077c" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj nv mi mj mk bi translated">使用Wandb跟踪进行大规模实验</li><li id="94ae" class="mc md iq kq b kr ml ku mm kx mn lb mo lf mp lj nv mi mj mk bi translated">当然，还要复制我们的实验结果</li></ul><p id="2246" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">📜arxiv预印本:<a class="ae mb" href="https://arxiv.org/abs/2205.12454" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2205.12454</a></p><p id="4b6b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">🔧Github回购:<a class="ae mb" href="https://github.com/rampasek/GraphGPS" rel="noopener ugc nofollow" target="_blank">https://github.com/rampasek/GraphGPS</a></p></div></div>    
</body>
</html>