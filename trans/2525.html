<html>
<head>
<title>How Random Forests &amp; Decision Trees Decide: Simply Explained With An Example In Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机森林和决策树是如何决定的:用Python简单解释了一个例子</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-do-random-forests-decision-trees-decide-simply-explained-with-an-example-in-python-6737eb183604#2022-06-01">https://towardsdatascience.com/how-do-random-forests-decision-trees-decide-simply-explained-with-an-example-in-python-6737eb183604#2022-06-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4ba6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">什么是决策树？什么是随机森林？他们如何决定分割数据？他们是如何在分类任务中预测类别标签的？9分钟解释一切。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/423521fa99658fe80d2b331e21d75974.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qEvu9GBt6tnvly2MyoEqPw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一个由3棵决策树组成的随机森林的可视化例子(<a class="ae ky" href="https://blog.tensorflow.org/2021/05/introducing-tensorflow-decision-forests.html" rel="noopener ugc nofollow" target="_blank"> Source </a>)。</p></figure><p id="1b64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">嗨伙计们！我希望你一切都好。我早就想写这个帖子了，所以现在就来了！我们开始吧！</p><h1 id="54e4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">介绍</h1><ul class=""><li id="645c" class="mn mo it lb b lc mp lf mq li mr lm ms lq mt lu mu mv mw mx bi translated">随机森林是<strong class="lb iu">一种用于分类、回归和其他任务的集成学习方法[1]。</strong>它是一种非常著名的机器学习(ML)算法，被广泛应用！</li><li id="3e54" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated"><strong class="lb iu">每个随机森林由多个决策树</strong>(有道理；一个森林包含许多树！).</li><li id="1ac3" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated">在<strong class="lb iu">分类</strong> <strong class="lb iu">任务</strong>中，随机森林的<strong class="lb iu">输出</strong>是<strong class="lb iu">样本</strong>的<strong class="lb iu">类</strong>。例如，您可以使用动物图像作为输入(以矢量化的形式)并预测动物种类(狗或猫)。</li><li id="58af" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated">在<strong class="lb iu">分类</strong> <strong class="lb iu">任务中，</strong>我们需要有一个<strong class="lb iu">标签为</strong>的数据集。这是指<strong class="lb iu">知道</strong>每个<strong class="lb iu">样本</strong>(上例中的猫/狗)的<strong class="lb iu">标签</strong> / <strong class="lb iu">类别</strong>。</li></ul><p id="db51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们了解<strong class="lb iu">随机</strong> <strong class="lb iu">森林</strong> (RF)如何工作和决策之前，我们需要先了解什么是<strong class="lb iu">决策</strong> <strong class="lb iu">树</strong>以及它们是如何工作的。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="8ce3" class="lv lw it bd lx ly nk ma mb mc nl me mf jz nm ka mh kc nn kd mj kf no kg ml mm bi translated">决策树</h1><h2 id="a11b" class="np lw it bd lx nq nr dn mb ns nt dp mf li nu nv mh lm nw nx mj lq ny nz ml oa bi translated">示例1:理想情况</h2><p id="42e7" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li ob lk ll lm oc lo lp lq od ls lt lu im bi translated">假设我们有一个标记为 <strong class="lb iu">的<strong class="lb iu">数据集</strong>，总共有<strong class="lb iu"> 10个样本</strong>。这些样品中的5个属于狗类(蓝色)，其余5个属于猫类(红色)。此外，我们假设我们只有2个特征/变量，因此我们的变量空间是2D。</strong></p><p id="5594" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是创建数据集并绘制数据集的一些Python代码:</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="25f7" class="np lw it of b gy oj ok l ol om">X , y = make_classification(n_samples=10, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, random_state=0)</span><span id="2977" class="np lw it of b gy on ok l ol om">c = ["blue" if i==0 else "red" for i in y]</span><span id="edd3" class="np lw it of b gy on ok l ol om">plt.figure(dpi=200)<br/>plt.scatter(X[:,0], X[:,1], c=c)<br/>plt.xlabel("x")<br/>plt.ylabel("y")<br/>plt.axvline(x=0.9, label="split at x=0.9", c = "k", linestyle="--")<br/>plt.legend()<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/e85491d497a9c5aeb07bbb05922d1958.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*97TPSFO3xSsDXJuLw_C3ng.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:数据集的例子。作者用python制作的图。</p></figure><p id="4cdc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">决策</strong> <strong class="lb iu">树</strong>做的事情很简单:它们找到方法<strong class="lb iu">分割</strong>数据，例如<strong class="lb iu">尽可能多地分离<strong class="lb iu">类</strong>的样本</strong>(增加类的可分离性)。</p><p id="8b64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的例子中，<strong class="lb iu">完美</strong> <strong class="lb iu">分裂</strong>将是在<strong class="lb iu"> x=0.9 </strong>处的分裂，因为这将导致5个红色点在左侧，5个蓝色点在右侧(<strong class="lb iu">完美</strong> <strong class="lb iu">类</strong> <strong class="lb iu">可分离性</strong>)。每次我们像这样分割空间/数据时，我们实际上构建了一个具有特定规则的决策树。</p><p id="bc2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树将如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/6292cfca1f1b6994a2a8cbe29aa82e6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qA6CDs-CZLBXvzNDrhw8Sw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:决策树。在x=0.9时分割。作者在<a class="ae ky" href="https://www.smartdraw.com/" rel="noopener ugc nofollow" target="_blank"> smartdraw </a>中制作的图。</p></figure><p id="de1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们最初有包含所有数据的<strong class="lb iu">根</strong> <strong class="lb iu">节点</strong>，然后，我们在<strong class="lb iu"> x=0.9 </strong>处分割数据，导致两个<strong class="lb iu">分支</strong>，导致两个<strong class="lb iu">叶</strong> <strong class="lb iu">节点</strong>。由于我们不能进一步分割数据(我们不能添加新的决策节点，因为数据已经完全分割)，决策树构造到此结束。</p><p id="3c1f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">无需在第二特征维度，即<strong class="lb iu"> y </strong>轴上分割数据。</p><p id="e4cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">进行新的预测:</strong>如果我们将传递一个<strong class="lb iu">新的</strong> <strong class="lb iu">样本</strong>(例如，来自测试集)，该样本具有一个特征/变量<strong class="lb iu"> x=5 </strong>，它将在左叶节点结束，并将被分类为<strong class="lb iu">类</strong><strong class="lb iu">红色</strong>。</p><blockquote class="oq"><p id="830e" class="or os it bd ot ou ov ow ox oy oz lu dk translated">每次我们在一个特定的x或y坐标上分裂，我们就创建了一个决策节点！</p></blockquote></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h2 id="d980" class="np lw it bd lx nq nr dn mb ns nt dp mf li nu nv mh lm nw nx mj lq ny nz ml oa bi translated">示例2:使用基尼系数的真实案例</h2><p id="a04f" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li ob lk ll lm oc lo lp lq od ls lt lu im bi translated">通常，数据不能如此容易地分离，并且需要大量的努力/迭代(这在模型训练/拟合期间完成)来找到<strong class="lb iu">最佳分割</strong>。</p><p id="2a04" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但问题是:</p><blockquote class="oq"><p id="b7e0" class="or os it bd ot ou ov ow ox oy oz lu dk translated">我们如何用数学公式找到这些分裂？</p></blockquote><p id="8d55" class="pw-post-body-paragraph kz la it lb b lc pa ju le lf pb jx lh li pc lk ll lm pd lo lp lq pe ls lt lu im bi translated">让我们看看另一个例子，现在有3个类，每个类有5个样本。以下是创建数据集并绘制数据集的一些Python代码:</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="7cdf" class="np lw it of b gy oj ok l ol om">X,y = make_classification(n_samples=15, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, random_state=1,n_classes=3, n_clusters_per_class=1)</span><span id="6202" class="np lw it of b gy on ok l ol om">c = ["blue" if i==0 else "red" if i==1 else "green" for i in y]</span><span id="b604" class="np lw it of b gy on ok l ol om">plt.figure(dpi=200)<br/>plt.scatter(X[:,0], X[:,1], c=c)<br/>plt.xlabel("x")<br/>plt.ylabel("y")<br/>plt.axvline(x=0.9, label="split at x=0.9", c = "k", linestyle="--")<br/>plt.axhline(y=0.4, xmin=0, xmax=0.59, label="split at y=0.4", c = "pink", linestyle="--")<br/>plt.legend()<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/6580af4b1ceeed47b61e5e6889db0b29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CCg-OQIewnhByl6UVp_Lfw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:数据集的例子。作者用python制作的图。</p></figure><p id="ca9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们首先使用和以前一样的初始规则(我们在x=0.9时分开)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/da475c6c23797d696b6bb43bb7b9fb0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*WxZgItVd62FEVybRI6USrw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4:决策树。<strong class="bd pg">在x=0.9处分割</strong>。作者在<a class="ae ky" href="https://www.smartdraw.com/" rel="noopener ugc nofollow" target="_blank"> smartdraw </a>中制作的图。</p></figure><p id="32f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们再次从一个包含所有数据的<strong class="lb iu">根</strong> <strong class="lb iu">节点</strong>开始，然后，我们在<strong class="lb iu"> x=0.9 </strong>处分割数据，导致两个<strong class="lb iu">分支</strong>，导致两个<strong class="lb iu">叶</strong> <strong class="lb iu">节点</strong>。右叶包含所有蓝色样本，左叶包含ref和绿色样本。</p><p id="7795" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们不能停下来，我们需要向前迈进一步。我们现在将在第二特征维度，即<strong class="lb iu"> y </strong>轴上分割数据，以尝试从红点中修复绿点。</p><p id="4f32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树变成(对于y=0.4的分割):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/8b8767f2ea257ee2f1be2220fdf574c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oBK7tjbNl1aiG_vOghsXeg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5:决策树。<strong class="bd pg">第二个</strong> <strong class="bd pg">在y=0.4时分裂</strong>。作者在<a class="ae ky" href="https://www.smartdraw.com/" rel="noopener ugc nofollow" target="_blank"> smartdraw </a>中制作的图。</p></figure><p id="757f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">进行新的预测:</strong>如果我们将传递一个<strong class="lb iu">新的</strong> <strong class="lb iu">样本</strong>(例如，来自测试集)，该样本具有一个特征/变量<strong class="lb iu"> x=5和y=-2 </strong>，它将在左叶节点结束，并将被分类为<strong class="lb iu">类</strong><strong class="lb iu">绿色</strong>。</p><blockquote class="oq"><p id="73e9" class="or os it bd ot ou ov ow ox oy oz lu dk translated">提醒:每次我们在一个特定的x或y坐标上分割，我们创建了一个决策节点！</p></blockquote></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="2e5c" class="lv lw it bd lx ly nk ma mb mc nl me mf jz nm ka mh kc nn kd mj kf no kg ml mm bi translated">基尼不纯与基尼收益</h1><p id="0afa" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li ob lk ll lm oc lo lp lq od ls lt lu im bi translated">基尼系数背后的原理很简单:</p><ul class=""><li id="4f40" class="mn mo it lb b lc ld lf lg li pi lm pj lq pk lu mu mv mw mx bi translated">假设您在我们的数据集中随机选择了一个样本。</li><li id="2db9" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated">接下来，根据数据集中的类分布对其进行随机分类。</li></ul><p id="4e24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">基尼杂质</strong>告诉我们，我们对数据点分类错误的<strong class="lb iu">概率是多少</strong>【3】。</p><p id="335d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的例子中，我们通过目视检查发现了完美的分离水平。然而，在现实世界的问题中，这是不可能的，因为样本和特征的数量很大。为了找到<strong class="lb iu">最佳分割级别</strong>(在每个特征维度中)，决策树<strong class="lb iu">选择<strong class="lb iu">最大化</strong><strong class="lb iu"/><strong class="lb iu">基尼增益</strong>【3】的</strong>分割。</p><blockquote class="oq"><p id="d998" class="or os it bd ot ou ov ow ox oy oz lu dk translated">可能的最佳基尼系数为0，它对应于模型当前水平/状态下的最优分割。</p></blockquote><p id="a36d" class="pw-post-body-paragraph kz la it lb b lc pa ju le lf pb jx lh li pc lk ll lm pd lo lp lq pe ls lt lu im bi translated"><strong class="lb iu">数学</strong> <strong class="lb iu">公式</strong>如下。</p><p id="e307" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于我们数据中的<strong class="lb iu"> C </strong>类，<strong class="lb iu"> <em class="pl"> p(i) </em> </strong>是选择属于<strong class="lb iu"> <em class="pl"> i </em> </strong>类的数据样本的概率，<strong class="lb iu"> Gini </strong> <strong class="lb iu">杂质</strong> (G)由下式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/bd039c1146fc517bb137463322b2e311.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ectdtz2gv21Lj-pU9qXmLQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在markdown制作的公式。</p></figure><p id="6bb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们回到我们的第一个(最简单的)例子(<strong class="lb iu">例1 </strong>)，<strong class="lb iu">定义一个新的不良分割</strong>并逐步估算<strong class="lb iu">基尼杂质</strong>和<strong class="lb iu">增益</strong>！</p><p id="7364" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们<strong class="lb iu">估计</strong> <strong class="lb iu"> p(i)对于每个类</strong> ( <strong class="lb iu">还没有拆分</strong>):</p><ul class=""><li id="d172" class="mn mo it lb b lc ld lf lg li pi lm pj lq pk lu mu mv mw mx bi translated"><strong class="lb iu">蓝色</strong> <strong class="lb iu">类</strong>:我们10个样本中有5个是蓝色的，所以<strong class="lb iu"> p(蓝色)= 5/10=0.5 </strong>概率随机挑选一个蓝色样本。</li><li id="de6b" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated"><strong class="lb iu">红色</strong></li></ul><p id="7184" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们估计<strong class="lb iu">初始G(还没有任何分割</strong>):</p><ul class=""><li id="c1ba" class="mn mo it lb b lc ld lf lg li pi lm pj lq pk lu mu mv mw mx bi translated"><strong class="lb iu">G0</strong>= 0.5 *(1–0.5)+0.5 *(1–0.5)= 0.5</li></ul><p id="790e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(50%的机会将其错误分类，因为我们有4个可能的事件，只有2个是正确的<strong class="lb iu"/>，即<strong class="lb iu">选择蓝色，将蓝色分类</strong>，选择蓝色，将红色分类，选择红色，将蓝色分类，<strong class="lb iu">选择红色，将红色分类</strong>)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/566af25b5a35a5217b80bea354f2fd90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IUKNvE0CtelRa167t-kzsw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6:数据集的例子。作者用python制作的图。</p></figure><p id="13c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们估计一下<strong class="lb iu"> x=0.2的“非完美”分割</strong>的<strong class="lb iu">基尼系数</strong>。</p><ul class=""><li id="1fe8" class="mn mo it lb b lc ld lf lg li pi lm pj lq pk lu mu mv mw mx bi translated"><strong class="lb iu">G _ left _ space</strong>=(3/3)*(1–3/3)(蓝色)+ 0(无红色)= <strong class="lb iu"> 0 </strong>(值不错，因为我们只有蓝点，没有红色，即杂质为0)。</li><li id="cebe" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated"><strong class="lb iu">G _ right _ space</strong>=(2/7)*(1–2/7)(对于蓝色)+(5/7)*(1–5/7)(对于红色)= <strong class="lb iu"> 0.4081 </strong>(此处杂质不为0，因此这意味着分离包含不同类别的样品)。</li></ul><p id="0c48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过从原始杂质G0(无分割)中减去加权杂质(G_left &amp; G_right)来计算最终基尼系数:</p><ul class=""><li id="820e" class="mn mo it lb b lc ld lf lg li pi lm pj lq pk lu mu mv mw mx bi translated"><strong class="lb iu"> Gini_gain </strong> = G0 — (3/10)*0(左)— (7/10)*0.4081 = 0.5— (3/10)*0(左)— (7/10)*0.4081 = <strong class="lb iu"> 0.214 </strong></li></ul><blockquote class="oq"><p id="56cf" class="or os it bd ot ou pn po pp pq pr lu dk translated"><strong class="ak">提醒:基尼系数越高，分割越好</strong>。</p></blockquote><p id="7727" class="pw-post-body-paragraph kz la it lb b lc pa ju le lf pb jx lh li pc lk ll lm pd lo lp lq pe ls lt lu im bi translated">总之，决策树算法尝试了许多不同的分割，它估计每个分割的基尼系数，并根据最大基尼系数进行分割。<strong class="lb iu">因此，我们最终得出基尼系数的最优分割。</strong></p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h2 id="4c6b" class="np lw it bd lx nq nr dn mb ns nt dp mf li nu nv mh lm nw nx mj lq ny nz ml oa bi translated"><strong class="ak">任务</strong>:现在尝试估算如图1所示的完美分割的基尼系数。</h2><p id="4859" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li ob lk ll lm oc lo lp lq od ls lt lu im bi translated">提示:你会很容易地发现G0 = 0.5，G_left &amp; G_right = 0，这样基尼增益为0.5-(5/10)*0-(5/10)*0 = <strong class="lb iu"> 0.5！</strong></p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="b665" class="lv lw it bd lx ly nk ma mb mc nl me mf jz nm ka mh kc nn kd mj kf no kg ml mm bi translated">随机森林</h1><p id="a98c" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li ob lk ll lm oc lo lp lq od ls lt lu im bi translated">既然你已经理解了决策树是什么，以及它是如何找到最佳分裂的(基于基尼增益)，那么是时候介绍<strong class="lb iu">随机</strong>森林<strong class="lb iu">了。</strong></p><p id="526a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">随机森林</strong>只不过是决策树的集合。这里需要注意的一件重要事情是，对于随机森林中的每个唯一决策树，随机森林通常使用可用变量/特征的子样本，而不是所有变量/特征。</p><p id="102a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如参见<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . ensemble . randomforestclassifier . html</a>模型中的输入参数“<strong class="lb iu"> max_features </strong>”。</p><p id="33a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，随机森林通常通过采取<strong class="lb iu">多数</strong> <strong class="lb iu">投票</strong>或构成森林的<strong class="lb iu">个体决策树</strong>的<strong class="lb iu">平均</strong>预测类来决定。</p><h1 id="739e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="ad73" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li ob lk ll lm oc lo lp lq od ls lt lu im bi translated">决策树和随机森林是强大的机器学习模型，可用于回归和分类。在分类问题的情况下，如上所述，最佳分割是基于基尼系数进行的。</p><p id="5374" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在不久的将来，我将写一篇纯python的文章，展示如何使用sklearn和真实数据拟合模型。</p><p id="6c33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请务必订阅我的邮件列表，以免错过！</p><p id="c8b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那都是乡亲们！希望你喜欢这篇文章。有什么问题吗？把它们作为评论贴出来，我会尽快回复。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><p id="37e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想在交互式路线图和活跃的学习社区的支持下自学数据科学，看看这个资源:<a class="ae ky" href="https://aigents.co/learn" rel="noopener ugc nofollow" target="_blank">https://aigents.co/learn</a></p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="2ce7" class="lv lw it bd lx ly nk ma mb mc nl me mf jz nm ka mh kc nn kd mj kf no kg ml mm bi translated">敬请关注并支持这一努力</h1><p id="9980" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li ob lk ll lm oc lo lp lq od ls lt lu im bi translated">如果你喜欢这篇文章并觉得有用，<strong class="lb iu">关注</strong>我，确保你订阅了我的邮件列表并成为会员:</p><ul class=""><li id="5115" class="mn mo it lb b lc ld lf lg li pi lm pj lq pk lu mu mv mw mx bi translated"><strong class="lb iu">短短5秒我的邮件列表</strong>:<a class="ae ky" href="https://seralouk.medium.com/subscribe" rel="noopener">https://seralouk.medium.com/subscribe</a></li><li id="f8b9" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated"><strong class="lb iu">成为会员，支持我</strong>:<a class="ae ky" href="https://seralouk.medium.com/membership" rel="noopener">https://seralouk.medium.com/membership</a></li></ul></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="81fd" class="lv lw it bd lx ly nk ma mb mc nl me mf jz nm ka mh kc nn kd mj kf no kg ml mm bi translated">参考</h1><p id="aa26" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li ob lk ll lm oc lo lp lq od ls lt lu im bi translated">[1]<a class="ae ky" href="https://en.wikipedia.org/wiki/Random_forest" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Random_forest</a></p><p id="5e0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://www.smartdraw.com/" rel="noopener ugc nofollow" target="_blank">【2】https://www.smartdraw.com/</a></p><p id="b797" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Decision_tree_learning" rel="noopener ugc nofollow" target="_blank">【3】https://en.wikipedia.org/wiki/Decision_tree_learning</a></p><h1 id="609e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">和我联系</h1><ul class=""><li id="3ef3" class="mn mo it lb b lc mp lf mq li mr lm ms lq mt lu mu mv mw mx bi translated"><strong class="lb iu">领英</strong>:<a class="ae ky" href="https://www.linkedin.com/in/serafeim-loukas/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/serafeim-loukas/</a></li></ul></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h2 id="35c6" class="np lw it bd lx nq nr dn mb ns nt dp mf li nu nv mh lm nw nx mj lq ny nz ml oa bi translated">您可能还喜欢:</h2><div class="ps pt gp gr pu pv"><a rel="noopener follow" target="_blank" href="/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8"><div class="pw ab fo"><div class="px ab py cl cj pz"><h2 class="bd iu gy z fp qa fr fs qb fu fw is bi translated">支持向量机(SVM)解释清楚:分类问题的python教程…</h2><div class="qc l"><h3 class="bd b gy z fp qa fr fs qb fu fw dk translated">在这篇文章中，我解释了支持向量机的核心，为什么以及如何使用它们。此外，我还展示了如何绘制支持…</h3></div><div class="qd l"><p class="bd b dl z fp qa fr fs qb fu fw dk translated">towardsdatascience.com</p></div></div><div class="qe l"><div class="qf l qg qh qi qe qj ks pv"/></div></div></a></div><div class="ps pt gp gr pu pv"><a rel="noopener follow" target="_blank" href="/k-means-clustering-how-it-works-finding-the-optimum-number-of-clusters-in-the-data-13d18739255c"><div class="pw ab fo"><div class="px ab py cl cj pz"><h2 class="bd iu gy z fp qa fr fs qb fu fw is bi translated">K-Means聚类:工作原理&amp;在数据中寻找最优的聚类数</h2><div class="qc l"><h3 class="bd b gy z fp qa fr fs qb fu fw dk translated">数学公式，寻找最佳聚类数和Python中的工作示例。</h3></div><div class="qd l"><p class="bd b dl z fp qa fr fs qb fu fw dk translated">towardsdatascience.com</p></div></div><div class="qe l"><div class="qk l qg qh qi qe qj ks pv"/></div></div></a></div><div class="ps pt gp gr pu pv"><a rel="noopener follow" target="_blank" href="/lstm-time-series-forecasting-predicting-stock-prices-using-an-lstm-model-6223e9644a2f"><div class="pw ab fo"><div class="px ab py cl cj pz"><h2 class="bd iu gy z fp qa fr fs qb fu fw is bi translated">LSTM时间序列预测:使用LSTM模型预测股票价格</h2><div class="qc l"><h3 class="bd b gy z fp qa fr fs qb fu fw dk translated">在这篇文章中，我将向你展示如何使用预测LSTM模型来预测股票价格</h3></div><div class="qd l"><p class="bd b dl z fp qa fr fs qb fu fw dk translated">towardsdatascience.com</p></div></div><div class="qe l"><div class="ql l qg qh qi qe qj ks pv"/></div></div></a></div><div class="ps pt gp gr pu pv"><a rel="noopener follow" target="_blank" href="/time-series-forecasting-predicting-stock-prices-using-an-arima-model-2e3b3080bd70"><div class="pw ab fo"><div class="px ab py cl cj pz"><h2 class="bd iu gy z fp qa fr fs qb fu fw is bi translated">时间序列预测:使用ARIMA模型预测股票价格</h2><div class="qc l"><h3 class="bd b gy z fp qa fr fs qb fu fw dk translated">在这篇文章中，我将向你展示如何使用预测ARIMA模型来预测特斯拉的股票价格</h3></div><div class="qd l"><p class="bd b dl z fp qa fr fs qb fu fw dk translated">towardsdatascience.com</p></div></div><div class="qe l"><div class="qm l qg qh qi qe qj ks pv"/></div></div></a></div><div class="ps pt gp gr pu pv"><a href="https://medium.com/@seralouk/the-best-free-data-science-resources-free-books-online-courses-9c4a2df194e5" rel="noopener follow" target="_blank"><div class="pw ab fo"><div class="px ab py cl cj pz"><h2 class="bd iu gy z fp qa fr fs qb fu fw is bi translated">最佳免费数据科学资源:免费书籍和在线课程</h2><div class="qc l"><h3 class="bd b gy z fp qa fr fs qb fu fw dk translated">最有用的免费书籍和在线课程，适合想了解更多数据科学知识的人。</h3></div><div class="qd l"><p class="bd b dl z fp qa fr fs qb fu fw dk translated">medium.com</p></div></div><div class="qe l"><div class="qn l qg qh qi qe qj ks pv"/></div></div></a></div><div class="ps pt gp gr pu pv"><a rel="noopener follow" target="_blank" href="/roc-curve-explained-using-a-covid-19-hypothetical-example-binary-multi-class-classification-bab188ea869c"><div class="pw ab fo"><div class="px ab py cl cj pz"><h2 class="bd iu gy z fp qa fr fs qb fu fw is bi translated">用新冠肺炎假设的例子解释ROC曲线:二分类和多分类…</h2><div class="qc l"><h3 class="bd b gy z fp qa fr fs qb fu fw dk translated">在这篇文章中，我清楚地解释了什么是ROC曲线以及如何阅读它。我用一个新冠肺炎的例子来说明我的观点，我…</h3></div><div class="qd l"><p class="bd b dl z fp qa fr fs qb fu fw dk translated">towardsdatascience.com</p></div></div><div class="qe l"><div class="qo l qg qh qi qe qj ks pv"/></div></div></a></div><div class="ps pt gp gr pu pv"><a rel="noopener follow" target="_blank" href="/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8"><div class="pw ab fo"><div class="px ab py cl cj pz"><h2 class="bd iu gy z fp qa fr fs qb fu fw is bi translated">支持向量机(SVM)解释清楚:分类问题的python教程…</h2><div class="qc l"><h3 class="bd b gy z fp qa fr fs qb fu fw dk translated">在这篇文章中，我解释了支持向量机的核心，为什么以及如何使用它们。此外，我还展示了如何绘制支持…</h3></div><div class="qd l"><p class="bd b dl z fp qa fr fs qb fu fw dk translated">towardsdatascience.com</p></div></div><div class="qe l"><div class="qf l qg qh qi qe qj ks pv"/></div></div></a></div><div class="ps pt gp gr pu pv"><a rel="noopener follow" target="_blank" href="/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e"><div class="pw ab fo"><div class="px ab py cl cj pz"><h2 class="bd iu gy z fp qa fr fs qb fu fw is bi translated">PCA清楚地解释了——如何、何时、为什么使用它以及特性的重要性:Python指南</h2><div class="qc l"><h3 class="bd b gy z fp qa fr fs qb fu fw dk translated">在这篇文章中，我解释了什么是PCA，何时以及为什么使用它，以及如何使用scikit-learn在Python中实现它。还有…</h3></div><div class="qd l"><p class="bd b dl z fp qa fr fs qb fu fw dk translated">towardsdatascience.com</p></div></div><div class="qe l"><div class="qp l qg qh qi qe qj ks pv"/></div></div></a></div><div class="ps pt gp gr pu pv"><a rel="noopener follow" target="_blank" href="/everything-you-need-to-know-about-min-max-normalization-in-python-b79592732b79"><div class="pw ab fo"><div class="px ab py cl cj pz"><h2 class="bd iu gy z fp qa fr fs qb fu fw is bi translated">关于Python中的最小-最大规范化，您需要知道的一切</h2><div class="qc l"><h3 class="bd b gy z fp qa fr fs qb fu fw dk translated">在这篇文章中，我将解释什么是最小-最大缩放，什么时候使用它，以及如何使用scikit在Python中实现它</h3></div><div class="qd l"><p class="bd b dl z fp qa fr fs qb fu fw dk translated">towardsdatascience.com</p></div></div><div class="qe l"><div class="qq l qg qh qi qe qj ks pv"/></div></div></a></div><div class="ps pt gp gr pu pv"><a rel="noopener follow" target="_blank" href="/how-and-why-to-standardize-your-data-996926c2c832"><div class="pw ab fo"><div class="px ab py cl cj pz"><h2 class="bd iu gy z fp qa fr fs qb fu fw is bi translated">Scikit-Learn的标准定标器如何工作</h2><div class="qc l"><h3 class="bd b gy z fp qa fr fs qb fu fw dk translated">在这篇文章中，我将解释为什么以及如何使用scikit-learn应用标准化</h3></div><div class="qd l"><p class="bd b dl z fp qa fr fs qb fu fw dk translated">towardsdatascience.com</p></div></div><div class="qe l"><div class="qr l qg qh qi qe qj ks pv"/></div></div></a></div></div></div>    
</body>
</html>