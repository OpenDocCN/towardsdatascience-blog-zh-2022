<html>
<head>
<title>Let’s Get to the Bottom of Optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让我们来看看优化的本质</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lets-get-to-the-bottom-of-optimization-509f06c8314c#2022-09-04">https://towardsdatascience.com/lets-get-to-the-bottom-of-optimization-509f06c8314c#2022-09-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/8e27962000ed347e46b3f42694c798e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WvQ_v8ZXJn3WKksfcJ8qjQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">Himmelblau函数的最速下降(自生成图像)</p></figure><div class=""/><div class=""><h2 id="7830" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">构建一个可视化工具来试验不同的优化算法</h2></div><p id="9937" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在这篇文章中，我将展示我创建的一个可视化工具，以了解优化算法如何在2D函数上工作。我将简要介绍算法中的一些数学知识，并深入研究visualizer工具的实现细节。</p><h1 id="3e77" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">形象化</h1><p id="23d7" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">如果你想在不同的非凸函数<a class="ae mn" href="https://sachag678-optimization-ui-sv2xnb.streamlitapp.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="kw jg">上运行不同的优化算法，点击这里</strong> </a>。我用streamlit开发了这个，帮助我理解不同的算法如何在函数空间中移动。</p><h1 id="f8c5" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">背景</h1><p id="1354" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">传统上有两种寻找极小值的方法——线搜索和信赖域优化方法。</p><p id="03d3" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这两种方法在选择走向最小值的迭代过程中的下一步的方向和大小的顺序上有所不同。线搜索方法确定方向，然后找到最佳步长，而信赖域方法确定步长，然后选择方向。</p><p id="776f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">一个直观的阶跃方向是函数的负梯度。这是因为梯度指向最快速增加的方向。这是在许多ML优化框架中普遍存在的最速下降法的基础。它很有用，因为我们只需要计算一阶导数。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/8639c01fb2033041fa0cba167e4c94a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:178/1*siREx_ulv0D27D6IOpmKwA.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">最陡下降方向</p></figure><p id="4902" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">一个不太直观但很重要的步进方向是牛顿方向。这使用了海森矩阵<em class="mt"> H </em>的逆矩阵，该矩阵描述了<em class="mt"> f </em>的二阶偏导数。该矩阵提供了关于在<em class="mt"> x </em>处<em class="mt"> f </em>的局部曲率的信息。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/c05b42b8cdd0a53b24b30aaf61a436ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:244/1*kO8IK3_FmtV9nVzOcapiGQ.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">牛顿方向</p></figure><p id="31e3" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">上述方程的推导以及收敛性的证明超出了本文的范围(<em class="mt">参见第20-25页的</em>和<em class="mt">Nocedal和Wright </em>的数值优化)，但是我想提供一个手动波形解释，说明为什么hessian在寻找搜索方向时是有用的。</p><p id="4d43" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">最陡下降方向是函数<em class="mt"> f </em>的切线。因此，通过沿着该线直到从当前点有足够的减少，来确定最佳步长。</p><p id="4876" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">牛顿方向不是通过考虑直线来计算的，而是考虑更好地逼近函数<em class="mt"> f </em>的二次曲线。因此，当查看搜索方向时，它试图找到二次型最小值的方向，这允许在给定的某些条件下更快地收敛。这通常消除了计算步长的需要，实际上步长被设置为1。</p><p id="34c1" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我认为阅读建议的页面来理解证明是有用的，但是这需要一些线性代数的背景知识。我建议参考吉尔伯特·斯特朗的《线性代数和从数据中学习》。但是如果你对代码更感兴趣，我们开始吧！</p><h1 id="6726" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">实施细节</h1><p id="1137" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">本节将深入解释最速下降法和牛顿步算法的代码，并有望为上述数学提供一些具体内容。</p><p id="f04e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg">最陡下降</strong></p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="4977" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">最速下降法使用近似导数函数计算步长方向<em class="mt"> p </em>。</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="f5d2" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">导数函数使用导数的定义以数字方式计算梯度，该定义为:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/f4062608eafd5647ef98299a59ea733c.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/1*wpTzmnA6LXiZ3yCx0L2eXQ.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">导数的定义</p></figure><p id="2f71" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">关键的一点是，我们使用一个小的<em class="mt"> h </em>，而不是取h趋于0时的极限，来得到一个近似导数。使用这些近似函数允许对任何可微函数求导，而不必事先知道它。这就是为什么我们把f作为函数直接传递给它。</p><p id="a27d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">下一步是计算步长，我们使用回溯搜索来完成。</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="9b9c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">该方法使用第2行和第3行所示的不等式，称为<a class="ae mn" href="https://en.wikipedia.org/wiki/Wolfe_conditions#cite_note-4" rel="noopener ugc nofollow" target="_blank"> Armijo条件</a>，以找到保证充分降低的最佳步长。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi my"><img src="../Images/0b57577152b71cbc637c11bbf75fe9d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Wu49I6i0lpiPY3kFDqJog.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">充分下降条件(自生成图像— Sacha Gunaratne 2022)</p></figure><p id="cd51" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">上图显示了<em class="mt"> l( </em> α <em class="mt"> ) </em>和φ<em class="mt">(</em>α<em class="mt">)</em>是不等式的两块。LHS为φ<em class="mt">(</em>α<em class="mt">)</em>，RHS为<em class="mt"> l( </em> α <em class="mt"> ) </em>即直线。直觉是，当满足以下条件时，φ会充分减小:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/56a9241c4ad9007fe10d7f4e16e28387.png" data-original-src="https://miro.medium.com/v2/resize:fit:172/1*t1fzOjmJBI99DOiwJoI_nw.gif"/></div></figure><p id="3e61" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">很容易看出，这个条件在可接受的范围内是成立的，因此在这些范围内的任何α值都满足这个条件。该算法以α的设定值开始，然后缓慢降低，直到满足条件。在该书中，建议将牛顿法的α设置为1，并对其他方法使用其初始值进行实验。</p><p id="481a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">一旦找到步长和方向，就可以用它们来更新当前点，使其更接近最小值。重复这一过程，直到满足退出条件，该条件检查对当前点的两次更新之间的欧几里德距离。</p><p id="7db6" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg">牛顿法</strong></p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="a2b0" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这两种方法有很多相似之处——主要区别在第8、9行，在那里计算hessian并用于计算步长方向<em class="mt"> p. </em></p><p id="ea48" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">hessian是描述函数局部曲率的函数的二阶导数。它告诉我们，如果一个函数<em class="mt"> f (x) </em>有一个鞍点，局部最小值或最大值在<em class="mt">x。</em>hessian只有在它是正定的时候才有用，因为这提供了一个下降方向。<strong class="kw jg">这加上需要计算hessian及其逆是它不是最常用方法的部分原因。</strong></p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="542f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">近似hessian在上面使用与近似导数相同的方法计算，因为hessian正好是二阶导数。</p><p id="9463" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">使用hessian和梯度的逆来计算步长方向，然后遵循与最速下降法相同的步骤。</p><p id="c13c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在可视化工具中，你会注意到牛顿法比最速下降法收敛得更快。然而，在某些情况下，牛顿法会失败，你会注意到它找不到任何最小值(<a class="ae mn" href="https://en.wikipedia.org/wiki/Himmelblau%27s_function" rel="noopener ugc nofollow" target="_blank"> Himmelblau函数</a>当x，y = 0，0时)。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi na"><img src="../Images/252574c40372e84567003fbffcc642ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CGyPaEQXqW9QR2BNYBfjjQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">牛顿的方法失败了</p></figure><p id="e938" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这是因为在这些情况下，hessian不是正定的，因此没有下降方向。有几种方法可以解决这个问题，这里应用了其中的两种方法:</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="bc1f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这些方法归结为修改hessian矩阵，使它是正定的。</p><p id="6944" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">方法1增加了单位矩阵的<strong class="kw jg">倍，使得hessian矩阵的对角元素为某个因子<em class="mt">β的正。</em>方法2 <strong class="kw jg">将矩阵中的所有负值</strong>翻转为正值。这两种方法都确保特征值大于零，这是正定性的一个条件。一旦应用了这些方法，牛顿法将在Himmelblau函数中找到最小值。</strong></p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi na"><img src="../Images/9d019ba38ce240b99f1aac05ab2f98df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UQqVtq8vL60YI9X_A5OlQg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">牛顿法在对黑森(Sacha Gunaratne 2022)进行修改后仍然有效</p></figure><h1 id="8e91" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">外卖食品</h1><p id="e65b" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">在实现这些算法的过程中有许多错综复杂和细微差别，只有当你深陷其中时才会出现。我发现构建这个可视化工具的练习非常有助于加强我对许多机器学习和优化框架中使用的各种算法的理解。</p><p id="d3df" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">还有几件事我想实现——BFGS算法，这是一种准牛顿法，不需要完全计算hessian。考虑完全Wolfe条件的线搜索算法。我还希望允许用户输入他们自己的函数，这样他们就可以以一种更少约束的方式使用这个工具。</p><p id="9308" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我希望这篇文章对优化方法及其背后的直觉提供了一些见解，并且希望你有机会使用<a class="ae mn" href="https://sachag678-optimization-ui-sv2xnb.streamlitapp.com/" rel="noopener ugc nofollow" target="_blank">可视化工具</a>。</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="b435" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">大部分这些我都是通过阅读Nocedal和Wright 的<em class="mt">数值优化学来的，我发现这本书写得很好，以直观的方式处理复杂的主题。</em></p><p id="59ea" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">如果你对改进可视化工具有任何建议，或者对本文有任何疑问，请随时通过<a class="ae mn" href="https://twitter.com/sacha0" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p></div></div>    
</body>
</html>