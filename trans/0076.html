<html>
<head>
<title>Train and Deploy Fine-Tuned GPT-2 Model Using PyTorch on Amazon SageMaker to Classify News Articles</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用亚马逊SageMaker上的PyTorch训练和部署微调的GPT-2模型，以对新闻文章进行分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/train-and-deploy-fine-tuned-gpt-2-model-using-pytorch-on-amazon-sagemaker-to-classify-news-articles-612f9957c7b#2022-02-03">https://towardsdatascience.com/train-and-deploy-fine-tuned-gpt-2-model-using-pytorch-on-amazon-sagemaker-to-classify-news-articles-612f9957c7b#2022-02-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="32bf" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">亚马逊SageMaker上使用GPT-2进行文本分类的教程</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2f35312926f0b8ab48b2f21be4ac2cf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nt4beeksX5KWDEme.jpg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="kv">照片由</em> <a class="ae kw" href="https://unsplash.com/@impatrickt?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> <em class="kv">帕特里克·托马索</em> </a> <em class="kv">上</em> <a class="ae kw" href="https://unsplash.com/s/photos/machine-learning-news?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> <em class="kv">下</em> </a></p></figure><p id="1644" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">文本分类是自然语言处理中非常常见的任务。它可以用于许多应用，从垃圾邮件过滤、情感分析到客户支持自动化和新闻分类。使用深度学习语言模型进行大规模文本分类任务最近在业界变得相当流行，尤其是近年来随着<a class="ae kw" href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)" rel="noopener ugc nofollow" target="_blank">变形金刚</a>的出现。因为这些变压器模型的规模往往太大，无法在本地机器上训练，所以通常使用云计算平台(例如<a class="ae kw" href="https://cloud.google.com/" rel="noopener ugc nofollow" target="_blank"> GCP </a>、<a class="ae kw" href="https://aws.amazon.com/" rel="noopener ugc nofollow" target="_blank"> AWS </a>、<a class="ae kw" href="https://azure.microsoft.com/" rel="noopener ugc nofollow" target="_blank"> Azure </a>、<a class="ae kw" href="https://us.alibabacloud.com/" rel="noopener ugc nofollow" target="_blank"> Alibabacloud </a>)。因此，在这篇博客中，我想展示如何使用Amazon SageMaker来训练和部署一个用于文本分类任务的微调的GPT-2模型。</p><h1 id="fdd4" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">1.介绍</h1><h2 id="0d2a" class="ml lu iq bd lv mm mn dn lz mo mp dp md lg mq mr mf lk ms mt mh lo mu mv mj mw bi translated">1.1变形金刚:GPT-2 vs伯特</h2><p id="8afc" class="pw-post-body-paragraph kx ky iq kz b la mx jr lc ld my ju lf lg mz li lj lk na lm ln lo nb lq lr ls ij bi translated"><a class="ae kw" href="https://en.wikipedia.org/wiki/GPT-2" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>属于深度学习模型家族，名为“<a class="ae kw" href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)" rel="noopener ugc nofollow" target="_blank">变形金刚</a>”。变压器是当前最先进的NLP架构的构建模块。在这里不可能用一段话来解释变形金刚是如何工作的，但总结一下，变形金刚使用了一种“自我关注”机制，通过“学习”句子中不同位置的单词之间的关系来计算序列的表示。典型的变压器设计包含两个部分，<strong class="kz ir">编码器</strong>和<strong class="kz ir">解码器</strong>，两者都作为单词关系的矢量化表示。</p><p id="abb1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><a class="ae kw" href="https://en.wikipedia.org/wiki/GPT-2" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>和<a class="ae kw" href="https://en.wikipedia.org/wiki/BERT_(language_model)" rel="noopener ugc nofollow" target="_blank">伯特</a>分别是2018年和2019年发布的两款知名变形金刚型号。从那时起，有更新更好的模型发布(例如，2019年的<a class="ae kw" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank">罗伯塔</a>和2020年的<a class="ae kw" href="https://en.wikipedia.org/wiki/GPT-3" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>)，然而，这两个仍然非常受许多工业应用的欢迎，直到现在，由于它们伟大的可用性和性能。GPT-2和伯特之间的关键区别在于，GPT-2本质上是一个生成模型，而伯特不是。这就是为什么你可以发现很多技术博客使用BERT进行文本分类任务，使用GPT-2进行文本生成任务，但是很少使用GPT-2进行文本分类任务。这就是我决定在这里使用GPT-2的原因——它更具挑战性！</p><h2 id="1037" class="ml lu iq bd lv mm mn dn lz mo mp dp md lg mq mr mf lk ms mt mh lo mu mv mj mw bi translated">1.2亚马逊SageMaker</h2><p id="b33e" class="pw-post-body-paragraph kx ky iq kz b la mx jr lc ld my ju lf lg mz li lj lk na lm ln lo nb lq lr ls ij bi translated"><a class="ae kw" href="https://aws.amazon.com/sagemaker/" rel="noopener ugc nofollow" target="_blank"> Amazon SageMaker </a>是一个很好的工具，可以通过AWS提供的完全托管的基础设施，在云实例上训练和部署深度学习模型。几分钟内，您就可以在Jupyter笔记本中构建、训练和部署模型，而不必担心环境设置，因为它附带了许多预构建的Conda环境和Docker容器。对于像我这样的数据科学家来说，这是一个巨大的救命稻草。</p><p id="eec0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">值得一提的是，SageMaker还可以用于<a class="ae kw" href="https://streamlit.io/" rel="noopener ugc nofollow" target="_blank"> Streamlit </a> app开发。这对于产品原型非常有用，因为在模型训练之后，您可以直接在同一个实例上构建应用程序。您将在本文中看到这一点。</p><h1 id="0bfb" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">2.系统需求</h1><p id="a914" class="pw-post-body-paragraph kx ky iq kz b la mx jr lc ld my ju lf lg mz li lj lk na lm ln lo nb lq lr ls ij bi translated">由于大多数模型培训和部署都将在AWS上进行，因此对您的本地机器没有任何系统要求。这是你需要的一切:</p><ul class=""><li id="da2d" class="nc nd iq kz b la lb ld le lg ne lk nf lo ng ls nh ni nj nk bi translated">AWS帐户(<a class="ae kw" href="https://aws.amazon.com/resources/create-account/" rel="noopener ugc nofollow" target="_blank">在此注册</a>)</li><li id="6f6c" class="nc nd iq kz b la nl ld nm lg nn lk no lo np ls nh ni nj nk bi translated">一个<a class="ae kw" href="https://drive.google.com/" rel="noopener ugc nofollow" target="_blank"> Google Drive </a>帐户(可选，用于Colab笔记本培训)</li><li id="61e7" class="nc nd iq kz b la nl ld nm lg nn lk no lo np ls nh ni nj nk bi translated">本地bash/zsh终端(可选，用于Streamlit应用程序部署)</li></ul><h1 id="9ec0" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">3.资料组</h1><p id="971d" class="pw-post-body-paragraph kx ky iq kz b la mx jr lc ld my ju lf lg mz li lj lk na lm ln lo nb lq lr ls ij bi translated">我们将在这个项目中使用的数据集是带有公共许可证的<a class="ae kw" href="http://mlg.ucd.ie/datasets/bbc.html" rel="noopener ugc nofollow" target="_blank"> <em class="nq"> BBC新闻分类数据集</em> </a>。你可以从<a class="ae kw" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>下载<a class="ae kw" href="https://www.kaggle.com/c/learn-ai-bbc/data" rel="noopener ugc nofollow" target="_blank">数据集</a>。</p><p id="cefd" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这个数据集是CSV格式的，它有两列:<strong class="kz ir">文本</strong>和<strong class="kz ir">类别</strong>。它包含了2226个不同的文本，每个文本都被归入5个类别之一:<strong class="kz ir">娱乐</strong>、<strong class="kz ir">体育</strong>、<strong class="kz ir">科技</strong>、<strong class="kz ir">商业</strong>或<strong class="kz ir">政治</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/1b3231148724efe829d15865f26b5c13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AFt2d77LLdyOeHIf.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="kv">作者图片</em></p></figure><h1 id="2344" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">4.演示</h1><p id="312d" class="pw-post-body-paragraph kx ky iq kz b la mx jr lc ld my ju lf lg mz li lj lk na lm ln lo nb lq lr ls ij bi translated">我使用运行训练好的模型的Streamlit 构建了一个在线新闻分类器。您可以在这里输入或粘贴任何新闻，它将非常准确地生成新闻类别的预测。这个应用程序看起来很简单，但它运行着一个非常强大的深度学习模型！</p><p id="5917" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">看看这里:<a class="ae kw" href="https://youtu.be/7rg8umeAtts" rel="noopener ugc nofollow" target="_blank"> GPT-2新闻分类器</a>。</p><p id="804f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">(更新2022.11.11:由于服务器成本，我关闭了这个应用的AWS服务器，并录制了<a class="ae kw" href="https://youtu.be/7rg8umeAtts" rel="noopener ugc nofollow" target="_blank">一段YouTube视频</a>作为这个应用的演示。不好意思！)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/f87d19b65e2576f62abb4e59a5bc2089.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DP_HzUTzQNjunE5e.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="kv">作者图片</em></p></figure><h1 id="059f" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">5.在SageMaker上训练和部署GPT-2</h1><h2 id="3540" class="ml lu iq bd lv mm mn dn lz mo mp dp md lg mq mr mf lk ms mt mh lo mu mv mj mw bi translated">5.1.创建一个Amazon SageMaker笔记本实例</h2><p id="8bd7" class="pw-post-body-paragraph kx ky iq kz b la mx jr lc ld my ju lf lg mz li lj lk na lm ln lo nb lq lr ls ij bi translated">按照AWS的这个<a class="ae kw" href="https://aws.amazon.com/getting-started/hands-on/build-train-deploy-machine-learning-model-sagemaker/" rel="noopener ugc nofollow" target="_blank">实践教程</a>创建一个Amazon SageMaker笔记本实例。使用“<em class="nq">gp T2-新闻分类器</em>作为<strong class="kz ir">实例名</strong>，使用“<em class="nq"> ml.t2.medium </em>作为<strong class="kz ir">实例类型</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/da92bb85d92fadfeedf5b6cbbfddcdc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3vys_KbMYlYJVfyD.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="kv">作者图片</em></p></figure><h2 id="9aba" class="ml lu iq bd lv mm mn dn lz mo mp dp md lg mq mr mf lk ms mt mh lo mu mv mj mw bi translated">5.2.培训和部署</h2><p id="9a2c" class="pw-post-body-paragraph kx ky iq kz b la mx jr lc ld my ju lf lg mz li lj lk na lm ln lo nb lq lr ls ij bi translated">当笔记本状态变为<em class="nq">在用</em>时，选择<strong class="kz ir">打开Jupyter</strong>,<strong class="kz ir">上传</strong>该Git文件夹中的所有文件，结构如下:</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="3bb9" class="ml lu iq nu b gy ny nz l oa ob">├── gpt2-news-classifier-sagemaker-train-deploy.ipynb #main notebook<br/>├── utils.py               # utility functions used by main notebook<br/>├── code                   # separate PyTorch script folder<br/>│   ├── requirements.txt   # libraries used by train_deploy.py<br/>│   └── train_deploy.py    # PyTorch training/deployment script<br/>├── data                   # data folder to be uploaded to S3 bucket<br/>│   ├── test               # test data<br/>│   │   └── test.csv<br/>│   ├── train              # train data<br/>│   │   └── train.csv<br/>│   └── val                # validation data<br/>│       └── val.csv<br/>└── streamlit_app          # Streamlit app folder<br/>    ├── Dockerfile         # Dockerfile for the app (container)<br/>    ├── requirements.txt   # libraries used by app.py<br/>    └── src                <br/>        └── app.py         # main code for the Streamlit app</span></pre><p id="f61e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在SageMaker上运行<a class="ae kw" href="https://github.com/haocai1992/GPT2-News-Classifier/blob/main/sagemaker-train-deploy/gpt2-news-classifier-sagemaker-train-deploy.ipynb" rel="noopener ugc nofollow" target="_blank">这个笔记本</a>来训练和部署GPT-2模型。通读它以获得关于实现的更多细节。</p><h2 id="f907" class="ml lu iq bd lv mm mn dn lz mo mp dp md lg mq mr mf lk ms mt mh lo mu mv mj mw bi translated">5.3.培训_部署. py</h2><p id="1fe8" class="pw-post-body-paragraph kx ky iq kz b la mx jr lc ld my ju lf lg mz li lj lk na lm ln lo nb lq lr ls ij bi translated">由于我们在这个项目中构建和训练的是PyTorch模型，所以<a class="ae kw" href="https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#train-a-model-with-pytorch" rel="noopener ugc nofollow" target="_blank"><strong class="kz ir">SageMaker Python SDK</strong></a>建议准备一个单独的<code class="fe oc od oe nu b">train_deploy.py</code>脚本来构建和存储SageMaker使用的模型函数。有两个基本功能，<code class="fe oc od oe nu b">SimpleGPT2SequenceClassifier</code>和<code class="fe oc od oe nu b">train</code>。</p><p id="f48e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><em class="nq"> train_deploy.py </em>中的<code class="fe oc od oe nu b">SimpleGPT2SequenceClassifier</code>类负责在预训练的GPT-2模型之上构建一个分类器。这里的技巧是在GPT-2的12层解码器上添加一个线性层，其输出维度等于我们的标签数量。这样，我们可以使用GPT-2来输出5个数字，这5个数字对应于我们的5个新闻类别！</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="4d68" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><em class="nq"> train_deploy.py </em>中的<code class="fe oc od oe nu b">train</code>函数在给定输入数据的情况下构建分类器的训练循环。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="8860" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">一个重要的注意事项:GPT-2是一个文本生成模型，它的最后一个标记嵌入预测后续标记。因此，与使用第一个标记嵌入的BERT不同，在输入文本的标记化步骤中，我们应该使用最后一个标记，如下所示。(<a class="ae kw" href="https://gmihaila.medium.com/" rel="noopener"> George Mihaila </a>在他的文章<a class="ae kw" href="https://gmihaila.medium.com/gpt2-for-text-classification-using-hugging-face-transformers-574555451832" rel="noopener">“使用拥抱脸变形金刚进行文本分类的gp T2”</a>中提供了一种优雅的方法，这就是我在这里使用的方法。)</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><h1 id="484f" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">6.Colab笔记本培训</h1><p id="fff4" class="pw-post-body-paragraph kx ky iq kz b la mx jr lc ld my ju lf lg mz li lj lk na lm ln lo nb lq lr ls ij bi translated">当谈到在云笔记本上训练深度学习模型时，亚马逊SageMaker的一个方便的替代品是谷歌的<a class="ae kw" href="https://colab.research.google.com/?utm_source=scs-index#" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a>。它在AWS中跳过所有你需要的云服务设置，最重要的是，它为模型训练提供免费的CPU/GPU实例(尽管有12小时的限制)！要使用它，只需打开你的<a class="ae kw" href="https://drive.google.com" rel="noopener ugc nofollow" target="_blank"> Google Drive </a>，选择<strong class="kz ir">新建- &gt;更多- &gt; Google协同实验室</strong>。如果想用GPU加速训练，选择<strong class="kz ir">运行时- &gt;改变运行时类型- &gt; GPU </strong>，就可以在那里写你的代码了！</p><p id="c023" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我的<a class="ae kw" href="https://colab.research.google.com/drive/1dMTdO5vxdVX0NA2Qe7AV9WGEy8ZH67Xn?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="kz ir"> Colab笔记本</strong> </a>以及资料可以在这里 找到<a class="ae kw" href="https://drive.google.com/drive/folders/1q_4pJKDAv21vpO232ZEyxedfpWWVx7wu?usp=sharing" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="d9df" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">7.使用Amazon EC2和Docker进行部署</h1><p id="70ce" class="pw-post-body-paragraph kx ky iq kz b la mx jr lc ld my ju lf lg mz li lj lk na lm ln lo nb lq lr ls ij bi translated">尽管模型部署可以在SageMaker Notebook实例中完成，正如我刚才所展示的，但是在实际的应用程序开发实践中，为了简单性和可再现性，通常建议将培训和部署分离。因此，我还使用Docker在Amazon EC2实例上部署了我们训练过的GPT-2模型。</p><h2 id="9f17" class="ml lu iq bd lv mm mn dn lz mo mp dp md lg mq mr mf lk ms mt mh lo mu mv mj mw bi translated">7.1.创建Amazon EC2实例</h2><p id="5f0d" class="pw-post-body-paragraph kx ky iq kz b la mx jr lc ld my ju lf lg mz li lj lk na lm ln lo nb lq lr ls ij bi translated">按照AWS的教程<a class="ae kw" href="https://docs.aws.amazon.com/efs/latest/ug/gs-step-one-create-ec2-resources.html" rel="noopener ugc nofollow" target="_blank">创建并启动Amazon EC2实例。此项目的一些自定义设置:</a></p><ul class=""><li id="141b" class="nc nd iq kz b la lb ld le lg ne lk nf lo ng ls nh ni nj nk bi translated">在<strong class="kz ir">步骤1:选择一个亚马逊机器映像(AMI) </strong>中，选择<strong class="kz ir">深度学习AMI (Ubuntu) AMI </strong>。使用这个映像确实会引入一些额外的开销，但是，它保证我们会预装git和Docker，这就省去了很多麻烦。</li><li id="aed0" class="nc nd iq kz b la nl ld nm lg nn lk no lo np ls nh ni nj nk bi translated">在<strong class="kz ir">步骤2:选择实例类型</strong>中，选择<strong class="kz ir"> t2.large </strong>以确保我们有足够的空间来构建和运行我们的Docker映像。</li><li id="55bf" class="nc nd iq kz b la nl ld nm lg nn lk no lo np ls nh ni nj nk bi translated">在<strong class="kz ir">步骤6:配置安全组</strong>中，选择<strong class="kz ir">添加规则</strong>并为端口<strong class="kz ir"> 8501 </strong>创建自定义tcp规则，以使我们的streamlit应用程序公开可用。</li><li id="0560" class="nc nd iq kz b la nl ld nm lg nn lk no lo np ls nh ni nj nk bi translated">点击<strong class="kz ir">启动</strong>后，选择<strong class="kz ir">创建新的密钥对</strong>，输入“<strong class="kz ir"> ec2-gpt2-streamlit-app </strong>，点击“<strong class="kz ir">下载密钥对</strong>”将<code class="fe oc od oe nu b">ec2-gpt2-streamlit-app.pem</code>密钥对保存到本地。</li></ul><h2 id="bff4" class="ml lu iq bd lv mm mn dn lz mo mp dp md lg mq mr mf lk ms mt mh lo mu mv mj mw bi translated">7.2.在云中运行Docker容器</h2><p id="e66f" class="pw-post-body-paragraph kx ky iq kz b la mx jr lc ld my ju lf lg mz li lj lk na lm ln lo nb lq lr ls ij bi translated">启动EC2实例后，使用SSH连接到该实例:</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="d6e0" class="ml lu iq nu b gy ny nz l oa ob">ssh -i ec2-gpt2-streamlit-app.pem ubuntu@your-instance-DNS-address.us-east-1.compute.amazonaws.com</span></pre><p id="9a80" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">然后，使用<code class="fe oc od oe nu b">git</code>将我的代码复制到云中:</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="f202" class="ml lu iq nu b gy ny nz l oa ob">git clone <a class="ae kw" href="https://github.com/haocai1992/GPT2-News-Classifier.git" rel="noopener ugc nofollow" target="_blank">https://github.com/haocai1992/GPT2-News-Classifier.git</a></span></pre><p id="923a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">然后，进入<code class="fe oc od oe nu b">ec2-docker-deploy</code>文件夹构建并运行映像:</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="5b82" class="ml lu iq nu b gy ny nz l oa ob">cd ec2-docker-deploy/<br/>docker image build -t streamlit:gpt2-news-classifier-app .<br/>docker container run -p 8501:8501 -d streamlit:gpt2-news-classifier-app</span></pre><p id="d7db" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在，您可以在<code class="fe oc od oe nu b">http://&lt;EC2 public IP address&gt;:8501</code>访问Streamlit应用程序(EC2公共IP地址可以在AWS控制台的“IPv4公共IP”下找到)！</p><h1 id="a378" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">8.摘要</h1><p id="42fc" class="pw-post-body-paragraph kx ky iq kz b la mx jr lc ld my ju lf lg mz li lj lk na lm ln lo nb lq lr ls ij bi translated">我希望你能从这篇文章中了解到，使用亚马逊SageMaker训练和部署一个深度学习模型一点也不复杂。无论如何，还有更简单的替代方案，比如Google Colab培训和Amazon EC2部署。希望这篇文章对你有用。</p><p id="44bc" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">所有的源代码都可以在这个Github回购中找到:<a class="ae kw" href="https://github.com/haocai1992/GPT2-News-Classifier" rel="noopener ugc nofollow" target="_blank">https://github.com/haocai1992/GPT2-News-Classifier</a></p><div class="oh oi gp gr oj ok"><a href="https://github.com/haocai1992/GPT2-News-Classifier" rel="noopener  ugc nofollow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd ir gy z fp op fr fs oq fu fw ip bi translated">GitHub-haokai 1992/gp T2-News-Classifier:一个运行GPT-2语言模型的Streamlit应用程序，用于文本…</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">一个Streamlit应用程序，运行用于文本分类的GPT-2语言模型，用Pytorch、Transformers和AWS构建…</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">github.com</p></div></div><div class="ot l"><div class="ou l ov ow ox ot oy kp ok"/></div></div></a></div><h1 id="d4d2" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">9.参考</h1><ul class=""><li id="63c2" class="nc nd iq kz b la mx ld my lg oz lk pa lo pb ls nh ni nj nk bi translated"><strong class="kz ir">特征图</strong>:帕特里克·托马索<a class="ae kw" href="https://unsplash.com/@impatrickt?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">在</a><a class="ae kw" href="https://unsplash.com/s/photos/machine-learning-news?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。<a class="ae kw" href="https://unsplash.com/photos/BQTHOGNHo08" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/BQTHOGNHo08</a>。</li><li id="f405" class="nc nd iq kz b la nl ld nm lg nn lk no lo np ls nh ni nj nk bi translated"><strong class="kz ir"> GPT-2 vs伯特</strong>:<a class="ae kw" href="https://judithvanstegeren.com/blog/2020/GPT2-and-BERT-a-comparison.html" rel="noopener ugc nofollow" target="_blank">https://judithvanstegeren . com/blog/2020/gp T2-and-BERT-a-comparison . html</a></li><li id="3dff" class="nc nd iq kz b la nl ld nm lg nn lk no lo np ls nh ni nj nk bi translated">英国广播公司新闻分类数据集:d .格林和p .坎宁安。核心文档聚类中对角优势问题的实际解决方案。ICML 2006。</li><li id="330c" class="nc nd iq kz b la nl ld nm lg nn lk no lo np ls nh ni nj nk bi translated"><strong class="kz ir">用于文本分类的GPT-2</strong>:https://github.com/huggingface/transformers/issues/3168<a class="ae kw" href="https://github.com/huggingface/transformers/issues/3168" rel="noopener ugc nofollow" target="_blank"/></li><li id="f7af" class="nc nd iq kz b la nl ld nm lg nn lk no lo np ls nh ni nj nk bi translated"><strong class="kz ir"> GPT2用于使用拥抱脸的文本分类🤗变形金刚</strong>(作者<a class="ae kw" href="https://gmihaila.medium.com/" rel="noopener">乔治·米海拉</a>):<a class="ae kw" href="https://gmihaila.medium.com/gpt2-for-text-classification-using-hugging-face-transformers-574555451832" rel="noopener">https://gmi haila . medium . com/gp T2-for-text-class ification-using-hugging-face-transformers-574555451832</a></li><li id="55f3" class="nc nd iq kz b la nl ld nm lg nn lk no lo np ls nh ni nj nk bi translated"><strong class="kz ir">在AWS sage maker</strong>:<a class="ae kw" href="https://medium.com/@thom.e.lane/streamlit-on-aws-a-fully-featured-solution-for-streamlit-deployments-ba32a81c7460" rel="noopener">https://medium . com/@ thom . e . lane/streamlit-on-AWS-a-full-featured-solution-for-streamlit-deployments-ba 32 a 81 c 7460</a></li><li id="c631" class="nc nd iq kz b la nl ld nm lg nn lk no lo np ls nh ni nj nk bi translated"><strong class="kz ir">在AWS EC2上部署Streamlit app</strong>:<a class="ae kw" href="https://medium.com/usf-msds/deploying-web-app-with-streamlit-docker-and-aws-72b0d4dbcf77" rel="noopener">https://medium . com/usf-MSDS/deploying-web-app-with-Streamlit-docker-and-AWS-72 b 0 D4 dbcf 77</a></li></ul></div><div class="ab cl pc pd hu pe" role="separator"><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph"/></div><div class="ij ik il im in"><h1 id="ae3f" class="lt lu iq bd lv lw pj ly lz ma pk mc md jw pl jx mf jz pm ka mh kc pn kd mj mk bi translated">接触</h1><ul class=""><li id="e1ce" class="nc nd iq kz b la mx ld my lg oz lk pa lo pb ls nh ni nj nk bi translated"><strong class="kz ir">作者</strong>:蔡皋</li><li id="40b2" class="nc nd iq kz b la nl ld nm lg nn lk no lo np ls nh ni nj nk bi translated"><strong class="kz ir">电子邮件</strong>:<a class="ae kw" href="https://haocai1992.github.io/data/science/2022/01/29/haocai3@gmail.com" rel="noopener ugc nofollow" target="_blank">haocai3@gmail.com</a></li><li id="805d" class="nc nd iq kz b la nl ld nm lg nn lk no lo np ls nh ni nj nk bi translated"><strong class="kz ir">Github</strong>:<a class="ae kw" href="https://github.com/haocai1992" rel="noopener ugc nofollow" target="_blank">https://github.com/haocai1992</a></li><li id="3a56" class="nc nd iq kz b la nl ld nm lg nn lk no lo np ls nh ni nj nk bi translated"><strong class="kz ir">领英</strong>:<a class="ae kw" href="https://www.linkedin.com/in/haocai1992/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/haocai1992/</a></li></ul></div></div>    
</body>
</html>