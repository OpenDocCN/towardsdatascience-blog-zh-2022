<html>
<head>
<title>7 Evaluation Metrics for Clustering Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">聚类算法的 7 个评估指标</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2#2022-12-09">https://towardsdatascience.com/7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2#2022-12-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="53e2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用 Python 示例深入解释无监督学习评估指标</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9194206521fe948a1cf5e5930fc3a5ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Y5-UL3_FOhtqbWB4"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">马库斯·斯皮斯克在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="9789" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在监督学习中，标签是已知的，可以通过将预测值与标签进行比较来计算正确程度，从而进行评估。然而，在无监督学习中，标签是未知的，这使得很难评估正确的程度，因为没有<strong class="lb iu">地面真理</strong>。</p><p id="4a9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也就是说，<em class="lv">好的</em>聚类算法的聚类具有<strong class="lb iu">小的类内方差</strong>(类中的数据点彼此相似)和<strong class="lb iu">大的类间方差</strong>(类与其他类不同)。</p><p id="d6b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有两种类型的聚类评估指标，</p><ul class=""><li id="3918" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><strong class="lb iu">外在测量</strong>:这些测量需要基本事实标签，但在实践中可能无法获得</li><li id="a5f9" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><strong class="lb iu">内在度量</strong>:这些度量不需要基础事实标签(适用于所有无监督学习结果)</li></ul><p id="3b37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文将讨论聚类算法的各种评估指标，重点是它们的定义、直觉、何时使用它们，以及如何用<code class="fe mk ml mm mn b">sklearn</code>库实现它们。所有算法的公式都可以在文章的附录部分找到。</p><blockquote class="mo mp mq"><p id="3792" class="kz la lv lb b lc ld ju le lf lg jx lh mr lj lk ll ms ln lo lp mt lr ls lt lu im bi translated"><strong class="lb iu">注</strong>:我手动检查了所有的算法和公式，如果你需要计算，请联系我！另外，对于每种算法，变量和公式都用文字和等式来解释，以便更好地理解——更多信息请见附录:)</p></blockquote><h1 id="0594" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">目录</h1><h2 id="fe79" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated"><strong class="ak">外在措施</strong></h2><ul class=""><li id="bd13" class="lw lx it lb b lc ny lf nz li oa lm ob lq oc lu mb mc md me bi translated"><a class="ae ky" href="https://medium.com/p/bdc537ff54d2/#7226" rel="noopener">兰德指数</a></li><li id="8cc4" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><a class="ae ky" href="https://medium.com/p/bdc537ff54d2/#ebd4" rel="noopener">相互信息</a></li><li id="6bb9" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><a class="ae ky" href="https://medium.com/p/bdc537ff54d2/#7bda" rel="noopener">垂直测量</a></li><li id="661d" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><a class="ae ky" href="https://medium.com/p/bdc537ff54d2/#2a9e" rel="noopener">福尔克斯-马洛得分</a></li></ul><h2 id="28e5" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">内在措施</h2><ul class=""><li id="e05e" class="lw lx it lb b lc ny lf nz li oa lm ob lq oc lu mb mc md me bi translated"><a class="ae ky" href="https://medium.com/p/bdc537ff54d2/#2071" rel="noopener">轮廓系数</a></li><li id="ca51" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><a class="ae ky" href="https://medium.com/p/bdc537ff54d2/#949d" rel="noopener">卡林斯基-哈拉巴斯指数</a></li><li id="c5d9" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><a class="ae ky" href="https://medium.com/p/bdc537ff54d2/#5f41" rel="noopener">戴维斯-波尔丁指数</a></li></ul><h2 id="2cdb" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated"><a class="ae ky" href="https://medium.com/p/bdc537ff54d2/#0968" rel="noopener">附录:措施公式</a></h2></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/e8cd66a1662de9bacdcb7de1d63d4bff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bMgCIkpBI0sx6x_D"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@angelekamp?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">安格拉·坎普</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><blockquote class="ol"><p id="6e2e" class="om on it bd oo op oq or os ot ou lu dk translated">外在测量需要地面真实标签，这可能是不可用的或需要人工标记。</p></blockquote><h1 id="7226" class="mu mv it bd mw mx my mz na nb nc nd ne jz ov ka ng kc ow kd ni kf ox kg nk nl bi translated">№1.兰德指数</h1><blockquote class="mo mp mq"><p id="4574" class="kz la lv lb b lc ld ju le lf lg jx lh mr lj lk ll ms ln lo lp mt lr ls lt lu im bi translated">Rand Index (RI，ARI)通过进行成对比较来测量聚类分配之间的相似性。更高的分数意味着更高的相似性。</p></blockquote><p id="9a5f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于每一对，如果当它们在同一个聚类中时，预测该对在同一个聚类中，则认为是正确的(有点像“真正”)，如果当它们确实在不同的聚类中时，预测该对在不同的聚类中，则认为是正确的(有点像“真负”)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/613f3f283d64e7eef82aa88d50fe4ee4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pDf3jrL-VFVxgV2ITPfcig.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 1:兰德指数公式——作者图片</p></figure><p id="d686" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，兰德指数不考虑偶然性；如果群集分配是随机的，那么可能会有许多侥幸“真阴性”的情况。理想情况下，我们希望随机(统一)标签分配的分数接近 0，这需要随机调整。</p><p id="d39d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">调整后的 Rand 指数(ARI) </strong>通过贴现一个机会标准化项来调整机会。ARI 的公式可以在本文的附录(图 2)中找到，以避免视觉混乱。</p><h2 id="06ab" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">何时使用兰德指数</h2><ul class=""><li id="7a57" class="lw lx it lb b lc ny lf nz li oa lm ob lq oc lu mb mc md me bi translated">你想要<strong class="lb iu">可解释性</strong> : RI 直观且易于理解。</li><li id="66a0" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">您<strong class="lb iu">对聚类结构</strong>不确定:RI 和 ARI 不对聚类结构做出假设，并且可以应用于所有聚类算法。</li><li id="4854" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">你想要一个<strong class="lb iu">的比较基础</strong> : RI 有界在<code class="fe mk ml mm mn b">[0, 1]</code>区间，ARI 有界在<code class="fe mk ml mm mn b">[-1, 1]</code>区间。有界范围便于比较不同算法之间的分数。</li></ul><h2 id="449f" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">何时不使用 Rand 指数</h2><ul class=""><li id="4462" class="lw lx it lb b lc ny lf nz li oa lm ob lq oc lu mb mc md me bi translated">您没有基本事实标签:RI 和 ARI 是外部度量，需要基本事实聚类分配。</li></ul><h2 id="607d" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">实施兰德指数</h2><pre class="kj kk kl km gt oz mn pa bn pb pc bi"><span id="32f5" class="pd mv it mn b be pe pf l pg ph">from sklearn.metrics import rand_score, adjusted_rand_score<br/>labels = [0, 0, 0, 1, 1, 1]<br/>labels_pred = [1, 1, 2, 2, 3, 3]<br/><br/>RI = rand_score(labels, labels_pred)<br/>ARI = adjusted_rand_score(labels, labels_pred)</span></pre></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="ebd4" class="mu mv it bd mw mx pi mz na nb pj nd ne jz pk ka ng kc pl kd ni kf pm kg nk nl bi translated">№2.相互信息(MI、NMI、AMI)</h1><blockquote class="mo mp mq"><p id="c880" class="kz la lv lb b lc ld ju le lf lg jx lh mr lj lk ll ms ln lo lp mt lr ls lt lu im bi translated">互信息(MI、NMI、AMI)测量集群分配之间的一致性。更高的分数意味着更高的相似性。</p></blockquote><p id="04e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">聚类之间的一致程度通过联合概率和边际概率来计算。互信息有两种变体:<strong class="lb iu">归一化互信息(NMI) </strong>和<strong class="lb iu">调整互信息(AMI) </strong>。</p><p id="cbcb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">归一化 MI 是指 MI 除以平均聚类熵，通常在文献中使用，而调整 MI 是指通过贴现机会归一化项对机会进行调整的归一化 MI。MI、NMI 和 AMI 的公式可在本文的附录中找到(图 3 和图 4)。</p><h2 id="5897" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">何时使用互信息</h2><ul class=""><li id="98c8" class="lw lx it lb b lc ny lf nz li oa lm ob lq oc lu mb mc md me bi translated">你需要一个<strong class="lb iu">的比较基础</strong> : MI、NMI 和 AMI 的上界都是 1。</li></ul><h2 id="b31c" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">何时不使用交互信息</h2><ul class=""><li id="039d" class="lw lx it lb b lc ny lf nz li oa lm ob lq oc lu mb mc md me bi translated">您没有基本事实标签:MI、NMI 和 AMI 是外部度量，需要基本事实聚类分配。</li></ul><h2 id="0fc3" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">实现相互信息</h2><pre class="kj kk kl km gt oz mn pa bn pb pc bi"><span id="cd3e" class="pd mv it mn b be pe pf l pg ph">from sklearn.metrics import (<br/>    mutual_info_score,<br/>    normalized_mutual_info_score,<br/>    adjusted_mutual_info_score,<br/>)<br/>labels = [0, 0, 0, 1, 1, 1]<br/>labels_pred = [1, 1, 2, 2, 3, 3]<br/><br/>MI = mutual_info_score(labels, labels_pred)<br/>NMI = normalized_mutual_info_score(labels, labels_pred)<br/>AMI = adjusted_mutual_info_score(labels, labels_pred)</span></pre></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="7bda" class="mu mv it bd mw mx pi mz na nb pj nd ne jz pk ka ng kc pl kd ni kf pm kg nk nl bi translated">№3.垂直测量</h1><blockquote class="mo mp mq"><p id="9f04" class="kz la lv lb b lc ld ju le lf lg jx lh mr lj lk ll ms ln lo lp mt lr ls lt lu im bi translated">V-measure 使用条件熵分析来测量聚类分配的正确性。更高的分数意味着更高的相似性。</p></blockquote><p id="8945" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">两个度量标准衡量聚类分配的正确性，这是直观的，因为它们来自监督学习。</p><ul class=""><li id="49da" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><strong class="lb iu">同质性</strong>:每个集群只包含一个类的成员(有点像“<em class="lv">精度</em>”)</li><li id="44dd" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><strong class="lb iu">完整性</strong>:给定类的所有成员都被分配到同一个集群中(有点像“<em class="lv">回忆</em>”)</li></ul><p id="380c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">V-measure 是同质性和完整性度量的调和平均值，类似于 F-score 是精确度和召回率的调和平均值。同质性、完整性和 V-measure 的公式可以在本文的附录中找到(图 5)。</p><h2 id="5ba0" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">何时使用垂直测量</h2><ul class=""><li id="2843" class="lw lx it lb b lc ny lf nz li oa lm ob lq oc lu mb mc md me bi translated">您需要<strong class="lb iu">可解释性</strong> : V-measure 在同质性和完整性方面直观且易于理解。</li><li id="3dfd" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">您<strong class="lb iu">对聚类结构</strong>不确定:V-measure 不对聚类结构做出假设，并且可以应用于所有聚类算法。</li><li id="a408" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">您需要一个<strong class="lb iu">基础来进行比较</strong>:同质性、完整性和 V-measure 在<code class="fe mk ml mm mn b">[0, 1]</code>范围内。有界范围便于比较不同算法之间的分数。</li></ul><h2 id="d4a9" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">何时不使用垂直测量</h2><ul class=""><li id="8bca" class="lw lx it lb b lc ny lf nz li oa lm ob lq oc lu mb mc md me bi translated">您没有基本事实标签:同质性、完整性和 V-measure 是外在度量，需要基本事实聚类分配。</li><li id="b183" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">您的样本大小小于 1000，而聚类数大于 10: V-measure 不针对机会进行调整。这意味着随机标记不会产生零分，尤其是当聚类数量很大时。</li></ul><h2 id="6e88" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">实施垂直测量</h2><pre class="kj kk kl km gt oz mn pa bn pb pc bi"><span id="983a" class="pd mv it mn b be pe pf l pg ph">from sklearn.metrics import (<br/>    homogeneity_score,<br/>    completeness_score,<br/>    v_measure_score,<br/>)<br/>labels = [0, 0, 0, 1, 1, 1]<br/>labels_pred = [1, 1, 2, 2, 3, 3]<br/><br/>HS = homogeneity_score(labels, labels_pred)<br/>CS = completeness_score(labels, labels_pred)<br/>V = v_measure_score(labels, labels_pred, beta=1.0)</span></pre></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="2a9e" class="mu mv it bd mw mx pi mz na nb pj nd ne jz pk ka ng kc pl kd ni kf pm kg nk nl bi translated">№4.福尔克斯-马洛分数</h1><blockquote class="mo mp mq"><p id="7883" class="kz la lv lb b lc ld ju le lf lg jx lh mr lj lk ll ms ln lo lp mt lr ls lt lu im bi translated">Fowlkes-Mallows 分数使用成对精度和召回率来测量聚类分配的正确性。更高的分数意味着更高的相似性。</p></blockquote><p id="06d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">V-measure 是同质性("<em class="lv">精度</em>")和完整性("<em class="lv">召回</em>")之间的调和平均值，而 Fowlkes-Mallows 指数(FMI)是成对精度和召回的几何平均值，使用真阳性(TP)、假阳性(FP)和假阴性(FN)。</p><p id="031b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">We Fowlkes-Mallows 评分不考虑真阴性(TN)，它不会受到偶然性的影响，也不需要偶然性调整，不像 Rand 指数和互信息。</p><p id="c58e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TP、FP、FN 的定义以及 Fowlkes-Mallows 指数(FMI)的公式见本文附录(图 6 和图 7)。</p><h2 id="4ab9" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">何时使用 Fowlkes-Mallows 评分</h2><ul class=""><li id="a87e" class="lw lx it lb b lc ny lf nz li oa lm ob lq oc lu mb mc md me bi translated">你<strong class="lb iu">不确定聚类结构</strong> : Fowlkes-Mallows Score 不对聚类结构做任何假设，可以应用于所有聚类算法。</li><li id="8e4c" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">你要一个<strong class="lb iu">的对比基础</strong> : Fowlkes-Mallows 评分有一个上限<code class="fe mk ml mm mn b">1</code>。有界范围便于比较不同算法之间的分数。</li></ul><h2 id="d8c6" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">何时不使用福尔克斯-马洛分数</h2><ul class=""><li id="7f64" class="lw lx it lb b lc ny lf nz li oa lm ob lq oc lu mb mc md me bi translated">您没有基本事实标签:Fowlkes-Mallows 分数是外在的度量，需要基本事实聚类分配。</li></ul><h2 id="57ae" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">实现 Fowlkes-Mallows 评分</h2><pre class="kj kk kl km gt oz mn pa bn pb pc bi"><span id="3164" class="pd mv it mn b be pe pf l pg ph">from sklearn.metrics import fowlkes_mallows_score<br/>labels = [0, 0, 0, 1, 1, 1]<br/>labels_pred = [1, 1, 2, 2, 3, 3]<br/><br/>FMI = fowlkes_mallows_score(labels, labels_pred)</span></pre></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pn"><img src="../Images/e2a13d8b6c6af324d9cd74edd2e00f06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GpfER1FYG7XNR_wY"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@bamin?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">皮埃尔·巴明</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><blockquote class="ol"><p id="c1d9" class="om on it bd oo op oq or os ot ou lu dk translated">内在的措施不需要地面真相标签，使他们适用于所有聚类结果</p></blockquote><h1 id="2071" class="mu mv it bd mw mx my mz na nb nc nd ne jz ov ka ng kc ow kd ni kf ox kg nk nl bi translated">№5.轮廓系数</h1><blockquote class="mo mp mq"><p id="aa7c" class="kz la lv lb b lc ld ju le lf lg jx lh mr lj lk ll ms ln lo lp mt lr ls lt lu im bi translated">剪影系数衡量聚类之间的距离与聚类内的距离。较高的分数表示更好定义的聚类。</p></blockquote><p id="8da0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">样本的轮廓系数测量样本与下一个最近聚类中所有其他点相对于其聚类中所有其他点的平均距离。较高的比率表示该聚类远离其最近的聚类，并且该聚类被更好地定义。</p><p id="b08c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一组样本的轮廓系数取每个样本的平均轮廓系数。该公式可在本文的附录中找到(图 8)。</p><h2 id="41e3" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">何时使用轮廓系数</h2><ul class=""><li id="4db1" class="lw lx it lb b lc ny lf nz li oa lm ob lq oc lu mb mc md me bi translated">你要<strong class="lb iu">可解释性</strong>:廓形系数直观易懂。</li><li id="8c5d" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">你要一个<strong class="lb iu">的比较基础</strong>:剪影系数有一个<code class="fe mk ml mm mn b">[-1, 1]</code>的范围，从不正确的聚类到高度密集的聚类，<code class="fe mk ml mm mn b">0</code>是重叠的聚类。有界范围便于比较不同算法之间的分数。</li><li id="a20a" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">您将好的聚类定义为<strong class="lb iu">定义明确的聚类</strong>:剪影系数遵循好的聚类的一般定义，即密集且分离良好。</li></ul><h2 id="2562" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">何时不使用轮廓系数</h2><ul class=""><li id="b5e0" class="lw lx it lb b lc ny lf nz li oa lm ob lq oc lu mb mc md me bi translated">您正在比较不同类型的聚类算法:基于密度的聚类算法的轮廓系数得分往往更高，与其他类型的聚类算法进行比较是不公平的。</li></ul><h2 id="dd61" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">实施轮廓系数</h2><pre class="kj kk kl km gt oz mn pa bn pb pc bi"><span id="94fc" class="pd mv it mn b be pe pf l pg ph">from sklearn.metrics import silhouette_score<br/>data = [<br/>    [5.1, 3.5, 1.4, 0.2],<br/>    [4.9, 3. , 1.4, 0.2],<br/>    [4.7, 3.2, 1.3, 0.2],<br/>    [4.6, 3.1, 1.5, 0.2],<br/>    [5. , 3.6, 1.4, 0.2],<br/>    [5.4, 3.9, 1.7, 0.4],<br/>]<br/>clusters = [1, 1, 2, 2, 3, 3]<br/><br/>s = silhouette_score(data, clusters, metric="euclidean")</span></pre></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="949d" class="mu mv it bd mw mx pi mz na nb pj nd ne jz pk ka ng kc pl kd ni kf pm kg nk nl bi translated">№6.卡林斯基-哈拉巴斯指数</h1><blockquote class="mo mp mq"><p id="e873" class="kz la lv lb b lc ld ju le lf lg jx lh mr lj lk ll ms ln lo lp mt lr ls lt lu im bi translated">Calinski-Harabasz 指数衡量的是组间离差和组内离差。较高的分数表示更好定义的聚类。</p></blockquote><p id="20d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Calinski-Harabasz 指数或方差比标准衡量组间离差之和与组内离差之和，其中离差是距离平方之和。</p><p id="446b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">较高的比率表示该分类远离其其他分类，并且该分类更加明确。该公式可在本文的附录中找到(图 9)。</p><h2 id="5bb2" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">何时使用卡林斯基-哈拉巴斯指数</h2><ul class=""><li id="73e7" class="lw lx it lb b lc ny lf nz li oa lm ob lq oc lu mb mc md me bi translated">你想要效率:卡林斯基-哈拉巴斯指数计算起来很快</li><li id="8c1d" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">你将好的集群定义为<strong class="lb iu">定义明确的集群</strong> : Calinski-Harabasz 指数遵循好的集群的一般定义，即密集且分离良好。</li></ul><h2 id="a0c2" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">何时不使用卡林斯基-哈拉巴斯指数</h2><ul class=""><li id="62b7" class="lw lx it lb b lc ny lf nz li oa lm ob lq oc lu mb mc md me bi translated">您正在比较不同类型的聚类算法:对于基于密度的聚类算法，Calinski-Harabasz 指数往往更高，与其他类型的聚类算法进行比较是不公平的。</li></ul><h2 id="243f" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">实施卡林斯基-哈拉巴斯指数</h2><pre class="kj kk kl km gt oz mn pa bn pb pc bi"><span id="f511" class="pd mv it mn b be pe pf l pg ph">from sklearn.metrics import calinski_harabasz_score<br/>data = [<br/>    [5.1, 3.5, 1.4, 0.2],<br/>    [4.9, 3. , 1.4, 0.2],<br/>    [4.7, 3.2, 1.3, 0.2],<br/>    [4.6, 3.1, 1.5, 0.2],<br/>    [5. , 3.6, 1.4, 0.2],<br/>    [5.4, 3.9, 1.7, 0.4],<br/>]<br/>clusters = [1, 1, 2, 2, 3, 3]<br/><br/>s = calinski_harabasz_score(data, clusters)</span></pre></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="5f41" class="mu mv it bd mw mx pi mz na nb pj nd ne jz pk ka ng kc pl kd ni kf pm kg nk nl bi translated">№7.戴维斯-波尔丁指数</h1><blockquote class="mo mp mq"><p id="9d1e" class="kz la lv lb b lc ld ju le lf lg jx lh mr lj lk ll ms ln lo lp mt lr ls lt lu im bi translated">Davies-Bouldin 指数根据簇之间的平均距离来测量簇的大小。较低的分数表示更好定义的聚类。</p></blockquote><p id="0916" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Davies-Bouldin 指数衡量聚类之间的平均相似性，其中相似性将聚类的大小与聚类之间的距离进行比较。</p><p id="433d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">较低的分数意味着该聚类与到另一个聚类的距离相比相对较小，因此定义良好。该公式可在本文的附录中找到(图 10)。</p><h2 id="920c" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">何时使用戴维斯-波尔丁指数</h2><ul class=""><li id="d930" class="lw lx it lb b lc ny lf nz li oa lm ob lq oc lu mb mc md me bi translated">你想要<strong class="lb iu">可解释性</strong>:戴维斯-波尔丁指数比剪影分数更容易计算，它使用逐点距离。</li></ul><h2 id="1d84" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">何时不使用戴维斯-波尔丁指数</h2><ul class=""><li id="fc9a" class="lw lx it lb b lc ny lf nz li oa lm ob lq oc lu mb mc md me bi translated">您正在比较不同类型的聚类算法:基于密度的聚类的 Davies-Bouldin 指数往往更高，与其他类型的聚类算法进行比较是不公平的。</li><li id="87dd" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">除了欧几里德距离之外，您还需要其他距离度量:由质心距离计算的聚类大小将距离度量限制在欧几里德空间。</li></ul><h2 id="883a" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">实施戴维斯-波尔丁指数</h2><pre class="kj kk kl km gt oz mn pa bn pb pc bi"><span id="40ac" class="pd mv it mn b be pe pf l pg ph">from sklearn.metrics import davies_bouldin_score<br/>data = [<br/>    [5.1, 3.5, 1.4, 0.2],<br/>    [4.9, 3. , 1.4, 0.2],<br/>    [4.7, 3.2, 1.3, 0.2],<br/>    [4.6, 3.1, 1.5, 0.2],<br/>    [5. , 3.6, 1.4, 0.2],<br/>    [5.4, 3.9, 1.7, 0.4],<br/>]<br/>clusters = [1, 1, 2, 2, 3, 3]<br/><br/>DB = davies_bouldin_score(data, clusters)</span></pre></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="df36" class="mu mv it bd mw mx pi mz na nb pj nd ne jz pk ka ng kc pl kd ni kf pm kg nk nl bi translated">结论</h1><p id="77e8" class="pw-post-body-paragraph kz la it lb b lc ny ju le lf nz jx lh li po lk ll lm pp lo lp lq pq ls lt lu im bi translated">每种算法的特征总结如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/4c8f89837d0d17343c5a68e8d8c1aeaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hXuKtq71Gsa06rJeZa1K6A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表 1:聚类算法的特征—按作者分类的图片</p></figure><p id="cc15" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望您已经更多地了解了评估聚类算法的不同方法——根据您是否有基本事实标签，使用内部和外部度量。在实践中，我们可能更关心集群<strong class="lb iu">是否有商业意义</strong>，而不是通过统计测量集群内部或之间的距离。尽管如此，这些评估指标仍然值得了解！</p></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="0968" class="mu mv it bd mw mx pi mz na nb pj nd ne jz pk ka ng kc pl kd ni kf pm kg nk nl bi translated">附录</h1><h2 id="604a" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">兰德指数(ARI)公式</h2><p id="cba3" class="pw-post-body-paragraph kz la it lb b lc ny ju le lf nz jx lh li po lk ll lm pp lo lp lq pq ls lt lu im bi translated">机会标准化项考虑在实际聚类分配和预测聚类分配中出现在同一聚类中的对的数量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ps"><img src="../Images/2875a8cf9164db13e489c0a399893942.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nO5X_UDLKsBLoSG2N-S_MQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 2:机会标准化术语和调整后的 Rand 指数的公式——作者图片</p></figure></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h2 id="8f02" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">互信息公式(MI，NMI，AMI)</h2><p id="bfa2" class="pw-post-body-paragraph kz la it lb b lc ny ju le lf nz jx lh li po lk ll lm pp lo lp lq pq ls lt lu im bi translated">联合概率、边际概率和熵的公式构成了计算互信息的基础。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pt"><img src="../Images/82b4f91815ed59acd70fda187587fa01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8zfVKjTfqnfNz4iODeT38Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3:联合概率、边际概率和熵的公式——作者图片</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pu"><img src="../Images/87f2d5ff1a19b04e022262ca71791162.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lb6xPicIylntqn1OD35kEA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 4:MI、标准化 MI 和调整 MI 的公式——作者图片</p></figure></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h2 id="0d32" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">V-measure 公式</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pv"><img src="../Images/bded65cfd5a9431bdc7dd1654f0de64d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mSv7G_LE_flWXKHWAFU2Hw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 5:同质性、完整性和 V-measure 公式——作者图片</p></figure></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h2 id="152a" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">福尔克斯-马洛得分公式</h2><p id="7985" class="pw-post-body-paragraph kz la it lb b lc ny ju le lf nz jx lh li po lk ll lm pp lo lp lq pq ls lt lu im bi translated">Fowlkes-Mallows 指数(FMI)是使用真阳性、假阳性和假阴性计算的。TP、FP 和 FN 的定义是通过计算成对点的数量来完成的，如果它们被分配在预测和实际标签的相同或不同聚类中的话。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pw"><img src="../Images/ffe49e9b25c1dba8fc3f2b6a60751e20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qJVJuaxuMZpwDq_ZMd95hA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 6:TP、FP、TN、FN 的定义——作者图片</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi px"><img src="../Images/ecb347f1c6a844cba0eb7e15a2a0f22a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AHBDarOoRCPFb37ctl4BTQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 7:fowl KES-Mallows 评分公式——作者图片</p></figure></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h2 id="aebe" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">轮廓系数公式</h2><p id="89b9" class="pw-post-body-paragraph kz la it lb b lc ny ju le lf nz jx lh li po lk ll lm pp lo lp lq pq ls lt lu im bi translated">请注意，对于<code class="fe mk ml mm mn b">b</code>的计算，它考虑的是距离样本本身最近的下一个聚类，而不是距离指定聚类最近的下一个聚类。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi py"><img src="../Images/a7806d215a43daee936e8aed8b7bdc0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dzEBdMir_SuvWDR2vZgvKQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 8:剪影系数公式—作者图片</p></figure></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h2 id="1776" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">卡林斯基-哈拉巴斯指数</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pz"><img src="../Images/da0a46206067341764639b89bd8e84bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qjWhuVv6rhG4-siM3V_XpQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 9:卡林斯基-哈拉巴斯指数公式——作者图片</p></figure></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h2 id="4a5f" class="nm mv it bd mw nn no dn na np nq dp ne li nr ns ng lm nt nu ni lq nv nw nk nx bi translated">戴维斯-波尔丁指数</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qa"><img src="../Images/8ff9d7dd56699cad524e68d0410793d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IYGG9cygelJRhBi_mCdMwg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 10:戴维斯-波尔丁指数公式——作者图片</p></figure></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="6ca0" class="mu mv it bd mw mx pi mz na nb pj nd ne jz pk ka ng kc pl kd ni kf pm kg nk nl bi translated">相关链接</h1><ul class=""><li id="db6a" class="lw lx it lb b lc ny lf nz li oa lm ob lq oc lu mb mc md me bi translated"><code class="fe mk ml mm mn b">sklearn</code>文档:<a class="ae ky" href="https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/clustering . html # clustering-performance-evaluation</a></li></ul></div></div>    
</body>
</html>