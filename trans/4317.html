<html>
<head>
<title>Object detection with Vision Transformer for Open-World Localization(OWL-ViT)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于开放世界定位(OWL-ViT)的视觉转换器的对象检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/object-detection-with-vision-transformer-for-open-world-localization-owl-vit-cb1ab6d4d7a9#2022-09-23">https://towardsdatascience.com/object-detection-with-vision-transformer-for-open-world-localization-owl-vit-cb1ab6d4d7a9#2022-09-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ad29" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用变压器进行物体检测</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/0d344dbbc75301f5e78c26328c886c2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2hq4hCigSblzJXt2"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">杰恩·贝勒在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="6d07" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">卷积神经网络已经成为应用于目标检测的主要网络。最近，变形金刚在自然语言处理和计算机视觉领域越来越受欢迎。</p><p id="7c83" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们探索了OWL-ViT在物体检测中的应用。</p><p id="b299" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们开始吧。</p><h1 id="fcdc" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">什么是视觉转换器？</h1><p id="11aa" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated"><a class="ae kv" href="https://huggingface.co/docs/transformers/index" rel="noopener ugc nofollow" target="_blank">变形金刚</a>已经广泛应用于自然语言处理任务。然而，它们最近被应用于计算机视觉——因此被命名为<strong class="ky ir">视觉转换器</strong>。</p><p id="beaf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://huggingface.co/docs/transformers/model_doc/vit" rel="noopener ugc nofollow" target="_blank">视觉变形金刚</a>的工作原理如下:</p><ul class=""><li id="9872" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">图像被分割成固定大小的小块。</li><li id="d9b3" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">图像补片被展平并线性嵌入。</li><li id="c85b" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">添加位置嵌入以保留位置信息。</li><li id="6ce4" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">将它们传送到<strong class="ky ir">标准变压器编码器</strong>。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/37a8e77f7d30e47aa1600ea2c745166d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pIv_HaR19tCxEiC9"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://arxiv.org/pdf/2010.11929v2.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h1 id="1f63" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">OWL-ViT是如何工作的？</h1><p id="0bdc" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">OWL-ViT模型是一个开放词汇表的对象检测模型，它使用标准的视觉转换器来执行检测。变压器通过以下方式用于物体检测:</p><ul class=""><li id="06d8" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">用分类和箱头替换最终的令牌池层。</li><li id="08fa" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">通过用来自文本模型的类名嵌入改变固定的分类层权重来实现开放词汇分类。</li><li id="8fad" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">使用<a class="ae kv" href="https://arxiv.org/pdf/2005.12872.pdf" rel="noopener ugc nofollow" target="_blank">二分匹配损失</a>对具有异议检测数据集的预训练模型进行微调。</li><li id="5e97" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">端到端地微调文本和图像模型。</li></ul><p id="d325" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">OWL-ViT模型可以用作一次性检测学习器。在一次性学习中，模型根据每个类别中的一个或很少几个示例进行训练，并用于对未知样本进行预测。例如，当人类看到一只狮子时，他们可以很容易地在未来的各种环境中认出狮子。</p><p id="a426" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">OWL-ViT模型接受一个图像和一个或多个文本查询，并寻找与查询匹配的对象。</p><h1 id="a315" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">OWL-ViT架构</h1><p id="61fa" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在让我们看看OWL-ViT模型的架构。</p><p id="f846" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://huggingface.co/course/chapter1/5?fw=pt" rel="noopener ugc nofollow" target="_blank">仅编码器</a> OWL-ViT模型由以下人员开发:</p><ul class=""><li id="9029" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated"><strong class="ky ir">使用图像-文本对的图像和文本编码器的对比预训练</strong>。</li><li id="e102" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated"><strong class="ky ir">转移预先训练好的编码器</strong>打开词汇表。这是通过用异议检测和本地化头取代令牌池来实现的。物体检测和定位头附在图像编码器输出标记上。</li><li id="5b9c" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated"><strong class="ky ir">用文本编码器</strong>嵌入查询字符串进行开放词汇检测。然后将查询字符串用于分类。</li></ul><p id="8dd3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用焦点sigmoid交叉熵进行分类，因为图像可以具有多个标签。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/3e76ad16082bc130dcb2778ff8263f83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*U7uNTryFRt6LXVp_"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://arxiv.org/pdf/2205.06230.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="299c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用<strong class="ky ir">对比损失函数</strong>来完成训练，该函数鼓励对应的图像-文本对具有相似的嵌入，而不对应的对具有不同的嵌入。对比预训练包括同时学习文本和图像嵌入。目标是学习与图像嵌入一致的文本嵌入。多头注意力池(MAP)用于聚集图像表示。</p><p id="d7da" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">文本嵌入是通过将类别名称传递给文本编码器来获得的。在OWL-ViT中，文本嵌入被称为<strong class="ky ir">查询</strong>。OWL-ViT模型预测一个<strong class="ky ir">边界框</strong>和某个文本嵌入应用于特定对象的<strong class="ky ir">概率</strong>。OWL-ViT还可以用于对图像嵌入的查询进行预测，使得检测无法描述的对象成为可能。这是因为文本和图像编码器之间没有融合。</p><p id="c049" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://github.com/OpenAI/CLIP" rel="noopener ugc nofollow" target="_blank"> CLIP </a>(对比语言-图像预处理)作为OWL-ViT的主干。剪辑作品由:</p><ul class=""><li id="9d51" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">训练图像和文本编码器来预测文本和图像对。</li><li id="f2a7" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">在推理过程中执行零镜头分类。</li></ul><p id="b9ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">CLIP通过学习多模态嵌入空间来预测文本和图像对。这是通过训练图像编码器和文本编码器来最大化正确的图像和文本嵌入对之间的余弦相似性，同时使用对称损失函数来降低不正确对的余弦相似性来实现的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/1116ed1be2dcfeb2303c9c141f74ceed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1jRwnm-8qNRfoZyL"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://arxiv.org/pdf/2103.00020.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="badf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">CLIP有一些限制。它们包括:</p><ul class=""><li id="7c65" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">CLIP的零拍性能在区分车模、花种等一些任务上表现较弱。</li><li id="1886" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">为训练数据中没有的任务提供近乎随机的预测。</li><li id="c1ae" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">不适用于超出分布的图像分布。</li></ul><h1 id="6632" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">如何使用拥抱脸的OWL-ViT</h1><p id="7851" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">你可以通过克隆<a class="ae kv" href="https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit" rel="noopener ugc nofollow" target="_blank"> OWL-ViT </a> repo来使用这个模型。然而，你可以更快地使用来自<a class="ae kv" href="https://huggingface.co/docs/transformers/model_doc/owlvit#owlvit" rel="noopener ugc nofollow" target="_blank">拥抱面部变形金刚的模型。</a></p><p id="7946" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">拥抱脸提供以下功能:</p><ul class=""><li id="6df3" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">按照OWL-ViT的要求处理图像。</li><li id="6f75" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">对文本和图像输入运行预测。</li></ul><p id="089d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下面的例子中，我们:</p><ul class=""><li id="150e" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">使用Pillow下载图像。</li><li id="da82" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">使用<code class="fe nf ng nh ni b">OwlViTProcessor</code>函数处理图像并标记文本查询。处理器负责调整图像的大小、缩放和标准化。使用片段标记器对文本查询进行标记化。</li><li id="674e" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">将输入传递给<code class="fe nf ng nh ni b">OwlViTForObjectDetection</code>模型以获得对象预测。模型的输出是预测的逻辑、边界框、类、图像和文本嵌入。</li><li id="87c0" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">获取并打印给定文本查询的预测。</li></ul><pre class="kg kh ki kj gt nj ni nk bn nl nm bi"><span id="e6a0" class="nn lt iq ni b be no np l nq nr">import requests<br/>from PIL import Image<br/>import torch<br/><br/>from transformers import OwlViTProcessor, OwlViTForObjectDetection<br/><br/>processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")<br/>model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch32")<br/><br/>url = "https://images.unsplash.com/photo-1637336660118-b4635ce8790c?ixlib=rb-1.2.1&amp;ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;auto=format&amp;fit=crop&amp;w=1074&amp;q=80"<br/>image = Image.open(requests.get(url, stream=True).raw)<br/>texts = [["a photo of a zebra", "a photo of a gazelle"]]<br/>inputs = processor(text=texts, images=image, return_tensors="pt")<br/>outputs = model(**inputs)<br/><br/># Target image sizes (height, width) to rescale box predictions [batch_size, 2]<br/>target_sizes = torch.Tensor([image.size[::-1]])<br/># Convert outputs (bounding boxes and class logits) to COCO API<br/>results = processor.post_process(outputs=outputs, target_sizes=target_sizes)<br/><br/>i = 0  # Retrieve predictions for the first image for the corresponding text queries<br/>text = texts[i]<br/>boxes, scores, labels = results[i]["boxes"], results[i]["scores"], results[i]["labels"]<br/><br/>score_threshold = 0.1<br/>for box, score, label in zip(boxes, scores, labels):<br/>    box = [round(i, 2) for i in box.tolist()]<br/>    if score &gt;= score_threshold:<br/>        print(f"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/1a42bb35c5d1f33ed0e1f18b2d6e0994.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GuS6v-yog7Ub5oNC"/></div></div></figure><p id="4784" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">查看带有OWL-ViT 拥抱空间的<a class="ae kv" href="https://huggingface.co/spaces/adirik/OWL-ViT" rel="noopener ugc nofollow" target="_blank">零拍物体检测进行演示。</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/242ea43c9133d87c87531f686e13d5e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hwVv6A54PT-X8AmU"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><h1 id="b8cf" class="ls lt iq bd lu lv ob lx ly lz oc mb mc jw od jx me jz oe ka mg kc of kd mi mj bi translated">最后的想法</h1><p id="cd26" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在本文中，我们已经看到了如何使用OWL-ViT进行对象定向。特别是，您已经了解到:</p><ul class=""><li id="d3af" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">OWL-ViT如何工作</li><li id="45c3" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">OWL-ViT建筑</li><li id="c2ba" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">如何使用拥抱脸的OWL-ViT</li></ul><p id="1dab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://www.linkedin.com/in/mwitiderrick/" rel="noopener ugc nofollow" target="_blank">在LinkedIn上关注我</a>获取更多技术资源。</p></div></div>    
</body>
</html>