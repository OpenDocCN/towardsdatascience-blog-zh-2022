<html>
<head>
<title>A Comprehensive Guide to Microsoft’s Swin Transformer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">微软Swin Transformer综合指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-comprehensive-guide-to-swin-transformer-64965f89d14c#2022-05-20">https://towardsdatascience.com/a-comprehensive-guide-to-swin-transformer-64965f89d14c#2022-05-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1492" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深入的解释和动画</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/54b6e80dbe1d5dc510f9987f093639ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G557wN1aar3AdCZSaJb-mQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">免费使用来自<a class="ae ky" href="https://www.pexels.com/photo/a-woman-using-virtual-goggles-8728379/" rel="noopener ugc nofollow" target="_blank">像素</a>的图像。</p></figure><h1 id="383e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="ea58" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Swin Transformer(【刘】等，2021 )是一个基于Transformer的深度学习模型，在视觉任务中具有最先进的性能。与之前的视觉转换器(ViT) ( <a class="ae ky" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank"> Dosovitskiy等人，2020 </a>)不同，Swin Transformer效率高，精度更高。由于这些理想的特性，Swin变压器被用作当今许多基于视觉的模型架构的主干。</p><p id="9122" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">尽管它被广泛采用，但我发现在这个主题中缺乏详细解释的文章。因此，本文旨在使用插图和动画为Swin变压器提供全面的指南，以帮助您更好地理解概念。</p><p id="ebc1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们开始吧！</p><h1 id="bdb9" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">Swin变压器:改进ViT</h1><p id="5339" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">近年来，变形金刚<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> (Vaswani et al .，2017) </a>在自然语言处理(NLP)任务中主导了深度学习架构。《变形金刚》在NLP中的巨大成功激发了将《变形金刚》用于视觉任务的研究努力。</p><p id="6c5e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2020年，视觉变压器(ViT)获得了人工智能社区的大量关注，以其在视觉任务中具有良好结果的纯变压器架构而闻名。尽管vit很有前途，但仍有几个缺点。最值得注意的是，vit难以处理高分辨率图像，因为它的计算复杂度是图像大小的二次方。此外，ViTs中的固定标度标记不适用于视觉元素具有可变标度的视觉任务。</p><p id="6c25" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">ViT之后出现了一系列研究工作，其中大部分都对标准变压器架构进行了改进，以解决上述缺点。2021年，微软研究人员发表了Swin Transformer ( <a class="ae ky" href="https://arxiv.org/abs/2103.14030" rel="noopener ugc nofollow" target="_blank">刘等人，2021 </a>)，可以说是继最初的ViT之后最令人兴奋的研究之一。</p><h1 id="36bf" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">Swin变压器架构和关键概念</h1><p id="af17" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Swin Transformer引入了两个关键概念来解决原始ViT面临的问题— <strong class="lt iu">分层特征映射</strong>和<strong class="lt iu">转移窗口注意力。</strong>其实Swin变压器的名字来源于“<strong class="lt iu">S</strong>hifted<strong class="lt iu">win</strong>Dow<strong class="lt iu">Transformer</strong>”。Swin变压器的整体架构如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/8fd9c9117f33203a43eb212708c775f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EfsMkLk1zyCd1G_RKDSAPQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">整体Swin变压器架构。图片作者。改编自<a class="ae ky" href="https://arxiv.org/abs/2103.14030" rel="noopener ugc nofollow" target="_blank">刘等，2021 </a>。请注意，在本文中，“补丁分区”被用作第一块。为简单起见，我使用“补丁合并”作为此图中的第一个模块，因为它们的操作是相似的。</p></figure><p id="9e1e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">正如我们所见,“补丁合并”模块和“Swin转换器模块”是Swin转换器中的两个关键构建模块。在接下来的部分中，我们将详细介绍这两个模块。</p><h1 id="49c1" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">分级特征地图</h1><p id="3210" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">与ViT的第一个显著偏差是Swin Transformer构建了'<strong class="lt iu"> <em class="ms">【层次特征地图'</em> </strong>。让我们把它分成两部分，以便更好地理解这意味着什么。</p><p id="9687" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">首先，‘<strong class="lt iu">特征图’</strong>只是从每个连续层生成的中间张量。至于<strong class="lt iu">‘分层’，</strong>在这个上下文中，是指特征图逐层合并(更多细节在下一节)，有效地减少了特征图从一层到另一层的空间维度(即下采样)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/efebf5d8bcdc0fbc7d2a8ff99e92d3fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zOHXJFzdnMpQTHZPDTynuQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="mv">Swin Transformer中的层次特征图。在每个图层后，要素地图会逐步合并和缩减采样，从而创建具有分层结构的要素地图。注意，为了简单起见，省略了特征图的深度。图片作者。</em></p></figure><p id="3f70" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">您可能会注意到，这些等级要素地图的空间分辨率与ResNet中的相同。这样做是有意的，以便Swin变压器可以方便地取代现有视觉任务方法中的ResNet主干网络。</p><p id="34b8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">更重要的是，这些分层特征图允许Swin转换器应用于需要精细预测的领域，例如语义分割。相比之下，ViT在其整个架构中使用单一、低分辨率的特征地图。</p><h1 id="b369" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">补丁合并</h1><p id="c563" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在上一节中，我们已经了解了如何通过逐步合并和缩减要素地图的空间分辨率来构建等级要素地图。在诸如ResNet的卷积神经网络中，使用卷积运算来完成特征图的下采样。那么，我们如何在不使用卷积的情况下，在纯变压器网络中对特征图进行下采样呢？</p><p id="73aa" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Swin Transformer中使用的无卷积下采样技术被称为<strong class="lt iu">面片合并</strong>。在这种情况下,“面片”是指特征图中的最小单元。换句话说，在14x14特征地图中，有14x14=196个补丁。</p><p id="6efe" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了通过因子<em class="ms"> n </em>对特征图进行缩减采样，面片合并将每组<em class="ms"> n </em> x <em class="ms"> n </em>个相邻面片的特征连接起来。我知道这可能很难理解，所以我制作了一个动画来更好地说明这一点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/3fb026f4b7c907f2d494fd9e1d1e96b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*0MDU8PIJ-wS_fpz-48xGJQ.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">补片合并操作通过将n×n个补片分组并在深度方向上连接补片，以因子<em class="mv"> n </em>对输入进行下采样。图片作者。</p></figure><p id="7f52" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">正如我们从上面的动画中看到的，面片合并将每个<em class="ms"> n </em> x <em class="ms"> n </em>相邻的面片分组，并在深度方向上连接它们。这实际上以系数<em class="ms"> n对输入进行了下采样，</em>将输入从形状为<em class="ms">H</em>x<em class="ms">W</em>x<em class="ms">C</em>转换为(<em class="ms">H/n</em>)x(<em class="ms">W/n</em>)x(<em class="ms">n * C</em>)，其中<em class="ms"> H </em>、<em class="ms"> W </em>和<em class="ms"> C </em>表示</p><h1 id="061f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">Swin变压器组</h1><p id="8a8d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Swin Transformer中使用的变压器模块用一个<strong class="lt iu"> <em class="ms">窗口MSA (W-MSA) </em> </strong>和一个<strong class="lt iu"> <em class="ms">移位窗口MSA (SW-MSA) </em> </strong>模块取代了ViT中使用的标准多头自关注(MSA)模块。Swin变压器模块如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/a7dca38e5d361a0e00fea44b7e52049d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2stjGbbpwLu2pAdo6gR9ag.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Swin变压器模块，有两个子单元。第一个子单元应用W-MSA，第二个子单元应用s W-MSA。图片作者，改编自<a class="ae ky" href="https://arxiv.org/abs/2103.14030" rel="noopener ugc nofollow" target="_blank">刘等，2021 </a>。</p></figure><p id="14df" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Swin变压器模块由两个子单元组成。每个子单元由一个标准化层、一个注意力模块、另一个标准化层和一个MLP层组成。第一个子单元使用<strong class="lt iu">窗口MSA (W-MSA) </strong>模块，而第二个子单元使用<strong class="lt iu">移位窗口MSA (SW-MSA) </strong>模块。</p><h2 id="ff19" class="my la it bd lb mz na dn lf nb nc dp lj ma nd ne ll me nf ng ln mi nh ni lp nj bi translated">基于窗口的自我关注</h2><p id="c9ff" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">ViT中使用的标准MSA执行全局自我关注，并且每个补丁之间的关系是针对所有其他补丁计算的。这导致相对于补片数量的二次复杂度，使其不适用于高分辨率图像。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/10cc0d017c1fb88664bd512c1a27fb5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*XbTV-X6eZ8iXEvhsl04N8Q.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">ViT中使用的标准MSA计算每个补丁相对于所有补丁的注意力。图片作者。</p></figure><p id="cb53" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了解决这个问题，Swin Transformer使用了一种基于窗口的MSA方法。一个窗口只是一个补丁的集合，注意力只在每个窗口内计算。例如，下图使用大小为2 x 2的窗口，基于窗口的MSA仅在每个窗口内计算注意力。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/c66c40c10981f263b3477c96ce013d29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*qJ6egEhj-KtW1MAJ-sxwxQ.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Swin Transformer中使用的窗口MSA仅在每个窗口中计算注意力。图片作者。</p></figure><p id="4911" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">由于窗口大小在整个网络中是固定的，所以基于窗口的MSA的复杂度相对于小块的数量(即图像的大小)是线性的，这是对标准MSA的二次复杂度的巨大改进。</p><h2 id="01cd" class="my la it bd lb mz na dn lf nb nc dp lj ma nd ne ll me nf ng ln mi nh ni lp nj bi translated">转移窗口自我注意</h2><p id="d596" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">然而，基于窗口的MSA的一个明显的缺点是将自己的注意力限制在每个窗口限制了网络的建模能力。为了解决这个问题，Swin Transformer在W-MSA模块之后使用了移位窗口MSA (SW-MSA)模块。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/03d91748a53a3807ee3cde3d221cbc6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*sincgodQpiqGet67un55rg.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">移位窗口MSA。图片作者。</p></figure><p id="9dd3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了引入跨窗口连接，移动窗口MSA将窗口向右下角移动一个因子<em class="ms"> M </em> /2，其中<em class="ms"> M </em>是窗口大小(上面动画中的步骤1)。</p><p id="2486" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，这种转变导致不属于任何窗口的“孤立”补丁，以及具有不完整补丁的窗口。Swin Transformer应用“循环移位”技术(上面动画中的步骤2)，将“孤立”补丁移动到带有不完整补丁的窗口中。注意，在这种移位之后，窗口可能由在原始特征图中不相邻的面片组成，因此在计算期间应用掩码以将自我关注限制到相邻的面片。</p><p id="bc41" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这种移动窗口方法在窗口之间引入了重要的交叉连接，并且被发现改善了网络的性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/1ef9c453c82ebde382bb09383f5a6909.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*irgvfuWpu9lQxMm3NrhbQw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/abs/2103.14030" rel="noopener ugc nofollow" target="_blank">刘等，2021 </a>。</p></figure><h1 id="b8cf" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">结论</strong></h1><p id="bf8f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Swin Transformer可能是继最初的Vision Transformer之后最令人兴奋的研究成果。使用分层特征映射和移位窗口MSA，Swin变压器解决了困扰原始ViT的问题。如今，Swin变压器通常用作各种视觉任务的主干架构，包括图像分类和物体检测。</p><p id="d2bc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我很期待看到基于变压器的架构在计算机视觉领域的未来！</p><h1 id="9faf" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">喜欢这篇文章？</strong></h1><p id="20ba" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">感谢您的阅读！我希望这篇文章对你有用。如果您想订阅中级会员，请考虑使用<a class="ae ky" href="https://medium.com/@jamesloyys/membership" rel="noopener">我的链接</a>。这有助于我继续创建对社区有用的内容！😄</p></div></div>    
</body>
</html>