<html>
<head>
<title>GraphSAGE: Scaling up Graph Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GraphSAGE:放大图形神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-graphsage-in-python-a9e7f9ecf9d7#2022-04-20">https://towardsdatascience.com/introduction-to-graphsage-in-python-a9e7f9ecf9d7#2022-04-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="eeea" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">PyTorch几何的GraphSAGE简介</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a96b307b86e6af8a48e8fed20d4fbef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8TFvXqI2uTq8t-ISMu1e3Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者，表情符号作者<a class="ae ky" href="https://openmoji.org/" rel="noopener ugc nofollow" target="_blank">open moji</a>(<a class="ae ky" href="https://creativecommons.org/licenses/by-sa/4.0/#" rel="noopener ugc nofollow" target="_blank">CC BY-SA 4.0</a>)</p></figure><p id="0253" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> UberEats </strong>和<strong class="lb iu"> Pinterest </strong>有什么共同点？</p><p id="79ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们都使用GraphSAGE <strong class="lb iu"> </strong>来为他们的推荐系统提供大规模的支持:数以百万计的节点和边。</p><ul class=""><li id="0815" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">🖼️ <strong class="lb iu"> Pinterest </strong>开发了自己的名为<a class="ae ky" href="https://medium.com/pinterest-engineering/pinsage-a-new-graph-convolutional-neural-network-for-web-scale-recommender-systems-88795a107f48" rel="noopener"> PinSAGE </a>的版本，向用户推荐最相关的图片(pin)。<br/> →他们的图有180亿个连接，30亿个节点。</li><li id="1e90" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">🍽️ <strong class="lb iu"> UberEats </strong>也报道了使用GraphSAGE 的<a class="ae ky" href="https://eng.uber.com/uber-eats-graph-learning/" rel="noopener ugc nofollow" target="_blank">修改版来推荐菜肴、餐馆和美食<strong class="lb iu">。</strong> <br/> → UberEats号称支持超过60万家餐厅，6600万用户。</a></li></ul><p id="2c72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本教程中，我们将使用具有20k个节点的数据集，而不是数十亿个节点，因为Google Colab无法处理我们的雄心。我们将坚持使用最初的GraphSAGE架构，但是之前的变体也带来了我们将讨论的令人兴奋的特性。</p><p id="2634" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以用下面的<a class="ae ky" href="https://colab.research.google.com/drive/1udeUfWJzvMlLO7sGUDGsHo8cRPMicajl?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Google Colab笔记本</a>运行代码。</p><h1 id="40a7" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">🌐I. PubMed数据集</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/68286da3cafb1fc93fc1c31379b28d10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*Q2s9Pvwl596gVRDjPdJeJA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">考研的t-SNE情节(图片由作者提供)</p></figure><p id="5f91" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将使用<strong class="lb iu"> PubMed </strong>数据集。正如我们在<a class="ae ky" rel="noopener" target="_blank" href="/graph-attention-networks-in-python-975736ac5c0c">上一篇文章</a>中看到的，PubMed是<a class="ae ky" href="https://arxiv.org/abs/1603.08861" rel="noopener ugc nofollow" target="_blank">小行星</a>数据集的一部分(麻省理工学院许可)。这里有一个简短的总结:</p><ul class=""><li id="e79f" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">它包含来自PubMed数据库的<strong class="lb iu"> 19，717篇关于糖尿病的科学出版物</strong>；</li><li id="77e3" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">节点特征是<a class="ae ky" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">TF-IDF</strong></a><strong class="lb iu">500维的加权词向量</strong>，这是一种无需变形金刚的高效文档汇总方式；</li><li id="d8e5" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">该任务是一个多级分类，具有<strong class="lb iu">三个类别</strong>:实验性糖尿病、1型糖尿病和2型糖尿病。</li></ul><p id="173c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是深度学习的美丽和诅咒:我对糖尿病一无所知，但如果我们达到70%的准确率，我仍然会感到非常满意。至少我们不会制造下一个IBM Watson。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="72e3" class="nj mk it nf b gy nk nl l nm nn">Dataset: Pubmed()<br/>------------------- <br/>Number of graphs: 1<br/>Number of nodes: 19717<br/>Number of features: 500<br/>Number of classes: 3</span><span id="085c" class="nj mk it nf b gy no nl l nm nn">Graph:<br/>------<br/>Training nodes: <strong class="nf iu">60</strong><br/>Evaluation nodes: <strong class="nf iu">500</strong><br/>Test nodes: <strong class="nf iu">1000</strong><br/>Edges are directed: False<br/>Graph has isolated nodes: False<br/>Graph has loops: False</span></pre><p id="df80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所见，与整个图表相比，PubMed的训练节点数量低得惊人<strong class="lb iu">。只有60个样本来学习如何对1000个测试节点进行分类。</strong></p><p id="61fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管面临这一挑战，GNNs仍设法获得高水平的准确性。以下是已知技术排行榜(更详尽的基准可以在<a class="ae ky" href="https://paperswithcode.com/sota/node-classification-on-pubmed" rel="noopener ugc nofollow" target="_blank">的论文中找到，代码为</a>):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/ae5dd0d206606cf5de7687ddd8f6b006.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WiHpZpQ0CpFx2tFYD-SAcw.png"/></div></div></figure><p id="2499" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我在PubMed上找不到GraphSAGE在这种特定设置下的任何结果(60个训练节点，1000个测试节点)，所以我不期望有很高的准确性。但是在处理大型图表时，另一个度量标准也同样重要:<strong class="lb iu">训练时间</strong>。</p><h1 id="18e9" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">🧙‍♂️二世。理论上的文法</h1><div class="kj kk kl km gt ab cb"><figure class="nq kn nr ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/c55428ffcef6dd50c817453135373918.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*0-cn-tQHOxyoFp9VIC8T5g.png"/></div></figure><figure class="nq kn nw ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/3c18656d71f037b929e7cc3d99660d2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*Z-dhW00goSMbqKjLrXbBUQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk nx di ny nz translated">作者图片</p></figure></div><p id="832b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GraphSAGE算法可以分为两个步骤:</p><ol class=""><li id="4c49" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu oa mb mc md bi translated"><strong class="lb iu">邻居抽样；</strong></li><li id="e0df" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oa mb mc md bi translated"><strong class="lb iu">聚合</strong>。</li></ol><h1 id="3070" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">🎰a .邻居抽样</h1><p id="ee3a" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">小批量是机器学习中常用的技术。</p><p id="11b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它通过<strong class="lb iu">将数据集</strong> <strong class="lb iu">分解成更小的批次</strong>来工作，这允许我们更有效地训练模型。小批量有几个好处<strong class="lb iu"> : </strong></p><ol class=""><li id="eab4" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu oa mb mc md bi translated"><strong class="lb iu">提高精度</strong> —小批量有助于减少过度拟合(梯度被平均)，以及误差率的变化；</li><li id="d06a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oa mb mc md bi translated"><strong class="lb iu">提高速度</strong> —小批量并行处理，比大批量花费更少的训练时间；</li><li id="61c3" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oa mb mc md bi translated"><strong class="lb iu">改进的可扩展性</strong> —整个数据集可以超过GPU内存，但较小的批处理可以绕过这一限制。</li></ol><p id="f8b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">小型批处理非常有用，它已经成为常规神经网络的标准。然而，对于图形数据来说就不那么简单了，因为将数据集分割成更小的块会破坏节点之间的基本连接。</p><p id="535a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，我们能做什么呢？近年来，研究人员开发了不同的策略来创建图形小批量。我们感兴趣的那个叫做<a class="ae ky" href="https://arxiv.org/abs/1706.02216" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">邻居采样</strong> </a>。你可以在<a class="ae ky" href="https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html" rel="noopener ugc nofollow" target="_blank"> PyG的文档</a>上找到很多其他技术，比如<a class="ae ky" href="https://arxiv.org/pdf/1905.07953.pdf" rel="noopener ugc nofollow" target="_blank">子图聚类</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/277eadcb0244c31996695f74275bab18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*5Oo3-SVO110a9pEs-Nkrjw.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">邻居采样(图片由作者提供)</p></figure><p id="7aa8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">邻居采样只考虑随机邻居中固定数量的。流程是这样的:</p><ol class=""><li id="8e47" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu oa mb mc md bi translated">我们定义<strong class="lb iu">邻居数量</strong> (1跳)，邻居的邻居数量(2跳)等等。我们想要。</li><li id="425e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oa mb mc md bi translated">采样器查看邻居列表、邻居的邻居列表等。并且<strong class="lb iu">随机选择</strong>预定数量的目标节点；</li><li id="12bb" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oa mb mc md bi translated">采样器<strong class="lb iu">输出包含目标节点和随机选择的邻居节点的子图</strong>。</li></ol><p id="b1b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对列表中的每个节点或整个图重复该过程<strong class="lb iu">。然而，为每个节点创建一个子图是没有效率的，这就是为什么我们可以成批处理它们。在这种情况下，每个子图由多个目标节点共享。</strong></p><p id="8c07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相邻采样还有一个额外的好处。有时，我们会观察到非常受欢迎的节点，这些节点就像枢纽一样，比如社交媒体上的名人。获得这些节点的隐藏向量在计算上是非常昂贵的，因为它需要计算数千甚至数百万个邻居的隐藏向量。GraphSAGE通过简单地忽略大多数节点解决了这个问题！</p><p id="0677" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在PyG中，邻居采样是通过<code class="fe oh oi oj nf b"><a class="ae ky" href="https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.NeighborLoader" rel="noopener ugc nofollow" target="_blank">NeighborLoader</a></code>对象实现的。假设我们想要<strong class="lb iu"> 5个邻居和他们的10个邻居</strong> ( <code class="fe oh oi oj nf b">num_neighbors</code>)。正如我们所讨论的，我们还可以通过为多个目标节点创建子图来指定一个<code class="fe oh oi oj nf b">batch_size</code>来加速这个过程。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="93d3" class="nj mk it nf b gy nk nl l nm nn"><strong class="nf iu">Subgraph 0</strong>: Data(x=[389, 500], edge_index=[2, 448], batch_size=16)<br/><strong class="nf iu">Subgraph 1</strong>: Data(x=[264, 500], edge_index=[2, 314], batch_size=16)<br/><strong class="nf iu">Subgraph 2</strong>: Data(x=[283, 500], edge_index=[2, 330], batch_size=16)<br/><strong class="nf iu">Subgraph 3</strong>: Data(x=[189, 500], edge_index=[2, 229], batch_size=12)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/8137793e3d23a8083123d451eea47136.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*edaTuK1oAVAzYo9bcFN11g.png"/></div></div></figure><p id="d292" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们创建了4个不同大小的子图。它允许我们并行处理它们，并且它们更容易安装在GPU上，因为它们更小。</p><p id="8090" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">邻居的数量是一个重要的参数，因为修剪我们的图形会删除很多信息。具体多少钱？嗯，相当多。我们可以通过查看<strong class="lb iu">节点度数</strong>(邻居数量)来形象化这种影响。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/2015770e24ea62a881853d068db95206.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OUCmqikcOBad9V1u7Z6l3A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">原始图中的节点度</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/e9b0cb56fe0657ab923d5e690d08b7de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z0nbVfFD6kXGTzM-aIrpUw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">邻居采样后的节点度数</p></figure><p id="df89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个例子中，我们的子图的<strong class="lb iu">最大节点度</strong>是5，这比最初的最大值低得多。在谈论GraphSAGE时，记住这种权衡是很重要的。</p><p id="cc5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PinSAGE <strong class="lb iu"> </strong>使用<strong class="lb iu">随机游走</strong>实现另一种采样解决方案。它有两个主要目标:</p><ol class=""><li id="c6ab" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu oa mb mc md bi translated">样本a <strong class="lb iu">固定数量的邻居</strong>(如graph sage)；</li><li id="0308" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oa mb mc md bi translated">获取它们的<strong class="lb iu">相对重要性</strong>(重要节点比其他节点更频繁出现)。</li></ol><p id="31cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个策略感觉有点像快速<strong class="lb iu">注意力机制</strong>。它为节点分配权重，并增加最受欢迎的节点的相关性。</p><h1 id="5810" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak">💥b .汇总</strong></h1><p id="9c87" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">聚集过程确定如何组合特征向量以产生节点嵌入。原始论文提出了三种聚合特征的方法:</p><ul class=""><li id="144f" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">表示</strong>聚合器；</li><li id="baaf" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu"> LSTM </strong>聚合器；</li><li id="70af" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">汇集</strong>聚合器。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/370bf88b4194f98fe7cd59fcce7bbde7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*dWTIARwYsjnZhpnVe5vYNQ.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">聚合(作者图片)</p></figure><p id="02fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">均值聚合器</strong>是最简单的一个。这个想法接近于GCN的方法:</p><ol class=""><li id="6bcc" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu oa mb mc md bi translated">目标节点及其所选邻居的隐藏特征被平均(ñᵢ)；</li><li id="b319" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oa mb mc md bi translated">应用具有加权矩阵𝐖的线性变换。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/c41b189c4972fd7830d43529eea2524a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8TAmEAlcMCB4E_vWvy7GTg.png"/></div></div></figure><p id="1173" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，可以将结果反馈给非线性激活函数，如<em class="oo"> ReLU </em>。</p><p id="b217" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> LSTM聚合器</strong>可能看起来是一个奇怪的想法，因为这个架构是顺序的:它给无序的节点分配一个顺序。这就是为什么作者随机打乱它们来迫使LSTM只考虑隐藏的特征。在他们的基准测试中，这是性能最好的技术。</p><p id="ab11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">汇集聚集器</strong>将每个邻居的隐藏向量馈送给前馈神经网络。对结果应用最大池操作。</p><h1 id="fa68" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">🧠三世。PyTorch几何中的GraphSAGE</h1><p id="b7d8" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">我们可以用PyTorch Geometric的<code class="fe oh oi oj nf b">SAGEConv</code>层轻松实现GraphSAGE架构。这种实现使用两个权重矩阵，而不是一个，就像UberEats的GraphSAGE版本:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/338174c62e887940957e48a4ce2d142c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pbxbATEdJW_LYAleH_GK8g.png"/></div></div></figure><p id="145d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们创建一个具有两层<code class="fe oh oi oj nf b">SAGEConv</code>的网络:</p><ul class=""><li id="a3f5" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">第一个将使用<strong class="lb iu"> <em class="oo"> ReLU </em> </strong>作为激活函数和一个<strong class="lb iu">脱落层</strong>；</li><li id="ba26" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">第二个将直接输出<strong class="lb iu">节点嵌入</strong>。</li></ul><p id="88ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于我们正在处理多类分类任务，我们将使用交叉熵损失作为我们的损失函数。我还添加了一个0.0005的L2正则化值，以便更好地测量。</p><p id="ccd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要了解GraphSAGE的优势，让我们<strong class="lb iu">将</strong>与<strong class="lb iu"/>GCN和GAT进行比较，无需任何采样。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="4fbd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用GraphSAGE，我们遍历由邻居采样过程创建的<strong class="lb iu">批</strong>(我们的4个子图)。因此，我们计算准确度和验证损失的方法也不同。</p><p id="b784" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是<strong class="lb iu"/>GCN、GAT和GraphSAGE的结果(根据<strong class="lb iu">准确度</strong>和<strong class="lb iu">训练时间</strong>):</p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="d98a" class="nj mk it nf b gy nk nl l nm nn"><strong class="nf iu">GCN </strong>test accuracy:<strong class="nf iu">        78.40% (52.6 s)<br/>GAT </strong>test accuracy:<strong class="nf iu">        77.10% (18min 7s)<br/>GraphSAGE </strong>test accuracy:<strong class="nf iu">  77.20% (12.4 s)</strong></span></pre><p id="03e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">三个模型在精确度方面获得<strong class="lb iu">相似的</strong>结果。我们希望GAT性能更好，因为它的聚合机制更加细致，但情况并非总是如此。</p><p id="e2fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">真正的区别是训练时间:在这个例子中，GraphSAGE比GAT快88倍，比GCN快4倍。</p><p id="292a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是GraphSAGE的真正力量。通过用邻居采样修剪我们的图，我们确实丢失了许多信息。最终的节点嵌入可能<strong class="lb iu">不如我们用GCN或GAT找到的</strong>好。但这不是重点:GraphSAGE旨在提高可伸缩性。反过来，它可以构建更大的图表，从而提高准确性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/9cd59c3a27b13f8bc6366748d778021d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8VzWimEXwGrcPfNG"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="4286" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这项工作是在有监督的训练环境中完成的(节点分类)，但是我们也可以用<strong class="lb iu">无监督的方式</strong>训练GraphSAGE。</p><p id="8624" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，我们不能使用交叉熵损失。我们必须设计一个损失函数，迫使原始图中相邻的节点在嵌入空间中保持彼此靠近。相反，相同的函数必须确保图中的<strong class="lb iu">距离节点</strong>在嵌入空间中必须具有<strong class="lb iu">距离表示</strong>。这是GraphSAGE的论文中提出的损失。</p><p id="0e52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在PinSAGE和UberEeats的改进GraphSAGE的例子中，我们正在处理<strong class="lb iu">推荐系统</strong>。</p><p id="ce86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目标是为每个用户正确地排列最相关的项目(大头针，餐馆)，这是非常不同的。我们不仅想知道最接近的嵌入是什么，我们还必须产生可能的最佳排名。这就是为什么这些系统也以无监督的方式训练，但具有另一个损失函数:最大利润排名损失。</p><h1 id="d16d" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak">结论</strong></h1><p id="b4bb" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">GraphSAGE是一个处理大型图形的非常快速的架构。它可能没有GCN或GAT精确，但它是处理海量数据的基本模型。在本例中，它提供了这样的速度，这要归功于1/邻居采样修剪图形和2/快速聚合与平均聚合器的巧妙结合。在这篇文章中，</p><ul class=""><li id="ef71" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">我们用PubMed探索了一个<strong class="lb iu">新数据集</strong>，比之前大了好几倍；</li><li id="24d3" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">我们解释了<strong class="lb iu">邻居采样</strong>背后的思想，它在每一跳只考虑预定数量的随机邻居；</li><li id="f98f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">我们在GraphSAGE的论文中看到了三个聚合器,并专注于均值聚合器；</li><li id="ba69" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">我们根据<strong class="lb iu">精度</strong>和<strong class="lb iu">训练时间</strong>对<strong class="lb iu">和</strong>三款车型(GraphSAGE、GAT和GCN)进行了基准测试。</li></ul><p id="5dc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到了三种具有相同终端应用的架构:节点分类。但是GNNs已经成功应用于其他任务。在接下来的教程中，我想在两个不同的环境中使用它们:<strong class="lb iu">图形和边缘预测</strong>。这将是一个发现新的数据集和应用的好方法，在这些数据集和应用中，gnn占据了最先进的水平。</p><p id="b614" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你喜欢这篇文章，让我们在Twitter上联系<a class="ae ky" href="https://twitter.com/maximelabonne" rel="noopener ugc nofollow" target="_blank">@ maxime labanne</a>以获得更多的图形学习内容。</p><p id="370d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的关注！📣</p><h1 id="00c6" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">相关文章</h1><div class="oq or gp gr os ot"><a rel="noopener follow" target="_blank" href="/how-to-design-the-most-powerful-graph-neural-network-3d18b07a6e66"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd iu gy z fp oy fr fs oz fu fw is bi translated">如何设计最强大的图形神经网络</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">基于图同构网络的图分类</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">towardsdatascience.com</p></div></div><div class="pc l"><div class="pd l pe pf pg pc ph ks ot"/></div></div></a></div><div class="oq or gp gr os ot"><a rel="noopener follow" target="_blank" href="/graph-attention-networks-in-python-975736ac5c0c"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd iu gy z fp oy fr fs oz fu fw is bi translated">图形注意网络:自我注意的解释</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">使用PyTorch几何图形的带自我注意的GNNs指南</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">towardsdatascience.com</p></div></div><div class="pc l"><div class="pi l pe pf pg pc ph ks ot"/></div></div></a></div></div></div>    
</body>
</html>