<html>
<head>
<title>K-means Clustering from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-均值聚类从零开始</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/unsupervised-learning-and-k-means-clustering-from-scratch-f4e5e9947c39#2022-12-19">https://towardsdatascience.com/unsupervised-learning-and-k-means-clustering-from-scratch-f4e5e9947c39#2022-12-19</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="93cb" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">K-means:聚类数据的最佳 ML 算法</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/71857e612a771a9ed4c790180ee10b48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j7Z3i2ac8pwRUrWX6mzvlA.png"/></div></div></figure><h2 id="56e7" class="kv kw iu bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">动机</h2><p id="3a9f" class="pw-post-body-paragraph lr ls iu lt b lu lv jv lw lx ly jy lz le ma mb mc li md me mf lm mg mh mi mj in bi translated">机器学习的主要思想是创建一个通用模型，该模型可以根据以前的数据提供理性的决策，而无需显式编程。机器学习问题可以是有监督的，也可以是无监督的。本文重点介绍一种称为<strong class="lt iv">‘K-means’</strong>聚类的无监督机器学习算法。</p><p id="2e45" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">当无监督机器学习这个术语出现时，我通常会在进行机器学习课程时向我的学生提供示例。假设给了你一些<code class="fe mp mq mr ms b"><em class="mt">duck, snake, cow, dove, goat, hen, ship, crocodile, etc</em></code> <em class="mt">的玩具。不幸的是，你不知道这些玩具的名字。如果让你把动物分成不同的种类，你会怎么做？如果你理性地思考，基于其外观的可能集群将是<strong class="lt iv">集群 1:</strong><code class="fe mp mq mr ms b">duck, hen, dove</code>；<strong class="lt iv">集群二:</strong><code class="fe mp mq mr ms b">goat, cow, ship</code>；和<strong class="lt iv">簇 3 </strong> : <code class="fe mp mq mr ms b"> crocodiles, snake</code>。虽然确切的名字不为人知，但你可以将这些动物归类。因此，基于相似特征的聚类被称为无监督机器学习算法。</em></p><p id="8f48" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">对于基于相似性的数据分组，无监督机器学习是最适合的。</p></div><div class="ab cl mu mv hy mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="in io ip iq ir"><h2 id="7c84" class="kv kw iu bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated"><strong class="ak">目录</strong></h2><ol class=""><li id="eadd" class="nb nc iu lt b lu lv lx ly le nd li ne lm nf mj ng nh ni nj bi translated"><code class="fe mp mq mr ms b"><a class="ae nk" href="#b7ae" rel="noopener ugc nofollow"><strong class="lt iv">Overview of Unsupervised Learning</strong></a></code></li><li id="f45b" class="nb nc iu lt b lu nl lx nm le nn li no lm np mj ng nh ni nj bi translated"><code class="fe mp mq mr ms b"><a class="ae nk" href="#85d1" rel="noopener ugc nofollow"><strong class="lt iv">Coordinate Distance Calculation for K-means</strong></a></code></li><li id="54f0" class="nb nc iu lt b lu nl lx nm le nn li no lm np mj ng nh ni nj bi translated"><code class="fe mp mq mr ms b"><a class="ae nk" href="#b8ad" rel="noopener ugc nofollow"><strong class="lt iv">Overview of K-means</strong></a></code></li><li id="4cb6" class="nb nc iu lt b lu nl lx nm le nn li no lm np mj ng nh ni nj bi translated"><code class="fe mp mq mr ms b"><a class="ae nk" href="#f558" rel="noopener ugc nofollow"><strong class="lt iv">Optimum Number of Cluster Selection</strong></a></code></li><li id="8282" class="nb nc iu lt b lu nl lx nm le nn li no lm np mj ng nh ni nj bi translated"><code class="fe mp mq mr ms b"><a class="ae nk" href="#cbb9" rel="noopener ugc nofollow"><strong class="lt iv">Why K-means?</strong></a></code></li><li id="4dc9" class="nb nc iu lt b lu nl lx nm le nn li no lm np mj ng nh ni nj bi translated"><code class="fe mp mq mr ms b"><a class="ae nk" href="#c110" rel="noopener ugc nofollow"><strong class="lt iv">Challenges of K-means</strong></a></code></li><li id="54e9" class="nb nc iu lt b lu nl lx nm le nn li no lm np mj ng nh ni nj bi translated"><code class="fe mp mq mr ms b"><a class="ae nk" href="#f9b7" rel="noopener ugc nofollow"><strong class="lt iv">Step-by-step Hands-on Implementation</strong></a></code></li></ol></div><div class="ab cl mu mv hy mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="in io ip iq ir"><h2 id="b7ae" class="kv kw iu bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">无监督学习概述</h2><p id="f1e0" class="pw-post-body-paragraph lr ls iu lt b lu lv jv lw lx ly jy lz le ma mb mc li md me mf lm mg mh mi mj in bi translated">无监督学习，也称为无监督机器学习，使用机器学习算法来分析和聚类未标记的数据集。这些算法发现隐藏的模式或数据分组，无需人工干预[1]。</p><p id="3bef" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">假设你是一名理学硕士，你有一个论文导师。你的导师会指导你完成论文，因为他知道如何进行研究和最终目标。有监督的机器学习算法以同样的方式工作。每个输入都有一个目标值，算法试图从标记的数据中优化其参数，以预测新的实例。无监督学习方法正好是监督学习的逆过程。这些方法处理未标记的数据。无监督学习的主要目的是发现潜在的隐藏模式和见解[2]。这些算法通常用于解决聚类问题。</p><p id="4e02" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">无监督机器学习算法有两种类型。它们如下—</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nq"><img src="../Images/9ac92e81060f4348ef00f3404db03e1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*KjWNLg9B1iD8P4LLdPZHxg.png"/></div><p class="nr ns gk gi gj nt nu bd b be z dk translated">作者图片</p></figure><p id="3c67" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated"><code class="fe mp mq mr ms b"><em class="mt">The article only focuses on the clustering algorithm (K-means). </em></code> <strong class="lt iv"> <em class="mt">聚类是将具有相似特征的数据点进行分组。</em> </strong>有时候无监督学习算法的作用变得非常重要。</p><p id="3513" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">已经给出了一些优点[2] —</p><ul class=""><li id="e434" class="nb nc iu lt b lu mk lx ml le nv li nw lm nx mj ny nh ni nj bi translated">无监督学习有助于从数据中找到有价值的见解。</li><li id="28a2" class="nb nc iu lt b lu nl lx nm le nn li no lm np mj ny nh ni nj bi translated">无监督学习与人类非常相似。我们通过自己的经历来学习思考，这让它更接近真实的 AI。</li><li id="66b2" class="nb nc iu lt b lu nl lx nm le nn li no lm np mj ny nh ni nj bi translated">无监督学习对未标记和未分类的数据起作用，这使得无监督学习变得更加重要。</li><li id="3ed4" class="nb nc iu lt b lu nl lx nm le nn li no lm np mj ny nh ni nj bi translated">在现实世界中，我们并不总是有相应输出的输入数据，因此要解决这种情况，我们需要无监督学习。</li></ul><h2 id="85d1" class="kv kw iu bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">K-means 的坐标距离计算</h2><ul class=""><li id="d028" class="nb nc iu lt b lu lv lx ly le nd li ne lm nf mj ny nh ni nj bi translated"><strong class="lt iv">欧几里德距离</strong></li></ul><p id="d55b" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">欧几里德距离是计算两个坐标点之间距离的最著名的方法。它计算对象对的坐标之间的平方差的平方根[4]。它是两个数据点之间的直线距离。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nz"><img src="../Images/1d304309a2cbfe3affd285dcfd85bdce.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*bMkTloFMpJwDH982L5yzjQ.png"/></div></div><p class="nr ns gk gi gj nt nu bd b be z dk translated">欧几里德距离(图片由作者提供)</p></figure><p id="53bf" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">欧几里得距离可以用下面的等式来测量。公式用<code class="fe mp mq mr ms b"> <strong class="lt iv"><em class="mt">x</em> </strong>and <strong class="lt iv"><em class="mt">y</em></strong></code>表示两点。<code class="fe mp mq mr ms b"><strong class="lt iv"><em class="mt">K</em></strong> </code>是维度的数量<em class="mt">(在数据科学中，每个数据集的特征被认为是一个维度)。</em></p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oa"><img src="../Images/1b454bdd3e898f8b4a89d9bf7b3d9267.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*SSgrH1ZhjsFJg6lpADQ5yw.png"/></div></figure><ul class=""><li id="7515" class="nb nc iu lt b lu mk lx ml le nv li nw lm nx mj ny nh ni nj bi translated"><strong class="lt iv">曼哈顿距离</strong></li></ul><p id="9eec" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">曼哈顿距离计算对象对的坐标之间的绝对差异[4]。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nz"><img src="../Images/d6e801c717375832d2ac336f049fc414.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*ZTmrXJ_IgHJmjsR05O08MQ.png"/></div></div><p class="nr ns gk gi gj nt nu bd b be z dk translated">曼哈顿距离(图片作者提供)</p></figure><p id="5dca" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">曼哈顿距离是坐标绝对距离的总和。可以描述如下。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj ob"><img src="../Images/a239cfd82a3b120df299524bf8bb086a.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*lynROsGU0HzjAL8OjcD_jA.png"/></div></figure><p id="0962" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">这里，<code class="fe mp mq mr ms b">x <strong class="lt iv">and </strong>y</code>是两个坐标点，<code class="fe mp mq mr ms b">‘‘k’’</code>是尺寸/特征的数量。</p><ul class=""><li id="2632" class="nb nc iu lt b lu mk lx ml le nv li nw lm nx mj ny nh ni nj bi translated"><strong class="lt iv">切比雪夫距离</strong></li></ul><p id="863c" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">切比雪夫距离也称为最大值距离，计算为一对对象的坐标之间差异的绝对大小[4]。这是最大坐标值。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nz"><img src="../Images/c05ca8f240d3c83af4256116c4dcd4fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*ATPsJn7XQaoCTU8HtLDO7g.png"/></div></div><p class="nr ns gk gi gj nt nu bd b be z dk translated">切比雪夫距离(图片作者)</p></figure><p id="96b5" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated"><code class="fe mp mq mr ms b"><strong class="lt iv">x and y</strong></code> <strong class="lt iv"> </strong>代表两个坐标点。他们的切比雪夫距离可以通过找到坐标中的最大距离来计算。<code class="fe mp mq mr ms b"><strong class="lt iv">k</strong></code>表示特征的数量。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oc"><img src="../Images/4d288a34543bef16e2e846eab7a1cf15.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*kru1ThPtpgFRYiGWimVJiw.png"/></div></figure><p id="336c" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">假设我们有两点，<code class="fe mp mq mr ms b"><strong class="lt iv">x(1, 3)</strong> and<strong class="lt iv"> y(5,10)</strong></code>，<strong class="lt iv">。</strong>T3。所以，<code class="fe mp mq mr ms b">max (4, 7) is 7</code>。这意味着<strong class="lt iv">切比雪夫的距离</strong>是<strong class="lt iv"> 7。</strong></p><ul class=""><li id="c92f" class="nb nc iu lt b lu mk lx ml le nv li nw lm nx mj ny nh ni nj bi translated"><strong class="lt iv">闵可夫斯基距离</strong></li></ul><p id="5fe1" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">闵可夫斯基距离是一个统一的距离公式。有了这个距离公式，我们只要改变一个参数就可以得到以上所有的距离。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj od"><img src="../Images/ada3dcf7cf87381c39ffcb4724825587.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*ang9JEGjZAAes6F6Z2tEpQ.png"/></div><p class="nr ns gk gi gj nt nu bd b be z dk translated">闵可夫斯基距离(图片作者提供)</p></figure><p id="3599" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">该距离可以用下面的公式计算。两点之间的距离，<code class="fe mp mq mr ms b"><strong class="lt iv">x and y</strong></code>、<strong class="lt iv">、</strong>、<code class="fe mp mq mr ms b"><strong class="lt iv"> k</strong> </code>为特征的个数。<code class="fe mp mq mr ms b"><strong class="lt iv">P</strong></code> <strong class="lt iv"> </strong>是一个唯一的参数，它转换方程来计算不同的距离。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj oe"><img src="../Images/76272782627a9db4dbffd3ac8c558ad2.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*evAk8Dibxg6AOWn9SHj_eQ.png"/></div></div></figure><p id="b827" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">注意，当<code class="fe mp mq mr ms b"> <strong class="lt iv">p=2</strong>,</code>时，距离变成欧几里德距离。当<code class="fe mp mq mr ms b"><strong class="lt iv">p=1</strong></code>时，变为城市街区距离。切比雪夫距离是闵可夫斯基距离的变体，其中<code class="fe mp mq mr ms b"> <strong class="lt iv">p=∞</strong> </code>(取一个极限)【4】。</p><p id="89f9" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">关于描述距离，研究论文[4]和文章[5]对我帮助很大。】</p><p id="4b32" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated"><code class="fe mp mq mr ms b">The research finding shows that Euclidean distance is the best method for calculating the distances among the data points for the K-means clustering algorithm.</code></p><h2 id="b8ad" class="kv kw iu bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated"><strong class="ak">K-means 聚类算法概述</strong></h2><p id="1f5c" class="pw-post-body-paragraph lr ls iu lt b lu lv jv lw lx ly jy lz le ma mb mc li md me mf lm mg mh mi mj in bi translated">K-means 聚类是一种流行的无监督聚类机器学习算法。让我们解释一下它是如何工作的。</p><p id="76ed" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated"><strong class="lt iv"> <em class="mt">第一步:</em> </strong>最开始，我们需要选择<strong class="lt iv"> K </strong>的值。<strong class="lt iv"> K </strong>表示您想要的集群数量。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj of"><img src="../Images/6272652187b57b5985b1af7d37e023eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*17jAJCgXZkGHmf_rUO4H0Q.png"/></div><p class="nr ns gk gi gj nt nu bd b be z dk translated">样本数据点(图片由作者提供)</p></figure><p id="a71c" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated"><strong class="lt iv"> <em class="mt">第二步:</em> </strong>为每个簇随机选择质心。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj og"><img src="../Images/bb4432bd14a69cbd15e7680285f9c881.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*v3IqLINqFMnqzz-KRmhXDw.png"/></div><p class="nr ns gk gi gj nt nu bd b be z dk translated">随机选择的质心(图片由作者提供)</p></figure><p id="35e2" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">假设对于以上数据点；我们想要创建 3 个集群。因此，<strong class="lt iv"> <em class="mt"> K=3 </em> </strong>并且正方形颜色的数据点是 3 个随机选择的质心。</p><p id="cf32" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated"><strong class="lt iv"> <em class="mt">步骤 3: </em> </strong>计算数据点到质心的距离，并根据最小距离将数据点分配到聚类中。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj oh"><img src="../Images/6c53c23ad90405737ff0160af0318c48.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*6CaWh7SVM2KS1zACn8kpwg.png"/></div></div><p class="nr ns gk gi gj nt nu bd b be z dk translated">每个质心下分配的聚类(图片由作者提供)</p></figure><p id="0ffd" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">从上面的图像中，我们清楚地看到，每个质心都被分配了一些数据点，这些数据点基于用不同颜色表示的最小距离。</p><p id="a291" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated"><strong class="lt iv"> <em class="mt">第四步:</em> </strong>计算每个聚类的平均值，将新的质心重新集中到平均值。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj oi"><img src="../Images/57c423bc3b419799d1bdc2065364b8ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*jQPrNgH0uvjVN148rtiUCA.png"/></div></div><p class="nr ns gk gi gj nt nu bd b be z dk translated">重新排列聚类(图片由作者提供)</p></figure><p id="485d" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">该图像描绘了根据质心的平均值将质心居中到新位置。</p><p id="c1e4" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated"><strong class="lt iv"> <em class="mt">第五步:</em> </strong>重复第三步和第四步，直到质心收敛。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oj"><img src="../Images/208a27944fc120fc8c9aecd6d43308bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*qUhYlqHFPzU24CfVc2p9cg.png"/></div><p class="nr ns gk gi gj nt nu bd b be z dk translated">重新调整集群(图片由作者提供)</p></figure><p id="8d53" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">重复步骤 3 和步骤 4 之后，我们得到了上面的集群。对于下一次迭代，我们得到以下集群。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj ok"><img src="../Images/6891c1b0ac40cb3940611775e3823bf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*aQ01ZwK8QKXCEEo7DfEyPA.png"/></div><p class="nr ns gk gi gj nt nu bd b be z dk translated">迭代后的新聚类(图片由作者提供)</p></figure><p id="7f04" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">下一次迭代做什么？让我们看看。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj ol"><img src="../Images/55b94e7c8949be6d90a5be3232d06646.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*NycpguBMeQkHljWKHmnliw.png"/></div><p class="nr ns gk gi gj nt nu bd b be z dk translated">汇聚成最终的集群(图片由作者提供)</p></figure><p id="d468" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">最后两个簇和质心是相同的。我们可以说，质心收敛，我们达到了我们的最终目标。</p><h2 id="f558" class="kv kw iu bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">K-均值的最佳聚类数</h2><p id="69a1" class="pw-post-body-paragraph lr ls iu lt b lu lv jv lw lx ly jy lz le ma mb mc li md me mf lm mg mh mi mj in bi translated">K-means 聚类算法的一个大问题是如何选择最佳的聚类数目。如果不知道最佳集群数，就要应用<a class="ae nk" href="https://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iv">肘法</strong> </a>来找出。<em class="mt">为了保持文章的精确和适度，我简单说明一下方法。</em></p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj om"><img src="../Images/4369992a33d00c1769bd0b457dac9325.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lg47gzpJ1x5BxfJWyxK5cQ.png"/></div></div><p class="nr ns gk gi gj nt nu bd b be z dk translated">肘法图(图片由作者提供)[6]</p></figure><p id="e2c2" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">应用肘方法后，我们会发现一个如上图所示的线图。从图中，我们需要找出肘点和相应的集群数。并且它将被认为是最佳的聚类数。对于上图，最佳聚类数是 4。肘法的详细解释可用<a class="ae nk" href="https://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iv"> <em class="mt">这里的</em> </strong> </a> <strong class="lt iv"> <em class="mt">。</em> </strong></p><h2 id="cbb9" class="kv kw iu bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">为什么是 K-means？</h2><p id="5443" class="pw-post-body-paragraph lr ls iu lt b lu lv jv lw lx ly jy lz le ma mb mc li md me mf lm mg mh mi mj in bi translated">K-means 是最流行的聚类算法。这是一种简单的聚类算法，适用于大型数据集。相比之下，它比其他聚类算法运行得更快。它总是保证收敛到最终的聚类，并容易适应新的数据点[3]。</p><h2 id="c110" class="kv kw iu bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">K-means 的挑战</h2><p id="d958" class="pw-post-body-paragraph lr ls iu lt b lu lv jv lw lx ly jy lz le ma mb mc li md me mf lm mg mh mi mj in bi translated">在上一节中，我们看到初始聚类质心在 K-means 聚类算法中是随机分配的，这导致了随机的迭代和执行时间。因此，初始质心点的选择是算法中的一个关键问题。你可以阅读下面的文章，它代表了一种系统地选择初始质心的技术。</p><div class="on oo gq gs op oq"><a rel="noopener follow" target="_blank" href="/efficient-k-means-clustering-algorithm-with-optimum-iteration-and-execution-time-9358a794406c"><div class="or ab fp"><div class="os ab ot cl cj ou"><h2 class="bd iv gz z fq ov fs ft ow fv fx it bi translated">具有最优迭代和执行时间的高效 K-均值聚类算法</h2><div class="ox l"><h3 class="bd b gz z fq ov fs ft ow fv fx dk translated">高效 K-means 聚类算法的实现:研究成果</h3></div><div class="oy l"><p class="bd b dl z fq ov fs ft ow fv fx dk translated">towardsdatascience.com</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe kt oq"/></div></div></a></div><p id="bbfe" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">该算法对复杂的分布式数据不太适用。</p><h2 id="f9b7" class="kv kw iu bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">逐步动手实施</h2><p id="b055" class="pw-post-body-paragraph lr ls iu lt b lu lv jv lw lx ly jy lz le ma mb mc li md me mf lm mg mh mi mj in bi translated">本节将从头开始展示 K-means 聚类算法的实际实现。对于任何机器学习模型，我们首先需要加载数据集。出于演示的目的，我使用了<code class="fe mp mq mr ms b"><a class="ae nk" href="https://www.kaggle.com/datasets/shwetabh123/mall-customers?resource=download" rel="noopener ugc nofollow" target="_blank"><strong class="lt iv">mall_customer</strong></a> </code>数据集。这是一个受欢迎的数据集。</p><p id="c1ad" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated"><code class="fe mp mq mr ms b">[N.B. — <em class="mt">have used the </em><a class="ae nk" href="https://www.kaggle.com/datasets/shwetabh123/mall-customers?resource=download" rel="noopener ugc nofollow" target="_blank"><em class="mt">mall_customer</em></a><em class="mt"> dataset, which is a public domain dataset under the “</em><a class="ae nk" href="https://creativecommons.org/publicdomain/zero/1.0/" rel="noopener ugc nofollow" target="_blank"><em class="mt">CC0: Public Domain</em></a><em class="mt">” license.</em>]</code></p><ul class=""><li id="078d" class="nb nc iu lt b lu mk lx ml le nv li nw lm nx mj ny nh ni nj bi translated"><strong class="lt iv"> <em class="mt">导入必要的库</em> </strong></li></ul><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pf pg l"/></div></figure><ul class=""><li id="e1e9" class="nb nc iu lt b lu mk lx ml le nv li nw lm nx mj ny nh ni nj bi translated"><strong class="lt iv"> <em class="mt">加载数据集和一些预处理</em> </strong></li></ul><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="ph pg l"/></div></figure><p id="4f33" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated"><em class="mt">数据集的信息</em></p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pi pg l"/></div></figure><p id="93aa" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated"><code class="fe mp mq mr ms b">Customer ID</code>和<code class="fe mp mq mr ms b">Gender </code>不是那么重要，所以我已经丢弃了这些列。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pj pg l"/></div></figure><p id="12f9" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">将<code class="fe mp mq mr ms b">dataframe</code>转换成一个<code class="fe mp mq mr ms b">NumPy</code>数组。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pj pg l"/></div></figure><p id="2189" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">提取列号和行号。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pk pg l"/></div></figure><ul class=""><li id="95bc" class="nb nc iu lt b lu mk lx ml le nv li nw lm nx mj ny nh ni nj bi translated"><strong class="lt iv"> <em class="mt">选择聚类数</em> </strong></li></ul><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pj pg l"/></div></figure><ul class=""><li id="9615" class="nb nc iu lt b lu mk lx ml le nv li nw lm nx mj ny nh ni nj bi translated"><strong class="lt iv"> <em class="mt">随机选择质心</em> </strong></li></ul><p id="83d1" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">定义特征尺寸的空数组。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pk pg l"/></div></figure><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pl pg l"/></div></figure><p id="705c" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">为 3 个簇随机选择 3 个质心。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pf pg l"/></div></figure><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pm pg l"/></div></figure><ul class=""><li id="ae2c" class="nb nc iu lt b lu mk lx ml le nv li nw lm nx mj ny nh ni nj bi translated"><strong class="lt iv"> <em class="mt">下面的代码实现了 K-means 聚类概述章节</em> </strong>中提到的  <code class="fe mp mq mr ms b"><strong class="lt iv"><em class="mt">step-3, step-4 and step-5</em></strong></code> <strong class="lt iv"> <em class="mt"/></strong></li></ul><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pn pg l"/></div></figure><p id="813b" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">我在上面的代码中添加了一些命令，以便于理解功能。</p><ul class=""><li id="db7a" class="nb nc iu lt b lu mk lx ml le nv li nw lm nx mj ny nh ni nj bi translated"><strong class="lt iv"> <em class="mt">可视化集群</em> </strong></li></ul><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="po pg l"/></div></figure><p id="5ee5" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated">每个聚类在 3D 空间中用不同的颜色表示。</p><h2 id="156a" class="kv kw iu bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">结论</h2><p id="c5a5" class="pw-post-body-paragraph lr ls iu lt b lu lv jv lw lx ly jy lz le ma mb mc li md me mf lm mg mh mi mj in bi translated">K-means 聚类算法简单易用。在实现算法之前，我们需要小心算法的用例以及底层工作原理。对于非常复杂的分布式数据，该算法效果并不好。</p><p id="8bc4" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated"><code class="fe mp mq mr ms b">I am writing a series of articles on machine learning. The<strong class="lt iv"> </strong><a class="ae nk" href="https://medium.com/towards-data-science/knn-algorithm-from-scratch-37febe0c15b3" rel="noopener"><strong class="lt iv">article describes KNN</strong></a> algorithm implementation from scratch.</code></p></div><div class="ab cl mu mv hy mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="in io ip iq ir"><blockquote class="pp pq pr"><p id="fd07" class="lr ls mt lt b lu mk jv lw lx ml jy lz ps mm mb mc pt mn me mf pu mo mh mi mj in bi translated">最后，如果你觉得这篇文章有帮助，别忘了给我<code class="fe mp mq mr ms b"><a class="ae nk" href="https://medium.com/@mzh706" rel="noopener"><strong class="lt iv">follow</strong></a></code>。也可以用我的<code class="fe mp mq mr ms b"><a class="ae nk" href="https://mzh706.medium.com/membership" rel="noopener"><strong class="lt iv">referral link</strong></a></code>加入 medium。通过电子邮件获取我所有的文章更新<code class="fe mp mq mr ms b"><a class="ae nk" href="https://mzh706.medium.com/subscribe" rel="noopener"><strong class="lt iv">subscribe </strong></a></code>。</p></blockquote></div><div class="ab cl mu mv hy mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="in io ip iq ir"><h2 id="078a" class="kv kw iu bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">参考</h2><ol class=""><li id="eb92" class="nb nc iu lt b lu lv lx ly le nd li ne lm nf mj ng nh ni nj bi translated"><a class="ae nk" href="https://www.ibm.com/cloud/learn/unsupervised-learning#:~:text=Unsupervised%20learning%2C%20also%20known%20as,the%20need%20for%20human%20intervention." rel="noopener ugc nofollow" target="_blank">什么是无监督学习？IBM </a></li><li id="61f5" class="nb nc iu lt b lu nl lx nm le nn li no lm np mj ng nh ni nj bi translated">https://www.javatpoint.com/unsupervised-machine-learning</li><li id="c4bd" class="nb nc iu lt b lu nl lx nm le nn li no lm np mj ng nh ni nj bi translated"><a class="ae nk" href="https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages#:~:text=Advantages%20of%20k%2Dmeans&amp;text=Guarantees%20convergence.,sizes%2C%20such%20as%20elliptical%20clusters." rel="noopener ugc nofollow" target="_blank"> k-Means 优缺点|机器学习|谷歌开发者</a></li><li id="9dc9" class="nb nc iu lt b lu nl lx nm le nn li no lm np mj ng nh ni nj bi translated">辛格、亚达夫和拉纳(2013 年)。具有三种不同距离度量的 K-means。<em class="mt">《国际计算机应用杂志》</em>，<em class="mt"> 67 </em> (10)。</li><li id="edd7" class="nb nc iu lt b lu nl lx nm le nn li no lm np mj ng nh ni nj bi translated"><a class="ae nk" rel="noopener" target="_blank" href="/9-distance-measures-in-data-science-918109d069fa">https://towards data science . com/9-distance-measures-in-data-science-918109d 069 fa</a></li><li id="3d20" class="nb nc iu lt b lu nl lx nm le nn li no lm np mj ng nh ni nj bi translated">Zubair，m .、Iqbal，M.A .、Shil，A. <em class="mt">等</em>一种面向高效数据驱动建模的改进 K 均值聚类算法。<em class="mt">安。数据。Sci。</em> (2022)。【https://doi.org/10.1007/s40745-022-00428-2 T4】</li></ol></div><div class="ab cl mu mv hy mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="in io ip iq ir"><p id="98e7" class="pw-post-body-paragraph lr ls iu lt b lu mk jv lw lx ml jy lz le mm mb mc li mn me mf lm mo mh mi mj in bi translated"><strong class="lt iv"> <em class="mt">其他相关文章。你可以看看。</em>T9】</strong></p><div class="on oo gq gs op oq"><a rel="noopener follow" target="_blank" href="/knn-algorithm-from-scratch-37febe0c15b3"><div class="or ab fp"><div class="os ab ot cl cj ou"><h2 class="bd iv gz z fq ov fs ft ow fv fx it bi translated">KNN 算法从零开始</h2><div class="ox l"><h3 class="bd b gz z fq ov fs ft ow fv fx dk translated">KNN 算法的实现和细节解释</h3></div><div class="oy l"><p class="bd b dl z fq ov fs ft ow fv fx dk translated">towardsdatascience.com</p></div></div><div class="oz l"><div class="pv l pb pc pd oz pe kt oq"/></div></div></a></div><div class="on oo gq gs op oq"><a rel="noopener follow" target="_blank" href="/efficient-k-means-clustering-algorithm-with-optimum-iteration-and-execution-time-9358a794406c"><div class="or ab fp"><div class="os ab ot cl cj ou"><h2 class="bd iv gz z fq ov fs ft ow fv fx it bi translated">具有最优迭代和执行时间的高效 K-均值聚类算法</h2><div class="ox l"><h3 class="bd b gz z fq ov fs ft ow fv fx dk translated">高效 K-均值聚类算法的实现:研究成果</h3></div><div class="oy l"><p class="bd b dl z fq ov fs ft ow fv fx dk translated">towardsdatascience.com</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe kt oq"/></div></div></a></div><div class="on oo gq gs op oq"><a rel="noopener follow" target="_blank" href="/road-map-from-naive-bayes-theorem-to-naive-bayes-classifier-6395fc6d5d2a"><div class="or ab fp"><div class="os ab ot cl cj ou"><h2 class="bd iv gz z fq ov fs ft ow fv fx it bi translated">从朴素贝叶斯定理到朴素贝叶斯分类器的路线图(Stat-09)</h2><div class="ox l"><h3 class="bd b gz z fq ov fs ft ow fv fx dk translated">朴素贝叶斯分类器的完整指南，从头开始实现</h3></div><div class="oy l"><p class="bd b dl z fq ov fs ft ow fv fx dk translated">towardsdatascience.com</p></div></div><div class="oz l"><div class="pw l pb pc pd oz pe kt oq"/></div></div></a></div></div></div>    
</body>
</html>